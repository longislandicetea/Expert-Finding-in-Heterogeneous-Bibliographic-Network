#index 265773
#* Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence
#@ Jack Mostow;Charles Rich;Bruce Buchanan
#t 1998
#c 10

#index 265781
#* Learning evaluation functions for global optimization and Boolean satisfiability
#@ Justin A. Boyan;Andrew W. Moore
#t 1998
#c 10
#% 46262
#% 173793
#% 174161
#% 210434
#% 217814
#% 670853
#% 703288
#% 708120
#% 1478779
#% 1650783
#! This paper describes STAGE, a learning approach to automatically improving search performance on optimization problems. STAGE learns an evaluation function which predicts the outcome of a local search algorithm, such as hillclimbing or WALKSAT, as a function of state features along its search trajectories. The learned evaluation function is used to bias future search trajectories toward better optima. We present positive results on six large-scale optimization domains.

#index 265782
#* The interactive museum tour-guide robot
#@ Wolfram Burgard;Armin B. Cremers;Dieter Fox;Dirk Hähnel;Gerhard Lakemeyer;Dirk Schulz;Walter Steiner;Sebastian Thrun
#t 1998
#c 10
#% 39654
#% 241012
#% 247930
#% 263035
#% 266407
#% 266410
#% 267315
#% 380686
#% 418645
#% 644560
#% 669507
#% 696198
#% 1290038
#% 1476254
#! This paper describes the software architecture of an autonomous tour-guideltutor robot. This robot was recently deployed in the "Deutsches Museum Bonn," were it guided hundreds of visitors through the museum during a six-day deployment period. The robot's control software integrates low-level probabilistic reasoning with high-level problem solving embedded in first order logic. A collection of software innovations, described in this paper, enabled the robot to navigate at high speeds through dense crowds, while reliably avoiding collisions with obstacles--some of which could not even be perceived. Also described in this paper is a user interface tailored towards non-expert users, which was essential for the robot's success in the museum. Based on these experiences, this paper argues that time is ripe for the development of AI-based commercial service robots that assist people in everyday life.

#index 265783
#* Acceleration methods of numeric CSPc
#@ Yahia Lebbah;Olivier Lhomme
#t 1998
#c 10
#% 21144
#% 126388
#% 160383
#% 181030
#% 225336
#% 320265
#% 1273569
#! This paper introduces a new way of accelerating the convergence of numeric csp filtering algorithms, through the use of extrapolation methods. Extrapolation methods are used in numerical analysis to accelerate the convergence of real number sequences. We will show how to use them for solving numeric cSPs, leading to drastic improvement in efficiency.

#index 265784
#* Minimal social laws
#@ David Fitoussi;Moshe Tennenholtz
#t 1998
#c 10
#% 92695
#% 122889
#% 181622
#% 243698
#% 1272361
#% 1273211
#% 1275316
#! Research on social laws in computational environments has proved the usefulness of the law-based approach for the coordination of multi-agent systems. Though researchers have noted that the imposition of a specification could be attained by a variety of different laws, there has been no attempt to identify a criterion for selection among alternative useful social laws. We propose such a criterion which is based on the notion of minimality. A useful social law puts constraints on the agents' actions in such a way that as a result of these constraints, they are able to achieve their goals. A minimal social law is a useful social law that minimizes the amount of constraints the agents shall obey. Minimal social laws give an agent maximal flexibility in choosing a new behavior as a function of various local changes either in his capabilities or in his objectives, without interfering with the other agents. We show that this concept can be usefully applied to a problem in robotics and present a computational study of minimal social laws.

#index 265785
#* Optimal auctions revisited
#@ Dov Monderer;Moshe Tennenholtz
#t 1998
#c 10
#! The Internet offers new challenges to the fields of economics and artificial intelligence. This paper addresses several basic problems inspired by the adaptation of economic mechanisms, and auctions in particular, to the Internet. Computational environments such as the Internet offer a high degree of flexibility in auctions'rules. This makes the study of optimal auctions especially interesting in such environments. Although the problem of optimal auctions has received a lot of attention in economics, only partial solutions are supplied in the existing literature. We present least upper bounds (l.u.b) Rn on the revenue obtained by a seller in any auction with n participants. Our bounds imply that if the number of participants is large then the revenue obtained by standard auctions (e.g., English auctions) approach the theoretical bound. Our results heavily rely on the risk-aversion assumption made in the economics literature. We further show that without this assumption, the seller's revenue (for a fixed number of participants) may significantly exceed the upper bound.

#index 265788
#* Leveled commitment contracts with myopic and strategic agents
#@ Martin R. Andersson;Tuomas W. Sandholm
#t 1998
#c 10
#% 208275
#% 636336
#% 704123
#% 1013352
#% 1499484
#! In automated negotiation systems consisting of self-interested agents, contracts have traditionally been binding, i.e., impossible to breach. Such contracts do not allow the agents to efficiently deal with future events. This deficiency can be tackled by using a leveled commitment contracting protocol which allows the agents to decommit from contracts by paying a monetary penalty to the contracting partner. The efficiency of such protocols depends heavily on how the penalties are decided. In this paper, different leveled commitment protocols and their parameterizations are empirically compared to each other and to several full commitment protocols. In the different experiments, the agents are of different types: self-interested or cooperative, and they can perform different levels of lookahead.Surprisingly, self-interested myopic agents reach a higher social welfare quicker than cooperative myopic agents when decommitment penalties are low. The social welfare in settings with agents that performed lookahead did not vary as much with the decommitment penalty as the social welfare in settings that consisted of myopic agents. For a short range of values of the decommitment penalty, myopic agents performed almost as well as agents that performed lookahead. In all of the settings studied, the best way to set the decommitment penalties was to choose low penalties, but ones that were greater than zero. This indicates that leveled commitment contracting protocols outperform both full commitment protocols and commitment free protocols.

#index 265790
#* Anytime coalition structure generation with worst case guarantees
#@ Tuomas Sandholm;Kate Larson;Martin Andersson;Onn Shehory;Fernando Tohmé
#t 1998
#c 10
#% 160153
#% 171142
#% 204318
#% 233135
#% 452545
#% 1275312
#% 1499485
#! Coalition formation is a key topic in multiagent systems. One would prefer a coalition structure that maximizes the sum of the values of the coalitions, but often the number of coalition structures is too large to allow exhaustive search for the optimal one. But then, can the coalition structure found via a partial search be guaranteed to be within a bound from optimum?We show that none of the previous coalition structure generation algorithms can establish any bound because they search fewer nodes than a threshold that we show necessary for establishing a bound. We present an algorithm that establishes a tight bound within this minimal amount of search, and show that any other algorithm would have to search strictly more. The fraction of nodes needed to be searched approaches zero as the number of agents grows.If additional time remains, our anytime algorithm searches further, and establishes a progressively lower tight bound. Surprisingly, just searching one more node drops the bound in half. As desired, our algorithm lowers the bound rapidly early on, and exhibits diminishing returns to computation. It also drastically outperforms its obvious contenders. Finally, we show how to distribute the desired search across self-interested manipulative agents.

#index 265792
#* A motivational system for regulating human-robot interaction
#@ Cynthia Breazeal (Ferrell)
#t 1998
#c 10
#% 241096
#% 704951
#! This paper presents a motivational system for an autonomous robot which is designed to regulate human-robot interaction. The mode of social interaction is that of a caretaker-infant dyad where a human acts as the caretaker for the robot. An infant's emotions and drives playa very important role in generating meaningful interactions with the caretaker, and regulating these interactions to maintain an environment suitable for the learning process (Bullowa 1979). Similarly, the learning task for the robot is to apply various communication skills acquired during social exchanges to manipulate the caretaker such that its drives are satisfied. Toward this goal, the motivational system implements drives, emotions, and facial expressions. Although the details of the learning itself are beyond the scope of this paper, this work represents an important step toward realizing robots that can engage in meaningful bi-directional social interactions with humans.

#index 265793
#* Emotion model for life-like agent and its evaluation
#@ Hirohide Ushida;Yuji Hirayama;Hiroshi Nakajima
#t 1998
#c 10
#% 18600
#% 52577
#% 68009
#% 127564
#% 198113
#% 240949
#% 518789
#% 1478731
#! This paper proposes an emotion model for life-like agents with emotions and motivations. This model consists of reactive and deliberative mechanisms. The former generates low-level instantaneous responses to external stimuli that come from the real world and virtual worlds. The latter mechanism especially focuses on emotions. A basic idea of the model comes from a psychological theory, called the cogniti ve appraisal theory. In the model, cognitive and emotional processes interact with each other based on the theory. A multi-module architecture is employed in order to carry out the interactions. The model also has a learning mechanism to diversify behavioral patterns. These features are effective in giving users the illusion of life. We applied the proposed model to characters in a virtual world and show the results obtained from three experiments with users.

#index 265794
#* When robots weep: emotional memories and decision-making
#@ Juan D. Velásquez
#t 1998
#c 10
#% 18600
#% 127564
#% 154006
#% 159119
#% 170982
#% 198113
#% 238395
#% 1275296
#% 1383858
#% 1478731
#! We describe an agent architecture that integrates emotions, drives, and behaviors, and that focuses on modeling some of the aspects of emotions as fundamental components within the process of decision-making. We show how the mechanisms of primary emotions can be used as building blocks for the acquisition of emotional memories that serve as biasing mechanisms during the process of making decisions and selecting actions. The architecture has been implemented into an object-oriented framework that has been successfully used to develop and control several synthetic agents and which is currently being used as the control system for an emotional pet robot.

#index 265795
#* Natural language multiprocessing: a case study
#@ Enrico Pontelli;Gopal Gupta;Janyce Wiebe;David Farwell
#t 1998
#c 10
#% 23804
#% 171038
#% 395240
#% 464741
#% 660996
#% 705885
#% 741056
#% 748529
#% 748540
#! This paper presents two case studies of parallelization of large Natural Language Processing (NLP) applications using a parallel logic programming system (called "ACE") that automatically exploits implicit parallelism. The first system considered is Artwork, a system for semantic disambiguation, speech act resolution, and temporal reference resolution. The second system is ULTRA, a multilingual translation system. Both applications were originally developed in Prolog without any consideration for parallel processing. The results obtained confirm that NLP is a ripe area for exploitation of parallelism. Most previous work on parallelism in NLP focused primarily on parallelizing the parsing phase of language processing. The case studies presented here show that parallelism is also present in the semantic and discourse processing phases, which are often the most computationally intensive part of the application.

#index 265796
#* Metacognition in software agents using classifier systems
#@ Zhaohua Zhang;Stan Franklin;Dipankar Dasgupta
#t 1998
#c 10
#% 114994
#% 186478
#% 369236
#% 557050
#% 638177
#% 686757
#% 1022798
#% 1022822
#! Software agents "living" and acting in a real world software environment, such as an operating system, a network, or a database system, can carry out many tasks for humans. Metacognition is very important for humans. It guides people to select, evaluate, revise, and abandon cognitive tasks, goals, and strategies. Thus, metacognition plays an important role in human-like software agents. Metacognition includes metacognitive knowledge, metacognitive monitoring, and metacognitive regulation. Conscious Mattie (CMattie), "living" in a Unix machine, automatically reads and understands email concerning seminars (in natural language), and composes and distributes weekly seminar schedule announcements. CMattie implements Baar's global workspace theory of consciousness and some other cognitive theories concerning metacognition, episodic memory, emotions, and learning. Thus, the CMattie project has its cognitive science side (cognitive modeling) as well as its computer science side (intelligent software). This paper describes a case study of the design and implementation of modeling meta cognition in software agents like CMattie by using a classifier system.

#index 265797
#* Agents that work in harmony by knowing and fulfilling their obligations
#@ Mihai Barbuceanu
#t 1998
#c 10
#% 82804
#% 136356
#% 181622
#% 189698
#% 240956
#% 518814
#% 748582
#% 1477084
#% 1478733
#% 1478767
#! Societies constrain the behavior of agents by imposing multiple, often contradictory, obligations and interdictions amongst them. To work in harmony, agents must find ways to satisfy these constraints, or to break less important ones when necessary. In this paper, we present a solution to this problem based on a representation of obligations and interdictions in an organizational framework, together with an inference method that also decides which obligations to break in contradictory situations. These are integrated in an operational, practically useful agent development language that covers the spectrum from defining organizations, roles, agents, obligations, goals, conversations to inferring and executing coordinated agent behaviors in multi-agent applications. One strength of the approach is the way it supports negotiation by exchanging deontic constraints amongst agents. We illustrate this and the entire system with a negotiated solution to the feature interaction problem in the telecommunications industry.

#index 265798
#* What is wrong with us? Improving robustness through social diagnosis
#@ Gal A. Kaminka;Milind Tambe
#t 1998
#c 10
#% 69486
#% 75896
#% 189698
#% 215532
#% 241027
#% 1272316
#% 1476265
#% 1478730
#% 1478818
#% 1499477
#! Robust behavior in complex, dynamic environments mandates that intelligent agents autonomously monitor their own run-time behavior, detect and diagnose failures, and attempt recovery. This challenge is intensified in multiagent settings, where the coordinated and competitive behaviors of other agents affect an agent's own performance. Previous approaches to this problem have often focused on single agent domains and have failed to address or exploit key facets of multi-agent domains, such as handling team failures. We present SAM, a complementary approach to monitoring and diagnosis for multi-agent domains that is particularly well-suited for collaborative settings. SAM includes the following key novel concepts: First, SAM's failure detection technique, inspired by social psychology, utilizes other agents as information sources and detects failures both in an agent and in its teammates. Second, SAM performs social diagnosis, reasoning about the failures in its team using an explicit model of teamwork (previously, teamwork models have been employed only in prescribing agent behaviors in teamwork). Third, SAM employs model sharing to alleviate the inherent inefficiencies associated with representing multiple agent models. We have implemented SAM in a complex, realistic multi-agent domain, and provide detailed empirical results assessing its benefits.

#index 265799
#* Procedural help in Andes: generating hints using a Bayesian network student model
#@ Abigail S. Gertner;Cristina Conati;Kurt VanLehn
#t 1998
#c 10
#% 44876
#% 147680
#% 180201
#% 191979
#% 568856
#% 1650681
#! One of the most important problems for an intelligent tutoring system is deciding how to respond when a student asks for help. Responding cooperatively requires an understanding of both what solution path the student is pursuing, and the student's current level of domain knowledge. Andes, an intelligent tutoring system for Newtonian physics, refers to a probabilistic student model to make decisions about responding to help requests. Andes' student model uses a Bayesian network that computes a probabilistic assessment of three kinds of information: (I) the student's general knowledge about physics, (2) the student's specific knowledge about the current problem, and (3) the abstract plans that the student may be pursuing to solve the problem. Using this model, Andes provides feedback and hints tailored to the student's knowledge and goals.

#index 265800
#* Generating coordinated natural language and 3D animations for complex spatial explanations
#@ Stuart G. Towns;Charles B. Callaway;James C. Lester
#t 1998
#c 10
#% 21734
#% 73518
#% 109058
#% 114522
#% 135335
#% 145399
#% 145400
#% 145644
#% 175130
#% 177915
#% 180124
#% 1003706
#% 1134782
#% 1290062
#% 1478783
#% 1499486
#% 1499487
#! Dynamically providing students with clear explanations of complex spatial concepts is critical for a broad range of knowledge-based educational and training systems. This calls for a realtime solution that can dynamically create 3D animated explanations that artfully integrate well-chosen speech with rich visualizations. Unfortunately, planning the integrated creation of 3D animation and spatial linguistic utterances in realtime requires coordinating the visual presentation of 3D objects and generating appropriate spatial phrases that accurately reflect the relative position, orientation, and direction of the objects presented. We present a visuo-linguistic framework for generating multimedia spatial explanations combining 3D animation and speech that complement one another. Because 3D animation planners require spatial knowledge in a geometric form and natural language generators require spatial knowledge in a linguistic form, a realtime multimedia planner interposed between the visual and linguistic components can serve as a mediator. This framework has been implemented in CINESPEAK, a multimedia explanation generator consisting of a visuo-linguistic mediator, a 3D animation planner, and a realtime natural language generator with a speech synthesizer. CINESPEAK has been used in conjunction with a prototype 3D learning environment in the domain of physics to generate realtime multimedia explanations of three dimensional electromagnetic fields, forces, and electrical current.

#index 265801
#* Reasoning under inconsistency based on implicitly-specified partial qualitative probability relations: a unified framework
#@ S. Benferhat;D. Dubois;J. Lang;H. Prade;A. Saffiotti;P. Smets
#t 1998
#c 10
#% 130212
#% 167544
#% 366370
#% 1273433
#% 1273626
#% 1290085
#% 1650630
#% 1650798
#! Coherence-based approaches to inconsistency handling proceed by selecting preferred consistent subbases of the belief base according to a predefined method which takes advantage of explicitly stated priorities. We propose here a general framework where the preference relation between subsets of the belief base is induced by a system of constraints directly expressed by the user. Postulates taking their source in the qualitative modelling of uncertainty, either probabilistic or possibilistic, are used for completing the implicit specification of the preference relations. This enables us to define various types of preference relations, including as particular cases several well-known systems such as Brewka's preferred subtheories or the lexicographical system. Since the number of preferred consistent subbases may be prohibitive, we propose to compile the inconsistent belief base into a new one from which it is easier to select one preferred consistent subbase.

#index 265802
#* Belief revision with unreliable observations
#@ Craig Boutilier;Nir Friedman;Joseph Y. Halpern
#t 1998
#c 10
#% 90860
#% 163719
#% 188086
#% 211580
#% 224753
#% 242666
#% 243704
#% 250128
#% 1290096
#% 1290098
#% 1650781
#% 1650784
#! Research in belief revision has been dominated by work that lies firmly within the classic AGM paradigm, characterized by a well-known set of postulates governing the behavior of "rational" revision functions. A postulate that is rarely criticized is the success postulate: the result of revising by an observed proposition 驴 results in belief in 驴. This postulate, however, is often undesirable in settings where an agent's observations may be imprecise or noisy. We propose a semantics that captures a new ontology for studying revision functions, which can handle noisy observations in a natural way while retaining the classical AGM model as a special case. We present a characterization theorem for our semantics, and describe a number of natural special cases that allow ease of specification and reasoning with revision functions. In particular, by making the Markov assumption, we can easily specify and reason about revision.

#index 265803
#* Toward design as collaboration
#@ Susan L. Epstein
#t 1998
#c 10
#% 59933
#% 409503
#% 1273347
#% 1273642
#% 1478882
#% 1650801
#! In design, multiple disparate goals must be addressed simultaneously. It is the thesis of this work that problems in two-dimensional layout design can be solved by collaboration among single-goal, intelli gent agents, each responsible for a class of objects and responsive to explicit metrics. In this model, each agent produces conflict-free designs for its own class of objects, and then, when objects conflict with each other in the combined design, the agents that own those objects address the conflicts. A limitedly rational implementation demonstrates its efficacy for park layout design in the two-dimensional plane.

#index 265804
#* An architecture for exploring large design spaces
#@ John R. Josephson;B. Chandrasekaran;Mark Carroll;Naresh Iyer;Bryon Wasacz;Giorgio Rizzoni;Qingyuan Li;David A. Erb
#t 1998
#c 10
#% 28144
#% 36672
#% 68659
#% 84384
#% 90718
#% 837649
#% 1272294
#! We describe an architecture for exploring very large design spaces, for example, spaces that arise when design candidates are generated by combining components systematically from component libraries. A very large number of candidates are methodically considered and evaluated. This architecture is especially appropriate during the stage of conceptual design when high-level design decisions are under consideration, multiple evaluation criteria apply, and a designer seeks assurance that good design possibilities have not been overlooked. We present a filtering technique based on a dominance criterion that can be used to select, from millions of design candidates, a relatively small number of promising candidates for further analysis. The dominance criterion is lossless in that it insures that each candidate not selected is inferior to at least one of the selected candidates. We also describe an interactive interface in which the selected designs are presented to the designer for analysis of tradeoffs and further exploration. In our current implementation, the computational load is distributed among a large number of workstations in a client-server computing environment. We describe the results of experiments using the architecture to explore designs for hybrid electric vehicles. In a recent experiment more than two million distinct designs were evaluated.

#index 265805
#* Constructing the correct diagnosis when symptoms disappear
#@ Nancy E. Reed
#t 1998
#c 10
#% 21137
#% 21138
#% 53411
#% 67569
#% 78634
#% 107135
#% 116331
#% 129326
#% 175292
#% 183497
#% 215466
#% 1272957
#! When multiple defects (also called diseases or faults) are present, there is a possibility of interactions between the defects. When defects interact, the cues (data obtainable) for a combination of defects is not a simple sum of the cues observable for the component defects. Expected cues may be missing, altered, or new cues may appear. Each of these alterations of cues makes diagnosis more difficult, as the correct defect combination may not even be considered (triggered) by a diagnostic system. We present an algorithm for heuristic solution construction that integrates multiple types of information about the case. Solutions are evaluated based on how many of the abnormal cues are accounted for, with a method that combines cues that may be altered due to interactions between defects. The method can account for cues that combine with one another in three basic ways, set union, additively and ordered dominance (some values mask other values) or with a combination of those basic ways.For the solution space of one task, diagnosing congenital heart defects, we considered seven major defects and found the solution space (exhaustive) was reduced by approximately 50% because some of the defects could not physically occur together. Experimental results on cases from hospital files demonstrate the effectiveness of the heuristic solution construction algorithm to generate the correct solution early which reduced the number of solutions explored (compared to an exhaustive search) even further on most cases. With the computational power of current workstations, even cases requiring exploration of this entire solution space required less than 4 minutes of CPU time per case.

#index 265806
#* Structured representation of complex stochastic systems
#@ Nir Friedman;Daphne Koller;Avi Pfeffer
#t 1998
#c 10
#% 44876
#% 75936
#% 128629
#% 179925
#% 190683
#% 214028
#% 246836
#% 1650568
#% 1650666
#% 1650731
#% 1650767
#! This paper considers the problem of representing complex systems that evolve stochastically over time. Dynamic Bayesian networks provide a compact representation for stochastic processes. Unfortunately, they are often unwieldy since they cannot explicitly model the complex organizational structure of many real life systems: the fact that processes are typically composed of several interacting subprocesses, each of which can, in tum, be further decomposed. We propose a hierarchically structured representation language which extends both dynamic Bayesian networks and the object-oriented Bayesian network framework of [9], and show that our language allows us to describe such systems in a natural and modular way. Our language supports a natural representation for certain system characteristics that are hard to capture using more traditional frameworks. For example, it allows us to represent systems where some processes evolve at a different rate than others, or systems where the processes interact only intermittently. We provide a simple inference mechanism for our representation via translation to Bayesian networks, and suggest ways in which the inference algorithm can exploit the additional structure encoded in our representation.

#index 265807
#* Solving very large weakly coupled Markov decision processes
#@ Nicolas Meuleau;Milos Hauskrecht;Kee-Eung Kim;Leonid Peshkin;Leslie Pack Kaelbling;Thomas Dean;Craig Boutilier
#t 1998
#c 10
#% 75936
#% 194647
#% 224762
#% 272663
#% 272665
#% 363744
#% 393786
#% 644560
#% 1271882
#% 1290041
#% 1290043
#% 1291498
#% 1478746
#! We present a technique for computing approximately optimal solutions to stochastic resource allocation problems modeled as Markov decision processes (MDPS). We exploit two key properties to avoid explicitly enumerating the very large state and action spaces associated with these problems. Fist, the problems are composed of multiple tasks whose utilities are independent. Second, the actions taken with respect to (or resources allocated to) a task do not influence the status of any other task. We can therefore view each task as an MDP. However these MDPS are weakly coupled by resource constraints: actions selected for one MDP restrict the actions available to others. We describe heuristic techniques for dealing with several classes of constraints that use the solutions for individual MDPS to construct an approximate global solution. We demonstrate this technique on problems involving thousands of tasks, approximating the solution to problems that are far beyond the reach of standard methods.

#index 266086
#* Speech recognition with dynamic Bayesian networks
#@ Geoffrey Zweig;Stuart Russell
#t 1998
#c 10
#% 75936
#% 101510
#% 137711
#% 185079
#% 225837
#% 246836
#% 395470
#% 405882
#% 624713
#% 673654
#% 707089
#% 1290046
#! Dynamic Bayesian networks (DBNs) are a useful tool for representing complex stochastic processes. Recent developments in inference and learning in DBNs allow their use in real-world applications. In this paper, we apply DBNs to the problem of speech recognition. The factored state representation enabled by DBNs allows us to explicitly represent long-term articulatory and acoustic context in addition to the phonetic-state information maintained by hidden Markov models (HMMs). Furthermore it enables us to model the short-term correlations among multiple observation streams within single time-frames. Given a DBN structure capable of representing these long- and short-term correlations, we applied the EM algorithm to learn models with up to 500,000 parameters. The use of structured DBN models decreased the error rate by 12 to 29% on a large-vocabulary isolated-word recognition task, compared to a discrete HMM; it also improved significantly on other published results for the same task. This is the first successful application of DBNs to a large-scale speech recognition problem. Investigation of the learned models indicates that the hidden state variables are strongly correlated with acoustic properties of the speech signal.

#index 266089
#* Multimodal reasoning for automatic model construction
#@ Reinhard Stolle;Elizabeth Bradley
#t 1998
#c 10
#% 1116
#% 6200
#% 7688
#% 17145
#% 24538
#% 42473
#% 109846
#% 109848
#% 109851
#% 146596
#% 166232
#% 207263
#% 565051
#% 1272579
#% 1476267
#% 1499592
#! This paper describes a program called PRET that automates system identification, the process of finding a dynamical model of a black-box system. PRET performs both structural identification and parameter estimation by integrating several reasoning modes: qualitative reasoning, qualitative simulation, numerical simulation, geometric reasoning, constraint reasoning, resolution, reasoning with abstraction levels, declarative meta-level control, and a simple form of truth maintenance.Unlike other modeling programs that map structural or functional descriptions to model fragments, PRET combines hypotheses about the mathematics involved into candidate models that are intelligently tested against observations about the target system.We give two examples of system identification tasks that this automated modeling tool has successfully performed. The first, a simple linear system, was chosen because it facilitates a brief and clear presentation of PRET's features and reasoning techniques. In the second example, a difficult real-world modeling task, we show how PRET models a radio-controlled car used in the University of British Columbia's soccer-playing robot project.

#index 266091
#* Discovering admissible simultaneous equations of large scale systems
#@ Takashi Washio;Hiroshi Motoda
#t 1998
#c 10
#% 9526
#% 24538
#% 25884
#% 451038
#% 451039
#% 1271832
#! SSF is a system to discover the structure of simultaneous equations governing an objective process through experiments. SSF combined with another system SDS to discover a quantitative formula of a complete equation derives the quantitative model consisting of simultaneous equations reflecting the first principles underlying in the objective process. The power of SSF comes from the use of the complete subset structure in a set of simultaneous equations which can be experimentally identified. The theoretical foundations of the structure identification and the algorithm of SSF are described, and its efficiency and practicality are demonstrated and discussed with large scale working examples. This work is to promote the research of scientific discovery to a novel and promising direction, since the conventional equation discovery systems could not handle such a simultaneous equation process.

#index 266093
#* Decompositional, model-based learning and its analogy to diagnosis
#@ Brian C. Williams;William Millar
#t 1998
#c 10
#% 3460
#% 21138
#% 25886
#% 132173
#% 179984
#% 567876
#% 1272363
#% 1273478
#! A new generation of sensor rich, massively distributed autonomous system is being developed, such as smart buildings and reconfigurable factories. To achieve high performance these systems will need to accurately model themselves and their environment from sensor information. Accomplishing this on a grand scale requires automating the art of large-scale modeling. To this end we have developed decompositional, modelbased learning (DML). DML takes a parameterized model and sensed variables as input, decomposes It, and synthesizes a coordinated sequence of "simplest" estimation tasks. The method exploits a rich analogy between parameter estimation and consistency-based diagnosis. Moriarty, an implementation of DML, has been applied to thermal modeling of a smart building, demonstrating a significant improvement in learning rate.

#index 266095
#* What can knowledge representation do for semi-structured data?
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini
#t 1998
#c 10
#% 33376
#% 61005
#% 116299
#% 165656
#% 172927
#% 215671
#% 215674
#% 459260
#% 464720
#% 464724
#% 1268741
#% 1272306
#! The problem of modeling semi-structured data is important in many application areas such as multimedia data management, biological databases, digital libraries, and data integration. Graph schemas (Buneman et al. 1997) have been proposed recently as a simple and elegant formalism for representing semistructured data. In this model, schemas are represented as graphs whose edges are labeled with unary formulae of a theory, and the notions of conformance of a database to a schema and of subsumption between two schemas are defined in terms of a simulation relation. Several authors have stressed the need of extending graph schemas with various types of constraints, such as edge existence and constraints on the number of outgoing edges. In this paper we analyze the appropriateness of various knowledge representation formalisms for representing and reasoning about graph schemas extended with constraints. We argue that neither First Order Logic, nor Logic Programming nor Frame-based languages are satisfactory for this purpose, and present a solution based on very expressive Description Logics. We provide techniques and complexity analysis for the problem of deciding schema subsumption and conformance in various interesting cases, that differ by the expressive power in the specification of constraints.

#index 266102
#* Modeling Web sources for information integration
#@ Craig A. Knoblock;Steven Minton;José Luis Ambite;Naveen Ashish;Pragnesh Jay Modi;Ion Muslea;Andrew G. Philpot;Sheila Tejada
#t 1998
#c 10
#% 213437
#% 227886
#% 227987
#% 227992
#% 240955
#% 248801
#% 705442
#% 1271899
#% 1290109
#% 1290115
#% 1476298
#% 1478839
#% 1499470
#% 1499471
#! The Web is based on a browsing paradigm that makes it difficult to retrieve and integrate data from multiple sites. Today, the only way to do this is to build specialized applications, which are time-consuming to develop and difficult to maintain. We are addressing this problem by creating the technology and tools for rapidly constructing information agents that extract, query, and integrate data from web sources. Our approach is based on a simple, uniform representation that makes it efficient to integrate multiple sources. Instead of building specialized algorithms for handling web sources, we have developed methods for mapping web sources into this uniform representation. This approach builds on work from knowledge representation, machine learning and automated planning. The resulting system, called Ariadne, makes it fast and cheap to build new information agents that access existing web sources. Ariadne also makes it easy to maintain these agents and incorporate new sources as they become available.

#index 266103
#* An ontology for transitions in physical dynamic systems
#@ Pieter J. Mosterman;Feng Zhao;Gautam Biswas
#t 1998
#c 10
#% 518147
#% 518299
#% 1476267
#% 1789166
#! Physical systems often exhibit complex nonlinear behaviors in continuous time at multiple temporal and spatial scales. Abstractions simplify behavioral analysis and help focus on dominant system behaviors by defining sets of equivalent behavior types called modes. System behavior evolves in continuous modes with discrete transitions between modes. Subtle interactions between the continuous behaviors and discrete transitions need to be captured by well-defined hybrid modeling and analysis semantics. This paper presents a taxonomy of transition modes, and develops a formal semantics for transition conditions that lead to efficient and physically consistent simulation algorithms for physical systems.

#index 266104
#* A new architecture for automated modelling
#@ Neil Smith
#t 1998
#c 10
#% 109848
#% 109851
#% 132175
#% 179982
#% 179984
#% 207776
#% 407731
#! Existing automated modelling systems either rely on large, complex libraries or require complete access to the modelled system's behaviour, neither of which is desirable. To address these problems, a simpler architecture for modelling knowledge is described, based on the separation between ideal models of components and corrections that can be applied to these ideal models. The use of this architecture to develop accurate model boundaries is described, based on consideration of interactions within such ideal models. A novel algorithm for refining models is also proposed. This algorithm considers behavioural differences between models and applies the corrections that cause the greatest differences in behaviour. Finally, some models generated by this method are shown to be parsimonious.

#index 266105
#* Qualitative analysis of distributed physical systems with applications to control synthesis
#@ Christopher Bailey-Kellogg;Feng Zhao
#t 1998
#c 10
#% 1115
#% 4178
#% 109857
#% 201257
#% 374536
#% 674906
#% 1272294
#% 1476272
#% 1478748
#% 1499542
#! Many important physical phenomena, such as temperature distribution, air flow, and acoustic waves, are described as continuous, distributed parameter fields. Analyzing and controlling these physical processes and systems are common tasks in many scientific and engineering domains. However, the challenges are multifold: distributed fields are conceptually harder to reason about than lumped parameter models; computational methods are prohibitively expensive for complex spatial domains; the underlying physics imposes severe constraints on observability and controllability.This paper develops an ontological abstraction and a structure-based design mechanism, in a framework collectively known as spatial aggregation (SA), for reasoning about and synthesizing distributed control schemes for physical fields. The ontological abstraction models a physical field as a hierarchy of networks of spatial objects. SA applies a small number of generic operators to a field to compute concise structural descriptions such as iso-contours, gradient trajectories, and influence graphs. The design mechanism uses these representations to find feasible control configurations. We illustrate the mechanism using a thermal control problem from industrial heat treatment and demonstrate that the active exploitation of structural knowledge in physical fields yields a significant computational advantage.

#index 266106
#* Qualitative simulation as a temporally-extended constraint satisfaction problem
#@ Daniel J. Clancy;Benjamin J. Kuipers
#t 1998
#c 10
#% 1116
#% 55926
#% 101955
#% 166232
#% 417599
#% 679785
#% 1476266
#% 1478748
#! Traditionally, constraint satisfaction problems (CSPs) are characterized using a finite set of constraints expressed within a common, shared constraint language. When reasoning across time, however, it is possible to express both temporal and state-based constraints represented within multiple constraint languages. Qualitative simulation provides an instance of this class of CSP in which, traditionally, all solutions to the CSP are computed. In this paper, we formally describe this class of temporally-extended CSPs and situate qualitative simulation within this description. This is followed by a description of the DecSIM algorithm which is used to incrementally generate all possible solutions to a temporally-extended CSP. DecSIM combines problem decomposition, a tree-clustering algorithm and ideas similar to directed arc--consistency to exploit structure and causality within a qualitative model resulting in an exponential speed-up in simulation time when compared to existing techniques.

#index 266107
#* Backtracking algorithms for disjunctions of temporal constraints
#@ Kostas Stergiou;Manolis Koubarakis
#t 1998
#c 10
#% 100136
#% 172500
#% 184792
#% 210193
#% 210195
#% 224767
#% 231740
#% 534160
#% 1279714
#! We extend the framework of simple temporal problems studied originally by Dechter, Meiri and Pearl to consider constraints of the form x1 - y1 驴 r1 V... V xn - yn 驴 rn where x1... xn, y1... yn are variables ranging over the real numbers, r1... rn are real constants, and n 驴 1. We have implemented four progressively more efficient algorithms for the consistency checking problem for this class of temporal constraints. We have partially ordered those algorithms according to the number of visited search nodes and the number of performed consistency checks. Finally, we have carried out a series of experimental results on the location of the hard region. The results show that hard problems occur at a critical value of the ratio of disjunctions to variables. This value is between 6 and 7.

#index 266108
#* Fast transformation of temporal plans for efficient execution
#@ Ioannis Tsamardinos;Nicola Muscettola;Paul Morris
#t 1998
#c 10
#% 70370
#% 107137
#% 207879
#% 252826
#% 418651
#! Temporal plans permit significant flexibility in specifying the occurrence time of events. Plan execution can make good use of that flexibility. However, the advantage of execution flexibility is counterbalanced by the cost during execution of propagating the time of occurrence of events throughout the flexible plan. To minimize execution latency, this propagation needs to be very efficient. Previous work showed that every temporal plan can be reformulated as a dispatchable plan, Le., one for which propagation to immediate neighbors is sufficient. A simple algorithm was given that finds a dispatchable plan with a minimum number of edges in cubic time and quadratic space. In this paper, we focus on the efficiency of the reformulation process, and improve on that result. A new algorithm is presented that uses linear space and has time complexity equivalent to Johnson's algorithm for all-pairs shortest-path problems. Experimental evidence confirms the practical effectiveness of the new algorithm. For example, on a large commercial application, the performance is improved by at least two orders of magnitude. We further show that the dispatchable plan, already minimal in the total number of edges, can also be made minimal in the maximum number of edges incoming or outgoing at any node.

#index 266109
#* An algorithm to evaluate quantified Boolean formulae
#@ Marco Cadoli;Andrea Giovanardi;Marco Schaerf
#t 1998
#c 10
#% 131559
#% 183640
#% 210193
#% 558706
#% 1476298
#! The high computational complexity of advanced reasoning tasks such as belief revision and planning calls for efficient and reliable algorithms for reasoning problems harder than NP. In this paper we propose Evaluate, an algorithm for evaluating Quantified Boolean Formulae, a language that extends propositional logic in a way such that many advanced forms of propositional reasoning, e.g., reasoning about knowledge, can be easily formulated as evaluation of a QBF. Algorithms for evaluation of QBFs are suitable for the experimental analysis on a wide range of complexity classes, a property not easily found in other formalisms. Evaluate is based on a generalization of the Davis-Putnam procedure for SAT, and is guaranteed to work in polynomial space. Before presenting Evaluate, we discuss all the abstract properties of QBFs that we singled out to make the algorithm more efficient. We also briefly mention the main results of the experimental analysis, which is reported elsewhere.

#index 266110
#* Two forms of dependence in propositional logic: controllability and definability
#@ Jérôme Lang;Pierre Marquis
#t 1998
#c 10
#% 89961
#% 147926
#% 160188
#% 181220
#% 233132
#% 243719
#% 243721
#% 459577
#% 1275337
#% 1478845
#% 1499491
#! We investigate two forms of dependence between variables and/or formulas within a propositional knowledge base: controllability (a set of variables X controls a formula 驴 if there is a way to fix the truth value of the variables in X in order to achieve 驴 to have a prescribed truth value) and definability (X defines a variable y if every truth assignment of the variables in X enables us finding out the truth value of y). Several characterization results are pointed out, complexity issues are analyzed, and some applications of both notions, including decision under incomplete knowledge and/or partial observability, and hypothesis discrimination, are sketched.

#index 266111
#* Anytime approximate model reasoning
#@ Fabio Massacci
#t 1998
#c 10
#% 36815
#% 55925
#% 68240
#% 100134
#% 116625
#% 131357
#% 184793
#% 197422
#% 204396
#% 211584
#% 517276
#% 1279696
#! Propositional modal logics have two independent sources of complexity: unbounded logical omniscience and unbounded logical introspection. This paper discusses an approximation method to tame both of them, by merging propositional approximations with a new technique tailored for multimodal logics. It provides both skeptical and credulous approximations (or approximation that are neither of the two).On this semantics we build an anytime proof procedure with a simple modification to classical modal tableaux. The procedure yields approximate proofs whose precision increases as we have more resources (time, space etc.) and we analyze its semantical and computational "quality guarantees".

#index 266112
#* Algorithms for propositional KB approximation
#@ Yacine Boufkhad
#t 1998
#c 10
#% 159434
#% 204396
#% 288165
#% 288927
#% 289115
#% 1273542
#% 1273692
#% 1275334
#% 1275335
#% 1478773
#% 1499541
#! One of the obstacles to the effective compilation of propositional knowledge bases (KBs) using Hom approximations, as introduced by (Selman & Kautz 1991), is the lack of computationally feasible methods for generating Hom bounds. In this paper new algorithms for generating Hom Greatest Lower Bounds (GLB) that can apply to large size KBs, are presented. The approach is extended through a more general target language: the renamable Hom class. The conditions under which a renamable Hom formula is a renamable Hom GLB of a KB are established and algorithms for computing it are derived. These algorithms can be used in the other approaches based on computation of Hom or renamable lower bounds as (Boufkhad et al. 1997). The efficiency of these algorithms and the tightness with respect to the KB in terms of number of models of the bounds, are experimentally evaluated. The renamable Hom GLB proves to be closer to the KB than the Horn GLB.

#index 266113
#* A non-deterministic semantics for tractable inference
#@ James M. Crawford;David W. Etherington
#t 1998
#c 10
#% 93634
#% 100134
#% 100156
#% 210195
#% 1274600
#% 1478761
#% 1499510
#% 1499558
#! Unit resolution is arguably the most useful known algorithm for tractable reasoning in propositional logic. Intuitively, if one knows a, b, and a 驴 b 驴 c, then c should be an obvious implication. However, devising a tractable semantics that allows unit resolution has proven to be an elusive goal. We propose a 3-valued semantics for a tractable fragment of propositional logic that is inherently non-deterministic: the denotation of a formula is not uniquely determined by the denotation of the variables it contains. We show that this semantics yields a tractable, sound and complete decision procedure. We generalize this semantics to a family of semantics, tied to Dalal's notion of intricacy, of increasing deductive power and computational complexity.

#index 266114
#* Computing intersections of Horn theories for reasoning with models
#@ Thomas Eiter;Toshihide Ibaraki;Kazuhisa Makino
#t 1998
#c 10
#% 39702
#% 126392
#% 158909
#% 181220
#% 181339
#% 198240
#% 216972
#% 243723
#% 266114
#% 408396
#% 442755
#% 443185
#% 496601
#% 1272175
#% 1272324
#! We consider computational issues in combining logical knowledge bases represented by their characteristic models; in particular, we study taking their logical intersection. We present efficient algorithms or prove intractability for the major computation problems for Horn knowledge bases. We also consider an extension of Horn theories, for which negative results are obtained. They indicate that generalizing the positive results beyond Hom theories is not immediate.

#index 266115
#* The branching factor of regular search spaces
#@ Stefan Edelkamp;Richard E. Korf
#t 1998
#c 10
#% 2194
#% 190611
#% 539601
#! Many problems, such as the sliding-tile puzzles, generate search trees where different nodes have different numbers of children, in this case depending on the position of the blank. We show how to calculate the asymptotic branching factors of such problems, and how to efficiently compute the exact numbers of nodes at a given depth. This information is important for determining the complexity of various search algorithms on these problems. In addition to the slidingg-tile puzzles, we also apply our technique to Rubik's Cube. While our techniques are fairly straightforward, the literature is full of incorrect branching factors for these problems, and the errors in several incorrect methods are fairly subtle.

#index 266116
#* Complexity analysis admissible heuristic search
#@ Richard E. Korf;Michael Reid
#t 1998
#c 10
#% 208
#% 2194
#% 266115
#% 533951
#% 688914
#% 1478838
#! We analyze the asymptotic time complexity of admissible heuristic search algorithms such as A*, IDA*, and depth-first branch-and-bound. Previous analyses relied on an abstract analytical model, and characterize the heuristic function in terms of its accuracy, but do not apply to real problems. In contrast, our analysis allows us to accurately predict the performance of these algorithms on problems such as the slidingtile puzzles and Rubik's Cube. The heuristic function is characterized simply by the distribution of heuristic values in the problem space. Contrary to conventional wisdom, our analysis shows that the asymptotic heuristic branching factor is the same as the bruteforce branching factor, and that the effect of a heuristic function is to reduce the effective depth of search, rather than the effective branching factor.

#index 266117
#* On the conversion between non-binary constraint satisfaction problems
#@ Fahiem Bacchus;Peter van Beek
#t 1998
#c 10
#% 55926
#% 56471
#% 75817
#% 320265
#% 1268731
#% 1272481
#% 1275311
#% 1499497
#! It is well known that any non-binary discrete constraint satisfaction problem (CSP) can be translated into an equivalent binary CSP. Two translations are known: the dual graph translation and the hidden variable translation. However, there has been little theoretical or experimental work on how well backtracking algorithms perfonn on these binary representations in comparison to their perfonnance on the corresponding non-binary CSP. We present both theoretical and empirical results to help understand the tradeoffs involved. In particular, we show that translating a non-binary CSP into a binary representation can be a viable solution technique in certain circumstances. The ultimate aim of this research is to give guidance for when one should consider translating between non-binary and binary representations. Our results supply some initial answers to this question.

#index 266118
#* Generalizing partial order and dynamic backtracking
#@ Christian Bliek
#t 1998
#c 10
#% 160254
#% 1268731
#! Recently, two new backtracking algorithms, dynamic backtracking (DB) and partial order dynamic backtracking (PDB) have been presented. These algorithms have the property to be additive on disjoint subproblems and yet use only polynomial space. Unlike DB, PDB only imposes a partial search order and therefore appears to have more freedom than DB to explore the search space. However, both algorithms are not directly comparable in terms of flexibility. In this paper we present new backtracking algorithms that are obtained by relaxing the ordering conditions of PDB. This gives them additional flexibility while still being additive on disjoint subproblems. In particular, we show that our algorithms generalize both DB and PDB.

#index 266119
#* On the computation of local interchangeability in discrete constraint satisfaction problems
#@ Berthe Y. Choueiry;Guevara Noubir
#t 1998
#c 10
#% 130206
#% 1273576
#% 1275298
#% 1290116
#! In. [4], Freuder defines several types of interchangeability to capture the equivalence among the values of a variable in a discrete constraint satisfaction problem (CSP), and provides a procedure for computing one type of local interchangeability. In this paper, we first extend this procedure for computing a weak form of local interchangeability. Second, we show that the modified procedure can be used to generate a conjunctive decomposition of the CSP by localizing, in the CSP, independent subproblems. Third, for the case of constraints of mutual exclusion, we show that locally Interchangeable values can be computed in a straightforward manner, and that the only possible type of ~ocal interchangeability is the one that induces locally Independent subproblems. Finally, we give hints on how to exploit these results in practice, establish a lattice that relates some types of interchangeability, and identify directions for future research.

#index 266125
#* Supermodels and robustness
#@ Matthew L. Ginsberg;Andrew J. Parkes;Amitabha Roy
#t 1998
#c 10
#% 24546
#% 160248
#% 210195
#% 1478761
#% 1478782
#% 1499491
#! When search techniques are used to solve a practical problem, the solution produced is often brittle in the sense that small execution difficulties can have an arbitrarily large effect on the viability of the solution. The AI community has responded to this difficulty by investigating the development of "robust problem solvers" that are intended to be proof against this difficulty.We argue that robustness is best cast not as a property of the problem solver, but as a property of the solution. We introduce a new class of models for a logical theory, called supermodels, that captures this idea. Supermodels guarantee that the model in question is robust, and allow us to quantify the degree to which it is so.We investigate the theoretical properties of supermodels, showing that finding supermodels is typically of the same theoretical complexity as finding models. We provide a general way to modify a logical theory so that a model of the modified theory is a supermodel of the original. Experimentally, we show that the supermodel problem exhibits phase transition behavior similar to that found in other satisfiability work.

#index 266127
#* “Squeaky Wheel” optimization
#@ David E. Joslin;David P. Clements
#t 1998
#c 10
#% 383492
#% 408396
#! We describe a general approach to optimization which we term "Squeaky Wheel" Optimization (swo). In swo, a greedy algorithm is used to construct a solution which is then analyzed to find the trouble spots, i.e., those elements, that, if improved, are likely to improve the objective function score. That analysis is used to generate new priorities that determine the order in which the greedy algorithm constructs the next solution. This Construct/Analyze/Prioritize cycle continues until some limit is reached, or an acceptable solution is found.SWO can be viewed as operating on two search spaces: solutions and prioritizations. Successive solutions are only indirectly related, via the re-prioritization that results from analyzing the prior solution. Similarly, successive prioritizations are generated by constructing and analyzing solutions. This "coupled search" has some interesting properties, which we discuss.We report encouraging experimental results on two domains, scheduling problems that arise in fiber-optic cable manufacturing, and graph coloring problems. The fact that these domains are very different supports our claim that swo is a general technique for optimization.

#index 266129
#* Reversible DAC and other improvements for solving Max-CSP
#@ Javier Larrosa;Pedro Meseguer;Thomas Schiex;Gérard Verfaillie
#t 1998
#c 10
#% 36814
#% 126386
#% 535469
#% 1275308
#% 1275309
#% 1499492
#% 1499493
#! Following the work of R. Wallace on Max-CSP, later improved by J. Larrosa and P. Meseguer, we tested a number of possible improvements of the usage of directed arc consistency for the partial forward checking algorithm (PFC). The main improvement consists in exploiting a non standard. form of DAC called reversible DAC where each constraint is exploited in a direction which is not necessarily determined by the variable ordering and can change dynamically during the search. Other improvements include: (i) avoiding some constraint checks when forward-checking by exploiting the constraint checks perfonned during DAC preprocessing (ii) using a dynamic variable ordering during the search, (iii) maintaining the directed arc-consistency counts during the search as values get deleted. These improvements have been assessed empirically on random CSP instances. Some of them lead to very large performance gains with respect to the initial algorithm.

#index 266130
#* Branch and bound algorithm selection by performance prediction
#@ Lionel Lobjois;Michel Lemaître
#t 1998
#c 10
#% 115963
#% 126386
#% 419942
#% 535469
#% 1275309
#% 1290102
#% 1499490
#! We propose a method called Selection by Performance Prediction (SPP) which allows one, when faced with a particular problem instance, to select a Branch and Bound algorithm from among several promising ones. This method is based on Knuth's sampling method which estimates the efficiency of a backtrack program on a particular instance by iteratively generating random paths in the search tree. We present a simple adaptation of this estimator in the field of combinatorial optimization problems, more precisely for an extension of the maximal constraint satisfaction framework. Experiments both on random and strongly structured instances show that, in most cases, the proposed method is able to select, from a candidate list, the best algorithm for solving a given instance.

#index 266132
#* A fast algorithm for the bound consistency of alldiff constraints
#@ Jean-Francois Puget
#t 1998
#c 10
#% 160208
#! Some n-ary constraints such as the alldiff constraints arise naturally in real life constraint satisfaction problems (CSP). General purpose filtering algorithms could be applied to such constraints. By taking the semantics of the constraint into account, it is possible to design more efficient filtering algorithms. When the domains of the variables are totally ordered (e.g. all values are integers). then filtering based on bound consistency may be very useful. We present in this paper a filtering algorithm for the alldiff constraint based on bound consistency whose running time complexity is very low. More precisely, for a constraint involving n variables, the time complexity of the algorithm is O(nlog(n)) which improyes preyionsly published results. The implementation of this algorithm is cliscussed. and we give some experimental results that prove its practical utility.

#index 266133
#* Using Arc weights to improve iterative repair
#@ John Thornton;Abdul Sattar
#t 1998
#c 10
#% 125386
#% 486312
#% 1273577
#% 1273728
#% 1275266
#% 1499515
#% 1499516
#! One of the surprising findings from the study of CNF satisfiability in the 1990's has been the success of iterative repair techniques, and in particular of weighted iterative repair. However, attempts to improve weighted iterative repair have either produced marginal benefits or rely on domain specific heuristics. This paper introduces a new extension of constraint weighting called Arc Weighting Iterative Repair, that is applicable outside the CNF domain and can significantly improve the perfonnance of constraint weighting. The new weighting strategy extends constraint weighting by additionally weighting the connections or arcs between constraints. These arc weights represent increased knowledge of the search space and can be used to guide the search more efficiently. The main aim of the research is to develop an arc weighting algorithm that creates more benefit than overhead in reducing moves in the search space. Initial empirical tests indicate the algorithm does reduce search steps and times for a selection ofCNF and CSP problems.

#index 266136
#* An integer local search method with application to capacitated production planning
#@ Joachim P. Walser;Ramesh Iyer;Narayan Venkatasubramanyan
#t 1998
#c 10
#% 36698
#% 144599
#% 149628
#% 149633
#% 160270
#% 421201
#% 535481
#% 1273577
#% 1478771
#% 1478779
#% 1499519
#! Production planning is an important task in manufacturing systems. We consider a real-world capacitated lot-sizing problem (CLSP) from the process industry. Because the problem requires discrete lot-sizes, domain-specific methods from the literature are not directly applicable. We therefore approach the problem with WSAT (OIP), a new domainindependent heuristic for integer optimization which generalizes the Walks at algorithm. WSAT (OIP) performs stochastic tabu search and operates on over-constrained integer programs. We empirically compare WSAT(OIP) to a state-of the-art mixed integer programming branch-and-bound solver (CPLEX 4.0) on real problem data. We find that integer local search is considerably more robust than MIP branchand-bound in finding feasible solutions in limited time, and branch-and-bound can only solve a sub-class of the CLSP with discrete lot-sizes. With respect to production cost, both methods find solutions of similar quality.

#index 266139
#* Extending GENET to solve fuzzy constraint satisfaction problems
#@ Jason H. Y. Wong;Ho-fung Leung
#t 1998
#c 10
#% 126390
#% 212710
#% 422129
#% 585059
#% 618457
#! Despite much research that has been done on constraint satisfaction problems (CSP's), the framework is sometimes inflexible and the results are not very satisfactory when applied to real-life problems. With the incorporation of the concept of fuzziness, fuzzy constraint satisfaction problems (FCSP's) have been exploited. FCSP's model real-life problems better by allowing individual constraints to be either fully or partially satisfied. GENET, which has been shown to be efficient and effective in solving certain traditional CSP's, is extended to handle FCSP's. Through transforming FCSP's into 0 - 1 integer programming problems, we display the equivalence between the underlying working mechanism of fuzzy GENET and the discrete Lagrangian method. Simulator of fuzzy GENET for single-processor machines is implemented. Benchmarking results confirm its feasibility in tackling CSP's and flexibility in dealing with over-constrained problems.

#index 266141
#* Local search for statistical counting
#@ Olivier Bailleux
#t 1998
#c 10
#% 51676
#% 160270
#% 205391
#% 601159
#% 1080900
#% 1272321
#% 1478773
#% 1499490
#! In this paper, statistical counting is introduced in the context of stochastic local search. From a sample of trajectories by independent local search computations, it is shown that interesting statistical information can be actually extracted about the search space, most notably an unbiased estimate of the number of solutions. Computational results for random #SAT instances are provided.

#index 266190
#* A tractable Walsh analysis of SAT and its implications for genetic algorithms
#@ Soraya Rana;Robert B. Heckendorn;Darrell Whitley
#t 1998
#c 10
#% 114994
#% 232652
#% 466550
#% 547780
#! Walsh Transforms measure all sources of nonlinear interactions for functions that have a bit representation. There can be exponentially many nonlinear interactions and exactly computing all Walsh coefficients is usually intractable for non-trivial functions. In this paper we will show that SAT problems evaluated as MAXSAT functions have a highly restricted set of nonzero Walsh coefficients and those coefficients can be computed in linear time with respect to the number of clauses. This analysis suggests why standard simple genetic algorithms should perform poorly on MAXSAT problems.

#index 266193
#* Hard problems for CSP Algorithms
#@ David G. Mitchell
#t 1998
#c 10
#% 3463
#% 36814
#% 50688
#% 68183
#% 75817
#% 115329
#% 198757
#% 477221
#% 656686
#% 681647
#% 1080890
#% 1268731
#% 1273311
#% 1274033
#% 1478761
#! We prove exponential lower bounds on the running time of many algorithms for Constraint Satisfaction, by giving a simple family of instances on which they always take exponential time. Although similar lower bounds for most of these algorithms have been shown before, we provide a uniform treatment which illustrates a powerful general approach and has stronger implications for the practice of algorithm design.

#index 266195
#* The constrainedness knife-edge
#@ Toby Walsh
#t 1998
#c 10
#% 130208
#% 319789
#% 1268732
#% 1275261
#% 1275263
#% 1279383
#% 1279714
#% 1478761
#% 1478764
#% 1499502
#! A general rule of thumb is to tackle the hardest part of a search problem first. Many heuristics therefore try to branch on the most constrained variable. To test their effectiveness at this, we measure the constrainedness of a problem during search. We run experiments in several different domains, using both random and nonrandom problems. In each case, we observe a constrainedness "knife-edge" in which critically constrained problems tend to remain critically constrained. We show that this knife-edge is predicted by a theoretical lower-bound calculation. We also observe a very simple scaling with problem size for various properties measured during search including the ratio of clauses to variables, and the average clause size. Finally, we use this picture of search to propose some branching heuristics for propositional satisfiability.

#index 266196
#* Heuristic search in cyclic AND/OR graphs
#@ Eric A. Hansen;Shlomo Zilberstein
#t 1998
#c 10
#% 1722
#% 25470
#% 36817
#% 64788
#% 68238
#% 179957
#% 194647
#% 224762
#% 320272
#% 361730
#% 1274275
#% 1291498
#! Heuristic search algorithms can find solutions that take the form of a simple path (A*), a tree or an acyclic graph (AO*). We present a novel generalization of heuristic search (called LAO*) that can find solutions with loops, that is, solutions that take the form of a cyclic graph. We show that it can be used to solve Markov decision problems without evaluating the entire state space, giving it an advantage over dynamic-programming algorithms such as policy iteration and value iteration as an approach to stochastic planning.

#index 266197
#* Single-agent search in the presence of deadlocks
#@ Andreas Junghanns;Jonathan Schaeffer
#t 1998
#c 10
#% 2194
#% 534113
#% 1478838
#% 1499499
#! Single-agent search is a powerful tool for solving a variety of applications. Most of the application domains used to explore single-agent search techniques have the property that if you start with a solvable state, at no time in the search can you reach a state that is unsolvable. In this paper we address the implications that arise when state transitions can lead to unsolvable (deadlock) states. Deadlock states are partially responsible for the failure of our attempts to solve positions in the game of Sokoban. In this paper, we introduce pattern search, a real-time learning algorithm that identifies the minimal conditions (pattern) necessary for a deadlock, and applies that knowledge to eliminate provably irrelevant parts of the search tree. Identification of deadlock patterns is equivalent to correcting the heuristic lower bound of a position to infinity. Generalizing pattern searches to find arbitrary lower bound increases yields a powerful new search enhancement. In the game of Sokoban, pattern searches result in a 15-fold reduction of the cost of each additional IDA* iteration.

#index 266199
#* Complete anytime beam search
#@ Weixiong Zhang
#t 1998
#c 10
#% 241
#% 2194
#% 25998
#% 115680
#% 120809
#% 126386
#% 205385
#% 677386
#% 687883
#% 690812
#% 694210
#! Beam search executes a state-space search, but may abandon nonpromising search avenues in order to reduce complexity. Although it has existed for more than two decades and has been applied to many real-world problems, beam search still suffers from the drawback of possible termination with no solution or a solution of unsatisfactory quality. In this paper, we first propose a domain-independent heuristic for node pruning, and a method to reduce the possibility that beam search will fail. We then develop a complete beam search algorithm. The new algorithm can not only find an optimal solution, but can also reach better solutions sooner than its underlying search method. We apply complete beam search to the maximum boolean satisfiability and the symmetric and asymmetric Traveling Salesman Problems. Our experimental results show that the domain-independent pruning heuristic is effective and the new algorithm significantly improves the performance of its underlying search algorithm.

#index 266200
#* Boosting combinatorial search through randomization
#@ Carla P. Gomes;Bart Selman;Henry Kautz
#t 1998
#c 10
#% 119087
#% 155827
#% 175378
#% 179689
#% 179960
#% 218167
#% 327779
#% 572841
#% 1268731
#% 1273577
#% 1273727
#% 1275311
#% 1476298
#% 1478761
#% 1478764
#% 1478777
#% 1499506
#! Unpredictability in the running time of complete search procedures can often be explained by the phenomenon of "heavy-tailed cost distributions", meaning that at any time during the experiment there is a non-negligible probability of hitting a problem that requires exponentially more time to solve than any that has been encountered before (Gomes et al. 1998a). We present a general method for introducing controlled randomization into complete search algorithms. The "boosted" search methods provably eliminate heavy-tails to the right of the median. Furthermore, they can take advantage of heavy-tails to the left of the median (that is, a nonnegligible chance of very short runs) to dramatically shorten the solution time. We demonstrate speedups of several orders of magnitude for state-of-the-art complete search procedures running on hard, real-world problems.

#index 266201
#* Which search problems are random?
#@ Tad Hogg
#t 1998
#c 10
#% 20731
#% 68183
#% 115608
#% 175367
#% 210191
#% 210195
#% 319789
#% 356652
#% 408440
#% 836006
#% 840577
#% 1268732
#% 1279714
#% 1499502
#% 1499503
#% 1499505
#% 1650718
#! The typical difficulty of various NP-hard problems varies with simple parameters describing their structure. This behavior is largely independent of the search algorithm, but depends on the choice of problem ensemble. A given problem instance belongs to many different ensembles, so applying these observations to individual problems requires identifying which ensemble is most appropriate for predicting its search behavior, e.g., cost or solubility. To address this issue, we introduce a readily computable measure of randomness for search problems called "approximate entropy". This new measure is better suited to search than other approaches, such as algorithmic complexity and information entropy. Experiments with graph coloring and 3-SAT show how this measure can be applied.

#index 266202
#* A* with bounded costs
#@ Brian Logan;Natasha Alechina
#t 1998
#c 10
#% 126386
#% 1476298
#! A key assumption of all problem-solving approaches based on utility theory, including heuristic search, is that we can assign a utility or cost to each state. This in tum requires that all criteria of interest can be reduced to a common ratio scale. However, many real-world problems are difficult or impossible to formulate in terms of minimising a single criterion, and it is often more natural to express problem requirements In this of a set of constraints which a solution should satisfy. In this paper, we present a generalisation of the A* search algorithm, A* with bounded costs (ABC), which searches for a solution which best satisfies a set of prioritised soft constraints, and show that, given certain reasonable assumptions about the constraints, the algorithm is both complete and optimal. We briefly describe a route planner based on ABC and illustrate the advantages of our approach in a simple route planning problem.

#index 266203
#* Stochastic node caching for memory-bounded search
#@ Teruhisa Miura;Toru Ishida
#t 1998
#c 10
#% 2194
#% 46465
#% 64788
#% 111939
#% 137995
#% 1273312
#! Linear-space search algorithms such as IDA* (Iterative Deepening A*) cache only those nodes on the current search path, but may revisit the same node again and again. This causes IDA* to take an impractically long time to find a solution. In this paper, we propose a simple and effective algorithm called Stochastic Node Caching (SNC) for reducing the number of revisits. SNC caches a node with the best estimate, which is currently known of the minimum estimated cost from the node to the goal node. Unlike previous related research such as MREC, SNC caches nodes selectively, based on a fixed probability. We demonstrate that SNC can effectively reduce the number of revisits compared to MREC, especially when the state-space forms a lattice.

#index 266204
#* A feature-based learning method for theorem proving
#@ Matthias Fuchs
#t 1998
#c 10
#% 57896
#% 92533
#% 384112
#% 458309
#% 459795
#% 494579
#% 528879
#% 559895
#% 559900
#% 560238
#% 560754
#% 560923
#% 561087
#% 1273689
#% 1275250
#! Automated reasoning or theorem proving essentially amounts to solving search problems. Despite significant progress in recent years theorem provers still have many shortcomings. The use of machine-learning techniques is acknowledged as promising, but difficult to apply in the area of theorem proving. We propose here to learn search-guiding heuristics by employing features in a simple, yet effective manner. Features are used to adapt a heuristic to a solved source problem. The adapted heuristic can then be utilized profitably for solving related target problems. Experiments have demonstrated that the approach not only allows for significant speed-ups, but also makes it possible to prove problems that were out of reach before.

#index 266205
#* Learning investment functions for controlling the utility of control knowledge
#@ Oleg Ledeniov;Shaul Markovitch
#t 1998
#c 10
#% 1579
#% 42481
#% 85574
#% 140191
#% 370528
#% 449588
#% 1272292
#% 1273423
#! The utility problem occurs when the cost of the acquired knowledge outweighs its benefits. When the learner acquires control knowledge for speeding up a problem solver, the benefit is the speedup gained due to the better control, and the cost is the added time required by the control procedure due to the added knowledge. Previous work in this area was mainly concerned with the costs of matching control rules. The solutions to this kind of utility problem involved some kind of selection mechanism to reduce the number of control rules. In this work we deal with a control mechanism that carries very high cost regardless of the particular knowledge acquired. We propose to use in such cases explicit reasoning about the economy of the control process. The solution includes three steps. First, the control procedure must be converted to anytime procedure. Second, a resource-investment function should be acquired to learn the expected return in speedup time for additional control time. Third, the function is used to determine a stopping condition for the anytime procedure. We have implemented this framework within the context of a program for speeding up logic inference by subgoal ordering. The control procedure utilizes the acquired control knowledge to find efficient subgoal ordering. The cost of ordering, however, may outweigh its benefit. Resource investment functions are used to cut-off ordering when the future net return is estimated to be negative.

#index 266208
#* Fast probabilistic modeling for combinatorial optimization
#@ Shumeet Baluja;Scott Davies
#t 1998
#c 10
#% 24196
#% 44876
#% 91872
#% 197387
#% 266208
#% 465762
#% 465882
#% 465892
#% 466377
#% 704486
#! Probabilistic models have recently been utilized for the optimization of large combinatorial search problems. However, complex probabilistic models that attempt to capture inter-parameter dependencies can have prohibitive computational costs. The algorithm presented in this paper, termed COMIT, provides a method for using probabilistic models in conjunction with fast search techniques. We show how COMIT can be used with two very different fast search algorithms: hillclimbing and Population-based incremental learning (PBIL). The resulting algorithms maintain many of the benefits of probabilistic modeling, with far less computational expense. Extensive empirical results are provided; COMIT has been successfully applied to jobshop scheduling, traveling salesman, and knapsack problems. This paper also presents a review of probabilistic modeling for combinatorial optimization.

#index 266209
#* Highest utility first search across multiple levels of stochastic design
#@ Louis Steinberg;J. Storrs Hall;Brian D. Davison
#t 1998
#c 10
#% 107142
#% 159239
#% 174102
#% 205385
#% 207252
#% 218436
#% 369236
#% 1476303
#! Many design problems are solved using multiple levels of abstraction, where a design at one level has combinatorially many children at the next level. A stochastic optimization methods, such as simulated annealing, genetic algorithms and multi-start hill climbing, is often used in such cases to generate the children of a design. This gives rise to a search tree for the overall problem characterized by a large branching factor, objects at different levels that are hard to compare, and a child-generator that is too expensive to run more than a few times at each level. We present the Highest Utility First Search (HUFS) control algorithm for searching such trees. HUFS is based on an estimate we derive for the expected utility of starting the design process from any given design alternative, where utility reflects both the intrinsic value of the final result and the cost in computing resources it will take to get that result. We also present an empirical study applying HUFS to the problem of VLSI module placement, in which HUFS demonstrates significantly better performance than the common "waterfall" control method.

#index 266211
#* Evolvable hardware chip for high precision printer image compression
#@ Hidenori Sakanashi;Mehrdad Salami;Masaya Iwata;Shogo Nakaya;Tsukasa Yamauchi;Takeshi Inuo;Nobuki Kajihara;Tetsuya Higuchi
#t 1998
#c 10
#% 114994
#% 257644
#% 369236
#% 465228
#% 656124
#% 768827
#% 1776252
#! This paper describes a data compression chip for the high-precision electrophotographic printer using Evolvable Hardware (EHW). EHW is a new hardware paradigm which combines Genetic Algorithm (GA) and reconfigurable hardware technology such as FPGA (Field Programmable Gate Array). In EHW, GA is used to search for the most desirable hardware structure to a given task. If the task requirement changes, GA is invoked to get a better hardware structure and EHW is reconfigured that way. In data compression, EHW is used to implement the most adequate compression method directly in hardware according to the characteristics of the target image. The EHW-based compression chip attains approximately twice the compression compared with the international standard called JBIG. This chip is the first EHW-chip to lead to a commercial product.

#index 266213
#* Opponent modeling in poker
#@ Darse Billings;Denis Papp;Jonathan Schaeffer;Duane Szafron
#t 1998
#c 10
#% 150987
#% 233137
#% 323701
#% 534115
#% 1499483
#! Poker is an interesting test-bed for artificial intelligence research. It is a game of imperfect knowledge, where multiple competing agents must deal with risk management, agent modeling, unreliable information and deception, much like decision-making applications in the real world. Agent modeling is one of the most difficult problems in decision-making applications and in poker it is essential to achieving high performance. This paper describes and evaluates Loki, a poker program capable of observing its opponents, constructing opponent models and dynamically adapting its play to best exploit patterns in the opponents' play.

#index 266214
#* Finding optimal strategies for imperfect information games
#@ Ian Frank;David Basin;Hitoshi Matsubara
#t 1998
#c 10
#% 21944
#% 130237
#% 251781
#% 357678
#% 1499499
#! We examine three heuristic algorithms for games with imperfect information: Monte-carlo sampling, and two new algorithms we call vector minimaxing and payoff-reduction minimaxing. We compare these algorithms theoretically and experimentally, using both simple game trees and a large database of problems from the game of Bridge. Our experiments show that the new algorithms both out-perform Monte-carlo sampling, with the superiority of payoff-reduction minimaxing being especially marked. On the Bridge problem set, for example, Monte-carlo sampling only solves 66% of the problems, whereas payoff-reduction minimaxing solves over 95%. This level of performance was even good enough to allow us to discover five errors in the expert text used to generate the test database.

#index 266215
#* Learning to extract symbolic knowledge from the World Wide Web
#@ Mark Craven;Dan DiPasquo;Dayne Freitag;Andrew McCallum;Tom Mitchell;Kamal Nigam;Seán Slattery
#t 1998
#c 10
#% 219052
#% 242310
#% 255161
#% 255165
#% 266215
#% 458257
#! The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system.

#index 266216
#* Information extraction from HTML: application of a general machine learning approach
#@ Dayne Freitag
#t 1998
#c 10
#% 240955
#% 283136
#% 449508
#% 449566
#% 677332
#% 705442
#% 747945
#! Because the World Wide Web consists primarily of text, information extraction is central to any effort that would use the Web as a resource for knowledge discovery. We show how information extraction can be cast as a standard machine learning problem, and argue for the suitability of relational learning in solving it. The implementation of a general-purpose relational learner for information extraction, SRV, is described. In contrast with earlier learning systems for information extraction, SRV makes no assumptions about document structure and the kinds of information available for use in learning extraction patterns. Instead, structural and other information is supplied as input in the form of an extensible token-oriented feature set. We demonstrate the effectiveness of this approach by adapting SRV for use in learning extraction rules for a domain consisting of university course and research project pages sampled from the Web. Making SRV Web-ready only involves adding several simple HTML-specific features to its basic feature set.

#index 266218
#* Towards text knowledge engineering
#@ Udo Hahn;Klemens Schnattinger
#t 1998
#c 10
#% 41351
#% 42996
#% 58549
#% 72266
#% 83949
#% 92533
#% 173628
#% 176972
#% 177237
#% 179610
#% 191678
#% 395603
#% 496875
#% 539445
#% 1273968
#! We introduce a methodology for automating the maintenance of domain-specific taxonomies based on natural language text understanding. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual "quality" of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base.

#index 266219
#* Answering questions for an organization online
#@ Vladimir A. Kulyukin;Kristian J. Hammond;Robin D. Burke
#t 1998
#c 10
#% 25942
#% 55921
#% 118728
#% 144033
#% 168280
#% 169729
#% 198058
#% 245242
#% 246769
#% 266219
#% 406493
#! The World Wide Web continues to challenge organizations to make online access to their expertise convenient for their clients. One means of expertise access that many clients find convenient in everyday life is asking natural language questions of the organization. To support it online, we developed an approach to building organization-embedded question-answering intermediaries, called Information Exchange systems. These systems use their knowledge of the organization's structure to answer the clients' questions and to acquire new expertise from the organization's experts. Our approach uses techniques of hierarchical and predictive indexing, combined term weighting, abstraction-based retrieval, and negative evidence acquisition. We illustrate our approach with the Chicago Information Exchange system, an Information Exchange application embedded in one university's computer science department.

#index 266221
#* BIG: a resource-bounded information gathering agent
#@ Victor Lesser;Bryan Horling;Frank Klassner;Anita Raja
#t 1998
#c 10
#% 110017
#% 210985
#% 219048
#% 219051
#% 240955
#% 241037
#% 251972
#% 656701
#% 1273446
#% 1279696
#% 1290067
#% 1478775
#! Effective information gathering on the WWW is a complex task requiring planning, scheduling, text processing, and interpretation-style reasoning about extracted data to resolve inconsistencies and to refine hypotheses about the data. This paper describes the rationale, architecture, and implementation of a next generation information gathering system - a system that integrates several areas of AI research under a single research umbrella. The goal of this system is to exploit the vast number of information sources available today on the NII including a growing number of digital libraries, independent news agencies, government agencies, as well as human experts providing a variety of services. The large number of information sources and their different levels of accessibility, reliability and associated costs present a complex information gathering coordination problem. Our solution is an information gathering agent, BIG, that plans to gather information to support a decision process, reasons about the resource tradeoffs of different possible gathering approaches, extracts information from both unstructured and structured documents. and uses the extracted information to refine its search and processing activities.

#index 266223
#* Design principles for intelligent environments
#@ Michael H. Coen
#t 1998
#c 10
#% 94227
#% 107365
#% 127465
#% 127814
#% 219879
#% 234364
#% 239643
#% 483152
#% 592127
#% 1478926
#! This paper describes design criteria for creating highly embedded, interactive spaces that we call Intelligent Environments. The motivation for building these systems is to bring computation into the real, physical world to support what is traditionally considered non-computational activity. We describe an existing prototype space, known as the Intelligent Room, which was created to experiment with different forms of natural, multimodal human-computer interaction. We discuss design decisions encountered while creating the Intelligent Room and how the experiences gained during its use have shaped the creation of its successor.

#index 266224
#* Cooperating with people: the intelligent classroom
#@ David Franklin
#t 1998
#c 10
#% 240967
#% 592284
#% 676041
#% 1275236
#% 1476259
#! People frequently complain that it is too difficult to figure out how to get computers to do what they want. However, with a computer system that actually tries to understand what its users are doing, people can interact in ways that are more natural to them. We have been developing a system, the Intelligent Classroom, that does exactly this. The Intelligent Classroom uses cameras and microphones to sense a speaker's actions and then infers his intentions from those actions. Finally, it uses these intentions to decide what to do to best cooperate with the speaker. In the Intelligent Classroom, the speaker need not worry about how to operate the Classroom; he may simply go about his lecture and trust the Classroom to assist him at the appropriate moments.

#index 266225
#* Integrating AI components for a military planning application
#@ Marie A. Bienkowski;Louis J. Hoebel
#t 1998
#c 10
#% 44836
#% 319244
#% 445002
#% 445006
#! We integrated three mature AI reasoning systems and several legacy military systems in order to provide human planners with advanced capabilities in a military planning domain. The integration demonstrates the operation of a diverse set of AI applications that present a unified system to a human planner in a realistic and meaningful context. We began with an operational planning support system and integrated into it AI components that filled technology gaps. These components, drawn from AI research laboratories, were a generative planner, a temporal reasoner and plan visualizer, and a knowledge-based plan critiquer. The resulting system uses a shared representation to support plan authoring, consistency checking, feasibility analysis, replanning, and visibility into the plan. The support provided is flexible and user controlled. We describe lessons learned from the simultaneous application and integration of mature AI research software, and for individual system components, primarily as they relate to the representation of plans.

#index 266227
#* TRIPs: an integrated intelligent problem-solving assistant
#@ George Ferguson;James F. Allen
#t 1998
#c 10
#% 79997
#% 195094
#% 241019
#% 437790
#% 457148
#% 678234
#% 678242
#% 748706
#% 968363
#! We discuss what constitutes an integrated system in AI, and why AI researchers should be interested in building and studying them. Taking integrated systems to be ones that integrate a variety of components in order to perform some task from start to finish, we believe that such systems (a) allow us to better ground our theoretical work in actual tasks, and (b) provide an opportunity for much-needed evaluation based on task performance. We describe one particular integrated system we have developed that supports spoken-language dialogue to collaboratively solve planning problems. We discuss how the integrated system provides key advantages for helping both our work in natural language dialogue processing and in interactive planning and problem solving, and consider the opportunities such an approach affords for the future.

#index 266228
#* Knowledge intensive exception spaces
#@ Sarabjot S. Anand;David W. Patterson;John G. Hughes
#t 1998
#c 10
#% 5182
#% 136350
#% 140588
#% 229972
#% 368984
#% 445120
#% 501205
#% 501211
#% 697311
#% 700962
#% 1273392
#! In this paper we extend the concept of exception spaces as defined by Cost and Salzberg (Cost and Salzberg, 1993), in the context of exemplar-based reasoning. Cost et al. defined exception spaces based on the goodness, in terms of performance, of an exemplar. While this is straightforward when using exemplars for classification problems, such a definition does not exist for regression problems. Thus, firstly we define a measure of goodness of an exemplar. We then use this measure of goodness to compare the effectiveness of exception spaces with a variant that we introduce, called Knowledge Intensive Exception Spaces or KINS. KINS remove the restriction on the geometric shape of exception spaces as defined by Cost et al. We provide a rationale for KINS and use a data set from the domain of colorectal cancer to support our hypothesis that KINS are a useful extension to exception spaces.

#index 266230
#* Probabilistic frame-based systems
#@ Daphne Koller;Avi Pfeffer
#t 1998
#c 10
#% 44876
#% 147677
#% 228812
#% 405391
#% 1478789
#% 1650731
#% 1650767
#! Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS's) and Bayesian networks (BNs). FRS's provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty that cannot be accomodated within the framework of traditional BNs: uncertainty over the set of entities present in our model, and uncertainty about the relationships between these entities. We provide an inference algorithm for our language via a reduction to inference in standard Bayesian networks. We describe an implemented system that allows most of the main frame systems in existence today to annotate their knowledge bases with probabilistic information, and to use that information in answering probabilistic queries.

#index 266232
#* Logical representation and computation of optimal decisions in a qualitative setting
#@ Didier Dubois;Daniel Le Berre;Henri Prade;Régis Sabbadin
#t 1998
#c 10
#% 3461
#% 179919
#% 229087
#% 277105
#% 288165
#% 327779
#% 442844
#% 780340
#% 1290145
#% 1478741
#% 1650714
#% 1650765
#! This paper describes a logical machinery for computing decisions based on an ATMS procedure, where the available knowledge on the state of the world is described by a possibilistic propositional logic base (i.e., a collection of logical statements associated with qualitative certainty levels). The preferences of the user are also described by another possibilistic logic base whose formula weights are interpreted in terms of priorities and formulas express goals. Two attitudes are allowed for the decision maker: a pessimistic uncertainty-averse one and an optimistic one. The computed decisions are in agreement with a qualitative counterpart to classical expected utility theory for decision under uncertainty.

#index 266233
#* A fuzzy description logic
#@ Umberto Stracia
#t 1998
#c 10
#% 101435
#% 205391
#% 288770
#% 497624
#% 564584
#% 1273635
#% 1273693
#% 1279732
#% 1279735
#% 1478789
#! Description. Logics (DLs, for short) allow reasoning about mdlvlduals and concepts, i.e. set of individuals with common properties. Typically, DLs are limited to dealing with crisp, well defined concepts. That is, concepts for which the problem whether an individual is an instance of it is a yes/no question. More often than not, the concepts encountered in the real world do not have a precisely defined criteria of membership: we may say that an individual is an instance of a concept only to a certain degree, depending on the individual's properties. Concepts of this kind are rather vague than precise. As fuzzy logic directly deals with the notion of vagueness and imprecision, it offers an appealing foundation for a generalisation of DLs to vague concepts.In this paper we present a general fuzzy DL, which combines fuzzy logic with DLs. We define its syntax, semantics and present constraint propagation calculi for reasoning in it.

#index 266237
#* OKBC: a programmatic foundation for knowledge base interoperability
#@ Vinay K. Chaudhri;Adam Farquhar;Richard Fikes;Peter D. Karp;James P. Rice
#t 1998
#c 10
#% 44836
#% 184573
#% 230386
#% 405391
#% 444759
#% 747813
#% 1018492
#% 1275326
#% 1478937
#! The technology for building large knowledge bases (KBs) is yet to witness a breakthrough so that a KB can be constructed by the assembly of prefabricated knowledge components. Knowledge components include both pieces of domain knowledge (for example, theories of economics or fault diagnosis) and KB tools (for example, editors and theorem provers). Most of the current KB development tools can only manipulate knowledge residing in the knowledge representation system (KRS) for which the tools were originally developed. Open Knowledge Base Connectivity (OKBC) is an application programming interface for accessing KRSs, and was developed to enable the construction of reusable KB tools. OKBC improves upon its predecessor, the Generic Frame Protocol (GFP), in several significant ways. OKBC can be used with a much larger range of systems because its knowledge model supports an assertional view of a KRS. OKBC provides an explicit treatment of entities that are not frames, and it has a much better way of controlling inference and specifying default values. OKBC can be used on practically any platform because it supports network transparency and has implementations for multiple programming languages. In this paper, we discuss technical design issues faced in the development of OKBC, highlight how OKBC improves upon GFP, and report on practical experiences in using it.

#index 266238
#* Usability issues in knowledge representation systems
#@ Deborah L. McGuinness;Peter F. Patel-Schneider
#t 1998
#c 10
#% 58347
#% 108696
#% 461605
#% 703948
#% 736403
#% 1268735
#% 1275332
#% 1290162
#% 1478937
#% 1499535
#! The amount of use a knowledge representation system receives depends on more than just the theoretical suitability of the system. Some critical determiners of usage have to do with issues related to the representation formalism of the system, some have to do with non-representational issues of the system itself, and some might be most appropriately labeled public relations. We rely on over eight years of industrial application experiences using a particular family of knowledge representation systems based on description logics to identify and describe usability issues that were mandatory for our application successes.

#index 266239
#* Representing scientific experiments: implications for ontology design and knowledge sharing
#@ Natalya Fridman Noy;Carole D. Hafner
#t 1998
#c 10
#% 1116
#% 82733
#% 179748
#% 706462
#% 1478786
#! As part of the development of knowledge sharing technology, it is necessary to consider a variety of domains and tasks in order to ensure that the shared framework is widely applicable. This paper describes an ontology design project in experimental molecular biology, focusing on extensions to previous ontological models and frame-based formalisms that allow us to handle problems in the representation of experimental science knowledge. We define object histories, which are used to track substances through a series of experimental processes, including those which transform their participants from one category to another. We define object and process complexestemporary configurations with features of their own. We present extensions to a frame-based formalism to support these features. Additional features of our frame formalism include slot groups for identifying sets of relations with common properties, and partial filler restrictions that combine knowledge of the most likely slot values with the ability to handle unexpected values. We demonstrate how these extensions enable the use of (relatively) domain independent inference rules, support intelligent information retrieval, and improve the quality of query interfaces; and we describe the translation of our formalism into Ontolingua.

#index 266241
#* An action language based on causal explanation: preliminary report
#@ Enrico Giunchiglia;Vladimir Lifschitz
#t 1998
#c 10
#% 26351
#% 107123
#% 236024
#% 243712
#% 417575
#% 1290152
#% 1290153
#% 1478800
#% 1499561
#! Action languages serve for describing changes that are caused by performing actions. We define a new action language C, based on the theory of causal explanation proposed recently by McCain and Turner, and illustrate its expressive power by applying it to a number of examples. The mathematical results presented in the paper relate C to the Baral-Gelfond theory of concurrent actions.

#index 266242
#* Abductive planning with sensing
#@ Matthew Stone
#t 1998
#c 10
#% 117869
#% 169209
#% 198870
#% 544790
#% 1275422
#% 1476290
#! In abductive planning, plans are constructed as reasons for an agent to act: plans are demonstrations in logical theory of action that a goal will result assuming that given actions occur successfully. This paper shows how to construct plans abductively for an agent that can sense the world to augment its partial information. We use a formalism that explicitly refers not only to time but also to the information on which the agent deliberates. Goals are reformulated to represent the successive stages of deliberation and action the agent follows in carrying out a course of action, while constraints on assumed actions ensure that an agent at each step performs a specific action selected for its known effects. The result is a simple formalism that can directly inform extensions to implemented planners.

#index 266244
#* A formal methodology for verifying situated agents
#@ Phan Minh Dung
#t 1998
#c 10
#% 68239
#% 136356
#% 181582
#% 181611
#% 181617
#% 194658
#% 553822
#% 1271891
#% 1279751
#! In this paper, we develop a formal methodology for verifying situated agents. The methodology consists of two elements, a specification language for specifying the agent capabilities to execute its actions in dynamic environments and a repertoire of proof methods by which the correctness of an agent, relative to its capabilities, can be formally verified.

#index 266245
#* An algebra for cyclic ordering of 2D orientations
#@ Amar Isli;Anthony G. Cohn
#t 1998
#c 10
#% 89749
#% 115329
#% 121993
#% 180131
#% 181229
#% 289332
#% 319244
#% 539600
#% 539603
#! We define an algebra of ternary relations for cyclic ordering of 2D orientations. The algebra (1) is a refinement of the CYCORD theory; (2) contains 24 atomic relations, hence 224 general relations, of which the usual CYCORD relation is a particular relation; and (3) is NP-complete, which is not surprising since the CYCORD theory is. We then provide: (1) a constraint propagation algorithm for the algebra, which we show is polynomial, and complete for a subclass including all atomic relations; (2) a proof that another subclass, expressing only information on parallel orientations, is NP-complete; and (3) a solution search algorithm for a general problem expressed in the algebra.

#index 266247
#* The temporal analysis of Chisholm's paradox
#@ Leendert W. N. van der Toor
#t 1998
#c 10
#% 191643
#% 242883
#% 912266
#% 1290095
#% 1650336
#! Deontic logic, the logic of obligations and permissions, is plagued by several paradoxes that have to be understood before deontic logic can be used as a knowledge representation language. In this paper we extend the temporal analysis of Chisholm's paradox using a deontic logic that combines temporal and preferential notions.

#index 266248
#* Temporal reasoning with qualitative and quantitative information about points and durations
#@ Rattana Wetprasit;Abdul Sattar
#t 1998
#c 10
#% 21136
#% 107137
#% 126395
#% 137043
#% 216976
#% 319244
#% 320265
#% 484901
#% 1271924
#! A duration is known as a time distance between two point events. This relationship has recently been formalized as the point duration network (PDN) in (Navarrete & Marin 1997). However, only the qualitative information about points and durations was considered. This paper presents an augmented point duration network (APDN) to represent both qualitative and quantitative information about point events. We further extend APDN to capture quantitative information about durations. We propose algorithms to solve reasoning tasks such as determining satisfiability of the network, and finding a consistent scenario with minimal domains. Thus, we present an expressively richer framework than the existing ones to handle both qualitative and quantitative information about points as well as durations.

#index 266249
#* Iterated phantom induction: a little knowledge can go a long way
#@ Mark Brodie;Gerald DeJong
#t 1998
#c 10
#% 1116
#% 6200
#% 92533
#% 103267
#% 160386
#% 369236
#% 1272286
#! We advance a knowledge-based learning method that augments conventional generalization to permit concept acquisition in failure domains. These are domains in which learning must proceed exclusively with failure examples that are relatively uninformative for conventional methods. A domain theory is used to explain and then systematically perturb the observed failures so that they can be treated as if they were positive training examples. The concept induced from these "phantom" examples is exercised in the world, yielding additional observations, and the process repeats. Surprisingly, an accurate concept can often be learned even if the phantom examples are themselves failures and the domain theory is only imprecise and approximate. We investigate the behavior of the method in a stylized air-hockey domain which demands a nonlinear decision concept. Learning is shown empirically to be robust in the face of degraded domain knowledge. An interpretation is advanced which indicates that the information available from a plausible qualitative domain theory is sufficient for robust successful learning.

#index 266251
#* SUSTAIN: a model of human category learning
#@ Bradley C. Love;Douglas L. Medin
#t 1998
#c 10
#% 60576
#% 80232
#% 94926
#% 132583
#% 266251
#% 835998
#! SUSTAIN (Supervised and Unsupervised STratified Adaptive Incremental Network) is a network model of human category learning. SUSTAIN is a three layer model where learning between the first two layers is unsupervised, while learning between the top two layers is supervised. SUSTAIN clusters inputs in an unsupervised fashion until it groups input patterns inappropriately (as signaled by the supervised portion of the network). When such an error occurs, SUSTAIN alters its architecture, recruiting a new unit that is tuned to correctly classify the exception. Units recruited to capture exceptions can evolve into prototypes/attractor/rules in their own right. SUSTAIN's adaptive architecture allows it to master simple classification problems quickly, while still retaining the capacity to learn difficult mappings. SUSTAIN also adjusts its sensitivity to input dimensions during the course of learning, paying more attention to dimensions relevant to the classification task. Shepard, Hovland, and Jenkins's (1961) challenging category learning data is fit successfully by SUSTAIN. Other applications of SUSTAIN are discussed. SUSTAIN is compared to other classification models.

#index 266253
#* Optimal 2D model matching using a messy genetic algorithm
#@ J. Ross Beveridge
#t 1998
#c 10
#% 3478
#% 34959
#% 90846
#% 92717
#% 98397
#% 137661
#% 146667
#% 236293
#% 372042
#% 679922
#% 1271927
#! A Messy Genetic Algorithm is customized to find optimal many-to-many matches for 2D line segment models. The Messy GA is a variant upon the Standard Genetic Algorithm in which chromosome length can vary. Consequently, population dynamics can be made to drive a relatively efficient and robust search for larger and better matches. Run-times for the Messy GA are as much as an order of magnitude smaller than for random starts local search. When compared to a faster Key-Feature Algorithm, the Messy Genetic Algorithm more reliably finds optimal matches. Empirical results are presented for both controlled synthetic and real world line matching problems.

#index 266254
#* Learning cooperative lane selection strategies for highways
#@ David E. Moriarty;Pat Langley
#t 1998
#c 10
#% 92148
#% 203594
#% 258868
#% 369236
#% 449561
#% 703481
#% 1290139
#! This paper presents a novel approach to traffic management by coordinating driver behaviors. Current traffic management systems do not consider lane organization of the cars and only affect traffic flows by controlling traffic signals or ramp meters. However, drivers should be able to increase traffic throughput and more consistently maintain desired speeds by selecting lanes intelligently. We pose the problem of intelligent lane selection as a challenging and potentially rewarding problem for artificial intelligence, and we propose a methodology that uses supervised and reinforcement learning to form distributed control strategies. Initial results are promising and demonstrate that intelligent lane selection can better approximate desired speeds and reduce the total number of lane changes.

#index 266255
#* Boosting in the limit: maximizing the margin of learned ensembles
#@ Adam J. Grove;Dale Schuurmans
#t 1998
#c 10
#% 136350
#% 197394
#% 214401
#% 235377
#% 424997
#% 465746
#% 565528
#% 637522
#% 1478814
#% 1499573
#! The "minimum margin" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new "LPboosting" algorithms that achieve better minimum margins than Adaboost.However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open.Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed.

#index 266257
#* Boosting classifiers regionally
#@ Richard Maclin
#t 1998
#c 10
#% 96699
#% 132938
#% 163316
#% 209021
#% 226499
#% 443616
#% 1271839
#% 1275295
#% 1478814
#% 1499573
#! This paper presents a new algorithm for Boosting the performance of an ensemble of classifiers. In Boosting, a series of classifiers is used to predict the class of data where later members of the series concentrate on training data that is incorrectly predicted by earlier members. To make a prediction about a new pattern, each classifier predicts the class of the pattern and these predictions are then combined. In standard Boosting, the predictions are combined by weighting the predictions by a term related to the accuracy of the classifier on the training data. This approach ignores the fact that later classifiers focus on small subsets of the patterns and thus may only be good at classifying similar patterns. In RegionBoost, this problem is addressed by weighting each classifier's predictions by a factor measuring how well that classifier performs on similar patterns. In this paper we examine several methods for determining how well a classifier performs on similar patterns. Empirical tests indicate RegionBoost produces gains in performance for some data sets and has little effect on others.

#index 266280
#* Robust classification systems for imprecise environments
#@ Foster Provost;Tom Fawcett
#t 1998
#c 10
#% 218961
#% 272995
#% 393792
#% 420065
#% 466086
#% 1378224
#! In real-world environments it is usually difficult to specify target operating conditions precisely. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. In some cases, the performance of the hybrid can actually surpass that of the best known classifier. The hybrid is also efficient to build, to store, and to update. Finally, we provide empirical evidence that a robust hybrid classifier is needed for many real-world problems.

#index 266281
#* Recommendation as classification: using social and content-based information in recommendation
#@ Chumki Basu;Haym Hirsh;William Cohen
#t 1998
#c 10
#% 99690
#% 202009
#% 202011
#% 220709
#% 249135
#% 1499473
#% 1499571
#! Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals, and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact - such as cast lists o r movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.

#index 266282
#* Learning to predict user operations for adaptive scheduling
#@ Melinda T. Gervasio;Wayne Iba;Pat Langley
#t 1998
#c 10
#% 1409
#% 194655
#% 198076
#% 449588
#% 539620
#% 1274399
#% 1290042
#% 1499473
#! Mixed-initiative systems present the challenge of finding an effective level of interaction between humans and computers. Machine learning presents a promising approach to this problem in the form of systems that automatically adapt their behavior to accommodate different users. In this paper, we present an empirical study of learning user models in an adaptive assistant for crisis scheduling. We describe the problem domain and the scheduling assistant, then present an initial formulation of the adaptive assistant's learning task and the results of a baseline study. After this, we report the results of three subsequent experiments that investigate the effects of problem reformulation and representation augmentation. The results suggest that problem reformulation leads to significantly better accuracy without sacrificing the usefulness of the learned behavior. The studies also raise several interesting issues in adaptive assistance for scheduling.

#index 266283
#* Adaptive Web sites: automatically synthesizing Web pages
#@ Mike Perkowitz;Oren Etzioni
#t 1998
#c 10
#% 18713
#% 46809
#% 115478
#% 227995
#% 240957
#% 452516
#% 1273676
#! The creation of a complex web site is a thorny problem in user interface design. In IJCAI '97, we challenged the AI community to address this problem by creating adaptive web sites: sites that automatically improve their organization and presentation by mining visitor access data collected in Web server logs. In this paper we introduce our own approach to this broad challenge. Specifically, we investigate the problem of index page synthesis -- the automatic creation of pages that facilitate a visitor's navigation of a Web site.First, we formalize this problem as a clustering problem and introduce a novel approach to clustering, which we call cluster mining: Instead of attempting to partition the entire data space into disjoint clusters, we search for a small number of cohesive (and possibly overlapping) clusters. Next, we present PageGather, a cluster mining algorithm that takes Web server logs as input and outputs the contents of candidate index pages. Finally, we show experimentally that PageGather is both faster (by a factor of three) and more effective than traditional clustering algorithms on this task. Our experiment relies on access logs collected over a month from an actual web site.

#index 266284
#* Feature generation for sequence categorization
#@ Daniel Kudenko;Haym Hirsh
#t 1998
#c 10
#% 136350
#% 160862
#% 179776
#% 220492
#% 1478788
#% 1478820
#! The problem of sequence categorization is to generalize from a corpus of labeled sequences procedures for accurately labeling future unlabeled sequences. The choice of representation of sequences can have a major impact on this task, and in the absence of background knowledge a good representation is often not known and straightforward representations are often far from optimal. We propose a feature generation method (called FGEN) that creates Boolean features that check for the presence or absence of heuristically selected collections of subsequences. We show empirically that the representation computed by FGEN improves the accuracy of two commonly used learning systems (C4.5 and Ripper) when the new features are added to existing representations of sequence data. We show the superiority of FGEN across a range of tasks selected from three domains: DNA sequences, Unix command sequences, and English text.

#index 266285
#* Concepts from time series
#@ Michael T. Rosenstein;Paul R. Cohen
#t 1998
#c 10
#% 85153
#% 167323
#% 204015
#% 252804
#% 677369
#! This paper describes a way of extracting concepts from streams of sensor readings. In particular, we demonstrate the value of attract or reconstruction techniques for transforming time series into clusters of points. These clusters, in turn, represent perceptual categories with predictive value to the agent/environment system. We also discuss the relationship between categories and concepts, with particular emphasis on class membership and predictive inference.

#index 266286
#* The dynamics of reinforcement learning in cooperative multiagent systems
#@ Caroline Claus;Craig Boutilier
#t 1998
#c 10
#% 124691
#% 164502
#% 266286
#% 307102
#% 782311
#% 1272286
#% 1273580
#% 1650766
#! Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multi agent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-leaming in cooperative multi agent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.

#index 266287
#* Bayesian Q-learning
#@ Richard Dearden;Nir Friedman;Stuart Russell
#t 1998
#c 10
#% 98073
#% 111440
#% 115608
#% 124691
#% 135414
#% 284108
#% 361100
#% 644560
#% 837639
#% 1272286
#! A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information-the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins' Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.

#index 266288
#* Tree based discretization for continuous state space reinforcement learning
#@ William T. B. Uther;Manuela M. Veloso
#t 1998
#c 10
#% 124691
#% 132779
#% 160859
#% 449588
#% 644560
#% 676983
#% 690846
#% 702594
#% 1272286
#% 1280031
#% 1290043
#! Reinforcement learning is an effective technique for learning action policies in discrete stochastic environments, but its efficiency can decay exponentially with the size of the state space. In many situations significant portions of a large state space may be irrelevant to a specific goal and can be aggregated into a few, relevant, states. The U Tree algorithm generates a tree based state discretization that efficiently finds the relevant state chunks of large propositional domains. In this paper, we extend the U Tree algorithm to challenging domains with a continuous state space for which there is no initial discretization. This Continuous U Tree algorithm transfers traditional regression tree techniques to reinforcement learning. We have performed experiments in a variety of domains that show that Continuous U Tree effectively handles large continuous state spaces. In this paper, we report on results in two domains, one gives a clear visualization of the algorithm and another empirically demonstrates an effective state discretization in a simple multi-agent environment.

#index 266290
#* A sampling-based heuristic for tree search applied to grammar induction
#@ Hugues Juillé;Jordan B. Pollack
#t 1998
#c 10
#% 115963
#% 116138
#% 124073
#% 318041
#% 369236
#! In the field of Operation Research and Artificial Intelligence, several stochastic search algorithms have been designed based on the theory of global random search (Zhigljavsky 1991). Basically, those techniques iteratively sample the search space with respect to a probability, distribution which is updated according to the result of previous samples and some predefined strategy. Genetic Algorithms (GAs) (Goldberg 1989) or Greedy Randomized Adaptive Search Procedures (GRASP) (Feo & Resende 1995) are two particular instances of this paradigm. In this paper, we present SAGE, a search algorithm based on the same fundamental mechanisms as those techniques. However, it addresses a class of problems for which it is difficult to design transformation operators to perform local search because of intrinsic constraints in the definition of the problem itself. For those problems, a procedural approach is the natural way to construct solutions, resulting in a state space represented as a tree or a DAG. The aim of this paper is to describe the underlying heuristics used by SAGE to address problems belonging to that class. The performance of SAGE is analyzed on the problem of grammar induction and its successful application to problems from the recent Abbadingo DFA learning competition is presented.

#index 266291
#* Ambiguity and constraint in mathematical expression recognition
#@ Erik G. Miller;Paul A. Viola
#t 1998
#c 10
#% 229279
#% 404772
#% 443698
#% 493479
#% 669161
#! The problem of recognizing mathematical expressions differs significantly from the recognition of standard prose. While in prose significant constraints can be put on the interpretation of a character by the characters immediately preceding and following it, few such simple constraints are present in a mathematical expression. In order to make the problem tractable, effective methods of recognizing mathematical expressions will need to put intelligent constraints on the possible interpretations. The authors present preliminary results on a system for the recognition of both handwritten and typeset mathematical expressions. While previous systems perform character recognition out of context, the current system maintains ambiguity of the characters until context can be used to disambiguate the interpretation. In addition, the system limits the number of potentially valid interpretations by decomposing the expressions into a sequence of compatible convex regions. The system uses A-star to search for the best possible interpretation of an expression. We provide a new lower bound estimate on the cost to goal that improves performance significantly.

#index 266292
#* Learning to classify text from labeled and unlabeled documents
#@ Kamal Nigam;Andrew McCallum;Sebastian Thrun;Tom Mitchell
#t 1998
#c 10
#% 192878
#% 219052
#% 219053
#% 230532
#% 232117
#% 246831
#% 266215
#% 420054
#% 458379
#% 465754
#% 465895
#% 837668
#% 1499473
#! In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%.

#index 266293
#* Knowledge lean word-sense disambiguation
#@ Ted Pedersen;Rebecca Bruce
#t 1998
#c 10
#% 185079
#% 741083
#% 748550
#% 748601
#% 748703
#% 817954
#! We present a corpus-based approach to word-sense disambiguation that only requires information that can be automatically extracted from untagged text. We use unsupervised techniques to estimate the parameters of a model describing the conditional distribution of the sense group given the known contextual features. Both the EM algorithm and Gibbs Sampling are evaluated to determine which is most appropriate for our data. We compare their disambiguation accuracy in an experiment with thirteen different words and three feature sets. Gibbs Sampling results in small but consistent improvement in disambiguation accuracy over the EM algorithm.

#index 266368
#* Learning to resolve natural language ambiguities: a unified approach
#@ Dan Roth
#t 1998
#c 10
#% 697
#% 101898
#% 116179
#% 164838
#% 179693
#% 180945
#% 190581
#% 196896
#% 203129
#% 252047
#% 266368
#% 449559
#% 451055
#% 465743
#% 496884
#% 747797
#% 748550
#% 748594
#% 748738
#% 756253
#% 757840
#% 837668
#% 1476277
#! We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.

#index 266369
#* Generating inference-rich discourse through revisions of RST-Trees
#@ Helmut Horacek
#t 1998
#c 10
#% 76415
#% 158691
#% 217068
#% 423966
#% 491744
#% 555570
#% 560582
#% 741182
#% 744311
#% 748590
#% 1271853
#% 1271854
#! The majority of generation systems to date are able to communicate information only by uttering it explicitly. Rhetorical Structure Theory (RST), one of the most frequently used discourse theories for text planning in natural language generation, does not support more flexibility either, because it ignores implicit rhetorical relations and accepts only one prominent relation between clauses. In formal systems, however, the underlying information is represented in a very detailed way, which requires easily inferable parts to be left implicit for producing natural and comprehensible discourse. In order to improve the quality of texts generated from fine-grained semantic specifications, we present an approach that successively revises an explicit text plan by introducing addressee dependent short-cuts and communicatively justified reorganizations. Text plan revisions include the compactification of stateaction and reasoning sequences, the omission of redundant conditions, and the reorganization of arguments for presentation purposes. Our techniques enable us to generate shorter and better understandable texts from detailed representations, as in formal systems, especially in deduction systems.

#index 266370
#* Machine learning of generic and user-focused summarization
#@ Inderjeet Mani;Eric Bloedorn
#t 1998
#c 10
#% 136350
#% 144013
#% 169770
#% 185287
#% 194251
#% 198058
#% 198294
#% 198297
#% 288614
#% 382141
#% 740329
#% 740900
#% 741058
#% 742437
#% 748499
#% 815336
#% 815342
#% 1273169
#% 1306081
#% 1478826
#! A key problem in text summarization is finding a salience function which determines what information in the source should be included in the summary. This paper describes the use of machine learning on a training corpus of documents and their abstracts to discover salience functions which describe what combination of features is optimal for a given summarization task. The method addresses both "generic" and user-focused summaries.

#index 266371
#* Hermes: supporting argumentative discourse in multi-agent decision making
#@ Nikos Karacapilidis;Dimitris Papadias
#t 1998
#c 10
#% 1145
#% 39630
#% 64888
#% 116294
#% 200419
#% 235441
#% 284895
#% 361152
#! This paper describes HERMES, a system that enhances group decision making by providing an argumentation framework to the agents involved. The system organizes the existing knowledge in a discussion graph, which consists of issues, alternatives, positions and preference relations. Argumentation is performed through a set of discourse acts which trigger appropriate procedures for the propagation of information in the graph. HERMES is able to handle incomplete, qualitative and inconsistent information, and provides mechanisms for weighing arguments.

#index 266372
#* Bayesian reasoning in an abductive mechanism for argument generation and analysis
#@ Ingrid Zukerman;Richard McConachy;Kevin B. Korb
#t 1998
#c 10
#% 44876
#% 68244
#% 145393
#% 147680
#% 373996
#% 423966
#% 748543
#% 1271854
#% 1271863
#! Our argumentation system, NAG, uses Bayesian networks in a user model and in a normative model to assemble and assess arguments which balance persuasiveness with normative correctness. Attentional focus is simulated in both models to select relevant subnetworks for Bayesian propagation. The subnetworks are expanded in an iterative abductive process until argumentative goals are achieved in both models, when the argument is presented to the user.

#index 266374
#* Fixpoint 3-valued semantics for autoepistemic logic
#@ Marc Denecker;Victor Marek;Mirosław Truszczyński
#t 1998
#c 10
#% 1146
#% 30096
#% 36545
#% 68240
#% 103705
#% 150827
#% 189738
#% 383293
#! The paper presents a constructive 3-valued semantics for autoepistemic logic (AEL). We introduce a derivation operator and define the semantics as its least fixpoint. The semantics is 3-valued in the sense that, for some formulas, the least fixpoint does not specify whether they are believed or not. We show that complete fixpoints of the derivation operator correspond to Moore's stable expansions. In the case of modal representations of logic programs our least fixpoint semantics expresses well-founded semantics or 3-valued Fitting-Kunen semantics (depending on the embedding used). We show that, computationally, our semantics is simpler than the semantics proposed by Moore (assuming that the polynomial hierarchy does not collapse).

#index 266376
#* Experimenting with power default reasoning
#@ Eric Klavins;William C. Rounds;Guo-Qiang Zhang
#t 1998
#c 10
#% 47708
#% 100146
#% 148021
#% 198885
#% 288165
#% 491213
#% 499512
#% 587424
#! In this paper we explore the computational aspects of Propositional Power Default Reasoning (PDR), a form of non-monotonic reasoning in which the underlying logic is Kleene's 3-valued propositional logic. PDR leads to a concise meaning of the problem of skeptical entailment which has better complexity characteristics than the usual formalisms (co-NP(3)-Complete instead of 驴2P-Complete). We take advantage of this in an implementation called powdef to encode and solve hard graph problems and explore randomly generated instances of skeptical entailment.

#index 266377
#* Reducing query answering to satisfiability in nonmonotonic logics
#@ Riccardo Rosati
#t 1998
#c 10
#% 400
#% 1146
#% 68240
#% 101922
#% 130784
#% 131560
#% 163716
#% 175359
#% 289333
#% 383293
#% 558568
#% 559898
#% 1274577
#% 1279721
#% 1478795
#! We propose a unifying view of negation as failure, integrity constraints, and epistemic queries in nonmonotonic reasoning. Specifically, we study the relationship between satisfiability and logical implication in nonmonotonic logics, showing that, in many nonmonotonic formalisms, it is possible and easy to reduce logical implication to satisfiability. This result not only allows for establishing new complexity results for the satisfiability problem in nonmonotonic logics, but also establishes a clear relationship between the studies on epistemic queries and integrity constraints in monotonic knowledge bases with the work on negation by default in nonmonotonic reasoning and logic programming. From the perspective of the design of knowledge representation systems, such a reduction allows for defining both a simple method for answering epistemic queries in knowledge bases with nonmonotonic abilities, and a procedure for identifying integrity constraints in the knowledge base, which can be employed for optimizing reasoning in such systems.

#index 266379
#* Improving big plans
#@ Neal Lesh;Nathaniel Martin;James Allen
#t 1998
#c 10
#% 55921
#% 68243
#% 89748
#% 89781
#% 116293
#% 120806
#% 179766
#% 194652
#% 194656
#% 463903
#% 1272367
#! Past research on assessing and improving plans in domains that contain uncertainty has focused on analytic techniques that are exponential in the length of the plan. Little work has been done on choosing from among the many ways in which a plan can be improved. We present the IMPROVE algorithm which simulates the execution of large, probabilistic plans. IMPROVE runs a data mining algorithm on the execution traces to pinpoint defects in the plan that most often lead to plan failure. Finally, IMPROVE applies qualitative reasoning and plan adaptation algorithms to modify the plan to correct these defects. We have tested IMPROVE on plans containing over 250 steps in an evacuation domain, produced by a domain-specific scheduling routine. In these experiments, the modified plans have over a 15% higher probability of achieving their goal than the original plan.

#index 266383
#* Controlling communication in distributed planning using irrelevance reasoning
#@ Michael Wolverton;Marie DesJardins
#t 1998
#c 10
#% 44836
#% 175108
#% 241026
#% 1273556
#% 1274402
#% 1290106
#% 1478733
#! Efficient and effective distributed planning requires careful control over how much information the planning agents broadcast to one another. Sending too little information could result in incorrect plans, while sending too much information could overtax the distributed planning system's resources (bandwidth and computational power). Ideally, distributed planning systems would have an efficient technique for filtering a large amount of irrelevant information from the message stream while retaining all the relevant messages. This paper describes an approach to controlling information distribution among planning agents using irrelevance reasoning (Levy & Sagiv 1993). In this approach, each planning agent maintains a data structure encoding the planning effects that could potentially be relevant to each of the other agents, and uses this structure to decide which of the planning effects that it generates will be sent to other agents. We describe an implementation of this approach within a distributed version of the SIPE-2 planner. Our experiments with this implementation show two important benefits of the approach: first, a noticeable speedup of the distributed planners; second--and, we argue, more importantly--a substantial reduction in message traffic.

#index 266384
#* Automatic OBDD-based generation of universal plans in non-deterministic domains
#@ Alssandro Cimatti;Marco Roveri;Paolo Traverso
#t 1998
#c 10
#% 3873
#% 145228
#% 179940
#% 194647
#% 230399
#% 243697
#% 243707
#% 544788
#% 1271884
#% 1272468
#% 1275454
#% 1290112
#! Most real world environments are non-deterministic. Automatic plan formation in non-deterministic domains is, however, still an open problem. In this paper we present a practical algorithm for the automatic generation of solutions to planning problems in nondeterministic domains. Our approach has the followmg main features. First, the planner generates Universal Plans. Second, it generates plans which are guaranteed to achieve the goal in spite of non-determinism, if such plans exist. Otherwise, the planner generates plans which encode iterative trial-and-error strategies (e.g. try to pick up a block until succeed), which are guaranteed to achieve the goal under the assumption that if there is a non-deterministic possibility for the iteration to terminate, this will not be ignored forever. Third, the implementation of the planner is based on symbolic model checking techniques which have been designed to explore efficiently large state spaces. The implementation exploits the compactness of OBDDS (Ordered Binary Decision Diagrams) to express in a practical way universal plans of extremely large size.

#index 266385
#* Hybrid planning for partially hierarchical domains
#@ Subbarao Kambhampati;Amol Mali;Biplav Srivastava
#t 1998
#c 10
#% 21145
#% 79993
#% 109935
#% 179965
#% 194651
#% 214718
#% 224758
#% 459615
#% 544793
#% 1272550
#% 1273680
#% 1274156
#% 1290103
#% 1290109
#% 1383858
#% 1476298
#! Hierarchical task network and action-based planning approaches have traditionally been studied separately. In many domains, human expertise in the form of hierarchical reduction schemas exists, but is incomplete. In such domains, hybrid approaches that use both HTN and action-based planning techniques are needed. In this paper, we extend our previous work on refinement planning to include hierarchical planning. Specifically, we provide a generalized plan-space refinement that is capable of handling non-primitive actions. The generalization provides a principled way of handling partially hierarchical domains, while preserving systematicity, and respecting the user-intent inherent in the reduction schemas. Our general account also puts into perspective the many surface differences between the HTN and action-based planners, and could support the transfer of progress between HTN and action-based planning approaches.

#index 266386
#* Conformant Graphplan
#@ David E. Smith;Daniel S. Weld
#t 1998
#c 10
#% 124601
#% 179945
#% 194652
#% 224480
#% 266387
#% 544766
#% 544791
#% 707175
#% 1272287
#% 1290109
#! Planning under uncertainty is a difficult task. If sensory information is available, it is possible to do contingency planning - that is, develop plans where certain branches are executed conditionally, based on the outcome of sensory actions. However, even without sensory information, it is often possible to develop useful plans that succeed no matter which of the allowed states the world is actually in. We refer to this type of planning as conformant planning.Few conformant planners have been built, partly because conformant planning requires the ability to reason about disjunction. In this paper we describe Conformant Graphplan (CGP), a Graphplan-based planner that develops sound (non-contingent) plans when faced with uncertainty in the initial conditions and in the outcome of actions. The basic idea is to develop separate plan graphs for each possible world. This requires some subtle changes to both the graph expansion and solution extraction phases of Oraphplan. In particular, the solution extraction phase must consider the unexpected side effects of actions in other possible worlds, and must confront any undesirable effects.We show that COP performs significantly better than two previous (probabilistic) conformant planners.

#index 266387
#* Extending Graphplan to handle uncertainty and sensing actions
#@ Daniel S. Weld;Corin R. Anderson;David E. Smith
#t 1998
#c 10
#% 124601
#% 188086
#% 224480
#% 266386
#% 544766
#% 544791
#% 1272287
#% 1275454
#! If an agent does not have complete information about the world-state, it must reason about alternative possible states of the world and consider whether any of its actions can reduce the uncertainty. Agents controlled by a contingent planner seek to generate a robust plan, that accounts for and handles all eventualities, in advance of execution. Thus a contingent plan may include sensing actions which gather information that is later used to select between different plan branches. Unfortunately, previous contingent planners suffered defects such as confused semantics, incompleteness, and inefficiency. In this paper we describe SGP, a descendant of Graphplan that solves contingent planning problems. SGP distinguishes between actions that sense the value of an unknown proposition from those that change its value. SGP does not suffer from the forms of incompleteness displayed by CNLP and Cassandra. Furthermore, SGP is relatively fast.

#index 266388
#* Inferring state constraints for domain-independent planning
#@ Alfonso Gerevini;Lenhart Schubert
#t 1998
#c 10
#% 26722
#% 131357
#% 160270
#% 544766
#% 1271884
#% 1272297
#% 1290109
#% 1476298
#% 1478761
#! We describe some new preprocessing techniques that enable faster domain-independent planning. The first set of techniques is aimed at inferring state constraints from the structure of planning operators and the initial state. Our methods consist of generating hypothetical state constraints by inspection of operator effects and preconditions, and checking each hypothesis against all operators and the initial conditions. Another technique extracts (supersets of) predicate domains from sets of ground literals obtained by Graphplan-like forward propagation from the initial state. Our various techniques are implemented in a package called DISCOPLAN. We show preliminary results on the effectiveness of adding computed state constraints and predicate domains to the specification of problems for SAT-based planners such as SATPLAN or MEDIC. The results suggest that large speedups in planning can be obtained by such automated methods, potentially obviating the need for adding hand-coded state constraints.

#index 266390
#* Analyzing external conditions to improve the efficiency of HTN planning
#@ Reiko Tsuneto;James Hendler;Dana Nau
#t 1998
#c 10
#% 103050
#% 109935
#% 131357
#% 179879
#% 179935
#% 192708
#% 224480
#% 321639
#% 544793
#% 1272311
#% 1272550
#% 1279707
#% 1291452
#% 1499481
#% 1499545
#! One difficulty with existing theoretical work on HTN planning is that it does not address some of the planning constructs that are commonly used in HTN planners for practical applications. Although such constructs can make it difficult to ensure the soundness and completeness of HTN planning, they are important because they can greatly improve the efficiency of planning in practice. In this paper, we describe a way to achieve some of the advantages of such constructs while preserving soundness and completeness, through the use of what we will call external conditions. We describe how to detect some kinds of external conditions automatically by preprocessing the planner's knowledge base, and how to use this knowledge to improve the efficiency of the planner's refinement strategy. We present experimental results showing that by making use of external conditions as described here, an HTN planner can be significantly more efficient and scale better to large problems.

#index 266391
#* Managing multiple tasks in complex, dynamic environments
#@ Michael Freed
#t 1998
#c 10
#% 49254
#% 241013
#% 695783
#% 706387
#% 1273721
#% 1291505
#% 1469459
#! Sketchy planners are designed to achieve goals in realistically complex, time-pressured, and uncertain task environments. However, the ability to manage multiple, potentially interacting tasks in such environments requires extensions to the functionality these systems typically provide. This paper identifies a number of factors affecting how interacting tasks should be prioritized, interrupted, and resumed, and then describes a sketchy planner called APEX that takes account of these factors when managing multiple tasks.

#index 266393
#* Maintaining consistency in hierarchical reasoning
#@ Robert E. Wray, III;John Laird
#t 1998
#c 10
#% 23011
#% 154456
#! We explore techniques for maintaining consistency in reasoning when employing dynamic hierarchical task decompositions. In particular, we consider the difficulty of maintaining consistency when an agent nonmonotonically modifies an assumption in one level of the task hierarchy and that assumption depends upon potentially dynamic assertions higher in the hierarchy. The hypothesis of our work is that reasoning maintenance can be extended to hierarchical systems such that consistency is maintained across all levels of the hierarchy. We introduce two novel extensions to standard reason maintenance approaches, assumption justification and dynamic hierarchical justification, both of which provide the necessary capabilities. The key difference between the two methods is whether a particular assumption (assumption justification) or an entire level of the hierarchy (dynamic hierarchical justification) is disabled when an inconsistency is found. Our investigations suggest that dynamic hierarchical justification has advantages over assumption justification, especially when the task decomposition is wellconstructed. Agents using dynamic hierarchical justification also compare favorably to agents using less complete methods for reasoning consistency, improving the reactivity of hierarchical architectures while eliminating the need for knowledge that otherwise would be required to maintain reasoning consistency.

#index 266395
#* Acquisition of abstract plan descriptions for plan recognition
#@ Mathias Bauer
#t 1998
#c 10
#% 51913
#% 103865
#% 184048
#% 423981
#% 1274731
#! While most plan recognition systems make use of a plan library that contains the set of available plan hypotheses, little effort has been devoted to the question of how to create such a library. This problem is particularly difficult to deal with when only little domain knowledge is available-a common situation when e.g. developing a help system for an already existing software system. This paper describes how operational decompositions of plans can be extracted from a set of sample action sequences, thus providing the basis for automating the acquisition of plan libraries. Efficient algorithms for the approximation of optimal decompositions and experimental results supporting their feasibility are presented.

#index 266397
#* Needles in a haystack: plan recognition in large spatial domains involving multiple agents
#@ Mark Devaney;Ashwin Ram
#t 1998
#c 10
#% 103864
#% 132099
#% 243705
#% 362928
#% 408493
#% 418882
#! While plan recognition research has been applied to a wide variety of problems, it has largely made identical assumptions about the number of agents participating in the plan, the observability of the plan execution process, and the scale of the domain. We describe a method for plan recognition in a real-world domain involving large numbers of agents performing spatial maneuvers in concert under conditions of limited observability. These assumptions differ radically from those traditionally made in plan recognition and produce a problem which combines aspects of the fields of plan recognition, pattern recognition, and object tracking. We describe our initial solution which borrows and builds upon research from each of these areas, employing a pattern-directed approach to recognize individual movements and generalizing these to produce inferences of large-scale behavior.

#index 266398
#* Act, and the rest will follow: exploiting determinism in planning as satisfiability
#@ Enrico Giunchiglia;Alessandro Massarotto;Roberto Sebastiani
#t 1998
#c 10
#% 131357
#% 210195
#% 405344
#% 1271884
#% 1274084
#% 1290109
#% 1476298
#! In this paper we focus on Planning as Satisfiability (SAT). We build from the simple consideration that the values of fiuents at a certain time point derive deterministically from the initial situation and the sequence of actions performed till that point. Thus, the choice of actions to perform is the only source of nondeterminism. This is a rather trivial consideration, but which has important positive consequences if implemented in current planners via SAT. In fact, it produces a dramatic size reduction of the space of the truth assignments searched in by the SAT decider used to solve the final SAT problem. To justify this claim, we repeat many of the experiments reported in (Ernst, Millstein, & Weld 1997), and show that the CPU time requested to solve a problem can go down up to 4 orders of magnitude.

#index 266400
#* Using caching to solve larger probabilistic planning problems
#@ Stephen M. Majercik;Michael L. Littman
#t 1998
#c 10
#% 44876
#% 194652
#% 1272331
#% 1476298
#% 1478845
#! Probabilistic planning algorithms seek effective plans for large, stochastic domains. MAXPLAN is a recently developed algorithm that converts a planning problem into an E-MAJSAT problem, an NpPP-complete problem that is essentially a probabilistic version of SAT, and draws on techniques from Boolean satisfiability and dynamic programming to solve the E-MAJSAT problem. This solution method is able to solve planning problems at state-of-the-art speeds, but it depends on the ability to store a value for each CNF subformula encountered in the solution process and is therefore quite memory intensive; searching for moderate-size plans even on simple problems can exhaust memory. This paper presents two techniques, based on caching, that overcome this problem without significant performance degradation. The first technique uses an LRU cache to store a fixed number of subformula values. The second technique uses a heuristic based on a measure of subformula difficulty to selectively save the values of only those subformulas whose values are sufficiently difficult to compute and are likely to be reused later in the solution process. We report results for both techniques on a stochastic test problem.

#index 266402
#* Alternative essences of intelligence
#@ Rodney A. Brooks;Cynthia Breazeal (Ferrell);Robert Irie;Charles C. Kemp;Matthew Marjanović;Brian Scassellati;Matthew M. Williamson
#t 1998
#c 10
#% 18317
#% 44876
#% 97619
#% 184277
#% 265792
#% 426494
#% 668809
#% 1023092
#% 1023102
#% 1279751
#% 1478798
#% 1478799
#% 1478800
#% 1478833
#% 1478841
#% 1478842
#% 1478843
#% 1478845
#! We present a novel methodology for building humanlike artificially intelligent systems. We take as a model the only existing systems which are universally accepted as intelligent: humans. We emphasize building intelligent systems which are not masters of a single domain, but, like humans, are adept at performing a variety of complex tasks in the real world. Using evidence from cognitive science and neuroscience, we suggest four alternative essences of intelligence to those held by classical AI. These are the parallel themes of development, social interaction, embodiment, and integration. Following a methodology based on these themes, we have built a physical humanoid robot. In this paper we present our methodology and the insights it affords for facilitating learning, simplifying the computation underlying rich behavior, and building systems that can scale to more complex tasks in more challenging environments.

#index 266404
#* Eye finding via face detection for a foveated, active vision system
#@ Brian Scassellati
#t 1998
#c 10
#% 56269
#% 184277
#% 215893
#% 266402
#% 593492
#% 647033
#% 668809
#% 668902
#% 678257
#% 1022958
#% 1271933
#% 1279751
#! Eye finding is the first step toward building a machine that can recognize social cues, like eye contact and gaze direction, in a natural context. In this paper, we present a real-time implementation of an eye finding algorithm for a foveated active vision system. The system uses a motion-based prefilter to identify potential face locations. These locations are analyzed for faces with a template-based algorithm developed by Sinha (1996). Detected faces are tracked in real time, and the active vision system saccades to the face using a learned sensorimotor mapping. Once gaze has been centered on the face, a high-resolution image of the eye can be captured from the foveal camera using a self-calibrated peripheral-ta-foveal mapping.We also present a performance analysis of Sinha's ratio template algorithm on a standard set of static face images. Although this algorithm performs relatively poorly on static images, this result is a poor indicator of real-time performance of the behaving system. We find that our system finds eyes in 94% of a set of behavioral trials. We suggest that alternate means of evaluating behavioral systems are necessary.

#index 266406
#* Template-based recognition of pose and motion gestures on a mobile robot
#@ Stefan Waldherr;Sebastian Thrun;Roseli Romero;Dimitris Margaritis
#t 1998
#c 10
#% 39654
#% 120270
#% 235356
#% 263023
#% 263035
#% 265782
#% 364575
#% 380686
#% 592225
#% 592284
#% 618445
#% 1271842
#% 1275236
#% 1476257
#! For mobile robots to assist people in everyday life, they must be easy to instruct. This paper describes a gesture-based interface for human robot interaction, which enables people to instruct robots through easy-to-perform arm gestures. Such gestures might be static pose gestures, which involve only a specific configuration of the person's arm, or they might be dynamic motion gestures (such as waving). Gestures are recognized in real-time at approximate frame rate, using a hybrid approach that integrates neural networks and template matching. A fast, color-based tracking algorithm enables the robot to track and follow a person reliably through office environments with drastically changing lighting conditions. Results are reported in the context of an interactive clean-up task, where a person guides the robot to specific locations that need to be cleaned, and the robot picks up trash which it then delivers to the nearest trash-bin.

#index 266407
#* Position estimation for mobile robots in dynamic environments
#@ Dieter Fox;Wolfram Burgard;Sebastian Thrun;Armin B. Cremers
#t 1998
#c 10
#% 199610
#% 263023
#% 265782
#% 267315
#% 380686
#% 418645
#% 1290038
#% 1476254
#% 1476258
#! For mobile robots to be successful, they have to navigate safely in populated and dynamic environments. While recent research has led to a variety of localization methods that can track robots well in static environments, we still lack methods that can robustly localize mobile robots in dynamic environments, in which people block the robot's sensors for extensive periods of time or the position of furniture may change. This paper proposes extensions to Markov localization algorithms enabling them to localize mobile robots even in densely populated environments. Two different filters for determining the "believability" of sensor readings are employed. These filters are designed to detect sensor readings that are corrupted by humans or unexpected changes in the environment. The technique was recently implemented and applied as part of an installation, in which a mobile robot gave interactive tours to visitors of the "Deutsches Museum Bonn." Extensive empirical tests involving datasets recorded during peak traffic hours in the museum demonstrate that this approach is able to accurately estimate the robot's position in more than 98% of the cases even in such highly dynamic environments.

#index 266410
#* Integrating topological and metroc maps for mobile robot navigation: a statistical approach
#@ Sebastian Thrun;Jens-Steffen Gutmann;Dieter Fox;Wolfram Burgard;Benjamin J. Kuipers
#t 1998
#c 10
#% 39654
#% 161243
#% 179928
#% 214284
#% 247930
#% 263035
#% 267315
#% 418645
#% 541689
#% 696198
#% 1271848
#% 1290038
#% 1476254
#! The problem of concurrent mapping and localization has received considerable attention in the mobile robotics community. Existing approaches can largely be grouped into two distinct paradigms: topological and metric. This paper proposes a method that integrates both. It poses the mapping problem as a statistical maximum likelihood problem, and devises an efficient algorithm for search in likelihood space. It presents an novel mapping algorithm that integrates two phases: a topological and a metric mapping phase. The topological mapping phase solves a global position alignment problem between potentially indistinguishable, significant places. The subsequent metric mapping phase produces a fine-grained metric map of the environment in floating-point resolution. The approach is demonstrated empirically to scale up to large, cyclic, and highly ambiguous environments.

#index 266412
#* The role of data reprocessing in complex acoustic environments
#@ Frank Klassner;Victor Lesser;Hamid Nawab
#t 1998
#c 10
#% 36638
#% 117660
#% 193506
#% 703623
#% 704183
#% 1275248
#% 1275408
#% 1478832
#! The Integrated Processing and Understanding of Signals (IPUS) architecture is a general blackboard framework for structuring bidirectional interaction between front-end signal processing algorithms (SPAs) and signal understanding processes. To date, reported work on the architecture has focused on proof-of-concept demonstrations of how well a sound-understanding testbed (SUT) based on IPUS could use small libraries of sound models and small sets of SPAs to analyze acoustic scenarios. In this paper we evaluate how well the architecture scales up to more complex environments. We describe knowledge-representation and control-strategy issues involved in scaling up an IPUS-based SUT for use with a library of 40 sound models, and present empirical evaluation that shows (a) the IPUS data reprocessing paradigm can increase interpretation accuracy by 25% - 50% in complex scenarios, and (b) the benefit increases with increasing complexity of the environment.

#index 266414
#* Sound ontology for computational auditory scence analysis
#@ Tomohiro Nakatani;Hiroshi G. Okuno
#t 1998
#c 10
#% 173497
#% 257158
#% 968480
#% 1273678
#% 1275247
#% 1275248
#% 1476282
#! This paper proposes that sound ontology should be used both as a common vocabulary for sound representation and as a common terminology for integrating various sound stream segregation systems. Since research on computational auditory scene analysis (CASA) focuses on recognizing and understanding various kinds of sounds, sound stream segregation which extracts each sound stream from a mixture of sounds is essential for CASA. Even if sound stream segregation systems use a harmonic structure of sound as a cue of segregation, it is not easy to integrate such systems because the definition of a harmonic structure differs or the precision of extracted harmonic structures differs. Therefore, sound ontology is needed as a common knowledge representation of sounds.Another problem is to interface sound stream segregation systems with applications such as automatic speech recognition systems. Since the requirement of the quality of segregated sound streams depends on applications, sound stream segregation systems must provide a flexible interface. Therefore, sound ontology is needed to fulfill the requirements imposed by them. In addition, the hierarchical structure of sound ontology provides a means of controlling top-down and bottom-up processing of sound stream segregation.

#index 266416
#* Multi machine scheduling: an agent-based approach
#@ Rama Akkiraju;Pinar Keskinocak;Sesh Murthy;Frederick Wu
#t 1998
#c 10
#% 76273
#% 81089
#% 92291
#% 146919
#% 152535
#% 1147758
#! Scheduling of multiple parallel machines in the face of sequence dependent setups and downstream considerations is a hard problem. No single efficient algorithm is guaranteed to produce optimal results. We describe a solution for an instance of this problem, in the domain of paper manufacturing. The problem has additional job machine restrictions and fixed costs of assigning jobs to machines. We consider multiple objectives such as minimizing (weighted) tardiness, minimizing job-machine assignment costs. We solve the problem using a simple agent architecture called the Asynchronous team (A-team), in which agents cooperate by exchanging results. We have built agents each of which encapsulates a different problem solving strategy for solving the multi-machine scheduling problem. The A-team framework enables the agents to cooperate to produce better results than those of any individual agent. In this paper we define the problem, describe the individual agents, and show with experimental results that the A-team produces very good results compared to schedulers alone.

#index 266417
#* Producing BT's yellow pages with formation
#@ Gail Anderson;Andrew Casson-du Mont;Ann Macintosh;Robert Rae;Barry Gleeson
#t 1998
#c 10
#! This case study illustrates how the adoption of AI technology can benefit smaller companies as well as major corporations.Pindar Set is a small UK company which has originated the Yellow Pages directories for British Telecommunications plc since 1979. AlAI is a technology transfer organisation which has delivered innovative solutions to industrial clients since 1984. Together, AlAI and Pindar have developed a next-generation layout system, Formation.Formation is fast, easy to use and flexible, and had already delivered benefits through marketing trials before being successfully deployed in production of the Yellow Pages in December 1997.The heart of Formation is a 2D layout engine which formats input data according to styles written in LSSL, a domain-specific language developed at AIAI.Through representing the layout knowledge in Formation explicitly in LSSL styles, and ensuring that it can easily be modified, Pindar has enabled itself to respond far better to its customer's present and future needs.

#index 266419
#* Using artificial intelligence planning to automate SAR image processing for scientific data analysis
#@ Forest Fisher;Steve Chien;Edisanter Lo;Ronald Greeley
#t 1998
#c 10
#% 58384
#% 159715
#% 212692
#% 216499
#% 266419
#! In recent times, improvements in imaging technology have made available an incredible array of information in image format. While powerful and sophisticated image processing software tools are available to prepare and analyze the data, these tools are complex and cumbersome, requiring significant expertise to properly operate. Thus, in order to extract (e.g., mine or analyze) useful information from the data, a user (in our case a scientist) often must possess both significant science and image processing expertise.This paper describes the use of AI planning techniques to represent scientific, image processing, and software tool knowledge to automate elements of science data preparation and analysis of synthetic aperture radar (SAR) imagery for planetary geology. In particular, we describe the Automated SAR Image Processing system (ASIP) which is currently in use by the Dept. of Geology at ASU supporting aeolian science analysis of synthetic aperture radar images. ASIP reduces the number of manual inputs in science product generation by 1O-fold, decreases the CPU time to produce images by 30%, and allows scientists to directly produce certain science products.

#index 266420
#* Turbine engine diagnostics (TED): an expert diagnostic system for the M1 Abrams turbine engine
#@ Richard Helfman;Ed Baur;John Dumer;Tim Hanratty;Holly Ingham
#t 1998
#c 10
#% 9604
#% 661034
#% 1272935
#! Turbine Engine Diagnostics (TED) is a diagnostic expert system to aid the M1 Abrams tank mechanic find and fix problems in the AGT-1500 turbine engine. TED was designed to provide the apprentice mechanic the ability to diagnose and repair the turbine engine like an expert mechanic. The expert system was designed and built by the U.S. Army Research Laboratory (ARL) and the U.S. Army Ordnance Center and School (OC&S). This paper discusses the relevant background, development issues, reasoning method, system overview, test results, return on investment, and fielding history of the project. Limited fielding began in 1994 to select Army National Guard units, and complete fielding to all M1 Abrams tank maintenance units started in 1997 and will finish by the end of 1998. The Army estimates that TED will save roughly $10 million per year through improved diagnostic accuracy and reduced waste. The development and fielding of the TED program represents the Army's first successful fielded maintenance system in the area of AI. There are several reasons associated with the success of the TED program: an appropriate domain with proper scope, a close relationship with the expert, extensive user involvement, plus others that are discussed in this paper.

#index 266421
#* Countrywide automated property evaluation system—CAPES
#@ Ingemar A. E. Hulthage;Iain Stobie
#t 1998
#c 10
#% 25468
#% 126227
#% 382826
#! The purpose of CAPES is to estimate the market value of residential properties in order to assess the collateral on Countrywide mortgage loans. CAPES estimates market value by comparison of the subject property to other similar nearby properties, for which recent sales information is available. In some cases price indices describing the change in property values over time are also used. In addition to the estimated market value, CAPES produces a measure of the uncertainty in the result. It uses several models, including heuristics derived from companyspecific business rules, and accesses both commercial and proprietary property databases. Its accuracy has been validated extensively on batches of properties by comparing its results to known sales prices. It is integrated with Countrywide's underwriting expert system and is currently being used by over thirty departments on a daily basis.

#index 266422
#* Automated intelligent pilots for combat flight simulation
#@ Randolph M. Jones;John E. Laird;Paul E. Nielsen
#t 1998
#c 10
#% 75896
#% 1478931
#% 1499580
#! TacAir-Soar is an intelligent, rule-based system that generates believable "human-like" behavior for military simulations. The innovation of the application is primarily a matter of scale and integration. The system is capable of executing most of the airborne missions that the United States military flies in fixed-wing aircraft. It accomplishes this by integrating a wide variety of intelligent capabilities, including reasoning about interacting goals, reacting to rapid changes in real time (or faster), communicating and coordinating with other agents (including humans), maintaining situational awareness, and accepting new orders while in flight. The system is currently deployed at the Oceana Naval Air Station WISSARD, and its most dramatic use to date was in the Synthetic Theater Of War 1997, an operational training exercise consisting of 48 straight hours and approximately 700 fixed-wing aircraft flights, all flown by instances of the TacAir-Soar system.

#index 266423
#* The NASD regulation advanced detection system (ADS)
#@ J. Dale Kirkland;Ted E. Senator;James J. Hayden;Tom Dybala;Henry G. Goldberg;Ping Shyr
#t 1998
#c 10
#% 136350
#% 152934
#% 176990
#% 420064
#% 703705
#! The NASD Regulation Advanced Detection System (ADS) monitors trades and quotations in the Nasdaq stock market to identify patterns and practices of behavior of potential regulatory interest. ADS has been in operational use at NASD Regulation since summer 1997 by several groups of analysts, processing approximately 2 million transactions per day, generating over 7000 breaks. More important, it has greatly expanded surveillance coverage to new areas of the market and to many new types of behavior of regulatory concern. ADS combines detection and discovery components in a single system which supports multiple regulatory domains and which share the same market data. ADS makes use of a variety of Al techniques, including visualization, pattern recognition, and data mining, in support of the activities of regulatory analysis, alert and pattern detection, and knowledge discovery.

#index 266424
#* A new technique enables dynamic replanning and rescheduling of aeromedical evacuation
#@ Alexander Kott;Victor Saks;Albert Mercer
#t 1998
#c 10
#% 111050
#% 595271
#! We describe an application of a dynamic replanning technique in a highly dynamic and complex domain: the military aeromedical evacuation of patients to medical treatment facilities. U.S. Transportation Command (USTRANSCOM) is the DoD agency responsible for evacuating patients during wartime and peace. Doctrinally, patients requiring extended treatment must be evacuated by air to a suitable Medical Treatment Facility (MTF). The Persian Gulf war was the first significant armed contlict in which this concept has been put to a serious test. The results were far from satisfactory -- about 60% of the patients ended up at the wrong destinations. In early 1993, the Department of Defense tasked USTRANSCOM to consolidate the command and control of medical regulation and aeromedical evacuation operations. The ensuing analysis led to TRAC2ES (TRANSCOM Regulating and Command and Control Evacuation System), a decision support system for planning and scheduling medical evacuation operations. Probably the most challenging aspect of the problem has to do with the dynamics of a domain in which requirements and constraints continuously change over time. Continuous dynamic replanning is a key capability of TRAC2ES. This paper describes the application and the AI approach we took in providing this capability.

#index 266672
#* Knowledge-based avoidance of drug-resistant HIV mutants
#@ Richard H. Lathrop;Nicholas R. Steffen;Miriam P. Raphael;Sophia Deeds-Rubin;Michael J. Pazzani;Paul J. Cimoch;Darryl M. See;Jeremiah G. Tilles
#t 1998
#c 10
#% 124708
#% 633387
#! We describe an artificial intelligence (AI) system (CTSHIV) that connects the scientific AIDS literature describing specific HIV drug resistances directly to the Customized Treatment Strategy of a specific HIV patient. Rules in the CTSHIV knowledge base encode knowledge about sequence mutations in the HIV genome that have been found to result in drug resistance in the HIV virus. Rules are applied to the actual HIV sequences of the virus strains infecting the specific patient undergoing clinical treatment in order to infer current drug resistance. A search through mutation sequence space identifies nearby drug resistant mutant strains that might arise. The possible drug treatment regimens currently approved by the US Food and Drug Administration (FDA) are considered and ranked by their estimated ability to avoid identified current and nearby drug resistant mutants. The highest-ranked treatments are recommended to the attending physician. The result is more precise treatment of individual HIV patients, and a decreased tendency to select for drug resistant genes in the global HIV gene pool. The application is currently in use in human clinical trials on HIV patients. Initial results from a small clinical trial are encouraging and further clinical trials are planned. From an AI viewpoint the case study demonstrates the extensibility of knowledge-based systems because it illustrates how existing encoded knowledge can be used to support new applications that were unanticipated when the original knowledge was encoded.

#index 266673
#* Success in spades: using AI planning techniques to win the world championship of computer bridge
#@ Stephen J. J. Smith;Dana S. Nau;Thomas A. Throop
#t 1998
#c 10
#% 3358
#% 179879
#% 180116
#% 201061
#% 242306
#% 243343
#% 544779
#% 1272550
#% 1499481
#% 1499545
#% 1650755
#! The latest world-championship competition for computer bridge programs was the Baron Barclay World Bridge Computer Challenge, hosted in July 1997 by the American Contract Bridge League. As reported in The New York Times and The Washington Post, the competition's winner was a new version of Great Game Products' Bridge Baron program. This version, Bridge Baron 8, has since gone on the market; and during the last three months of 1997 it was purchased by more than 1000 customers.The Bridge Baron's success also represents a significant success for research on AI planning systems, because Bridge Baron 8 uses Hierarchical Task-Network (HTN) planning techniques to plan its declarer play. This paper gives an overview of those techniques and how they are used.

#index 266674
#* ANSWER: network monitoring using object-oriented rules
#@ Gary M. Weiss;Johannes P. Ros;Anoop Singhal
#t 1998
#c 10
#% 53531
#% 116185
#% 168251
#% 314494
#% 445037
#% 461716
#% 1290131
#! This paper describes ANSWER, the expert system responsible for monitoring AT&T's 4ESS switches. These switches are extremely important, since they handle virtually all of AT&T's long distance traffic. ANSWER is implemented in R++, a rule-based extension to the C++ object-oriented programming language, and is innovative because it employs both rule-based and object-oriented programming paradigms. The use of object technology in ANSWER has provided a principled way of modeling the 4ESS and of reasoning about failures within the 4ESS. This has resulted in an expert system that is more clearly organized, easily understood and maintainable than its predecessor, which was implemented using the rule-based paradigm alone. ANSWER has been deployed for more than a year and handles all 140 of AT&T's 4ESS switches and processes over 100,000 4ESS alarms per week.

#index 266675
#* Warfighter's information packager
#@ Yigal Arens;Weixiong Zhang;Yongwon Lee;Jon Dukes-Schlossberg;Marc Zev
#t 1998
#c 10
#% 213437
#% 1290115
#! The Warfighter's Information Packager (WIP) is a suite of distributed components that allows users to easily obtain information from diverse heterogeneous data sources and to display the results in a user-defined predictable manner.WIP is based on research performed under DARPA's Intelligent rntegration of Information program. WIP uses a combination of AI and non-AI technologies to take advantage of the information push technology being developed for DARPA's Battlefield Awareness and Data Dissemination (BADD) program, and being deployed during the Fall of 1998.Together. the WIP components create a distributed system that serves as a valuable tool for information analysis by: 1) allowing the user to define high-level information products, information packages, which are parameterized by user interests and specific tasks and roles; 2) providing a web-based package viewer that dynamically constructs packages for the user on demand and performs value-added information linking; 3) allowing users to make high-value complex information requests that can span multiple data sources without a priori knowledge of the schema of the sources,; and 4) monitoring data sources and anticipating useful modifications to a user's information package.

#index 266676
#* Realtime constraint-based cinematography for complex interactive 3D worlds
#@ William H. Bares;Joël P. Grégoire;James C. Lester
#t 1998
#c 10
#% 82064
#% 109058
#% 159119
#% 179849
#% 213567
#% 402784
#% 687715
#% 1003706
#% 1478783
#% 1478924
#% 1499487
#! In 3D interactive fiction systems, a virtual camera must "film" the behaviors of multiple autonomous characters as they unpredictably interact with one another, are modified by the viewer, and manipulate artifacts in 3D worlds with complex scene geometries. It must continuously plan camera movements to clearly shoot the salient visual features of each relevant character. To address these issues, we have developed a 3D interactive fiction system with a narrative planner that, together with a bank of autonomous character directors, creates cinematic goals for a constraintbased realtime 3D virtual cinematography planner. As interactive narratives unfold, a cinematic goal selector creates view constraints to film the most salient activities performed by the characters. These constraints are then passed to a camera planner, which employs a partial constraintbased approach to compute the position and orientation of the virtual camera. This framework has been implemented in a prototype 3D interactive fiction system, COPS&ROBBERS, a testbed with multiple characters interacting in an intricate cityscape.

#index 266677
#* Expert system technology for nondestructive waste assay
#@ J. C. Determan;G. K. Becker
#t 1998
#c 10
#% 110053
#% 361854
#% 475290
#! A system combining genetic algorithms and a fuzzy-rule induction routine has been developed. Two prototype expert systems, one derived analytically, and one derived empirically by the fuzzy-rule induction system, were developed to evaluate the utility of expert system technology relative to waste nondestructive assay (NDA) data review. Technical review of waste NDA measurement data, though warranted with respect to present day waste NDA system capabilities, is labor intensive. Hence it is desirable to have an automated system to perform technical review. It has been shown that both the analytically and empirically derived expert systems produce reasonable results, but that the automatic system can produce fuzzy rules more efficiently and accurately than the analytical method. A visual explanation facility for fuzzy expert systems has also been developed.

#index 266678
#* Bayesian network models for generation of crisis management training scenarios
#@ Eugene Grois;William H. Hsu;Mikhail Voloshin;David C. Wilkins
#t 1998
#c 10
#% 241
#% 25470
#% 44876
#% 174161
#% 183492
#% 405882
#% 1650791
#! We present a noisy-OR Bayesian network model for simulation-based training, and an efficient search-based algorithm for automatic synthesis of plausible training scenarios from constraint specifications. This randomized algorithm for approximate causal inference is shown to outperform other randomized methods, such as those based on perturbation of the maximally plausible scenario. It has the added advantage of being able to generate acceptable scenarios (based on a maximum penalized likelihood criterion) faster than human subject matter experts, and with greater diversity than deterministic inference. We describe a field-tested interactive training system for crisis management and show how our model can be applied offline to produce scenario specifications. We then evaluate the performance of our automatic scenario generator and compare its results to those achieved by human instructors, stochastic simulation, and maximum likelihood inference. Finally, we discuss the applicability of our system and framework to a broader range of modeling problems for computer-assisted instruction.

#index 266679
#* Hybrid knowledge based system for automatic classification of B-scan images from ultrasonic rail inspection
#@ J. Jarmulak;E. J. H. Kerckhoffs;P. P. van't Veen
#t 1998
#c 10
#% 412755
#! Dutch Railways use a special train for the ultrasonic inspection of rails. The output of the ultrasonic scanning system installed on the train consists of echo images - so-called B-scans. The Bscans contain images of rail constructions, noise artifacts, and/or defects. Originally, all the images were interpreted and classified by an operator. Later, a simple rule-based classifier was build which could classifY some of the images automatically. Recently, a new version of the train has been built capable of faster and more detailed rail inspection. This necessitated improvement of the automatic classification software. A prototype system has been developed which uses both a rule-based expert system and case-based reasoning (CBR) for the image classification. A hybrid architecture has been chosen because it satisfies the requirements better than systems based on one technique only. The paper describes the overall system design and presents the results of tests on real data. The future work necessary for the deployment of the system on the inspection train is outlined.

#index 266680
#* Control strategies in HTN planning: theory versus practice
#@ Dana S. Nau;Stephen J. J. Smith;Kutluhan Erol
#t 1998
#c 10
#% 25470
#% 132174
#% 179879
#% 180116
#% 243343
#% 266673
#% 544779
#% 1272550
#% 1290113
#% 1499481
#% 1499545
#! AI planning techniques are beginning to find use in a number of practical planning domains. However, the backward-chaining and partial-order-planning control strategies traditionally used in AI planning systems are not necessarily the best ones to use for practical planning problems. In this paper, we discuss some of the difficulties that can result from the use of backward chaining and partial-order planning, and we describe how these difficulties can be overcome by adapting Hierarchical Task-Network (HTN) planning to use a total-order control strategy that generates the steps of a plan in the same order that those steps will be executed. We also examine how introducing the total-order restriction into HTN planning affects its expressive power, and propose a way to relax the total-order restriction to increase its expressive power and range of applicability.

#index 266681
#* A prototype application of fuzzy logic and expert systems in education assessment
#@ James R. Nolan
#t 1998
#c 10
#% 223517
#% 360803
#! This paper reports on the design and development of an expert fuzzy classification scoring system for grading student writing samples. The growing use of written response tests in the education sector provides fertile domain areas for new and innovative applications of soft computing and expert systems technology. The main function of the expert fuzzy classification scoring system is to support teachers in the evaluation of student writing samples by providing them with a uniform framework for generating ratings based on the consistent application of scoring rubrics. The system has been tested using actual student response data. A controlled experiment demonstrated that teachers using the expert fuzzy classification scoring system can make assessments in less time and with a level of accuracy comparable to the best teacher graders. The paper introduces fuzzy classification techniques that can encapsulate knowledge about imprecise qualities needed for constructing rule-based scoring models that provide consistent, uniform scoring results. This increased consistency in the application of the scoring rubrics allows for more valid individual and group assessment.

#index 266682
#* Intelligent control of life support systems for space habitats
#@ Debra Schreckenghost;Daniel Ryan;Carroll Thronesbery;Peter Bonasso;Daniel Poirot
#t 1998
#c 10
#% 1383855
#% 1478923
#! The Interchamber Monitoring and Control (IMC) system is semi-autonomous, intelligent software that controls life support systems designed for recycling air in remote space habitats. The IMC system was developed using the 3T autonomous control architecture. This architecture integrates traditional control with reactive task sequencing and deliberative planning technology. The IMC system was used during the Phase ill test of NASA's LunarlMars Life Support Test Program (LMLSTP). For this test, four crew members lived in a closed habitat for 91 days. The objective of using an intelligent control system was to reduce the need for crew involvement in nominal control of life support by automating control operations. Prior to this test, manually intensive traditional process control software had been used to control LMLSTP life support systems. It is impractical and inefficient for crew at a remote site to continuously monitor day-to-day operation of life support systems. Our intelligent control software autonomously handles nominal and expected anomalous situations. The crew only intervenes in exceptional or novel situations. During the Phase ill test we demonstrated the viability of using intelligent control for such automation.

#index 266683
#* Split up: the use of an argument based knowledge representation to meet expectations of different user for discretionary decision making
#@ Andrew Stranieri;John Zeleznikow
#t 1998
#c 10
#% 64714
#% 142469
#% 149858
#% 177915
#% 198464
#% 200419
#% 235456
#% 296914
#% 367665
#% 535963
#% 699051
#! Split Up is a rule / neural hybrid that represents knowledge using frames based on the argument structure proposed by the British philosopher, Toulmin. Split Up makes predictions about marital property following a divorce in Australia; a domain that is considered discretionary in that a judge has considerable flexibility. The end users of Split Up are judges and registrars of the Family Court of Australia, mediators and lawyers. Each end user has specific and divergent needs and thus uses the system in different ways however all users rely on effective explanations. The argument based representation of knowledge enables the system to have the flexibility required of different users, to generate effective explanations and also facilitates knowledge acquisition. The framework has been used to integrate rules with neural networks but can easily be used to integrate other inferencing methods.

#index 266684
#* An expert system for alarm system planning
#@ Akira Tsurushima;Kenji Urushima;Daigo Sakata
#t 1998
#c 10
#! This paper discusses the design and implementation of ESSPL, an expert system which generates security plans for alarm systems (Figure 1). Security planning is the task of determining an efficient layout of sensors, alarms, and the associated wiring for a building. ESSPL uses Rule Based System technology to generate a plan and Constraint Based technology to position the alarm equipment. A Constraint Logic Programming Language (CONTA) is developed to solve the positioning problem.ESSPL proved to be an excellent tool in automatic planning in laboratory tests. Field tests were also carried out and examined whether there is a drawback to use ESSPL in the real world context. This is also discussed in this paper.

#index 266685
#* Conversation machines for transaction processing
#@ Wlodek Zadrozny;Catherine Wolf;Nanda Kambhatla;Yiming Ye
#t 1998
#c 10
#% 202032
#% 218349
#% 247327
#% 674577
#! We have built a set of integrated AI systems (called conversation machines) to enable transaction processing over the telephone for limited domains like stock trading and banking. The conversation machines integrate the state-of-the-art technologies from computer telephony, continuous speech recognition, natural language processing and humancomputer interaction. Users can interact with these systems using natural language to process simple transactions. We are currently installing a prototype conversation machine at a customer site (a large bank), while continuing research on each of the modules mentioned above and their integration.In this paper, we describe the architecture of conversation machines and explain the design choices related to natural language dialog design, speech recognition errors, and human-computer interaction. We also discuss our experience with the new "market-driven research" methodology currently being tested at our company, of which the conversation machines project is an example. Our experience suggests that with this new methodology we can build integrated natural language dialog systems, even when working with error-prone recognition engines and imperfect grammars, by designing the dialog flow to reduce the likelihood of errors, and to enable quick error recovery. In this process, having a customer allows us to make more realistic design choices.

#index 266686
#* Optimizing information agents by selectively materializing data
#@ Naveen Ashish
#t 1998
#c 10
#% 266102

#index 266687
#* Generating adequate instructions: knowing when to stop
#@ Juliet C. Bourne
#t 1998
#c 10
#% 746888

#index 266688
#* HR—automatic concept formation in finite algebras
#@ Simon Colton
#t 1998
#c 10

#index 266689
#* Optimizing initial configurations of neural networks for the task of natural language learning
#@ Jaime J. Dávila
#t 1998
#c 10
#% 1042790

#index 266690
#* Pragmatic multi-agent learning
#@ Andrew Garland
#t 1998
#c 10

#index 266691
#* Perception, memory, and the field of view problem
#@ William S. Gribble
#t 1998
#c 10

#index 266692
#* Exploiting diversity for natural language processing
#@ John C. Henderson
#t 1998
#c 10
#% 209021
#% 235377

#index 266693
#* Multimodal, multilevel selective attention
#@ Michael Hewett
#t 1998
#c 10
#% 128686
#% 153134

#index 266694
#* Learning in Markov games with incomplete information
#@ Junling Hu
#t 1998
#c 10
#% 223835
#% 252803

#index 266695
#* Extending the classification paradigm to temporal domains
#@ Mohommed Waleed Kadous
#t 1998
#c 10

#index 266696
#* Data mining for maintenance of complex systems
#@ Sylvain Létourneau
#t 1998
#c 10
#% 458386

#index 266697
#* Empirical acquisition of word-sense distinctions
#@ Tom O'Hara
#t 1998
#c 10
#% 756952
#% 818038

#index 266698
#* Neural approaches to blind separation and cumulant analysis and its application to diagnostics of nuclear power plants
#@ Alexei Ourmanov
#t 1998
#c 10

#index 266699
#* Bayesian reasoning for tropical cyclone intensity forecasting and risk analysis
#@ Grace W. Rumantir
#t 1998
#c 10

#index 266700
#* Rational multiagent organization and reorganization
#@ Wayne A. Smith
#t 1998
#c 10

#index 266701
#* A script-based approach to modifying knowledge-based systems
#@ Marcelo Tallis
#t 1998
#c 10
#% 55936
#% 179741

#index 266702
#* Learning to teach with a reinforcement learning agent
#@ Joseph E. Beck
#t 1998
#c 10
#% 172901

#index 266703
#* Genetic search for accurate feature sets
#@ Brendan Burns
#t 1998
#c 10
#% 143189
#% 216715
#% 465756
#% 703361

#index 266704
#* A first analysis of qualitative influences and synergies
#@ Jesús Cerquides;Ramon López de Màntaras
#t 1998
#c 10

#index 266705
#* A new approach to rule interest measures
#@ Jesús Cerquides;Ramon López de Màntarar
#t 1998
#c 10

#index 266706
#* Classification using an online genetic algorithm
#@ Brian D. Davison
#t 1998
#c 10
#% 369236
#% 383262

#index 266707
#* Plan recognition in complex spatial domains
#@ Mark Devaney
#t 1998
#c 10

#index 266708
#* Nested joint probability model for morphological analysis and its grid pruning
#@ Koji Fujimoto;Nobuo Inui;Yoshiyuki Kotani
#t 1998
#c 10
#% 266708
#! In recent work on morphological analysis based on statistical models the conditional probability of the observed i-th word Wi with the i-th tag ti after the (i-1)-th tag ti-1 is defined as the product of observation symbol probability and the state transition probability (i.e. P(Wi|ti)驴P(ti|ti-1)). In order to improve accuracy, we face the following problems: 1) If we build hidden state levels using stricter categories (e.g. lowest POS class, over 3-gram, or word themselves), the state transition probability matrix becomes much bigger and more sparse; 2) If we use rough categories the reliability of statistical information becomes lower in some parts of speech; and 3) the best state level is not the same among POS category, and some heuristic knowledge is necessary to select the best state structure.

#index 266709
#* Generlized A* for cyclic AND/OR graphs
#@ Supriyo Ghose
#t 1998
#c 10
#% 160381
#% 209196
#! The A* algorithm (Hart, Nilsson and Raphael 1968) has been the cornerstone of state-space search methods. Simultaneously, the vexing problem of cycles in AND/OR graphs has received considerable attention in recent times (Ghose 1998, Hvalica 1996, Chakrabarti 1994). We propose a generalization of A* to search AND/OR graphs that may contain cycles. The basic idea is that, if each AND node in an AND/OR graph has exactly one child, then the graph is virtually an ordinary (OR) graph and can be searched by applying A*-like steps. This idea is simulated in our algorithm, GA*, by making full expansion of OR nodes (as in A*) but partial expansion of AND nodes. While expanding an AND node, GA* generates only the leftmost unsolved child, and adds to its cost the costs of all other children of the AND parent. This updated cost is maintained as the current cost provided the child has not been generated earlier in this iteration, or if the updated cost is less than the previously computed cost through some other path. This is done iteratively, using two lists OPEN and CLOSED. An iteration starts by putting s in OPEN, continues by selecting and expanding nodes like A*, and ends either (a) successfully by selecting a terminal leaf or a previously SOLVED node, or (b) unsuccessfully when it finds it has no more nodes to expand (in which case GA* terminates with FAILURE). At the end of an iteration, if s is SOLVED then GA* terminates with SUCCESS. Furthermore, in each iteration, backpointers are set from nodes to their parents, as in A*, to indicate the current minimum costly path to each node. When an iteration of GA* ends successfully, GA* traces these backpointers and updates the heuristic estimates of nodes "higher up" in the solution graph, and declares some of them SOLVED. Our conjecture is that, in each successful iteration of GA*, at least one distinct node is labeled SOLVED; this node has its heuristic estimate set to its minimum cost of solution. If N is the number of nodes lying on any path P from s such that cost of P 驴 h* (s), then GA* has a complexity of O(N2) node expansions with monotone heuristics; under admissible heuristics, its worst-case complexity of O(N2N) can be reduced to O(N3) by applying modifications similar to (Martelli 1977). The empirical performance of GA* is currently under investigation. A broad outline of GA* is given below.

#index 266710
#* Selection of conflict resolution strategies in dynamically organized sensible agent-based systems
#@ T. H. Liu;K. S. Barber
#t 1998
#c 10
#% 82812

#index 266711
#* Refinement-based planning as satisfiability
#@ Amol D. Mali
#t 1998
#c 10
#% 1476298

#index 266712
#* Goal and responsibility allocation in sensible agent-based systems
#@ Ryan McKay;K. S. Barber
#t 1998
#c 10
#% 1013352

#index 266713
#* Tutorial response generation in a writing tool for deaf learners of English
#@ Lisa N. Michaud
#t 1998
#c 10
#% 740937

#index 266714
#* Dependent bigram identification
#@ Ted Pedersen
#t 1998
#c 10

#index 266715
#* Raw corpus word sense disambiguation
#@ Ted Pederson
#t 1998
#c 10

#index 266716
#* DISCOURSE LEARNING: dialogue act tagging with transformation-based learning
#@ Ken Samuel
#t 1998
#c 10
#% 196896
#% 515213

#index 266717
#* Estimating the expected error of empirical minimizers for model selection
#@ Tobias Scheffer;Thorsten Joachims
#t 1998
#c 10
#% 697
#% 229806
#! Model selection [e.g., 1] is considered the problem of choosing a hypothesis language which provides an optimal balance between low empirical error and high structural complexity. In this Abstract, we discuss the intuition of a new, very efficient approach to model selection. Our approach is inherently Bayesian [e.g., 2], but instead of using priors on target functions or hypotheses, we talk about priors on error values - which leads us to a new mathematical characterization of the expected true error. In the setting of classification learning, a learner is given a sample, drawn according to an unknown distribution of labeled instances and returns the empirical minimizer (the hypothesis with the least empirical error) which has a certain (unknown) true error. If this process is carried out repeatedly, the true error of the empirical minimizers, will vary from run to run as the empirical minimizer depends on the (randomly drawn) sample. This induces a distribution of true errors of empirical minimizers, over the possible samples drawn according to the unknown distribution. If this distribution would be known one could easily derive the expected true error of the empirical minimizer of a model by integrating over this distribution. This would immediately lead to an optimal model selection algorithm: Enumerate the models, calculate the expected error of each model by integrating over the error distribution, and select the model with the least expected error. PAC theory [3] and the vC framework provide worst-case bounds on the chance of drawing a sample such that the true error of the minimizer exceeds some 驴 - "worst-case" meaning that they hold for any distribution of instances and any concept in a given class. By contrast, we focus on how to determine this distribution for a fixed, given learning problem (under some specified assumptions). Unlike the worst-case bound (which depends only on the size, or VC-dimension of the hypothesis space) the actual error distribution depends on the hypothesis space and the unknown distribution of labeled instances itself. Howevr, we can prove that, under a certain assumption of independence of hypotheses, the distribution of true errors - and hence the expected true error - can be expressed as a function of the distribution of empirical errors of uniformly drawn hypotheses (which can be thought of as a prior on error values). The latter distribution (which is always one-dimensional) can be estimated from a fixed-sized initial portion of the training data and a fixed-sized set of randomly drawn hypotheses. This estimate of the distribution now leads us' to an estimate of the expected true error of the empirical minimizer of the model - which, in turn, leads to a highly efficient model selection algorithm We study the behavior of this approach in several controlled experiments. Our results show that the accuracy of the error estimate is at least comparable to the accuracy of the estimate obtained by 10-fold cross-validation - provided the prior on error values can be estimated using at least 50 examples. But while 10-CV requires ten invocations of the learner per model, the time which our algorithm requires to assess each model is constant in the size of the model. We also study the robustness of our algorithm against violations of our in dependence assumptions. We can observe a bias in our predictions when the hypotheses space is of size four or less. When the hypothesis space is of size 40 or more. the dependencies are so diluted that the violations: of our assumptions are negligible and do not incur a significant error. The full paper is available at http://ki.cs.tuberlin. de/~scheffer/papers/ eed-report.ps.

#index 266718
#* Pluto: managing multistrategy learning through planning
#@ Gordon T. Shippey;J. William Murdock;Ashwin Ram
#t 1998
#c 10

#index 266719
#* A framework for reinforcement learning on real robots
#@ William D. Smart;Leslie Pack Kaelbling
#t 1998
#c 10

#index 266720
#* Handling inconsistency for multi-source integration
#@ Sheila Tejada;Craig A. Knoblock;Steven Minton
#t 1998
#c 10
#! The overwhelming amount of information sources now available through the internet has increased the need to combine or integrate the data retrieved from these sources in an intelligent and efficient manner. A desirable approach for information integration would be to have a single interface, like the SIMS information broker [1], which allows access to multiple information sources. An example application is to retrieve all the menus of restaurants from Joe's Favorite Restaurants site which have been rated highly by the Department of Health.

#index 266721
#* Emotion-based agents
#@ Rodrigo M. M. Ventura;Carlos A. Pinto-Ferreira
#t 1998
#c 10

#index 266722
#* DL-$;elect: a decision-list-based data-mining system
#@ Karl Weinmeister
#t 1998
#c 10
#% 179770
#% 449559

#index 266723
#* Ensuring reasoning consistency in hierarchical architectures
#@ Robert E. Wray, III;John Laird
#t 1998
#c 10
#% 23011
#% 266393
#% 444751
#% 1499580

#index 266724
#* Building agents from shared ontologies through apprenticeship multistrategy learning
#@ Kathryn Wright;Mihai Boicu;Seok Won Lee;Gheorghe Tecuci
#t 1998
#c 10
#% 356978

#index 266725
#* Development of outdoor navigation for a robotic wheelchair system
#@ Holly A. Yanco
#t 1998
#c 10
#% 359129

#index 266726
#* Structured probabilistic models: Bayesian networks and beyond
#@ Daphne Koller
#t 1998
#c 10
#% 44876
#% 75936
#% 265806
#% 266230
#% 536408
#% 1650767
#! For many years, probabilistic models were largely neglected within the AI community. Now they play a fundamental role in many areas in AI, including diagnosis, planning, and learning. One of the crucial reasons for this transition is the use of structured model-based representations such as Bayesian networks. Building on this idea, we can extend the success of probabilistic modeling to much more complex domains, ones involving many components that interact and evolve ove time. These domains are significantly beyond the scope of traditional Bayesian networks. I describe a broad class of structured probabilistic representations that extend Bayesian networks to deal with these new challenges. I argue that these representations can form the basis for agents that reason and act in complex uncertain environments.

#index 282981
#* Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence
#@ Jim Hendler;Devika Subramanian;Ramasamy Uthurusamy;Barbara Hayes-Roth
#t 1999
#c 10

#index 282989
#* Time-quality tradeoffs in reallocative negotiation with combinatorial contract types
#@ Martin Andersson;Tuomas Sandholm
#t 1999
#c 10
#% 2194
#% 70370
#% 82813
#% 174569
#% 188076
#% 208275
#% 256502
#% 267752
#% 559090
#% 636336
#% 704123
#% 1013352
#! The capability to reallocate items--e.g. tasks, securities, bandwidth slices, Mega Watt hours of electricity, and collectibles--is a key feature in automated negotiation. Especially when agents have preferences over combinations of items, this is highly nontrivial. Marginal cost based reallocation leads to an anytime algorithm where every agent's payoff increases monotonically over time. Different contract types head toward different locally optimal allocations of items, and OCSM-contracts head toward the global optimum. Reaching it can take impractically long, so it is important to trade off solution quality against negotiation time. To construct negotiation protocols that lead to good allocations quickly, we evaluated original (O), cluster (C), swap (S), and multiagent (M) contracts experimentally. O-contracts led to the highest social welfare when the ratio of agents to tasks was large, and C-contract were best when that ratio was small. O-contracts led to the largest number of contracts made. M-contracts were slower per contract, and required a significantly larger number of contracts to be tried to verify that a local optimum had been reached. S-contracts were not competitive because they restrict the search space by keeping the number of items per agent invariant. O-contracts spread the items across agents while C-contracts and M-contracts concentrated them on a few agents.

#index 283007
#* Distributed games: from mechanisms to protocols
#@ Dov Monderer;Moshe Tennenholtz
#t 1999
#c 10
#% 52273
#% 54742
#% 86446
#% 233131
#% 233135
#% 265785
#! The theory of mechanism design in economics/game theory deals with a center who wishes to maximize an objective function which depends on a vector of information variables. The value of each variable is known only to a selfish agent, which is not controlled by the center. In order to obtain its objective the center constructs a game, in which every agent participates and reveals its information, because these actions maximize its utility. However, several crucial new issues arise when one tries to transform existing economic mechanisms into protocols to be used in computational environments. In this paper we deal with two such issues: 1. The communication structure, and 2. the representation (syntax) of the agents' information. The existing literature on mechanism design implicitly assumes that these two features are not relevant. In particular, it assumes a communication structure in which every agent is directly connected to the center. We present new protocols that can be implemented in a large variety of communication structures, and discuss the sensitivity of these protocols to the way in which information is presented.

#index 283035
#* Power, dependence and stability in multiagent plans
#@ Sviatoslav Brainov;Tuomas Sandholm
#t 1999
#c 10
#% 68239
#% 1275317
#! In this paper we present a decision-theoretic model of social power and social dependence that accounts for origins of different choices available in different situations. According to the model almost every group activity, whether it is cooperation or exploitation, has its origins in resolving some dependence or power relation. The model is intended for self-interested agents and explains power and dependence in terms of relations between agents' plans. It is a generalization of the dependence network model and accounts for situations of group dependence, i.e., situations in which an agent depends on a group or a group depends on an agent. The model is applied to the analysis of stability of multiagent plans. Stable dependence structures of multiagent plans are identified. Necessary and sufficient conditions for stability of joint plans are provided.

#index 283038
#* Combatting maelstroms in networks of communicating agents
#@ James E. Hanson;Jeffrey O. Kephart
#t 1999
#c 10
#% 89841
#! Multi-agent systems in which agents can respond to messages by automatically generating and multicasting other messages are inherently vulnerable to a phenomenon that we call a maelstrom. We define a maelstrom to be a self-sustaining chain reaction in which a single message can unintentionally trigger the generation of a rapidly growing, potentially infinite number of messages, quickly incapacitating the communications network. There is reason to fear that modest advances in agent technology and usability could lead to spontaneous maelstroms on the Internet in the near future, particularly in the realm of electronic mail. In this article we describe various classes of maelstroms that may arise due to automated forwarding of messages and propose a novel and practical means of combatting them.

#index 283042
#* Learning quantitative knowledge for multiagent coordination
#@ David Jensen;Michael Atighetchi;Régis Vincent;Victor Lesser
#t 1999
#c 10
#% 252772
#% 252803
#% 252804
#% 256564
#% 256565
#% 275427
#% 307109
#% 636359
#% 677346
#% 677395
#% 1478735
#% 1650665
#! A central challenge of multiagent coordination is reasoning about how the actions of one agent affect the actions of another. Knowledge of these interrelationships can help coordinate agents -- preventing conflicts and exploiting beneficial relationships among actions. We explore three interlocking methods that learn quantitative knowledge of such non-local effects in T脝EMS, a well-developed framework for multiagent coordination. The surprising simplicity and effectiveness of these methods demonstrates how agents can learn domain-specific knowledge quickly, extending the utility of coordination frameworks that explicitly represent coordination knowledge.

#index 283045
#* Evolutionary economic agents
#@ Fergus Nolan;Jarek Wilkiewicz;Dipankar Dasgupta;Stan Franklin
#t 1999
#c 10
#% 186478
#% 203566
#% 214373
#% 557050
#% 1022863
#% 1383855
#% 1383857
#! An empirical work is described which compares the optimization levels produced by a group of economic agents versus those of a similar group of economic agents which additionally employ a genetic algorithm (GA) to attain a higher level of optimization. The problem domain is multimodal. It incorporates multiple hard and soft constraints and dynamical behaviors. It also has areas of infeasibility and non-linear behaviors. The simulated model environment provides several types of sensors, actuators and opportunities for interagent resource mediation. Evidence is offered to support the theory that multiple weak methods operating in concert, on a shared problem, can produce better results than the individual weak methods acting alone. The problem area is resistant to the use of strong methods.

#index 283047
#* Bargaining with deadlines
#@ Tuomas Sandholm;Nir Vulkan
#t 1999
#c 10
#% 189700
#% 233135
#! This paper analyzes automated distributive negotiation where agents have firm deadlines that are private information. The agents are allowed to make and accept offers in any order in continuous time. We show that the only sequential equilibrium outcome is one where the agents wait until the first deadline, at which point that agent concedes everything to the other. This holds for pure and mixed strategies. So, interestingly, rational agents can never agree to a nontrivial split because offers signal enough weakness of bargaining power (early deadline) so that the recipient should never accept. Similarly, the offerer knows that it offered too much if the offer gets accepted: the offerer could have done better by out-waiting the opponent. In most cases, the deadline effect completely overrides time discounting and risk aversion: an agent's payoff does not change with its discount factor or risk attitude. Several implications for the design of negotiating agents are discussed. We also present an effective protocol that implements the equilibrium outcome in dominant strategies.

#index 283049
#* Verifying that agents implement a communication language
#@ Michael Wooldridge
#t 1999
#c 10
#% 56086
#% 114677
#% 188086
#% 190683
#% 320204
#% 636348
#! In recent years, a number of attempts have been made to develop standardized agent communication languages. A key issue in such languages is that of conformance testing. That is, given a program which claims to semantically conform to some agent communication standard, how can we determine whether or not it does indeed conform to it? In this article, we present an expressive agent communication language, and give a semantics for this language in such a way that verifying semantic conformance becomes a realistic possibility. The techniques we develop draw upon those used to give a semantics to reactive systems in theoretical computer science. To illustrate the approach, we give an example of a simple agent system, and show that it does indeed respect the semantics.

#index 283050
#* Recognizing structure in Web pages using similarity queries
#@ William W. Cohen
#t 1999
#c 10
#% 55490
#% 227992
#% 227994
#% 248801
#% 248852
#% 248862
#% 252834
#% 266102
#% 481923
#! We present general-purpose methods for recognizing certain types of structure in HTML documents. The methods are implemented using WHIRL, a "soft" logic that incorporates a notion of textual similarity developed in the information retrieval community. In an experimental evaluation on 82 Web pages, the structure ranked first by our method is "meaningful"--i.e., a structure that was used in a hand-coded "wrapper", or extraction program, for the page-nearly 70% of the time. This improves on a value of 50% obtained by an earlier method. With appropriate background information, the structure-recognition methods we describe can also be used to learn a wrapper from examples, or for maintaining a wrapper as a Web page changes format. In these settings, the top-ranked structure is meaningful nearly 85% of the time.

#index 283052
#* Navigational plans for data integration
#@ Marc Friedman;Alon Levy;Todd Millstein
#t 1999
#c 10
#% 123118
#% 210176
#% 229827
#% 244103
#% 248026
#% 248038
#% 248801
#% 252834
#% 266102
#% 273911
#% 273912
#% 458745
#% 479452
#% 504570
#% 1499471
#! We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.

#index 283053
#* Regression testing for wrapper maintenance
#@ Nicholas Kushmerick
#t 1999
#c 10
#% 275915
#% 705442
#! Recent work on Internet information integration assumes a library of wrappers, specialized information extraction procedures. Maintaining wrappers is difficult, because the formatting regularities on which they rely often change. The wrapper verification problem is to determine whether a wrapper is correct. Standard regression testing approaches are inappropriate, because both the formatting regularities and a site's underlying content may change. Wei ntroduce RAPTURE, a fully-implemented, domain-independenvt erification algorithm. RAPTURE uses well-motivated heuristics to compute the similarity between a wrapper's expected and observed output. Experiments with 27 actual Internet sites show a substantial performance improvement over standard regression testing.

#index 283055
#* A knowledge-based approach to organizing retrieved documents
#@ Wanda Pratt;Marti A. Hearst;Lawrence M. Fagan
#t 1999
#c 10
#% 23817
#% 46803
#% 50506
#% 115476
#% 151477
#% 218992
#% 249155
#% 709383
#! When people use computer-based tools to find answers to general questions, they often are faced with a daunting list of search results or "hits" returned by the search engine. Many search tools address this problem by helping users to make their searches more specific. However, when dozens or hundreds of documents are relevant to their question, users need tools that help them to explore and to understand their search results, rather than ones that eliminate a portion of those results. In this paper, we present DynaCat, a tool that dynamically categorizes search results into a hierarchical organization by using knowledge of important kinds of queries and a model of the domain terminology. Results from our evaluation show that DynaCat helps users find answers to those important types of questions more quickly and easily than when they use a relevance-ranking system or a clustering system.

#index 283057
#* A limitation of the generalized Vickrey auction in electronic commerce: robustness against false-name bids
#@ Yuko Sakurai;Makoto Yokoo;Shigeo Matsubara
#t 1999
#c 10
#% 252811
#% 265785
#% 274891
#% 743843
#% 978268
#! Electronic Commerce (EC) has rapidly grown with the expansion of the Internet. Among these activities, auctions have recently achieved huge popularity, and have become a promising field for applying agent and Artificial Intelligence technologies. Although the Internet provides an infrastructure for much cheaper auctioning with many more sellers and buyers, we must consider the possibility of a new type of cheating, i.e., an agent tries to get some profit by submitting several bids under fictitious names (false-name bids). Although false-name bids are easier to execute than forming collusion, the vulnerability of auction protocols to false-name bids has not been discussed before.In this paper, we examine the robustness of the generalized Vickrey auction (G.V.A.) against false-name bids. The G.V.A. has the best theoretical background among various auction mechanisms, i.e., it has proved to be incentive compatible and be able to achieve a Pareto efficient allocation. We show that false-name bids may be effective, i.e., the G.V.A. loses incentive compatibility under the possibility of false-name bids, when the marginal utility of an item increases or goods are complementary. Moreover, we prove that there exists no single-round sealed-bid auction protocol that simultaneously satisfies individual rationality, Pareto efficiency, and incentive compatibility in all cases if agents can submit false-name bids.

#index 283061
#* Hybrid neural plausibility networks for news agents
#@ Stefan Wermter;Christo Panchev;Garen Arevian
#t 1999
#c 10
#% 136369
#% 157035
#% 169717
#% 182126
#% 266215
#% 458379
#! This paper describes a learning news agent HyNeT which uses hybrid neural network techniques for classifying news titles as they appear on an internet newswire. Recurrent plausibility networks with local memory are developed and examined for learning robust text routing. HyNeT is described for the first time in this paper. We show that a careful hybrid integration of techniques from neural network architectures, learning and information retrieval can reach consistent recall and precision rates of more than 92% on an 82 000 word corpus; this is demonstrated for 10000 unknown news titles from the Reuters newswire. This new synthesis of neural networks, learning and information retrieval techniques allows us to scale up to a real-world task and demonstrates a lot of potential for hybrid plausibility networks for semantic text routing agents on the internet.

#index 283063
#* Cognitive classification
#@ Janet Aisbett;Greg Gibbon
#t 1999
#c 10
#% 420065
#% 486166
#% 520724
#! Classification assigns an entity to a category on the basis of feature values encoded from a stimulus. Provided they are presented with sufficient training data, inductive classifier builders such as C4.5 are limited by encoding deficiencies and noise in the data, rather than by the method of deciding the category. However, such classification techniques do not perform well on the small, dirty/or and dynamic data sets which are all that are available in many decision making domains. Moreover, their computational overhead may not be justified. This paper draws on conjectures about human categorization processes to design a frugal algorithm for use with such data. On presentation of an observation, case-specific rules are derived from a small subset of the stored examples, where the subset is selected on the basis of similarity to the encoded stimulus. Attention is focused on those features that appear to be most useful for distinguishing categories of observations similar to the current one. A measure of logical semantic information value is used to discriminate between categories that remain plausible after this. The new classifier is demonstrated against neural net and decision tree classifiers on some standard UCI data sets and shown to perform well.

#index 283065
#* What are contentful mental states?: Dretske's theory of mental content viewed in the light of robot learning and planning algorithms
#@ Paul Cohen;Mary Litch
#t 1999
#c 10
#% 252804
#% 266285
#% 283227
#% 466256
#% 549417
#! One concern of philosophy of mind is how sensorimotor agents such as human infants can develop contentful mental states. This paper discusses Fred Dretske's theory of mental content in the context of results from our work with mobile robots. We argue that Dretske's theory, while attractive in many ways, relies on a distinction between kinds of representations that cannot be practically maintained when the subject of one's study is robotic agents. In addition, Dretske fails to distinguish classes of representations that carry different kinds of mental content. We conclude with directions for a theory of mental content that maintains the strengths of Dretske's theory.

#index 283085
#* Student-sensitive multimodal explanation generation for 3D learning environments
#@ Brent H. Daniel;Charles B. Callaway;William H. Bares;James C. Lester
#t 1999
#c 10
#% 145400
#% 149389
#% 152767
#% 175130
#% 177915
#% 265800
#% 1290062
#% 1478783
#% 1478924
#% 1499487
#! Intelligent multimedia systems hold great promise for knowledge-based learning environments. Because of recent advances in Our understanding of how to dynamically generate multimodal explanations and the rapid growth in the performance of 3D graphics technologies, it is becoming feasible to create multimodal explanation generators that operate in realtime. Perhaps most compelling about these developments is the prospect of enabling generators to create explanations that are customized to the ongoing "dialogue" in which they occur. To address these issues, we have developed a student-sensitive multimodal explanation generation framework that exploits a discourse history to automatically create explanations whose content, cinematography, and accompanying natural language utterances are customized to the dialogue context. By these means, they create integrative explanations that actively promote knowledge integration. This framework has been implemented in CINESPEAK, a student-sensitive multimodal explanation generator.

#index 283086
#* Moving right along: a computational model of metaphoric reasoning about events
#@ Srinivas Narayanan
#t 1999
#c 10
#% 78338
#% 103309
#% 380725
#% 673836
#% 706138
#% 1273780
#% 1275282
#! This paper describes the results of an implemented computational model that cashes out the belief that reasoning about abstract events and actions relies on metaphoric projections of embodied primitives. The specific task addressed is the interpretation of simple causal narratives taken from newspaper articles in the domains of Politics and Economics. When presented with a surface-parsed version of these narratives as input, the system described is able to generate commonsense inferences consistent with the input.

#index 283087
#* Delivering hints in a dialogue-based intelligent tutoring system
#@ Yujian Zhou;Reva Freedman;Michael Glass;Joel A. Michael;Allen A. Rovick;Martha W. Evens
#t 1999
#c 10
#% 265799
#% 515063
#% 552770
#% 691027
#% 709588
#! Hinting is an important tutoring tactic in one-on-one tutoring, used when the tutor needs to respond to an unexpected answer from the student. To issue a follow-up hint that is pedagogically helpful and conversationally smooth, the tutor needs to suit the hinting strategy to the student's need while making the strategy fit the high level tutoring plan and the tutoring context. This paper describes a study of the hinting strategies in a corpus of human tutoring transcripts and the implementation of these strategies in a dialogue-based intelligent tutoring system, CIRcslM-Tutor v. 2. We isolated a set of hinting strategies from human tutoring transcripts. We describe our analysis of these strategies and a model for choosing among them based on domain knowledge, the type of error made by the student, the focus of the tutor's question, and the conversational history. We have tested our model with two classes totaling 74 medical students. Use of this extended model of hinting increases the percentage of questions that students are able to answer for themselves rather than needing to be told.

#index 283088
#* On integrating constraint propagation and linear programming for combinatorial optimization
#@ John N. Hooker;Greger Ottosson;Erlendur S. Thorsteinsson;Hak-Jin Kim
#t 1999
#c 10
#% 299169
#% 477206
#% 534162
#% 572899
#% 572907
#! Linear programming and constraint propagation are complementary techniques with the potential for integration to benefit the solution of combinatorial optimization problems. Attempts to combine them have mainly focused on incorporating either technique into the framework of the other -- traditional models have been left intact. We argue that a rethinking of our modeling traditions is necessary to achieve the greatest benefit of such an integration. We propose a declarative modeling framework in which the structure of the constraints indicates how LP and CP can interact to solve the problem.

#index 283089
#* Hierarchical constraint satisfaction in spatial databases
#@ Dimitris Papadias;Panos Kalnis;Nikos Mamoulis
#t 1999
#c 10
#% 86950
#% 126390
#% 152937
#% 172500
#% 213975
#% 273685
#% 273886
#% 319244
#% 427199
#% 442896
#% 479620
#% 526851
#% 1273792
#! Several content-based queries in spatial databases and geographic infonnation systems (GISs) can be modelled and processed as constraint satisfaction problems (CSPs). Regular CSP algorithms, however, work for main memory retrieval without utilizing indices to prune the search space. This paper shows how systematic and local search techniques can take advantage of the hierarchical decomposition of space, preserved by spatial data structures, to efficiently guide search. We study the conditions under which hierarchical constraint satisfaction outperfonns traditional methods with extensive experimentation.

#index 283090
#* A constraint-based model for cooperative response generation in information dialogues
#@ Yan Qu;Steve Beale
#t 1999
#c 10
#% 130070
#% 179883
#% 457156
#% 688553
#% 747740
#% 748199
#! This paper presents a constraint-based model for cooperative response generation for information systems dialogues, with an emphasis on detecting and resolving situations in which the user's information needs have been over-constrained. Our model integrates and extends the AI techniques of constraint satisfaction, solution synthesis and constraint hierarchy to provide an incremental computational mechanism for constructing and maintaining partial parallel solutions. Such a mechanism supports immediate detection of over-constrained situations. In addition, we explore using the knowledge in the solution synthesis network to support different relaxation strategies.

#index 283091
#* Solving crossword puzzles as probabilistic constraint satisfaction
#@ Noam M. Shazeer;Michael L. Littman;Greg A. Keim
#t 1999
#c 10
#% 44876
#% 205391
#% 283238
#% 408396
#% 669227
#% 1275309
#% 1848680
#! Crossword puzzle solving is a classic constraint satisfaction problem, but, when solving a real puzzle, the mapping from clues to variable domains is not perfectly crisp. At best, clues induce a probability distribution over viable targets, which must somehow be respected along with the constraints of the puzzle. Motivated by this type of problem, we describe a formal model of constraint satisfaction with probabilistic preferences on variable values. Two natural optimization problems are defined for this model: maximizing the probability of a correct solution, and maximizing the number of correct words (variable values) in the solution. To the latter, we apply an efficient iterative approximation equivalent to turbo decoding and present results on a collection of real and artificial crossword puzzles.

#index 283092
#* Encodings of non-binary constraint satisfaction problems
#@ Kostas Stergiou;Toby Walsh
#t 1999
#c 10
#% 160208
#% 224767
#% 266117
#% 688914
#! We perform a detailed theoretical and empirical comparison of the dual and hidden variable encodings of non-binary constraint satisfaction problems. We identify a simple relationship between the two encodings by showing how we can translate between the two by composing or decomposing relations. This translation suggests that we will tend to achieve more pruning in the dual than in the hidden variable encoding. We prove that achieving arc-consistency one dual encoding is strictly stronger than achieving arc-consistency on the hidden variable, and this itself is equivalent to achieving generalized arc-consistency on the original (non-binary) problem. We also prove that, as a consequence of the unusual topology of the constraint graph. In the hidden variable encoding, inverse consistencies like neighborhood. Inverse consistency and path inverse consistency collapse down onto arc-consistency. Finally, we propose the "double encoding", which combines together both the dual and the hidden variable encodings.

#index 283093
#* A generic customizable framework for inverse local consistency
#@ Gérard Verfaillie;David Martinez;Christian Bessière
#t 1999
#c 10
#% 3463
#% 56471
#% 115329
#% 131561
#% 147684
#% 160382
#% 189747
#% 289332
#% 320265
#% 408396
#% 534321
#% 1275303
#! Local consistency enforcing is at the core of CSP (Constraint Satisfaction Problem) solving. Although arc consistency is still the most widely used level of local consistency, researchers are going on investigating more powerful levels, such as path consistency, k-consistency, (i,j)-consistency. Recently, more attention has been turned to inverse local consistency levels, such as path inverse consistency, k-inverse consistency, neighborhood inverse consistency, which do not suffer from the drawbacks of the other local consistency levels (changes in the constraint definitions and in the constraint graph, prohibitive memory requirements).In this paper, we propose a generic framework for inverse local consistency, which includes most of the previously defined levels and allows a rich set of new levels to be defined. The first benefit of such a generic framework is to allow a user to define and test many different inverse local consistency levels, in accordance with the problem or even the instance he/she has to solve. The second benefit is to allow a generic algorithm to be defined. This algorithm, which is parameterized by the chosen inverse local consistency level, generalizes the AC7 algorithm used for arc consistency, and produces from any instance its locally consistent closure at the chosen level.

#index 283094
#* Functional elimination and 0/1/All constraints
#@ Yuanlin Zhang;Roland H. C. Yap;Joxan Jaffar
#t 1999
#c 10
#% 3463
#% 56471
#% 115329
#% 131561
#% 147684
#% 160382
#% 189747
#% 289332
#% 320265
#% 408396
#% 534321
#% 1275303
#! We present new complexity results on the class of 0/1/All constraints. The central idea involves functional elimination, a general method of elimination whose focus is on the subclass of functional constraints. One result is that for the subclass of "All" constraints, strong n-consistency and minimality is achievable in O(en) time, where e, n are the number of constraints and variables. The main result is that we can solve 0/1/All constraints in O(e(d + n)) time, where d is the domain size. This is an improvement over known results, which are O(ed(d+n)). Furthermore, our algorithm also achieves strong n-consistency and minimality.

#index 283095
#* An evolvable hardware chip and its application as a multi-function prosthetic hand controller
#@ Isamu Kajitani;Tsutomu Hoshino;Nobuki Kajihara;Musaya Iwata;Tetsuya Higuchi
#t 1999
#c 10
#% 154039
#% 369236
#% 460722
#% 465355
#% 565501
#! This paper describes the application of genetic algorithms to the biomedical engineering problem of a multi-function myoelectric prosthetic hand controller. This is achieved by an innovative LSI chip (EHW chip), i.e., a VLSI implementation of Evolvable Hardware (EHW), which can adapt its own circuit. structure to its environment autonomously and quickly by using genetic algorithms. Usually, a long training period (almost one month) is required before multi-function myoelectric prosthetic hands can be controlled, however, the EHW chip controller developed here can reduce this period and it. has been designed for easy implementation within a prosthetic hand. There are plans to commercialize the prosthetic hand with the EHW chip, and the medical department of Hokkaido University has already decided to adopt this for clinical treatment.

#index 283096
#* Initializing RBF-networks with small subsets of training examples
#@ Miroslav Kubat;Martin Cooperson, Jr.
#t 1999
#c 10
#% 73372
#% 78880
#% 81507
#% 132970
#% 197394
#% 209021
#% 465760
#% 1860366
#! An important research issue in RBF networks is how to determine the gaussian centers of the radial-basis functions. We investigate a technique that identifies these centers with carefully selected training examples, with the objective to minimize the network's size. The essence is to select three very small subsets rather than one larger subset whose size would exceed the size of the three small subsets unified. The subsets complement each other in the sense that when used by a nearest-neighbor classifier, each of them incurs errors in a different part of the instance space. The paper describes the example-selection algorithm and shows, experimentally, its merits in the design of RBF networks.

#index 283097
#* A neural network model of dynamically fluctuating perception of Necker cube as well as dot patterns
#@ Hiroaki Kudo;Tsuyoshi Yamamura;Noboru Ohnishi;Shin Kobayashi;Noboru Sugie
#t 1999
#c 10
#! The mechanism underlying perceptual grouping of visual stimuli is not static, but dynamic. In this paper, the dynamical grouping process is implemented with a neural network model consisting of an array of (hyper)columns suggested by Hubel &. Wiesel, where intracolumnar inhibition and intercolumnar facilitation are incorporated. The model was applied successfully to figures consisting of a set of dots yielding either of two ways of groupings from time to time due to neural fluctuations and fatigue. Then the model was extended to introduce dependency on fixation points as well as neural fluctuations and fatigue. Then, it was applied to the Necker Cube. The model output from time to time either of two ways of 3D interpretations depending on the fixation points.

#index 283098
#* What's in a fuzzy set?
#@ Marco Piastra
#t 1999
#c 10
#% 89958
#% 90371
#% 95350
#% 103743
#% 164788
#% 173470
#% 175356
#% 182919
#% 188267
#% 209373
#% 223517
#% 1650654
#! A modified version of the first-order logic of probability presented in (Halpern 1990) - with probability on possible worlds - makes it possible to formulate an alternative characterisation of fuzzy sets. In this approach, fuzzy sets are no longer seen as primitive entities with an intuitive justification, but rather as structured entities emerging in a suitable logical framework. Some fuzzy techniques of practical relevance are shown to be encodable in this way. In addition, the resulting approach leads to a clearer epistemological analysis in that it clarifies the purposive nature of the kind of uncertainty that can be modelled by fuzziness.

#index 283099
#* ARGUS: an automated multi-agent visitor identification system
#@ Rahul Sukthankar;Robert G. Stockton
#t 1999
#c 10
#% 229931
#% 247889
#% 252746
#% 407995
#% 485525
#% 1022958
#! ARGUS is a multi-agent visitor identification system distributed over several workstations. Human faces are extracted from security camera images by a neural-network-based face detector, and identified as frequent visitors by ARENA, a memory-based face recognition system. ARGUS then uses a messaging system to notify hosts that their guests have arrived. An interface agent enables users to submit feedback, which is immediately incorporated by ARENA to improve its face recognition performance. The ARGUS components were rapidly developed using JGram, an agent framework that is also detailed in this paper. JGram automatically converts high-level agent specifications into Java source code, and assembles complex tasks by composing individual agent services into a JGram pipeline. ARGUS has been operating successfully in an outdoor environment for several months.

#index 283100
#* Implicative and conjunctive fuzzy rules—a tool for reasoning from knowledge and examples
#@ Laurent Ughetto;Didier Dubois;Henri Prade
#t 1999
#c 10
#% 222212
#% 1787926
#! Fuzzy rule-based systems have been mainly used as a convenient tool for synthesizing control laws from data. Recently, in a knowledge representation-oriented perspective, a typology of fuzzy rules has been laid bare, by emphasizing the distinction between implicative and conjunctive fuzzy rules. The former describe pieces of generic knowledge either tainted with uncertainty or tolerant to similarity, while the latter encode examples-originated information expressing either mere possibilities or how typical situations can be extrapolated.The different types of fuzzy rules are first contrasted, and their representation discussed in the framework of possibility theory. Then, the paper studies the conjoint use of fuzzy rules expressing knowledge (as fuzzy constraints which restrict the possible states of the world), or gathering examples (which testify the possibility of appearance of some states). Coherence and inference issues are briefly addressed.

#index 283101
#* Does prior knowledge facilitate the development of knowledge-based systems?
#@ Paul Cohen;Vinay Chaudhri;Adam Pease;Robert Schrag
#t 1999
#c 10
#% 179876
#% 230386
#% 405391
#! One factor that affects the rate of knowledge base construction is the availability and reuse of prior knowledge in ontologies and domain-specific knowledge bases. This paper reports an empirical study of reuse performed in the first year of the High Performance Knowledge Bases (HPKB) initiative. The study shows that some kinds of prior knowledge help more than others, and that several factors affect how much use is made of the knowledge.

#index 283102
#* Representing problem-solving for knowledge refinement
#@ Susan Craw;Robin Boswell
#t 1999
#c 10
#% 145388
#% 161241
#% 186063
#% 459643
#% 459645
#% 661035
#! Knowledge refinement tools seek to correct faulty knowledge based systems (KBSs) by identifying and repairing potentially faulty rules. The goal of the KRusTWorks project is to provide a source of refinement components from which specialised refinement tools tailored to the needs of a range of KBSs are built. A core refinement algorithm reasons about the knowledge that has been applied, but this approach demands general knowledge structures to represent the reasoning of a particular problem solving episode. This paper investigates some complex forms of rule interaction and defines a knowledge structure encompassing these. The approach has been applied to KBSs built in four shells and is demonstrated on a small example that incorporates some of the complexity found in real applications.

#index 283115
#* Deriving expectations to guide knowledge base creation
#@ Jihie Kim;Yolanda Gil
#t 1999
#c 10
#% 55936
#% 134101
#% 179741
#% 197428
#% 443215
#% 1478786
#% 1478796
#% 1499535
#! Successful approaches to developing knowledge acquisition tools use expectations of whatthe user has to add or may want to add, based on how new knowledge fits within a knowledge base that already exists. When a knowledge base is first created or undergoes significant extensions and changes, these tools cannot provide much support. This paper presents an approach to creating expectations when a new knowledge base is built, and describes a knowledge acquisition tool that we implemented using this approach that supports users in creating problem-solving knowledge. As the knowledge base grows, the knowledge acquisition tool derives more frequent and more reliable expectations that result from enforcing constraints in the knowledge representation system, looking for missing pieces of knowledge in the knowledge base, and working out incrementally the interdependencies among the different components of the knowledge base. Our preliminary evaluations show a thirty percent time savings during knowledge acquisition. Moreover, by providing tools to support the initial phases of knowledge base development, many mistakes are detected early on and even avoided altogether. We believe that our approach contributes to improving the quality of the knowledge acquisition process and of the resulting knowledge-based systems as well.

#index 283116
#* Designing scripts to guide users in modifying knowledge-based systems
#@ Marcelo Tallis;Yolanda Gil
#t 1999
#c 10
#% 2303
#% 126289
#% 179741
#% 1478787
#% 1499535
#! Knowledge Acquisition (KA) Scripts capture typical modification sequences that users follow when they modify knowledge bases. KA tools can use these Scripts to guide users in making these modifications, ensuring that they follow all the ramifications of the change until it is completed. This paper describes our approach to design, develop, and organize a library of KA Scripts. We report the results of three different analysis to develop this library, including a detailed study of actual modification scenarios in two knowledge bases. In addition to identifying a good number of KA Scripts, we found a set of useful attributes to describe and organize the KA Scripts. These attributes allow us to analyze the size of the library and generate new KA Scripts in a systematic way. We have implemented a portion of this library and conducted two different studies to evaluate it. The result of this evaluation showed a 15 to 52 percent time savings in modifying knowledge bases and that the library included relevant and useful KA Scripts to assist users in realistic settings.

#index 283117
#* An integrated shell and methodology for rapid development of knowledge-based agents
#@ Gheorghe Tecuci;Mihai Boicu;Kathryn Wright;Seok Won Lee;Dorin Marcu;Michael Bowman
#t 1999
#c 10
#% 147678
#% 150854
#% 153537
#% 179741
#% 198055
#% 266237
#% 356978
#% 394745
#% 510344
#% 1478797
#! This paper introduces the concept of learning agent shell as a new class of tools for rapid development of practical end-to-end knowledge-based agents, by domain experts, with limited assistance from knowledge engineers. A learning agent shell consists of a learning and knowledge acquisition engine as well as an inference engine and supports building an agent with a knowledge base consisting of an ontology and a set of problem solving rules. The paper describes a specific learning agent shell and its associated agent building methodology. The process of developing an agent relies on importing ontologies from existing repositories of knowledge, and on teaching the agent how to perform various tasks, in a way that resembles how an expert would teach a human apprentice when solving. problems in cooperation. The shell and methodology represent a practical integration of knowledge representation, knowledge acquisition, learning and problem solving. This work is illustrated with an example of developing a hierarchical non-linear planning agent.

#index 283118
#* A new method for consequence finding and compilation in restricted languages
#@ Alvaro del Val
#t 1999
#c 10
#% 69150
#% 126345
#% 132176
#% 154456
#% 204396
#% 277465
#% 288165
#% 384112
#% 587385
#% 684874
#% 936786
#% 1274089
#% 1275334
#% 1275335
#% 1499539
#! SFK (skip-filtered, kernel) resolution is a new method for finding "interesting" consequencoefs a first order clausal theory 驴, namely those in some restricted target language LT. In its more restrictive form, SFK resolution corresponds to a relatively efficient SAT method, directional resolution; in its more general form, to a full prime implicate algorithm, namely Tison's. It generalizes both of them by offering much more flexible search, first order completeness, and a much wider range of inferential capabilities.SFK resolution has many applications: computing "characteristic" clauses for task-specific languages in abduction, explanation and non-monotonic reasoning (Inoue 1992); obtaining LUB approximations of the input theory (Selman and Kautz 1996) which are of polynomial size; incremental and lazy exact knowledge compilation (del Val 1994); and compilation into a tractable form for restricted target languages, independently of the tractability of inference in the given target language.

#index 283119
#* Constraint-based integrity checking in abductive and non-monotonic extensions of constraint logic programming
#@ Aditya K. Ghose;Srinivas Padmanabhuni
#t 1999
#c 10
#% 77167
#% 147548
#% 175371
#% 229241
#% 499514
#% 782325
#! Recent research on the integration of the abductive and constraint logic programming paradigms has led to systems which are both expressive and computationally efficient. This paper investigates the role of constraints in integrity checking in the context of such systems. Providing support for constraints in this role leads to a framework that is significantly more expressive, without significant loss in efficiency. We augment the Abductive Constraint Logic Programming framework with assumed constraints and provide model- and proof-theoretic accounts of two variants: one which involves commitment to such assumptions, and one which does not. We also show that such accounts extend easily to a constraint logic programming framework which supports both negation and assumed constraints. The gains in expressivity in these frameworks turn out to be particularly useful in a variety of application domains, including scheduling and constraint database updates.

#index 283120
#* Partonomic reasoning as taxonomic reasoning in medicine
#@ Udo Hahn;Stefan Schulz;Martin Romacker
#t 1999
#c 10
#% 102966
#% 221344
#% 1279731
#! Taxonomic anatomical knowledge, a major portion of medical ontologies, is fundamentally characterized by is-a and part-whole relations between concepts. While taxonomic reasoning in generalization hierarchies is well-understood, no fully conclusive mechanism as yet exists for partonomic reasoning. We here propose a new representation construct for part-whole relations, based on the formal framework of description logics, that allows us to fully reduce partonomic reasoning to classification-based taxonomic reasoning.

#index 283122
#* Verbalization of high-level formal proofs
#@ Amanda M. Holland-Minkley;Regina Barzilay;Robert L. Constable
#t 1999
#c 10
#% 1891
#% 5366
#% 7480
#% 130853
#% 139181
#% 145380
#% 200854
#% 216057
#% 473423
#% 479901
#% 481385
#% 499944
#% 748868
#! We propose a new approach to text generation from formal proofs that exploits the high-level and interactive features of a tactic-style theorem prover. The design of our system is based on communication conventions identified in a corpus of texts. We show how to use dialogue with the theorem prover to obtain information that is required for communication but is not explicitly used in reasoning.

#index 283125
#* On criteria for formal theory building: applying logic and automated reasoning tools to the social sciences
#@ Jaap Kamps
#t 1999
#c 10
#% 45220
#% 64343
#! This paper provides practical operationalizations of criteria for evaluating scientific theories, such as the consistency and falsifiability of theories and the soundness of inferences, that take into account definitions. The precise formulation of these criteria is tailored to the use of automated theorem provers and automated model generators--generic tools from the field of automated reasoning. The use of these criteria is illustrated by applying them to a first order logic representation of a classic organization theory, Thompson's Organizations in Action.

#index 283126
#* A policy description language
#@ Jorge Lobo;Randeep Bhatia;Shamim Naqvi
#t 1999
#c 10
#% 1797
#% 174161
#% 227951
#% 459282
#! A policy describes principles or strategies for a plan of action designed to achieve a particular set of goals. We define a policy as a function that maps a series of events into a set of actions. In this paper we introduce PDC, a simple but expressive language to specify policies. The design of the language has been strongly influenced by the action languages of Geffner and Bonet (Geffner & Bonet 1998) and Gelfond and Lifschitz (Gelfond & Lifschitz 1993) and the composite temporal event language of Motakis and Zaniolo (Motakis & Zaniolo 1997). The semantics is founded on recent results on formal descriptions of action theories based on automata and their application to active databases. We summarize some complexity results on the hardness of evaluating polices and briefly describe the implementation of a policy server being used to provide centralized administration of a soft switch in a communication network.

#index 283127
#* A semantic decomposition of defeasible logics
#@ M. J. Maher;G. Governatori
#t 1999
#c 10
#% 30096
#% 103705
#% 167542
#% 186896
#% 208197
#% 244373
#% 270798
#% 486460
#% 1469401
#! We investigate defeasible logics using a technique which decomposes the semantics of such logics into two parts: a specification of the structure of defeasible reasoning and a semantics for the meta-language in which the specification is written. We show that Nute's Defeasible Logic corresponds to Kunen's semantics, and develop a defeasible logic from the well-founded semantics of Van Gelder, Ross and Schlipf. We also obtain a new defeasible logic which extends an existing language by modifying the specification of Defeasible Logic. Thus our approach is productive in analysing, comparing and designing defeasible logics.

#index 283129
#* Sacre: a constraint satisfaction problem based theorem prover
#@ Jean-Michel Richer;Jean-Jacques Chabrier
#t 1999
#c 10
#% 2194
#% 32847
#% 49263
#% 126390
#% 288165
#% 420625
#% 420629
#% 517103
#% 560057
#% 837647
#% 1305150
#! The purpose of this paper is to present a new approach for solving first-order predicate logic problems stated in conjunctive normal form. We propose to combine resolution with the Constraint Satisfaction Problem (CSP) paradigm to prove the inconsistency and find a model of a problem. The resulting method benefits from resolution and constraint satisfaction techniques and seems very efficient when confronted to some problems of the CADE-13 competition.

#index 283131
#* Exploiting the architecture of dynamic systems
#@ Xavier Boyen;Daphne Koller
#t 1999
#c 10
#% 75936
#% 115608
#% 128629
#% 265806
#% 1290139
#% 1650568
#! Consider the problem of monitoring the state of a complex dynamic system, and predicting its future evolution. Exact algorithms for this task typically maintain a belief state, or distribution over the states at some point in time. Unfortunately, these algorithms fail when applied to complex processes such as those represented as dynamic Bayesian networks (DBNs), as the representation of the belief state grows exponentially with the size of the process. In (Boyen & Koller 1998), we recently proposed an efficient approximate tracking algorithm that maintains an approximate belief state that has a compact representation as a set of independent factors. Its performance depends on the error introduced by approximating a belief state of this process by a factored one. We informally argued that this error is low if the interaction between variables in the processes is "weak". In this paper, we give formal information-theoretic definitions for notions such as weak interaction and sparse interaction of processes. We use these notions to analyze the conditions under which the error induced by this type of approximation is small. We demonstrate several cases where our results formally support intuitions about strength of interaction.

#index 283134
#* Estimating generalization error using out-of-bag estimates
#@ Tom Bylander;Dennis Hanzlik
#t 1999
#c 10
#% 90661
#% 136350
#% 188076
#% 191910
#% 209021
#% 240783
#% 272995
#% 449588
#% 1290045
#% 1478814
#% 1499573
#! We provide a method for estimating the generalization error of a bag using out-of-bag estimates. In bagging, each predictor (single hypothesis) is learned from a bootstrap sample of the training examples; the output of a bag (a set of predictors) on an example is determined by voting. The out-of-bag estimate is based on recording the votes of each predictor on those training examples omitted from its bootstrap sample. Because no additional predictors are generated, the out-of-bag estimate requires considerably less time than 10- fold cross-validation. We address the question of how to use the out-of-bag estimate to estimate generalization error. Our experiments on several datasets show that the out-of-bag estimate and 10-fold cross-validation have very inaccurate (much too optimistic) confidence levels. We can improve the out-of-bag estimate by incorporating a correction.

#index 283136
#* Relational learning of pattern-match rules for information extraction
#@ Mary Elaine Califf;Raymond J. Mooney
#t 1999
#c 10
#% 179800
#% 278109
#% 311037
#% 442981
#% 449508
#% 465919
#% 496886
#% 707780
#% 1272165
#% 1290067
#! Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present a system, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains.

#index 283138
#* A simple, fast, and effective rule learner
#@ William W. Cohen;Yoram Singer
#t 1999
#c 10
#% 73372
#% 73374
#% 136350
#% 157162
#% 198701
#% 209023
#% 235377
#% 252009
#% 449508
#% 449566
#% 465746
#% 465922
#% 565528
#% 1272328

#index 283140
#* Monte Carlo localization: efficient position estimation for mobile robots
#@ Dieter Fox;Wolfram Burgard;Frank Dellaert;Sebastian Thrun
#t 1999
#c 10
#% 82083
#% 199610
#% 263023
#% 265782
#% 266407
#% 266616
#% 267315
#% 380686
#% 403449
#% 465918
#% 1650666
#! This paper presents a new algorithm for mobile robot localization, called Monte Carlo Localization (MCL). MCL is a version of Markov localization, a family of probabilistic approaches that have recently been applied with great practical success. However, previous approaches were either computationally cumbersome (such as grid-based approaches that represent the state space by high-resolution 3D grids), or had to resort to extremely coarse-grained resolutions. Our approach is computationally efficient while retaining the ability to represent (almost) arbitrary distributions. MCL applies sampling-based methods for approximating probability distributions, in a way that places computation "where needed." The number of samples is adapted on-line, thereby invoking large sample sets only when necessary. Empirical results illustrate that MCL yields improved accuracy while requiring an order of magnitude less computation when compared to previous approaches. It is also much easier to implement.

#index 283141
#* Selective sampling for nearest neighbor classifiers
#@ Michael Lindenbaum;Shaul Markovitch;Dmitry Rusakov
#t 1999
#c 10
#% 92533
#% 92546
#% 116165
#% 170649
#% 236729
#% 261307
#% 268069
#% 450951
#% 451056
#% 493094
#% 1854502
#! In the passive, traditional, approach to learning, the information available to the learner is a set of classified examples, which are randomly drawn from the instance space. In many applications, however, the initial classification of the training set is a costly process, and an intelligently selection of training examples from unlabeled data is done by an active learner.This paper proposes a lookahead algorithm for example selection and addresses the problem of active learning in the context of nearest neighbor classifiers. The proposed approach relies on using a random field model for the example labeling, which implies a dynamic change of the label estimates during the sampling process.The proposed selective sampling algorithm was evaluated empirically on artificial and real data sets. The experiments show that the proposed method outperforms other methods in most cases.

#index 283142
#* Detecting feature interactions from accuracies of random feature subsets
#@ Thomas R. Ioerger
#t 1999
#c 10
#% 42994
#% 88549
#% 103051
#% 126894
#% 136350
#% 179773
#% 272995
#% 449588
#! Interaction among features notoriously causes difficulty for machine learning algorithms because the relevance of one feature for predicting the target class can depend on the values of other features. In this paper, we introduce a new method for detecting feature interactions by evaluating the accuracies of a learning algorithm on random subsets of features. We give an operational definition for feature interactions based on when a set of features allows a learning algorithm to achieve higher than expected accuracy, assuming independence. Then we show how to adjust the sampling of random subsets in a way that is fair and balanced, given a limited amount of time. Finally, we show how decision trees built from sets of interacting features can be converted into DNF expressions to form constructed features. We demonstrate the effectiveness of the method empirically by showing that it can improve the accuracy of the C4.5 decision-tree algorithm on several benchmark databases.

#index 283143
#* Simulation-based inference for plan monitoring
#@ Neal Lesh;James Allen
#t 1999
#c 10
#% 34262
#% 44876
#% 75936
#% 124589
#% 128629
#% 194652
#% 527664
#% 527691
#% 1650568
#% 1650655
#% 1650666
#% 1650704
#% 1650711
#% 1650806
#! The dynamic execution of plans in uncertain domains requires the ability to infer likely current and future world states from past observations. We cast this task as inference on Dynamic Belief Networks (DBNs) but the resulting networks are difficult to solve with exact methods. We investigate and extend simulation algorithms for approximate inference on Bayesian networks and propose a new algorithm, called Rewind/Replay, for generating a set of simulations weighted by their likelihood given past observations. We validate our algorithm on a DBN containing thousands of variables, which models the spread of wildfire.

#index 283144
#* Toward a theoretical understanding of why and when decision tree pruning algorithms fail
#@ Tim Oates;David Jensen
#t 1999
#c 10
#% 17442
#% 42994
#% 61792
#% 136350
#% 465753
#! Recent empirical studies revealed two surprising pathologies of several common decision tree pruning algorithms. First, tree size is often a linear function of training set size, even when additional tree structure yields no increase in accuracy. Second, building trees with data in which the class label and the attributes are independent often results in large trees. In both cases, the pruning algorithms fail to control tree growth as one would expect them to. We explore thiS behavior theoretically by constructing a statistical model of reduced error pruning. The model explains why and when the pathologies occur, and makes predictions about how to lessen their effects. The predictions are operationalized in a variant of reduced error pruning that is shown to control tree growth far better than the original algorithm.

#index 283145
#* Feature selection for ensembles
#@ David W. Opitz
#t 1999
#c 10
#% 114994
#% 167633
#% 207535
#% 209021
#% 243728
#% 424997
#% 443616
#% 465746
#% 1272309
#% 1478814
#% 1499573
#! The traditional motivation behind feature selection algorithms is to find the best subset of features for a task using one particular learning algonthm. Given the recent success of ensembles, however, we investigate the notion of ensemble feature selection in this paper. This task is harder than traditional feature selection in that one not only needs to find features germane to the learning task and learning algorithm, but one also needs to find a set of feature subsets that will promote disagreement among the ensemble's classifiers. In this paper, we present an ensemble feature selection approach that is based on genetic algorithms. Our algorithm shows improved performance over the popular and powerful ensemble approaches of AdaBoost and Bagging and demonstrates the utility of ensemble feature selection.

#index 283146
#* Efficient exploration for optimizing immediate reward
#@ Dale Schuurmans;Lloyd Greenwald
#t 1999
#c 10
#% 61800
#% 66937
#% 78926
#% 90044
#% 92063
#% 96686
#% 101449
#% 101913
#% 124687
#% 124691
#% 170386
#% 186986
#% 203337
#% 266249
#% 361100
#% 384911
#% 466075
#% 1081020
#% 1272286
#% 1273584
#! We consider the problem of learning an effective behavior strategy from reward. Although much studied, the issue of how to use prior knowledge to scale optimal behavior learning up to real-world problems remains an important open issue.We investigate the inherent data-complexity of behavior-learning when the goal is simply to optimize immediate reward. Although easier than reinforcement learning, where one must also cope with state dynamics, immediate reward learning is still a common problem and is fundamentally harder than supervised learning.For optimizing immediate reward, prior knowledge can be expressed either as a bias on the space of possible reward models, or a bias on the space of possible controllers. We investigate the two paradigmatic learning approaches of indirect (reward-model) learning and direct-control learning, and show that neither uniformly dominates the other in general. Model-based learning has the advantage of generalizing reward experiences across states and actions, but direct-control learning has the advantage of focusing only on potentially optimal actions and avoiding learning irrelevant world details. Both strategies can be strongly advantageous in different circumstances. We introduce hybrid learning strategies that combine the benefits of both approaches, and uniformly improve their learning efficiency.

#index 283147
#* Towards diagram processing: a diagrammatic information system
#@ Michael Anderson
#t 1999
#c 10
#% 55882
#% 426494
#% 837641
#% 1275340
#% 1476270
#! We advocate the development of an agent capable of processing diagrammatic information directly in all its forms. In the same way that we will require intelligent agents to be conversant with natural language, we will expect them to be fluent with diagrammatic information and its processing. We present a methodology to this end, detail a diagrammatic information system that shows the merit of this line of research, and evaluate this system to motivate its future extensions.

#index 283151
#* Influence-based model decomposition
#@ Christopher Bailey-Kellogg;Feng Zhao
#t 1999
#c 10
#% 114994
#% 265806
#% 266105
#% 592143
#% 592316
#% 669617
#% 1478748
#! Recent rapid advances in MEMS and information processing technology have enabled a new generation of AI robotic systems -- so-called Smart Matter systems - that are sensor rich and physically embedded. These systems range from decentralized control systems that regulate building temperature (smart buildings) to vehicle on-board diagnostic and control systems that interrogate large amounts of sensor data. One of the core tasks in the construction and operation of these Smart Matter systems is to synthesize optimal control policies using data rich models for the systems and environment. Unfortunately, these models may contain thousands of coupled real-valued variables and are prohibitively expensive to reason about using traditional optimization techniques such as neural nets and genetic algorithms. This paper introduces a general mechanism for automatically decomposing a large model into smaller subparts so that these subparts can be separately optimized and then combined. The mechanism decomposes a model using an influence graph that records the coupling strengths among constituents of the model. This paper demonstrates the mechanism in an application of decentralized optimization for a temperature regulation problem. Performance data has shown that the approach is much more efficient than the standard discrete optimization algorithms and achieves comparable accuracy.

#index 283154
#* Model-based support for mutable parametric design optimization
#@ Ravi Kapadia;Gautam Biswas
#t 1999
#c 10
#% 92692
#% 858823
#% 1273488
#% 1784168
#! Traditional methods for parametric design optimization assume that the relations between performance criteria and design variables are known algebraic functions with fixed coefficients. However, the relations may be mutable, i.e., the functions and/or coefficients may not be known explicitly because they depend on input parameters and vary in different parts of the design space. We present a model-based reasoning methodology to support parametric, mutable, design optimization. First, we derive event models to represent the effects of the system's parameters on the material that flows through it. Next, we use these models to discover mutable relations between the system's design variables and its optimization criteria. We then present an algorithm that searches for "optimal" designs by employing sensitivity analysis techniques on the derived relations.

#index 283157
#* Qualifying the expressivity/efficiency tradeoff: reformation-based diagnosis
#@ Helmut Prendinger;Mitsuru Ishizuka
#t 1999
#c 10
#% 110365
#% 160384
#% 181220
#% 229098
#% 243720
#% 515368
#% 520872
#% 1476265
#% 1476298
#% 1478837
#! This paper presents an approach to model-based diagnosis that first compiles a first-order system description to a propositional representation, and then solves the diagnostic problem as a linear programming instance. Relevance reasoning is employed to isolate parts of the system that are related to certain observation types and to economically instantiate the theory, while methods from operations research offer promising results to generate near-optimal diagnoses efficiently.

#index 283161
#* The role of lexicalization and pruning for base noun phrase grammars
#@ Claire Cardie;David Pierce
#t 1999
#c 10
#% 5182
#% 196896
#% 740916
#% 742162
#% 742424
#% 747888
#% 747912
#% 748703
#% 748722
#% 757091
#% 1476274
#% 1478822
#! This paper explores the role of lexicalization and pruning of grammars for base noun phrase identification. We modify our original framework (Cardie & Pierce 1998) to extract lexicalized treebank grammars that assign a score to each potential noun phrase based upon both the part-of-speech tag sequence and the word sequence of the phrase. We evaluate the modified framework on the "simple" and "complex" base NP corpora of the original study. As expected, we find that lexicalization dramatically improves the performance of the unpruned treebank grammars; however, for the simple base noun phrase data set, the lexicalized grammar performs below the corresponding unlexicalized but pruned grammar, suggesting that lexicalization is not critical for recognizing very simple, relatively unambiguous constituents. Somewhat surprisingly, we also find that error-driven pruning improves the performance of the probabilistic, lexicalized base noun phrase grammars by up to 1.0% recall and 0.4% precision, and does so even using the original pruning strategy that fails to distinguish the effects of lexicalization. This result may have implications for many probabilistic grammar-based approaches to problems in natural language processing: error-driven pruning is a remarkably robust method for improving the performance of probabilistic and non-probabilistic grammars alike.

#index 283163
#* Two dimensional generalization in information extraction
#@ Joyce Yue Chai;Alan W. Biermann;Curry I. Guinn
#t 1999
#c 10
#% 210985
#% 217064
#% 278109
#% 283300
#% 497956
#% 708565
#% 814996
#% 815010
#% 1478922
#! In a user-trained information extraction system, the cost of creating the rules for information extraction can be greatly reduced by maximizing the effectiveness of user inputs. If the user specifies one example of a desired extraction, our system automatically tries a variety of generalizations of this rule including generalizations of the terms and permutations of the ordering of significant words. Where modifications of the rules are successful, those rules are incorporated into the extraction set. The theory of such generalizations and a measure of their usefulness is described.

#index 283169
#* Combining collaborative filtering with personal agents for better recommendations
#@ Nathaniel Good;J. Ben Schafer;Joseph A. Konstan;Al Borchers;Badrul Sarwar;Jon Herlocker;John Riedl
#t 1999
#c 10
#% 124004
#% 124010
#% 173879
#% 192122
#% 202009
#% 202011
#% 220707
#% 220709
#% 220711
#% 232708
#% 252755
#% 260778
#% 266281
#% 648320
#% 1650569
#! Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile. Information filtering (IF) focuses on the analysis of item content and the development of a personal user interest profile. Collaborative filtering (CF) focuses on identification of other users with similar tastes and the use of their opinions to recommend items. Each technique has advantages and limitations that suggest that the two could be beneficially combined.This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better recommendations than either agents or users can produce alone. It also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or other combination mechanisms. One key implication of these results is that users can avoid having to select among agents; they can use them all and let the CF framework select the best ones for them.

#index 283170
#* Application-embedded retrieval from distributed free-text collections
#@ Vladimir A. Kulyukin
#t 1999
#c 10
#% 118728
#% 198058
#% 199642
#% 232645
#% 235292
#% 246769
#% 253188
#% 266219
#% 406493
#% 706774
#% 742365
#% 1499534
#! A framework is presented for application-embedded information retrieval from distributed free-text collections. An application's usage is sampled by an embedded information retrieval system. Samples are converted into queries to distributed collections. Retrieval is adjusted through sample size and structure, anydata indexing, and dual space feedback. The framework is investigated with a retrieval system embedded in a text processor.

#index 283171
#* Towards multidocument summarization by reformulation: progress and prospects
#@ Kathleen R. McKeown;Judith L. Klavans;Vasileios Hatzivassiloglou;Regina Barzilay;Eleazar Eskin
#t 1999
#c 10
#% 46803
#% 71752
#% 120109
#% 175130
#% 194251
#% 199036
#% 262042
#% 741106
#% 742437
#% 747990
#% 748722
#% 786578
#% 1273057
#% 1478826
#% 1499571
#! By synthesizing information common to retrieved documents, multi-document summarization can help users of information retrieval systems to find relevant documents with a minimal amount of reading. We are developing a multidocument summarization system to automatically generate a concise summary by identifying and synthesizing similarities across a set of related documents. Our approach is unique in its integration of machine learning and statistical techniques to identify similar paragraphs, intersection of similar phrases within paragraphs, and language generation to reformulate the wording of the summary. Our evaluation of system components shows that learning over multiple extracted linguistic features is more effective than information retrieval approaches at identifying similar text units for summarization and that it is possible to generate a fluent summary that conveys similarities among documents even when full semantic interpretations of the input text are not available.

#index 283174
#* An automatic method for generating sense tagged corpora
#@ Rada Mihalcea;Dan I. Moldovan
#t 1999
#c 10
#% 198058
#% 741085
#% 742368
#% 748550
#% 748601
#% 748703
#% 756952
#% 817846
#% 817955
#% 818059
#! The unavailability of very large corpora with semantically disambiguated words is a major limitation in text processing research. For example, statistical methods for word sense disambiguation of free text are known to achieve high accuracy results when large corpora are available to develop context rules, to train and test them.This paper presents a novel approach to automatically generate arbitrarily large corpora for word senses. The method is based on (1) the information provided in WordNet, used to formulate queries consisting of synonyms or definitions of word senses, and (2) the information gathered from Internet using existing search engines. The method was tested on 120 word senses and a precision of 91% was observed.

#index 283177
#* Selecting text spans for document summaries: heuristics and metrics
#@ Vibhu Mittal;Mark Kantrowitz;Jade Goldstein;Jaime Carbonell
#t 1999
#c 10
#% 198294
#% 262112
#% 406493
#% 748583
#! Human-quality text summarization systems are difficult to design, and even more difficult to evaluate, in part because documents can differ along several dimensions, such as length, writing style and lexical usage. Nevertheless, certain cues can often help suggest the selection of sentences for inclusion in a summary. This paper presents an analysis of news-article summaries generated by sentence extraction. Sentences are ranked for potential inclusion in the summary using a weighted combination of linguistic features - derived from an analysis of news-wire summaries. This paper evaluates the relative effectiveness of these features. In order to do so, we discuss the construction of a large corpus of extraction-based summaries, and characterize the underlying degree of difficulty of summarization at different compression levels on articles in this corpus. Results on our feature set are presented after normalization by this degree of difficulty.

#index 283180
#* Learning dictionaries for information extraction by multi-level bootstrapping
#@ Ellen Riloff;Rosie Jones
#t 1999
#c 10
#% 217064
#% 266215
#% 278109
#% 496886
#% 707780
#% 747793
#% 747945
#% 1290067
#% 1476276
#! Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories.

#index 283182
#* Feature selection in SVM text categorization
#@ Hirotoshi Taira;Masahiko Haruno
#t 1999
#c 10
#% 46803
#% 115608
#% 136350
#% 169718
#% 190581
#% 197394
#% 260001
#% 278103
#% 458379
#% 465754
#% 592108
#! This paper investigates the effect of prior feature selection in Support Vector Machine (SVM) text categorization. The input space was gradually increased by using mutual information (MI) filtering and part-of-speech (POS) filtering, which determine the portion of words that are appropriate for learning from the information-theoretic and the linguistic perspectives, respectively. We tested the two filtering methods on SVMs as well as a decision tree algorithm C4.5. The SVMs' results common to both filtering are that 1) the optimal number of features differed completely across categories, and 2) the average performance for all categories was best when all of the words were used. In addition, a comparison of the two filtering methods clarified that POS filtering on SVMs consistently outperformed MI filtering, which indicates that SVMs cannot find irrelevant parts of speech. These results suggest a simple strategy for the SVM text categorization: use a full number of words found through a rough filtering technique like part-of-speech tagging.

#index 283183
#* Automatic construction of semantic lexicons for learning natural language interfaces
#@ Cynthia A. Thompson;Raymond J. Mooney
#t 1999
#c 10
#% 99650
#% 170649
#% 174527
#% 179873
#% 375369
#% 396021
#% 408396
#% 451052
#% 709112
#% 748347
#% 748473
#% 748739
#% 756751
#% 1476277
#% 1499532
#! This paper describes a system, WOLFIE (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with semantic representations. The lexicon learned consists of words paired with meaning representations. WOLFIE is part of an integrated system that learns to parse novel sentences into semantic representations, such as logical database queries. Experimental results are presented demonstrating WOLFIE's ability to learn useful lexicons for a database interface in four different natural languages. The lexicons learned by WOLFIE are compared to those acquired by a similar system developed by Siskind (1996).

#index 283192
#* Theory for coordinating concurrent hierarchical planning agents using summary information
#@ Bradley J. Clement;Edmund H. Durfee
#t 1999
#c 10
#% 160202
#% 178934
#% 188086
#% 266390
#% 271079
#% 283192
#% 319244
#% 695783
#% 1272550
#% 1273080
#% 1274402
#! Interacting agents that interleave planning, plan coordination, and plan execution for hierarchical plans (e.g. HTNs or procedures for PRS) should reason about abstract plans and their concurrent execution before they are fully refined. Poor decisions made at abstract levels can lead to costly backtracking or even failure. We claim that better decisions require information at abstract levels that summarizes the preconditions and effects that must or may apply when a plan is refined. Here we formally characterize concurrent hierarchical plans and a method for deriving summary information for them, and we illustrate how summary conditions can be used to coordinate the concurrent interactions of plans at different levels of abstraction. The properties of summary conditions and rules determining what interactions can or might hold among asynchronously executing plans are proven to support the construction of sound and complete coordination mechanisms for concurrent hierarchical planning agents.

#index 283195
#* Fast planning through greedy action graphs
#@ Alfonso Gerevini;Ivan Serina
#t 1999
#c 10
#% 126390
#% 143592
#% 149628
#% 160270
#% 544766
#% 678356
#% 1290109
#% 1476298
#% 1478839
#! Domain-independent planning is a notoriously hard search problem. Several systematic search techniques have been proposed in the context of various formalisms. However, despite their theoretical completeness, in practice these algorithms are incomplete because for many problems the search space is too large to be (even partially) explored.In this paper we propose a new search method in the context of Blum and Furst's planning graph approach, which is based on local search. Local search techniques are incomplete, but in practice they can efficiently solve problems that are unsolvable for current systematic search methods. We introduce three heuristics to guide the local search (Walkplan, Tabuplan and T-Walkplan), and we propose two methods for combining local and systematic search.Our techniques are implemented in a system called GPG, which can be used for both plan-generation and plan-adaptation tasks. Experimental results show that GPG can efficiently solve problems that are very hard for current planners based on planning graphs.

#index 283196
#* Control knowledge in planning: benefits and tradeoffs
#@ Yi-Cheng Huang;Bart Selman;Henry Kautz
#t 1999
#c 10
#% 131357
#% 145388
#% 154075
#% 172505
#% 215878
#% 266388
#% 544766
#% 1271884
#% 1273680
#% 1273776
#% 1290109
#% 1476298
#! Recent new planning paradigms, such as Graphplan and Satplan, have been shown to outperform more traditional domain-independent planners. An interesting aspect of these planners is that they do not incorporate domain specific control knowledge, but instead rely on efficient graph-based or propositional representations and advanced search techniques. An alternative approach has been proposed in the TLPlan system. TLPlan is an example of a powerful planner incorporating declarative control specified in temporal logic formulas. We show how these control rules can be parsed into Satplan. Our empirical results show up to an order of magnitude speed up. We also provide a detailed comparison with TLPlan, and show how the search strategies in TLPlan lead to efficient plans in terms of the number of actions but with little or no parallelism. The Satplan and Graphplan formalisms on the other hand do find highly parallel plans, but are less effective in sequential domains. Our results enhance our understanding of the various tradeoffs in planning technology, and extend earlier work on control knowledge in the Satplan framework by Ernst et al. (1997) and Kautz and Selman (1998).

#index 283197
#* A framework for recognizing multi-agent action from visual evidence
#@ Stephen S. Intille;Aaron F. Bobick
#t 1999
#c 10
#% 24005
#% 44876
#% 147680
#% 179925
#% 215532
#% 266397
#% 319244
#% 625138
#% 704328
#% 718343
#% 1290139
#% 1476310
#% 1499477
#% 1650681
#! A probabilistic framework for representing and visually recognizing complex multi-agent action is presented. Motivated by work in model-based object recognition and designed for the recognition of action from visual evidence, the representation has three components: (1) temporal structure descriptions representing the temporal relationships between agent goals, (2) belief networks for probabilistic ally representing and recognizing individual agent goals from visual evidence, and (3) belief networks automatically generated from the temporal structure descriptions that support the recognition of the complex action. We describe our current work on recognizing American football plays from noisy trajectory data.

#index 283198
#* State-space planning by integer optimization
#@ Henry Kautz;Joachim P. Walser
#t 1999
#c 10
#% 40368
#% 154075
#% 160270
#% 179938
#% 299169
#% 544766
#% 1271884
#% 1273774
#% 1279705
#% 1290109
#% 1290110
#% 1476298
#% 1478771
#% 1478840
#! This paper describes ILP-PLAN, a framework for solving AI planning problems represented as integer linear programs. ILP-PLAN extends the planning as satisfiability framework to handle plans with resources, action costs, and complex objective functions. We show that challenging planning problems can be effectively solved using both traditional branch and-bound IP solvers and efficient new integer local search algorithms. ILP-PLAN can find better quality solutions for a set of hard benchmark logistics planning problems than had been found by any earlier system.

#index 283207
#* Using planning graphs for solving HTN planning problems
#@ Amnon Lotem;Dana S. Nau;James A. Hendler
#t 1999
#c 10
#% 3358
#% 179879
#% 179965
#% 194651
#% 224480
#% 266387
#% 266680
#% 544766
#% 544791
#% 1273868
#% 1274156
#% 1290109
#% 1476298
#! In this paper we present the GraphHTN algorithm, a hybrid planning algorithm that does Hierarchical. Task-Network (HTN) planning using a combination of HTN-style problem reduction and Graphplan-style planning-graph generation. We also present experimental results comparing GraphHTN with ordinary HTN decomposition (as implemented in the UMCP planner) and ordinary Graphplan search (as implemented in the IPP planner). Our experimental results show that (1) the performance of HTN planning can be improved significantly by using planning graphs, and (2) that planning with planning graphs can be sped up by exploiting HTN control knowledge.

#index 283210
#* On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems
#@ Omid Madani;Steve Hanks;Anne Condon
#t 1999
#c 10
#% 17872
#% 30037
#% 102136
#% 167629
#% 194652
#% 203169
#% 473648
#% 475699
#% 591178
#% 703709
#% 836122
#% 1080992
#% 1272331
#% 1478845
#% 1789944
#! We investigate the computability of problems in probabilistic planning and partially observable infinite-horizon Markov decision processes. The undecidability of the string-existence problem for probabilistic finite automata is adapted to show that the following problem of plan existence in probabilistic planning is undecidable: given a probabilistic planning problem, determine whether there exists a plan with success probability exceeding a desirable threshold. Analogous policy-existence problems for partially observable infinite-horizon Markov decision processes under discounted and undiscounted total reward models, average-reward models, and state-avoidance models are all shown to be undecidable. The results apply to corresponding approximation problems as well.

#index 283215
#* Contingent planning under uncertainty via stochastic satisfiability
#@ Stephen M. Majercik;Michael L. Littman
#t 1999
#c 10
#% 2837
#% 25470
#% 266386
#% 266387
#% 544769
#% 1290265
#% 1291472
#% 1476294
#% 1476298
#% 1478761
#! We describe two new probabilistic planning techniques-- c-MAXPLAN and ZANDER--that generate contingent plans in probabilistic propositional domains. Both operate by transforming the planning problem into a stochastic satisfiability problem and solving that problem instead. C-MAXPLAN encodes the problem as an E-MAJSAT instance, while ZANDER encodes the problem as an S-SAT instance. Although S-SAT problems are in a higher complexity class than E-MAJSAT problems, the problem encodings produced by ZANDER are substantially more compact and appear to be easier to solve than the corresponding E-MAJSAT encodings. Preliminary results for ZANDER indicate that it is competitive with existing planners on a variety of problems.

#index 283216
#* On the utility of plan-space (causal) encodings
#@ Amol D. Mali;Subbarao Kambhampati
#t 1999
#c 10
#% 21145
#% 163715
#% 179932
#% 194651
#% 708101
#% 1271884
#% 1273681
#% 1279703
#% 1476298
#! Recently, casting planning as propositional satisfiability has been shown to be a very promising technique for plan synthesis. Although encodings based both on statespace planning and on plan-space (causal) planning have been proposed, most implementations and trade-off evaluations primarily use state-based encodings. This is surprising given both the prominence of plan-space planners in traditional planning, as well as the recent claim that lifted versions of causal encodings provide the smallest encodings. In this paper we attempt a systematic analytical and empirical comparison of plan-space (causal) encodings and state-space encodings. We start by pointing out the connection between the different ways of proving the correctness of a plan, and the spectrum of possible SAT encodings. We then characterize the dimensions along which causal proofs, and consequently, plan-space encodings, can vary. We provide two encodings that are much smaller than those previously proposed. We then show that the smallest causal encodings cannot be smaller in size than the smallest state-based encodings. We shall show that the "lifting" transformation does not affect this relation. Finally, we will present some empirical results that demonstrate that the relative encoding sizes are indeed correlated with the hardness of solving them. We end with a discussion on when the primacy of traditional plan-space planners over state-space planners might carry over to their respective SAT encodings.

#index 283217
#* Anytime coordination for progressive planning agents
#@ Abdel-Illah Mouaddib
#t 1999
#c 10
#% 160218
#% 189700
#% 240951
#% 1478734
#! We address in this paper the problem of coordinating resource-bounded agents under time constraints in a dynamic environment. The agent society we deal with consists of coordinated agents each of which has a goal to achieve before a deadline and new agents can asynchronously appear to achieve time-constrained goals and to coordinate their plans with the already coordinated agent society. Agents use progressive planning that adapt the detail of their local plans according to local deadlines and available resources. The plan consists of a hierarchy of partial plans where each partial plan satisfies a part of the goal. In such environments, constructing a complete plan and then coordinating it with other agents doesn't guarantee that the planning and coordination operations will finish before the given deadline. What we propose is an anytime coordination that allows an agent to return a coordinated plan at any time by using series of partial planning followed by a coordination until the complete plan is constructed and coordinated or the deadline is met. This progressive plan merging operation is assessed in a resource allocation problem.

#index 283218
#* Generating qualitatively different plans through metatheoretic biases
#@ Karen L. Myers;Thomas J. Lee
#t 1999
#c 10
#% 44836
#% 178934
#! Current methods for generating qualitatively different plans are either based on simple randomization of planning decisions and so cannot guarantee meaningful differences among generated plans, or require extensive user involvement to drive the system into different sections of the overall plan space. This paper presents a cost-effective method for automatically generating qualitatively different plans that is rooted in the creation of biases that focus the planner toward solutions with certain attributes. Biases are derived from analysis of a domain metatheory and enforced through compilation into preferences over planning decisions. Users can optionally direct the planner into desired regions of the plan space by designating aspects of the metatheory that should be used for bias generation. Experimental results are provided that validate the effectiveness of the biasing method for reliably generating a range of plans with meaningful semantic differences.

#index 283219
#* Conditional, probabilistic planning: a unifying algorithm and effective search control mechanisms
#@ Nilufer Onder;Martha E. Pollack
#t 1999
#c 10
#% 124601
#% 179961
#% 194652
#% 224762
#% 266387
#% 544769
#% 1272287
#% 1478833
#% 1650653
#! Several recent papers describe algorithms for generating conditional and/or probabilistic plans. In this paper, we synthesize this work, and present a unifying algorithm that incorporates and clarifies the main techniques that have been developed in the previous literature. Our algorithm decouples the search-control strategy for conditional and/or probabilistic planning from the underlying plan-refinement process. A similar decoupling has proven to be very useful in the analysis of classical planning algorithms, and we show that it can be at least as useful here, where the search-control decisions are even more crucial. Previous probabilistic/ conditional planners have been severely limited by the fact that they do not know how to handle failure points to advantage. We show how a principled selection of failure points can be performed within the framework our algorithm. We also describe and show the effectiveness of additional heuristics. We describe our implemented system called Mahinur and experimentally demonstrate that our methods produce efficiency improvements of several orders of magnitude.

#index 283220
#* CPlan: a constraint programming approach to planning
#@ Peter van Beek;Xinguang Chen
#t 1999
#c 10
#% 21145
#% 23012
#% 100159
#% 163715
#% 224480
#% 544766
#% 1279703
#% 1290108
#% 1290109
#% 1478840
#! Constraint programming, a methodology for solving difficult combinatorial problems by representing them as constraint satisfaction problems, has shown that a general purpose search algorithm based on constraint propagation combined with an emphasis on modeling can solve large, practical scheduling problems. Given the success of constraint programming on scheduling problems and the similarity of scheduling to planning, the question arises, would a constraint programming approach work as well in planning? In this paper, we present evidence that a constraint programming approach to planning does indeed work well and has the advantage in terms of time and space efficiency over the current state-of-the-art planners.

#index 283221
#* Total order planning is more efficient than we thought
#@ Vincent Vidal;Pierre Régnier
#t 1999
#c 10
#% 21145
#% 23012
#% 100159
#% 163715
#% 224480
#% 544766
#% 1279703
#% 1290108
#% 1290109
#% 1478840
#! In this paper, we present VVPLAN, a planner based on a classical state space search algorithm. The language used for domain and problem representation is ADL (Pednault 1989). We have compared VVPLAN to UCPOP (Penberthy and Weld 1992)(Weld 1994), a planner that admits the same representation language. Our experiments prove that such an algorithm is often more efficient than a planner based on a search in the space of partial plans. This result is achieved as soon as we introduce in VVPLAN's algorithm a loop test relating to previously visited states. In particular domains, VVPLAN can also outperform IPP (Koehler et al. 1997), which makes a planning graph analysis as GRAPHPLAN. We present here the details of our comparison with UCPOP, the results we obtain and our conclusions.

#index 283222
#* Cooperative plan identification: constructing concise and effective plan descriptions
#@ R. Michael Young
#t 1999
#c 10
#% 76415
#% 184068
#% 194651
#% 419838
#% 705939
#% 741056
#% 1272550
#% 1476281
#! Intelligent agents are often called upon to form plans that direct their own or other agents' activities. For these systems, the ability to describe plans to people in natural ways is an essential aspect of their interface. In this paper, we present the Cooperative Plan Identification (CPI) architecture, a computational model that generates concise, effective textual descriptions of plan data structures. The model incorporates previous theoretical work on the comprehension of plan descriptions, using a generate-and-test approach to perform efficient search through the space of candidate descriptions.We describe an empirical evaluation of the CPI architecture in which subjects following instructions produced by the CPI architecture performed their tasks with fewer execution errors and achieved a higher percentage of their tasks' goals than did subjects following instructions produced by alternative methods.

#index 283223
#* Exploiting symmetry in the planning graph via explanation-guided search
#@ Terry Zimmermann;Subbarao Kambhampati
#t 1999
#c 10
#% 160248
#% 160251
#% 179766
#% 262237
#% 544784
#% 676283
#% 1272318
#% 1273573
#% 1273870
#% 1290109
#! We present a method for exploiting the symmetry in the planning graph structure and certain redundancies inherent in the Graphplan algonthm, so as to improve its backward search. The main insight underlying our method is that due to these features the backward search conducted at level k + 1 of the graph is essentially a replay of the search conducted at the previous level k with certam well-defined extensions. Our method consists of maintaining a pilot explanation structure capturing the failures encountered at previous levels of the search, and using it in an intelligent way to guide the search at the newer levels. The standard EBL and DDB techniques can be employed to control the size of the pilot explanation. The technique has been implemented in the EGBG system, and we present a preliminary empirical study.

#index 283224
#* An integrated system for multi-rover scientific exploration
#@ Tara Estlin;Alexander Gray;Tobias Mann;Gregg Rabideau;Rebecca Castaño;Steve Chien;Eric Mjolsness
#t 1999
#c 10
#% 168288
#% 361100
#% 451042
#% 1499476
#! This paper describes an integrated system for coordinating multiple rover behavior with the overall goal of collecting planetary surface data. The MultiRover Integrated Science Understanding System combines concepts from machine learning with planning and scheduling to perform autonomous scientific exploration by cooperating rovers. The integrated system utilizes a novel machine learning clustering component to analyze science data and direct new science activities. A planning and scheduling system is employed to generate rover plans for achieving science goals and to coordinate activities among rovers. We describe each of these components and discuss some of the key integration issues that arose during development and influenced both system design and performance.

#index 283225
#* Integrated natural spoken dialogue system of Jijo-2 mobile robot for office services
#@ Toshihiro Matsui;Hideki Asoh;John Fry;Youichi Motomura;Futoshi Asano;Takio Kurita;Isao Hara;Nobuyuki Otsu
#t 1999
#c 10
#% 179705
#% 190444
#% 375503
#% 437830
#% 530715
#% 740955
#% 1274106
#% 1275237
#! Our Jijo-2 robot, whose purpose is to provide office services, such as answering queries about people's location, route guidance, and delivery tasks, is expected to conduct natural spoken conversation with the office dwellers. This paper describes dialogue technologies implemented on our Jijo-2 office robot, i.e. noise-free voice acquisition system by a microphone array, inference of under-specified referents and zero pronouns using the attentional states, and context-sensitive construction of semantic frames from fragmented utterances. The behavior of the dialogue system integrated with the sound source detection, navigation, and face recognition vision is demonstrated in real dialogue examples in a real office.

#index 283226
#* Gesture-based interaction with a pet robot
#@ Milyn C. Moy
#t 1999
#c 10
#% 3169
#% 109079
#% 142990
#% 248145
#% 405315
#% 457806
#% 536553
#% 592284
#% 593461
#% 593473
#% 593498
#% 1476257
#! Pet robots are autonomous robots capable of exhibiting animal-like behaviors, including emotional ones, as they interact with people and objects surrounding them. As pet robots become more integrated into our lives, a more natural way of communicating with them will become necessary. Similarly, they will need to understand human gestures in order to perceive our intentions and communicate with us more effectively. In this paper, we present an extensible, real-time, visionbased communication system that interprets 2D dynamic hand gestures in complex environments. Our strategy for interpreting hand gestures consists of: hand segmentation, feature extraction, and gesture recognition. To segment the hand from the cluttered background, this system uses both motion and color information. The location of the hand is subsequently tracked as the user makes the gesture and its trajectory information is stored in a feature vector. Finally, the gesture is interpreted using this vector and translated into a command that the robot understands. We implemented our system on Yuppy, a pet robot prototype. Currently, via an external microcamera, we can navigate Yuppy in unstructured environments using hand gestures.

#index 283227
#* Continuous categories for a mobile robot
#@ Michael T. Rosenstein;Paul R. Cohen
#t 1999
#c 10
#% 167323
#% 220998
#% 224475
#% 229084
#% 252812
#% 266285
#% 267314
#% 267321
#% 271756
#% 661026
#% 1280032
#% 1780572
#! Autonomouas gents make frequent use of knowledgei n the form of categories -- categories of objects, human gestures, web pages, and so on. This paper describes a way for agents to learn such categories for themselves through interaction with the environment. In particular, the learning algorithm transforms raw sensor readings into clusters of time series that have predictive value to the agent. We address several issues related to the use of an uninterpreted sensory apparatus and show specific examples where a Pioneer I mobile robot interacts with objects in a cluttered laboratory setting.

#index 283228
#* DISTANCE-SAT: complexity and algorithms
#@ Olivier Bailleux;Pierre Marquis
#t 1999
#c 10
#% 21137
#% 41220
#% 210195
#% 266125
#% 327779
#% 408396
#% 503248
#% 1273727
#% 1279714
#! In many AI fields, the problem of finding out a solution which is as close as possible to a given configuration has to be faced. This paper addresses this problem in a propositional framework. The decision problem DISTANCE-SAT that consists in determining whether a propositional CNF formula admits a model that disagrees with a given partial interpretation on at most d variables, is introduced. The complexity of DISTANCE-SAT an d of several restrictions of it are identified. Two algorithms based on the well-known Davis/Putnam search procedurea represented so as to solve DISTANCE-SAT. Their empirical evaluation enables deriving firm conclusionsa bout their respective performances and to relate the difficulty of DISTANCE-SAT with the difficulty of SAT from the practical side.

#index 283229
#* Beyond NP: the QSAT phase transition
#@ Ian P. Gent;Toby Walsh
#t 1999
#c 10
#% 175378
#% 233698
#% 266109
#% 558706
#% 1273704
#% 1275311
#% 1499502
#! We show that phase transition behavior similar to that observed in NP-complete problems like random 3-SAT occurs further up the polynomial hierarchy in problems like random 2-QSAT. The differences between QSAT and SAT in phase transition behavior that Cadoli et al report are largely due to the presence of trivially unsatisfiable problems. Once they are removed, we see behavior more familiar from SAT and other NP-complete domains. There are, however, some differences. Problems with short clauses show a large gap between worst case behavior and median, and the easy-hard-easy pattern is restricted to higher percentiles of search cost. We compute the "constrainedness" of k-QSAT problems for any k, and use this to predict the location of phase transitions. We conjecture that these predictions are less accurate than in NP-complete problems because of the super-exponential size of the state space, and of the weakness of first moment methods in complexity classes above NP. Finally, we predict that similar phase transition behavior will occur in other PSPACEcomplete problems like planning and game playing.

#index 283230
#* Morphing: combining structure and randomness
#@ Ian P. Gent;Holger H. Hoos;Patrick Prosser;Toby Walsh
#t 1999
#c 10
#% 266200
#% 1478764
#! We introduce a mechanism called "morphing" for introducing structure or randomness into a wide variety of problems. We illustrate the usefulness of morphing by performing several different experimental studies. These studies identify the impact of a "small-world" topology on the cost of coloring graphs, of asymmetry on the cost of finding the optimal TSP tour, and of the dimensionality of space on the cost of finding the optimal TSP tour. We predict that morphing will find many other uses.

#index 283231
#* On the run-time behaviour of stochastic local search algorithms for SAT
#@ Holger H. Hoos
#t 1999
#c 10
#% 109572
#% 116559
#% 160270
#% 1268732
#% 1476298
#% 1478779
#% 1499519
#% 1650591
#! Stochastic local search (SLS) algorithms for the propositional satisfiability problem (SAT) have been successfully applied to solve suitably encoded search problems from various domains. One drawback of these algorithms is that they are usually incomplete. We refine the notion of incompleteness for stochastic decision algorithms by introducing the notion of "probabilistic asymptotic completeness" (PAC) and prove for a number of well-known SLS algorithms whether or not they have this property. We also give evidence for the practical impact of the PAC property and show how to achieve the PAC property and significantly improved performance in practice for some of the most powerful SLS algorithms for SAT, using a simple and general technique called "random walk extension".

#index 283232
#* Initial experiments in stochastic satisfiability
#@ Michael L. Littman
#t 1999
#c 10
#% 2837
#% 30037
#% 205391
#% 266109
#% 283215
#% 327779
#% 656686
#% 1272331
#% 1476298
#% 1650778
#! This paper looks at the rich intersection between satisfiability problems and probabilistic models, opening the door for the use of satisfiability approaches in probabilistic domains. A generic stochastic satisfiability problem is examined, which can function for probabilistic domains as SAT does for deterministic domains. The paper defines a Davis-Putnam-Logemann-Loveland-style procedure for solving stochastic satisfiability problems, and reports on a preliminary empirical exploration of the complexity of the algorithm for a collection of randomly generated probabilistic problems. The results exhibit the familiar easyhardest-hard pattern for the difficulty of random SAT formulae. Special cases of the stochastic satisfiability problem lie in different complexity classes, and one counterintuitive result is that the computational complexity and the empirical complexity of the problems examined do not track each other exactly-problems in the hardest complexity class are not the hardest to solve.

#index 283233
#* Trap escaping strategies in discrete Lagrangian methods for solving hard satisfiability and maximum satisfiability problems
#@ Zhe Wu;Benjamin W. Wah
#t 1999
#c 10
#% 160270
#% 422129
#! In this paper, we present efficient trap-escaping strategies in a search based on discrete Lagrange multipliers to solve difficult SAT problems. Although a basic discrete Lagrangian method (DLM) can solve most of the satisfiable DIMACS SAT benchmarks efficiently, a few of the large benchmarks have eluded solutions by any local-search methods today. These difficult benchmarks generally have many traps that attract localsearch trajectories. To this end, we identify the existence of traps when any change to a variable will cause the resulting Lagrangian value to increase. Using the hanoi4 and par16-1 benchmarks, we illustrate that some unsatisfied clauses are trapped more often than others. Since it is too difficult to remember explicitly all the traps encountered, we propose to remember these traps implicitly by giving larger increases to Lagrange multipliers of unsatisfied clauses that are trapped more often. We illustrate the merit of this new update strategy by solving some of most difficult but satisfiable SAT benchmarks in the DIMACS archive (hanoi4, hanoi4-simple, par16-1 to par16-5, f2000, and par32-1-c to par32-3-c). Finally, we apply the same algorithm to improve on the solutions of some benchmark MAX-SAT problems that we solved before.

#index 283234
#* Scheduling alternative activities
#@ J. Christopher Beck;Mark S. Fox
#t 1999
#c 10
#% 188076
#% 690812
#% 709976
#% 1478766
#% 1478767
#! In realistic scheduling problems, there may be choices among resources or among process plans. We formulate a constraint-based representation of alternative activities to model problems containing such choices. We extend existing constraint-directed scheduling heuristic commitment techniques and propagators to reason directly about the fact that an activity does not necessarily have to exist in a final schedule. Experimental results show that an algorithm using a novel texture-based heuristic commitment technique together with extended edge-finding propagators achieves the best overall performance of the techniques tested.

#index 283235
#* Algorithm performance and problem structure for flow-shop scheduling
#@ Jean-Paul Watson;Laura Barbulescu;Adele E. Howe;L. Darrell Whitley
#t 1999
#c 10
#% 408396
#% 465874
#% 1022870
#% 1271915
#% 1275306
#% 1279714
#% 1499506
#! Test suites for many domains often fail to model features present in real-world problems. For the permutation flow-shop sequencing problem (PFSP), the most popular test suite consists of problems whose features are generated from a single uniform random distribution. Synthetic generation of problems with characteristics present in real-world problems is a viable alternative. We compare the performance of several competitive algorithms on problems produced with such a generator. We find that, as more realistic characteristics are introduced, the performance of a state-of-the-art algorithm degrades rapidly: faster and less complex stochastic algorithms provide superior performance. Our empirical results show that small changes in problem structure or problem size can influence algorithm performance. We hypothesize that these performance differences may be partially due to differences in search space topologies; we show that structured problems produce topologies with performance plateaus. Algorithm sensitivity to problem charaeteristics suggests the need to construct test suites more representative of real-world applications.

#index 283236
#* Using probabilistic knowledge and simulation to play poker
#@ Darse Billings;Lourdes Peña;Jonathan Schaeffer;Duane Szafron
#t 1999
#c 10
#% 68273
#% 183499
#% 233137
#% 266213
#% 527664
#% 527691
#% 1273813
#! Until recently, artificial intelligence researchers who use games as their experimental testbed have concentrated on games of perfect information. Many of these games have been amenable to brute-force search techniques. In contrast, games of imperfect information, such as bridge and poker, contain hidden information making similar search techniques impractical. This paper describes recent progress in developing a high-performance pokerplaying program. The advances come in two forms. First, we introduce a new betting strategy that returns a probabilistic betting decision, a probability triple, that gives the likelihood of a fold, call or raise occurring in a given situation. This component unifies all the expert knowledge used in the program, does a better job of representing the type of decision making needed to play strong poker, and improves the way information is propagated throughout the program. Second, real-time simulations are used to compute the expected values of betting decisions. The program generates an instance of the missing data, subject to any constraints that have been learned, and then simulates the rest of the game to determine a numerical result. By repeating this a sufficient number of times, a statistically meaningful sample is used in the program's decision-making process. Experimental results show that these enhancements each represent major advances in the strength of computer poker programs.

#index 283237
#* A space-time tradeoff for memory-based heuristics
#@ Robert C. Holte;István T. Hernádvölgyi
#t 1999
#c 10
#% 2194
#% 130194
#% 160388
#% 266116
#% 451046
#% 533951
#% 1275257
#% 1478838
#% 1499544
#! A memory-based heuristic is a function, h(s), stored in the form of a lookup table (pattern database): h(s) is computed by mapping s to an index and then retrieving the appropriate entry in the table. (Korf 1997) conjectures for search using memory-based heuristics that m 驴 t is a constant, where m is the size of the heuristic's lookup table and t is search time. In this paper we present a method for automatically generating memorybased heuristics and use this to test Korf's conjecture in a large-scale experiment. Our results confirm that there is a direct relationship between m and t.

#index 283238
#* Proverb: the probabilistic cruciverbalist
#@ Greg A. Keim;Noam M. Shazeer;Michael L. Littman;Sushant Agarwal;Catherine M. Cheves;Joseph Fitzgerald;Jason Grosland;Fan Jiang;Shannon Pollard;Karl Weinmeister
#t 1999
#c 10
#% 283091
#% 283320
#% 406493
#! We attacked the problem of solving crossword puzzles by computer: given a set of clues and a crossword grid, try to maximize the number of words correctly filled in. In our system, "expert modules" specialize in solving specific types of clues, drawing on ideas from information retrieval, database search, and machine learning. Each expert module generates a (possibly empty) candidate list for each clue, and the lists are merged together and placed into the grid by a centralized solver. We used a probabilistic representation throughout the system as a common interchange language between subsystems and to drive the search for an optimal solution. PROVERB, the complete system, averages 95.3% words correct and 98.1 % letters correct in under 15 minutes per puzzle on a sample of 370 puzzles taken from the New York Times and several other puzzle sources. This corresponds to missing roughly 3 words or 4 letters on a daily 15 脳 15 puzzle, making PROVERB a better-than-average cruciverbalist (crossword solver).

#index 283241
#* Value-update rules for real-time search
#@ Sven Koenig;Boleslaw Szymanski
#t 1999
#c 10
#% 68238
#% 98073
#% 248136
#% 647111
#% 837649
#% 1279695
#% 1290112
#% 1291498
#% 1478840
#% 1499507
#! Real-time search methods have successfully been used to solve a large variety of search problems but their properties are largely unknown. In this paper, we study how existing real-time search methods scale up. We compare two realtime search methods that have been used successfully in the literature and differ only in the update rules of their values: Node Counting, a real-time search method that always moves to the successor state that it has visited the least number of times so far, and Learning Real-Time A*, a similar real-time search method. Both real-time search methods seemed to perform equally well in many standard domains from artificial intelligence. Our formal analysis is therefore surprising. We show that the performance of Node Counting can be exponential in the number of states even in undirected domains. This solves an open problem and shows that the two real-time search methods do not always perform similarly in undirected domains since the performance of Learning RealTime A* is known to be polynomial in the number of states at worst.

#index 283243
#* Transposition table driven work scheduling in distributed search
#@ John W. Romein;Aske Plat;Henri E. Bal;Jonathan Schaeffer
#t 1999
#c 10
#% 2194
#% 57150
#% 94782
#% 103574
#% 125391
#% 144817
#% 192215
#% 202524
#% 439900
#% 443807
#% 465287
#% 465439
#% 522493
#% 543847
#% 707024
#% 1476299
#% 1478816
#% 1478838
#% 1499500

#index 283246
#* A sequential reversible belief revision method based on polynomials
#@ Salem Benferhat;Didier Dubois;Odile Papini
#t 1999
#c 10
#% 109945
#% 224753
#% 503680
#% 780340
#% 1273609
#% 1290097
#! This paper deals with iterated belief change and proposes a drastic revision rule that modifies a plausibility ordering of interpretations in such a way that any world where the input observartion holds is more plausible that any world where it does not. This change rule makes sense in a dynamic context where observations are received, and the newer observations are considered more plausible than older ones. It is shown how to encode an epistemic state using polynomials equipped with the lexicographical ordering. This encoding makes it very easy to implement and iterate the revision rule using simple operations on these polynomials. Moreover, polynomials allow to keep track of the sequence of observations. Lastly, it is shown how to efficiently compute the revision rule at the syntactical level, when the epistemic state is concisely represented by a prioritized belief base. Our revision rule is the most drastic one can think of, in accordance with Darwiche and Pearl's principles, and thus contrasts with the minimal change rule called natural belief revision.

#index 283249
#* Point-based approaches to qualitative temporal reasoning
#@ J. Delgrande;A. Gupta;T. Van Allen
#t 1999
#c 10
#% 70370
#% 82720
#% 126395
#% 184792
#% 319244
#% 836124
#% 1273474
#% 1499523
#! We address the general problem of finding algorithms for efficient, qualitative, point-based temporal reasoning over a set of operations. We consider general reasoners tailored for temporal domains that exhibit a particular structure and introduce such a reasoner based on the series-parallel graph reasoner of Delgrande and Gupta; this reasoner is also an extension of the Time Graph reasoner of Gerevini and Schubert. Test results indicate that for data with underlying structure, our reasoner performs better than other approaches. When there is no underlying structure in the data, our reasoner still performs better for query answering.

#index 283252
#* Querying temporal constraint networks in PTIME
#@ Manolis Koubarakis;Spiros Skiadopoulos
#t 1999
#c 10
#% 663
#% 21136
#% 54198
#% 82720
#% 84513
#% 90639
#% 94459
#% 107137
#% 181229
#% 184792
#% 184796
#% 224742
#% 228800
#% 230142
#% 268788
#% 289329
#% 319244
#% 366807
#% 477214
#% 497488
#% 527793
#% 598376
#% 1476304
#! We start with the assumption that temporal knowledge usually cared by constraint networks can be represented and queried more effectively by using the scheme of indefinite constraint databases proposed by Koubarakis. Although query evaluation in this scheme is in general a hard computational problem, we demonstrate that there are sevral interesting cases where query evalation can be done in PTIME. These tractability results are original and subsume previous results by van Beek, Brusoni, Console and Terenziani.

#index 283256
#* Polarity guided tractable reasoning
#@ Zbigniew Stachniak
#t 1999
#c 10
#% 2565
#% 212669
#% 558137
#% 1478737
#% 1478765
#! Non-clausal refinement of Boolean Constraint Propagation inference procedure for classical logic, called P-BCP, is introduced within a new knowledge representational formalism of polarized formulas. P-BCP is a sound, incomplete, and linear-time inference procedure. It is shown that P-BCP can be adopted for tractable reasoning in a number of non-classical logics (including some modal and finitely-valued logics).

#index 283261
#* Content-based retrieval from medical image databases: a synergy of human interaction, machine learning and computer vision
#@ C. Brodley;A. Kak;C. Shyu;J. Dy;L. Broderick;A. M. Aisen
#t 1999
#c 10
#% 3621
#% 80995
#% 243728
#% 294845
#% 376266
#% 403085
#% 437405
#! Content-based image retrieval (CBIR) refers to the ability to retrieve images on the basis of image content. Given a query image, the goal of a CBIR system is to search the database and return the n most visually similar images to the query image. In'this paper, we describe an approach to CBIR for medical databases that relies on human input, machine learning and computer vision. Specifically, we apply expert-level human interaction for solving that aspect of the problem which cannot yet be automated, we use computer vision for only those aspects of the problem to which it lends itself best - image characterization - and we employ machine learning algorithms to allow the system to be adapted to new clinical domains. We present empirical results for the domain of high resolution computed tomography (HRCT) of the lung. Our results illustrate the efficacy of a human-in-the-loop approach to image characterization and the ability of our approach to adapt the retrieval process to a particular clinical domain through the application of machine learning algorithms.

#index 283262
#* Using vision to improve sound source separation
#@ Yukiko Nakagawa;Hiroshi G. Okuno;Hiroaki Kitano
#t 1999
#c 10
#% 173497
#% 257158
#% 266402
#% 282681
#% 1273678
#% 1476282
#! We present a method of improving sound source separation using vision. The sound source separation is an essential function to accomplish auditory scene understanding by separating stream of sounds generated from multiple sound sources. By separating a stream of sounds, recognition process, such as speech recognition, can simply work on a single stream, not mixed sound of several speakers. The performance is known to be improved by using stereo/binaural microphone and microphone array which provides spatial information for separation. However, these methods still have more than 20 degree of positional ambiguities. In this paper, we further added visual information to provide more specific and accurate position information. As a result, separation capability was drastically improved. In addition, we found that the use of approximate direction information drastically improve object tracking accuracy of a simple vision system, which in turn improves performance of the auditory system. We claim that the integration of vision and auditory inputs improves performance of tasks in each perception, such as sound source separation and object tracking, by bootstrapping.

#index 283264
#* Automated instructor assistant for ship damage control
#@ Vadim V. Bulitko;David C. Wilkins
#t 1999
#c 10
#% 1857
#% 2768
#% 106669
#% 136350
#% 173784
#% 182994
#% 266422
#% 266678
#% 376589
#% 1272570
#! The decision making task of ship damage control includes addressing problems such as fire spread, flooding, smoke, equipment failures, and personnel casualties. It is a challenging and highly stressful domain with a limited provision for real-life training. In response to this need, a multimedia interactive damage control simulator system, called DC-Train 2.0 was recently deployed at a Navy officer training school; it provides officers with an immersive environment for damage control training. This paper describes a component of the DC-Train 2.0 system that provides feedback to the user, called the automated instructor assistant. This assistant is based on a blackboard-based expert system called Minerva-DCA, which is capable of solving damage control scenarios at the "expert" level. Its innovative blackboard architecture facilitates various forms of user assistance, including interactive explanation, advising, and critiquing. In a large exercise involving approximately 500 ship crises scenarios, Minerva-DCA showed a 76% improvement over Navy officers by saving 89 more ships.

#index 283267
#* HKIA SAS: a constraint-based airport stand allocation system developed with software components
#@ Andy Hon Wai Chun;Steve Ho Chuen Chan;Francis Ming Fai Tsang;Dennis Wai Ming Yeung
#t 1999
#c 10
#% 56471
#% 69528
#% 125386
#! SAS is an AI application developed for the Hong Kong International Airport (HKIA) at Chek Lap Kok. SAS uses constraint-programming techniques to assign parking stands to aircraft and schedules tow movements based on a set of business and operational constraints. The system provides planning, real-time operation, and problem solving capabilities. SAS generates a stand allocation plan that finely balances the objectives of the airlines/handling agents, the convenience of passengers, and the operational constraints of the airport. The system ensures a high standard of quality in customer service, airport safety, and utilization of stand resources. This paper also describes our experience in developing an AI system using standard off-the-shelf software components. SAS is an example of how development methodologies used to construct modern AI applications have become fully inline with mainstream practices.

#index 283271
#* Last minute travel application
#@ André Hübner;Mario Lenz;Roman Borch;Michael Posthoff
#t 1999
#c 10
#% 454954
#! In this article, we present a last minute travel application as part of a complete virtual travel agency. Each year a significant amount of tour packages is sold as last minute tours in Germany. It is impossible for a travel agent to keep track of all the offered tour packages. E-Commerce applications may help to present the best possible tour package for a specific customer request. Traditional database driven applications, as used by most of the tour operators, are not sufficient enough to implement a sales process with consultation in the WWW. The last minute travel application presented here uses case-based reasoning to bridge this gap and simulate the sales assistance of a human travel agent. A Case Retrieval Net (CRN), as internal data structure, proofed to be efficient to handle the large amount of data. Important for the acceptance by customers is also the integration into the Virtual Travel Agency and the interconnections to other parts of this system, like background information or the online car rental application.

#index 283282
#* A new basis for spreadsheet computing: Interval Solver for Microsoft Excel
#@ Eero Hyvönen;Stefano De Pascale
#t 1999
#c 10
#% 21144
#% 126388
#% 181030
#% 225336
#% 265783
#% 1273457
#! There is a fundamental mismatch between the computational basis of spreadsheets and our knowledge of the real world. In spreadsheets numerical daa is represented as exact numbers and their mutual relations as functions, whose values (output) are computed from given argument values (input). However, in the real wold data is often inexact and uncertain in many ways and the relationships, i.e., constraints, between input and output are far more complicated. This paper shows that Interval Constraint Solving, an emerging Artificial Intelligence based technology, provides a more versatile and useful foundation for spreadsheets. The new computational basis is 100% downward compatible with the traditional spreadsheet paradigm. The idea has been sessfully integrated with Microsoft Excel as the add-in Interval Solver that seamlessly upgrades the arithmetic core of Excel into interval constraint solving. The product has been downloaded by thousands of end-users in about 70 countries around the world and has been used in various applications on business computing, engineering, education and science. There is an intriguing chance for a major breakthrough of the AI technology on the spreadsheet platform: Tens of millions of Excel users are making important decisions based on spreadsheet calculations.

#index 283285
#* Ramp activity expert system for scheduling and co-ordination at an airport
#@ Geun-Sik Jo;Kang-Hee Lee;Hwi-Yoon Lee;Sang-Ho Hyun
#t 1999
#c 10
#% 48127
#% 68002
#% 160251
#% 444619
#% 452811
#% 1273573
#! In this project, we have developed the Ramp Activity Coordination Expert System (RACES) in order to solve aircraft parking problems. RACES includes a knowledge-based scheduling system which assigns all daily arriving and departing flights to the gates and remote spots with domain specific knowledge and heuristics acquired from human experts. RACES processes complex scheduling problems such as dynamic inter-relations among the characteristics of remote spots/gates and aircraft with various other constraints, for example, customs and ground handling factors at an airport. By user-driven modeling for end users and near optimal knowledge-driven scheduling acquired from human experts, RACES can produce parking schedules for about 400 daily flights in approximately 20 seconds, whereas it normally takes human experts 4 to 5 hours to do the same. Scheduling results in the form of Gantt charts produced by RACES are also accepted by the domain experts. RACES is also designed to deal with the partial adjustment of the schedule when unexpected events occur. After daily scheduling is completed, the messages for aircraft changes and delay messages are reflected and updated into the schedule according to the knowledge of the domain experts. By analyzing the knowledge model of the domain expert, the reactive scheduling steps are effectively represented as the rules, and the scenarios of the Graphic User Interfaces (GUI) are designed. Since the modification of the aircraft dispositions, such as aircraft changes and cancellations of flights, are reflected in the current schedule, the modification should be sent to RACES from the mainframe for the reactive scheduling. The adjustments of the schedule are made semi-automatically by RACES since there are many irregularities in dealing with the partial rescheduling.

#index 283289
#* Using iterative repair to automate planning and scheduling of shuttle payload operations
#@ Gregg Rabideau;Steve Chien;Jason Willis;Tobias Mann
#t 1999
#c 10
#% 55921
#% 168288
#% 669391
#% 669529
#% 1279697
#! This paper describes the DATA-CHASER Automated Planner/Scheduler (DCAPS) system for automated generation and repair of command sequences for the DATA-CHASER shuttle payload. DCAPS uses general Artificial Intelligence (AI) heuristic search techniques, including an iterative repair framework in which the system iteratively resolves conflicts with the state, resource, and temporal constraints of the payload activities. DCAPS was used in the operations of the shuttle payload for the STS-85 shuttle flight in August 1997 and enabled an 80% reduction in mission operations effort and a 40% increase in science return.

#index 283291
#* DLMS: ten years of AI for vehicle assembly process planning
#@ Nestor Rychtyckyj
#t 1999
#c 10
#% 263993
#% 266238
#% 406437
#% 546968
#! Since its presentation at the inaugural 1989 IAAI Conference (O'Brien et al. 1989), Ford's Direct Labor Management System (DLMS) has evolved from a prototype being tested at a single assembly plant to a fully-deployed application that is being utilized at Ford's assembly plants throughout the world. DLMS is Ford's automated solution to managing the automobile manufacturing process system at our vehicle assembly plants. This paper will describe our experiences and the lessons that have been learned in building and adapting an AI system to the rapidly-evolving world of automotive vehicle assembly process planning. We will cover issues such as knowledge base development and maintenance, knowledge representation, porting the system to different platforms and keeping the system viable and upto-date through various organizational and business practice changes. We will also discuss how DLMS has become an integral part of Ford's assembly process planning business.

#index 283292
#* Using intelligent agents in military simulation or “using agents intelligently”
#@ Gil Tidhar;Clint Heinze;Simon Goss;Graeme Murray;Dino Appla;Ian Lloyd
#t 1999
#c 10
#% 252831
#% 557218
#! Modern defence systems include advanced aircraft, ships, radar, weapons, command and control systems, and most importantly human operators. The main objective of modelling and simulation tools is to allow operational analysts to rapidly specify and evaluate existing and proposed systems and procedures for operating these systems. Such tools are required to model all aspects of defence systems including physical systems and human operators and the reasoning processes that they adopt.Agent-oriented technology is a natural candidate for developing a model of reasoning processes performed by human operators. It allows the operational analyst to work at a high level, formulating cognitive processes, while keeping the detailed computer programming hidden. This premise has led to the development of the Operator-Agent. The base model was completed in June 1996. The model is fully operational and is an integral part of the tools used by operational analysts from the Australian Department of Defence. It has been successfully used for operational analysis and evaluation of multi-billion dollar acquisitions.

#index 283293
#* Nurse scheduling using constraint logic programming
#@ Slim Abdennadher;Hans Schlenker
#t 1999
#c 10
#% 126386
#% 144558
#! The nurse scheduling problem consists of assigning working shifts to each nurse on each day of a certain period of time. A typical problem comprises 600 to 800 assignments that have to take into account several requirements such as minimal allocation of a station, legal regulations and wishes of the personnel. This planning is a difficult and time-consuming expert task and is still done manually. INTERDIP1 is an advanced industrial prototype that supports semi-automatic creation of such rosters. Using the artificial intelligence approach, constraint reasoning and constraint programming, INTERDIP creates a roster interactively within some minutes instead of by hand some hours. Additionally, it mostly produces better results. INTERDIP was developed in collaboration with Siemens Nixdorf. It was presented at the Systems'98 Computer exhibition in Munich and several companies have inquired to market our system.

#index 283297
#* The Wasabi Personal Shopper: a case-based recommender system
#@ Robin Burke
#t 1999
#c 10
#% 55921
#% 118771
#% 168251
#% 168280
#% 220711
#% 252834
#% 266102
#% 266215
#% 266283
#% 405727
#% 406493
#% 441058
#% 445152
#% 1273662
#! The Wasabi Personal Shopper (WPS) is a domain-independent database browsing tool designed for on-line information access, particularly for electronic product catalogs. Typically, web-based catalogs rely either on text search or query formulation. WPS introduces an alternative form of access via preference-based navigation. WPS is based on a line of academic research called FindMe systems. These systems were built in a variety of different languages and used custom-built ad-hoc databases. WPS is written in C++, and designed to be a commercial-grade software product, compatible with any SQL-accessible catalog. The paper describes the WPS and discusses some of the development issues involved in re-engineering our AI research system as a general-purpose commercial application.

#index 283300
#* The use of word sense disambiguation in an information extraction system
#@ Joyce Yue Chai;Alan W. Biermann
#t 1999
#c 10
#% 40100
#% 217064
#% 283163
#% 708565
#% 740916
#% 748549
#% 748550
#% 748594
#% 748601
#% 748703
#% 756952
#% 815010
#% 815342
#% 1478922
#! This paper describes a rule-based methodology for word sense disambiguation and an application of the methodology to information extraction using rules generalized with the help of the WordNet system. The methodology creates word sense disambiguation rules based on user trained examples working in the domain of interest. It achieves accuracy rates comparable to the best competing methods and can be easily integrated into higher level applications.

#index 283301
#* Using artificial intelligence planning to generate antenna tracking plans
#@ Forest Fisher;Tara Estlin;Darren Mutz;Steve Chien
#t 1999
#c 10
#% 44836
#% 126390
#% 168288
#! This paper describes the application of Artificial Intelligence planning techniques to the problem of antenna track plan generation for a NASA Deep Space Communications Station. The described system enables an antenna communications station to automatically respond to a set of tracking goals by correctly configuring the appropriate hardware and software to provide the requested communication services. To perform this task, the Automated Scheduling and Planning Environment (ASPEN) has been applied to automatically produce antenna tracking plans that are tailored to support a set of input goals. In this paper, we describe the antenna automation problem, the ASPEN planning and scheduling system, how ASPEN is used to generate antenna track plans, the results of several technology demonstrations, and future work utilizing dynamic planning technology.

#index 283307
#* In-time agent-based vehicle routing with a stochastic improvement heuristic
#@ Robert Kohout;Kutluhan Erol
#t 1999
#c 10
#% 35388
#! Vehicle routing problems (VRP's) involve assigning a fleet of limited capacity service vehicles to service a set of customers. This paper describes an innovative, agent-based approach to solving a real-world vehicle-routing problem embedded in a highly dynamic, unpredictable domain. Most VRP research, and all commercial products for solving VRP's, make a static-world assumption, ignoring the dynamism in the real world. Our system is explicitly designed to address dynamism, and employs an in-time algorithm that quickly finds partial solutions to a problem, and improves these as time allows. Our fundamental innovation is a stochastic improvement mechanism that enables a distributed, agent-based system to achieve highquality solutions in the absence of a centralized dispatcher. This solution-improvement technology overcomes inherent weaknesses in the distributed problem-solving approach that make it difficult to find high-quality solutions to complex optimization problems. In previous work on similar problems, the MARS system of Fischer and M眉ller, et al., achieved an average route performance of roughly 124% of Solomon's algorithm for a VRP problem, which is known to achieve results that average roughly 107% of optimal. Our algorithm produces routes that average 106% those produced by an adaptation of Solomon's algorithm to a more general problem.

#index 283308
#* HICAP: an interactive case-based planning architecture and its application to noncombatant evacuation operations
#@ Héctor Muñoz-Avila;David W. Aha;Len Breslow;Dana Nau
#t 1999
#c 10
#% 179879
#% 266225
#% 266227
#% 266383
#% 492178
#% 494593
#% 1478935
#% 1478941
#% 1499567
#! This paper describes HICAP, a general purpose planning architecture that we have developed and applied to assist military commanders and their staff with planning NEOs (Noncombatant Evacuation Operations). HICAP integrates a hierarchical task editor, HTE, with a conversational casebased planner, NaCoDAE/HTN. In this application, HTE maintains an agenda of tactical planning tasks that, according to military doctrine, must be addressed in a NEO. Military planning personnel select a task to decompose from HTE and then use NaCoDAE/HTN to interactively refine it into an operational plan by selecting and applying cases, which represent task decompositions from previous NEO operations. Thus, HICAP helps commanders by using previous experience to formulate operational plans that are in accordance with NEO doctrine.

#index 283309
#* Automated capture of rationale for the detailed design process
#@ Karen L. Myers;Nina B. Zumel;Pablo Garcia
#t 1999
#c 10
#% 444776
#% 1128799
#% 1129151
#% 1129156
#% 1478928
#! The value of comprehensive rationale information for documenting a design has long been recognized. However, detailed rationale is rarely produced in practice because of the substantial time investment required. Efforts to support the acquisition of rationale have focused on languages and tools for structuring the acquisition process, but still require substantial involvement on the part of the designer. This document describes an experimental system, the Rationale Construction Framework (RCF), that acquires rationale information for the detailed design process without disrupting a designer's normal activities. The underlying approach involves monitoring designer interactions with a commercial CAD tool to produce a rich process history. This history is subsequently structured and interpreted relative to a background theory of design metaphors that enable explanation of certain aspects of the design process. Evaluation of RCF within a robotic arm design case has shown that the system can acquire meaningful rationale information in a time-and cost-effective manner, with minimal disruption to the designer.

#index 283310
#* A multi-agent system for meting out influence in an intelligent environment
#@ M. V. Nagendra Prasad;Joseph F. McCarthy
#t 1999
#c 10
#% 143048
#% 152419
#% 234719
#% 258250
#% 258255
#% 260780
#% 344241
#% 483168
#% 636338
#% 1478926
#! Intelligent environments are physical spaces that can sense and respond to the people and events taking place within them, providing opportunities for people to influence environmental factors that affect them, such as the lighting, temperature, d茅cor or background music in the common areas of an office building. The designer of an environment that can be influenced by a group of collocated people rather than a single individual must decide how to accord influence among the individuals in the group. We have designed two multi-agent group preference arbitration schemes and tested them out in an intelligent environment, MUSICFX, which controls the selection of music played in a fitness center. One scheme seeks to maximize the average satisfaction of the inhabitants, the other seeks to maximize the equitable distribution of satisfaction among the inhabitants. We present the results of a series of experiments using real data collected from the deployed system, and discuss the ramifications of these two potentially conflicting goals.

#index 283311
#* CMUnited-98: a team of robotic soccer agents
#@ Manuela Veloso;Michael Bowling;Sorin Achim;Kwun Han;Peter Stone
#t 1999
#c 10
#% 241027
#% 367254
#% 506268
#% 506431
#! In this paper, we present the main research contributions of our champion CMUnited-98 small robot team. The team is a multi-agent system in the robotic soccer entertainment application area. The robotic system has global perception and distributed cognition and action. We describe the main features of the hardware design of the physical robots, including differential drive, robust mechanical structure, and a kicking device. We briefly overview the CMUnited- 98 global vision processing algorithm. We then introduce our new robot motion algorithm which reactively generates motion control to account for the target point, the desired robot orientation, and obstacle avoidance. Our robots exhibit successful collisionfree motion in the highly dynamic robotic soccer environment. At the strategic and decision-making level, we present the role-based behaviors of the CMUnited- 98 robotic agents. Team collaboration is remarkably achieved through a new algorithm that allows for team agents to anticipate possible collaboration opportunities. Robots position themselves strategically in open positions that increase passing opportunities. The chapter terminates with a summary of the results of the RoboCup-98 games in which the CMUnited-98 small robot team scored a total of 25 goals and suffered 6 goals in the 5 games that it played.

#index 283312
#* Sensible agents: demonstration of dynamic configuration of agent organizations for responsive planning operations
#@ K. S. Barber;A. Goel;D. Han;J. Kim;T. H. Liu;C. E. Martin;R. McKay
#t 1999
#c 10
#% 252822
#% 442797
#% 515679
#% 1272316
#% 1275423

#index 283313
#* The Disciple integrated shell and methodology for rapid development of knowledge-based agents
#@ Mihai Boicu;Kathryn Wright;Dorin Marcu;Seok Won Lee;Michael Bowman;Gheorghe Tecuci
#t 1999
#c 10
#% 266237
#% 283117
#% 356978
#! The Disciple Learning Agent Shell (Disciple-LAS) is an integrated set of modules for rapid development of practical end-to-end knowledge-based agents, by domain experts, with limited assistance from knowledge engineers. DiscipleLAS and its associated agent building methodology are presented in (Tecuci et al. 1999). Therefore, in this paper, we introduce two very different agents developed with Disciple-LAS, to show its applicability to a wide range of domains. Then we introduce the different modules that are part of Disciple-LAS, and present their use in the agent building process. Finally we summarize the solutions proposed by the Disciple approach to some of the issues that have been found to be limiting factors in developing knowledge-based agents.

#index 283314
#* SMILE: Structural Modeling, Inference, and Learning Engine and GeNIe: a development environment for graphical decision-theoretic models
#@ Marek J. Druzdzel
#t 1999
#c 10
#! SMILE (Structural Modeling, Inference, and Learning Engine) is a fully portable library of C++ classes implementing graphical decision-theoretic methods, such as Bayesian networks and influence diagrams, directly amenable to inclusion in intelligent systems. Its Windows user interface, GeNIe is a versatile and user-friendly development environment for graphical decision-theoretic models. Both modules, developed at the Decision Systems Laboratory, University of Pittsburgh, have been made available to the community in July 1998 at http://www2.sis.pitt.edu/~genie and have over 1,200 users worldwide (as of April 1999). This document summarizes the basic features of GeNIe and SMILE

#index 283315
#* Knowledge Base Discovery Tool
#@ Erik Eilerts;Kathleen Lossau;Christopher York
#t 1999
#c 10
#! The Knowledge Base Discovery Tool (KBDT) is a suite of tools and components to improve the indexing of and search for documents. KBDT extracts and displays content from documents and builds knowledge indexes based on meaning, rather than keywords. KBDT uses the indexes to perform more intelligent searches. It also includes visualization technology to display relevant results using multi-media, rather than plain text. This paper describes prototypes (If two tools in this suite that use components for searching, extraction, and display of requested information. The tools are the Knowledge Base Editor and the Intelligent Information Retrieval Engine.

#index 283316
#* TRIPS: the Rochester Interactive Planning System
#@ George Ferguson;James F. Allen
#t 1999
#c 10
#% 266227
#% 678234
#! This demonstration showcases TRIPS, The Rochester Interactive Planning System, an intelligent, collaborative, conversational planning assistant. TRIPS collaborates with the user using both spoken dialogue and graphical displays to solve problems in a transportation logistics domain. In our demonstrations, users are encouraged to sit down and try the system, with only rudimentary guidance from us. For further information, including QuickTime movies of the system in action, please visit our website at the URL listed above.

#index 283317
#* A natural-language speech interface constructed entirely as a set of executable specifications
#@ R. A. Frost
#t 1999
#c 10
#% 178069
#% 622464
#% 622657
#! SpeechNet is a collection of speech-accessible hyper-linked objects called sihlos. Sihlos are deployed over the Internet and are accessed by remote speech browsers. When a speech browser accesses a sihlo, it begins by downloading a grammar file which is used to configure the browser in order to achieve high recognition accuracy. Sihlos are hyperlinked in a manner that is similar to the linking of html pages. SpeechNet provides non-visual access to knowledge which is analogous to visual access provided by the web. One of the sihlos can answer thousands of spoken pseudonatural-language questions about the solar system. This sihlo has been constructed entirely as a set of executable specifications of the language that it can process.

#index 283318
#* A system for the semantic interpretation of unrestricted domains using WordNet
#@ Fernando Gomez;Carlos Segami
#t 1999
#c 10

#index 283319
#* DIPLOMAT: compiling prioritized default rules into ordinary logic programs, for e-commerce applications
#@ Benjamin N. Grosof
#t 1999
#c 10
#% 244373
#! Rules promise to be widely useful in Internet electronic commerce. Declarative prioritized default rule knowledge representations offer the advantage of handling conflicts that arise in updating rule sets, but have as yet had little practical deployment. DIPLOMAT is a Java library that embodies a new approach to the implementation of such prioritized default rules: to compile them into ordinary logic programs (LP's) cf. pure Prolog. We apply the approach to a newly generalized version of courteous LP's, a semantically attractive and computationally tractable form of prioritized default rules. Compilation enables courteous LP's functionality to be added modularly to ordinary LP rule engines, via a preprocessor, with tractable computational overhead. This takes a long step towards actual deployment of prioritized-default knowledge representation in commercially fielded technology and applications.We give in the demo storyboard an automated example e-commerce application scenario: inferencing in a 70-rule courteous LP that represents personalized pricing and promotions on a bookstore's Web storefront.

#index 283320
#* Solving crosswords with PROVERB
#@ Michael L. Littman;Greg A. Keim;Noam M. Shazeer
#t 1999
#c 10
#! We attacked the problem of solving crossword puzzles by computer: Given a set of clues and a crossword grid, try to maximize the number of words correctly filled in. PROVERB, the probabilistic crueiverbalist, separates the problem into two, more familiar subproblems: candidate generation and grid filling. In candidate generation, each clue is treated as a type of query to an information retrieval system, and relevant words of the correct length are returned along with confidence scores. In grid filling, the candidate words are fit into the puzzle grid to maximize an overall confidence score using a combination of ideas from belief network inference and constraint satisfaction. For our demonstration, we will have an interactive version of the candidate-generation process available via the web, and will also give people an opportunity to go head-to-head against PROVERB in solving complete puzzles.

#index 283321
#* Worldwide aeronautical route planner
#@ Charles B. McVey;David P. Clements;Barton C. Massey;Andrew J. Parkes
#t 1999
#c 10

#index 283322
#* Authoring new material in a reading tutor that listens
#@ Jack Mostow;Gregory Aist
#t 1999
#c 10
#! Project LISTEN's Reading Tutor helps children learn to read by providing assisted practice in reading connected text. A key goal is to provide assistance for reading any English text entered by students or adults. This live demonstration shows how the Reading Tutor helps users enter and narrate stories, and then helps children read them.

#index 283323
#* Demonstration of rational communicative behavior in coordinated defense
#@ Sanguk Noh;Piotr J. Gmytrasiewicz
#t 1999
#c 10
#% 271052

#index 283324
#* Automated team analysis
#@ Taylor Raines;Milind Tambe;Stacy Marsella
#t 1999
#c 10

#index 283325
#* eMediator: a next generation electronic commerce server
#@ Tuomas Sandholm
#t 1999
#c 10
#% 1273806
#% 1273807
#% 1499484
#! eMediator, a next generation electronic commerce server, demonstrates ways in which AI, algorithmic support, game theoretic incentive engineering, and GUI design can jointly improve the efficiency of ecommerce.The first component, eAuction House, is a configurable auction house that supports a large variety of parameterizable auction types. It supports generalized combinatorial auctions with new algorithms for winner determination. It also allows bidding via graphically drawn price-quantity graphs. It has an expert system for helping the user decide which auction type to use. Finally, it supports mobile software agents that bid optimally on the user's behalf based on game theoretic analyses.The second component, eCommitter, is a leveled commitment contract optimizer. In automated negotiation systems consisting of self-interested agents, contracts have traditionally been binding. Leveled commitment contracts--i.e. contracts where each party can decommit by paying a predetermined penalty--were recently shown to improve Pareto efficiency even if agents rationally decommit in Nash equilibrium using inflated thresholds on how good their outside offers must be before they decommit. eCommitter solves the Nash equilibrium thresholds. Furthermore, it optimizes the contract price and decommitment penalties themselves.

#index 283326
#* MailCat: an intelligent assistant for organizing e-mail
#@ Richard B. Segal;Jeffrey O. Kephart
#t 1999
#c 10
#% 159108
#% 271083
#! MailCat is an intelligent assistant that helps users organize their e-mail into folders. It uses a text classifier to learn each user's mail-filing habits. MailCat uses what it learns to predict the three folders in which the user is most likely to place each incoming message. It then provides shortcut buttons to file each message into one of these three folders. When one of MailCat's predictions is correct, the effort required to file a message is reduced to a single button click

#index 283327
#* HIKE (HPKB integrated knowledge environment)- a query interface and integrated knowledge environment for HPKB
#@ Barbara H. Starr;Vinay K. Chaudhri;Boris Katz;Benjamin Good;Jérôme Thoméré
#t 1999
#c 10
#% 230386
#% 266215
#% 266237
#% 304174
#% 569100
#! This demonstration is based upon the results of a research project sponsored by the Defense Advance Research Projects Agency (DARPA), called High Performance Knowledge Bases (HPKB). The demonstrated portion of HPKB follows a question-answering paradigm. The integrated architecture developed at Science Applications International Corporation (SAIC), called the HPKB Integrated Knowledge Environment (HIKE) is introduced. Following this, the components involved in the demonstration, which include a natural language understanding system, a first order theorem prover, and a knowledge server are briefly described. The demonstration effectively illustrates the use of both a graphical user interface and a natural language interface to query a first order theorem prover with similar results.

#index 283328
#* Intelligent agents in computer games
#@ Michael van Lent;John Laird;Josh Buckman;Joe Hartford;Steve Houchard;Kurt Steinkraus;Russ Tedrake
#t 1999
#c 10
#% 23011

#index 283329
#* Sensor based coverage of unknown environments for land mine detection
#@ Ercan Acar;Morgan Simmons;Michael Rosenblatt;Maayan Roth;Mary Berna;Yonatan Mittlefehldt;Howie Choset
#t 1999
#c 10
#% 367254
#! This paper introduces a sensor based coverage algorithm and an overview of a mobile robot system for demining. The algorithm is formulated in terms of critical points which are the points where the topology of an environment changes. We developed a provably complete coverage algorithm which makes a robot pass over all possible points of an unknown environment.

#index 283330
#* A natural interface and unified skills for a mobile robot
#@ William Adams;Dennis Perzanowski;Alan C. Schultz
#t 1999
#c 10

#index 283331
#* Kansas State robotics
#@ Frank Blecha;Tim Beese;Damon Kuntz;Jonathan Cameron;David Sexton;David Gustafson
#t 1999
#c 10
#! The Computing and Information Sciences Department at Kansas State University has developed a software control laboratory for the purpose of exposing undergraduate students to the problems of developing software on real, moving equipment. The equipment in the laboratory consists of two Nomad200 robots and two Scout robots from Nomadic Technology, Inc. The main use of the equipment is in a capstone, two-semester software engineering sequence. In this course, selected teams of students develop software to control the robot in tasks such as maze running, object identification or environment mapping. In the last few AAAI robotic contests, the tasks have been similar to projects in the course.

#index 283332
#* Web-based mobile robot simulator
#@ Dan Stormont
#t 1999
#c 10

#index 283333
#* Elaboration tolerance of logical theories
#@ Eyal Amir
#t 1999
#c 10
#% 1273751
#% 1275338

#index 283336
#* Approximation algorithms for solving cost observable Markov decision processes
#@ Valentina Bayer
#t 1999
#c 10

#index 283339
#* Using formal meta-data descriptions for automated ecological modeling
#@ Virgínia V. B. Biris Brilhante
#t 1999
#c 10
#% 90828

#index 283342
#* Modelling higher cognitive functions with Hebbian cell assemblies
#@ Marcin Chady
#t 1999
#c 10

#index 283345
#* Learning form-meaning mappings for language
#@ Nancy Chang
#t 1999
#c 10

#index 283347
#* Development of a methodology and software shell for the automatic generation of intelligent tutoring systems from existing generic task-based expert systems
#@ Eman M. El-Sheikh
#t 1999
#c 10

#index 283351
#* Towards bounded optimal meta-level control: a case study
#@ Daishi Harada
#t 1999
#c 10

#index 283352
#* Execution monitoring and diagnosis in multi-agent environments
#@ Gal A. Kaminka
#t 1999
#c 10
#% 271071

#index 283354
#* Corpus-based induction of lexical representation and meaning
#@ Maria Lapata
#t 1999
#c 10
#% 786558

#index 283356
#* Data driven profiling of dynamic system behavior using hidden Markov model based combined unsupervised and supervised classification
#@ Cen Li
#t 1999
#c 10
#% 136350
#% 232117
#% 246834

#index 283365
#* Planning under uncertainty via stochastic satisfiability
#@ Stephen M. Majercik
#t 1999
#c 10
#% 283215

#index 283369
#* Applying supervised learning to real-world problems
#@ Dragos D. Margineantu
#t 1999
#c 10

#index 283370
#* Modeling prosody automatically in concept-to-speech generation
#@ Shimei Pan
#t 1999
#c 10
#% 219844
#% 747776

#index 283373
#* A Bayesian approach to object identification
#@ Hanna Pasula
#t 1999
#c 10

#index 283375
#* Over-constrained systems
#@ Hana Rudová
#t 1999
#c 10
#% 520655

#index 283377
#* Reasoning about sensing actions and reactivity
#@ Son Cao Tran
#t 1999
#c 10

#index 283378
#* Applying genetic algorithms to pronoun resolution
#@ Donna K. Byron;James F. Allen
#t 1999
#c 10
#% 19917
#% 740916
#% 747754

#index 283379
#* Automatic sample-by-sample model selection between two off-the-shelf classifiers
#@ Steve P. Chadwick
#t 1999
#c 10
#% 132938

#index 283383
#* Structural knowledge discovery in chemical and spatio-temporal databases
#@ Ravindra N. Chittimoori;Jesus A. Gonzalez;Lawrence B. Holder
#t 1999
#c 10
#% 446083
#% 550398

#index 283385
#* Learning design guidelines by theory refinement
#@ Jacob Eisenstein
#t 1999
#c 10
#% 203611
#% 262195
#% 441346

#index 283387
#* Using neural networks in agent teams to speed up solution discovery for hard multi-criteria problems
#@ Shaun Gittens;Richard Goodwin;Jayant Kalagnanam;Sesh Murthy
#t 1999
#c 10
#% 217580

#index 283389
#* OBDD-based planning with real-valued variables in non-deterministic environments
#@ A. Goel;K. S. Barber
#t 1999
#c 10
#% 266384
#% 541594
#% 1478800

#index 283390
#* Expectation-based learning in design
#@ Dan L. Grecu;David C. Brown
#t 1999
#c 10
#% 243728

#index 283391
#* A framework for problem solving activities in multi-agent systems
#@ D. C. Han;T. H. Liu;K. S. Barber
#t 1999
#c 10

#index 283392
#* Robot navigation with a polar neural map
#@ Michail G. Lagoudakis;Anthony S. Maida
#t 1999
#c 10
#% 181785

#index 283394
#* Comparison of clustering metrics and unsupervised learning algorithms on genome-wide gene expression level data
#@ Sonia Leach;Lawrence Hunter;David Landsman
#t 1999
#c 10

#index 283395
#* Knowledge base revision through exception-driven discovery and learning
#@ Seok Won Lee;Gheorghe Tecuci
#t 1999
#c 10
#% 283117
#% 356978

#index 283398
#* Autonomous discovery in empirical domains
#@ Gary Livingston;Bruce G. Buchanan
#t 1999
#c 10

#index 283400
#* Learning in broker agent
#@ Xiaocheng Luan;Yun Peng;Timothy Finin
#t 1999
#c 10
#% 457160
#% 496732

#index 283401
#* Text compression as a test for artificial intelligence
#@ Matthew V. Mahoney
#t 1999
#c 10
#% 840577

#index 283403
#* Externalizing internal state
#@ Amol D. Mali
#t 1999
#c 10

#index 283404
#* Hybrid propositional encodings of planning
#@ Amol D. Mali
#t 1999
#c 10
#% 1273680

#index 283406
#* Causal discovery from population-based infant birth and death records
#@ Subramani Mani;Gregory F. Cooper
#t 1999
#c 10
#% 44876
#% 420059

#index 283407
#* Interacting with a pet robot using hand gestures
#@ Milyn C. Moy
#t 1999
#c 10
#% 283226

#index 283408
#* Active learning for hierarchical wrapper induction
#@ Ion Muslea;Steve Minton;Craig Knoblock
#t 1999
#c 10
#% 271065
#% 278109
#% 707780
#% 1274000
#% 1478821

#index 283410
#* Decision-theoretic layered robotic control architecture
#@ Gilbert Peterson;Diane J. Cook
#t 1999
#c 10
#% 266387

#index 283412
#* Learning of compositional hierarchies by data-driven chunking
#@ Karl Pfleger
#t 1999
#c 10

#index 283413
#* A representation reducing approach to scientific discovery
#@ Joseph Phillips
#t 1999
#c 10
#% 209148

#index 283415
#* Minimal cost complexity pruning of meta-classifiers
#@ Andreas L. Prodromidis;Salvatore J. Stolfo
#t 1999
#c 10
#% 61792
#% 132938

#index 283417
#* Comparison of second-order polynomial model selection methods: an experimental survey
#@ Grace W. Rumantir
#t 1999
#c 10

#index 283418
#* Learning state features from policies to bias exploration in reinforcement learning
#@ Bryan Singer;Manuela Veloso
#t 1999
#c 10
#% 64127
#% 384911

#index 283422
#* Investigating the effect of relevance and reachability constraints on SAT encodings of planning
#@ Biplav Srivastava
#t 1999
#c 10
#% 1273727
#% 1290109

#index 283423
#* Learning to handle inconsistency for multi-source integration
#@ Sheila Tejada;Craig A. Knoblock;Steven Minton
#t 1999
#c 10
#% 266102
#% 266720
#% 466095
#! Many problems arise when trying to integrate information from multiple sources on the web. One of these problems is that data instances can exist in inconsistent formats across several sources. An example application of information integration is trying to integrate all the reviews of Los Angeles restaurants from Yahoo's Restaurants webpage with the current health rating for each restaurant from the LA County Department of Health's website. Integrating these sources requires determining if they share any of the same restaurants by comparing the data instances from both sources (Figure 1). Because the instances can be in different formats, e.g. the restaurant "Jerry's Famous Deli" from Yahoo's webpage can appear as "Jerry's Famous Delicatessen" in the Dept. of Health's source, they can not be compared using equality; but must be judged according to similarity.

#index 283424
#* Learning rewrite rules to improve plan quality
#@ Muhammad Afzal Upal
#t 1999
#c 10
#% 515228

#index 283425
#* Word sense disambiguation for information retrieval
#@ Ozlem Uzuner;Boris Katz;Deniz Yuret
#t 1999
#c 10
#% 144031
#% 169768
#% 707797

#index 283429
#* Game playing (invited talk): the next moves
#@ Susan L. Epstein
#t 1999
#c 10
#% 1697
#% 36315
#% 36816
#% 39261
#% 68278
#% 68279
#% 183499
#% 235559
#% 243725
#% 266213
#% 266214
#% 419934
#% 535956
#% 535957
#% 536094
#% 1499546
#! Computer programs now play many board games as well or better than the most expert humans. Human players, however, learn, plan, allocate resources, and integrate multiple streams of knowledge. This paper highlights recent achievements in game playing, describes some cognitively-oriented work, and poses three related challenge problems for the AI community.

#index 381854
#* AAAI 1996: Proceedings of the Thirteenth National Conference on Artificial Intelligence, August 4-8, Portland, Oregon
#@  National Conference on Arificial Intelligence; American Association on Artificial Intelligence; AAAI
#t 1996
#c 10
#! From the Publisher:August 4-8, 1996, Portland, Oregon AAAI '96 provides a broad forum for information exchange and interaction among researchers working in different subdisciplines, in different research paradigms, and in different stages of research in artificial intelligence. Topics cover principles underlying cognition, perception and action; design, application, and evaluation of AI algorithms and systems; architectures and frameworks for classes of AI systems; and analysis of tasks and domains in which intelligent systems perform. Included are contributions that describe theoretical, empirical, or experimental results; represent areas of AI that may have been underrepresented in recent conferences; present promising new research concepts, techniques, or perspectives; or discuss issues that cross traditional subdisciplinary boundaries. Two-volume set Distributed for the AAAI Press

#index 922222
#* Thinking about Android Epistemology (AAAI Press Copublications)
#@ Kenneth M. Ford;Clark Glymour;Patrick J. Hayes
#t 2006
#c 10

#index 1186976
#* Agent-Mediated Electronic Commerce and Trading Agent Design and Analysis: AAMAS 2007 Workshop, AMEC 2007, Honolulu, Hawaii, May 14, 2007, and AAAI 2007 ... Notes in Business Information Processing)
#@ John Collins;Peyman Faratin;Simon Parsons;Juan A. Rodriguez-Aguilar;Norman M. Sadeh;Onn Shehory;Elizabeth Sklar
#t 2008
#c 10
#! This book constitutes the thoroughly refereed post-conference proceedings of the 9th International Workshop on Agent-Mediated Electronic Commerce, AMEC IX, co-located with the Sixth International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2007, held in Honolulu, Hawaii, in May 2007, and the 5th Workshop on Trading Agent Design and Analysis, TADA 2007, co-located with the Twenty-Second AAAI Conference on Artificial Intelligence, AAAI 2007, held in Vancouver, Canada, in July 2007. This volume presents 15 revised and selected papers from these workshops. The primary and complementary goal of both workshops was to continue to bring together novel work from diverse fields on modeling, implementation and evaluation of computational trading institutions and/or agent strategies. The papers originating from AMEC focus on a large variety of issues on auctions, negotiation, and strategic behavior in electronic marketplaces. The papers originating from TADA stem from the effort of the community to design scenarios where trading agent designers and market designers can be pitched against one another.

#index 1193296
#* Coordination, Organizations, Institutions and Norms in Agent Systems IV: COIN 2008 International Workshops, COIN@AAMAS 2008, Estoril, Portugal, May 12, 2008. COIN@AAAI 2008, Chicago, USA, July 14, 2008. Revised Selected Papers
#@ Jomi Fred Hübner;Eric Matson;Olivier Boissier;Virginia Dignum
#t 2009
#c 10

#index 1250117
#* Proceedings of the 19th national conference on Artifical intelligence
#@ Anthony G. Cohn
#t 2004
#c 10

#index 1250118
#* Preface: the AAAI-04 conference
#@ George Ferguson;Deborah McGuinness
#t 2004
#c 10

#index 1250119
#* Performance bounded reinforcement learning in strategic interactions
#@ Bikramjit Banerjee;Jing Peng
#t 2004
#c 10
#% 164502
#% 165663
#% 266286
#% 348821
#% 384911
#% 464283
#% 464437
#% 465913
#% 496735
#% 528018
#% 593734
#% 722895
#% 1289288
#! Despite increasing deployment of agent technologies in several business and industry domains, user confidence in fully automated agent driven applications is noticeably lacking. The main reasons for such lack of trust in complete automation are scalability and nonexistence of reasonable guarantees in the performance of selfadapting software. In this paper we address the latter issue in the context of learning agents in a Multiagent System (MAS). Performance guarantees for most existing on-line Multiagent Learning (MAL) algorithms are realizable only in the limit, thereby seriously limiting its practical utility. Our goal is to provide certain meaningful guarantees about the performance of a learner in a MAS, while it is learning. In particular, we present a novel MAL algorithm that (i) converges to a best response against stationary opponents, (ii) converges to a Nash equilibrium in self-play and (iii) achieves a constant bounded expected regret at any time (no-average-regret asymptotically) in arbitrary sized general-sum games with non-negative payoffs, and against any number of opponents.

#index 1250120
#* Searching for stable mechanisms: automated design for imperfect players
#@ Andrew J. Blumberg;Abhi Shelat
#t 2004
#c 10
#% 344879
#% 528018
#% 1650358
#! Recently Conitzer and Sandholm (Conitzer & Sandholm 2002) introduced the concept of "automated mechanism design", whereby mechanism design problems are solved using constraint-satisfaction methods. Traditionally, mechanism design has focused on producing games which yield the desired outcomes when played by ideal rational players. However actual players are never perfectly rational -- human irrationality has been exhaustively studied and computational agents have both resource bounds and potentially implementation flaws. In this paper, we discuss extensions of the techniques of automated mechanism design to produce games which are robust in the face of player imperfections. We model limited rationality by examining agents which converge on their strategy by using a simple variant of "fictitious play" (simulation of repeated play) (Singh, Kearns, & Mansour 2000). This model associates to each game a system of differential equations describing the trajectory of the agent's strategies. We describe additional constraints which guarantee that automated mechanism design search problems yield stable mechanisms. In particular, we present negative results for structural stability and positive results for asymptotic stability by considering strict Bayesian-Nash equilibria and by employing Lyapunov techniques.

#index 1250121
#* Affective recruitment of distributed heterogeneous agents
#@ Aaron Gage;Robin R. Murphy
#t 2004
#c 10
#% 265785
#% 634143
#% 1013352
#! Members of multi-robot teams may need to collaborate to accomplish a task due to differences in capabilities. This paper describes an extension of the ALLIANCE architecture that enables agent recruitment within a decentralized UAV-UGV robot team without task preemption but 1) uses a formal model of emotions and 2) handles heterogeneity. Affective computing allows recruitment to be robust under loss of communication between agents and minimizes the number of messages passed. Data from 66 simulations show that the affective strategy succeeds with a random message loss rate up to 25% and requires 19.1% fewer messages to be sent compared to greedy and random, and that of these, affective scales best with team size. Comparisons of broadcast to unicast messaging are also made in simulation.

#index 1250122
#* Visibility-based pursuit-evasion with limited field of view
#@ Brian P. Gerkey;Sebastian Thrun;Geoff Gordon
#t 2004
#c 10
#% 36176
#% 47569
#% 126316
#% 274390
#% 348966
#% 746731
#! We study a form of the pursuit-evasion problem, in which one or more searchers must move through a given environment so as to guarantee detection of any and all evaders, which can move arbitrarily fast. Our goal is to develop techniques for coordinating teams of robots to execute this task in application domains such as clearing a building, for reasons of security or safety. To this end, we introduce a new class of searcher, the Φ-searcher, which can be readily instantiated as a physical mobile robot. We present a detailed analysis of the pursuit-evasion problem using Φ-searchers. We show that computing the minimum number of Φ-searchers required to search a given environment is NP-hard, and present the first complete search algorithm for a single Φ-searcher. We show how this algorithm can be extended to handle multiple searchers, and give examples of computed trajectories.

#index 1250123
#* Task allocation via self-organizing swarm coalitions in distributed mobile sensor network
#@ Kian Hsiang Low;Wee Kheng Leow;Marcelo H. Ang
#t 2004
#c 10
#% 34601
#% 252199
#% 284645
#% 294103
#% 378931
#% 417746
#% 418831
#% 418848
#% 426521
#% 531449
#% 719917
#% 1279465
#% 1478828
#! This paper presents a task allocation scheme via self-organizing swarm coalitions for distributed mobile sensor network coverage. Our approach uses the concepts of ant behavior to self-regulate the regional distributions of sensors in proportion to that of the moving targets to be tracked in a non-stationary environment. As a result, the adverse effects of task interference between robots are minimized and sensor network coverage is improved. Quantitative comparisons with other tracking strategies such as static sensor placement, potential fields, and auction-based negotiation show that our approach can provide better coverage and greater flexibility to respond to environmental changes.

#index 1250124
#* GROWRANGE: anytime VCG-based mechanisms
#@ David C. Parkes;Grant Schoenebeck
#t 2004
#c 10
#% 267752
#% 314918
#% 314944
#% 413867
#% 453488
#% 453489
#% 496250
#% 535163
#% 578713
#% 723935
#% 739626
#% 1289299
#! We introduce anytime mechanisms for distributed optimization with self-interested agents. Anytime mechanisms retain good incentive properties even when interrupted before the optimal solution is computed, and provide better quality solutions when given additional time. Anytime mechanisms can solve easy instances of a hard problem quickly and optimally, while providing approximate solutions on very hard instances. In a particular instantiation, GROWRANGE, we successively expand the range of outcomes considered, computing the optimal solution for each range. Truth-revelation remains a dominant strategy equilibrium with a stage-based interruption, and is a best-response with high probability when the interruption is time-based.

#index 1250125
#* Useful roles of emotions in artificial agents: a case study from artificial life
#@ Matthias Scheutz
#t 2004
#c 10
#% 127564
#% 159119
#% 168422
#% 238395
#% 240977
#% 271084
#% 302055
#% 334318
#% 350689
#% 550928
#% 576177
#% 1275351
#! In this paper, we discuss the role of emotions in AI and possible ways to determine their utility for the design of artificial agents. We propose a research methodology for determining the utility of emotional control and apply it to the study of autonomous agents that compete for resources in an artificial life environment. The results show that the emotional control can improve performance in some circumstances.

#index 1250126
#* Low-knowledge algorithm control
#@ Tom Carchrae;J. Christopher Beck
#t 2004
#c 10
#% 155827
#% 188076
#% 230332
#% 295780
#% 453072
#% 466598
#% 528307
#% 535143
#% 535163
#% 535321
#% 574248
#% 578756
#% 761838
#! This paper addresses the question of allocating computational resources among a set of algorithms in order to achieve the best performance on a scheduling problem instance. Our primary motivation in addressing this problem is to reduce the expertise needed to apply constraint technology. Therefore, we investigate algorithm control techniques that make decision based only on observations of the improvement in solution quality achieved by each algorithm. We call our approach "low-knowledge" since it does not rely on complex prediction models. We show that such an approach results in a system that achieves significantly better performance than all of the pure algorithms without requiring additional human expertise. Furthermore the low knowledge approach achieves performance equivalent to a perfect high-knowledge classification approach.

#index 1250127
#* Implementing a generalized version of resolution
#@ Heidi E. Dixon;Matthew L. Ginsberg;David K. Hofer;Eugene M. Luks;Andrew J. Parkes
#t 2004
#c 10
#% 25242
#% 42976
#% 567951
#% 569112
#! We have recently proposed augmenting clauses in a Boolean database with groups of permutations, the augmented clauses then standing for the set of all clauses constructed by acting on the original clause with a permutation in the group. This approach has many attractive theoretical properties, including representational generality and reductions from exponential to polynomial proof length in a variety of settings. In this paper, we discuss the issues that arise in implementing a group-based generalization of resolution, and give preliminary results describing this procedure's effectiveness.

#index 1250128
#* SAT-based answer set programming
#@ Enrico Giunchiglia;Yuliya Lierler;Marco Maratea
#t 2004
#c 10
#% 244091
#% 327779
#% 336874
#% 417649
#% 417651
#% 578673
#% 746485
#% 752744
#% 752745
#% 790727
#! The relation between answer set programming (ASP) and propositional satisfiability (SAT) is at the center of many research papers, partly because of the tremendous performance boost of SAT solvers during last years. Various translations from ASP to SAT are known but the resulting SAT formula either includes many new variables or may have an unpractical size. There are also well known results showing a one-to-one correspondence between the answer sets of a logic program and the model of its completion. Unfortunately, these results only work for specific classes of problems. In this paper we present a SAT-based decision procedure for answer set programming that (i) deals with any (non disjunctive) logic program, (ii) works on a SAT formula without additional variables, and (iii) is guaranteed to work in polynomial space. Further, our procedure can be extended to compute all the answer sets still working in polynomial space. The experimental results of a prototypical implementation show that the approach can pay off sometimes by orders of magnitude.

#index 1250129
#* A polynomial-time algorithm for simple temporal problems with piecewise constant domain preference functions
#@ T. K. Satish Kumar
#t 2004
#c 10
#% 70370
#% 107137
#% 266107
#% 544940
#% 736897
#% 743461
#% 1289192
#% 1289210
#! In this paper, we provide a polynomial-time algorithm for solving an important class of metric temporal problems that involve simple temporal constraints between various events (variables) and piecewise constant preference functions over variable domains. We are given a graph G = (χ, ε) where χ = {X0, X1... Xn} is a set of events (X0 is the "beginning of the world" node and is set to 0 by convention) and e = (Xi, Xj) ∈ ε. annotated with the bounds [LB(e), UB(e)], is a simple temporal constraint between Xi and Xj indicating that Xj must be scheduled between LB(e) and UB(e) seconds after Xi is scheduled (LB(e) ≤ UB(e)). A family of stepwise constant preference functions F = {fxi (t) : R → R} specifies the preference attached with scheduling Xi at time t. The goal is to find a schedule for all the events such that all the temporal constraints are satisfied and the sum of the preferences is maximized. Our polynomial-time algorithm for solving such problems (which we refer to as extended simple temporal problems (ESTPs)) has important consequences in dealing with limited forms of disjunctions and preferences in metric temporal reasoning that would otherwise require an exponential search space.

#index 1250130
#* Using performance profile trees to improve deliberation control
#@ Kate Larson;Tuomas Sandholm
#t 2004
#c 10
#% 98073
#% 110379
#% 159239
#% 205385
#% 233133
#% 233135
#% 243725
#% 307212
#% 329490
#% 329491
#% 344879
#% 495928
#% 527850
#% 535156
#% 1272375
#% 1499506
#% 1650299
#! Performance profile trees have recently been proposed as a theoretical basis for fully normative deliberation control. In this paper we conduct the first experimental study of their feasibility and accuracy in making stopping decisions for anytime algorithms on optimization problems. Using data and algorithms from two different real-world domains, we compare performance profile trees to other well-established deliberation-control techniques. We show that performance profile trees are feasible in practice and lead to significantly better deliberation control decisions. We then conduct experiments using performance profile trees where deliberation-control decisions are made using conditioning on multiple features of the solution to illustrate that such an approach is feasible in practice.

#index 1250131
#* On odd and even cycles in normal logic programs
#@ Fangzhen Lin;Xishun Zhao
#t 2004
#c 10
#% 53385
#% 60502
#% 94456
#% 103705
#% 122390
#% 125134
#% 171033
#% 173127
#% 243711
#% 388218
#% 400988
#% 408396
#% 417651
#% 428362
#% 578673
#! An odd cycle of a logic program is a simple cycle that has an odd number of negative edges in the dependency graph of the program. Similarly, an even cycle is one that has an even number of negative edges. For a normal logic program that has no odd cycles, while it is known that such a program always has a stable model, and such a stable model can be computed in polynomial time, we show in this paper that checking whether an atom is in a stable model is NP-complete, and checking whether an atom is in all stable models is co-NP complete, both are the same as in the general case for normal logic programs. Furthermore, we show that if a normal logic program has exactly one odd cycle, then checking whether it has a stable model is NP-complete, again the same as in the general case. For normal logic programs with a fixed number of even cycles, we show that there is a polynomial time algorithm for computing all stable models. Furthermore, this polynomial time algorithm can be improved significantly if the number of odd cycles is also fixed.

#index 1250132
#* Logic programs with abstract constraint atoms
#@ Victor W. Marek;Mirosław Truszczynski
#t 2004
#c 10
#% 95248
#% 101949
#% 289052
#% 333223
#% 399653
#% 400986
#% 400992
#% 411814
#% 464918
#% 499512
#% 578673
#% 578750
#% 752748
#% 790727
#% 880394
#% 1279333
#! We propose and study extensions of logic programming with constraints represented as generalized atoms of the form C(X), where X is a finite set of atoms and C is an abstract constraint (formally, a collection of sets of atoms). Atoms C(X) are satisfied by an interpretation (set of atoms) M, if M ∩ X ∈ C. We focus here on monotone constraints, that is, those collections C that are closed under the superset. They include, in particular, weight (or pseudo-boolean) constraints studied both by the logic programming and SAT communities. We show that key concepts of the theory of normal logic programs such as the one-step provability operator, the semantics of supported and stable models, as well as several of their properties including complexity results, can be lifted to such case.

#index 1250133
#* Adding time and intervals to procedural and hierarchical control specifications
#@ Tran Cao Son;Chitta Baral;Le-Chi Tuan
#t 2004
#c 10
#% 77167
#% 265782
#% 314845
#% 400987
#% 400992
#% 417651
#% 529345
#% 529354
#% 557385
#% 578673
#! In this paper we introduce the language Golog+HTNTI for specifying control using procedural and HTN-based constructs together with deadlines and time restrictions. Our language starts with features from GOLOG and HTN and extends them so that we can deal with actions with duration by being able to specify time intervals between the start (or end) of an action (or a program) and the start (or end) of another action (or program). We then discuss an off-line interpreter based on the answer set planning paradigm such that the answer sets of the logic program have a one to one correspondence with the traces of the Golog+HTNTI specification.

#index 1250134
#* Model checking temporal logics of knowledge in distributed systems
#@ Kaile Su
#t 2004
#c 10
#% 3873
#% 56086
#% 188086
#% 190683
#% 234819
#% 257869
#% 285967
#% 365338
#% 480345
#% 480681
#% 502751
#% 543494
#% 766972
#% 1068329
#! Model checking is a promising approach to automatic verification, which has concentrated on specification expressed in temporal logic. Comparatively little attention has been given to temporal logics of knowledge, although such logics have been proven to be very useful in the specifications of protocols for distributed systems. In this paper, we address ourselves to the model checking problem for a temporal logic of knowledge (Halpern and Vardi's logic of C K Ln). Based on the semantics of interpreted systems with local propositions, we develop an approach to symbolic C K Ln model checking via OBDDs. In our approach to model checking specifications involving agents' knowledge, the knowledge modalities are eliminated via quantifiers over agents' non-observable variables.

#index 1250135
#* Identifying linear causal effects
#@ Jin Tian
#t 2004
#c 10
#% 44876
#% 297171
#% 578735
#% 578740
#% 1272178
#% 1650356
#% 1650649
#% 1650678
#! This paper concerns the assessment of linear cause-effect relationships from a combination of observational data and qualitative causal structures. The paper shows how techniques developed for identifying causal effects in causal Bayesian networks can be used to identify linear causal effects, and thus provides a new approach for assessing linear causal effects in structural equation models. Using this approach the paper develops a systematic procedure for recognizing identifiable direct causal effects.

#index 1250136
#* The complexity of global constraints
#@ Christian Bessiere;Emmanuel Hebrard;Brahim Hnich;Toby Walsh
#t 2004
#c 10
#% 126120
#% 160208
#% 408396
#% 420009
#% 420010
#% 464907
#% 534497
#% 534993
#% 535153
#% 535172
#% 1499496
#! We study the computational complexity of reasoning with global constraints. We show that reasoning with such constraints is intractable in general. We then demonstrate how the same tools of computational complexity can be used in the design and analysis of specific global constraints. In particular, we illustrate how computational complexity can be used to determine when a lesser level of local consistency should be enforced, when decomposing constraints will lose pruning, and when combining constraints is tractable. We also show how the same tools can be used to study symmetry breaking, meta-constraints like the cardinality constraint, and learning nogoods.

#index 1250137
#* The backdoor key: a path to understanding problem hardness
#@ Yongshao Ruan;Henry Kautz;Eric Horvitz
#t 2004
#c 10
#% 220203
#% 266200
#% 283230
#% 336874
#% 341488
#% 427631
#% 529517
#% 561083
#% 1271817
#% 1272314
#% 1273727
#% 1279379
#% 1289182
#% 1289196
#% 1476298
#% 1478761
#% 1478764
#% 1499502
#! We introduce our work on the backdoor key, a concept that shows promise for characterizing problem hardness in backtracking search algorithms. The general notion of backdoors was recently introduced to explain the source of heavy-tailed behaviors in backtracking algorithms (Williams, Gomes, & Selman 2003a; 2003b). We describe empirical studies that show that the key faction, i.e., the ratio of the key size to the corresponding backdoor size, is a good predictor of problem hardness of ensembles and individual instances within an ensemble for structure domains with large key fraction.

#index 1250138
#* Complexity of contextual reasoning
#@ Floris Roelofsen;Luciano Serafini
#t 2004
#c 10
#% 28185
#% 116625
#% 160385
#% 261139
#% 334493
#% 405391
#% 541789
#% 601159
#% 757481
#% 797452
#% 1499557
#! This paper delineates the computational complexity of propositional multi-context systems. We establish NP-membership by translating multi-context systems into bounded modal Kn, and obtain more refined complexity results by achieving the so-called bounded model property: the number of local models needed to satisfy a set of formulas Φ in a multicontext system MS is bounded by the number of contexts addressed by Φ plus the number of bridge rules in MS. Exploiting this property of multi-context systems, we are able to encode contextual satisfiability into purely propositional satisfiabliIty, providing for the implementation of contextual reasoners based on already existing specialized SAT solvers. Finally, we apply our results to improve complexity bounds for McCarthy's propositional logic of context - we show that satisfiability in this framework can be settled in nondeterministic polynomial time O(|Φ|2).

#index 1250139
#* Hiding satisfying assignments: two are better than one
#@ Dimitris Achlioptas;Haixia Jia;Cristopher Moore
#t 2004
#c 10
#% 17298
#% 75299
#% 210191
#% 263189
#% 338404
#% 341485
#% 347205
#% 460819
#% 495934
#% 529517
#% 1273727
#% 1289196
#! The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances are k-CNF formulas whose clauses are chosen uniformly at random among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner are relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, as the number of clauses is increased, A acts as a stronger and stronger attractor. Motivated by recent results on the geometry of the space of solutions for random k-SAT and NAE-k-SAT instances, we propose a very simple twist on this model that greatly increases the hardness of the resulting formulas. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and A are satisfying. It appears that under this "symmetrization" the effects of the two attractors largely cancel out, making it much harder for an algorithm to "feel" (and find) either one. We give theoretical and experimental evidence supporting this assertion.

#index 1250140
#* Modeling choices in quasigroup completion: SAT vs. CSP
#@ Carlos Ansótegui;Alvaro del Val;Iván Dotú;Cèsar Fernández;Felip Manyà
#t 2004
#c 10
#% 283220
#% 336874
#% 419950
#% 420713
#% 655781
#% 1272031
#% 1289196
#! We perform a systematic comparison of SAT and CSP models for a challenging combinatorial problem, quasigroup completion (QCP). Our empirical results clearly indicate the superiority of the 3D SAT encoding (Kautz et al. 2001), with various solvers, over other SAT and CSP models. We propose a partial explanation of the observed performance. Analytically, we focus on the relative conciseness of the 3D model and the pruning power of unit propagation. Empirically, the focus is on the role of the unit-propagation heuristic of the best performing solver, Satz (Li & Anbulagan 1997), which proves crucial to its success, and results in a significant improvement in scalability when imported into the CSP solvers. Our results strongly suggest that SAT encodings of permutation problems (Hnich, Smith, & Walsh 2004) may well prove quite competitive in other domains, in particular when compared with the currently preferred channeling CSP models.

#index 1250141
#* Leap before you look: an effective strategy in an oversubscribed scheduling problem
#@ Laura Barbulescu;L. Darrell Whitley;Adele E. Howe
#t 2004
#c 10
#% 81961
#% 564392
#% 736279
#% 739299
#% 1272400
#% 1279385
#% 1478767
#% 1499506
#! Oversubscribed scheduling problems require removing or partially satisfying tasks when enough resources are not available. For a particular oversubscribed problem, Air Force Satellite Control Network scheduling, we find that the best approaches make long leaps in the search space. We find this is in part due to large plateaus in the search space. Algorithms moving only one task at a time are impractical. Both a genetic algorithm and Squeaky Wheel Optimization (SWO) make long leaps in the search space and produce good solutions almost 100 times faster than local search. Greedy initialization is shown to be critical to good performance, but is not as important as directed leaps. When using fewer than 2000 evaluations, SWO shows superior performance; with 8000 evaluations, a genetic algorithm using a population seeded with greedy solutions further improves on the SWO results.

#index 1250142
#* Domain transmutation in constraint satisfaction problems
#@ James Bowen;Chavalit Likitvivatanavong
#t 2004
#c 10
#% 3463
#% 130206
#% 266119
#% 266125
#% 292075
#% 419990
#% 735792
#% 1275298
#! We study local interchangeability of values in constraint networks based on a new approach where a single value in the domain of a variable can be treated as a combination of "subvalues". We present an algorithm for breaking up values and combining identical fragments. Experimental results show that the transformed problems take less time to solve for all solutions and yield more compactly-representable, but equivalent, solution sets. We obtain new theoretical results on context dependent interchangeability and full interchangeability, and suggest some other applications.

#index 1250143
#* Collapsibility and consistency in quantified constraint satisfaction
#@ Hubie Chen
#t 2004
#c 10
#% 2028
#% 115329
#% 189747
#% 224751
#% 237054
#% 237055
#% 251197
#% 252213
#% 268708
#% 289332
#% 320265
#% 460791
#% 534512
#% 535150
#% 546311
#! The concept of consistency has pervaded studies of the constraint satisfiction problem. We introduce two concepts, which are inspired by consistency, for the more general framework of the quantified constraint satisfaction problem (QCSP). We use these concepts to derive, in a uniform fashion, proofs of polynomial-time tractability and corresponding algorithms for certain cases of the QCSP where the types of allowed relations are restricted. We not only unify existing tractability results and algorithms, but also identify new classes of tractable QCSPs.

#index 1250144
#* Complete local search for propositional satisfiability
#@ Hai Fang;Wheeler Ruml
#t 2004
#c 10
#% 160245
#% 160270
#% 283231
#% 336874
#% 417606
#% 420727
#% 422129
#% 529172
#% 535307
#% 1273681
#% 1279382
#% 1279416
#% 1289194
#% 1499515
#! Algorithms based on following local gradient information are surprisingly effective for certain classes of constraint satisfaction problems. Unfortunately, previous local search algorithms are notoriously incomplete: They are not guaranteed to find a feasible solution if one exists and they cannot be used to determine unsatisfiability. We present an algorithmic framework for complete local search and discuss in detail an instantiation for the propositional satisfiability problem (SAT). The fundamental idea is to use constraint learning in combination with a novel objective function that converges during search to a surface without local minima. Although the algorithm has worst-case exponential space complexity, we present empirical resulls on challenging SAT competition benchmarks that suggest that our implementation can perform as well as state-of-the-art solvers based on more mature techniques. Our framework suggests a range of possible algorithms lying between tree-based search and local search.

#index 1250145
#* QUICKXPLAIN: preferred explanations and relaxations for over-constrained problems
#@ Ulrich Junker
#t 2004
#c 10
#% 3460
#% 55926
#% 578661
#% 668322
#% 729049
#% 1499513
#! Over-constrained problems can have an exponential number of conflicts, which explain the failure, and an exponential number of relaxations, which restore the consistency. A user of an interactive application, however, desires explanations and relaxations containing the most important constraints. To address this need, we define preferred explanations and relaxations based on user preferences between constraints and we compute them by a generic method which works for arbitrary CP, SAT, or DL solvers. We significantly accelerate the basic method by a divide-and-conquer strategy and thus provide the technological basis for the explanation facility of a principal industrial constraint programming tool, which is, for example, used in numerous configuration applications.

#index 1250146
#* MAX-2-SAT: how good is Tabu search in the worst-case?
#@ Monaldo Mastrolilli;Luca Maria Gambardella
#t 2004
#c 10
#% 95582
#% 143611
#% 171278
#% 177764
#% 205305
#% 237696
#% 283231
#% 302716
#% 388196
#% 408396
#% 420718
#% 529172
#% 1081290
#% 1389681
#% 1478773
#! Tabu search algorithms are amongst the most successful local search based methods for the maximum satisfiability problem. The practical superiority of tabu search over the local search alone lias been already shown experimentally several times. A natural question addressed here is to understand if this superiority holds also from the worst-case point of view. Moreover, it is well known that the main critical parameter of tabu techniques is the tabu list length. Focussing on MAX-2-SAT problem, the main contribution of this paper is a worst-case analysis of tabu search as a function of the tabu list length. We give a first theoretical evidence of the advantage of a tabu search strategy over the basic local search alone that critically depends on the tabu list length.

#index 1250147
#* The practice of approximated consistency for Knapsack constraints
#@ Meinolf Sellmann
#t 2004
#c 10
#% 534496
#% 1068349
#! Knapsack constraints are a key modeling structure in discrete optimization and form the core of many real-life problem formulations. Only recently, a cost-based filtering algorithm for Knapsack constraints was published that is based on some previously developed approximation algorithms for the Knapsack problem. In this paper, we provide an empirical evaluation of approximated consistency for Knapsack constraints by applying it to the Market Split Problem and the Automatic Recording Problem.

#index 1250148
#* Study of lower bound functions for MAX-2-SAT
#@ Haiou Shen;Hantao Zhang
#t 2004
#c 10
#% 95582
#% 282156
#% 312336
#% 327779
#% 344542
#% 420725
#% 449468
#! Recently. several lower bound functions are proposed for solving the MAX-2-SAT problem optimally in a branch-and-bound algorithm. These lower bounds improve significantly the performance of these algorithms. Based on the study of these lower bound functions, we propose a new, linear-time lower bound function. We show that the new lower bound function is admissible and it is consistently and substantially better than other known lower bound functions. The result of this study is a high-performance implementation of an exact algorithm for MAX-2-SAT which outperforms any implementation of the same class.

#index 1250149
#* Additive versus multiplicative clause weighting for SAT
#@ John Thornton;Duc Nghia Pham;Stuart Bain;Valnir Ferreira
#t 2004
#c 10
#% 529172
#% 535307
#% 564832
#% 578753
#% 1289194
#% 1478779
#! This paper examines the relative performance of additive and multiplicative clause weighting schemes for propositional satisfiability testing. Starting with one of the most recently developed multiplicative algorithms (SAPS), an experimental study was constructed to isolate the effects of multiplicative in comparison to additive weighting, while controlling other key features of the two approaches, namely the use of random versus flat moves, deterministic versus probabilistic weight smoothing and multiple versus single inclusion of literals in the local search neighborhood. As a result of this investigation we developed a pure additive weighting scheme (PAWS) which can outperform multiplicative weighting on a range of difficult problems, whtle requiring considerably less effort in terms of parameter turning. We conclude that additive weighting shows better scaling properties because it makes less distinction between costs and so considers a larger domain of possible moves.

#index 1250150
#* Tractable tree convex constraint networks
#@ Yuanlin Zhang;Eugene C. Freuder
#t 2004
#c 10
#% 44931
#% 115329
#% 189747
#% 237054
#% 289332
#% 320265
#% 1279250
#% 1289191
#! A binary cnnstraint network is tree convex if we can construct a tree for the domain of the variables so that for any constraint, no matter what value one variable takes, all the values allowed for the other variable form a subtree of the constructed tree. It is known that a tree convex network is globally consistent if it is path consistent. However, if a tree convex network is not path consistent, enforcing path consistency on it may not make it globally consistent. In this paper, we identify a subclass of tree convex networks which are locally chain convex and union closed. This class of problems can be made globally consistent by path consistency and thus is tractable. More interestingly, we also find that some scene labeling problems can be modeled by tree convex constraints in a natural and meaningful way.

#index 1250151
#* Eliciting bid taker non-price preferences in (combinatorial) auctions
#@ Craig Boutilier;Tuomas Sandholm;Rob Shields
#t 2004
#c 10
#% 36698
#% 261358
#% 267752
#% 341943
#% 345429
#% 378898
#% 496094
#% 528176
#% 529348
#% 578692
#% 578711
#% 580548
#% 1279257
#% 1289299
#% 1650721
#% 1784525
#! Recent algorithms provide powerful solutions to the problem of determining cost-minimizing (or revenue-maximizing) allocations of items in combinatorial auctions. However, in many settings, criteria other than cost (e.g., the number of winners, the delivery date of items, etc.) are also relevant in judging the quality of an allocation. Furthermore, the bid taker is usually uncertain about her preferences regarding tradeoffs between cost and nonprice features. We describe new methods that allow the bid taker to determine (approximately) optimal allocations despite this. These methods rely on the notion of minimax regret to guide the elicitation of preferences from the bid taker and to measure the quality of an allocation in the presence of utility function uncertainty. Computational experiments demonstrate the practicality of minimax computation and the efficacy of our elicitation techniques.

#index 1250152
#* Combinatorial auctions with structured item graphs
#@ Vincent Conitzer;Jonathan Derryberry;Tuomas Sandholm
#t 2004
#c 10
#% 31482
#% 55330
#% 70370
#% 267752
#% 281615
#% 320150
#% 341924
#% 423117
#% 496094
#% 496250
#% 529167
#% 529664
#% 578710
#% 590624
#% 739626
#% 1289299
#! Combinatorial auctions (CAs) are important mechanisms for allocating interrelated items. Unfortunately, winner determination is NP-complete unless there is special structure. We study the setting where there is a graph (with some desired property), with the items as vertices, and every bid bids on a connected set of items. Two computational problems arise: 1) clearing the auction when given the item graph, and 2) constructing an item graph (if one exists) with the desired property. 1 was previously solved for the case of a tree or a cycle, and 2 for the case of a line graph or a cycle. We generalize the first result by showing that given an item graph with bounded treewidth, the clearing problem can be solved in polynomial time (and every CA instance has some treewidth; the complexity is exponential in only that parameter). We then give an algorithm for constructing an item tree (treewidth 1) if such a tree exists, thus closing a recognized open problem. We show why this algorithm does not work for treewidth greater than 1, but leave open whether item graphs of (say) treewidth 2 can be constructed in polynomial time. We show that finding the item graph with the fewest edges is NP-complete (even when a graph of treewidth 2 exists). Finally, we study how the results change if a bid is allowed to have more than one connected component. Even for line graphs, we show that clearing is hard even with 2 components, and constructing the line graph is hard even with 5.

#index 1250153
#* Computing shapley values, manipulating value division schemes, and checking core membership in multi-issue domains
#@ Vincent Conitzer;Tuomas Sandholm
#t 2004
#c 10
#% 160153
#% 171142
#% 233135
#% 252199
#% 578703
#% 580524
#% 751581
#% 1279301
#% 1279324
#% 1499485
#! Coalition formation is a key problem in automated negotiation among self-interested agents. In order for coalition formation to be successful, a key question that must be answered is how the gains from cooperation are to be distributed. Various solution concepts have been proposed, but the computational questions around these solution concepts have received little attention. We study a concise representation of characteristic functions which allows for the agents to be concerned with a number of independent issues that each coalition of agents can address. For example, there may be a set of tasks that the capacity-unconstrained agents could undertake, where accomplishing a task generates a certain amount of value (possibly depending on how well the task is accomplished). Given this representation, we show how to quickly compute the Shapley value--a seminal value division scheme that distributes the gains from cooperation fairly in a certain sense. We then show that in (distributed) marginal-contribution based value division schemes, which are known to be vulnerable to manipulation of the order in which the agents are added to the coalition, this manipulation is NP-complete. Thus, computational complexity serves as a barrier to manipulating the joining order. Finally, we show that given a value division, determining whether some subcoalition has an incentive to break away (in which case we say the division is not in the core) is NP-complete. So, computational complexity serves to increase the stability of the coalition.

#index 1250154
#* Learning social preferences in games
#@ Ya'akov Gal;Avi Pfeffer;Francesca Marzo;Barbara J. Grosz
#t 2004
#c 10
#% 130878
#% 266213
#% 464274
#% 466418
#% 773284
#! This paper presents a machine-learning approach to modeling human behavior in one-shot games. It provides a framework for representing and reasoning about the social factors that affect people's play. The model predicts how a human player is likely to react to different actions of another player, and these predictions are used to determine the best possible strategy for that player. Data collection and evaluation of the model were performed on a negotiation game in which humans played against each other and against computer models playing various strategies. A computer player trained on human data outplayed Nash equilibrium and Nash bargaining computer players as well as humans. It also generalized to play people and game situations it had not seen before.

#index 1250155
#* Methods for boosting revenue in combinatorial auctions
#@ Anton Likhodedov;Tuomas Sandholm
#t 2004
#c 10
#% 723935
#% 1650358
#! We study the recognized open problem of designing revenue-maximizing combinatorial auctions. It is unsolved even for two bidders and two items for sale. Rather than pursuing the pure economic approach of attempting to characterize the optimal auction, we explore techniques for automatically modifying existing mechanisms in a way that increase expected revenue. We introduce a general family of auctions, based on bidder weighting and allocation boosting, which we call virtual valuations combinatorial auctions (VVCA). All auctions in the family are based on the Vickrey-Clarke-Groves (VCG) mechanism, executed on virtual valuations that are linear transformations of the bidders' real valuations. The restriction to linear transformations is motivated by incentive compatibility. The auction family is parameterized by the coefficients in the linear transformations. The problem of designing a high revenue mechanism is therefore reduced to search in the parameter space of VVCA. We analyze the complexity of the search for the optimal such mechanism and conclude that the search problem is computationally hard. Despite that, optimal parameters for VVCA can be found at least in settings with few items and bidders (the experiments show that VVCA yield a substantial increase in revenue over the traditionally used VCG). In larger auctions locally optimal parameters, which still yield an improvement over VCG, can be found.

#index 1250156
#* Using contracts to influence the outcome of a game
#@ Robert McGrew;Yoav Shoham
#t 2004
#c 10
#% 233136
#% 580515
#! We consider how much influence a center can exert on a game if its only power is to propose contracts to the agents before the original game, and enforce the contracts after the game if all agents sign it. Modelling the situation as an extensive-form game, we note that the outcomes that are enforceable are precisely those in which the payoff to each agent is higher than its payoff In at least one of the Nash equilibria of the original game. We then show that these outcomes can still be achieved without any effort actually expended by the center: We propose a mechanism in which the center does not monitor the game, and the contracts are written so that in equilibrium all agents sign and obey the contract, with no need for center intervention.

#index 1250157
#* Scaling up reasoning about actions using relational database technology
#@ Giuseppe De Giacomo;Toni Mancini
#t 2004
#c 10
#% 229083
#% 314845
#% 333218
#% 342119
#% 384978
#% 495950
#% 836134
#% 1279223
#! Reiter's variant of the Situation Calculus is tightly related to relational databases, when complete information on the initial situation is available. In particular, the information on the initial situation can be seen as a relational database, and actions, as specified by the preconditions and successor state axioms, can be seen as operations that change the state of the database. In this paper, we show how to exploit such a correspondence to build systems for reasoning about actions based on standard rdational database technology. Indeed, by exploiting standlrd relational DBMS services, a system may be able to perform both Projection, exploiting DBMS querying services, and Progression, exploiting DBMS update services, in very large action theories. A key result towards such a realization, is that under very natural conditions Reiter's basic action theories turn out to be made of "safe formulas" (where basically negation is used as a form of difference between predicates only) and that regression and progression preserve such a safeness. This is a fundamental property to efficiently exploit relational database technology for reasoning. We then show that, even when action theories are not "safe", they can be made so while trying to retain efficiency as much as possible. Finally, we briefly discuss how such results can be extended to certain forms of incomplete information.

#index 1250158
#* Conservative belief change
#@ James P. Delgrande;Ahhaya C. Nayak;Maurice Pagnucco
#t 2004
#c 10
#% 90860
#% 291003
#% 326595
#% 782324
#! A standard assumption underlying traditional accounts of belief change is the principle of minimal change, that an agent's belief state should be modified minimany to incorporate new information. In this paper we introduce a novel account of belief change in which the agent's belief state is modified minimally to incorporate exactly the new information. Thus a revision by p ??? q will result in a new belief state in which p ??? q is believed, but a stronger proposition (such as p Λ q) is not, regardless of the initial form of the belief state. This form of belief change is termed conservative belief change and corresponds to a Gricean interpretation of the input formula. We investigate belief revision in this framework, and provide a representation result between a set of postulates characterising this form of belief change and a construction in terms of systems of spheres. This approach is extended to that of belief revision with respect to a specified context. Last, we show how this approach resolves a longstanding problem in belief revision.

#index 1250159
#* Mereological semantics for bio-ontologies
#@ Udo Hahn;Stefan Schulz;Kornél Markó
#t 2004
#c 10
#% 221344
#% 221346
#% 283120
#% 665859
#% 1279262
#! Biomedical ontologies are typically structured in a biaxial way, reflecting both a taxonomic and a mereological order. Common examples such as the Gene Ontology and the Unified Medical Language System (UMLS) excel in terms of coverage but lack an adequate semantics of the mereological relations they incorporate. This shortcoming is particularly evident as far as the (non-)mandatory existence of parts for their wholes is cOllcerned, on the one hand, and the propagation of properties acmss part-whole hierarchies, on the other hand. We provide a formal specification of the semantic foundations of mereology in the biomedical domain that is closely linked to the paradigm of description logics. In essence, we here propose to emulate mereological reasoning by taxonomic reasoning. In an attempt to capture much of the shared intuition underlying merelogical reasoning in the biomedical domain, we distinguish for each mereologically relevant concept four different classes of parts and wholes which allow for the expression of five different propagation patterns.

#index 1250160
#* CASEE: a hierarchical event representation for the analysis of videos
#@ Asaad Hakeem;Yaser Sheikh;Mubarak Shah
#t 2004
#c 10
#% 289931
#% 313957
#% 424148
#% 529330
#% 592328
#% 592369
#% 593442
#% 1393974
#! A representational gap exists between low-level measurements (segmentation, object classification, tracking) and high-level understanding of video sequences. In this paper, we propose a novel representation of events in videos to bridge this gap, based on the CASE representation of natural languages. The proposed representation has three significant contributions over existing frameworks. First, we recognize the importance of causal and temporal relationships between subevents and extend CASE to allow the representation of temporal structure and causality between sub-events. Second, in order to capture both multi-agent and multithreaded events, we introduce a hierarchical CASE representation of events in terms of sub-events and case-lists. Last, for purposes of implementation we present the concept of a temporal event-tree, and pose the problem of event detection as subtree pattern matching. By extending CASE, a natural language representation, for the representation of events, the proposed work allows a plausible means of interface between users and the computer. We show two important applications of the proposed event representation for the automated annotation of standard meeting video sequences, and for event detection in extended videos of railroad crossings.

#index 1250161
#* Making argumentation more believable
#@ Anthony Hunter
#t 2004
#c 10
#% 167544
#% 330290
#% 337502
#% 1279226
#% 1279281
#! There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments. A problem with these proposals is that they do not consider the believability of the arguments from the perspective of the intended audience. In this paper, we start by reviewing a logic-based framework for argumentation based on argument trees which provide a way of exhaustively collating arguments and counter-arguments. We then extend this framework to it model-theoretic evaluation of the believability of arguments. This extension assumes that the beliefs of a typical member of the audience for argumentation can be represented by a set of classical formulae (a beliefbase). We compare a beliefbase with each argument to evaluate the empathy (or similarly the antipathy) that an agent has for the argument. We show how we can use empathy and antipathy to define a pre-ordering relation over argument trees that captures how nne argument tree is "more believable" than another. We also use these to define criteria for deciding whether an argument at the root of an argument tree is defeated or undeleated given the other arguments in the tree.

#index 1250162
#* Towards higher impact argumentation
#@ Anthony Hunter
#t 2004
#c 10
#% 130838
#% 184793
#% 306015
#% 337502
#! There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments, An example is the framework by Besnard and Hunter that is based on classical logic and in which an argument (obtained from a knowledgebase) is a pair where the first item is a minimal consistent set of formulae that proves the second item (which is a formula). In the framework, the only counter-arguments (defeaters) that need to be taken into account are canonical arguments (a form of minimal undercut). Argumem trees then provide a way of exhaustively collating arguments and counter-arguments. A problem with this set up is that some argument trees may be "too big" to have sufficient impact. In this paper, we address the need to increase the impact of argumentation by using pruned argument trees. We formalize this in terms of how arguments resonate with the intended audience of the arguments. For example, if a politician Wants to make a case for raising taxes, the arguments used would depend on what is important to the audience: Arguments based on increased taxes are needed to pay for improved healthcare would resonate better with an audience of pensioners, whereas arguments based on increased taxes are needtd to pay for improved transport infrastructure would resonate better with an audience of business executives. By analysing the resonance of arguments, we can prune argument trees to raise their impact.

#index 1250163
#* Loop formulas for circumscription
#@ Joohyung Lee;Fangzhen Lin
#t 2004
#c 10
#% 3035
#% 36550
#% 117869
#% 243712
#% 337497
#% 417649
#% 420607
#% 578673
#% 752744
#% 763743
#% 1250128
#% 1271999
#% 1478800
#! Clark's completion is a simple nonmonotonic formalism and a special case of many nonmonotonic logics. Recently there has been work on extending completion with "loop formulas" so that general cases of nonmonotonic logics such as logic programs (under the answer set semantics) and McCain-Turner causal logic can be characterized by propositional logic in the form of "comple(ion + loop formulas". In this paper, we show that the idea is applicable to McCarthy's circumscription in the propositional case. We also show how to embed propositional circumscription in logic programs and in causal logic, inspired by the uniform characterization of "completion + loop formulas".

#index 1250164
#* An instance-based state representation for network repair
#@ Michael L. Littman;Nishkam Ravi;Eitan Fenson;Rich Howard
#t 2004
#c 10
#% 252183
#% 252329
#% 384911
#% 425072
#% 464639
#% 466575
#% 495927
#% 702594
#% 1272369
#! We describe a formal framework for diagnosis and repair problems that shares elements of the well known partially observable MOP and cost-sensitive classification models. Our cost-sensitive fault remediation model is amenable to implementation as a reinforcement-learning system, and we describe an instance-based state representation that is compatible with learning and planning in this framework. We demonstrate a system that uses these ideas to learn to efficiently restore network connectivity after a failure.

#index 1250165
#* Logical foundations of negotiation: outcome, concession and adaptation
#@ Thomas Meyer;Norman Foo;Rex Kwok;Dongmo Zhang
#t 2004
#c 10
#% 263126
#% 342124
#% 781207
#% 1250169
#! This paper provides a logical framework for negotiation between agents that are assumed to be rational, cooperative and truthful. We present a characterisation of the permissible outcomes of a process of negotiation in terms of a set of rationality postulates, as well as a method for constructing exactly the rational outcomes. The framework is extended by describing two modes of negotiation from which an outcome can be reached. In the concessionary mode, agents are required to weaken their demands in order to accommodate the demands of others. In the adaptationist mode, agents are required to adapt to the demands of others in some appropriate fashion. Both concession and adaptation are characterised in terms of rationality postulates. We also provide methods for constructing exactly the rational concessions, as well as the rational adaptations. The central result of the paper is the observation that the outcomes obtained from the concessionary and adaptationist modes both correspond to the rational outcomes. We conclude by pointing out the links between negotiation and AGM belief change, and providing a glimpse of how this may be used to define a notion of preference-based negotiation.

#index 1250166
#* Repeated observation models
#@ Avi Pfeffer
#t 2004
#c 10
#% 292004
#% 292235
#% 400984
#% 1271978
#% 1272315
#% 1650767
#! Repetition is an important phenomenon in a variety of domains, such as music, computer programs and architectural drawings. A generative model for these domains should account for the possibility of repetition. We present repeated observation models (ROMs), a framework for modeling sequences that explicitly allows for repetition. In a ROM, an element is either generated by copying a previous element, or by using a base model. We show how to build ROMs using n- grams and hidden Markov models as the base model. We also describe an extension of ROMs in which entire subsequences are repeated together. Results from a music modeling domain show that ROMs can lead to dramatic improvement in predictive ability.

#index 1250167
#* Encoding probabilistic causal model in probabilistic action language
#@ Nam Tran;Chitta Baral
#t 2004
#c 10
#% 194652
#% 233132
#% 284644
#% 342119
#% 496256
#% 578731
#% 1478845
#% 1673000
#% 1673004
#! Pearl's probabilistic causal model has been used in many domains to reason about causality. Pearl's treatment of actions is very diffewnt from the way actions are represented explicitly in action languages. In this paper we show how to encode Pearl's probabilistic causal model in the action language PAL thus relating this two distinct approaches to reasoning about actions.

#index 1250168
#* Evaluating ontology cleaning
#@ Christopher Welty;Ruchi Mahindru;Jennifer Chu-Carroll
#t 2004
#c 10
#% 342984
#% 344391
#% 433821
#% 459487
#% 529524
#% 740888
#% 815146
#% 854659
#% 1279457
#! Ontology as a discipline of Computer Science has made many claims about its usefulness, however to date there has been very little evaluation of those claims. We present the results of an experiment using a hybrid search system with a significant knowledge-based component to measure, using precision and recall, the impact of improving the quality of an ontology on overall performance. We demonstrate that improving the ontology using OntoClean (Guarino and Welty, 2002)., does positively impact performance, and that having knowledge of the search domain is more effective than domain-knowledge-free search techniques such as link analysis.

#index 1250169
#* Negotiation as mutual belief revision
#@ Dongmo Zhang;Norman Foo;Thomas Meyer;Rex Kwok
#t 2004
#c 10
#% 224753
#% 263126
#% 422032
#% 443185
#% 460246
#% 557217
#% 557551
#% 781207
#% 782310
#% 1290096
#! This paper presents a logical framework for negotiation based on belief revision theory. We consider that a negotiation process is a course or multiple courses of mutual belief revision. A set of AGM-style postulates are proposed to capture the rationality of competitive and cooperative behaviors of negotiation. We first show that the AGM revision and its iterated extension is a special case of negotiation function. Then we show that a negotiation function can be constructed by two related iterated belief revision functions under a certain coordination mechanism. This provides a qualitative method for constructing negotiation space and rational concessions. It also shows glimpse of how to express game-theoretical concepts in logical framework.

#index 1250170
#* Hierarchical hidden Markov models with general state hierarchy
#@ Hung H. Bui;Dinh Q. Phung;Svetha Venkatesh
#t 2004
#c 10
#% 95730
#% 292235
#% 529347
#% 613384
#% 899483
#% 1279275
#% 1279398
#! The hierarchical hidden Markov model (HHMM) is an extension of the hidden Markov model to include a hierarchy of the hidden states. This form of hierarchical modeling has been found useful in applications such as handwritten character recognition, behavior recognition, video indexing, and text retrieval. Nevertheless, the state hierarchy in the original HHMM is restricted to a tree structure. This prohibits two different states from having the same child, and thus does not allow for sharing of common substructures in the model. In this paper, we present a general HHMM in which the state hierarchy can be a lattice allowing arbitrary sharing of substructures. Furthermore, we provide a method for numerical scaling to avoid underflow, an important issue in dealing with long observation sequences. We demonstrate the working of our method in a simulated environment where a hierarchical behavioral model is automatically learned and later used for recognition.

#index 1250171
#* An ensemble technique for stable learners with performance bounds
#@ Ian Davidson
#t 2004
#c 10
#% 73372
#% 209021
#% 376266
#% 466583
#% 729437
#% 1279286
#! Ensemble techniques such as bagging and DECORATE exploit the "instability" of learners, such as decision trees, to create a diverse set of models. However, creating a diverse set of models for stable learners such as naïve Bayes is difficult as they are relatively insensitive to training data changes. Furthermore, many popular ensemble techniques do not have a rigorous underlying theory and often provide no insight into how many models to build. We formally define stable learner as having a second order derivative of the posterior density function and propose an ensemble technique specifically for stable learners. Our ensemble technique, bootstrap model averaging, creates a number of bootstrap samples from the training data, builds a model from each and then sums the joint instance and class probability over all models built. We show that for stable learners our ensemble technique for infinite bootstrap samples approximates posterior model averaging (aka the optimal Bayes classifier (OBC)). For finite bootstrap samples we estimate the increase over the aBC error using Chebychev bounds. We empirically illustrate our approach's usefulness for several stable learners and verify our bound's correctness.

#index 1250172
#* On the optimality of probability estimation by random decision trees
#@ Wei Fan
#t 2004
#c 10
#% 136350
#% 214236
#% 400847
#% 458361
#% 459008
#% 727888
#! Random decision tree is an ensemble of decision trees. The feature at any node of a tree in the ensemble is chosen randomly from remaining features. A chosen discrete feature on a decision path cannot be chosen again. Continuous feature can be chosen multiple times, however, with a different splitting value each time. During classification, each tree outputs raw posterior probability. The probabilities from each tree in the ensemble are averaged as the final posterior probability estimate. Although remarkably simple and somehow counter-intuitive, random decision tree has been shown to be highly accurate under 0-1 loss and cost-sensitive loss functions. Preliminary explanation of its high accuracy is due to the "error-tolerance" property of probabilistic decision making. Our study has shown that the actual reason for random tree's superior performance is due to its optimal approximation to each example's true probability to be a member of a given class.

#index 1250173
#* Fibring neural networks
#@ Artur S. d'Avila Garcez;Dov M. Gabbay
#t 2004
#c 10
#% 61477
#% 90391
#% 90393
#% 92148
#% 175368
#% 198461
#% 301773
#% 322914
#% 341864
#% 356892
#% 376683
#% 379346
#% 383886
#% 409507
#% 418100
#% 418103
#% 427296
#% 532974
#! Neural-symbolic systems are hybrid systems that integrate symbolic logic and neural networks. The goal of neural-symbolic integration is to benefit from the combination of features of the symbolic and connectionist paradigms of artificial intelligence. This paper introduces a new neural network architecture based on the idea of fibring logical systems. Fibring allows one to combine different logical systems in a principled way. Fibred neural networks may be composed not only of interconnected neurons but also of other networks, forming a recursive architecture. A fibring function then defines how this recursive architecture must behave by defining how the networks in the ensemble relate to each other, typically by allowing the activation of neurons in one network (A) to influence the change of weights in another network (B). Intuitively, this can be seen as training network B at the same time that one runs network A. We show that, in addition to being universal approximators like standard feedforward networks, fibred neural networks can approximate any polynomial function to any desired degree of accuracy, thus being more expressive than standard feedforward networks.

#index 1250174
#* Learning and inferring transportation routines
#@ Lin Liao;Dieter Fox;Henry Kautz
#t 2004
#c 10
#% 235061
#% 280408
#% 394009
#% 528169
#% 716892
#% 723186
#% 1272356
#% 1279342
#% 1279398
#! This paper introduces a hierarchical Markov model that can learn and infer a user's daily movements through the community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user's mode of transportation or her goal. We apply Rao-Blackwellised particle filters for efficient inference both at the low level and at the higher levels of the hierarchy. Significant locations such as goals or locations where the user frequently changes mode of transportation are learned from GPS data logs without requiring any manual labeling. We show how to detect abnormal behaviors (e.g. taking a wrong bus) by concurrently tracking his activities with a trained and a prior model. Experiments show that our model is able to accurately predict the goals of a person and to recognize situations in which the user performs unknown activities.

#index 1250175
#* Learning and applying competitive strategies
#@ Esther Lock;Susan L. Epstein
#t 2004
#c 10
#% 1474
#% 156190
#% 286423
#% 449567
#% 711673
#% 715736
#% 746678
#% 1272292
#% 1272330
#! Learning reusable sequences can support the development of expertise in many domains, either by improving decision-making quality or decreasing execution speed. This paper introduces and evaluates a method to learn action sequences for generalized states from prior problem experience. From experienced sequences, the method induces the context that underlies a sequence of actions. Empirical results indicate that the sequences and contexts learned for a class of problems are actually those deemed important by experts for that particular class, and can be used to select appropriate action sequences when solving problems there.

#index 1250176
#* Bayesian network classifiers versus k-NN classifier using sequential feature selection
#@ Franz Pernkopf
#t 2004
#c 10
#% 44876
#% 177826
#% 227486
#% 243728
#% 246832
#% 302398
#% 388024
#% 723035
#% 729437
#! The aim of this paper is to compare Bayesian network classifiers to the k-NN classifier based on a subset of features. This subset is established by means of sequential feature selection methods. Experimental results show that Bayesian network classifiers more often achieve a better classification rate on different data sets than selective k-NN classifiers. The k-NN classifier performs well in the case where the number of samples for learning the parameters of the Bayesian network is small. Bayesian network classifiers outperform selective k- NN methods in terms of memory requirements and computational demands. This paper demonstrates the strength of Bayesian networks for classification.

#index 1250177
#* Online parallel boosting
#@ Jesse A. Reichler;Harlan D. Harris;Michael A. Savchenko
#t 2004
#c 10
#% 280496
#% 312728
#% 342628
#% 424997
#% 496419
#% 552042
#% 720011
#% 856941
#% 1042787
#% 1051514
#! This paper presents a new boosting (arcing) algorithm called POCA, Parallel Online Continuous Arcing. Unlike traditional boosting algorithms (such as Arc-x4 and Adaboost), that construct ensembles by adding and training weak learners sequentially on a round-by-round basis, training in POCA is performed over an entire ensemble continuously and in parallel. Since members of the ensemble are not frozen after an initial learning period (as in traditional boosting) POCA is able to adapt rapidly to non-stationary environments, and because POCA does not require the explicit scoring of a fixed exemplar set, it can perform online learning of non-repeating data. We present results from experiments conducted using neural network experts that show POCA is typically faster and more adaptive than existing boosting algorithms. Results presented for the UCI letter dataset are, to our knowledge, the best published scores to date.

#index 1250178
#* Bayesian inference on principal component analysis using reversible jump Markov chain Monte Carlo
#@ Zhihua Zhang;Kap Luk Chan;James T. Kwok;Dit-Yan Yeung
#t 2004
#c 10
#% 266411
#% 304879
#% 330125
#% 341451
#% 360691
#! Based on the probabilistic reformulation of principal component analysis (PCA), we consider the problem of determining the number of principal components as a model selection problem. We present a hierarchical model for probabilistic PCA and construct a Bayesian inference method for this model using reversible jump Markov chain Monte Carlo (MCMC). By regarding each principal component as a point in a one-dimensional space and employing only birth-death moves in our reversible jump methodology, our proposed method is simple and capable of automatically determining the number of principal components and estimating the parameters simultaneously under the same disciplined framework. Simulation experiments are performed to demonstrate the effectiveness of our MCMC method.

#index 1250179
#* Error detection and impact-sensitive instance ranking in noisy datasets
#@ Xingquan Zhu;Xindong Wu;Ying Yang
#t 2004
#c 10
#% 17144
#% 27068
#% 136350
#% 207198
#% 242235
#% 332913
#% 346340
#% 443491
#% 449588
#% 466236
#% 466249
#% 727901
#! Given a noisy dataset, how to locate erroneous instances and attributes and rank suspicious instances based on their impacts on the system performance is an interesting and important research issue. We provide in this paper an Error Detection and Impact-sensitive instance Ranking (EDIR) mechanism to address this problem. Given a noisy dataset D, we first train a benchmark classifier T from D. The instances, that cannot be effectively classified by T are treated as suspicious and forwarded to a subset S. For each attribute Ai, we switch Ai and the class label C to train a classifier APi for Ai. Given an instance Ik in S, we use APi and the benchmark classifier T to locate the erroneous value of each attribute Ai. To quantitatively rank instances in S, we define an impact measure based on the Information-gain Ratio (IR). We calculate IRi between attribute Ai and C, and use IRi as the impact-sensitive weight of Ai. The sum of impact-sensitive weights from all located erroneous attributes of Ik indicates its total impact value. The experimental results demonstrate the effectiveness of our strategies.

#index 1250180
#* Comparing cognitive and computational models of narrative structure
#@ David B. Christian;R. Michael Young
#t 2004
#c 10
#% 252806
#% 292074
#% 428286
#% 686680
#! A growing number of applications seek to incorporate automatically generated narrative structure into interactive virtual environments. In this paper, we evaluate a representation for narrative structure generated by an automatic planning system by 1) mapping the plans that control plot into conceptual graphs used by QUEST, an existing framework for question-answering analysis that includes structures for modeling a reader's narrative comprehension and 2) using methods originally employed by QUEST's developers to determine if the plan structures can serve as effective models of the understanding that human users form after viewing corresponding stories played out within a virtual world. Results from our analysis are encouraging, though additional work is required to expand the plan language to cover a broader class of narrative structure.

#index 1250181
#* Methods for domain-independent information extraction from the web: an experimental comparison
#@ Oren Etzioni;Michael Cafarella;Doug Downey;Ana-Maria Popescu;Tal Shaked;Stephen Soderland;Daniel S. Weld;Alexander Yates
#t 2004
#c 10
#% 179800
#% 240955
#% 246831
#% 252011
#% 278109
#% 283180
#% 301241
#% 311027
#% 312861
#% 330616
#% 348146
#% 504443
#% 754068
#% 756964
#% 815868
#% 815916
#% 854652
#% 1476317
#% 1673026
#! Our KNOWITALL system aims to automate the tedious process of extracting large collections of facts (e.g., names of scientists or politicians) from the Web in an autonomous, domain-independent, and scalable manner. In its first major run, KNOWITALL extracted over 50,000 facts with high precision, but suggested a challenge: How can we improve KNOWITALL's recall and extraction rate without sacrificing precision? This paper presents three distinct ways to address this challenge and evaluates their performance. Rule Learning learns domain-specific extraction rules. Subclass Extraction automatically identifies sub-classes in order to boost recall. List Extraction locates lists of class instances, learns a "wrapper" for each list, and extracts elements of each list. Since each method bootstraps from KNOWITALL's domain-independent methods, no hand-labeled training examples are required. Experiments show the relative coverage of each method and demonstrate their synergy. In concert, our methods gave KNOWITALL a 4-fold to 19-fold increase in recall, while maintaining high precision, and discovered 10,300 cities missing from the Tipster Gazetteer.

#index 1250182
#* Interpreting loosely encoded questions
#@ James Fan;Bruce Porter
#t 2004
#c 10
#% 21141
#% 283115
#% 341643
#% 389155
#% 723392
#% 747268
#% 835738
#% 1271861
#% 1279457
#% 1289277
#! Knowledge-based question-answering systems have become quite competent and robust at answering a wide range of questions in different domains, however in order to ask questions correctly, one needs to have intimate knowledge of the structure of the knowledge base, and typical users lack this knowledge. We address this problem by developing a system that uses the content of the knowledge base to automatically align a user's encoding of a query to the structure of the knowledge base. Our preliminary evaluation shows the system detects and corrects most misalignments, and users are able to pose most questions quickly.

#index 1250183
#* Learning indexing patterns from one language for the benefit of others
#@ Udo Hahn;Kornél Markó;Stefan Schulz
#t 2004
#c 10
#% 262048
#% 344447
#! Using language technology for text analysis and light-weight ontologies as a content-mediating level, we acquire indexing patterns from vast amounts of indexing data for English-language medical documents. This is achieved by statistically relating interlingual representations of these documents (based on text token bigrams) to their associated index terms. From these 'English' indexing patterns, we then induce the associated index terms for German and Portuguese documents when their interlingual representations match those of English documents. Thus, we learn from past English indexing experience and transfer it in an unsupervised way to non-English texts, without ever having seen concrete indexing data for languages other than English.

#index 1250184
#* Interactive information extraction with constrained conditional random fields
#@ Trausti Kristjansson;Aron Culotta;Paul Viola;Andrew McCallum
#t 2004
#c 10
#% 31391
#% 342529
#% 464434
#% 549447
#% 643004
#% 648984
#% 816181
#% 855102
#% 855119
#% 1264970
#% 1673026
#! Information Extraction methods can be used to automatically "fill-in" database forms from unstructured data such as Web documents or email. State-of-the-art methods have achieved low error rates but invariably make a number of errors. The goal of an interactive information extraction system is to assist the user in filling in database fields while giving the user confidence in the integrity of the data. The user is presented with an interactive interface that allows both the rapid verification of automatic field assignments and the correction of errors. In cases where there are multiple errors, our system takes into account user corrections, and immediately propagates these constraints such that other fields are often corrected automatically. Linear-chain conditional random fields (CRFs) have been shown to perform well for information extraction and other language modelling tasks due to their ability to capture arbitrary, overlapping features of the input in a Markov model. We apply this framework with two extensions: a constrained Viterbi decoding which finds the optimal field assignments consistent with the fields explicitly specified or corrected by the user; and a mechanism for estimating the confidence of each extracted field, so that low-confidence extractions can be highlighted. Both of these mechanisms are incorporated in a novel user interface for form filling that is intuitive and speeds the entry of data--providing a 23% reduction in error due to automated corrections.

#index 1250185
#* Identification and tracing of ambiguous names: discriminative and generative approaches
#@ Xin Li;Paul Morie;Dan Roth
#t 2004
#c 10
#% 201889
#% 577263
#% 729913
#% 740995
#% 815876
#% 855094
#% 870902
#! A given entity - representing a person, a location or an organization - may be mentioned in text in multiple, ambiguous ways. Understanding natural language requires identifying whether different mentions of a name, within and across documents, represent the same entity. We present two machine learning approaches to this problem, which we call the "Robust Reading" problem. Our first approach is a discriminative approach, trained in a supervised way. Our second approach is a generative model, at the heart of which is a view on how documents are generated and how names (of different entity types) are "sprinkled" into them. In its most general form, our model assumes: (1) a joint distribution over entities (e.g., a document that mentions "President Kennedy" is more likely to mention "Oswald" or "White House" than "Roger Clemens"), (2) an "author" model, that assumes that at least one mention of an entity in a document is easily identifiable, and then generates other mentions via (3) an appearance model, governing how mentions are transformed from tile "representative" mention. We show that both approaches perform very accurately, in the range of 90% - 95% F1 measure for different entity types, much better than previous approaches to (some aspects of) this problem. Our extensive experiments exhibit the contribution of relational and structural features and, somewhat surprisingly, that the assumptions made within our generative model are strong enough to yield a very powerful approach, that performs better than a supervised approach with limited supervised information.

#index 1250186
#* Text classification by labeling words
#@ Bing Liu;Xiaoli Li;Wee Sun Lee;Philip S. Yu
#t 2004
#c 10
#% 36672
#% 169717
#% 190581
#% 252011
#% 311027
#% 406493
#% 458379
#% 464291
#% 464604
#% 464631
#% 464641
#% 464777
#% 465754
#% 466888
#% 565531
#% 577235
#! Traditionally, text classifiers are built from labeled training examples. Labeling is usually done manually by human experts (or the users), which is a labor intensive and time consuming process. In the past few years, researchers investigated various forms of semi-supervised learning to reduce the burden of manual labeling. In this paper, we propose a different approach. Instead of labeling a set of documents, the proposed method labels a set of representative words for each class. It then uses these words to extract a set of documents for each class from a set of unlabeled documents to form the initial training set. The EM algorithm is then applied to build the classifier. The key issue of the approach is how to obtain a set of representative words for each class. One way is to ask the user to provide them, which is difficult because the user usually can only give a few words (which are insufficient for accurate learning). We propose a method to solve the problem. It combines clustering and feature selection. The technique can effectively rank the words in the unlabeled set according to their importance. The user then selects/labels some words from the ranked list for each class. This process requires less effort than providing words with no help or manual labelillg of documents. Our results show that the new method is highly effective and promising.

#index 1250187
#* On the relationship between lexical semantics and syntax for the inference of context-free grammars
#@ Tim Oates;Tom Armstrong;Justin Harris;Mark Nejman
#t 2004
#c 10
#% 31215
#% 112019
#% 145336
#% 289372
#% 311464
#% 464398
#% 464464
#% 466885
#% 503437
#% 711814
#% 714552
#! Context-free grammars cannot be identified in the limit from positive examples (Gold 1967), yet natural language grammars are more powerful than context-free grammars and humans learn them with remarkable ease from positive examples (Marcus 1993). Identifiability results for formal languages ignore a potentially powerful source of information available to learners of natural languages, namely, meanings. This paper explores the learnability of syntax (i.e. context-free grammars) given positive examples and knowledge of lexical semantics, and the learnability of lexical semantics given knowledge of syntax. The long-term goal is to develop an approach to learning both syntax and semantics that bootstraps itself, using limited knowledge about syntax to infer additional knowledge about semantics, and limited knowledge about semantics to infer additional knowledge about syntax.

#index 1250188
#* Distributed representation of syntactic structure by tensor product representation and non-linear compression
#@ Heidi H. T. Yeung;Peter W. M. Tsang
#t 2004
#c 10
#% 90391
#% 90393
#% 111449
#% 136369
#% 203678
#% 578551
#! Representing lexicons and sentences with the subsymbolic approach (using techniques such as Self Organizing Map (SOM) or Artificial Neural Network (ANN)) is a relatively new but important research area in natural language processing. The performance of this approach however, is highly dependent on whether representations are well formed so that members within each cluster are corresponding to sentences or phrases of similar meaning. Despite the moderate success and the rapid advancement of contemporary computing power, it is still difficult to establish an efficient learning method so that natural language can be represented in a way close to the benchmark exhibited by human beings. One of the major problems is due to the general lack of effective method(s) to encapsulate semantic information into quantitative expressions or structures. In this paper, we propose to alleviate this problem with a novel technique based on Tensor Product Representation and Non-linear Compression. The method is capable of encoding sentences into distributed representations that are closely associated with the semantic contents, being more comprehensible and analyzable from the perspective of human intelligence.

#index 1250189
#* Rapid object recognition from discriminative regions of interest
#@ Gerald Fritz;Christin Seifert;Lucas Paletta;Horst Bischof
#t 2004
#c 10
#% 136350
#% 188519
#% 212689
#% 317313
#% 350323
#% 457753
#% 457842
#% 457908
#% 635689
#% 1394017
#! Object recognition and detection represent a relevant component in cognitive computer vision systems, such as in robot vision, intelligent video surveillance systems, or multimodal interfaces. Object identification from local information has recently been investigated with respect to its potential for robust recognition, e.g., in case of partial object occlusions, scale variation, noise, and background clutter in detection tasks. This work contributes to this research by a thorough analysis of the discriminative power of local appearance patterns and by proposing to exploit local information content to model object representation and recognition. We identify discriminative regions in the object views from a posterior entropy measure, and then derive object models from selected discriminative local patterns. For recognition, we determine rapid attentive search for locations of high information content from learned decision trees. The recognition system is evaluated by various degrees of partial occlusion and Gaussian image noise, resulting in highly robust recognition even in the presence of severe occlusion effects.

#index 1250190
#* Automatically transforming symbolic shape descriptions for use in sketch recognition
#@ Tracy Hammond;Randall Davis
#t 2004
#c 10
#% 109079
#% 231306
#% 279107
#% 297614
#% 437306
#% 625272
#% 715113
#% 725508
#% 909381
#% 1279280
#! Sketch recognition systems are currently being developed for many domains, but can be time consuming to build if they are to handle the intricacies of each domain. This paper presents the first translator that takes symbolic shape descriptions (written in the LADDER sketch language) and automatically transforms them into shape recognizers, editing recognizers, and shape exhibitors for use in conjunction with a domain independent sketch recognition system. This transformation allows us to build a single domain independent recognition system that can be customized for multiple domains. We have tested our framework by writing several domain descriptions and automatically created a domain specific sketch recognition system for each domain.

#index 1250191
#* Large-scale map-making
#@ Kurt Konolige
#t 2004
#c 10
#% 28541
#% 418645
#% 469837
#! Current mapping algorithms using Consistent Pose Estimation (CPE) algorithms can successfully map areas of 104 square meters, using thousands of poses. However, the computation to construct the map grows as O(n log n), so larger maps get increasingly difficult to build. We present an abstraction method for postponing the growth in computation. This method solves a much smaller problem in the space of the connection graph of the map.

#index 1250192
#* A multi-resolution pyramid for outdoor robot terrain perception
#@ Michael Montemerlo;Sebastian Thrun
#t 2004
#c 10
#% 39654
#% 131463
#% 531449
#! This paper addresses the problem of outdoor terrain modeling for the purposes of mobile robot navigation. We propose an approach in which a robot acquires a set of terrain models at differing resolutions. Our approach addresses one of the major shortcomings of Bayesian reasoning when applied to terrain modeling, namely artifacts that arise from the limited spatial resolution of robot perception. Limited spatial resolution causes small obstacles to be detectable only at close range. Hence, a Bayes filter estimating the state of terrain segments must consider the ranges at which that terrain is observed. We develop a multi-resolution approach that maintains multiple navigation maps, and derive rational arguments for the number of layers and their resolutions. We show that our approach yields significantly better results in a practical robot system, capable of acquiring detailed 3-D maps in large-scale outdoor environments.

#index 1250193
#* Self-organizing visual maps
#@ Robert Sim;Gregory Dudek
#t 2004
#c 10
#% 28541
#% 60576
#% 234978
#% 332007
#% 418661
#% 578682
#% 724303
#% 1271848
#! This paper deals with automatically learning the spatial distribution of a set of images. That is, given a sequence of images acquired from well-separated locations, how can they be arranged to best explain their genesis? The solution to this problem can be viewed as an instance of robot mapping although it can also be used in other contexts. We examine the problem where only limited prior odometric information is available, employing a feature-based method derived from a probabilistic pose estimation framework. Initially, a set of visual features is selected from the images and correspondences are found across the ensemble. The images are then localized by first assembling the small subset of images for which odometric confidence is high, and sequentially inserting the remaining images, localizing each against the previous estimates, and taking advantage of any priors that are available. We present experimental results validating the approach, and demonstrating metrically and topologically accurate results over two large image ensembles. Finally, we discuss the results, their relationship to the autonomous exploration of an unknown environment, and their utility for robot localization and navigation.

#index 1250194
#* Reconstruction of 3D models from intensity images and partial depth
#@ Luz A. Torres-Méndez;Gregory Dudek
#t 2004
#c 10
#% 8153
#% 82595
#% 100252
#% 213502
#% 308489
#% 308601
#% 308635
#% 340384
#% 340385
#% 443998
#% 457637
#% 492405
#% 622176
#% 635735
#! This paper addresses the probabilistic inference of geometric structures from images. Specifically, of synthesizing range data to enhance the reconstruction of a 3D model of an indoor environment by using video images and (very) partial depth information. In our method, we interpolate the available range data using statistical inferences learned from the concurrently available video images and from those (sparse) regions where both range and intensity information is available. The spatial relationships between the variations in intensity and range can be efficiently captured by the neighborhood system of a Markov Random Field (MRF). In contrast to classical approaches to depth recovery (i.e. stereo, shape from shading), we can afford to make only weak prior assumptions regarding specific surface geometries or surface reflectance functions since we compute the relationship between existing range data and the images we start with. Experimental results show the feasibility of our method.

#index 1250195
#* Perceptually based learning of shape descriptions for sketch recognition
#@ Olya Veselova;Randall Davis
#t 2004
#c 10
#% 109079
#% 438383
#% 740157
#! We are interested in enabling a generic sketch recognition system that would allow more natural interaction with design tools in various domains, such as mechanical engineering, military planning, logic design, etc. We would like to teach the system the symbols for a particular domain by simply drawing an example of each one - as easy as it is to teach a person. Studies in cognitive science suggest that, when shown a symbol, people attend preferentially to certain geometric features. Relying on such biases, we built a system capable of learning descriptions of hand-drawn symbols from a single example. The generalization power is derived from a qualitative vocabulary reflecting human perceptual categories and a focus on perceptually relevant global properties of the symbol. Our user study shows that the system agrees with the subjects' majority classification about as often as any individual subject did.

#index 1250196
#* On the integration of grounding language and learning objects
#@ Chen Yu;Dana H. Ballard
#t 2004
#c 10
#% 85153
#% 120270
#% 227145
#% 266396
#% 300373
#% 349208
#% 420077
#% 457912
#% 578675
#% 578701
#% 668807
#% 1271929
#% 1860652
#! This paper presents a multimodal learning system that can ground spoken names of objects in their physical referents and learn to recognize those objects simultaneously from naturally co-occurring multisensory input. There are two technical problems involved: (1) the correspondence problem in symbol grounding - how to associate words (symbols) with their perceptually grounded meanings from multiple cooccurrences between words and objects in the physical environment. (2) object learning - how to recognize and categorize visual objects. We argue that those two problems can be fundamentally simplified by considering them in a general system and incorporating the spatio-temporal and cross-modal constraints of multimodal data. The system collects egocentric data including image sequences as well as speech while users perform natural tasks. It is able to automatically infer the meanings of object names from vision, and categorize objects based on teaching signals potentially encoded in speech. The experimental results reported in this paper reveal the effectiveness of using multimodal data and integrating heterogeneous techniques in machine learning, natural language processing and computer vision.

#index 1250197
#* Generating safe assumption-based plans for partially observable, nondeterministic domains
#@ Alexandre Albore;Piergiorgio Bertoli
#t 2004
#c 10
#% 262737
#% 1279265
#! Reactive planning using assumptions is a well-known approach to tackle complex planning problems for nondeterministic, partially observable domains. However, assumptions may be wrong; this may cause an assumption-based plan to fail. In general, it is not possible to decide at runtime whether an assumption has failed and is putting at danger the success of the plan; thus, plan execution has to be controlled taking into account every possible success-endangering assumption failure. The possibility of tracing such failures strongly depends on the actions performed by the plan. In this paper, focusing on a simple assumption language, we provide two main contributions. First, we formally characterize safe assumption-based plans, i.e. plans that not only succeed whenever the assumption holds, but also guarantee that any success-endangering assumption failure is traced by a suitable monitor. In this way, replanning may be triggered only when actually needed. Second, we extend the planner in a reactive platform in order to produce safe assumption-based plans. We experimentally show that safe assumption-based (re)planning is a good alternative to its unsafe counterpart, minimizing the need for replanning while retaining the efficiency in plan generation.

#index 1250198
#* Regrets only! online stochastic optimization under time constraints
#@ Russell Bent;Pascal Van Hentenryck
#t 2004
#c 10
#% 794018
#% 1673031
#! This paper considers online stochastic optimization problems where time constraints severely limit the number of offline optimizations which can be performed at decision time and/or in between decisions. It proposes a novel approach which combines the salient features of the earlier approaches: the evaluation of every decision on all samples (expectatio0n) and the ability to avoid distributing the samples among decisions (consensus). The key idea underlying the novel algorithm is to approximate the regret of a decision d. The regret algorithm is evaluated on two fundamentally different applications: online packet scheduling in networks and online multiple vehicle routing with time windows. On both applications, it produces significant benefits over prior approaches.

#index 1250199
#* Assessing the complexity of plan recognition
#@ Christopher W. Geib
#t 2004
#c 10
#% 147680
#% 179879
#% 233132
#% 354301
#% 516330
#% 567880
#% 1279398
#% 1279469
#% 1650293
#% 1650593
#! This paper presents a discussion of the theoretical complexity of plan recognition on the basis of an analysis of the number of explanations that any complete plan recognition algorithm must consider given various properties of the plan library. On the basis of these results it points out properties of plan libraries that make them computationally expensive.

#index 1250200
#* Forward-chaining planning in nondeterministic domains
#@ Ugur Kuter;Dana Nau
#t 2004
#c 10
#% 124601
#% 224480
#% 243697
#% 266386
#% 266387
#% 296170
#% 417703
#% 529665
#% 544930
#% 544943
#% 655322
#% 655323
#% 1271962
#% 1272019
#% 1272287
#! In this paper, we present a general technique for taking forward-chaining planners for deterministic domains (e.g., HSP, TLPlan, TALplanner, and SHOP2) and adapting them to work in nondeterministic domains. Our results suggest that our technique preserves many of the desirable properties of these planners, such as the ability to use heuristic techniques to achieve highly efficient planning. In our experimental studies on two problem domains, the well-known MBP algorithm took exponential time, confirming prior results by others. A nondeterminized version of SHOP2 took only polynomial time. The polynomial-time figures are confirmed by a complexity analysis, and a similar complexity analysis shows that a nondeterminized version of TLPlan would perform similarly.

#index 1250201
#* Transport logistics planning with service-level constraints
#@ Hoong Chuin Lau;Kien Ming Ng;Xiaotao Wu
#t 2004
#c 10
#% 25240
#% 25998
#% 36412
#% 43567
#% 129010
#% 175426
#% 216128
#% 369236
#% 686757
#% 1080819
#! In this paper, we study a logistics problem arising in military transport planning. A military organization operates a large fleet of vehicles in a depot to serve the requests of various operational units. Each request has a fixed start and end time, and is served by a prescribed number of vehicles. We address the following two problems: (1) how many vehicles are at least needed to meet a given service level of requests; and (2) suppose we allow each request to shift its start time by a constant duration, call all the requests be met? A Niche genetic algorithm, together with a hybridized variant, are applied to the problem.

#index 1250202
#* Distance estimates for planning in the discrete belief space
#@ Jussi Rintanen
#t 2004
#c 10
#% 224480
#% 252608
#% 266386
#% 337980
#% 1289211
#! We present a general framework for studying heuristics for planning in the belief space. Earlier work has focused on giving implementations of heuristics that work well on benchmarks, without studying them at a more analytical level. Existing heuristics have evaluated belief states in terms of their cardinality or have used distance heuristics directly based on the distances in the underlying state space. Neither of these types of heuristics is very widely applicable: often goal belief state is not approached through a sequence of belief states with a decreasing cardinality, and distances in the state space ignore the main implications of partial observability. To remedy these problems we present a family of admissible, increasingly accurate distance heuristics for planning in the belief space, parameterized by an integer n. We show that the family of heuristics is theoretically robust: it includes the simplest heuristic based on the state space as a special case and as a limit the exact distances in the belief space.

#index 1250203
#* Continuous time in a SAT-based planner
#@ Ji-Ae Shin;Ernest Davis
#t 2004
#c 10
#% 1116
#% 131357
#% 179938
#% 224480
#% 420767
#% 496111
#% 496277
#% 559310
#% 743416
#% 777662
#% 1271884
#% 1272008
#% 1476298
#! The TM-LPSAT planner can construct plans in domains containing atomic actions and durative actions; events and processes; discrete, real-valued, and interval-valued fluents; and continuous linear change to quantities. It works in three stages. In the first stage, a representation of the domain and problem in an extended version of PDDL+ is compiled into a system of propositional combinations of propositional variables and linear constraints over numeric variables. In the second stage, the LPSAT constraint engine (Wolfman & Weld 2000) is used to find a solution to the system of constraints. In the third stage, a correct parallel plan is extracted from this solution. We discuss the structure of the planner and show how a real-time temporal model is compiled into LPSAT constraints.

#index 1250204
#* Analogical path planning
#@ Saul Simhon;Gregory Dudek
#t 2004
#c 10
#% 367254
#% 369820
#% 376266
#% 1290038
#% 1499594
#! We present a probabilistic method for path planning that considers trajectories constrained by both the environment and an ensemble of restrictions or preferences on preferred motions for a moving robot. Our system learns constraints and preference biases on a robot's motion from examples, and then synthesizes behaviors that satisfy these constraints. This behavior can encompass motions that satisfy diverse requirements such as a sweep pattern for floor coverage, or, in particular in our experiments, satisfy restrictions on the robot's physical capabilities such as restrictions on its turning radius. Given an approximate path that may not satisfy the required conditions, our system computes a refined path that satisfies the constraints and also avoids obstacles. Our approach is based on a Bayesian framework for combining a prior probability distribution on the trajectory with environmental constraints. The prior distribution is generated by decoding a Hidden Markov Model, which is itself is trained over a particular set of preferred motions. Environmental constraints are modeled using a potential field over the configuration space. This paper poses the requisite theoretical framework and demonstrates its effectiveness with several examples.

#index 1250205
#* An effective algorithm for project scheduling with arbitrary temporal constraints
#@ Tristan B. Smith;John M. Pyle
#t 2004
#c 10
#% 69607
#% 107137
#% 421314
#% 453072
#% 535149
#% 578656
#% 739265
#% 746720
#% 1272400
#! The resource-constrained project scheduling problem with time windows (RCPSP/max) is an important generalization of a number of well studied scheduling problems. In this paper, we present a new heuristic algorithm that combines the benefits of squeaky wheel optimization with an effective conflict resolution mechanism, called bulldozing, to address RCPSP/max problems. On a range of benchmark problems, the algorithm is competitive with state-of-the-art systematic and non-systematic methods and scales well.

#index 1250206
#* Shortest path discovery problems: a framework, algorithms and experimental results
#@ Csaba Szepesvári
#t 2004
#c 10
#% 68238
#% 211658
#% 303309
#% 529808
#! In this paper we introduce and study Shortest Path Discovery (SPD) problems, a generalization of shortest path problems: In SPD one is given a directed edge-weighted graph and the task is to find a the shortest path for fixed source and target nodes such that initially the edge-weights are unknown, but they can be queried. Querying the cost of an edge is expensive and hence the goal is to minimize the total number of edge cost queries executed. In this article we characterize some common properties of sound SPD algorithms, propose a particular algorithm that is shown to be sound and effective. Experimental results on real-world OCR task demonstrate the usefulness of the approach whereas the proposed algorithm is shown to yield a substantial speed-up of the recognition process.

#index 1250207
#* Regression with respect to sensing actions and partial states
#@ Le-Chi Tuan;Chitta Baral;Xin Zhang;Tran Cao Son
#t 2004
#c 10
#% 34011
#% 124601
#% 266384
#% 266387
#% 318489
#% 322911
#% 337980
#% 345431
#% 572366
#% 1272287
#% 1272399
#% 1476290
#! In this paper, we present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions. We consider binary domains, and employ the 0-approximation (Son & Baral 2001) to define the regression function. In binary domains, the use of 0-approximation means using 3-valued states. Although planning using this approach is incomplete with respect to the full semantics, we adopt it to have a lower complexity. We prove the soundness and completeness of our regression formulation with respect to the definition of progression and develop a conditional planner that utilizes our regression function.

#index 1250208
#* Effective approaches for partial satisfaction (over-subscription) planning
#@ Menkes van den Briel;Romeo Sanchez;Minh B. Do;Subbarao Kambhampati
#t 2004
#c 10
#% 167629
#% 224480
#% 344878
#% 345431
#% 495768
#% 496111
#% 1271962
#% 1272006
#% 1272007
#% 1272014
#% 1272392
#% 1279385
#! In many real world planning scenarios, agents often do not have enough resources to achieve all of their goals. Consequently, they are forced to find plans that satisfy only a subset of the goals. Solving such partial satisfaction planning (PSP) problems poses several challenges, including an increased emphasis on modeling and handling plan quality (in terms of action costs and goal utilities). Despite the ubiquity of such PSP problems, very little attention has been paid to them in the planning community. In this paper, we start by describing a spectrum of PSP problems and focus on one of the more general PSP problems, termed PSP NET BENEFIT. We develop three techniques, (i) one based on integer programming, called OptiPlan, (ii) the second based on regression planning with reachability heuristics, called AltAltps, and (iii) the third based on anytime heuristic search for a forward state-space heuristic planner, called Sapaps. Our empirical studies with these planners show that the heuristic planners generate plans that are comparable to the quality of plans generated by OptiPlan, while incurring only a small fraction of the cost.

#index 1250209
#* Branching and pruning: an optimal temporal POCL planner based on constraint programming
#@ Vincent Vidal;Héctor Geffner
#t 2004
#c 10
#% 63222
#% 107137
#% 126389
#% 194651
#% 283216
#% 283220
#% 305372
#% 391417
#% 453072
#% 495772
#% 496111
#% 544766
#% 743461
#% 1272007
#% 1272008
#% 1272020
#% 1289210
#% 1290109
#% 1290110
#% 1476297
#% 1478840
#! A key feature of modern optimal planners such as Graphplan and Blackbox is their ability to prune large parts of the search space. Previous Partial Order Causal Link (POCL) planners provide an alternative branching scheme but lacking comparable pruning mechanisms do not perform as well. In this paper, a domain-independent formulation of temporal planning based on Constraint Programming is introduced that successfully combines a POCL branching scheme with powerful and sound pruning rules. The key novelty in the formulation is the ability to reason about supports, precedences, and causal links involving actions that are not in the plan. Experiments over a wide range of benchmarks show that the resulting optimal temporal planner is much faster than current ones and is competitive with the best parallel planners in the special case in which actions have all the same duration.

#index 1250210
#* High-level goal recognition in a wireless LAN
#@ Jie Yin;Xiaoyong Chai;Qiang Yang
#t 2004
#c 10
#% 147680
#% 266397
#% 401172
#% 423981
#% 567880
#% 613334
#% 622181
#% 716892
#% 748738
#% 1113048
#% 1279397
#% 1290117
#! Plan recognition has traditionally been developed for logically encoded application domains with a focus on logical reasoning. In this paper, we present an integrated plan-recognition model that combines low-level sensory readings with high-level goal inference. A two-level architecture is proposed to infer a user's goals in a complex indoor environment using an RF-based wireless network. The novelty of our work derives from our ability to infer a user's goals from sequences of signal trajectory, and the ability for us to make a trade-off between model accuracy and inference efficiency. The model relies on a dynamic Bayesian network to infer a user's actions from raw signals, and an N-gram model to infer the users' goals from actions. We present a method for constructing the model from the past data and demonstrate the effectiveness of our proposed solution through empirical studies using some real data that we have collected.

#index 1250211
#* Spatial aggregation for qualitative assessment of scientific computations
#@ Chris Bailey-Kellogg;Naren Ramakrishnan
#t 2004
#c 10
#% 55385
#% 65345
#% 109848
#% 109856
#% 335889
#% 341407
#% 529503
#% 1272294
#% 1289153
#% 1499542
#% 1776221
#! Qualitative assessment of scientific computations is an emerging application area that applies a data-driven approach to characterize, at a high level, phenomena including conditioning of matrices, sensitivity to various types of error propagation, and algorithmic convergence behavior. This paper develops a spatial aggregation approach that formalizes such analysis in terms of model selection utilizing spatial structures extracted from matrix perturbation datasets. We focus in particular on the characterization of matrix eigenstructure, both analyzing sensitivity of computations with spectral portraits and determining eigenvalue multiplicity with Jordan portraits. Our approach employs spatial reasoning to overcome noise and sparsity by detecting mutually reinforcing interpretations, and to guide subsequent data sampling. It enables quantitative evaluation of properties of a scientific computation in terms of confidence in a model, explainable in terms of the sampled data and domain knowledge about the underlying mathematical structure. Not only is our methodology more rigorous than the common approach of visual inspection, but it also is often substantially more efficient, due to well-defined stopping criteria. Results show that the mechanism efficiently samples perturbation space and successfully uncovers high level properties of matrices.

#index 1250212
#* A qualitative-quantitative methods-based e-learning support system in economic education
#@ Tokuro Matsuo;Takayuki Ito;Toramatsu Shintani
#t 2004
#c 10
#% 174161
#% 624248
#% 624309
#% 741446
#! This paper describes a new qualitative-quantitative simulator to help buyers learn how to make decisions when they purchase goods. In this paper, we propose an e-learning support system (LSDM) for assisting user decision making by applying artificial intelligence technology. When buyers purchase expensive items, they must carefully select these items from many alternatives. The learning support system provides useful information that assists consumers in purchasing goods. We employ qualitative simulations because the output simulation results are useful. Our system consists of a qualitative processing system and a quantitative calculation system. When users use the system, they first input information on the goods they want to purchase. The information input by users is used in the qualitative simulation. Next, they supply the details of their budgets, the rate of loans, and several other factors, on a form. The system then integrates the results of simulation and the user's input data and proposes plans to aid in their decision process. The system has several advantages: it can be used by simple input, the process of simulation is easy to understand, users can learn how to make decisions by trial and error, and the users can base their decision making on synthetic results.

#index 1250213
#* Skill acquisition and use for a dynamically-balancing soccer robot
#@ Brett Browning;Ling Xu;Manuela Veloso
#t 2004
#c 10
#% 229931
#% 334329
#% 357083
#% 443798
#% 643109
#% 643210
#% 732418
#% 858084
#! Dynamically-balancing robots have recently been made available by Segway LLC, in the form of the Segway RMP (Robot Mobility Platform). We have addressed the challenge of using these RMP robots to play soccer, building up upon our extensive previous work in this multi-robot research domain. In this paper, we make three contributions. First, we present a new domain, called Segway Soccer, for investigating the coordination of dynamically formed, mixed human-robot teams within the realm of a team task that requires real-time decision making and response. Segway Soccer is a game of soccer between two teams consisting of both Segway riding humans and Segway RMPs. We believe Segway Soccer is the first game involving both humans and robots in cooperative roles and with similar capabilities. In conjunction with this new domain, we present our work towards developing a soccer playing robot using the RMP platform with vision as its primary sensor. Our third contribution is that of skill acquisition from a human teacher, where the learned skill is then used seamlessly during robot execution as part of its control hierarchy. Skill acquisition and use addresses the challenge or rapidly developing the low-level actions that are environment dependent and are not transferable across robots.

#index 1250214
#* Common sense data acquisition for indoor mobile robots
#@ Rakesh Gupta;Mykel J. Kochenderfer
#t 2004
#c 10
#% 198058
#% 274913
#% 457139
#% 672546
#! Common sense knowledge can be efficiently collected from non-experts over the web in a similar fashion to the Open Mind family of distributed knowledge capture projects. We describe the collection of common sense data through the Open Mind Indoor Common Sense (OMICS) website. We restrict the domain to indoor home and office environments to obtain dense knowledge. The knowledge was collected through sentence templates that were generated dynamically based on previous user input. Entries were converted into relations and saved into a database. We discuss the results of this online collaborative effort and describe two applications of the collected data to indoor mobile robots. We discuss active desire selection based on current beliefs and commands and a room-labeling application based on probability estimates from the common sense knowledge base.

#index 1250215
#* Machine learning for fast quadrupedal locomotion
#@ Nate Kohl;Peter Stone
#t 2004
#c 10
#% 36160
#% 207535
#% 465907
#% 466895
#% 505097
#% 1272385
#! For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform.

#index 1250216
#* Interleaving temporal planning and execution in robotics domains
#@ Solange Lemai;Félix Ingrand
#t 2004
#c 10
#% 100136
#% 172505
#% 194654
#% 262737
#% 544932
#% 1289215
#! Many autonomous systems such as mobile robots, UAVs or spacecraft, have limited resource capacities and move in dynamic environments. Performing on-board mission planning and execution in such a context requires deliberative capabilities to generate plans achieving mission goals while respecting deadlines and resource constraints, as well as run-time plan adaption mechanisms during execution. In this paper we propose a framework to integrate deliberative planning, plan repair and execution control in a dynamic environment with stringent temporal constraints. It is based on lifted partial order temporal planning techniques which produce flexible plans and allow, under certain conditions discussed in the paper, plan repair interleaved with plan execution. This framework has been implemented using the IXTET planner and used to control a robotic platform.

#index 1250217
#* Reinforcement learning for a CPG-driven biped robot
#@ Takeshi Mori;Yutaka Nakamura;Masa-Aki Sato;Shin Ishii
#t 2004
#c 10
#% 124687
#% 203596
#% 305093
#% 384911
#% 393786
#% 716656
#! Animal's rhythmic movements such as locomotion are considered to be controlled by neural circuits called central pattern generators (CPGs). This article presents a reinforcement learning (RL) method for a CPG controller, which is inspired by the control mechanism of animals. Because the CPG controller is an instance of recurrent neural networks, a naive application of RL involves difficulties. In addition, since state and action spaces of controlled systems are very large in real problems such as robot control, the learning of the value function is also difficult. In this study, we propose a learning scheme for a CPG controller called a CPG-actor-critic model, whose learning algorithm is based on a policy gradient method. We apply our RL method to autonomous acquisition of biped locomotion by a biped robot simulator. Computer simulations show our method is able to train a CPG controller such that the learning process is stable.

#index 1250218
#* Advice generation from observed execution: abstract Markov decision process learning
#@ Patrick Riley;Manuela Veloso
#t 2004
#c 10
#% 103865
#% 147680
#% 203611
#% 224762
#% 302097
#% 431471
#% 477296
#% 552937
#% 1273677
#% 1650399
#! An advising agent, a coach, provides advice to other agents about how to act. In this paper we contribute an advice generation method using observations of agents acting in an environment. Given an abstract state definition and partially specified abstract actions, the algorithm extracts a Markov Chain, infers a Markov Decision Process, and then solves the MDP (given an arbitrary reward signal) to generate advice. We evaluate our work in a simulated robot soccer environment and experimental results show improved agent performance when using the advice generated from the MDP for both a sub-task and the full soccer game.

#index 1250219
#* Compressing pattern databases
#@ Ariel Felner;Ram Meshulam;Robert C. Holte;Richard E. Korf
#t 2004
#c 10
#% 348576
#% 529516
#% 1250221
#% 1478838
#! A pattern database is a heuristic function stored as a lookup table which stores the lengths of optimal solutions for instances of subproblems. All previous pattern databases had a distinct entry in the table for each subproblem instance. In this paper we investigate compressing pattern databases by merging several adjacent entries into one, thereby allowing the use of pattern databases that exceed available memory in their uncompressed form. We show that since adjacent entries are highly correlated, much of the information is preserved. Experiments on the sliding tile puzzles and the 4-peg Towers of Hanoi puzzle show that, for a given amount of memory, search time is reduced by up to 3 orders of magnitude by using compressed pattern databases.

#index 1250220
#* A general solution to the graph history interaction problem
#@ Akihiro Kishimoto;Martin Müller
#t 2004
#c 10
#% 159245
#% 216974
#% 287703
#% 322121
#% 690802
#% 1348981
#! Since the state space of most games is a directed graph, many game-playing systems detect repeated positions with a transposition table. This approach can reduce search effort by a large margin. However, it suffers from the so-called Graph History Interaction (GHI) problem, which causes errors in games containing repeated positions. This paper presents a practical solution to the GHI problem that combines and extends previous techniques. Because our scheme is general, it is applicable to different game tree search algorithms and to different domains. As demonstrated with the two algorithms αβ and df-pn in the two games checkers and Go, our scheme incurs only a very small overhead, while guaranteeing the correctness of solutions.

#index 1250221
#* Best-first frontier search with delayed duplicate detection
#@ Richard E. Korf
#t 2004
#c 10
#% 2194
#% 178285
#% 387508
#% 496261
#% 529516
#% 541474
#% 1250219
#% 1279391
#% 1279478
#! Best-first search is limited by the memory needed to store the Open and Closed lists, primarily to detect duplicate nodes. Magnetic disks provide vastly more storage, but random access of a disk is extremely slow. Instead of checking generated nodes immediately against existing nodes in a hash table, delayed duplicate detection (DDD) appends them to a file, then periodically removes the duplicate nodes using only sequential disk accesses. Frontier search saves storage in a best-first search by storing only the Open list and not the Closed list. The main contributions of this paper are to provide a scalable implementation of DDD, to combine it with frontier search, and to extend it to more general best-first searches such as A*. We illustrate these ideas by performing complete breadth-first searches of sliding-tile puzzles up to the 3×5 Fourteen Puzzle. For the 4-peg Towers of Hanoi problem, we perform complete searches with up to 20 disks, searching a space of over a trillion nodes, and discover a surprising anomaly concerning the problem-space diameter of the 15 and 20-disk problems. We also verify the presumed optimal solution lengths for up to 24 disks. In addition, we implement A* with DDD on the Fifteen Puzzle. Finally, we present a scalable implementation of DDD based on hashing rather than sorting.

#index 1250222
#* Temperature discovery search
#@ Martin Müller;Markus Enzenberger;Jonathan Schaeffer
#t 2004
#c 10
#% 60140
#% 306266
#% 496102
#! Temperature Discovery Search (TDS) is a new minimax-based game tree search method designed to compute or approximate the temperature of a combinatorial game. TDS is based on the concept of an enriched environment, where a combinatorial game G is embedded in an environment consisting of a large set of simple games of decreasing temperature. Optimal play starts in the environment, but eventually must switch to G. TDS finds the temperature of G by determining when this switch must happen. Both exact and heuristic versions of TDS are described and evaluated experimentally. In experiments with sum games in Amazons, TDS outperforms an αβ searcher.

#index 1250223
#* Simple search methods for finding a Nash equilibrium
#@ Ryan Porter;Eugene Nudelman;Yoav Shoham
#t 2004
#c 10
#% 30034
#% 338466
#% 644201
#% 773295
#% 1279321
#! We present two simple search methods for computing a sample Nash equilibrium in a normal-form game: one for 2- player games and one for n-player games. We test these algorithms on many classes of games, and show that they perform well against the state of the art- the Lemke-Howson algorithm for 2-player games, and Simplicial Subdivision and Govindan-Wilson for n-player games.

#index 1250224
#* Towards efficient sampling: exploiting random walk strategies
#@ Wei Wei;Jordan Erenrich;Bart Selman
#t 2004
#c 10
#% 8387
#% 109572
#% 125556
#% 205391
#% 361100
#% 388154
#% 420743
#% 496111
#% 529186
#% 535147
#% 593947
#% 593952
#% 601159
#% 1478779
#% 1650391
#! From a computational perspective, there is a close connection between various probabilistic reasoning tasks and the problem of counting or sampling satisfying assignments of a propositional theory. We consider the question of whether state-of-the-art satisfiability procedures, based on random walk strategies, can be used to sample uniformly or nearuniformly from the space of satisfying assignments. We first show that random walk SAT procedures often do reach the full set of solutions of complex logical theories. Moreover, by interleaving random walk steps with Metropolis transitions, we also show how the sampling becomes near-uniform.

#index 1250225
#* Space-efficient memory-based heuristics
#@ Rong Zhou;Eric A. Hansen
#t 2004
#c 10
#% 46465
#% 268042
#% 337980
#% 348576
#% 529180
#% 529508
#% 578766
#% 1279389
#% 1478838
#% 1499544
#! A memory-based heuristic is a heuristic function that is stored in a lookup table. Very accurate heuristics have been created by building very large lookup tables, sometimes called pattern databases. Most previous work assumes that a memory-based heuristic is computed for the entire state space, and the cost of computing it is amortized over many problem instances. But in some cases, it may be useful to compute a memory-based heuristic for a single problem instance. If the start and goal states of the problem instance are used to restrict the region of the state space for which the heuristic is needed, the time and space used to compute the heuristic may be substantially reduced. In this paper, we review recent work that uses this idea to compute space-efficient heuristics for the multiple sequence alignment problem. We then describe a novel development of this idea that is simpler and more general. Our approach leads to improved performance in solving the multiple sequence alignment problem, and is general enough to apply to other domains.

#index 1250226
#* Structured duplicate detection in external-memory graph search
#@ Rong Zhou;Eric A. Hansen
#t 2004
#c 10
#% 1156
#% 2194
#% 41684
#% 217077
#% 282771
#% 529516
#% 548494
#% 567949
#% 728026
#% 1279389
#% 1279478
#! We consider how to use external memory, such as disk storage, to improve the scalability of heuristic search in state-space graphs. To limit the number of slow disk I/O operations, we develop a new approach to duplicate detection in graph search that localizes memory references by partitioning the search graph based on an abstraction of the state space, and expanding the frontier nodes of the graph in an order that respects this partition. We demonstrate the effectiveness of this approach both analytically and empirically.

#index 1250227
#* Stochastic local search for POMDP controllers
#@ Darius Braziunas;Craig Boutilier
#t 2004
#c 10
#% 171032
#% 179940
#% 252330
#% 464448
#% 578692
#% 706380
#% 1279358
#% 1650313
#% 1650314
#% 1650588
#% 1650702
#! The search for finite-state controllers for partially observable Markov decision processes (POMDPs) is often based on approaches like gradient ascent, attractive because of their relatively low computational cost. In this paper, we illustrate a basic problem with gradient-based methods applied to POMDPs, where the sequential nature of the decision problem is at issue, and propose a new stochastic local search method as an alternative. The heuristics used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming (DP) approaches. We show that our algorithm consistently finds higher quality controllers than gradient ascent, and is competitive with (and, for some problems, superior to) other state-of-the-art controller and DP-based algorithms on large-scale POMDPs.

#index 1250228
#* A computational study of the Kemeny rule for preference aggregation
#@ Andrew Davenport;Jayant Kalagnanam
#t 2004
#c 10
#% 330769
#% 1272396
#! We consider from a computational perspective the problem of how to aggregate the ranking preferences of a number of alternatives by a number of different voters into a single consensus ranking, following the majority voting rule. Social welfare functions for aggregating preferences in this way have been widely studied since the time of Condorcet (1785). One drawback of majority voting procedures when three or more alternatives are being ranked is the presence of cycles in the majority preference relation. The Kemeny order is a social welfare function whicll has been designed to tackle the presence of such cycles. However computing a Kemeny order is known to be NP-hard. We develop a greedy heuristic and an exact branch and bound procedure for computing Kemeny orders. We present results of a computational study on these procedures.

#index 1250229
#* PROBCONS: probabilistic consistency-based multiple alignment of amino acid sequences
#@ Chuong B. Do;Michael Brudno;Serafim Batzoglou
#t 2004
#c 10
#% 46465
#% 136313
#% 564979
#! Obtaining an accurate multiple alignment of protein sequences is a difficult computational problem for which many heuristic techniques sacrifice optimality to achieve reasonable running times. The most commonly used heuristic is progressive alignment, which merges sequences into a multiple alignment by pairwise comparisons along the nodes of a guide tree. To improve accuracy, consistency-based methods take advantage of conservation across many sequences to provide a stronger signal for pairwise comparisons. In this paper, we introduce the concept of probabilistic consistency for multiple sequence alignments. 'We also present PROBCONS, an HMM-based protein muhiple sequence aligner, based on an approximation of the probabilistic consistency objective function. On the BAliBASE benchmark alignment database, PROBCONS demonstrates a statistically significant improvement in accuracy compared to several leading alignment programs while maintaining practical running times. Source code and program updates are freely available under the GNU Public License at http://probcons.stanford.edu/.

#index 1250230
#* Dynamic programming for partially observable stochastic games
#@ Eric A. Hansen;Daniel S. Bernstein;Shlomo Zilberstein
#t 2004
#c 10
#% 176299
#% 223835
#% 233137
#% 252183
#% 450852
#% 496272
#% 527987
#% 528153
#% 643084
#% 722895
#% 734918
#% 1279314
#% 1650588
#% 1650702
#! We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination or dominated strategies in normal form games. We prove that when applied to finite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without first forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to find an optimal solution. We present preliminary empirical results and discuss ways to further exploit POMDP theory in solving POSGs.

#index 1250231
#* Solving concurrent Markov decision processes
#@  Mausam;Daniel S. Weld
#t 2004
#c 10
#% 181627
#% 224480
#% 265807
#% 272665
#% 337981
#% 361730
#% 528302
#% 1272006
#% 1272015
#% 1290042
#% 1650355
#! Typically, Markov decision problems (MDPs) assume a single action is executed per decision epoch, but in the real world one may frequently execute certain actions in parallel. This paper explores concurrent MDPs, MDPs which allow multiple non-conflicting actions to be executed simultaneously, and presents two new algorithms. Our first approach exploits two provably sound pruning rules, and thus guarantees solution optimality. Our second technique is a fast, sampling-based algorithm, which produces c1ose-to-optimal solutions extremely quickly. Experiments show that our approaches outperform the existing algorithms producing up to two orders of magnitude speedup.

#index 1250232
#* Low-cost addition of preferences to DTPs and TCSPs
#@ Bart Peintner;Martha E. Pollack
#t 2004
#c 10
#% 107137
#% 307179
#% 736897
#% 1279395
#% 1289192
#% 1289215
#! We present an efficient approach to adding soft constraints, in the form of preferences, to Disjunctive Temporal Problems (DTPs) and Their subclass Temporal Constraint Satisfaction Problems (TCSPs). Specifically, we describe an algorithm for checking the consistency of and finding optimal solutions to such probkms. The algorithm borrows concepts from previous algorithms for solving TCSPs and Simple Temporal Problems with Preferences (STPPs), in both cases using techniques for projecting and solving component sub-problems. We show that adding preferences to DTPs and TCSPs requires only slightly more time than corresponding algorithms for TCSPs and DTPs without preferences. Thus, for problems where DTPs and TCSPs make sense, adding preferences provides a substantial gain in expressiveness for a marginal cost.

#index 1250233
#* mCP nets: representing and reasoning with preferences of multiple agents
#@ F. Rossi;K. B. Venable;T. Walsh
#t 2004
#c 10
#% 216969
#% 329580
#% 1279242
#% 1650274
#! We introduce mCP nets, an extension of the CP net formalism to model and handle the qualitative and conditional preferences of multiple agents. We give a number of different semantics for reasoning with mCP nets. The semantics are all based on the idea of individual agents voting. We describe how to test optimality and preference ordering within a mCP net, and we give complexity results for such tasks. We also discuss whether the voting schemes fairly combine together the preference s of the individual agents.

#index 1250234
#* Extending CP-nets with stronger conditional preference statements
#@ Nic Wilson
#t 2004
#c 10
#% 1272026
#% 1279242
#% 1650274
#% 1650354
#! A logic of conditional preferences is defined, with a language which allows she compact representation of certain kinds of conditional preference statements, a semantics and a proof theory. CP-nets can be expressed in this language, and the semantics and proof theory generalise those of CP-nets. Despite being substantially more expressive, the formalism maintains important properties of CP-nets; there are simple sufficient conditions for consistency, and, under these conditions, optimal outcomes can be efficiently generated. It is also then easy to find a total order on outcomes which extends the conditional preference order, and an approach to constrained optimisation can be used which generalises a natural approach for CP-nets. Some results regarding the expressive power of CP-nets are also given.

#index 1250235
#* Solving generalized semi-Markov decision processes using continuous phase-type distributions
#@ Håkan L. S. Younes;Reid G. Simmons
#t 2004
#c 10
#% 363744
#% 430721
#% 602267
#% 1650297
#! We introduce the generalized semi-Markov decision process (GSMDP) as an extension of continuous-time MDPs and semi-Markov decision processes (SMDPs) for modeling stochastic decision processes with asynchronous events and actions. Using phase-type distributions and uniformization, we show how an arbitrary GSMDP can be approximated by a discrete-time MDP, which can then be solved using existing MDP techniques. The techniques we present can also be seen as an alternative approach for solving SMDPs, and we demonstrate that the introduction of phases allows us to generate higher quality policies than those obtained by standard SMDP solution techniques.

#index 1250236
#* Exploring more realistic evaluation measures for collaborative filtering
#@ Giuseppe Carenini;Rita Sharma
#t 2004
#c 10
#% 173879
#% 202011
#% 301590
#% 342767
#% 528156
#% 564810
#% 1650569
#! Collaborative filtering is a popular technique for recommending items to people. Several methods for collaborative filtering have been proposed in the literature and the quality of their predictions compared in empirical studies, In this paper, we argue that the measures of quality used in these studies are based on rather simple assumptions. We propose and apply additional measures for comparing the effectiveness of collaborative filtering methods which are grounded in decision-theory.

#index 1250237
#* Mining opinion features in customer reviews
#@ Minqing Hu;Bing Liu
#t 2004
#c 10
#% 71752
#% 78171
#% 194251
#% 279755
#% 481290
#% 577355
#% 741106
#% 855043
#% 1478826
#! It is a common practice that merchants selling products on the Web ask their customers to review the products and associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds. This makes it difficult for a potential customer to read them in order to make a decision on whether to buy the product. In this project, we aim to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we are only interested in the specific features of the product that customers have opinions on and also whether the opinions are positive or negative. We do not summarize the reviews by selecting or rewriting a subset of the original sentences from the reviews to capture their main points as in the classic text summarization. In this paper, we only focus on mining opinion/product features that the reviewers have commented on. A number of techniques are presented to mine such features. Our experimental results show that these techniques are highly effective.

#index 1250238
#* Just how mad are you? finding strong and weak opinion clauses
#@ Theresa Wilson;Janyce Wiebe;Rebecca Hwa
#t 2004
#c 10
#% 89589
#% 269217
#% 311034
#% 428246
#% 529193
#% 577246
#% 577355
#% 722308
#% 723399
#% 727877
#% 746865
#% 746885
#% 747891
#% 815074
#% 817445
#% 854646
#% 855093
#% 855279
#% 855282
#! There has been a recent swell of interest in the automatic identification and extraction of opinions and emotions in text. In this paper, we present the first experimental results classifying the strength of opinions and other types of subjectivity and classifying the subjectivity of deeply nested clauses. We use a wide range of features, including new syntactic features developed for opinion recognition. In 10-fold cross-validation experiments using support vector regression, we achieve improvements in mean-squared error over baseline ranging from 57% to 64%.

#index 1250239
#* A correspondence metric for imitation
#@ R. Amit;Maja Mataric
#t 2004
#c 10
#% 1784592

#index 1250240
#* Fuzzy induction in dynamic user profiling for information filtering
#@ Rafal A. Angryk;Costin Barbu
#t 2004
#c 10
#% 144031
#% 241033
#% 252753
#% 302417
#% 406493
#% 1499472
#% 1780619

#index 1250241
#* Semantically guiding a first-order theorem prover with a soft model
#@ Arnold Binas;John Slaney
#t 2004
#c 10
#% 288529
#% 560570
#% 560731
#% 565158
#% 938541

#index 1250242
#* Metrics for finite Markov decision processes
#@ Norm Ferns;Prakash Panangaden;Doina Precup
#t 2004
#c 10
#% 104387
#% 363744
#% 374130
#% 473120
#% 528124
#% 655325

#index 1250243
#* Robust solutions for constraint satisfaction and optimization
#@ Emmanuel Hebrard
#t 2004
#c 10
#% 266125
#% 477221
#! Super solutions are solutions in which, if a small number of variables lose their values, we are guaranteed to be able to repair the solution with only a few changes. In this paper, we stress the need to extend the super solution framework along several dimensions to make it more useful practically. We demonstrate the usefulness of those extensions on an example from jobshop scheduling, an optimization problem solved through constraint satisfaction. In such a case there is indeed a trade-off between optimality and robustness, however robustness may be increased without sacrificing optimality.

#index 1250244
#* Generation of emotional behavior for non-player characters: development of EmoBot for Quake II
#@ Tye Hooley;Bart Hunking;Mike Henry;Atsushi Inoue
#t 2004
#c 10
#% 379636

#index 1250245
#* Knowledge state reconsideration: hindsight belief revision
#@ Frances L. Johnson;Stuart C. Shapiro
#t 2004
#c 10
#% 100157
#% 154456
#% 1273685

#index 1250246
#* Utilizing internal state in multi-robot coordination tasks
#@ Chris Jones;Maja J. Mataric
#t 2004
#c 10
#% 179940
#% 181630
#% 280042
#% 379063

#index 1250247
#* Generating "random" 3-SAT instances with specific solution space structure
#@ Pushkin R. Pari;Jane Lin;Lin Yuan;Gang Qu
#t 2004
#c 10
#% 529517
#% 1271817

#index 1250248
#* Occam's Razor and a non-syntactic measure of decision tree complexity
#@ Goutam Paul
#t 2004
#c 10
#% 61792
#% 136350
#% 234979
#% 376266
#% 840577
#% 1272290
#% 1810000

#index 1250249
#* Discriminating among word meanings by identifying similar contexts
#@ Amruta Purandare;Ted Pedersen
#t 2004
#c 10
#% 266293
#% 741083
#% 816128
#% 1250281
#! Word sense discrimination is an unsupervised clustering problem, which seeks to discover which instances of a word/s are used in the same meaning. This is done strictly based on information found in raw corpora, without using any sense tagged text or other existing knowledge sources. Our particular focus is to systematically compare the efficacy of a range of lexical features, context representations, and clustering algorithms when applied to this problem.

#index 1250250
#* A bayes net approach to argumentation
#@ Sabyasachi Saha;Sandip Sen
#t 2004
#c 10
#% 263126
#% 380725
#% 643176
#! Argumentation-based negotiation approaches have been proposed to present realistic negotiation contexts. This paper presents a novel Bayesian network based argumentation and decision making framework that allows agents to utilize models of other agents. Our goal is to use Bayesian networks to capture the opponent model through an incremental learning process and use the model to generate more effective arguments to convince the opponent to accept favorable contracts.

#index 1250251
#* Identifying an object that is perceptually indistinguishable from one previously perceived
#@ John F. Santore;Stuart C. Shapiro
#t 2004
#c 10
#% 578682

#index 1250252
#* Evaluating consistency algorithms for temporal metric constraints
#@ Yang Shi;Anagh Lal;Berthe Y. Choueiry
#t 2004
#c 10
#% 2028
#% 107137
#% 307179
#% 410276
#% 618512
#% 644201
#% 822218
#! We study the performance of some known algorithms for solving the Simple Temporal Problem (STP) and the Temporal Constraint Satisfaction Problem (TCSP). In particular, we empirically compare the Bellman-Ford (BF) algorithm and its incremental version (incBF) by (Cesta & Oddi 1996) to the ΔSTP of (Xu & Choueiry 2003a). Among the tested algorithms, we show that ΔSTP is the most efficient for determining the consistency of an STP, and that incBF combined with the heuristics of (Xu & Choueiry 2003b) is the most efficient for solving the TCSP. We plan to improve ΔSTP by exploiting incrementality as in incBF and other new incremental algorithms.

#index 1250253
#* Mixed-initiative workflow composition
#@ Marc Spraragen
#t 2004
#c 10
#% 734965

#index 1250254
#* Mobile agent-based search for service discovery on dynamic peer-to-peer networks
#@ Evan A. Sultanik
#t 2004
#c 10
#% 484377
#% 1279447

#index 1250255
#* Finding redundant constraints in FSM minimization
#@ Lin Yuan;Pushkin R. Pari;Gang Qu
#t 2004
#c 10
#% 336874
#% 576844

#index 1250256
#* Semi-supervised clustering with limited background knowledge
#@ Sugato Basu
#t 2004
#c 10
#% 203341
#% 464291
#% 464608
#% 464631
#% 769881
#% 770782

#index 1250257
#* Learnable similarity functions and their applications to clustering and record linkage
#@ Mikhail Bilenko
#t 2004
#c 10
#% 235941
#% 251405
#% 333990
#% 464291
#% 464434
#% 577263
#% 722803
#% 723241
#% 729437
#% 729913
#% 769881
#% 770782

#index 1250258
#* Flexible decision-making in sequential auctions
#@ Gangshu Cai
#t 2004
#c 10
#% 810181
#% 842399

#index 1250259
#* A framework for optimal sequential planning in multiagent settings
#@ Prashant J. Doshi
#t 2004
#c 10
#% 234979
#% 450852

#index 1250260
#* User-sensitive text summarization thesis summary
#@ Noemie Elhadad
#t 2004
#c 10
#% 855269

#index 1250261
#* Connecting cognitive and physical worlds with dynamic cost function definition
#@ Jamie Lennon
#t 2004
#c 10
#% 75896
#% 165833
#% 367254
#% 589648
#% 856893
#% 1134790
#! Our goal is to mesh the symbolic reasoning capabilities of a cognitive model with the constrained optimization possibilities inherent in optimal controls. We plan to develop and test such a system for several different dynamical models in environments of differing certainty and differing efficiency requirements.

#index 1250262
#* Interesting instance discovery in multi-relational data
#@ Shou-De Lin
#t 2004
#c 10
#% 727932
#% 748025

#index 1250263
#* Adaptive algorithms for routing and traffic engineering in stochastic networks
#@ Sudip Misra;B. John Oommen
#t 2004
#c 10
#% 56461
#% 205419
#% 303309
#% 576460
#% 580683
#% 858666
#% 1781178
#% 1827979
#% 1848374
#! In this paper we report some of the research endeavors we are embarking on as part of the Doctoral research of the first author. We have already completed an investigation of some of the existing algorithms in the areas of Network Routing and Traffic Engineering, and we propose superior algorithms that would adapt to the changes in the environment in which they operate. In this attempt, we intend to use the theory of Learning Automata (LA) (Narendra and Thathachar, 1989; Obaidat et al. 2002) to address the problems we are investigating.

#index 1250264
#* Inducing constraint-based grammars using a domain ontology
#@ Smaranda Muresan
#t 2004
#c 10

#index 1250265
#* Capturing user intent for information retrieval
#@ Hien Nguyen
#t 2004
#c 10
#% 25945
#% 44876
#% 423985
#% 424021
#% 1389390
#! We study the problem of employing a cognitive user model for information retrieval in which knowledge about a user is captured and used for improving retrieval performance and user satisfaction. In this proposed research, we improve retrieval performance and user satisfaction for information retrieval by building a user model to capture user intent dynamically through analyzing behavioral information from retrieved relevant documents, and by combining captured user intent with the elements of an information retrieval system. We use decision theoretic principles and bayesian networks for building this model. The novelties of our approach lie with the fine-grained representation of the model, the ability to learn user knowledge incrementally and dynamically, the integration of user intent and system elements for improving retrieval performance and the unified evaluation framework to assess the accuracy of user intent captured and effectiveness of our model.

#index 1250266
#* A metric for the evaluation of imitation
#@ Amit Ramesh
#t 2004
#c 10
#% 465902
#% 570019
#% 1784514
#% 1784592

#index 1250267
#* Planning and verification for stochastic processes with asynchronous events
#@ Håkan L. S. Younes
#t 2004
#c 10
#% 181338
#% 307272
#% 568467
#% 641994
#% 1250235
#% 1272020
#! We consider a general model of stochastic discrete event systems with asynchronous events, and propose to develop efficient algorithms for verification and control of such systems.

#index 1250268
#* Intelligent systems demonstration: the secure wireless agent testbed (SWAT)
#@ Gustave Anderson;Andrew Burnheimer;Vincent Cicirello;David Dorsey;Saturnino Garcia;Moshe Kam;Joseph Kopena;Kris Malfettone;Andy Mroczkowski;Gaurav Naik;Max Peysakhov;William Regli;Joshua Shaffer;Evan Sultanik;Kenneth Tsang;Leonardo Urbano;Kyle Usbeck;Jacob Warren
#t 2004
#c 10
#% 636025
#% 963754
#% 1279447
#! We will demonstrate the Secure Wireless Agent Testbed (SWAT), a unique facility developed at Drexel University to study integration, networking and information assurance for next-generation wireless mobile agent systems. SWAT is an implemented system that fully integrates: 1) mobile agents, 2) wireless ad hoc multi-hop networks, and 3) security. The demonstration will show the functionality of a number of decentralized agent-based applications, including applications for authentication, collaboration, messaging, and remote sensor monitoring. The demonstration will take place on a live mobile ad hoc network consisting of approximately a dozen nodes (PDAs, tablet PCs, and laptops) and hundreds of mobile software agents.

#index 1250269
#* Multi-agent system development: design, runtime, and analysis
#@ K. S. Barber;J. Ahn;K. Fullam;T. Graser;N. Gujral;D. C. Han;D. N. Lam;R. McKay;J. Park;M. Vanzin
#t 2004
#c 10

#index 1250270
#* Visual odometry using commodity optical flow
#@ Jason Campbell;Rahul Sukthankar;Illah Nourbakhsh
#t 2004
#c 10
#% 163348
#% 625200
#% 703727
#! A wide variety of techniques for visual navigation using robot-mounted cameras have been described over the past several decades, yet adoption of optical flow navigation techniques has been slow. This demo illustrates what visual navigation has to offer: robust hazard detection (including precipices and obstacles), high-accuracy open-loop odometry, and stable closed-loop motion control implemented via an optical flow based visual odometry system. This work is based on 1) open source vision code, 2) common computing hardware, and 3) inexpensive, consumer-quality cameras, and as such should be accessible to many robot builders.

#index 1250271
#* Engineering open multi-agent systems as electronic institutions
#@ M. Esteva;D. de la Cruz;B. Rosell;J. Ll. Arcos;J. A. Rodríguez-Aguilar;G. Cuní
#t 2004
#c 10
#% 379153
#% 773202
#! In this demo we focus on the engineering of open multi-agent systems as electronic institutions. Electronic institutions are a formalism to define the rules which structure agent interactions, establishing what agents are permitted and forbidden to do. We present a set of tools that support the specification, analysis and execution of institutions, as well as the implementation of agents. Our methodology allows for a successive refinement approach to multi-agent systems engineering.

#index 1250272
#* iBundler: an agent-based decision support service for combinatorial negotiations
#@ A. Giovannucci;J. A. Rodríguez-Aguilar;Jesús Cerquides;A. Reyes;F. X. Noria
#t 2004
#c 10
#% 378898
#% 724007
#% 773203
#! Negotiation events in industrial procurement involving multiple, highly customisable goods pose serious challenges to buying agents when trying to determine the best set of providing agents' offers. Typically, a buying agent's decision involves a large variety of constraints that may involve attributes of a very same item as well as attributes of multiple items. In this paper we describe iBundler, an agentaware negotiation service to solve the winner determination problem considering buyers' and providers' constraints and preferences.

#index 1250273
#* Domain-independent reason-enhanced controller for task-oriented systems-DIRECTOR
#@ Darsana P. Josyula;Michael L. Anderson;Don Perlis
#t 2004
#c 10
#% 80631
#% 637658
#% 1279502

#index 1250274
#* Agent-based modeling with social networks for terrorist recruitment
#@ Teresa H. Ko;Nina M. Berry
#t 2004
#c 10
#! The Seldon model combines concepts from agent-based modeling and social network analysis to create a computation model of social dynamics for terrorist recruitment. The underlying recruitment model is based on a unique hybrid agent-based architecture that contains simple agents (individuals such as expatriates) and abstract agents (conceptual entities such as society and mosques). Interactions between agents are determined by multiple social networks which form and dissipate according to the actions of the individual. We have implemented a Java-based toolkit to evaluate the dynamics of social behavior and the specific dynamics associated with terrorist recruitment described by expert social scientists, creating an architecture for simple adaptation to other group phenomenon.

#index 1250275
#* Mobile emergency triage support system
#@ Wojtek Michalowski;Roman Slowinski;Szymon Wilk
#t 2004
#c 10
#% 287068
#% 366687
#% 444808
#% 498428
#! We are designing and developing a mobile clinical decision support system, known as MET (Mobile Emergency Triage), for supporting emergency triage of different types of acute pain presentations. MET needs to interact with an existing hospital information system, run on handheld computing devices and be suitable for operation in weak connectivity conditions (with unstable connections between mobile clients and a server). The MET system captures necessary hospital data, allows for patients' data entry and provides triage support. By operating on handheld computers, it fits to the regular clinical workflow without introducing any hindrances and disruptions. It supports triage anytime and anywhere, directly at the point of care, and can be used as an electronic patient chart that facilitates structured data collection.

#index 1250276
#* CMRadar: a personal assistant agent for calendar management
#@ Pragnesh Jay Modi;Manuela Veloso;Stephen F. Smith;Jean Oh
#t 2004
#c 10
#% 159108
#% 159114
#% 461618

#index 1250277
#* Centibots: very large scale distributed robotic teams
#@ Charlie Ortiz;Kurt Konolige;Regis Vincent;Benoit Morisset;Andrew Agno;Michael Eriksen;Dieter Fox;Benson Limketkai;Jonathan Ko;Benjamin Steward;Dirk Schulz
#t 2004
#c 10
#% 275743
#% 418645
#% 643099
#% 643148

#index 1250278
#* WordNet: similarity - measuring the relatedness of concepts
#@ Ted Pedersen;Siddharth Patwardhan;Jason Michelizzi
#t 2004
#c 10
#% 465914
#% 748600
#% 1275285
#% 1279327
#% 1414358
#! WordNet: Similarity is a freely available software package that makes it possible to measure the semantic similarity or relatedness between a pair of concepts (or word senses). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.

#index 1250279
#* PRECISE on ATIS: semantic tractability and experimental results
#@ Ana-Maria Popescu;Alex Armanasu;Oren Etzioni;David Ko;Alexander Yates
#t 2004
#c 10
#% 744513
#% 748319
#% 818054
#! The need for Natural Language Interfaces to databases (NLIs) has become increasingly acute as more and more people access information through their web browsers, PDAs, and cell phones. Yet NLIs are only usable if they map natural language questions to SQL queries correctly -- people are unwilling to trade reliable and predictable user interfaces for intelligent but unreliable ones. We describe a reliable NLI, PRECISE, that incorporates a modern statistical paser and a semantic module. PRECISE provably handles a large class of natural language questions correctly. On the benchmark ATIS data set, PRECISE achieves 93.8% accuracy.

#index 1250280
#* SEM-Ether: semantic web based pervasive computing framework - integrating web, devices and people
#@ Sushil Puradkar;Sachin Singh;Chintan Patel;Kartik Vishwanath;Rahul Gupta;Yugyung Lee
#t 2004
#c 10
#! Pervasive computing aims to build an aggregated environment around a user by knitting diverse computing and communicating devices and software services into a single homogeneous unit. Our work is to develop a Pervasive computing framework which harnesses the power of Semantic Web and Web Services, facilitating the development of effective and intelligent Pervasive environments. This paper presents a high level view of the framework and how different Pervasive services can be built on this framework.

#index 1250281
#* SenseClusters - finding clusters that represent word senses
#@ Amruta Purandare;Ted Pedersen
#t 2004
#c 10
#% 678676
#% 741083
#% 816128
#! SenseClusters is a freely available word sense discrimination system that takes a purely unsupervised clustering approach. It uses no knowledge other than what is available in a raw unstructured corpus, and clusters instances of a given target word based only on their mutual contextual similarities. It is a complete system that provides support for feature selection from large corpora, several different context representation schemes, various clustering algorithms, and evaluation of the discovered clusters.

#index 1250282
#* CAMEO: modeling human activity in formal meeting situations
#@ Paul E. Rybski;Fernando de la Torre;Raju Patil;Carlos Vallespi;Manuela Veloso;Brett Browning
#t 2004
#c 10
#% 95730
#% 592420
#% 716892
#! We present CAMEO, the Camera Assisted Meeting Event Observer, which is a physical awareness system designed for use by an agent-based electronic assistant. CAMEO is used to observe formal meeting environments and infer the activities of people attending them.

#index 1250283
#* A robotic model of human reference resolution
#@ Matthias Scheutz;Virgil Andronache;Kathleen Eberhard
#t 2004
#c 10
#! Evidence from psychology suggests that humans process definite descriptions that refer to objects present in a visual scene incrementally upon hearing them, rather than constructing explicit parse trees after the whole sentence was said, which are then used to determine the referents. In this paper, we describe a real-time distributed robotic architectures for human reference resolution that demonstrates various interactions of auditory, visual, and semantic processing components hypothesized to underlie human processes.

#index 1250284
#* SCoT: a spoken conversational tutor
#@ Karl Schultz;Brady Clark;Heather Pon-Barry;Elizabeth Owen Bratt;Stanley Peters
#t 2004
#c 10
#% 748449
#! We describe SCoT, a Spoken Conversational Tutor, which has been implemented in order to investigate the advantages of natural language in tutoring, especially spoken language. SCoT uses a generic architecture for conversational intelligence which has capabilities such as turn management and coordination of multi-modal input and output. SCoT also includes a set of domain independent tutorial recipes, a domain specific production-rule knowledge base, and many natural language components including a bi-directional grammar, a speech recognizer, and a text-to-speech synthesizer. SCoT leads a reflective tutorial discussion based on the details of a problem solving session with a real-time Navy shipboard damage control simulator. The tutor attempts to identify and remediate gaps in the student's understanding of damage control doctrine by decomposing its tutorial goals into dialogue acts, which are then acted on by the dialogue manager to facilitate the conversation.

#index 1250285
#* Intelligent agents for coalition search and rescue task support
#@ Austin Tate;Jeff Dalton;Clauirton de Siebra;Stuart Aitken;Jeffrey M. Bradshaw;Andrzej Uszok
#t 2004
#c 10
#% 613798
#% 1394530
#! The Coalition Search and Rescue Task Support demonstration shows cooperative agents supporting a highly dynamic mission in which AI task planning, interagent collaboration, workflow enactment, policy-managed communications, semantic web queries, semantic web services matchmaking and knowledge-based notifications are employed.

#index 1250286
#* The autonomous sciencecraft experiment onboard the EO-1 spacecraft
#@ Daniel Tran;Steve Chien;Rob Sherwood;Rebecca Castano;Benjamin Cichy;Ashley Davies;Gregg Rabideau
#t 2004
#c 10
#% 773230
#! The Autonomous Sciencecraft Experiment (ASE), currently flying on board the Earth Observing-1 (EO-1) spacecraft, integrates several autonomy software technologies enabling autonomous science analysis and mission planning. The experiment demonstrates the potential for future space missions to use onboard decision-making to respond autonomously to capture short-lived science phenomena. The AAAI software demonstration will consist of two sections: a real-time display of an ASE-commanded ground contact from the EO-1 spacecraft, and a simulation of the full ASE autonomous science-response scenario.

#index 1250287
#* Online semantic extraction by backpropagation neural network with various syntactic structure representations
#@ Heidi H. T. Yeung
#t 2004
#c 10
#% 90391
#! The sub-symbolic approach on Natural Language Processing (NLP) is one of the mainstreams in Artificial Intelligence. Indeed, we have plenty of algorithms for variations of NLP such as syntactic structure representation or lexicon classification theoretically. The goal of these researches is obviously for developing a hybrid architecture which can process natural language as what human does. Thus, we propose an online intelligent system to extract the semantics (utterance interpretation) by applying a 3-layer back propagation neural network to classify the encoded syntactic structures into corresponding semantic frame types (e.g. AGENT_ACTION_PATIENT). The results are generated dynamically according to training sets and user inputs in webpage-form. It can diminish the manipulating time while using extra tools and share the statistical results with colleagues in clear and standard forms.

#index 1250288
#* Responsive information architect: a context-sensitive multi-media conversation framewrok for information seeking
#@ Michelle Zhou;Keith Houck;Rosario Uceda-Sosa;Shimei Pan;Min Chen;Vikram Aggarwal;James Shaw
#t 2004
#c 10

#index 1250315
#* proceedings of the 21st national conference on Artificial intelligence - Volume 2
#@ Anthony Cohn
#t 2006
#c 10

#index 1250316
#* A competitive Texas Hold'em poker player via automated abstraction and real-time equilibrium computation
#@ Andrew Gilpin;Tuomas Sandholm
#t 2006
#c 10
#% 172505
#% 218443
#% 233137
#% 348584
#% 495931
#% 847048
#% 868462
#% 1269482
#% 1279308
#% 1499499
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250317
#* Estimating search tree size
#@ Philip Kilby;John Slaney;Sylvie Thiébaux;Toby Walsh
#t 2006
#c 10
#% 115963
#% 266130
#% 294983
#% 349896
#% 528307
#% 533952
#% 535163
#% 535321
#% 578756
#% 958640
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the unexplored part of the search tree will be similar to the part we have so far explored. We compare these methods against an old method due to Knuth based on random probing. We show that these methods can reliably estimate the size of search trees explored by both optimization and decision procedures. We also demonstrate that these methods for estimating search tree size can be used to select the algorithm likely to perform best on a particular problem instance.

#index 1250318
#* Properties of forward pruning in game-tree search
#@ Yew Jin Lim;Wee Sun Lee
#t 2006
#c 10
#% 241
#% 180122
#% 688748
#% 866768
#% 1289384
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250319
#* RankCut: a domain independent forward pruning method for games
#@ Yew Jin Lim;Wee Sun Lee
#t 2006
#c 10
#% 12293
#% 60140
#% 180118
#% 306259
#% 606193
#% 686951
#% 688748
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250320
#* DD* lite: efficient incremental search with state dominance
#@ G. Ayorkor Mills-Tettey;Anthony Stentz;M. Bernardine Dias
#t 2006
#c 10
#% 41959
#% 289069
#% 303309
#% 408504
#% 578727
#% 842310
#! This paper presents DD* Lite, an efficient incremental search algorithm for problems that can capitalize on state dominance. Dominance relationships between nodes are used to prune graphs in search algorithms. Thus, exploiting state dominance relationships can considerably speed up search problems in large state spaces, such as mobile robot path planning considering uncertainty, time, or energy constraints. Incremental search techniques are useful when changes can occur in the search graph, such as when re-planning paths for mobile robots in partially known environments. While algorithms such as D* and D* Lite are very efficient incremental search algorithms, they cannot be applied as formulated to search problems in which state dominance is used to prune the graph. DD* Lite extends D* Lite to seamlessly support reasoning about state dominance. It maintains the algorithmic simplicity and incremental search capability of D* Lite, while resulting in orders of magnitude increase in search efficiency in large state spaces with dominance. We illustrate the efficiency of DD* Lite with simulation results from applying the algorithm to a path planning problem with time and energy constraints. We also prove that DD* Lite is sound, complete, optimal, and efficient.

#index 1250321
#* Sequential and parallel algorithms for frontier A* with delayed duplicate detection
#@ Robert Niewiadomski;José Nelson Amaral;Robert C. Holte
#t 2006
#c 10
#% 121528
#% 140385
#% 167926
#% 496261
#% 529516
#% 578766
#% 829310
#% 1250221
#% 1269579
#% 1279478
#! We present sequential and parallel algorithms for Frontier A* (FA*) algorithm augmented with a form of Delayed Duplicate Detection (DDD). The sequential algorithm, FA*-DDD, overcomes the leak-back problem associated with the combination of FA* and DDD. The parallel algorithm, PFA*-DDD, is a parallel version of FA*-DDD that features a novel workload distribution strategy based on intervals. We outline an implementation of PFA*-DDD designed to run on a cluster of workstations. The implementation computes intervals at run-time that are tailored to fit the workload at hand. Because the implementation distributes the workload in a manner that is both automated and adaptive, it does not require the user to specify a workload mapping function, and, more importantly, it is applicable to arbitrary problems that may be irregular. We present the results of an experimental evaluation of the implementation where it is used to solve instances of the multiple sequence alignment problem on a cluster of workstations running on top of a commodity network. Results demonstrate that the implementation offers improved capability in addition to improved performance.

#index 1250322
#* Overconfidence or paranoia? search in imperfect-information games
#@ Austin Parker;Dana Nau;V. S. Subrahmanian
#t 2006
#c 10
#% 251781
#% 1279308
#% 1289391
#% 1289395
#! We derive a recursive formula for expected utility values in imperfect- information game trees, and an imperfect-information game tree search algorithm based on it. The formula and algorithm are general enough to incorporate a wide variety of opponent models. We analyze two opponent models. The "paranoid" model is an information-set analog of the minimax rule used in perfect-information games. The "overconfident" model assumes the opponent moves randomly. Our experimental tests in the game of kriegspiel chess (an imperfect-information variant of chess) produced surprising results: (1) against each other, and against one of the kriegspiel algorithms presented at IJCAI-05, the overconfident model usually outperformed the paranoid model; (2) the performance of both models depended greatly on how well the model corresponded to the opponent's behavior. These results suggest that the usual assumption of perfect-information game tree search--that the opponent will choose the best possible move--isn't as useful in imperfect-information games.

#index 1250323
#* Disco - Novo - GoGo: integrating local search and complete search with restarts
#@ Meinolf Sellmann;Carlos Ansótegui
#t 2006
#c 10
#% 155827
#% 535138
#% 535158
#% 857142
#% 1250144
#% 1275306
#% 1289196
#% 1674538
#% 1698013
#! A hybrid algorithm is devised to boost the performance of complete search on under-constrained problems. We suggest to use random variable selection in combination with restarts, augmented by a coarse-grained local search algorithm that learns favorable value heuristics over the course of several restarts. Numerical results show that this method can speed-up complete search by orders of magnitude.

#index 1250324
#* Prob-Maxn: playing N-player games with opponent models
#@ Nathan Sturtevant;Martin Zinkevich;Michael Bowling
#t 2006
#c 10
#% 98073
#% 174161
#% 243725
#% 347813
#% 890311
#% 1271963
#% 1499483
#! Much of the work on opponent modeling for game tree search has been unsuccessful. In two-player, zero-sum games, the gains from opponent modeling are often outweighed by the cost of modeling. Opponent modeling solutions simply cannot search as deep as the highly optimized minimax search with alpha-beta pruning. Recent work has begun to look at the need for opponent modeling in n-player or general-sum games. We introduce a probabilistic approach to opponent modeling in n-player games called prob-maxn, which can robustly adapt to unknown opponents. We implement prob-maxn in the game of Spades, showing that prob-maxn is highly effective in practice, beating out the maxn and soft-max n algorithms when faced with unknown opponents.

#index 1250325
#* An efficient algorithm for scatter chart labeling
#@ Sebastian Theophil;Arno Schödl
#t 2006
#c 10
#% 97053
#% 191472
#% 284213
#% 287368
#% 582471
#% 1720817
#! This paper presents an efficient algorithm for a new variation of the point feature labeling problem. The goal is to position the largest number of point labels such that they do not intersect each other or their points. First we present an algorithm using a greedy algorithm with limited lookahead. We then present an algorithm that iteratively regroups labels, calling the first algorithm on each group, thereby identifying a close to optimal labeling order. The presented algorithm is being used in a commercial product to label charts, and our evaluation shows that it produces results far superior to those of other labeling algorithms.

#index 1250326
#* Monte Carlo go has a way to go
#@ Haruhiro Yoshimoto;Kazuki Yoshizoe;Tomoyuki Kaneko;Akihiro Kishimoto;Kenjiro Taura
#t 2006
#c 10
#% 68273
#% 71095
#% 220037
#% 348582
#% 348584
#% 348585
#% 495931
#% 1348977
#% 1740194
#! Monte Carlo Go is a promising method to improve the performance of computer Go programs. This approach determines the next move to play based on many Monte Carlo samples. This paper examines the relative advantages of additional samples and enhancements for Monte Carlo Go. By parallelizing Monte Carlo Go, we could increase sample sizes by two orders of magnitude. Experimental results obtained in 9 × 9 Go show strong evidence that there are trade-offs among these advantages and performance, indicating a way for Monte Carlo Go to go.

#index 1250327
#* Dual search in permutation state spaces
#@ Uzi Zahavi;Ariel FeIner;Robert Holte;Jonathan Schaeffer
#t 2006
#c 10
#% 348576
#% 1250219
#% 1250225
#% 1272322
#% 1289367
#% 1478838
#! Geometrical symmetries are commonly exploited to improve the efficiency of search algorithms. We introduce a new logical symmetry in permutation state spaces which we call duality. We show that each state has a dual state. Both states share important attributes and these properties can be used to improve search efficiency. We also present a new search algorithm, dual search, which switches between the original state and the dual state when it seems likely that the switch will improve the chances of a cutoff. The decision of when to switch is very important and several policies for doing this are investigated. Experimental results show significant improvements for a number of applications.

#index 1250328
#* Domain-independent structured duplicate detection
#@ Rong Zhou;Eric A. Hansen
#t 2006
#c 10
#% 266388
#% 282771
#% 541474
#% 544936
#% 829310
#% 873948
#% 1250221
#% 1250226
#% 1269579
#% 1269582
#! The scalability of graph-search algorithms can be greatly extended by using external memory, such as disk, to store generated nodes. We consider structured duplicate detection, an approach to external-memory graph search that limits the number of slow disk I/O operations needed to access search nodes stored on disk by using an abstract representation of the graph to localize memory references. For graphs with sufficient locality, structured duplicate detection outperforms other approaches to external-memory graph search. We develop an automatic method for creating an abstract representation that reveals the local structure of a graph. We then integrate this approach into a domain-independent STRIPS planner and show that it dramatically improves scalability for a wide range of planning problems. The success of this approach strongly suggests that similar local structure can be found in many other graph-search problems.

#index 1250329
#* An iterative algorithm for solving constrained decentralized Markov decision processes
#@ Aurélie Beynier;Abdel-Illah Mouaddib
#t 2006
#c 10
#% 450852
#% 527987
#% 643084
#% 773216
#% 823962
#% 1271975
#% 1272045
#% 1279314
#! Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits: it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.

#index 1250330
#* An anytime scheme for bounding posterior beliefs
#@ Bozhena Bidyuk;Rina Dechter
#t 2006
#c 10
#% 44876
#% 136358
#% 216980
#% 289947
#% 1223257
#% 1271994
#% 1389692
#% 1650763
#% 1672982
#% 1673019
#! This paper presents an any-time scheme for computing lower and upper bounds on posterior marginals in Bayesian networks. The scheme draws from two previously proposed methods, bounded conditioning (Horvitz, Suermondt, & Cooper 1989) and bound propagation (Leisink & Kappen 2003). Following the principles of cutset conditioning (Pearl 1988), our method enumerates a subset of cutset tuples and applies exact reasoning in the network instances conditioned on those tuples. The probability mass of the remaining tuples is bounded using a variant of bound propagation. We show that our new scheme improves on the earlier schemes.

#index 1250331
#* Preferences over sets
#@ R. I. Brafman;C. Domshlak;S. E. Shimony;Y. Silver
#t 2006
#c 10
#% 781167
#% 1269456
#% 1272026
#% 1272103
#! Research on preference elicitation and reasoning typically focuses on preferences over single objects of interest. However, in a number of applications the "outcomes" of interest are sets of such atomic objects. For instance, when creating the program for a film festival, editing a newspaper, or putting together a team, we need to select a set of films (resp. articles, members) that is optimal with respect to quality, diversity, cohesiveness, etc. This paper describes an intuitive approach for specifying preferences over sets of objects. An algorithm for computing an optimal subset, given a set of candidate objects and a preference specification, is developed and evaluated.

#index 1250332
#* An edge deletion semantics for belief propagation and its practical impact on approximation quality
#@ Arthur Choi;Adnan Darwiche
#t 2006
#c 10
#% 44876
#% 272514
#% 329486
#% 715096
#% 724276
#% 788106
#% 806734
#% 1272302
#% 1289558
#% 1650318
#% 1650361
#% 1650778
#% 1814768
#% 1815596
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250333
#* When gossip is good: distributed probabilistic inference for detection of slow network intrusions
#@ Denver Dash;Branislav Kveton;John Mark Agosta;Eve Schooler;Jaideep Chandrashekar;Abraham Bachrach;Alex Newman
#t 2006
#c 10
#% 75936
#% 302479
#% 466745
#% 788047
#% 790040
#% 804739
#% 963783
#% 963799
#! Intrusion attempts due to self-propagating code are becoming an increasingly urgent problem, in part due to the homogeneous makeup of the internet. Recent advances in anomaly-based intrusion detection systems (IDSs) have made use of the quickly spreading nature of these attacks to identify them with high sensitivity and at low false positive (FP) rates. However, slowly propagating attacks are much more difficult to detect because they are cloaked under the veil of normal network traffic, yet can be just as dangerous due to their exponential spread pattern. We extend the idea of using collaborative IDSs to corroborate the likelihood of attack by imbuing end hosts with probabilistic graphical models and using random messaging to gossip state among peer detectors. We show that such a system is able to boost a weak anomaly detector D to detect an order-of-magnitude slower worm, at false positive rates less than a few per week, than would be possible using D alone at the end-host or on a network aggregation point. We show that this general architecture is scalable in the sense that a fixed absolute false positive rate can be achieved as the network size grows, spreads communication bandwidth uniformly throughout the network, and makes use of the increased computation power of a distributed system. We argue that using probabilistic models provides more robust detections than previous collaborative counting schemes and allows the system to account for heterogeneous detectors in a principled fashion.

#index 1250334
#* MPE and partial inversion in lifted probabilistic variable elimination
#@ Rodrigo de Salvo Braz;Eyal Amir;Dan Roth
#t 2006
#c 10
#% 44876
#% 144840
#% 266230
#% 289947
#% 484593
#% 837639
#% 1279353
#% 1289560
#% 1650727
#! It is often convenient to represent probabilistic models in a first-order fashion, using logical atoms such as partners (X, Y) as random variables parameterized by logical variables. (de Salvo Braz, Amir, & Roth 2005), following (Poole 2003), give a lifted variable elimination algorithm (FOVE) for computing marginal probabilities from first-order probabilistic models (belief assessment, or BA). FOVE is lifted because it works directly at the first-order level, eliminating all the instantiations of a set of atoms in a single step, in some cases independently of the number of these instantiations. Previous work could treat only restricted potential functions. There, atoms' instantiations cannot constrain each other: predicates can appear at most once, or logical variables must not interact across atoms. In this paper, we present two contributions. The first one is a significantly more general lifted variable elimination algorithm, FOVE-P, that covers many cases where atoms share logical variables. The second contribution is to use FOVE-P for solving the Most Probable Explanation (MPE) problem, which consists of calculating the most probable assignment of the random variables in a model. The transition from BA to MPE is straightforward in propositional models, but the lifted first-order case is harder. We introduce the notion of lifted assignments, a distribution of values to a set of random variables rather than to each individual one. Lifted assignments are cheaper to compute while being as useful as regular assignments over that group. Both contributions advance the theoretical understanding of lifted probabilistic inference.

#index 1250335
#* On the difficulty of achieving equilibrium in interactive POMDPs
#@ Prashant Doshi;Piotr J. Gmytrasiewicz
#t 2006
#c 10
#% 1250230
#% 1272071
#! We analyze the asymptotic behavior of agents engaged in an infinite horizon partially observable stochastic game as formalized by the interactive POMDP framework. We show that when agents' initial beliefs satisfy a truth compatibility condition, their behavior converges to a subjective Ε-equilibrium in a finite time, and subjective equilibrium in the limit. This result is a generalization of a similar result in repeated games, to partially observable stochastic games. However, it turns out that the equilibrating process is difficult to demonstrate computationally because of the difficulty in coming up with initial beliefs that are both natural and satisfy the truth compatibility condition. Our results, therefore, shed some negative light on using equilibria as a solution concept for decision making in partially observable stochastic games.

#index 1250336
#* CUI networks: a graphical representation for conditional utility independence
#@ Yagil Engel;Michael P. Wellman
#t 2006
#c 10
#% 44876
#% 69605
#% 124647
#% 528176
#% 1650274
#% 1650307
#% 1650628
#! We introduce CUI networks, a compact graphical representation of utility functions over multiple attributes. CUI networks model multiattribute utility functions using the well studied and widely applicable utility independence concept. We show how conditional utility independence leads to an effective functional decomposition that can be exhibited graphically, and how local, compact data at the graph nodes can be used to calculate joint utility. We discuss aspects of elicitation and network construction, and contrast our new representation with previous graphical preference modeling.

#index 1250337
#* Solving MAP exactly by searching on compiled arithmetic circuits
#@ Jinbo Huang;Mark Chavira;Adnan Darwiche
#t 2006
#c 10
#% 302413
#% 528175
#% 571102
#% 788111
#% 1269433
#% 1272302
#% 1289558
#% 1650727
#% 1650767
#% 1650778
#% 1673033
#! The MAP (maximum a posteriori hypothesis) problem in Bayesian networks is to find the most likely states of a set of variabls given partial evidence on the complement of that set. Standard structure-based inference methods for finding exact solutions to MAP, such as variable elimination and join-tree algorithms, have complexities that are exponential in the constrained treewidth of the network. A more recent algorithm, proposed by Park and Darwiche, is exponential only in the treewidth and has been shown to handle networks whose constrained treewidth is quite high. In this paper we present a new algorithm for exact MAP that is not necessarily limited in scalability even by the treewidth. This is achieved by leveraging recent advances in compilation of Bayesian networks into arithmetic circuits, which can circumvent treewidth-imposed limits by exploiting the local structure present in the network. Specifically, we implement a branch-and-bound search where the bounds are computed using linear-time operations on the compiled arithmetic circuit. On networks with local structure, we observe orders-of-magnitude improvements over the algorithm of Park and Darwiche. In particular, we are able to efficiently solve many problems where the latter algorithm runs out of memory because of high treewidth.

#index 1250338
#* Identifiability in causal Bayesian networks: a sound and complete algorithm
#@ Yimin Huang;Marco Valtorta
#t 2006
#c 10
#% 297171
#% 578740
#% 1650407
#% 1650649
#% 1650678
#! This paper addresses the problem of identifying causal effects from nonexperimental data in a causal Bayesian network, i.e., a directed acyclic graph that represents causal relationships. The identifiability question asks whether it is possible to compute the probability of some set of (effect) variables given intervention on another set of (intervention) variables, in the presence of non-observable (i.e., hidden or latent) variables. It is well known that the answer to the question depends on the structure of the causal Bayesian network, the set of observable variables, the set of effect variables, and the set of intervention variables. Our work is based on the work of Tian, Pearl, Huang, and Valtorta (Tian & Pearl 2002a; 2002b; 2003; Huang & Valtorta 2006a) and extends it. We show that the identify algorithm that Tian and Pearl define and prove sound for semi-Markovian models can be transfered to general causal graphs and is not only sound, but also complete. This result effectively solves the identifiability question for causal Bayesian networks that Pearl posed in 1995 (Pearl 1995), by providing a sound and complete algorithm for identifiability.

#index 1250339
#* A Bayesian network for outbreak detection and prediction
#@ Xia Jiang;Garrick L. Wallstrom
#t 2006
#c 10
#% 232117
#% 280413
#% 788047
#% 919561
#! Health care officials are increasingly concerned with knowing early whether an outbreak of a particular disease is unfolding. We often have daily counts of some variable that are indicative of the number of individuals in a given community becoming sick each day with a particular disease. By monitoring these daily counts we can possibly detect an outbreak in an early stage. A number of classical time-series methods have been applied to outbreak detection based on monitoring daily counts of some variables. These classical methods only give us an alert as to whether there may be an outbreak. They do not predict properties of the outbreak such as its size, duration, and how far we are into the outbreak. Knowing the probable values of these variables can help guide us to a cost-effective decision that maximizes expected utility. Bayesian networks have become one of the most prominent architectures for reasoning under uncertainty in artificial intelligence. We present an intelligent system, implemented using a Bayesian network, which not only detects an outbreak, but predicts its size and duration, and estimates how far we are into the outbreak. We show results of investigating the performance of the system using simulated outbreaks based on real outbreak data. These results indicate that the system shows promise of being able to predict properties of an outbreak.

#index 1250340
#* Learning basis functions in hybrid domains
#@ Branislav Kveton;Milos Hauskrecht
#t 2006
#c 10
#% 363744
#% 496267
#% 578699
#% 644560
#% 739715
#% 788064
#% 1269517
#% 1289564
#% 1290041
#% 1650355
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250341
#* Incremental least squares policy iteration for POMDPs
#@ Hui Li;Xuejun Liao;Lawrence Carin
#t 2006
#c 10
#% 92301
#% 252183
#% 272652
#% 384911
#% 644560
#% 788098
#% 1279358
#% 1478842
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250342
#* Performing incremental Bayesian inference by dynamic model counting
#@ Wei Li;Peter van Beek;Pascal Poupart
#t 2006
#c 10
#% 101213
#% 283232
#% 529186
#% 571102
#% 1289558
#% 1344168
#% 1499510
#% 1650575
#% 1650715
#% 1673005
#! The ability to update the structure of a Bayesian network when new data becomes available is crucial for building adaptive systems. Recent work by Sang, Beame, and Kautz (AAAI 2005) demonstrates that the well-known Davis-Putnam procedure combined with a dynamic decomposition and caching technique is an effective method for exact inference in Bayesian networks with high density and width. In this paper, we define dynamic model counting and extend the dynamic decomposition and caching technique to multiple runs on a series of problems with similar structure. This allows us to perform Bayesian inference incrementally as the structure of the network changes. Experimental results show that our approach yields significant improvements over the previous model counting approaches on multiple challenging Bayesian network instances.

#index 1250343
#* Efficient active fusion for decision-making via VOI approximation
#@ Wenhui Liao;Qiang Ji
#t 2006
#c 10
#% 44876
#% 221442
#% 351595
#% 443640
#% 751031
#% 1269442
#% 1394388
#% 1650332
#% 1650712
#% 1650804
#! Active fusion is a process that purposively selects the most informative information from multiple sources as well as combines these information for achieving a reliable result efficiently. This paper presents a general mathematical framework based on Influence Diagrams (IDs) for active fusion and timely decision making. Within this framework, an approximation algorithm is proposed to efficiently compute nonmyopic value-of-information (VOI) for multiple sensory actions. Meanwhile a sensor selection algorithm is proposed to choose optimal sensory action sets efficiently. Both the experiments with synthetic data and real data from a real-world application demonstrate that the proposed framework together with the algorithms are well suited to applications where the decision must be made efficiently and timely from dynamically available information of diverse and disparate sources.

#index 1250344
#* Functional value iteration for decision-theoretic planning with general utility functions
#@ Yaxin Liu;Sven Koenig
#t 2006
#c 10
#% 59947
#% 124159
#% 276244
#% 319222
#% 337981
#% 363744
#% 418651
#% 532955
#% 564970
#% 573232
#% 578700
#% 578779
#% 773345
#% 828447
#% 1269516
#% 1269546
#% 1650297
#% 1790393
#! We study how to find plans that maximize the expected total utility for a given MDP, a planning objective that is important for decision making in high-stakes domains. The optimal actions can now depend on the total reward that has been accumulated so far in addition to the current state. We extend our previous work on functional value iteration from one-switch utility functions to all utility functions that can be approximated with piecewise linear utility functions (with and without exponential tails) by using functional value iteration to find a plan that maximizes the expected total utility for the approximate utility function. Functional value iteration does not maintain a value for every state but a value function that maps the total reward that has been accumulated so far into a value. We describe how functional value iteration represents these value functions in finite form, how it performs dynamic programming by manipulating these representations and what kinds of approximation guarantees it is able to make. We also apply it to a probabilistic blocksworld problem, a standard test domain for decision-theoretic planners.

#index 1250345
#* Learning representation and control in continuous Markov decision processes
#@ Sridhar Mahadevan;Mauro Maggioni;Kimberly Ferguson;Sarah Osentoski
#t 2006
#c 10
#% 384911
#% 393786
#% 457998
#% 593842
#% 734920
#% 765552
#% 840904
#% 876020
#% 916799
#! This paper presents a novel framework for simultaneously learning representation and control in continuous Markov decision processes. Our approach builds on the framework of proto-value functions, in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold. The proto-value functions correspond to the eigenfunctions of the graph Laplacian. We describe an approach to extend the eigenfunctions to novel states using the Nyström extension. A least-squares policy iteration method is used to learn the control policy, where the underlying subspace for approximating the value function is spanned by the learned proto-value functions. A detailed set of experiments is presented using classic benchmark tasks, including the inverted pendulum and the mountain car, showing the sensitivity in performance to various parameters, and including comparisons with a parametric radial basis function method.

#index 1250346
#* Memory intensive branch-and-bound search for graphical models
#@ Radu Marinescu;Rina Dechter
#t 2006
#c 10
#% 44876
#% 52784
#% 329486
#% 337983
#% 419954
#% 448887
#% 644201
#% 788050
#% 1275299
#% 1289364
#% 1289386
#% 1289387
#% 1672980
#! AND/OR search spaces have recently been introduced as a unifying paradigm for advanced algorithmic schemes for graphical models. The main virtue of this representation is its sensitivity to the structure of the model, which can translate into exponential time savings for search algorithms. AND/OR Branch-and-Bound (AOBB) is a new algorithm that explores the AND/OR search tree for solving optimization tasks in graphical models. In this paper we extend the algorithm to explore an AND/OR search graph by equipping it with a context-based adaptive caching scheme similar to good and no-good recording. The efficiency of the new graph search algorithm is demonstrated empirically on various benchmarks, including the very challenging ones that arise in genetic linkage analysis.

#index 1250347
#* Bayesian reputation modeling in E-marketplaces sensitive to subjecthity, deception and change
#@ Kevin Regan;Pascal Poupart;Robin Cohen
#t 2006
#c 10
#% 243727
#% 277480
#% 334325
#% 424787
#% 643088
#% 657509
#% 773289
#% 823966
#% 1289499
#! We present a model for buying agents in e-marketplaces to interpret evaluations of sellers provided by other buying agents, known as advisors. The interpretation of seller evaluations is complicated by the inherent subjectivity of each advisor, the possibility that advisors may deliberately provide misleading evaluations to deceive competitors and the dynamic nature of seller and advisor behaviours that may naturally change seller evaluations over time. Using a Bayesian approach, we demonstrate how to cope with subjectivity, deception and change in a principled way. More specifically, by modeling seller properties and advisor evaluation functions as dynamic random variables, buyers can progressively learn a probabilistic model that naturally and "correctly" calibrates the interpretation of seller evaluations without having to resort to heuristics to explicitely detect and filter/discount unreliable seller evaluations. Our model, called BLADE, is shown empirically to achieve lower mean error in the estimation of seller properties when compared to other models for reasoning about advisor ratings of sellers in electronic maketplaces.

#index 1250348
#* Targeting specific distributions of trajectories in MDPs
#@ David L. Roberts;Mark J. Nelson;Charles L. Isbell;Michael Mateas;Michael L. Littman
#t 2006
#c 10
#% 114760
#% 334536
#% 466418
#% 705161
#% 890319
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250349
#* Identification of joint interventional distributions in recursive semi-Markovian causal models
#@ Ilya Shpitser;Judea Pearl
#t 2006
#c 10
#% 44876
#% 297171
#% 528178
#% 578740
#% 716378
#% 1271819
#% 1289407
#% 1650678
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250350
#* Focused real-time dynamic programming for MDPs: squeezing more out of a heuristic
#@ Trey Smith;Reid Simmons
#t 2006
#c 10
#% 181627
#% 337981
#% 393786
#% 788098
#% 840906
#% 1271954
#% 1279387
#! Real-time dynamic programming (RTDP) is a heuristic search algorithm for solving MDPs. We present a modified algorithm called Focused RTDP with several improvements. While RTDP maintains only an upper bound on the long-term reward function, FRTDP maintains two-sided bounds and bases the output policy on the lower bound. FRTDP guides search with a new rule for outcome selection, focusing on parts of the search graph that contribute most to uncertainty about the values of good policies. FRTDP has modified trial termination criteria that should allow it to solve some problems (within Ε) that RTDP cannot. Experiments show that for all the problems we studied, FRTDP significantly outperforms RTDP and LRTDP, and converges with up to six times fewer backups than the state-of-the-art HDP algorithm.

#index 1250351
#* Point-based dynamic programming for DEC-POMDPs
#@ Daniel Szer;François Charpillet
#t 2006
#c 10
#% 92301
#% 266286
#% 450852
#% 527987
#% 773196
#% 890238
#% 1250230
#% 1272052
#% 1272071
#% 1279314
#% 1279358
#% 1289555
#% 1699607
#! We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.

#index 1250352
#* A characterization of interventional distributions in semi-Markovian causal models
#@ Jin Tian;Changsung Kang;Judea Pearl
#t 2006
#c 10
#% 44876
#% 160190
#% 297171
#% 578741
#% 1271819
#% 1272178
#! We offer a complete characterization of the set of distributions that could be induced by local interventions on variables governed by a causal Bayesian network of unknown structure, in which some of the variables remain unmeasured. We show that such distributions are constrained by a simply formulated set of inequalities, from which bounds can be derived on causal effects that are not directly measured in randomized experiments.

#index 1250353
#* Compact, convex upper bound iteration for approximate POMDP planning
#@ Tao Wang;Pascal Poupart;Michael Bowling;Dale Schuurmans
#t 2006
#c 10
#% 310835
#% 361729
#% 425074
#% 527859
#% 565547
#% 655321
#% 757953
#% 842579
#% 890249
#% 1271823
#% 1271954
#% 1272055
#% 1272075
#% 1279358
#% 1289243
#% 1289556
#% 1290039
#% 1476294
#% 1478843
#% 1650702
#! Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).

#index 1250354
#* A platform to evaluate the technology for service discovery in the semantic web
#@ Cecile Aberg;Johan Aberg;Patrick Lambrix;Nahid Shahmehri
#t 2006
#c 10
#% 110011
#% 348132
#% 445444
#% 754113
#% 770318
#% 805902
#% 845138
#% 845143
#% 1655411
#! Since the description of the Semantic Web paradigm in 2001, technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.

#index 1250355
#* Using semantics to identify web objects
#@ Nathanael Chambers;James Allen;Lucian Galescu;Hyuckchul Jung;William Taysom
#t 2006
#c 10
#% 259991
#% 271065
#% 275915
#% 320418
#% 330786
#% 629701
#% 765411
#% 854256
#! Many common web tasks can be automated by algorithms that are able to identify web objects relevant to the user's needs. This paper presents a novel approach to web object identificalion that finds relationships between the user's actions and linguistic information associated with web objects. From a single training example involving demonstration and a natural language description, we create a parameterized object description. The approach performs as well as a popular web wrapper on a routine task, but it has the additional capability of performing in dynamic environments and the attractive property of being reusable in other domains without additional training.

#index 1250356
#* Comparative experiments on sentiment classification for online product reviews
#@ Hang Cui;Vibhu Mittal;Mayur Datar
#t 2006
#c 10
#% 279755
#% 465754
#% 577355
#% 722308
#% 746885
#% 769892
#% 854646
#% 938687
#% 939346
#% 939896
#% 939897
#! Evaluating text fragments for positive and negative subjective expressions and their strength can be important in applications such as single- or multi- document summarization, document ranking, data mining, etc. This paper looks at a simplified version of the problem: classifying online product reviews into positive and negative classes. We discuss a series of experiments with different machine learning algorithms in order to experimentally evaluate various trade-offs, using approximately 100K product reviews from the web.

#index 1250357
#* On the update of description logic ontologies at the instance level
#@ Giuseppe De Giacomo;Maurizio Lenzerini;Antonella Poggi;Riccardo Rosati
#t 2006
#c 10
#% 90860
#% 131559
#% 326595
#% 342119
#% 517280
#% 561740
#% 572366
#% 665856
#% 1269448
#% 1269453
#% 1269460
#% 1269632
#% 1655398
#% 1702407
#! We study the notion of update of an ontology expressed as a Description Logic knowledge base. Such a knowledge base is constituted by two components, called TBox and ABox. The former expresses general knowledge about the concepts and their relationships, whereas the latter describes the state of affairs regarding the instances of concepts. We investigate the case where the update affects only the instance level of the ontology, i.e., the ABox. Building on classical approaches on knowledge base update, our first contribution is to provide a general semantics for instance level update in Description Logics. We then focus on DL-Lite, a specific Description Logic where the basic reasoning tasks are computationally tractable. We show that DL-Lite is closed with respect to instance level update, in the sense that the result of an update is always expressible as a new DL-Lite ABox. Finally we provide an algorithm that computes the result of an update in DL-Lite, and we show that it runs in polynomial time with respect to the size of both the original knowledge base and the update formula.

#index 1250358
#* Inexact matching of ontology graphs using expectation-maximization
#@ Prashant Doshi;Christopher Thomas
#t 2006
#c 10
#% 344551
#% 348187
#% 1289178
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250359
#* Mining and re-ranking for answering biographical queries on the web
#@ Donghui Feng;Deepak Ravichandran;Eduard Hovy
#t 2006
#c 10
#% 312861
#% 342630
#% 465895
#% 474645
#% 748473
#% 765411
#% 815868
#% 938677
#% 938705
#% 938707
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250360
#* Towards modeling threaded discussions using induced ontology knowledge
#@ Donghui Feng;Jihie Kim;Erin Shaw;Eduard Hovy
#t 2006
#c 10
#% 156337
#% 219049
#% 262087
#% 297539
#% 311027
#% 465895
#% 577286
#% 705920
#% 754106
#% 766450
#% 790455
#% 807420
#% 818214
#% 828971
#% 848650
#% 939368
#% 940017
#% 1219552
#% 1250186
#% 1704979
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250361
#* Inconsistencies, negations and changes in ontologies
#@ Giorgos Flouris;Zhisheng Huang;Jeff Z. Pan;Dimitris Plexousakis;Holger Wache
#t 2006
#c 10
#% 665856
#% 1279264
#% 1289423
#% 1655407
#! Ontology management and maintenance are considered cornerstone issues in current Semantic Web applications in which semantic integration and ontological reasoning play a fundamental role. The ability to deal with inconsistency and to accommodate change is of utmost importance in real-world applications of ontological reasoning and management, wherein the need for expressing negated assertions also arises naturally. For this purpose, precise, formal definitions of the the different types of inconsistency and negation in ontologies are required. Unfortunately, ontology languages based on Description Logics (DLs) do not provide enough expressive power to represent axiom negations. Furthermore, there is no single, well-accepted notion of inconsistency and negation in the Semantic Web community, due to the lack of a common and solid foundational framework. In this paper, we propose a general framework accounting for inconsistency, negation and change in ontologies. Different levels of negation and inconsistency in DL-based ontologies are distinguished. We demonstrate how this framework can provide a foundation for reasoning with and management of dynamic ontologies.

#index 1250362
#* Overcoming the brittleness bottleneck using wikipedia: enhancing text categorization with encyclopedic knowledge
#@ Evgeniy Gabrilovich;Shaul Markovitch
#t 2006
#c 10
#% 69498
#% 147065
#% 169777
#% 260001
#% 280817
#% 344447
#% 413955
#% 458379
#% 629683
#% 763708
#% 785352
#% 854646
#% 961134
#% 1289518
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250363
#* Mixed collaborative and content-based filtering with user-contributed semantic features
#@ Matthew Garden;Gregory Dudek
#t 2006
#c 10
#% 124010
#% 220706
#% 220709
#% 280852
#% 319705
#% 397153
#% 414514
#% 420515
#% 428231
#% 578684
#% 734590
#% 796909
#% 813966
#% 839726
#% 1650569
#! We describe a recommender system which uses a unique combination of content-based and collaborative methods to suggest items of interest to users, and also to learn and exploit item semantics. Recommender systems typically use techniques from collaborative filtering, in which proximity measures between users are formulated to generate recommendations, or content-based filtering, in which users are compared directly to items. Our approach uses similarity measures between users, but also directly measures the attributes of items that make them appealing to specific users. This can be used to directly make recommendations to users, but equally importantly it allows these recommendations to be justified. We introduce a method for predicting the preference of a user for a movie by estimating the user's attitude toward features with which other users have described that movie. We show that this method allows for accurate recommendations for a sub-population of users, but not for the entire user population. We describe a hybrid approach in which a user-specific recommendation mechanism is learned and experimentally evaluated. It appears that such a recommender system can achieve significant improvements in accuracy over alternative methods, while also retaining other advantages.

#index 1250364
#* Table extraction using spatial reasoning on the CSS2 visual box model
#@ Wolfgang Gatterbauer;Paul Bohunsky
#t 2006
#c 10
#% 319244
#% 348146
#% 348147
#% 504443
#% 658628
#% 805845
#% 805846
#% 807369
#% 838491
#% 844289
#% 869598
#% 869616
#% 875423
#% 1250181
#% 1394469
#! Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS2 visual box model, which shows a high level of robustness even without any form of learning (F-measure ≈ 90%). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.

#index 1250365
#* Deciding semantic matching of stateless services
#@ Duncan Hull;Evgeny Zolin;Andrey Bovykin;Ian Horrocks;Ulrike Sattler;Robert Stevens
#t 2006
#c 10
#% 248026
#% 833049
#% 1092030
#% 1269448
#% 1702396
#% 1722486
#! We present a novel approach to describe and reason about stateless information processing services. It can be seen as an extension of standard descriptions which makes explicit the relationship between inputs and outputs and takes into account OWL ontologies to fix the meaning of the terms used in a service description. This allows us to define a notion of matching between services which yields high precision and recall for service location. We explain why matching is decidable, and provide biomedical example services to illustrate the utility of our approach.

#index 1250366
#* OntoSearch: a full-text search engine for the semantic web
#@ Xing Jiang;Ah-Hwee Tan
#t 2006
#c 10
#% 156337
#% 375017
#% 387427
#% 413603
#% 445309
#% 571674
#% 577373
#% 637576
#% 654442
#% 733374
#% 734591
#% 754095
#% 769473
#% 829312
#% 1015258
#! OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of his/her queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.

#index 1250367
#* Mining comparative sentences and relations
#@ Nitin Jindal;Bing Liu
#t 2006
#c 10
#% 269217
#% 463903
#% 464434
#% 577246
#% 577355
#% 727877
#% 742368
#% 756207
#% 769892
#% 815915
#% 828958
#% 829971
#% 854646
#% 855279
#% 855282
#% 879595
#% 939848
#% 939896
#% 1250238
#! This paper studies a text mining problem, comparative sentence mining (CSM). A comparative sentence expresses an ordering relation between two sets of entities with respect to some common features. For example, the comparative sentence "Canon's optics are better than those of Sony and Nikon" expresses the comparative relation: (better, {optics}, {Canon}, {Sony, Nikon}). Given a set of evaluative texts on the Web, e.g., reviews, forum postings, and news articles, the task of comparative sentence mining is (1) to identify comparative sentences from the texts and (2) to extract comparative relations from the identified comparative sentences. This problem has many applications. For example, a product manufacturer wants to know customer opinions of its products in comparison with those of its competitors. In this paper, we propose two novel techniques based on two new types of sequential rules to perform the tasks. Experimental evaluation has been conducted using different types of evaluative texts from the Web. Results show that our techniques are very promising.

#index 1250368
#* Using semantic web technologies for policy management on the web
#@ Lalana Kagal;Tim Berners-Lee;Dan Connolly;Daniel Weitzner
#t 2006
#c 10
#% 286980
#% 870505
#! With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.

#index 1250369
#* Social network-based trust in prioritized default logic
#@ Yarden Katz;Jennifer Golbeck
#t 2006
#c 10
#% 318597
#% 342695
#% 507249
#% 577367
#% 753425
#% 842605
#% 943767
#% 978648
#! A drawback of traditional default logic is that there is no general mechanism for preferring one default rule over another. To remedy this problem, numerous default logics augmented with priority relations have been introduced. In this paper, we show how trust values, derived from web-based social networks, can be used to prioritize defaults. We provide a coupling between the method for computing trust values in social networks and the prioritized Reiter defaults of (Baader & Hollunder 1995), where specificity of terminological concepts is used to prioritize defaults. We compare our approach with specificity-based prioritization, and discuss how the two can be combined. Finally, we show how our approach can be applied to other variants of prioritized default logic.

#index 1250370
#* Detecting spam blogs: a machine learning approach
#@ Pranam Kolari;Akshay Java;Tim Finin;Tim Oates;Anupam Joshi
#t 2006
#c 10
#% 116149
#% 458379
#% 769885
#% 772018
#% 807297
#% 1016177
#% 1699583
#! Weblogs or blogs are an important new way to publish information, engage in discussions, and form communities on the Internet. The Blogosphere has unfortunately been infected by several varieties of spam-like content. Blog search engines, for example, are inundated by posts from splogs - false blogs with machine generated or hijacked content whose sole purpose is to host ads or raise the PageRank of target sites. We discuss how SVM models based on local and link-based features can be used to detect splogs. We present an evaluation of learned models and their utility to blog search engines; systems that employ techniques differing from those of conventional web search engines.

#index 1250371
#* Novel relationship discovery using opinions mined from the web
#@ Lun-Wei Ku;Hsiu-Wei Ho;Hsin-Hsi Chen
#t 2006
#c 10
#% 279755
#% 577220
#% 577246
#% 577355
#% 791663
#% 805873
#% 818304
#% 854646
#% 855279
#% 939848
#% 1250237
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250372
#* Automatically labeling the inputs and outputs of web services
#@ Kristina Lerman;Anon Plangprasopchok;Craig A. Knoblock
#t 2006
#c 10
#% 262059
#% 307632
#% 333990
#% 431103
#% 765411
#% 830000
#% 1016160
#% 1269618
#% 1271981
#! Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem: namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.

#index 1250373
#* Predicting task-specific webpages for revisiting
#@ Arwen Twinkle Lettkeman;Simone Stumpf;Jed Irvine;Jonathan Herlocker
#t 2006
#c 10
#% 25855
#% 233808
#% 269218
#% 297611
#% 320432
#% 342671
#% 438551
#% 452634
#% 452635
#% 766422
#% 766454
#% 790446
#% 848639
#% 848656
#% 856251
#% 926881
#% 1650665
#! With the increased use of the web has come a corresponding increase in information overload that users face when trying to locate specific webpages, especially as a majority of visits to webpages are revisits. While automatically created browsing history lists offer a potential low-cost solution to re-locating webpages, even short browsing sessions generate a glut of webpages that do not relate to the user's information need or have no revisit value. We address how we can better support web users who want to return to information on a webpage that they have previously visited by building more useful history lists. The paper reports on a combination technique that semi-automatically segments the webpage browsing history list into tasks, applies heuristics to remove webpages that carry no intrinsic revisit value, and uses a learning model, sensitive to individual users and tasks, that predicts which webpages are likely to be revisited again. We present results from an empirical evaluation that report the likely revisit need of users and that show that adequate overall prediction accuracy can be achieved. This approach can be used to increase utility of history lists by removing information overload to users when revisiting webpages.

#index 1250374
#* Bookmark hierarchies and collaborative recommendation
#@ Ben Markines;Lubomira Stoilova;Filippo Menczer
#t 2006
#c 10
#% 173879
#% 202011
#% 220709
#% 247268
#% 314933
#% 379166
#% 447948
#% 465914
#% 609436
#% 805849
#% 1275285
#! GiveALink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely. The bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend, search, and personalize the Web. GiveALink does not use tags, content, or links in the submitted Web pages. Instead we present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure in the bookmark files of individual users, and of collaborative filtering across users. In addition, we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network. Search results can be personalized using the bookmarks submitted by a user. We evaluate a subset of the proposed ranking measures by conducting a study with human subjects.

#index 1250375
#* Spinning multiple social networks for semantic web
#@ Yutaka Matsuo;Masahiro Hamasaki;Yoshiyuki Nakamura;Takuichi Nishimura;Kôiti Hasida;Hideaki Takeda;Junichiro Mori;Danushka Bollegala;Mitsuru Ishizuka
#t 2006
#c 10
#% 279755
#% 549163
#% 736155
#% 745264
#% 792354
#% 796142
#% 803660
#% 805872
#% 805885
#% 830526
#% 838068
#% 863022
#% 869502
#% 869503
#% 907373
#% 1049667
#% 1269378
#% 1374378
#% 1655418
#% 1688084
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250376
#* Model-based collaborative filtering as a defense against profile injection attacks
#@ Bamshad Mobasher;Robin Burke;J. J. Sandvig
#t 2006
#c 10
#% 280852
#% 329569
#% 330687
#% 420134
#% 734590
#% 754097
#% 783438
#% 1650298
#! The open nature of collaborative recommender systems allows attackers who inject biased profile data to have a significant impact on the recommendations produced. Standard memory-based collaborative filtering algorithms, such as k- nearest neighbor, have been shown to be quite vulnerable to such attacks. In this paper, we examine the robustness of model-based recommendation algorithms in the face of profile injection attacks. In particular, we consider two recommendation algorithms, one based on k-means clustering and the other based on Probabilistic Latent Semantic Analysis (PLSA). These algorithms aggregate similar users into user segments that are compared to the profile of an active user to generate recommendations. Traditionally, model-based algorithms have been used to alleviate the scalability problems associated with memory-based recommender systems. We show, empirically, that these algorithms also offer significant improvements in stability and robustness over the standard k- nearest neighbor approach when attacked. Furthermore, our results show that, particularly, the PLSA-based approach can achieve comparable recommendation accuracy.

#index 1250377
#* An investigation into the feasibility of the semantic web
#@ Zhengxiang Pan;Abir Qasem;Jeff Heflin
#t 2006
#c 10
#% 445448
#% 713587
#% 832102
#% 1374378
#% 1655429
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250378
#* Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge
#@ Marius Pasca;Dekang Lin;Jeffrey Bigham;Andrei Lifchits;Alpa Jain
#t 2006
#c 10
#% 283180
#% 301241
#% 309127
#% 504443
#% 742092
#% 747738
#% 817419
#% 854663
#% 938705
#% 939924
#% 963669
#% 1476276
#! Due to the inherent difficulty of processing noisy text, the potential of the Web as a decentralized repository of human knowledge remains largely untapped during Web search. The access to billions of binary relations among named entities would enable new search paradigms and alternative methods for presenting the search results. A first concrete step towards building large searchable repositories of factual knowledge is to derive such knowledge automatically at large scale from textual documents. Generalized contextual extraction patterns allow for fast iterative progression towards extracting one million facts of a given type (e.g., Person-BornIn-Year) from 100 million Web documents of arbitrary quality. The extraction starts from as few as 10 seed facts, requires no additional input knowledge or annotated text, and emphasizes scale and coverage by avoiding the use of syntactic parsers, named entity recognizers, gazetteers, and similar text processing tools and resources.

#index 1250379
#* Minimally invasive randomization for collecting unbiased preferences from clickthrough logs
#@ Filip Radlinski;Thorsten Joachims
#t 2006
#c 10
#% 478627
#% 564279
#% 577224
#% 731615
#% 766472
#% 805200
#% 818206
#% 818221
#% 823348
#% 840853
#% 1137763
#% 1272396
#! Clickthrough data is a particularly inexpensive and plentiful resource to obtain implicit relevance feedback for improving and personalizing search engines. However, it is well known that the probability of a user clicking on a result is strongly biased toward documents presented higher in the result set irrespective of relevance. We introduce a simple method to modify the presentation of search results that provably gives relevance judgments that are unaffected by presentation bias under reasonable assumptions. We validate this property of the training data in interactive real world experiments. Finally, we show that using these unbiased relevance judgments learning methods can be guaranteed to converge to an ideal ranking given sufficient data.

#index 1250380
#* Inferring user's preferences using ontologies
#@ Vincent Schickel-Zuber;Boi Faltings
#t 2006
#c 10
#% 212217
#% 330687
#% 733577
#% 734590
#% 734591
#% 805841
#% 807421
#% 1815364
#! We consider recommender systems that filter information and only show the most preferred items. Good recommendations can be provided only when an accurate model of the user's preferences is available. We propose a novel technique for filling in missing elements of a user's preference model using the knowledge captured in an ontology. Furthermore, we show through experiments on the MovieLens data set that our model achieves a high prediction accuracy and personalization level when little about the user's preferences is known.

#index 1250381
#* WikiRelate! computing semantic relatedness using wikipedia
#@ Michael Strube;Simone Paolo Ponzetto
#t 2006
#c 10
#% 190581
#% 211044
#% 243728
#% 286069
#% 325502
#% 342963
#% 452991
#% 740995
#% 748600
#% 815329
#% 896031
#% 940015
#% 1269643
#% 1275285
#% 1279327
#% 1712197
#! Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than WordNet. In this work we present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet when applied to the largest available dataset designed for that purpose. The best results on this dataset are obtained by integrating Google, WordNet and Wikipedia based measures. We also show that including Wikipedia improves the performance of an NLP application processing naturally occurring texts.

#index 1250382
#* Trust representation and aggregation in a distributed agent system
#@ Yonghong Wang;Munindar P. Singh
#t 2006
#c 10
#% 173879
#% 342695
#% 507535
#% 643088
#% 762655
#% 1650569
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250383
#* Improve web search using image snippets
#@ Xiao-Bing Xue;Zhi-Hua Zhou;Zhongfei Zhang
#t 2006
#c 10
#% 46803
#% 92696
#% 268079
#% 290830
#% 324984
#% 676177
#% 754078
#% 766464
#% 780874
#% 807299
#% 850002
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250384
#* QUICR-learning for multi-agent coordination
#@ Adrian K. Agogino;Kagan Tumer
#t 2006
#c 10
#% 124691
#% 378962
#% 384911
#% 465913
#% 565550
#% 643168
#% 773307
#% 1271827
#% 1272286
#% 1279314
#! Coordinating multiple agents that need to perform a sequence of actions to maximize a system level reward requires solving two distinct credit assignment problems. First, credit must be assigned for an action taken at time step t that results in a reward at time step t′ t. Second, credit must be assigned for the contribution of agent i to the overall system performance. The first credit assignment problem is typically addressed with temporal difference methods such as Q-learning. The second credit assignment problem is typically addressed by creating custom reward functions. To address both credit assignment problems simultaneously, we propose the "Q Updates with Immediate Counterfactual Rewards-learning" (QUICR-learning) designed to improve both the convergence properties and performance of Q-learning in large multi-agent problems. QUICR-learning is based on previous work on single-time-step counterfactual rewards described by the collectives framework. Results on a traffic congestion problem shows that QUICR-learning is significantly better than a Q-learner using collectives-based (single-time-step counterfactual) rewards. In addition QUICR-learning provides significant gains over conventional and local Q-learning. Additional results on a multi-agent grid-world problem show that the improvements due to QUICR-learning are not domain specific and can provide up to a ten fold increase in performance over existing methods.

#index 1250385
#* Perspective taking: an organizing principle for learning in human-robot interaction
#@ Matt Berlin;Jesse Gray;Andrea I. Thomaz;Cynthia Breazeal
#t 2006
#c 10
#% 166352
#% 398420
#% 415133
#% 424017
#% 465902
#% 643109
#% 917392
#% 1650593
#% 1784813
#! The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach.

#index 1250386
#* Self-supervised acquisition of vowels in American English
#@ Michael H. Coen
#t 2006
#c 10
#% 199557
#% 855557
#% 980150
#% 1269506
#% 1289341
#! This paper presents a self-supervised framework for perceptual learning based upon correlations in different sensory modalities. We demonstrate this with a system that has learned the vowel structure of American English - i.e., the number of vowels and their phonetic descriptions - by simultaneously watching and listening to someone speak. It is highly non-parametric, knowing neither the number of vowels nor their input distributions in advance, and it has no prior linguistic knowledge. This work is the first example of unsupervised phonetic acquisition of which we are aware, outside of that done by human infants. This system is based on the cross-modal clustering framework introduced by [4], which has been significantly enhanced here. This paper presents our results and focuses on the mathematical framework that enables this type of intersensory self-supervised learning.

#index 1250387
#* Automatic heuristic construction in a complete general game player
#@ Gregory Kuhlmann;Kurt Dresner;Peter Stone
#t 2006
#c 10
#% 2194
#% 60140
#% 116297
#% 147065
#% 169359
#% 348578
#% 529676
#! Computer game players are typically designed to play a single game: today's best chess-playing programs cannot play checkers, or even tic-tac-toe. General Game Playing is the problem of designing an agent capable of playing many different previously unseen games. The first AAAI General Game Playing Competition was held at AAAI 2005 in order to promote research in this area. In this article, we survey some of the issues involved in creating a general game playing system and introduce our entry to that event. The main feature of our approach is a novel method for automatically constructing effective search heuristics based on the formal game description. Our agent is fully implemented and tested in a range of different games.

#index 1250388
#* Know thine enemy: a champion robocup coach agent
#@ Gregory Kuhlmann;William B. Knox;Peter Stone
#t 2006
#c 10
#% 136350
#% 353887
#% 504942
#% 1499477
#% 1730377
#! In a team-based multiagent system, the ability to construct a model of an opponent team's joint behavior can be useful for determining an agent's expected distribution over future world states, and thus can inform its planning of future actions. This paper presents an approach to team opponent modeling in the context of the RoboCup simulation coach competition. Specifically, it introduces an autonomous coach agent capable of analyzing past games of the current opponent, advising its own team how to play against this opponent, and identifying patterns or weaknesses on the part of the opponent. Our approach is fully implemented and tested within the RoboCup soccer server, and was the champion of the RoboCup 2005 simulation coach competition.

#index 1250389
#* A unified cognitive architecture for physical agents
#@ Pat Langley;Dongkyu Choi
#t 2006
#c 10
#% 23011
#% 75896
#% 266391
#% 579927
#% 961150
#% 1837316
#! In this paper we describe ICARUS, a cognitive architecture for physical agents that integrates ideas from a number of traditions, but that has been especially influenced by results from cognitive psychology. We review ICARUS' commitments to memories and representations, then present its basic processes for performance and learning. We illustrate the architecture's behavior on a task from in-city driving that requires interaction among its various components. In addition, we discuss ICARUS' consistency with qualitative findings about the nature of human cognition. In closing, we consider the framework's relation to other cognitive architectures that have been proposed in the literature.

#index 1250390
#* Walk the talk: connecting language, knowledge, and action in route instructions
#@ Matt MacMahon;Brian Stankiewicz;Benjamin Kuipers
#t 2006
#c 10
#% 302042
#% 454815
#% 732422
#% 748809
#% 815309
#% 850007
#% 938772
#% 1379036
#% 1776525
#! Following verbal route instructions requires knowledge of language, space, action and perception. We present MARCO, an agent that follows free-form, natural language route instructions by representing and executing a sequence of compound action specifications that model which actions to take under which conditions. MARCO infers implicit actions from knowledge of both linguistic conditional phrases and from spatial action and local configurations. Thus, MARCO performs explicit actions, implicit actions necessary to achieve the stated conditions, and exploratory actions to learn about the world. We gathered a corpus of 786 route instructions from six people in three large-scale virtual indoor environments. Thirtysix other people followed these instructions and rated them for quality. These human participants finished at the intended destination on 69% of the trials. MARCO followed the same instructions in the same environments, with a success rate of 61%. We measured the efficacy of action inference with MARCO variants lacking action inference: executing only explicit actions, MARCO succeeded on just 28% of the trials. For this task, inferring implicit actions is essential to follow poor instructions, but is also crucial for many highly-rated route instructions.

#index 1250391
#* Intuitive linguistic joint object reference in human-robot interaction: human spatial reference systems and function-based categorisation for symbol grounding
#@ Reinhard Moratz
#t 2006
#c 10
#% 85153
#% 370876
#% 762999
#% 1137756
#! The visionary goal of an easy to use service robot implies intuitive styles of interaction between humans and robots. Such natural interaction can only be achieved if means are found to bridge the gap between the forms of object perception and spatial knowledge maintained by such robots, and the forms of language, used by humans, to communicate such knowledge. Part of bridging this gap consists of allowing user and robot to establish joint reference on objects in the environment - without forcing the user to use unnatural means for object reference. We present an approach to establishing joint object reference which makes use of natural object classification and a computational model of basic intrinsic and relative reference systems. Our object recognition approach assigns natural categories (e.g. "desk", "chair", "table") to new objects based on their functional design. With basic objects within the environment classified, we can then make use of a computational reference model, to process natural projective relations (e.g. "the briefcase to the left of the chair"), allowing users to refer to objects which cannot be classified reliably by the recognition system alone.

#index 1250392
#* TacTex-05: a champion supply chain management agent
#@ David Pardoe;Peter Stone
#t 2006
#c 10
#% 773342
#% 1272381
#% 1388353
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250393
#* Deeper natural language processing for evaluating student answers in intelligent tutoring systems
#@ Vasile Rus;Arthur C. Graesser
#t 2006
#c 10
#% 173758
#% 198058
#% 243206
#% 398948
#% 740916
#% 742218
#% 815843
#% 1672483
#% 1700509
#! This paper addresses the problem of evaluating students' answers in intelligent tutoring environments with mixed-initiative dialogue by modelling it as a textual entailment problem. The problem of meaning representation and inference is a pervasive challenge in any integrated intelligent system handling communication. For intelligent tutorial dialogue systems, we show that entailment cases can be detected at various dialog turns during a tutoring session. We report the performance of a lexico-syntactic approach on a set of entailment cases that were collected from a previous study we conducted with AutoTutor.

#index 1250394
#* Integrating joint intention theory, belief reasoning, and communicative action for generating team-oriented dialogue
#@ Rajah Annamalai Subramanian;Sanjeev Kumar;Philip Cohen
#t 2006
#c 10
#% 23011
#% 68239
#% 189698
#% 398946
#% 557055
#% 773326
#% 1272316
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250395
#* Multimodal cognitive architecture: making perception more central to intelligent behavior
#@ B. Chandrasekaran
#t 2006
#c 10
#% 75896
#! I propose that the notion of cognitive state be broadened from the current predicate-symbolic, Language-of-Thought framework to a multi-modal one, where perception and kinesthetic modalities participate in thinking. In contrast to the roles assigned to perception and motor activities as modules external to central cognition in the currently dominant theories in AI and Cognitive Science, in the proposed approach, central cognition incorporates parts of the perceptual machinery. I motivate and describe the proposal schematically, and describe the implementation of a bi-modal version in which a diagrammatic representation component is added to the cognitive state. The proposal explains our rich multimodal internal experience, and can be a key step in the realization of embodied agents. The proposed multimodal cognitive state can significantly enhance the agent's problem solving.

#index 1250396
#* Integrated AI in space: the autonomous sciencecraft on earth observing one
#@ Steve Chien
#t 2006
#c 10
#% 262737
#% 813033
#! The Earth Observing One spacecraft has been under the control of AI software for several years - experimentally since 2003 and since November 2004 as the primary operations system. This software includes: model-based planning and scheduling, procedural execution, and event detection software learned by support vector machine (SVM) techniques. This software has enabled a 100x increase in the mission science return per data downlinked and a $1M/year reduction in operations costs. In this paper we discuss the AI software used, the impact of the software, and lessons learned with implications for future AI research.

#index 1250397
#* Machine reading
#@ Oren Etzioni;Michele Banko;Michael J. Cafarella
#t 2006
#c 10
#% 301241
#% 504443
#% 815915
#% 830520
#% 1289516
#% 1672483

#index 1250398
#* Constraints: the ties that bind
#@ Eugene C. Freuder
#t 2006
#c 10
#! Constraints can serve as a unifying force in artificial intelligence.

#index 1250399
#* Deconstructing planning as satisfiability
#@ Henry Kautz
#t 2006
#c 10
#% 131357
#% 283215
#% 336874
#% 496243
#% 496277
#% 561083
#% 716356
#% 830717
#% 1269432
#% 1271884
#% 1272399
#% 1273727
#% 1290109
#% 1476298
#% 1478760

#index 1250400
#* Towards chemical universal turing machines
#@ Stephen Muggleton
#t 2006
#c 10
#% 89958
#% 290729
#% 425111
#! Present developments in the natural sciences are providing enormous and challenging opportunities for various AI technologies to have an unprecedented impact in the broader scientific world. If taken up, such applications would not only stretch present AI technology to the limit, but if successful could also have a radical impact on the way natural science is conducted. We review our experience with the Robot Scientist and other Machine Learning applications as examples of such AI-inspired developments. We also consider potential future extensions of such work based on the use of Uncertainty Logics. As a generalisation of the robot scientist we introduce the notion of a Chemical Universal Turing machine. Such a machine would not only be capable of complex cell simulations, but could also be the basis for programmable chemical and biological experimentation robots.

#index 1250401
#* From the programmer's apprentice to human-robot interaction: thirty years of research on human-computer collaboration
#@ Charles Rich;Candace L. Sidner
#t 2006
#c 10
#% 9197
#% 46221
#% 77058
#% 115727
#% 179887
#% 213409
#% 214457
#% 215532
#% 241019
#% 284787
#% 320670
#% 341644
#% 342757
#% 342769
#% 379043
#% 398946
#% 423991
#% 440662
#% 446246
#% 446950
#% 554103
#% 669534
#% 741112
#% 752003
#% 815807
#% 828823
#% 830715
#% 850184
#% 856899
#% 1014200
#% 1269647
#! We summarize the continuous thread of research we have conducted over the past thirty years on human-computer collaboration. This research reflects many of the themes and issues in operation in the greater field of AI over this period, such as knowledge representation and reasoning, planning and intent recognition, learning, and the interplay of human theory and computer engineering.

#index 1250402
#* Turing's dream and the knowledge challenge
#@ Lenhart Schubert
#t 2006
#c 10
#% 100172
#% 145393
#% 147677
#% 198055
#% 306630
#% 398257
#% 484593
#% 711139
#% 729449
#% 788095
#% 817611
#% 850430
#% 855153
#% 855154
#% 995514
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250403
#* Does the turing test demonstrate intelligence or not?
#@ Stuart M. Shieber
#t 2006
#c 10
#% 190611
#% 758447

#index 1250404
#* Virtual humans
#@ William R. Swartout
#t 2006
#c 10
#% 330268
#% 741916
#% 828823
#% 1219575
#! Imagine a simulated world where the characters you interact with are almost human - they converse with you in English, they understand the world they are in, can reason about what to do, and they exhibit emotions. Some of these characters may be your friends, while others will oppose you. Unlike current video garnes, being successful in this world won't just be a matter of who is quickest on the draw or most adroit at solving puzzles, instead it will be the person who understands the social fabric and cultural context and can use interpersonal skills most effectively. Such a simulation could open up whole new horizons for education, entertainment and simulation. And, given recent advances in AI and graphics, it may not be too far off. Virtual humans are computer-generated characters that can take the part of humans in a variety of limited contexts. These can include acting as role-players in simulations and training systems (Johnson, Rickel & Lester 2000; Swartout et al. 2005: Traum et al. 2005; Johnson, Vilhjálmsson & Marsella 2005), where they play a variety of parts, such as acting as friendly or hostile forces, or locals in the environment. Other uses for virtual humans include acting as museum guides (Gustafson & Bell 2000), marketing assistants (Cassell, Bickmore et al. 2000) or characters in entertainment systems, where the advent of video games such as The Sims 2 makes clear the growing interest of the computer game industry in virtual humans (see also (Mateas & Stern 2003)).

#index 1250405
#* Knowledge infusion
#@ Leslie G. Valiant
#t 2006
#c 10
#% 697
#% 26125
#% 55066
#% 61800
#% 66937
#% 82156
#% 157162
#% 160128
#% 203337
#% 239245
#% 278102
#% 296858
#% 317108
#% 398847
#% 449508
#% 449515
#% 451055
#% 458159
#% 495943
#% 834166
#% 1289319
#% 1393854
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250406
#* Methods for empirical game-theoretic analysis
#@ Michael P. Wellman
#t 2006
#c 10
#% 174934
#% 567883
#% 773283
#% 868462
#% 868478
#% 875419
#% 877157
#% 1024869
#% 1250223
#% 1269438
#% 1279323
#% 1289507
#% 1343874
#! An emerging empirical methodology bridges the gap between game theory and simulation for practical strategic reasoning.

#index 1250407
#* Building semantic mappings from databases to ontologies
#@ Yuan An;John Mylopoulos;Alex Borgida
#t 2006
#c 10
#% 188853
#% 398263
#% 480134
#% 519420
#% 572314
#% 577346
#% 577359
#% 765433
#% 769239
#% 830521
#% 1655384
#% 1725994
#! A recent special issue of AI Magazine (AAAI 2005) was dedicated to the topic of semantic integration -- the problem of sharing data across disparate sources. At the core of the solution lies the discovery the "semantics" of different data sources. Ideally, the semantics of data are captured by a formal ontology of the domain together with a semantic mapping connecting the schema describing the data to the ontology. However, establishing the semantic mapping from a database schema to a formal ontology in terms of formal logic expressions is inherently difficult to automate, so the task was left to humans. In this paper, we report on our study (An, Borgida, & Mylopoulos 2005a; 2005b) of a semi-automatic tool, called MAPONTO, that assists users to discover plausible semantic relationships between a database schema (relational or XML) and an ontology, expressing them as logical formulas/rules.

#index 1250408
#* Maintaining cooperation in noisy environments
#@ Tsz-Chiu Au;Dana Nau
#t 2006
#c 10
#% 274912
#% 355911
#% 392811
#% 890287
#% 1196405
#% 1279308
#! To prevent or alleviate conflicts in multi-agent environments, it is important to distinguish between situations where another agent has misbehaved intentionally and situations where the misbehavior was accidental. One situation where this problem arises is the Noisy Iterated Prisoner's Dilemma, a version of the Iterated Prisoner's Dilemma (IPD) in which there is a nonzero probability that a "cooperate" action will accidentally be changed into a "defect" action and vice versa. Tit-For-Tat and other strategies that do quite well in the ordinary (non-noisy) IPD can do quite badly in the Noisy IPD. This paper presents a technique called symbolic noise detection, for detecting whether anomalies in player's behavior are deliberate or accidental. This idea to use player's deterministic behavior to tell whether an action has been affected by noise. We also present DBS, an algorithm that uses symbolic noise detection in the Noisy IPD. DBS constructs a model of the other agent's deterministic behavior, and watches for any deviation from this model. If the other agent's next action is inconsistent with this model, the inconsistency can be due either to noise or to a genuine change in their behavior; and DBS can often distinguish between two cases by waiting to see whether this inconsistency persists in next few moves. This technique is effective because many IPD players often have clear deterministic patterns of behavior. We entered several different implementations of DBS in the 2005 Iterated Prisoner's Dilemma competition, in Category 2 (noisy environments). Out of the 165 contestants in this category, most of DBS implementations ranked among top ten. The best one ranked third, and it was beaten only by two "master-and-slaves strategy" programs that each had a large number of "slave" programs feeding points to them.

#index 1250409
#* Acquiring constraint networks using a SAT-based version space algorithm
#@ Christian Bessiere;Remi Coletta;Frédéric Koriche;Barry O'Sullivan
#t 2006
#c 10
#% 771604
#% 1699577
#! Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem: a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.

#index 1250410
#* Subjective mapping
#@ Michael Bowling;Dana Wilkinson;Ali Ghodsi
#t 2006
#c 10
#% 229084
#% 788097
#% 840843
#% 840958
#% 1289493
#% 1502529
#! Extracting a map from a stream of experience is a key problem in robotics and artificial intelligence in general. We propose a technique, called subjective mapping, that seeks to learn a fully specified predictive model, or map, without the need for expert provided models of the robot's motion and sensor apparatus. We briefly overview the recent advancements presented elsewhere (ICML, IJCAI, and ISRR) that make this possible, examine its significance in relationship to other developments in the field. and outline open issues that remain to be addressed.

#index 1250411
#* Preference elicitation and generalized additive utility
#@ Darius Braziunas;Craig Boutilier
#t 2006
#c 10
#% 529348
#% 578692
#% 1650628
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250412
#* Progress in textual case-based reasoning: predicting the outcome of legal cases from text
#@ Stefanie Brüninghaus;Kevin D. Ashley
#t 2006
#c 10
#% 127850
#% 235446
#% 340548
#% 367665
#% 735144
#% 735149
#% 866966
#% 894059
#% 1389746
#% 1476276
#% 1706005
#! This paper reports on a project that explored reasoning with textual cases in the context of legal reasoning. The work is anchored in both Case-Based Reasoning (CBR) and AI and Law. It introduces the SMILE+IBP framework that generates a case-based analysis and prediction of the outcome of a legal case given a brief textual summary of the case facts. The focal research question in this work was to find a good text representation for text classification. An evaluation showed that replacing case-specific names by roles and adding NLP lead to higher performance for assigning CBR indices. The NLP-based representation produced the best results for reasoning with the automatically indexed cases.

#index 1250413
#* B-ROC curves for the assessment of classifiers over imbalanced data sets
#@ Alvaro A. Cárdenas;John S. Baras
#t 2006
#c 10
#% 157045
#% 286972
#% 310519
#% 331909
#% 765519
#% 864876
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250414
#* Handling self-interest in groups, with minimal cost
#@ Ruggiero Cavallo
#t 2006
#c 10
#% 643229
#% 855913
#% 870914
#% 890339
#% 1289307
#% 1738938
#! In group decision-making problems that involve selfinterested agents with private information, reaching socially optimal outcomes requires aligning the goals of individuals with the welfare of the entire group. The well-known VCG mechanism achieves this by requiring specific payments from agents to a central coordinator. However, when the goal of coordination is to allow the group to jointly realize the greatest possible welfare, these payments amount to an unwanted cost of implementation, or waste. While it has often been stated that the payments VCG prescribes are necessary in order to implement the socially optimal outcome in dominant strategies without running a deficit, this is in fact not generally true. (Cavallo 2006) specified the mechanism that requires the minimal payments among all mechanisms that are socially optimal, never run a deficit, and are ex post individual rational with an anonymity property. The mechanism achieves significant savings over VCG in a broad range of practically relevant domains, including allocation problems, by using information about the structure of valuations in the domain. This paper gives a high-level overview of that result, and discusses some potential applications to AI.

#index 1250415
#* Constraint symmetry and solution symmetry
#@ David Cohen;Peter Jeavons;Christopher Jefferson;Karen E. Petrie;Barbara M. Smith
#t 2006
#c 10
#% 535133
#% 873074
#% 1838919
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250416
#* Traffic intersections of the future
#@ Kurt Dresner;Peter Stone
#t 2006
#c 10
#% 266254
#% 271067
#% 364575
#% 418731
#% 719141
#% 773254
#% 822616
#% 823903
#% 1740764
#! Few concepts embody the goals of artificial intelligence as well as fully autonomous robots. Countless films and stories have been made that focus on a future filled with autonomous agents that complete menial tasks or run errands that humans do not want or are too busy to carry out. One such task is driving automobiles. In this paper, we summarize the work we have dune towards a future of fully-autonomous vehicles, specifically coordinating such vehicles safely and efficiently at intersections. We then discuss the implications this work has for other areas of AI, including planning, multiagent learning, and computer vision.

#index 1250417
#* When a decision tree learner has plenty of time
#@ Saher Esmeir;Shaul Markovitch
#t 2006
#c 10
#% 101468
#% 136350
#% 159239
#% 190581
#% 235467
#% 376266
#% 449588
#% 677386
#% 703975
#% 770795
#% 823366
#% 829230
#% 1272037
#% 1279300
#% 1291589
#% 1843668
#! The majority of the existing algorithms for learning decision trees are greedy--a tree is induced top-down, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Furthermore, the greedy algorithms require a fixed amount of time and are not able to generate a better tree if additional time is available. To overcome this problem. we present a lookahead-based algorithm for anytime induction of decision trees which allows trading computational speed for tree quality. The algorithm uses a novel strategy for evaluating candidate splits; a stochastic version of ID3 is repeatedly invoked to estimate the size of the tree in which each split results, and the split that minimizes the expected size is preferred. Experimental results indicate that for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available.

#index 1250418
#* Overview of autofeed: an unsupervised learning system for generating webfeeds
#@ Bora Gazen;Steven Minton
#t 2006
#c 10
#% 266215
#% 283091
#% 283138
#% 312860
#% 348146
#% 464304
#% 480824
#% 533935
#% 654469
#% 729875
#% 745535
#% 754108
#% 765411
#% 828957
#! The AutoFeed system automatically extracts data from semistructured web sites. Previously, researchers have developed two types of supervised learning approaches for extracting web data: methods that create precise, site-specific extraction rules and methods that learn less-precise site-independent extraction rules. In either case, significant training is required. AutoFeed follows a third, more ambitious approach, in which unsupervised learning is used to analyze sites and discover their structure. Our method relies on a set of heterogeneous "experts", each of which is capable of identifying certain types of generic structure. Each expert represents its discoveries as "hints". Based on these hints, our system clusters the pages and identifies semi-structured data that can be extracted. To identify a good clustering, we use a probabilistic model of the hint-generation process. This paper summarizes our formulation of the fully-automatic web-extraction problem, our clustering approach, and our results on a set of experiments.

#index 1250419
#* Embedding heterogeneous data using statistical models
#@ Amir Globerson;Gal Chechik;Fernando Pereira;Naftali Tishby
#t 2006
#c 10
#% 1502529
#! Embedding algorithms are a method for revealing low dimensional structure in complex data. Most embedding algorithms are designed to handle objects of a single type for which pairwise distances are specified. Here we describe a method for embedding objects of different types (such as authors and terms) into a single common Euclidean space based on their co-occurrence statistics. The joint distributions of the heterogenous objects are modeled as exponentials of squared Euclidean distances in a low-dimensional embedding space. This construction links the problem to convex optimization over positive semidefinite matrices. We quantify the performance of our method on two text datasets, and show that it consistently and significantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling and correspondence analysis.

#index 1250420
#* Tempoexpress: an expressivity-preserving musical tempo transformation system
#@ Maarten Grachten;Josep-Lluís Arcos;Ramon López de Mántaras
#t 2006
#c 10
#% 490931
#% 571112
#% 852212
#% 911001
#% 932363
#% 1688400
#! The research described in this paper focuses on global tempo transformations of monophonic audio recordings of saxophone jazz performances. More concretely, we have investigated the problem of how a performance played at a particular tempo can be automatically rendered at another tempo while preserving its expressivity. To do so we have developed a case-based reasoning system called TempoExpress. The results we have obtained have been extensively compared against a standard technique called uniform time stretching (UTS), and show that our approach is superior to UTS.

#index 1250421
#* Towards a validated model of "emotional intelligence"
#@ Jonathan Gratch;Stacy Marsella;Wenji Mao
#t 2006
#c 10
#% 302046
#% 334512
#% 423996
#% 643118
#% 773211
#% 773218
#% 890356
#% 895698
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250422
#* Large scale knowledge base systems: an empirical evaluation perspective
#@ Yuanbo Guo;Abir Qasem;Jeff Heflin
#t 2006
#c 10
#% 55925
#% 312871
#% 517280
#% 519567
#% 743395
#% 832102
#% 1655434
#! In this paper, we discuss how our work on evaluating Semantic Web knowledge base systems (KBSs) contributes to address some broader AI problems. First, we show how our apprcach provides a benchmarking solution to the Semantic Web, a new application area of AI. Second, we discuss how the approach is also beneficial in a more traditional AI context. We focus on issues such as scalability, performance tradeoffs, and the comparison of different classes of systems.

#index 1250423
#* Opinion extraction and summarization on the web
#@ Minqing Hu;Bing Liu
#t 2006
#c 10
#% 71752
#% 118040
#% 180254
#% 577246
#% 577355
#% 727877
#% 741106
#% 769892
#% 805873
#% 815915
#% 828958
#% 829971
#% 854646
#% 855043
#% 855282
#% 939848
#% 939896
#% 939926
#% 1250238
#% 1269535
#% 1478826
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250424
#* The power of sequential single-item auctions for agent coordination
#@ S. Koenig;C. Tovey;M. Lagoudakis;V. Markakis;D. Kempe;P. Keskinocak;A. Kleywegt;A. Meyerson;S. Jain
#t 2006
#c 10
#% 504918
#% 531449
#% 1013352
#! Teams of robots are more fault tolerant than single robots, and auctions appear to be promising means for coordinating them. In a recent paper at "Robotics: Science and Systems 2005," we analyzed a coordination system based on sequential single-item auctions. We showed that the coordination system is simple to implement and computation and communication efficient, and that the resulting sum of all travel distances in known terrain is guaranteed to be only a constant factor away from optimum. In this paper, we put these results in perspective by comparing our coordination system against those based on either parallel single-item auction, or combinatorial auctions, demonstrating that it combines the advantages of both.

#index 1250425
#* Lessons on applying automated recommender systems to information-seeking tasks
#@ Joseph A. Konstan;Sean M. McNee;Cai-Nicolas Ziegler;Roberto Torres;Nishikant Kapoor;John T. Riedl
#t 2006
#c 10
#% 319705
#% 415107
#% 420515
#% 734590
#% 760853
#% 805841
#% 860672
#% 860673
#! Automated recommender systems predict user preferences by applying machine learning techniques to data on products, users, and past user preferences for products. Such systems have become increasingly popular in entertainment and e-commerce domains, but have thus far had little success in information-seeking domains such as identifying published research of interest. We report on several recent publications that show how recommenders can be extended to more effectively address information-seeking tasks by expanding the focus from accurate prediction of user preferences to identifying a useful set of items to recommend in response to the user's specific information need. Specific research demonstrates the value of diversity in recommendation lists, shows how users value lists of recommendations as something different from the sum of the individual recommendations within, and presents an analytic model for customizing a recommender to match user information-seeking needs.

#index 1250426
#* Activity-centric email: a machine learning approach
#@ Nicholas Kushmerick;Tessa Lau;Mark Dredze;Rinat Khoussainov
#t 2006
#c 10
#% 340910
#% 466593
#% 790445
#% 840410
#% 848637
#% 1137778

#index 1250427
#* Controlled search over compact state representations, in nondeterministic planning domains and beyond
#@ Ugur Kuter;Dana Nau
#t 2006
#c 10
#% 121397
#% 162493
#% 224762
#% 296170
#% 417703
#% 578695
#% 578724
#% 655322
#% 706874
#% 1250200
#% 1271826
#% 1271827
#% 1272019
#% 1290112
#% 1789985
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250428
#* A look at parsing and its applications
#@ Matthew Lease;Eugene Charniak;Mark Johnson;David McClosky
#t 2006
#c 10
#% 445496
#% 466736
#% 740916
#% 741046
#% 742218
#% 742230
#% 742410
#% 746865
#% 748738
#% 811376
#% 815808
#% 815887
#% 816072
#% 816182
#% 817420
#% 938657
#% 939353
#% 939367
#% 940010
#% 1478822
#! This paper provides a brief introduction to recent work in statistical parsing and its applications. We highlight successes to date, remaining challenges, and promising future work.

#index 1250429
#* Beyond bags of words: modeling implicit user preferences in information retrieval
#@ Donald Metzler;W. Bruce Croft
#t 2006
#c 10
#% 111303
#% 287253
#% 397205
#% 789959
#% 818262
#% 840846
#% 840882
#% 1715627
#! This paper reports on recent work in the field of information retrieval that attempts to go beyond the overly simplified approach of representing documents and queries as bags of words. Simple models make it difficult to accurately model a user's information need. The model presented in the paper is based on Markov random fields and allows almost arbitrary features to be encoded. This provides a powerful mechanism for modeling many of the implicit constraints a user has in mind when formulating a query. Simple instantiations of the model that consider dependencies between the terms in a query have shown to significantly outperform bag of words models. Further extensions of the model are possible to incorporate even more complex constraints based other domain knowledge. Finally, we describe what place our model has within the broader realm of artificial intelligence and propose several open questions that may be of general interest to the field.

#index 1250430
#* The role of context in head gesture recognition
#@ Louis-Philippe Morency;Candace Sidner;Christopher Lee;Trevor Darrell
#t 2006
#c 10
#% 398946
#% 828823
#% 830715
#% 1562606
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250431
#* Supporting queries with imprecise constraints
#@ Ullas Nambiar;Subbarao Kambhampati
#t 2006
#c 10
#% 387427
#% 445170
#% 458861
#% 631985
#% 769421
#% 769900
#% 864432
#% 993987
#% 994033
#! In this paper, we motivate the need for and challenges involved in supporting imprecise queries over Web databases. Then we briefly explain our solution, AIMQ - a domain independent approach for answering imprecise queries that automatically learns query relaxation order by using approximate functional dependencies. We also describe our approach for learning similarity between values of categorical attributes. Finally. we present experimental results demonstrating the robustness, efficiency and effectiveness of AIMQ.

#index 1250432
#* The synthy approach for end to end web services composition: planning with decoupled causal and resource reasoning
#@ Biplav Srivastava
#t 2006
#c 10
#% 341625
#% 445446
#% 531446
#% 544920
#% 710068
#% 769363
#% 805851
#% 1269580
#% 1270132
#% 1374395
#% 1561977
#% 1655415
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250433
#* AI support for building cognitive models
#@ Robert St. Amant;Sean P. McBride;Frank E. Ritter
#t 2006
#c 10
#% 212492
#% 377999
#% 389381
#% 452624
#% 751821
#% 751835
#% 1133878
#% 1837389
#! Cognitive modeling techniques provide a way of evaluating user interface designs, based on what is known about human cognitive strengths and limitations. Cognitive modelers face a tradeoff, however: more detailed models require disproportionately more time and effort to develop than coarser models. In this paper we describe a system, G2A, that automatically produces translations from abstract GOMS models into more detailed ACT-R models. G2A demonstrates how even simple AI techniques can facilitate the construction of cognitive models and suggests new directions for improving modeling tools.

#index 1250434
#* Optimizing similarity assessment in case-based reasoning
#@ Armin Stahl;Thomas Gabel
#t 2006
#c 10
#% 176887
#% 209623
#% 494260
#% 494577
#% 566463
#% 1389779
#% 1673743
#% 1706031
#! The definition of accurate similarity measures is a key issue of every Case-Based Reasoning application. Although some approaches to optimize similarity measures automatically have already been applied, these approaches are not suited for all CBR application domains. On the one hand, they are restricted to classification tasks. On the other hand, they only allow optimization of feature weights. We propose a novel learning approach which addresses both problems, i.e. it is suited for most CBR application domains beyond simple classification and it enables learning of more sophisticated similarity measures.

#index 1250435
#* Real-time evolution of neural networks in the NERO video game
#@ Kenneth O. Stanley;Bobby D. Bryant;Igor Karpov;Risto Miikkulainen
#t 2006
#c 10
#% 384911
#% 446024
#% 449980
#% 495781
#% 567946
#% 811800
#% 1248838
#% 1272024
#% 1272309
#% 1777370
#! A major goal for AI is to allow users to interact with agents that learn in real time, making new kinds of interactive simulations, training applications, and digital entertainment possible. This paper describes such a learning technology, called real-time NeuroEvolution of Augmenting Topologies (rtNEAT), and describes how rtNEAT was used to build the NeuroEvolving Robotic Operatives (NERO) video game. This game represents a new genre of machine learning games where the player trains agents in real time to perform challenging tasks in a virtual environment. Providing laymen the capability to effectively train agents in real time with no prior knowledge of AI or machine learning has broad implications, both in promoting the field of AI and making its achievements accessible to the public at large.

#index 1250436
#* Laughing with HAHAcronym, a computational humor system
#@ Oliviero Stock;Carlo Strapparava
#t 2006
#c 10
#% 179772
#% 732590
#% 943842
#% 1134812
#% 1279220
#! Computational humor is a challenge with implications for many classical fields in AI such as, for example, natural language processing, intelligent human-computer interaction, reasoning, not to mention cognitive science, linguistics and psychology. In this paper we summarize our experience in developing HAHAcronym, a system devoted to produce humorous acronyms, and we discuss some concrete prospects for this field.

#index 1250437
#* Explanation-based learning for image understanding
#@ Qiang Sun;Li-Lun Wang;Gerald Dejong
#t 2006
#c 10
#% 376266
#% 409857
#% 812488
#% 840943
#! Existing prior domain knowledge represents a valuable source of information for image interpretation problems such as classifying handwritten characters. Such domain knowledge must be translated into a form understandable by the learner. Translation can be realized with Explanation-Based Learning (EBL) which provides a kind of dynamic inductive bias, combining domain knowledge and training examples. The dynamic bias formed by the interaction of domain knowledge with training examples can yield solution knowledge of potential higher quality than can be anticipated by the static bias designer without seeing training examples. We detail how EBL can be used to dynamically integrate domain knowledge, training examples, and the learning mechanism, and describe the two EBL approaches in (Sun & DeJong 2005a) and (Sun & DeJong 2005b).

#index 1250438
#* An introduction to nonlinear dimensionality reduction by maximum variance unfolding
#@ Killan Q. Weinberger;Lawrence K. Saul
#t 2006
#c 10
#% 209961
#% 770767
#% 840843
#% 840933
#% 952897
#% 1502529
#! Many problems in AI are simplified by clever representations of sensory or symbolic input. How to discover such representations automatically, from large amounts of unlabeled data, remains a fundamental challenge. The goal of statistical methods for dimensionality reduction is to detect and discover low dimensional structure in high dimensional data. In this paper, we review a recently proposed algorithm-- maximum, variance unfolding--for learning faithful low dimensional representations of high dimensional data. The algorithm relies on modem tools in convex optimization that are proving increasingly useful in many areas of machine learning.

#index 1250439
#* Automatic wrapper generation using tree matching and partial tree alignment
#@ Yanhong Zhai;Bing Liu
#t 2006
#c 10
#% 91245
#% 271065
#% 273925
#% 275915
#% 312860
#% 330784
#% 348146
#% 424931
#% 480648
#% 480824
#% 488367
#% 577319
#% 654469
#% 660272
#% 729978
#% 765411
#% 805845
#% 805846
#% 805847
#! This paper is concerned with the problem of structured data extraction from Web pages. The objective of the research is to automatically segment data records in a page, extract data items/fields from these records and store the extracted data in a database. In this paper, we first introduce the extraction problem, and then discuss the main existing approaches and their limitations. After that, we introduce a novel technique (called DEPTA) to automatically perform Web data extraction. The method consists of three steps: (1) identifying data records with similar patterns in a page, (2) aligning and extracting data items from the identified data records and (3) generating tree-based regular expressions to facilitate later extraction from other similar pages. The key innovation is the proposal of a new multiple tree alignment algorithm called partial tree alignment, which was found to be particularly suitable for Web data extraction. This paper is based on our work published in KDD-03 and WWW-05.

#index 1250440
#* Responsive information architect: enabling context-sensitive information seeking
#@ Michelle X. Zhou;Keith Houck;Shimei Pan;James Shaw;Vikram Aggarwal;Zhen Wen
#t 2006
#c 10
#% 111873
#% 145638
#% 192824
#% 214548
#% 243124
#% 632442
#% 734958
#% 782289
#% 790441
#% 790452
#% 844532
#% 1279221
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250441
#* A breadth-first approach to memory-efficient graph search
#@ Rong Zhou;Eric A. Hansen
#t 2006
#c 10
#% 2194
#% 137995
#% 321332
#% 529516
#% 578755
#% 728026
#% 829310
#% 873948
#% 1279388
#% 1279391
#! Recent work shows that the memory requirements of A* and related graph-search algorithms can be reduced substantially by only storing nodes that are on or near the search frontier, using special techniques to prevent node regeneration, and recovering the solution path by a divide-and-conquer technique. When this approach is used to solve graph-search problems with unit edge costs, we have shown that a breadth-first search strategy can be more memory-efficient than a best-first strategy. We provide an overview of our work using this approach, which we call breadth-first heuristic search.

#index 1250442
#* Biconnected structure for multi-robot systems
#@ Mazda Ahmadi;Peter Stone
#t 2006
#c 10
#% 483901
#% 492905
#% 515424
#% 1080783
#! Many applications of distributed autonomous robotic systems can benefit from, or even may require, the team of robots staying within communication connectivity. For example, consider the problem of multirobot surveillance (Ahmadi & Stone 2006), in which a team of robots must collaboratively patrol a given area. If any two robots can directly communicate at all times, the robots can coordinate for efficient behavior. This condition holds trivially in environments that are smaller than the robots' communication range. However in larger environments, the robots must actively maintain physical locations such that any two robots can communicate -- possibly through a series of other robots. Otherwise, the robots may lose track of each others' activities and become miscoordinated. Furthermore, since robots are relatively unreliable and/or may need to change tasks (for example if a robot is suddenly called by a human user to perform some other task), in a stable multirobot surveillance system, if one of the robots leaves or crashes, the rest should still be able to communicate. Some examples of other tasks that could benefit from any pair of robots being able to communicate with each other, are multi-robot exploration, search and rescue, and cleaning robots. We say that robot R1 is connected to robot R2 if there is a series of robots, each within communication range of the previous, which can pass a message from R1 to R2. It is not possible to maintain connectivity in the face of arbitrary numbers of robot departures: if there are any two robots that are not within communication of one another and all other robots simultaneously depart, the system becomes disconnected. Thus we focus on the property of remaining robust to any single failure under the assumption that the team can readjust its positioning in response to a departure more quickly than a second departure will occur. In order for the team to stay connected, even in the face of any single departure, it must be the case that every robot is connected to each other robot either directly or via two distinct paths that do not share any robots in common. We call this property biconnectivity: the removal of any one robot from the system does not disconnect the remaining robots from each other.

#index 1250443
#* A benchmark for cooperative learning agents
#@ Jason M. Black;Dean F. Hougen
#t 2006
#c 10
#% 23011
#% 418731
#% 827316
#% 1693747

#index 1250444
#* Performance evaluation methods for the trading agent competition
#@ Brett Borghetti;Eric Sodomka
#t 2006
#c 10
#% 1738946
#! This paper proposes a novel method to characterize the performance of autonomous agents in the Trading Agent Competition for Supply Chain Management (TAC-SCM). We create benchmarking tools that manipulate market environments to control the conditions and provide guidelines to test trading agents. Using these tools, we show how developers can inspect their agents and unveil behaviors that might otherwise have gone undiscovered.

#index 1250445
#* Can we work around numerical methods? an insight
#@ Sandeep Chandana;Rene V. Mayorga
#t 2006
#c 10
#% 275764

#index 1250446
#* Local consistency in junction graphs for constraint-based inference
#@ Le Chang;Alan K. Mackworth
#t 2006
#c 10
#% 567872
#% 751442
#% 757932
#% 1650318

#index 1250447
#* RL-CD: dealing with non-stationarity in reinforcement learning
#@ Bruno C. da Silva;Eduardo W. Basso;Ana L. C. Bazzan;Paulo M. Engel
#t 2006
#c 10
#% 449447
#% 1272286
#! This student abstract describes ongoing investigations regarding an approach for dealing with non-stationarity in reinforcement learning (RL) problems. We briefly propose and describe a method for managing multiple partial models of the environment and comment previous results which show that the proposed mechanism has better convergence times comparing to standard RL algorithms. Current efforts include the development of a more robust approach, capable of dealing with noisy environments, and also investigations regarding the possibility of using partial models in order to aliviate learning problems in systems with an explosive number of states.

#index 1250448
#* Making autonomous intersection management backwards-compatible
#@ Kurt Dresner;Peter Stone
#t 2006
#c 10
#% 271067
#% 364575
#% 823903

#index 1250449
#* Exploring GnuGo's evaluation function with a SVM
#@ Christopher Fellows;Yuri Malitsky;Gregory Wojtaszczyk
#t 2006
#c 10
#! While computers have defeated the best human players in many classic board games, progress in Go remains elusive. The large branching factor in the game makes traditional adversarial search intractable while the complex interaction of stones makes it difficult to assign a reliable evaluation function. This is why most existing programs rely on hand-tuned heuristics and pattern matching techniques. Yet none of these solutions perform better than an amateur player. Our work introduces a composite approach, aiming to integrate the strengths of the proved heuristic algorithms, the AI-based learning techniques, and the knowledge derived from expert games. Specifically, this paper presents an application of the Support Vector Machine (SVM) for training the GnuGo evaluation function.

#index 1250450
#* Robot self-recognition using conditional probability-based contingency
#@ Kevin M. Godby;Jesse A. Lane
#t 2006
#c 10

#index 1250451
#* Multiclass support vector machines for articulatory feature classification
#@ Brian Hutchinson;Jianna Zhang
#t 2006
#c 10
#% 338577
#% 393059
#% 722816
#% 827577
#% 859126

#index 1250452
#* Further investigations into regular XORSAT
#@ Matti Järvisalo
#t 2006
#c 10
#% 275000
#% 529517
#% 895016
#% 895021
#% 1675288

#index 1250453
#* SemNews: a semantic news framework
#@ Akshay Java;Tim Finin;Sergei Nirenburg
#t 2006
#c 10
#% 757350
#% 783560
#% 805847
#% 847222

#index 1250454
#* KDMAS: a multi-agent system for knowledge discovery via planning
#@ Li Jin;Keith Decker
#t 2006
#c 10
#% 89781
#% 179879

#index 1250455
#* Kernel methods for word sense disambiguation and acronym expansion
#@ Mahesh Joshi;Ted Pedersen;Richard Maclin;Serguei Pakhomov
#t 2006
#c 10
#% 283174
#% 464299
#% 815883
#% 939381
#% 1344850
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250456
#* Memeta: a framework for multi-relational analytics on the blogosphere
#@ Pranam Kolari;Tim Finin
#t 2006
#c 10
#% 116149
#% 853532

#index 1250457
#* Automatic heuristic construction for general game playing
#@ Gregory Kuhlmann;Peter Stone
#t 2006
#c 10
#% 60140
#% 116297
#% 169359
#% 348578

#index 1250458
#* How many different "John Smiths", and who are they?
#@ Anagba Kulkarni;Ted Pedersen
#t 2006
#c 10
#% 375388
#% 741083
#% 1700523
#! In this work we propose three unsupervised measures to automatically identify the number of distinct entities a given ambiguous name refers to in a corpus. We experiment with 22 artificially created name conflations and observe that the measure (PK2) formulated as the ratio of two successive clustering criterion function values outperforms the other two measures. We also describe a method to assign a unique label to each discovered cluster so as to identify the underlying entity that it refers to.

#index 1250459
#* Population and agent based models for language convergence
#@ Kiran Lakkaraju;Les Gasser
#t 2006
#c 10
#% 431496
#% 450852
#% 770231
#% 1272045

#index 1250460
#* Boot camp for cognitive systems
#@ Douglas S. Lange
#t 2006
#c 10
#% 188076

#index 1250461
#* Algorithms for control and interaction of large formations of robots
#@ Ross Mead;Jerry B. Weinberg
#t 2006
#c 10
#% 765347
#% 788934
#! NSF and NASA sponsored a workshop to discuss harvesting solar power in space. One solution considered was the use of a swarm of robots to form a solar reflector. How can these robots organize to form a large parabolic structure and be effectively controlled? The approach of this project is to treat the formation as a lattice of cells. Each cell is in one of a given state governed by a set of mles. A command that indicates the geometric formation is sent to a seed robot; the formation would then transform as neighbors attain their calculated relationship based on the formation definition.

#index 1250462
#* Learning of agents with limited resources
#@ Sławomir Nowaczyk
#t 2006
#c 10
#% 188086
#% 1272047
#! In our research we investigate rational agent which consciously balances deliberation and acting, and uses learning to augment its reasoning. It creates several partial plans, uses past experience to choose the best one and, by executing it, gams new Knowledge about the world. We analyse a possible application of Inductive Logic Programming to learn how to evaluate partial plans in a resource-constrained way. We also discuss how ILP framework can generalise partial plans.

#index 1250463
#* Unsupervised order-preserving regression kernel for sequence analysis
#@ Young-In Shin
#t 2006
#c 10
#% 393059
#% 640416
#% 718832
#! In this work, a generalized method for learning from sequence of unlabelled data points based on unsupervised order-preserving regression is proposed. Sequence learning is a fundamental problem, which covers a wide area of research topic including, e.g. handwritten character recognition or speech and natural language processing. For this, one may compute feature vectors from sequence and learn a function in feature space or directly match sequence using methods like dynamic time warping. The former approach is not general in that they rely on sets of application-dependent features, while, in the latter, matching is often inefficient or ineffective. Our method takes the latter approach, while providing a very simple and robust matching. Results obtained from applying our method on a few different types of data show that the method is gerneral, while accuracy is enhanced or comparable.

#index 1250464
#* Curiosity-driven exploration with planning trajectories
#@ Tyler Streeter
#t 2006
#c 10
#% 384911

#index 1250465
#* Expectation-based vision for self-localization on a legged robot
#@ Daniel Stronger;Peter Stone
#t 2006
#c 10
#% 103572
#% 677787

#index 1250466
#* Inter-task action correlation for reinforcement learning tasks
#@ Matthew E. Taylor;Peter Stone
#t 2006
#c 10
#% 384911
#% 1269498
#% 1693747
#% 1699609

#index 1250467
#* A value theory of meta-learning algorithms
#@ Abraham Bagherjeiran
#t 2006
#c 10
#% 844009
#% 1271814
#! We use game theory to analyze meta-learning algorithms. The objective of meta-learning is to determine which algorithm to apply on a given task. This is an instance of a more general problem that consists of allocating knowledge consumers to learning producers. Solving this general problem in the field of meta-learning yields solutions for related fields such as information retrieval and recommender systems.

#index 1250468
#* A computational model of narrative generation for suspense
#@ Yun-Gyung Cheong
#t 2006
#c 10
#% 194651
#% 292074

#index 1250469
#* Multi-resolution learning for knowledge transfer
#@ Eric Eaton
#t 2006
#c 10
#% 58636
#% 359837
#% 706402
#% 760805
#% 961138
#% 1290057
#% 1762971
#! Related objects may look similar at low-resolutions; differences begin to emerge naturally as the resolution is increased. By learning across multiple resolutions of input, knowledge can be transfered between related objects. My dissertation develops this idea and applies it to the problem of multitask transfer learning.

#index 1250470
#* Learning models of macrobehavior in complex adaptive systems
#@ Andrew Fast
#t 2006
#c 10
#% 729982
#% 748024
#% 769942
#% 785353
#% 823385
#% 844322
#% 1279354
#% 1650403
#% 1699642
#% 1784801

#index 1250471
#* Techniques for generating optimal, robust plans when temporal uncertainty is present
#@ Janae N. Foss
#t 2006
#c 10
#% 107137
#% 1289552
#% 1650355
#! Planning under uncertainty has been well studied, but usually the uncertainty is in action outcomes. This work instead investigates uncertainty in the amount of time that actions require to execute. In addition to this temporal uncertainty, the problems being studied must have robust solution plans that are optimized based on an objective function. This thesis summary details two iterative approaches that have been used to solve these type of problems and discusses future work, including MDP approaches.

#index 1250472
#* Automatic summarization of conversational multi-party speech
#@ Michel Galley
#t 2006
#c 10
#% 528154
#% 529158
#% 817489
#% 938737

#index 1250473
#* Privatizing constraint optimization
#@ Rachel Greenstadt
#t 2006
#c 10
#% 15950
#% 443227
#% 535135
#% 539765
#% 774348
#% 823893
#% 823970
#% 855913
#% 890438
#% 1289393

#index 1250474
#* Darshak: an intelligent cinematic camera planning system
#@ Arnav Jhala
#t 2006
#c 10
#% 319244
#% 743353
#% 1269407
#% 1499487
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250475
#* Cross system personalization by learning manifold alignments
#@ Bhaskar Mehta
#t 2006
#c 10
#% 723241

#index 1250476
#* A generalized query framework for geospatial reasoning
#@ Martin Michalowski
#t 2006
#c 10
#% 830000
#% 1269425
#% 1271962
#% 1290115
#% 1499470

#index 1250477
#* Robust autonomous structure-based color learning on a mobile robot
#@ Mohan Sridharan
#t 2006
#c 10
#% 1269569

#index 1250478
#* Closest pairs data selection for support vector machines
#@ Chaofan Sun
#t 2006
#c 10
#% 466589
#% 1390178
#% 1704840
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250479
#* Action selection in Bayesian reinforcement learning
#@ Tao Wang
#t 2006
#c 10
#% 425074
#% 464282
#% 466731
#% 840955
#% 1650283
#! My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.

#index 1250480
#* SEMAPLAN: combining planning with semantic matching to achieve web service composition
#@ Rama Akkiraju;Biplav Srivastava;Anca-Andreea Ivan;Richard Goodwin;Tanveer Syeda-Mahmood
#t 2006
#c 10
#% 831986
#% 1016160
#! In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. We use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.

#index 1250481
#* An interactive constraint-based approach to minesweeper
#@ Ken Bayer;Josh Snyder;Berthe Y. Choueiry
#t 2006
#c 10
#% 644201
#% 1715565

#index 1250482
#* ScriptEase: motivational behaviors for interactive characters in computer role-playing games
#@ Maria Cutumisu;Duane Szafron;Jonathan Schaeffer;Kevin Waugh;Curtis Onuczko;Jeff Siegel;Allan Schumacher
#t 2006
#c 10
#% 778838
#% 1718556
#! ScriptEase is a tool that allows authors with no programming experience to create interactive stories for computer role-playing games. Instead of writing scripting code manually, game authors select design patterns that encapsulate frequent game scenarios, creating stories at a higher level of abstraction and being shielded from the underlying scripting language. ScriptEase has been extended to support behavior patterns that generate ambient behaviors for non-player characters. This demonstration shows how ScriptEase creates intricate non-player character scripts to generate compelling and engaging character behaviors. We demonstrate our ScriptEase motivational ambient and PC-interactive behaviors for a guard character using BioWare Corp.'s Neverwinter Nights game.

#index 1250483
#* LOCATE intelligent systems demonstration: adapting help to the cognitive styles of users
#@ Jack L. Edwards;Greg Scott
#t 2006
#c 10
#% 284787
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250484
#* SemNews: a semantic news framework
#@ Akshay Java;Tim Finin;Sergei Nirenburg
#t 2006
#c 10
#% 757350
#% 783560
#% 805847
#% 847222

#index 1250485
#* An end-to-end supervised target-word sense disambiguation system
#@ Mahesh Joshi;Serguei Pakhomov;Ted Pedersen;Richard Maclin;Christopher Chute
#t 2006
#c 10
#% 815884
#% 926881
#% 1414372
#% 1910954
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250486
#* Strategic sales management in an autonomous trading agent for TAC SCM
#@ Wolfgang Ketter;Eric Sodomka;Amrudin Agovic;John Collins;Maria Gini
#t 2006
#c 10
#% 310459
#% 1269621

#index 1250487
#* Factored MDP elicitation and plan display
#@ Krol Kevin Mathias;Casey Lengacher;Derek Williams;Austin Cornett;Alex Dekhtyar;Judy Goldsmith
#t 2006
#c 10
#% 89748
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250488
#* Phoebus: a system for extracting and integrating data from unstructured and ungrammatical sources
#@ Matthew Michelson;Craig A. Knoblock
#t 2006
#c 10
#% 459484
#% 654467
#% 754104
#% 1289525
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250489
#* Using the semantic web to integrate ecoinformatics resources
#@ Cynthia Sims Parr;Andriy Parafiynyk;Joel Sachs;Rong Pan;Lushan Han;Li Ding;Tim Finin;David Wang
#t 2006
#c 10
#% 433955
#% 783560
#% 879809
#! We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.

#index 1250490
#* Demonstration of music plus one: a real-time system for automatic orchestral accompaniment
#@ Christopher Raphael
#t 2006
#c 10
#% 911007
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250491
#* Real-time interactive learning in the NERO video game
#@ Kenneth O. Stanley;Igor Karpov;Risto Miikkulainen;Aliza Gold
#t 2006
#c 10
#% 449980
#% 1272024
#% 1777370
#! In the NeuroEvolving Robotic Operatives (NERO) video game, the player trains a team of virtual robots for combat against other players' teams. The virtual robots learn in real time through interacting with the player. Since NERO was originally released in June, 2005, it has been downloaded over 50,000 times, appeared on Slashdot, and won several honors. The real-time NeuroEvolution of Augmenting Topologies (rt-NEAT) method, which can evolve increasingly complex artificial neural networks in real time as a game is being played, drives the robots' learning, making possible this entirely new genre of video game. The live demo will show how agents in NERO adapt in real time as they interact with the player. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.

#index 1250492
#* The tactical language and culture training system: a demonstration
#@ Andre Valente;W. Lewis Johnson;Hannes Vilhjálmsson
#t 2006
#c 10
#% 1220671
#! In this demonstration we will present the Tactical Iraqi, one of the implementations of the Tactical Language and Culture Training System (TLTS). The system helps learners acquire basic communicative skills in foreign languages and cultures. Learners practice their communication skills in a simulated village, where they must develop rapport with the local people, who in turn will help them accomplish missions such as postwar reconstruction. Each learner is ac-companied by a virtual aide who can provide assistance and guidance if needed. The aide can also act as a virtual tutor as part of an intelligent tutoring system, giving the learners feedback on their performance. Learners communicate via a multimodal interface, which permits them to speak and choose gestures on behalf of their character in the game. The system employs video game technologies and design techniques, in order to motivate and engage learners.

#index 1250493
#* The keystone scavenger team
#@ Jacky Baltes;John Anderson
#t 2006
#c 10
#% 410276
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250494
#* The robot intelligence Kernel
#@ David J. Bruemmer;Douglas A. Few;Miles C. Walton;Curtis W. Nielsen
#t 2006
#c 10
#% 792118
#% 856874
#% 1250191
#% 1784816
#! The Robot Intelligence Kernel (RIK) is a portable, reconfigurable suite of perceptual, behavioral, and cognitive capabilities that can be used across many different platforms, environments, and tasks. The RIK coupled with a virtual 3D interface have been shown to dramatically improve human-robot interactions across a variety of navigation and exploration tasks.

#index 1250495
#* Introductory computer science with robots
#@ Debra Burhans;R. Mark Meyer;Patricia Van Verth;David Puehn;Victoria Steck;John Paul Wiejaczka
#t 2006
#c 10
#% 396399
#% 432839
#% 582329

#index 1250496
#* Using snarpy to connect a KR system to Pyro
#@ Debra T. Burhans;Alistair E. R. Campbell
#t 2006
#c 10
#% 432841

#index 1250497
#* Erdos: cost-effective peripheral robotics for AI education
#@ Zachary Dodds;Ben Tribelhorn
#t 2006
#c 10
#! We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the ...

#index 1250498
#* Object-sorting-by-color in a variety of lighting conditions using neural networks and LEGO mindstorms robot
#@ Natasa Lazetic;Jianna Zhang
#t 2006
#c 10
#! Recognizing object color in a variety of lighting conditions is a challenging area of pattern-recognition. Neural networks have been found to be a good solution for that problem, and they are also quick and accurate, and can be used in real-time. We use a LEGO Mindstorms [1] robot to sort objects based on color in a variety of lighting conditions. We will start from simpler objects (LEGO pieces) and move onto more complex objects (apples, oranges, etc). This project is in progress and we hope to achieve classification accuracies of at least 90%.

#index 1250499
#* Towards a higher level of human-robot interaction and integration
#@ F. Michaud;D. Létourneau;M. Fréchette;É Beaudry;C. Côté;F. Kabanza
#t 2006
#c 10
#% 732422
#% 772054
#% 1269557
#! Spartacus, our 2005 AAAI Mobile Robot Challenge entry, integrated planning and scheduling, sound source localization, tracking and separation, message reading, speech recognition and generation, and autonomous navigation capabilities onboard a custom-made interactive robot. Integration of such a high number of capabilities revealed interesting new issues such as coordinating audio/visual/graphical capabilities, monitoring the impacts of the capabilities in usage by the robot, and inferring the robot's intentions and goals. Our 2006 entry will be used to address these issues, to add new capabilities to the robot and to improve our software and computational architectures, with the objective of increasing, evaluating and improving our understanding of human-robot interaction and integration with an autonomous mobile platform.

#index 1250500
#* DIARC: a testbed for natural human-robot interaction
#@ Paul Schermerhorn;James Kramer;Christopher Middendorff;Matthias Scheutz
#t 2006
#c 10
#% 760805
#% 856891
#% 1269665

#index 1250501
#* A semi-autonomous interactive robot
#@ Brian Schlesinger;Michael Mensch;Christopher Rindosh;Joe Votta;Yunfeng Wang
#t 2006
#c 10
#% 174161

#index 1250502
#* Collective construction using Lego robots
#@ Crystal Schuil;Matthew Valente;Justin Werfel;Radhika Nagpal
#t 2006
#c 10
#% 426521
#! Social insects, such as ants and termites, collectively build large and complex structures, with many individuals following simple rules and no centralized control or planning [Theraulaz and Bonabeau 1995, Camazine et al. 2002]. Such swarm systems have many desirable properties: a high level of parallelism, cheap and expendable individuals, and robustness to loss, addition, and errors of individual insects. Our goal is to design systems for automating construction that are similarly adaptive and robust, but build what we want. Automated construction will impact our ability to operate in inhospitable habitats, from outer space to under water. and allow automated disassembly and repair.

#index 1250503
#* Educational robotics in Brooklyn
#@ Elizabeth Sklar;Simon Parsons;M. Q. Azhar;Valerie Andrewlevich
#t 2006
#c 10
#% 241027
#! We describe a number of efforts to engage university students with robotics through teaching and outreach. Teaching runs the gamut from undergraduate introductory computer science to graduate-level artificial intelligence courses. Outreach involves collaborations between students and New York City public school classrooms. Our efforts have always involved team-based projects that culminate in demonstrations or competitions, usually based on challenges from RoboCupJunior. Several research projects have followed from these initiatives.

#index 1250504
#* A multi agent approach to vision based robot scavenging
#@ Kamil Wnuk;Brian Fulkerson;Jeremi Sudol
#t 2006
#c 10
#% 337494
#% 578746
#% 739900
#% 760805
#% 883810
#! This paper proposes a design for our entry into the 2006 AAAI Scavenger Hunt Competition and Robot Exhibition. We will be entering a scalable two agent system consisting of off-the-shelf laptop robots, capable of monocular vision. Each robot will demonstrate the ability to localize itself, recognize a set of objects, and communicate with peer robots to share location and coordinate exploration.

#index 1250505
#* Proceedings of the 21st national conference on Artificial intelligence - Volume 1
#@ Anthony Cohn
#t 2006
#c 10

#index 1250506
#* AAAI-06 conference preface
#@ Yolanda Gil;Raymond J. Mooney
#t 2006
#c 10

#index 1250507
#* Unifying logical and statistical AI
#@ Pedro Domingos;Stanley Kok;Hoifung Poon;Matthew Richardson;Parag Singla
#t 2006
#c 10
#% 3034
#% 89958
#% 90371
#% 95730
#% 216970
#% 226495
#% 496116
#% 785353
#% 840890
#% 850430
#% 854636
#% 1250579
#% 1250584
#% 1269496
#% 1289560
#! Intelligent agents must be able to handle the complexity and uncertainty of the real world. Logical AI has focused mainly on the former, and statistical AI on the latter. Markov logic combines the two by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Inference algorithms for Markov logic draw on ideas from satisfiability, Markov chain Monte Carlo and knowledge-based model construction. Learning algorithms are based on the voted perceptron, pseudo-likelihood and inductive logic programming. Markov logic has been successfully applied to problems in entity resolution, link prediction, information extraction and others, and is the basis of the open-source Alchemy system.

#index 1250508
#* The impact of balancing on problem hardness in a highly structured domain
#@ Carlos Ansótegui;Ramón Béjar;César Fernàndez;Carla Gomes;Carles Mateu
#t 2006
#c 10
#% 282175
#% 336874
#% 529517
#% 873076
#% 1250140
#% 1269577
#% 1289196
#% 1478764
#! Random problem distributions have played a key role in the study and design of algorithms for constraint satisfaction and Boolean satisfiability, as well as in our understanding of problem hardness, beyond standard worst-case complexity. We consider random problem distributions from a highly structured problem domain that generalizes the Quasigroup Completion problem (QCP) and Quasigroup with Holes (QWH), a widely used domain that captures the structure underlying a range of real-world applications. Our problem domain is also a generalization of the well-known Sudoku puzzle: we consider Sudoku instances of arbitrary order, with the additional generalization that the block regions can have rectangular shape, in addition to the standard square shape. We evaluate the computational hardness of Generalized Sudoku instances, for different parameter settings. Our experimental hardness results show that we can generate instances that are considerably harder than QCP/QWH instances of the same size. More interestingly, we show the impact of different balancing strategies on problem hardness. We also provide insights into backbone variables in Generalized Sudoku instances and how they correlate to problem hardness.

#index 1250509
#* Abstract branching for quantified formulas
#@ Marco Benedetti
#t 2006
#c 10
#% 3873
#% 266109
#% 517569
#% 561083
#% 561731
#% 576843
#% 578752
#% 806338
#% 1269402
#% 1272399
#% 1675277
#! We introduce a novel search-based decision procedure for Quantified Boolean Formulas (QBFs), called Abstract Branching. As opposed to standard search-based procedures, it escapes the burdensome need for branching on both children of every universal node in the search tree. This is achieved by branching on existential variables only, while admissible universal assignments are inferred. Running examples and experimental results are reported.

#index 1250510
#* Exploiting tree decomposition and soft local consistency in weighted CSP
#@ Simon de Givry;Thomas Schiex;Gerard Verfaillie
#t 2006
#c 10
#% 451
#% 289947
#% 329486
#% 419942
#% 581814
#% 751442
#% 789556
#% 1275309
#% 1279246
#% 1289386
#% 1672980
#% 1700197
#! Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.

#index 1250511
#* Extending dynamic backtracking to solve weighted conditional CSPs
#@ Robert T. Effinger;Brian C. Williams
#t 2006
#c 10
#% 68183
#% 447404
#% 578663
#% 1275309
#! Many planning and design problems can be characterized as optimal search over a constrained network of conditional choices with preferences. To draw upon the advanced methods of constraint satisfaction to solve these types of problems, many dynamic and flexible CSP variants have been proposed. One such variant is the Weighted Conditional CSP (WCCSP). So far, however, little work has been done to extend the full suite of CSP search algorithms to solve these CSP variants. In this paper, we extend Dynamic Backtracking and similar backjumping-based CSP search algorithms to solve WCCSPs by utilizing activity constraints and soft constraints in order to quickly prune infeasible and suboptimal regions of the search space. We provide experimental results on randomly generated WCCSP instances to prove these claims.

#index 1250512
#* DNNF-based belief state estimation
#@ Paul Elliott;Brian Williams
#t 2006
#c 10
#% 342378
#% 529667
#% 535314
#% 746721
#% 1269409
#% 1289541
#! As embedded systems grow increasingly complex, there is a pressing need for diagnosing and monitoring capabilities that estimate the system state robustly. This paper is based on approaches that address the problem of robustness by reasoning over declarative models of the physical plant, represented as a variant of factored Hidden Markov Models, called Probabilistic Concurrent Constraint Automata. Prior work on Mode Estimation of PCCAs is based on a Best-First Trajectory Enumeration (BFTE) algorithm. Two algorithms have since made improvements to the BFTE algorithm: 1) the Best-First Belief State Update (BFBSU) algorithm has improved the accuracy of BFTE and 2) the MEXEC algorithm has introduced a polynomial-time bounded algorithm using a smooth deterministic decomposable negation normal form (sd-DNNF) representation. This paper introduces a new DNNF-based Belief State Estimation (DBSE) algorithm that merges the polynomial time bound of the MEXEC algorithm with the accuracy of the BFBSU algorithm. This paper also presents an encoding of a PCCA as a CNF with probabilistic data, suitable for compilation into an sd-DNNF-based representation. The sd-DNNF representation supports computing k belief states from k previous belief states in the DBSE algorithm.

#index 1250513
#* On the use of partially ordered decision graphs for knowledge compilation and quantified Boolean formulae
#@ Hélène Fargier;Pierre Marquis
#t 2006
#c 10
#% 156026
#% 292075
#% 342378
#% 345434
#% 529173
#% 543491
#% 787002
#% 1269404
#% 1272399
#% 1289171
#! Decomposable Negation Normal Form formulae (DNNFs) form an interesting propositional fragment, both for efficiency and succinctness reasons. A famous subclass of the DNNF fragment is the OBDD fragment which offers many polytime queries and transformations, including quantifier eliminations (under some ordering restrictions). Nevertheless, the decomposable AND nodes at work in OBDDs enable only sequential decisions: clusters of variables are never assigned "in parallel" like in full DNNFs. This is an serious drawback since succinctness for the full DNNF fragment relies on such a "parallelization property". This is why we suggest to go a step further, from (sequentially) ordered decision diagrams to (partially) ordered, decomposable decision graphs, in which any decomposable AND node is allowed, and not only assignment ones. We show that, like the OBDD fragment, such a new class offers many tractable queries and transformations, including quantifier eliminations under some ordering restrictions. Furthermore, we show that this class is strictly more succinct than OBDD.

#index 1250514
#* Length-lex ordering for set CSPs
#@ Carmen Gervet;Pascal Van Hentenryck
#t 2006
#c 10
#% 56471
#% 535153
#% 535309
#% 1272073
#% 1289398
#! Combinatorial design problems arise in many application areas and are naturally modelled in terms of set variables and constraints. Traditionally, the domain of a set variable is specified by two sets (R,E) and denotes all sets containing R and disjoint from E. This representation has inherent difficulties in handling cardinality and lexicographic constraints so important in combinatorial design. This paper takes a dual view of set variables. It proposes a representation that encodes directly cardinality and lexicographic information, by totally ordering a set domain with a length-lex ordering. The solver can then enforce bound-consistency on all unary constraints in time Õ(k) where k is the set cardinality. In analogy with finite-domain solvers, non-unary constraints can be viewed as inference rules generating new unary constraints. The resulting set solver achieves a pruning (at least) comparable to the hybrid domain of Sadler and Gervet at a fraction of the computational cost.

#index 1250515
#* Model counting: a new strategy for obtaining good bounds
#@ Carla P. Gomes;Ashish Sabharwal;Bart Selman
#t 2006
#c 10
#% 17802
#% 205391
#% 420743
#% 529186
#% 1080900
#% 1698716
#! Model counting is the classical problem of computing the number of solutions of a given propositional formula. It vastly generalizes the NP-complete problem of propositional satisfiability, and hence is both highly useful and extremely expensive to solve in practice. We present a new approach to model counting that is based on adding a carefully chosen number of so-called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner. Each of the additional constraints is a randomly chosen XOR or parity constraint on the problem variables, represented either directly or in the standard CNF form. Inspired by a related yet quite different theoretical study of the properties of XOR constraints, we provide a formal proof that with high probability, the number of XOR constraints added in order to bring the formula to the boundary of being unsatisfiable determines with high precision its model count. Experimentally, we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods. In fact, we obtain the first non-trivial solution counts for very hard, highly structured combinatorial problem instances. Note that unlike other counting techniques, such as Markov Chain Monte Carlo methods, we are able to provide high-confidence guarantees on the quality of the counts obtained.

#index 1250516
#* A BDD-based polytime algorithm for cost-bounded interactive configuration
#@ Tarik Hadzic;Henrik Reif Andersen
#t 2006
#c 10
#% 3873
#% 345434
#% 410276
#% 644201
#! Interactive configurators are decision support systems assisting users in selecting values for parameters that respect given constraints. The underlying knowledge can be conveniently formulated as a Constraint Satisfaction Problem where the constraints are propositional formulas. The problem of interactive configuration was originally inspired by the product configuration problem with the emergence of the masscustomization paradigm in product manufacturing, but has also been applied to other tasks requiring user interaction, such as specifying services or setting up complex equipment. The user-friendly requirements of complete, backtrack-free and real-time interaction makes the problem computationally challenging. Therefore, it is beneficial to compile the configuration constraints into a tractable representation such as Binary Decision Diagrams (BOD) (Bryant 1986) to support efficient user interaction. The compilation deals with the NP-hardness such that the online interaction is in polynomial time in the size of the BOD. In this paper we address the problem of extending configurators so that a user can interactively limit configuration choices based on a maximum cost (such as price or weight of a product) of any valid configuration, in a complete, backtrack-free and real-time manner. The current BOD compilation approach is not adequate for this purpose, since adding the total cost information to the constraints description can dramatically increase the size of the compiled BOD. We show how to extend this compilation approach to solve the problem while keeping the polynomial time guarantees.

#index 1250517
#* New inference rules for efficient Max-SAT solving
#@ Federico Heras;Javier Larrosa
#t 2006
#c 10
#% 160389
#% 327779
#% 344542
#% 350387
#% 572256
#% 578757
#% 751442
#% 789556
#% 817633
#% 819612
#% 1289364
#% 1289381
#! In this paper we augment the Max-SAT solver of (Larrosa & Heras 2005) with three new inference rules. The three of them are special cases of Max-SAT resolution with which better lower bounds and more value pruning is achieved. Our experimental results on several domains show that the resulting algorithm can be orders of magnitude faster than state-of-the-art Max-SAT solvers and the best Weighted CSP solver.

#index 1250518
#* Simple randomized algorithms for tractable row and tree convex constraints
#@ T. K. Satish Kumar
#t 2006
#c 10
#% 3463
#% 109572
#% 131561
#% 189747
#% 252213
#% 275221
#% 1250150
#% 1250152
#% 1698017
#! We identify tractable classes of constraints based on the following simple property of a constraint: "At every infeasible point, there exist two directions such that with respect to any other feasible point, moving along at least one of these two directions decreases a certain distance metric to it". We show that connected row convex (CRC) constraints, arc-consistent consecutive tree convex (ACCTC) constraints, etc fit this characterization, and are therefore amenable to extremely simple polynomial-time randomized algorithms--the complexities of which are shown to be much less than that of the corresponding (known) deterministic algorithms and the (generic) lower bounds for establishing path-consistency. On a related note, we also provide a simple polynomial-time deterministic algorithm for finding tree embeddings of variable domains (if they exist) for establishing tree convexity in path-consistent networks.

#index 1250519
#* Weighted constraint satisfaction with set variables
#@ J. H. M. Lee;C. F. K. Siu
#t 2006
#c 10
#% 578663
#% 1272073
#! Set variables are ubiquitous in modeling (soft) constraint problems, but efforts on practical consistency algorithms for Weighted Constraint Satisfaction Problems (WCSPs) have only been on integer variables. We adapt the classical notion of set bounds consistency for WCSPs, and propose efficient representation schemes for set variables and common unary, binary, and ternary set constraints, as well as cardinality constraints. Instead of reasoning consistency on an entire set variable directly, we propose local consistency check at the set element level, and demonstrate that this apparent "micro"-management of consistency does imply set bounds consistency at the variable level. In addition, we prove that our framework captures classical CSPs with set variables, and degenerates to the classical case when the weights in the problem contain only 0 and T. Last but not least, we verify the feasibility and efficiency of our proposal with a prototype implementation, the efficiency of which is competitive against ILOG Solver on classical problems and orders of magnitude better than WCSP models using 0-1 variables to simulate set variables on soft problems.

#index 1250520
#* Detecting disjoint inconsistent subformulas for computing lower bounds for Max-SAT
#@ Chu-Min Li;Felip Manyà;Jordi Planes
#t 2006
#c 10
#% 267576
#% 420011
#% 535469
#% 819612
#% 829320
#% 1250148
#% 1273727
#% 1289381
#% 1698719
#! Many lower bound computation methods for branch and bound Max-SAT solvers can be explained as procedures that search for disjoint inconsistent subformulas in the Max-SAT instance under consideration. The difference among them is the technique used to detect inconsistencies. In this paper, we define five new lower bound computation methods: two of them are based on detecting inconsistencies via a unit propagation procedure that propagates unit clauses using an original ordering; the other three add an additional level of forward look-ahead based on detecting failed literals. Finally, we provide empirical evidence that the new lower bounds are of better quality than the existing lower bounds, as well as that a solver with our new lower bounds greatly outperforms some of the best performing state-of-the-art Max-SAT solvers on Max-2SAT, Max-3SAT, and Max-Cut instances.

#index 1250521
#* Fast SAT-based answer set solver
#@ Zhijun Lin;Yuanlin Zhang;Hector Hernandez
#t 2006
#c 10
#% 327779
#% 336874
#% 400992
#% 417651
#% 427631
#% 578673
#% 752745
#! Recent research shows that SAT (propositional satisfiability) techniques can be employed to build efficient systems to compute answer sets for logic programs. ASSAT and CMODELS are two well-known such systems. They find an answer set from the full models for the completion of the input program, which is (iteratively) augmented with loop formulas. Making use of the fact that, for non-tight programs, during the model generation, a partial assignment may be extensible to a full model but may not grow into any answer set, we propose to add answer set extensibility checking on partial assignments. Furthermore, given a partial assignment, we identify a class of loop formulas that are "active" on the assignment. These "active" formulas can be used to prune the search space. We also provide an efficient method to generate these formulas. These ideas can be implemented with a moderate modification on SAT solvers. We have developed a new answer set solver SAG on top of the SAT solver MCHAFF. Empirical studies on well-known benchmarks show that in most cases it is faster than the state-of-the-art answer set solvers, often by an order of magnitude. In the few cases when it is not the winner, it is close to the top performer, which shows its robustness.

#index 1250522
#* Local-search techniques for boolean combinations of pseudo-boolean constraints
#@ Lengning Liu;Mirosław Truszczynski
#t 2006
#c 10
#% 160270
#% 283231
#% 408396
#% 503248
#% 578750
#% 750050
#% 1478771
#! Some search problems are most directly specified by boolean combinations of pseudo-boolean constraints. We study a logic PL(PB) whose formulas are of this form, and design local-search methods to compute models of PL(PB)- theories. In our approach we view a PL(PB)-theory T as a data structure -- a concise representation of a certain propositional CNF theory cl(T) logically equivalent to T. We show that parameters needed by local-search algorithms for CNF theories, such as walksat, can be estimated on the basis of T, without the need to compute cl(T) explicitly. Since cl(T) is often much larger than T, running search based on T promises performance gains. Our experimental results confirm this expectation.

#index 1250523
#* Efficient haplotype inference with boolean satisfiability
#@ Inês Lynce;João Marques-Silva
#t 2006
#c 10
#% 535153
#% 742455
#% 857194
#% 958596
#% 1386504
#! One of the main topics of research in genornics is determining the relevance of mutations, described in haplotype data, as causes of some genetic diseases. However, due to technological limitations, genotype data rather than haplotype data is usually obtained. The haplotype inference by pure parsimony (HIPP) problem consists in inferring haplotypes from genotypes S.t. the number of required haplotypes is minimum. Previous approaches to the HIPP problem have focused on integer programming models and branch-and-bound algorithms. In contrast, this paper proposes the utilization of Boolean Satisfiability (SAT). The proposed solution entails a SAT model, a number of key pruning techniques, and an iterative algorithm that enumerates the possible solution values for the target optimization problem. Experimental results, obtained on a wide range of instances, demonstrate that the SAT-based approach can be several orders of magnitude faster than existing solutions. Besides being more efficient, the SAT-based approach is also the only capable of computing the solution for a large number of instances.

#index 1250524
#* Temporal preference optimization as weighted constraint satisfaction
#@ Michael D. Moffitt;Martha E. Pollack
#t 2006
#c 10
#% 107137
#% 126386
#% 230551
#% 266107
#% 336874
#% 419951
#% 544940
#% 736897
#% 876759
#% 1250129
#% 1250232
#% 1269428
#% 1269548
#% 1275309
#% 1279395
#% 1289192
#% 1675274
#% 1718201
#% 1718230
#! We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.

#index 1250525
#* An efficient way of breaking value symmetries
#@ Jean-François Puget
#t 2006
#c 10
#% 534837
#% 534978
#% 535133
#% 535153
#% 535172
#% 738198
#% 810502
#% 1279251
#% 1289394
#% 1289398
#! Several methods for breaking value symmetries have been proposed recently in the constraint programming community. They can be used in conjunction with variable symmetry breaking methods. However, this combination does not break all symmetries in general. We present a combination of lex constraints and element constrants that can be used to break all combinations of variable and value symmetries. It is the first time to our knowledge that it is possible to break all combinations of value and variable symmetries by adding constraints. This method is quite efficient when the number of symmetries is not too large, as shown by experiments using graceful graph problems. We also present a new global constraint that deals with the case where there are too many value symmetries. Experiments show that this is highly effective.

#index 1250526
#* A quadratic propagator for the inter-distance constraint
#@ Claude-Guy Quimper;Alejandro López-Ortiz;Gilles Pesant
#t 2006
#c 10
#% 160208
#% 266132
#% 1279247
#! We present a new propagator achieving bound consistency for the INTER-DISTANCE constraint. This constraint ensures that, among a set of variables X1,..., Xn, the difference between two variables is at least p. This restriction models, in particular scheduling problems in which tasks require p contiguous units of a resource to be completed. Until now, the best known propagator for bound consistency had time complexity O(n3). In this work we propose a quadratic propagator for the same level of consistency. We then show that this theoretical gain gives savings of an order of magnitude in our benchmark of scheduling problems.

#index 1250527
#* Answer sets for logic programs with arbitrary abstract constraint atoms
#@ Tran Cao Son;Enrico Pontelli;Phan Huy Tu
#t 2006
#c 10
#% 123070
#% 400986
#% 400992
#% 411814
#% 464918
#% 480289
#% 484342
#% 790729
#% 1250132
#% 1269469
#% 1279333
#% 1656398
#% 1656416
#% 1656428
#! We present two equivalent approaches for defining answer sets for logic programs with arbitrary abstract constraint atoms (c-atoms). The first approach uses an immediate consequence operator for answer set checking. whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The second approach generalizes the notion of well-supported models of normal logic programs to programs with c-atoms. We prove that the newly defined semantics coincides with previously introduced semantics for logic programs with monotone c-atoms and extends the original answer set semantics for normal logic programs. We discuss different possibilities for treating negation-as-failure c-atoms and characterize situations in which they yield the same answer sets. We study some properties of answer sets of programs with c-atoms and relate our definition to several semantics for logic programs with aggregates.

#index 1250528
#* An asymptotically optimal algorithm for the max k-armed bandit problem
#@ Matthew J. Streeter;Stephen F. Smith
#t 2006
#c 10
#% 135414
#% 1269575
#! We present an asymptotically optimal algorithm for the max variant of the k-armed bandit problem. Given a set of k slot machines, each yielding payoff from a fixed (but unknown) distribution, we wish to allocate trials to the machines so as to maximize the expected maximum payoff received over a series of n trials. Subject to certain distributional assumptions, we show that O(k ln(k/δ) ln(n)2/ε2) trials are sufficient to identify, with probability at least 1 - δ, a machine whose expected maximum payoff is within ε of optimal. This result leads to a strategy for solving the problem that is asymptotically optimal in the following sense: the gap between the expected maximum payoff obtained by using our strategy for n trials and that obtained by pulling the single best arm for all n trials approaches zero as n → ∞.

#index 1250529
#* Solving QBF with combined conjunctive and disjunctive normal form
#@ Lintao Zhang
#t 2006
#c 10
#% 183640
#% 266109
#% 336403
#% 336874
#% 496131
#% 517569
#% 535315
#% 576843
#% 578752
#% 866034
#% 1269402
#% 1272399
#% 1675277
#% 1698720
#% 1698724
#! Similar to most state-of-the-art Boolean Satisfiabilily (SAT) solvers, all contemporary Quantified Boolean Formula (QBF) solvers require inputs to be in the Conjunctive Normal Form (CNF). Most of them also store the QBF in CNF internally for reasoning. In order to use these solvers, arbitrary Boolean formulas have to be transformed into equi-satisfiable formulas in Conjunctive Normal Form by introducing additional variables. In this paper, we point out an inherent limitation of this approach, namely the asymmetric treatment of satisfactions and conflicts. This deficiency leads to artificial increase of search space for QBF solving. To overcome the limitation, we propose to transform a Boolean formula into a combination of an equisatisfiable CNF formula and an equi-tautological DNF formula for QBF solving. QBF solvers based on this approach treat satisfactions and conflicts symmetrically, thus avoiding the exploration of unnecessary search space. A QBF solver called IQTest is implemented based on this idea. Exrerimental results show that it significantly outperforms existing QBF solvers.

#index 1250530
#* Classifying learner engagement through integration of multiple data sources
#@ Carole R. Beal;Lei Qu;Hyokyeong Lee
#t 2006
#c 10
#% 396063
#% 553779
#% 1219548
#! Intelligent tutoring systems (ITS) can provide effective instruction, but learners do not always use such systems effectively. In the present study, high school students' action sequences with a mathematics ITS were machine-classified into five finite-state machines indicating guessing strategies, appropriate help use, and independent problem solving; over 90% of problem events were categorized. Students were grouped via cluster analyses based on self reports of motivation. Motivation grouping predicted ITS strategic approach better than prior math achievement (as rated by classroom teachers). Learners who reported being disengaged in math were most likely to exhibit appropriate help use while working with the ITS, relative to average and high motivation learners. The results indicate that learners can readily report their motivation state and that these data predict how learners interact with the ITS.

#index 1250531
#* Evaluating critiquing-based recommender agents
#@ Li Chen;Pearl Pu
#t 2006
#c 10
#% 420458
#% 445152
#% 573639
#% 605717
#% 720162
#% 754156
#% 790460
#% 808386
#! We describe a user study evaluating two critiquing-based recommender agents based on three criteria: decision accuracy. decision effort, and user confidence. Results show that user-motivated critiques were more frequently applied and the example critiquing system employing only this type of critiques achieved the best results. In particular, the example critiquing agent significantly improves users' decision accuracy with less cognitive effort consumed than the dynamic critiquing recommender with system-proposed critiques. Additionally, the former is more likely to inspire users' confidence of their choice and promote their intention to purchase and return to the agent for future use.

#index 1250532
#* A dynamic mixture model to detect student motivation and proficiency
#@ Jeff Johns;Beverly Woolf
#t 2006
#c 10
#% 553935
#% 751826
#% 857094
#% 1111441
#% 1219541
#% 1219548
#! Unmotivated students do not reap the full rewards of using a computer-based intelligent tutoring system. Detection of improper behavior is thus an important component of an online student model. To meet this challenge, we present a dynamic mixture model based on Item Response Theory. This model, which simultaneously estimates a student's proficiency and changing motivation level, was tested with data of high school students using a geometry tutoring system. By accounting for student motivation, the dynamic mixture model can more accurately estimate proficiency and the probability of a correct response. The model's generality is an added benefit, making it applicable to many intelligent tutoring systems as well as other domains.

#index 1250533
#* Modeling human decision making in cliff-edge environments
#@ Ron Katz;Sarit Kraus
#t 2006
#c 10
#% 384911
#% 578714
#% 890309
#! In this paper we propose a model for human leaming and decision making in environments of repeated Cliff-Edge (CE) interactions. In CE environments, which include common daily interactions, such as sealed-bid auctions and the Ultimatum Game (UG), the probability of success decreases monotonically as the expected reward increases. Thus, CE environments are characterized by an underlying conflict between the strive to maximize profits and the fear of causing the entire deal to fall through. We focus on the behavior of people who repeatedly compete in one-shot CE interactions, with a different opponent in each interaction. Our model, which is based upon the Deviated Virtual Reinforcement Learning (DVRL) algorithm, integrates the Learning Direction Theory with the Reinforcement Learning algorithm. We also examined several other models, using an innovative methodology in which the decision dynamics of the models were compared with the empirical decision patterns of individuals during their interactions. An analysis of human behavior in auctions and in the UG reveals that our model fits the decision patterns of far more subjects than any other model.

#index 1250534
#* Using anticipation to create believable behaviour
#@ Carlos Martinho;Ana Paiva
#t 2006
#c 10
#% 238395
#% 495947
#% 580301
#% 643118
#! Although anticipation is an important part of creating believable behaviour, it has had but a secondary role in the field of life-like characters. In this paper, we show how a simple anticipatory mechanism can be used to control the behaviour of a synthetic character implemented as a software agent, without disrupting the user's suspension of disbelief. We describe the emotivector, an anticipatory mechanism coupled with a sensor, that: (1) uses the history of the sensor to anticipate the next sensor state; (2) interprets the mismatch between the prediction and the sensed value, by computing its attention grabbing potential and associating a basic qualitative sensation with the signal; (3) sends its interpretation along with the signal. When a signal from the sensor reaches the processing module of the agent, it carries recommendations such as: "you should seriously take this signal into consideration, as it is much better than we had expected" or "Just forget about this one, it is as bad as we predicted". We delineate several strategies to manage several emotivectors at once and show how one of these strategies (meta-anticipation) transparently introduces the concept of uncertainty. Finally, we describe an experiment in which an emotivector-controlled synthetic character interacts with the user in the context of a word-puzzle game and present the evaluation supporting the adequacy of our approach.

#index 1250535
#* Extracting knowledge about users' activities from raw workstation contents
#@ Tom M. Mitchell;Sophie H. Wang;Yifen Huang;Adam Cheyer
#t 2006
#c 10
#% 311027
#% 340962
#% 466564
#% 730950
#% 752006
#% 790445
#% 848637
#% 1289476
#! A long-standing goal of AI is the development of intelligent workstation-based personal agents to assist users in their daily lives. A key impediment to this goal is the unrealistic cost of developing and maintaining a detailed knowledge base describing the user's different activities, and which people, meetings, emails, etc. are affiliated with each such activity. This paper presents a clustering approach to automatically acquiring such a knowledge base by analyzing the raw contents of the workstation, including emails, contact person names, and online calendar meetings. Our approach analyzes the distribution of email words, the social network of email senders and recipients, and the results of Google Desktop Search queried with text from online calendar entries and person contact names. For each cluster it constructs, the program outputs a frame-based representation of the conesponding user activity. This paper describes our approach and experimentally assesses its perfonnance over the workstations of three different users.

#index 1250536
#* Probabilistic goal recognition in interactive narrative environments
#@ Bradford Mott;Sunyoung Lee;James Lester
#t 2006
#c 10
#% 147680
#% 241026
#% 334588
#% 368080
#% 423981
#% 567880
#% 643172
#% 705161
#% 706640
#% 781490
#% 823848
#% 838143
#% 1250306
#% 1279397
#! Recent years have witnessed a growing interest in interactive narrative-centered virtual environments for education, training, and entertainment. Narrative environments dynamically craft engaging story-based experiences for users, who are themselves active participants in unfolding stories. A key challenge posed by interactive narrative is recognizing users' goals so that narrative planners can dynamically orchestrate plot elements and character actions to create rich, customized stories. In this paper we present an inductive approach to predicting users' goals by learning probabilistic goal recognition models. This approach has been evaluated in a narrative environment for the domain of microbiology in which the user plays the role of a medical detective solving a science mystery. An empirical evaluation of goal recognition based on n-gram models and Bayesian networks suggests that the models offer significant predictive power.

#index 1250537
#* Salience in orientation-filter response measured as suspicious coincidence in natural images
#@ Subramonia Sarma;Yoonsuck Choe
#t 2006
#c 10
#% 424079
#% 625167
#% 635712
#% 1042844
#! Visual cortex neurons have receptive fields resembling oriented bandpass filters, and their response distributions on natural images are non-Gaussian. Inspired by this, we previously showed that comparing the response distribution to normal distribution with the same variance gives a good thresholding criterion for detecting salient levels of edginess in images. However, (1) the results were based on comparison with human data, thus, an objective, quantitative performance measure was not taken. Furthermore, (2) why a normal distribution would serve as a good baseline was not investigated in full. In this paper, we first conduct a quantitative analysis of the normal-distribution baseline, using artificial images that closely mimic the statistics of natural images. Since in these artificial images, we can control and obtain the exact saliency information, the performance of the thresholding algorithm can be measured objectively. We then interpret the issue of the normal distribution being an effective baseline for thresholding, under the general concept of suspicious coincidence proposed by Barlow. It turns out that salience defined our way can be understood as a deviation from the unsuspicious baseline. Our results show that the response distribution on white-noise images (where there is no structure, thus zero salience and nothing suspicious) has a near-Gaussian distribution. We then show that the response threshold directly calculated from the response distribution to white-noise images closely matches that of humans, providing further support for the analysis. In sum, our results and analysis show an intimate relationship among subjective perceptual measure of salience, objective measures of salience using normal distributions as a baseline, and the theory of suspicious coincidence.

#index 1250538
#* From pigeons to humans: grounding relational learning in concrete examples
#@ Marc T. Tomlinson;Bradley C. Love
#t 2006
#c 10
#% 65345
#! We present a cognitive model that bridges work in analogy and category learning. The model, Building Relations through Instance Driven Gradient Error Shifting (BRIDGES), extends ALCOVE, an exemplar-based connectionist model of human category learning (Kruschke, 1992). Unlike ALCOVE which is limited to featural or spatial representations, BRIDGES can appreciate analogical relationships between stimuli and stored predicate representations of exemplars. Like ALCOVE, BRIDGES learns to shift attention over the course of learning to reduce error and, in the process, alters its notion of similarity. A shift toward relational sources of similarity allows BRIDGES to display what appears to be an understanding of abstract domains, when in fact performance is driven by similarity-based structural alignment (i.e., analogy) to stored exemplars. Supportive simulations of animal, infant, and adult learning are provided. We end by considering possible extensions of BRIDGES suitable for computationally demanding applications.

#index 1250539
#* Evaluating preference-based search tools: a tale of two approaches
#@ Paolo Viappiani;Boi Faltings;Pearl Pu
#t 2006
#c 10
#% 248010
#% 297577
#% 428440
#% 445152
#% 490785
#% 573639
#% 779972
#% 787001
#% 808386
#% 1269443
#% 1279229
#% 1289344
#% 1389375
#% 1389761
#% 1706019
#! People frequently use the world-wide web to find their most preferred item among a large range of options. We call this task preference-based search. The most common tool for preference-based search on the WWW today obtains users' preferences by asking them to fill in a form. It then returns a list of items that most closely match these preferences. Recently, several researchers have proposed tools for preference-based search that elicit preferences from the critiques a user actively makes on examples shown to them. We carried out a user study in order to compare the performance of traditional preference-based search tools using form-filling with two different versions of an example-critiquing tool. The results show that example critiquing achieves almost three times the decision accuracy, while requiring only slightly higher interaction effort.

#index 1250540
#* Model-checking memory requirements of resource-bounded reasoners
#@ A. Albore;N. Alechina;P. Bertoli;C. Ghidini;B. Logan;L. Serafini
#t 2006
#c 10
#% 7051
#% 188086
#% 414950
#% 655322
#% 773267
#% 1223252
#% 1476298
#! Memory bounds may limit the ability of a reasoner to make inferences and therefore affect the reasoner's usefulness. In this paper, we propose a framework to automatically verify the reasoning capabilities of propositional memory-bounded reasoners which have a sequential architecture. Our framework explicitly accounts for the use of memory both to store facts and to support backtracking in the course of deductions. We describe an implementation of our framework in which proof existence is recast as a strong planning problem, and present results of experiments using the MBP planner which indicate that memory bounds may not be trivial to infer even for simple problems, and that memory bounds and length of derivations are closely inter-related.

#index 1250541
#* Explaining qualitative decision under uncertainty by argumentation
#@ Leila Amgoud;Henri Prade
#t 2006
#c 10
#% 77841
#% 116292
#% 179919
#% 277105
#% 315415
#% 431089
#% 442844
#% 788037
#% 819617
#% 1650765
#! Decision making under uncertainty is usually based on the comparative evaluation of different alternatives by means of a decision criterion. In a qualitative setting, pessimistic and optimistic criteria have been proposed. In that setting, the whole decision process is compacted into a criterion formula on the basis of which alternatives are compared. It is thus impossible for an end user to understand why an alternative is good, or better than another. Besides, argumentation is a powerful tool for explaining inferences, decisions, etc. This paper articulates optimistic and pessimistic decision criteria in terms of an argumentation process that consists of constructing arguments in favor/against decisions, evaluating the strengths of those arguments, and comparing pairs of alternatives on the basis of their supporting/attacking arguments.

#index 1250542
#* Compilation of query-rewriting problems into tractable fragments of propositional logic
#@ Yolifé Arvelo;Blai Bonet;María Esther Vidal
#t 2006
#c 10
#% 121397
#% 198465
#% 204396
#% 237190
#% 296931
#% 300138
#% 303884
#% 333964
#% 342378
#% 379503
#% 464203
#% 496091
#% 572307
#% 572311
#% 599549
#% 936786
#% 1272349
#% 1499470
#% 1499471
#! We consider the problem of rewriting a query efficiently using materialized views. In the context of information integration, this problem has received significant attention in the scope of emerging infrastructures such as WWW, Semantic Web, Grid, and P2P which require efficient algorithms. The problem is in general intractable, and the current algorithms do not scale well when the number of views or the size of the query grow. We show however that this problem can be encoded as a propositional theory in CNF such that its models are in correspondence with the rewritings of the query. The theory is then compiled into a normal form, that is called d-DNNF and supports several operations like model counting and enumeration in polynomial time (in the size of the compiled theory), for computing the rewritings. Although this method is also intractable in the general case, it is not necessarily so in all cases. We have developed, along these lines and from off-the-shelf propositional engines, novel algorithms for finding maximally-contained rewritings of the query given the set of accessible resources (views). The algorithms scale much better than the current state-of-the-art algorithm, the MiniCon algorithm, over a large number of benchmarks and show in some cases improvements in performance of a couple orders-of-magnitude.

#index 1250543
#* Goal specification, non-determinism and quantifying over policies
#@ Chitta Baral;Jicheng Zhao
#t 2006
#c 10
#% 413871
#% 417597
#% 578723
#% 655322
#% 823865
#% 1289213
#% 1289217
#! One important aspect in directing cognitive robots or agents is to formally specify what is expected of them. This is often referred to as goal specification. Temporal logics such as LTL, and CTL* have been used to specify goals of cognitive robots and agents when their actions have deterministic consequences. It has been suggested that in domains where actions have non-deterministic effects, temporal logics may not be able to express many intuitive and useful goals. In this paper we first show that this is indeed true with respect to existing temporal logics such as LTL, CTL*, and π-CTL*. We then propose the language, P-CTL*, which includes the quantifiers, exist a policy and for all policies. We show that this language allows for the specification of richer goals, including many intuitive and useful goals mentioned in the literature which cannot be expressed in existing temporal languages. We generalize our approach of showing the limitations of π-CTL* to develop a framework to compare expressiveness of goal languages.

#index 1250544
#* Forgetting and conflict resolving in disjunctive logic programming
#@ Thomas Eiter;Kewen Wang
#t 2006
#c 10
#% 77167
#% 342829
#% 411814
#% 417649
#% 772065
#% 801768
#% 880394
#% 1269466
#% 1271987
#% 1289451
#! We establish a declarative theory of forgetting for disjunctive logic programs. The suitability of this theory is justified by a number of desirable properties. In particular, one of our results shows that our notion of forgetting is completely captured by the classical forgetting. A transformation-based algorithm is also developed for computing the result of forgetting. We also provide an analysis of computational complexity. As an application of our approach, a fairly general framework for resolving conflicts in inconsistent knowledge bases represented by disjunctive logic programs is defined. The basic idea of our framework is to weaken the preferences of each agent by forgetting certain knowledge that causes inconsistency. In particular, we show how to use the notion of forgetting to provide an elegant solution for preference elicitation in disjunctive logic programming.

#index 1250545
#* Elementary sets for logic programs
#@ Martin Gebser;Joohyung Lee;Yuliya Lierler
#t 2006
#c 10
#% 103705
#% 231786
#% 268779
#% 772065
#% 1250128
#% 1289431
#% 1656393
#! By introducing the concepts of a loop and a loop formula, Lin and Zhao showed that the answer sets of a nondisjunctive logic program are exactly the models of its Clark's completion that satisfy the loop formulas of all loops. Recently, Gebser and Schaub showed that the Lin-Zhao theorem remains correct even if we restrict loop formulas to a special class of loops called "elementary loops." In this paper, we simplify and generalize the notion of an elementary loop, and clarify its role. We propose the notion of an elementary set, which is almost equivalent to the notion of an elementary loop for nondisjunctive programs, but is simpler, and, unlike elementary loops, can be extended to disjunctive programs without producing unintuitive results. We show that the maximal unfounded elementary sets for the "relevant" part of a program are exactly the minimal sets among the nonempty unfounded sets. We also present a graph-theoretic characterization of elementary sets for nondisjunctive programs, which is simpler than the one proposed in (Gebser & Schaub 2005). Unlike the case of nondisjunctive programs, we show that the problem of deciding an elementary set is coNP-complete for disjunctive programs.

#index 1250546
#* Bounded treewidth as a key to tractability of knowledge representation and reasoning
#@ Georg Gottlob;Reinhard Pichler;Fang Wei
#t 2006
#c 10
#% 1675
#% 57551
#% 93660
#% 101944
#% 115509
#% 136678
#% 181220
#% 219474
#% 400988
#% 427161
#% 567923
#% 1669580
#% 1972413
#! Several forms of reasoning in AI - like abduction, closed world reasoning, circumscription, and disjunctive logic programming - are well known to be intractable. In fact, many of the relevant problems are on the second or third level of the polynomial hierarchy. In this paper, we show how the powerful notion of treewidth can be fruitfully applied to this area. In particular, we show that all these problems become tractable (actually, even solvable in linear time), if the treewidth of the involved formulae (or of the disjunctive logic programs, resp.) is bounded by some constant. Experiments with a prototype implementation prove the feasibility of this new approach, in principle, and also give us hints for necessary improvements. In many areas of computer science, bounded treewidth has been shown to be a realistic and practically relevant restriction. We thus argue that bounded treewidth is a key factor in the development of efficient algorithms also in knowledge representation and reasoning - despite the high worst case complexity of the problems of interest.

#index 1250547
#* Belief change in the context of fallible actions and observations
#@ Aaron Hunter;James P. Delgrande
#t 2006
#c 10
#% 224753
#% 1289424
#! We consider the iterated belief change that occurs following an alternating sequence of actions and observations. At each instant, an agent has some beliefs about the action that occurs as well as beliefs about the resulting state of the world. We represent such problems by a sequence of ranking functions, so an agent assigns a quantitative plausibility value to every action and every state at each point in time. The resulting formalism is able to represent fallible knowledge, erroneous perception, exogenous actions, and failed actions. We illustrate that our framework is a generalization of several existing approaches to belief change, and it appropriately captures the non-elementary interaction between belief update and belief revision.

#index 1250548
#* Towards an axiom system for default logic
#@ Gerhard Lakemeyer;Hector J. Levesque
#t 2006
#c 10
#% 1146
#% 39263
#% 68240
#% 144552
#% 175359
#% 184797
#% 231748
#% 326595
#% 398266
#% 450455
#% 517131
#% 780339
#% 1269458
#! Recently, Lakemeyer and Levesque proposed a logic of only-knowing which precisely captures three forms of nonmonotonic reasoning: Moore's Autoepistemic Logic, Konolige's variant based on moderately grounded expansions, and Reiter's default logic. Defaults have a uniform representation under all three interpretations in the new logic. Moreover, the logic itself is monotonic, that is, nonmonotonic reasoning is cast in terms of validity in the classical sense. While Lakemeyer and Levesque gave a model-theoretic account of their logic, a proof-theoretic characterization remained open. This paper fills that gap for the propositional subset: a sound and complete axiom system in the new logic for all three varieties of default reasoning. We also present formal derivations for some examples of default reasoning. Finally we present evidence that it is unlikely that a complete axiom system exists in the first-order case, even when restricted to the simplest forms of default reasoning.

#index 1250549
#* Finding maximally satisfiable terminologies for the description logic ALC
#@ Thomas Meyer;Kevin Lee;Richard Booth;Jeff Z. Pan
#t 2006
#c 10
#% 21137
#% 101435
#% 517280
#% 561740
#% 665859
#% 1269460
#% 1269464
#% 1279264
#% 1374393
#! For ontologies represented as Description Logic Tboxes, optimised DL reasoners are able to detect logical errors, but there is comparatively limited support for resolving such problems. One possible remedy is to weaken the available information to the extent that the errors disappear, but to limit the weakening process as much as possible. The most obvious way to do so is to remove just enough Tbox sentences to eliminate the errors. In this paper we propose a tableau-like procedure for finding maximally concept-satisfiable terminologies represented in the description logic ALC. We discuss some optimisation techniques, and report on preliminary, but encouraging, experimental results.

#index 1250550
#* Characterizing data complexity for conjunctive query answering in expressive description logics
#@ Magdalena Ortiz;Diego Calvanese;Thomas Eiter
#t 2006
#c 10
#% 248026
#% 248038
#% 263136
#% 378409
#% 445447
#% 529498
#% 531450
#% 561419
#% 598376
#% 665856
#% 665867
#% 1269453
#% 1289422
#% 1289425
#! Description Logics (DLs) are the formal foundations of the standard web ontology languages OWL-DL and OWL-Lite. In the Semantic Web and other domains, ontologies are increasingly seen also as a mechanism to access and query data repositories. This novel context poses an original combination of challenges that has not been addressed before: (i) sufficient expressive power of the DL to capture common data modeling constructs; (ii) well established and flexible query mechanisms such as Conjunctive Queries (CQs); (iii) optimization of inference techniques with respect to data size, which typically dominates the size of ontologies. This calls for investigating data complexity of query answering in expressive DLs. While the complexity of DLs has been studied extensively, data complexity has been characterized only for answering atomic queries, and was still open for answering CQs in expressive DLs. We tackle this issue and prove a tight CONP upper bound for the problem in SHIQ, as long as no transitive roles occur in the query. We thus establish that for a whole range of DLs from AL to SHIQ, answering CQs with no transitive roles has CONP-complete data complexity. We obtain our result by a novel tableaux-based algorithm for checking query entailment, inspired by the one in [19], but which manages the technical challenges of simultaneous inverse roles and number restrictions (which leads to a DL lacking the finite model property).

#index 1250551
#* Merging stratified knowledge bases under constraints
#@ Guilin Qi;Weiru Liu;David A. Bell
#t 2006
#c 10
#% 167544
#% 417813
#% 442755
#% 443185
#% 520900
#% 763750
#% 767712
#% 772063
#% 1289418
#! In this paper, we propose a family of operators for merging stratified knowledge bases under integrity constraints. The operators are defined in a model-theoretic way. Our merging operators can be used to merge stratified knowledge bases where no numerical information is available. Furthermore, the original knowledge bases to be merged can be individually inconsistent. Both logical properties and computational complexity issues of the operators are studied.

#index 1250552
#* Reconciling situation calculus and fluent calculus
#@ Stephan Schiffel;Michael Thielscher
#t 2006
#c 10
#% 117869
#% 229083
#% 284647
#% 314845
#% 340735
#% 815506
#% 1576997
#! The Situation Calculus and the Fluent Calculus are successful action formalisms that share many concepts. But until now there is no formal relation between the two calculi that would allow to formally analyze the relationship between the two approaches as well as between the programming languages based on them, Golog and FLUX. Furthermore, such a formal relation would allow to combine Golog and FLUX and to analyze which of the underlying computation principles is better suited for different classes of programs. We develop a formal translation between domain axiomatizations of the Situation Calculus and the Fluent Calculus and present a Fluent Calculus semantics for Golog programs. For domains with deterministic actions our approach allows an automatic translation of Golog domain descriptions and execution of Golog programs with FLUX.

#index 1250553
#* Classification spanning private databases
#@ Ke Wang;Yabo Xu;Rong She;Philip S. Yu
#t 2006
#c 10
#% 136350
#% 300184
#% 479640
#% 479787
#% 572459
#% 635215
#% 654448
#% 1068712
#! In this paper, we study the classification problem involving information spanning multiple private databases. The privacy challenges lie in the facts that data cannot be collected in one place and the classifier itself may disclose private information. We present a novel solution that builds the same decision tree classifier as if data are collected in a central place, but preserves the privacy of participating sites.

#index 1250554
#* On the complexity of linking deductive and abstract argument systems
#@ Michael Wooldridge;Paul E. Dunne;Simon Parsons
#t 2006
#c 10
#% 121990
#% 159242
#% 198464
#% 337502
#% 428337
#% 836006
#! We investigate the computational complexity of a number of questions relating to deductive argument systems, in particular the complexity of linking deductive and more abstract argument systems. We start by presenting a simple model of deductive arguments based on propositional logic, and define logical equivalence and defeat over individual arguments. We then extend logical equivalence to sets of arguments, and show that the problem of checking equivalence of argument sets is co-NP-complete. We also show that the problem of checking that an argument set contains no two logically equivalent arguments is NP-complete, while the problem of checking that a set of arguments is maximal (i.e., that no argument could be added without such an argument being logically equivalent to one that is already present) is co-NP-complete. We then show that checking whether a digraph over an argument set is sound with respect to the defeat relation is co-NP-complete, while the problem of showing that such a digraph is complete is NP-complete, and the problem of showing both soundness and completeness is Dp-complete.

#index 1250555
#* A unified knowledge based approach for sense disambiguationm and semantic role labeling
#@ Peter Z. Yeh;Bruce Porter;Ken Barker
#t 2006
#c 10
#% 2298
#% 207677
#% 286069
#% 341640
#% 405391
#% 452991
#% 515699
#% 532186
#% 708351
#% 723412
#% 727824
#% 747891
#% 828974
#% 939403
#% 939810
#% 939834
#% 939964
#% 995513
#% 1269528
#% 1344851
#% 1414358
#! In this paper, we present a unified knowledge based approach for sense disambiguation and semantic role labeling. Our approach performs both tasks through a single algorithm that matches candidate semantic interpretations to background knowledge to select the best matching candidate. We evaluate our approach on a corpus of sentences collected from various domains and show how our approach performs well on both sense disambiguation and semantic role labeling.

#index 1250556
#* Clustering by exceptions
#@ Fabrizio Angiulli
#t 2006
#c 10
#% 210173
#% 248790
#% 273890
#% 296738
#% 342827
#% 375388
#% 420078
#% 438137
#% 722902
#% 818916
#% 1013086
#! A density-based clustering algorithm, called OUTCLUST, is presented. The algorithm exploits a notion of local density in order to find homogeneous groups of objects as opposite to objects mostly deviating from the overall population. The proposed algorithm tries to simultaneously consider several features of real data sets, namely finding clusters of different shapes and densities in high dimensional data in presence of noise. It is shown that the method is able to identify very meaningful clusters, and experimental comparison with partitioning, hierarchial, and density-based clustering algorithms, is presented, pointing out that the algorithm achieves good clustering quality.

#index 1250557
#* On the difficulty of modular reinforcement learning for real-world partial programming
#@ Sooraj Bhat;Charles L. Isbell;Michael Mateas
#t 2006
#c 10
#% 107140
#% 240248
#% 445553
#% 466066
#% 1279444
#! In recent years there has been a great deal of interest in "modular reinforcement learning" (MRL). Typically, problems are decomposed into concurrent subgoals, allowing increased scalability and state abstraction. An arbitrator combines the subagents' preferences to select an action. In this work, we contrast treating an MRL agent as a set of subagents with the same goal with treating an MRL agent as a set of subagents who may have different, possibly conflicting goals. We argue that the latter is a more realistic description of real-world problems, especially when building partial programs. We address a range of algorithms for single-goal MRL, and leveraging social choice theory, we present an impossibility result for applications of such algorithms to multigoal MRL. We suggest an alternative formulation of arbitration as scheduling that avoids the assumptions of comparability of preference that are implicit in single-goal MRL. A notable feature of this formulation is the explicit codification of the tradeoffs between the subproblems. Finally, we introduce A2BL, a language that encapsulates many of these ideas.

#index 1250558
#* On combining multiple classifiers using an evidential approach
#@ Yaxin Bi;Sally McClean;Terry Anderson
#t 2006
#c 10
#% 251145
#% 262059
#% 344447
#% 458379
#% 466572
#% 738972
#% 1784395
#! Combining multiple classifiers via combining schemes or meta-learners has led to substantial improvements in many classification problems. One of the challenging tasks is to choose appropriate combining schemes and classifiers involved in an ensemble of classifiers. In this paper we propose a novel evidential approach to combining decisions given by multiple classifiers. We develop a novel evidence structure - a focal triplet, examine its theoretical properties and establish computational formulations for representing classifier outputs as pieces of evidence to be combined. The evaluations on the effectiveness of the established formalism have been carried out over the data sets of 20- newsgroup and Reuters-21578, demonstrating the advantage of this novel approach in combining classifiers.

#index 1250559
#* Tensor embedding methods
#@ Guang Dai;Dit-Yan Yeung
#t 2006
#c 10
#% 593047
#% 727684
#% 729437
#% 770769
#% 812492
#% 812580
#% 815967
#% 836827
#% 855573
#! Over the past few years, some embedding methods have been proposed for feature extraction and dimensionality reduction in various machine learning and pattern classification tasks. Among the methods proposed are Neighborhood Preserving Embedding (NPE), Locality Preserving Projection (LPP) and Local Discriminant Embedding (LDE) which have been used in such applications as face recognition and image/video retrieval. However, although the data in these applications are more naturally represented as higher-order tensors, the embedding methods can only work with vectorized data representations which may not capture well some useful information in the original data. Moreover, high-dimensional vectorized representations also suffer from the curse of dimensionality and the high computational demand. In this paper, we propose some novel tensor embedding methods which, unlike previous methods, take data directly in the form of tensors of arbitrary order as input. These methods allow the relationships between dimensions of a tensor representation to be efficiently characterized. Moreover, they also allow the intrinsic local geometric and topological properties of the manifold embedded in a tensor space to be naturally estimated. Furthermore, they do not suffer from the curse of dimensionality and the high computational demand. We demonstrate the effectiveness of the proposed tensor embedding methods on a face recognition application and compare them with some previous methods. Extensive experiments show that our methods are not only more effective but also more efficient.

#index 1250560
#* Identifying and generating easy sets of constraints for clustering
#@ Ian Davidson;S. S. Ravi
#t 2006
#c 10
#% 464291
#% 770782
#! Clustering under constraints is a recent innovation in the artificial intelligence community that has yielded significant practical benefit. However, recent work has shown that for some negative forms of constraints the associated subproblem of just finding a feasible clustering is NP-complete. These worst case results for the entire problem class say nothing of where and how prevalent easy problem instances are. In this work, we show that there are large pockets within these problem classes where clustering under constraints is easy and that using easy sets of constraints yields better empirical results. We then illustrate several sufficient conditions from graph theory to identify a priori where these easy problem instances are and present algorithms to create large and easy to satisfy constraint sets.

#index 1250561
#* Nonnegative matrix factorization and probabilistic latent semantic indexing: equivalence, chi-square statistic, and a hybrid method
#@ Chris Ding;Tao Li;Wei Peng
#t 2006
#c 10
#% 252836
#% 643008
#% 722904
#% 755463
#% 818291
#% 823343
#% 1650298
#! Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of local minima of the other method successively, thus achieving better final solution. Extensive experiments on 5 real-life datasets show relations between NMF and PLSI, and indicate the hybrid method lead to significant improvements over NMF-only or PLSI-only methods. We also show that at first order approximation, NMF is identical to χ2-statistic.

#index 1250562
#* Any time induction of decision trees: an iterative improvement approach
#@ Saher Esmeir;Shaul Markovitch
#t 2006
#c 10
#% 26125
#% 92542
#% 100168
#% 101468
#% 110379
#% 136350
#% 190581
#% 205385
#% 209021
#% 216715
#% 246747
#% 413955
#% 420102
#% 449529
#% 464290
#% 466240
#% 703975
#% 770795
#% 770814
#% 926881
#% 1673023
#! Most existing decision tree inducers are very fast due to their greedy approach. In many real-life applications, however, we are willing to allocate more time to get better decision trees. Our recently introduced LSID3 contract anytime algorithm allows computation speed to be traded for better tree quality. As a contract algorithm, LSID3 must be allocated its resources a priori, which is not always possible. In this work, we present IIDT, a general framework for interruptible induction of decision trees that need not be allocated resources a priori. The core of our proposed framework is an iterative improvement algorithm that repeatedly selects a subtree whose reconstruction is expected to yield the highest marginal utility. The algorithm then rebuilds the subtree with a higher allocation of resources. IIDT can also be configured to receive training examples as they become available, and is thus appropriate for incremental learning tasks. Empirical evaluation with several hard concepts shows that IIDT exhibits good anytime behavior and significantly outperforms greedy inducers when more time is available. A comparison of IIDT to several modern decision tree learners showed it to be superior.

#index 1250563
#* Incremental least-squares temporal difference learning
#@ Alborz Geramifard;Michael Bowling;Richard S. Sutton
#t 2006
#c 10
#% 92142
#% 151769
#% 160859
#% 203596
#% 384911
#% 425076
#% 449561
#% 466235
#% 1271971
#! Approximate policy evaluation with linear function approximation is a commonly arising problem in reinforcement learning, usually solved using temporal difference (TD) algorithms. In this paper we introduce a new variant of linear TD learning, called incremental least-squares TD learning, or iLSTD. This method is more data efficient than conventional TD algorithms such as TD(0) and is more computationally efficient than non-incremental least-squares TD methods such as LSTD (Bradtke & Barto 1996; Boyan 1999). In particular, we show that the per-time-step complexities of iLSTD and TD(0) are O(n), where n is the number of features, whereas that of LSTD is O(n2). This difference can be decisive in modern applications of reinforcement learning where the use of a large number features has proven to be an effective solution strategy. We present empirical comparisons, using the test problem introduced by Boyan (1999), in which iLSTD converges faster than TD(0) and almost as fast as LSTD.

#index 1250564
#* Active learning with near misses
#@ Nela Gurevich;Shaul Markovitch;Ehud Rivlin
#t 2006
#c 10
#% 85573
#% 109408
#% 170649
#% 195101
#% 451034
#% 493092
#% 735357
#% 751439
#% 770807
#% 1042788
#% 1269509
#% 1478810
#! Assume that we are trying to build a visual recognizer for a particular class of objects--chairs, for example--using existing induction methods. Assume the assistance of a human teacher who can label an image of an object as a positive or a negative example. As positive examples, we can obviously use images of real chairs. It is not clear, however, what types of objects we should use as negative examples. This is an example of a common problem where the concept we are trying to learn represents a small fraction of a large universe of instances. In this work we suggest learning with the help of near misses--negative examples that differ from the learned concept in only a small number of significant points, and we propose a framework for automatic generation of such examples. We show that generating near misses in the feature space is problematic in some domains, and propose a methodology for generating examples directly in the instance space using modification operators--functions over the instance space that produce new instances by slightly modifying existing ones. The generated instances are evaluated by mapping them into the feature space and measuring their utility using known active learning techniques. We apply the proposed framework to the task of learning visual concepts from range images.

#index 1250565
#* Representing systems with hidden state
#@ Christopher Hundt;Prakash Panagaden;Joelle Pineau;Doina Precup
#t 2006
#c 10
#% 158924
#% 160327
#% 160859
#% 252183
#% 366058
#% 702594
#% 788097
#% 1271848
#% 1289461
#% 1289562
#! We discuss the problem of finding a good state representation in stochastic systems with observations. We develop a duality theory that generalizes existing work in predictive state representations as well as automata theory. We discuss how this theoretical framework can be used to build learning algorithms, approximate planning algorithms as well as to deal with continuous observations.

#index 1250566
#* Improving approximate value iteration using memories and predictive state representations
#@ Michael R. James;Ton Wessling;Nikos Vlassis
#t 2006
#c 10
#% 788097
#% 840958
#% 857087
#% 1269515
#% 1272075
#% 1279358
#% 1289468
#% 1289695
#% 1699604
#! Planning in partially-observable dynamical systems is a challenging problem, and recent developments in point-based techniques, such as Perseus significantly improve performance as compared to exact techniques. In this paper, we show how to apply these techniques to new models for non-Markovian dynamical systems called Predictive State Representatiolls (PSRs) and Memory-PSRs (mPSRs). PSRs and mPSRs are models of non-Markovian decision processes that differ from latent-variable models (e.g. HMMs, POMDPs) by representing state using only observable quantities. Further, mPSRs explicitly represent certain structural properties of the dynamical system that are also relevant to planning. We show how planning techniques can be adapted to leverage this structure to improve performance both in terms of execution time as well as quality of the resulting policy.

#index 1250567
#* Learning systems of concepts with an infinite relational model
#@ Charles Kemp;Joshua B. Tenenbaum;Thomas L. Griffiths;Takeshi Yamada;Naonori Ueda
#t 2006
#c 10
#% 73518
#% 495929
#% 578775
#% 722914
#% 840890
#% 868088
#% 1289267
#! Relationships between concepts account for a large proportion of semantic knowledge. We present a nonparametric Bayesian model that discovers systems of related concepts. Given data involving several sets of entities, our model discovers the kinds of entities in each set and the relations between kinds that are possible or likely. We apply our approach to four problems: clustering objects and features, learning ontologies, discovering kinship systems, and discovering structure in political data.

#index 1250568
#* kFOIL: learning simple relational kernels
#@ Niels Landwehr;Andrea Passerini;Luc De Raedt;Paolo Frasconi
#t 2006
#c 10
#% 198079
#% 217072
#% 243728
#% 398845
#% 449508
#% 550424
#% 722939
#% 727912
#% 731607
#% 763697
#% 771944
#% 829031
#% 961144
#% 1269484
#% 1499586
#% 1699582
#% 1718525
#! A novel and simple combination of inductive logic programming with kernel methods is presented. The kFOIL algorithm integrates the well-known inductive logic programming system FOIL with kernel methods. The feature space is constructed by leveraging FOIL search for a set of relevant clauses. The search is driven by the performance obtained by a support vector machine based on the resulting kernel. In this way, kFOIL implements a dynamic propositionalization approach. Both classification and regression tasks can be naturally handled. Experiments in applying kFOIL to well-known benchmarks in chemoinformatics show the promise of the approach.

#index 1250569
#* Quantifying the impact of learning algorithm parameter tuning
#@ Niklas Lavesson;Paul Davidsson
#t 2006
#c 10
#% 92148
#% 92533
#% 94926
#% 132938
#% 136350
#% 197394
#% 209021
#% 290482
#% 302400
#% 376266
#% 425040
#% 466086
#% 576214
#% 722805
#% 729437
#% 769882
#% 1862656
#! The impact of learning algorithm optimization by means of parameter tuning is studied. To do this, two quality attributes, sensitivity and classification performance, are investigated, and two metrics for quantifying each of these attributes are suggested. Using these metrics, a systematic comparison has been performed between four induction algorithms on eight data sets. The results indicate that parameter tuning is often more important than the choice of algorithm and there does not seem to be a trade-off between the two quality attributes. Moreover, the study provides quantitative support to the assertion that some algorithms are more robust than others with respect to parameter configuration. Finally, it is briefly described how the quality attributes and their metrics could be used for algorithm selection in a systematic way.

#index 1250570
#* EfficientL1regularized logistic regression
#@ Sun-In Lee;Honglak Lee;Pieter Abbeel;Andrew Y. Ng
#t 2006
#c 10
#% 757953
#% 770857
#% 1861282
#! L1 regularized logistic regression is now a workhorse of machine learning: it is widely used for many classification problems, particularly ones with many features. L1 regularized logistic regression requires solving a convex optimization problem. However, standard algorithms for solving convex optimization problems do not scale well enough to handle the large datasets encountered in many practical settings. In this paper, we propose an efficient algorithm for L1 regularized logistic regression. Our algorithm iteratively approximates the objective function by a quadratic approximation at the current point, while maintaining the L1 constraint. In each iteration, it uses the efficient LARS (Least Angle Regression) algorithm to solve the resulting L1 constrained quadratic optimization problem. Our theoretical results show that our algorithm is guaranteed to converge to the global optimum. Our experiments show that our algorithm significantly outperforms standard algorithms for solving convex optimization problems. Moreover, our algorithm outperforms four previously published algorithms that were specifically designed to solve the L1 regularized logistic regression problem.

#index 1250571
#* Minimum description length principle: generators are preferable to closed patterns
#@ Jinyan Li;Haiquan Li;Limsoon Wong;Jian Pei;Guozhu Dong
#t 2006
#c 10
#% 152934
#% 234979
#% 300120
#% 314836
#% 338594
#% 431033
#% 487998
#% 722493
#% 729933
#% 729984
#% 798763
#% 824931
#! The generators and the unique closed pattern of an equivalence class of itemsets share a common set of transactions. The generators are the minimal ones among the equivalent itemsets, while the closed pattern is the maximum one. As a generator is usually smaller than the closed pattern in cardinality, by the Minimum Description Length Principle, the generator is preferable to the closed pattern in inductive inference and classification. To efficiently discover frequent generators from a large dataset, we develop a depth-first algorithm called Gr-growth. The idea is novel in contrast to traditional breadth-first bottom-up generator-mining algorithms. Our extensive performance study shows that Gr-growth is significantly faster (an order or even two orders of magnitudes when the support thresholds are low) than the existing generator mining algorithms. It can be also faster than the state-of-the-art frequent closed itemset mining algorithms such as FPclose and CLOSET+.

#index 1250572
#* Value-function-based transfer for reinforcement learning using structure mapping
#@ Yaxin Liu;Peter Stone
#t 2006
#c 10
#% 65345
#% 75936
#% 89748
#% 297171
#% 366058
#% 495933
#% 505086
#% 771849
#% 1269498
#% 1272002
#% 1279355
#! Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.

#index 1250573
#* Semi-supervised multi-label learning by constrained non-negative matrix factorization
#@ Yi Liu;Rong Jin;Liu Yang
#t 2006
#c 10
#% 266408
#% 311034
#% 318412
#% 397142
#% 458379
#% 770763
#% 770783
#% 770866
#% 818234
#% 818236
#% 838412
#! We present a novel framework for multi-label learning that explicitly addresses the challenge arising from the large number of classes and a small size of training data. The key assumption behind this work is that two examples tend to have large overlap in their assigned class memberships if they share high similarity in their input patterns. We capitalize this assumption by first computing two sets of similarities, one based on the input patterns of examples, and the other based on the class memberships of the examples. We then search for the optimal assignment of class memberships to the unlabeled data that minimizes the difference between these two sets of similarities. The optimization problem is formulated as a constrained Non-negative Matrix Factorization (NMF) problem, and an algorithm is presented to efficiently find the solution. Compared to the existing approaches for multi-label learning, the proposed approach is advantageous in that it is able to explore both the unlabeled data and the correlation among different classes simultaneously. Experiments with text categorization show that our approach performs significantly better than several state-of-the-art classification techniques when the number of classes is large and the size of training data is small.

#index 1250574
#* A simple and effective method for incorporating advice into kernel methods
#@ Richard Maclin;Jude Shavlik;Trevor Walker;Lisa Torrey
#t 2006
#c 10
#% 272538
#% 384911
#% 464296
#% 769908
#% 793236
#% 1269488
#% 1699584
#% 1699609
#% 1718525
#! We propose a simple mechanism for incorporating advice (prior knowledge), in the form of simple rules, into support-vector methods for both classification and regression. Our approach is based on introducing inequality constraints associated with datapoints that match the advice. These constrained datapoints can be standard examples in the training set, but can also be unlabeled data in a semi-supervised, advice-taking approach. Our new approach is simpler to implement and more efficiently solved than the knowledge-based support vector classification methods of Fung, Mangasarian and Shavlik (2002; 2003) and the knowledge-based support vector regression method of Mangasarian, Shavlik, and Wild (2004), while performing approximately as well as these more complex approaches. Experiments using our new approach on a synthetic task and a reinforcement-learning problem within the RoboCup soccer simulator show that our advice-taking method can significantly outperform a method without advice and perform similarly to prior advice-taking, support-vector machines.

#index 1250575
#* Multi-conditional learning: generative/discriminative training for clustering and classification
#@ Andrew McCallum;Chris Pal;Greg Druck;Xuerui Wang
#t 2006
#c 10
#% 92145
#% 169358
#% 211044
#% 304919
#% 311027
#% 450888
#% 464434
#% 722904
#% 916788
#% 1672995
#% 1810385
#! This paper presents multi-conditional learning (MCL), a training criterion based on a product of multiple conditional likelihoods. When combining the traditional conditional probability of "label given input" with a generative probability of "input given label" the later acts as a surprisingly effective rerularizer. When applied to models with latent variables, MCL combines the structure-discovery capabilities of generative topic models, such as latent Dirichlet allocation and the exponential family harmonium, with the accuracy and robustness of discriminative classifiers, such as logistic regression and conditional random fields. We present results on several standard text data sets showing significant reductions in classification error due to MCL regularization, and substantial gains in precision and recall due to the latent structure discovered under MCL.

#index 1250576
#* Learning blocking schemes for record linkage
#@ Matthew Michelson;Craig A. Knoblock
#t 2006
#c 10
#% 310516
#% 376266
#% 420072
#% 577263
#% 659991
#% 722929
#% 729913
#% 844321
#! Record linkage is the process of matching records across data sets that refer to the same entity. One issue within record linkage is determining which record pairs to consider, since a detailed comparison between all of the records is impractical. Blocking addresses this issue by generating candidate matches as a preprocessing step for record linkage. For example, in a person matching problem, blocking might return all people with the same last name as candidate matches. Two main problems in blocking are the selection of attributes for generating the candidate matches and deciding which methods to use to compare the selected attributes. These attribute and method choices constitute a blocking scheme. Previous approaches to record linkage address the blocking issue in a largely ad-hoc fashion. This paper presents a machine learning approach to automatically learn effective blocking schemes. We validate our approach with experiments that show our learned blocking schemes outperform the ad-hoc blocking schemes of non-experts and perform comparably to those manually built by a domain expert.

#index 1250577
#* Strategy variations in analogical problem solving
#@ Tom Y. Ouyang;Kenneth D. Forbus
#t 2006
#c 10
#% 109848
#% 136370
#% 156189
#% 168280
#% 179766
#% 300533
#% 359837
#% 420653
#% 529803
#% 578785
#! While it is commonly agreed that analogy is useful in human problem solving, exactly how analogy can and should be used remains an intriguing problem. VanLehn (1998) for instance argues that there are differences in how novices and experts use analogy, but the VanLehn and Jones (1993) Cascade model does not implement these differences. This paper analyzes several variations in strategies for using analogy to explore possible sources of novice/expert differences. We describe a series of ablation experiments on an expert model to examine the effects of strategy variations in using analogy in problem solving. We provide evidence that failing to use qualitative reasoning when encoding problems, being careless in validating analogical inferences, and not using multiple retrievals can degrade the efficiency of problem-solving.

#index 1250578
#* Gradient boosting for sequence alignment
#@ Charles Parker;Alan Fern;Prasad Tadepalli
#t 2006
#c 10
#% 95730
#% 464434
#% 770763
#% 770850
#% 777419
#! Sequence alignment is a common subtask in many applications such as genetic matching and music information retrieval. Crucial to the performance of any sequence alignment algorithm is an accurate model of the reward of transforming one sequence into another. Using this model, we can find the optimal alignment of two sequences or perform query-based selection from a database of target sequences with a dynamic programming approach. In this paper, we describe a new algorithm to learn the reward models from positive and negative examples of matching sequences. We develop a gradient boosting approach that reduces sequence learning to a series of standard function approximation problems that can be solved by any function approximator. A key advantage of this approach is that it is able to induce complex features using function approximation rather than relying on the user to predefine such features. Our experiments on synthetic data and a fairly complex real-world music retrieval domain demonstrate that our approach can achieve better accuracy and faster learning compared to a state-of-the-art structured SVM approach.

#index 1250579
#* Sound and efficient inference with probabilistic and deterministic dependencies
#@ Hoifung Poon;Pedro Domingos
#t 2006
#c 10
#% 44876
#% 205391
#% 310516
#% 578757
#% 788050
#% 850430
#% 1250224
#% 1269496
#% 1672978
#! Reasoning with both probabilistic and deterministic dependencies is important for many real-world problems, and in particular for the emerging field of statistical relational learning. However, probabilistic inference methods like MCMC or belief propagation tend to give poor results when deterministic or near-deterministic dependencies are present, and logical ones like satisfiability testing are inapplicable to probabilistic ones. In this paper we propose MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. MC-SAT is based on Markov logic, which defines Markov networks using weighted clauses in first-order logic. From the point of view of MCMC, MC-SAT is a slice sampler with an auxiliary variable per clause, and with a satisfiability-based method for sampling the original variables given the auxiliary ones. From the point of view of satisfiability, MCSAT wraps a procedure around the SampleSAT uniform sampler that enables it to sample from highly non-uniform distributions over satisfying assignments. Experiments on entity resolution and collective classification problems show that MC-SAT greatly outperforms Gibbs sampling and simulated tempering over a broad range of problem sizes and degrees of determinism.

#index 1250580
#* Boosting expert ensembles for rapid concept recall
#@ Achim Rettinger;Martin Zinkevich;Michael Bowling
#t 2006
#c 10
#% 81507
#% 165663
#% 236497
#% 266792
#% 267044
#% 342600
#% 466408
#% 466482
#% 504942
#% 520224
#% 646007
#% 770858
#% 840891
#% 998561
#! Many learning tasks in adversarial domains tend to be highly dependent on the opponent. Predefined strategies optimized for play against a specific opponent are not likely to succeed when employed against another opponent. Learning a strategy for each new opponent from scratch, though, is inefficient as one is likely to encounter the same or similar opponents again. We call this particular variant of inductive transfer a concept recall problem. We present an extension to AdaBoost called ExpBoost that is especially designed for such a sequential learning tasks. It automatically balances between an ensemble of experts each trained on one known opponent and learning the concept of the new opponent. We present and compare results of Exp-Boost and other algorithms on both synthetic data and in a simulated robot soccer task. ExpBoost can rapidly adjust to new concepts and achieve performance comparable to a classifier trained exclusively on a particular opponent with far more data.

#index 1250581
#* Identification and evaluation of weak community structures in networks
#@ Jianhua Ruan;Weixiong Zhang
#t 2006
#c 10
#% 148149
#% 296738
#! Identifying intrinsic structures in large networks is a fundamental problem in many fields, such as engineering, social science and biology. In this paper, we are concerned with communities, which are densely connected sub-graphs in a network, and address two critical issues for finding community structures from large experimental data. First, most existing network clustering methods assume sparse networks and networks with strong community structures. In contrast, we consider sparse and dense networks with weak community structures. We introduce a set of simple operations that capture local neighborhood information of a node to identify weak communities. Second, we consider the issue of automatically determining the most appropriate number of communities, a crucial problem for all clustering methods. This requires to properly evaluate the quality of community structures. Built atop a function for network cluster evaluation by Newman and Girvan, we extend their work to weighted graphs. We have evaluated our methods on many networks of known structures, and applied them to analyze a collaboration network and a genetic network. The results showed that our methods can find superb community structures and correct numbers of communities. Comparing to the existing approaches, our methods performed significantly better on networks with weak community structures and equally well on networks with strong community structures.

#index 1250582
#* Thresholding for making classifiers cost-sensitive
#@ Victor S. Sheng;Charles X. Ling
#t 2006
#c 10
#% 136350
#% 191910
#% 209021
#% 280437
#% 342611
#% 424997
#% 466760
#% 477640
#% 727925
#% 765519
#% 926881
#% 1272000
#% 1272369
#% 1289281
#% 1673023
#! In this paper we propose a very simple, yet general and effective method to make any cost-insensitive classifiers (that can produce probability estimates) cost-sensitive. The method, called Thresholding, selects a proper threshold from training instances according to the misclassification cost. Similar to other cost-sensitive meta-learning methods, Thresholding can convert any existing (and future) costinsensitive learning algorithms and techniques into costsensitive ones. However, by comparing with the existing cost sensitive meta-learning methods and the direct use of the theoretical threshold, Thresholding almost always produces the lowest misclassification cost. Experiments also show that Thresholding has the least sensitivity on the misclassification cost ratio. Thus, it is recommended to use when the difference on misclassification costs is large.

#index 1250583
#* Cost-sensitive test strategies
#@ Victor S. Sheng;Charles X. Ling;Ailing Ni;Shichao Zhang
#t 2006
#c 10
#% 92554
#% 136350
#% 160852
#% 447606
#% 464639
#% 770791
#% 785338
#% 785413
#% 829982
#% 1272369
#% 1289281
#! In medical diagnosis doctors must often determine what medical tests (e.g., X-ray, blood tests) should be ordered for a patient to minimize the total cost of medical tests and misdiagnosis. In this paper, we design cost-sensitive machine learning algorithms to model this learning and diagnosis process. Medical tests are like attributes in machine learning whose values may be obtained at cost (attribute cost), and misdiagnoses are like misclassifications which may also incur a cost (misclassification cost). We first propose an improved decision tree learning algorithm that minimizes the sum of attribute costs and misclassification costs. Then we design several novel "test strategies" that may request to obtain values of unknown attributes at cost (similar to doctors' ordering of medical tests at cost) in order to minimize the total cost for test examples (new patients). We empirically evaluate and compare these test strategies, and show that they are effective and that they outperform previous methods. A case study on heart disease is given.

#index 1250584
#* Memory-efficient inference in relational domains
#@ Parag Singla;Pedro Domingos
#t 2006
#c 10
#% 26722
#% 310516
#% 316755
#% 336874
#% 578757
#% 729913
#% 850430
#% 1269496
#% 1279353
#% 1289560
#% 1476298
#! Propositionalization of a first-order theory followed by satisfiability testing has proved to be a remarkably efficient approach to inference in relational domains such as planning (Kautz & Selman 1996) and verification (Jackson 2000). More recently, weighted satisfiability solvers have been used successfully for MPE inference in statistical relational learners (Singla & Domingos 2005). However, fully instantiating a finite first-order theory requires memory on the order of the number of constants raised to the arity of the clauses, which significantly limits the size of domains it can be applied to. In this paper we propose LazySAT, a variation of the Walk-SAT solver that avoids this blowup by taking advantage of the extreme sparseness that is typical of relational domains (i.e., only a small fraction of ground atoms are true, and most clauses are trivially satisfied). Experiments on entity resolution and planning problems show that LazySAT reduces memory usage by orders of magnitude compared to Walk-SAT, while taking comparable time to run and producing the same solutions.

#index 1250585
#* Using Homomorphisms to transfer options across continuous reinforcement learning domains
#@ Vishal Soni;Satinder Singh
#t 2006
#c 10
#% 286423
#% 384911
#% 466070
#% 477304
#% 823852
#% 1272002
#% 1478746
#! We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.

#index 1250586
#* A fast decision tree learning algorithm
#@ Jiang Su;Harry Zhang
#t 2006
#c 10
#% 92542
#% 136350
#% 156186
#% 246831
#% 260001
#% 290482
#% 420091
#% 420109
#% 459008
#% 481945
#% 723244
#% 799751
#% 1650783
#! There is growing interest in scaling up the widely-used decision-tree learning algorithms to very large data sets. Although numerous diverse techniques have been proposed, a fast tree-growing algorithm without substantial decrease in accuracy and substantial increase in space complexity is essential. In this paper, we present a novel, fast decision-tree learning algorithm that is based on a conditional independence assumption. The new algorithm has a time complexity of O(m ċ n), where m is the size of the training data and n is the number of attributes. This is a significant asymptotic improvement over the time complexity O(m ċ n2) of the standard decision-tree learning algorithm C4.5, with an additional space increase of only O(n). Experiments show that our algorithm performs competitively with C4.5 in accuracy on a large number of UCI benchmark data sets, and performs even better and significantly faster than C4.5 on a large number of text classification data sets. The time complexity of our algorithm is as low as naive Bayes'. Indeed, it is as fast as naive Bayes but outperforms naive Bayes in accuracy according to our experiments. Our algorithm is a core tree-growing algorithm that can be combined with other scaling-up techniques to achieve further speedup.

#index 1250587
#* Cross-domain knowledge transfer using structured representations
#@ Samarth Swarup;Sylvian R. Ray
#t 2006
#c 10
#% 170665
#% 204015
#% 236497
#% 429833
#% 451937
#% 563271
#% 729938
#% 769576
#! Previous work in knowledge transfer in machine learning has been restricted to tasks in a single domain. However, evidence from psychology and neuroscience suggests that humans are capable of transferring knowledge across domains. We present here a novel learning method, based on neuroevolution, for transferring knowledge across domains. We use many-layered, sparsely-connected neural networks in order to learn a structural representation of tasks. Then we mine frequent sub-graphs in order to discover sub-networks that are useful for multiple tasks. These sub-networks are then used as primitives for speeding up the learning of subsequent related tasks, which may be in different domains.

#index 1250588
#* Conflict resolution and a framework for collaborative interactive evolution
#@ Sean R. Szumlanski;Annie S. Wu;Charles E. Hughes
#t 2006
#c 10
#% 109078
#% 114994
#% 272825
#% 300553
#% 535568
#! Interactive evolutionary computation (IEC) has proven useful in a variety of applications by combining the subjective evaluation of a user with the massive parallel search power of the genetic algorithm (GA). Here, we articulate a framework for an extension of IEC into collaborative interactive evolution, in which multiple users guide the evolutionary process. In doing so, we introduce the ability for users to combine their efforts for the purpose of evolving effective solutions to problems. This necessarily gives rise to the possibility of conflict between users. We draw on the salient features of the GA to resolve these conflicts and lay the foundation for this new paradigm to be used as a tool for conflict resolution in complex group-wise human-computer interaction tasks.

#index 1250589
#* Sample-efficient evolutionary function approximation for reinforcement learning
#@ Shimon Whiteson;Peter Stone
#t 2006
#c 10
#% 92148
#% 124692
#% 160859
#% 169359
#% 275472
#% 305081
#% 369236
#% 384911
#% 449980
#% 452359
#% 734920
#% 820343
#% 961164
#% 1250215
#% 1699601
#! Reinforcement learning problems are commonly tackled with temporal difference methods, which attempt to estimate the agent's optimal value function. In most real-world problems, learning this value function requires a function approximator, which maps state-action pairs to values via a concise, parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. A recently developed approach called evolutionary function approximation uses evolutionary computation to automate the search for effective representations. While this approach can substantially improve the performance of TD methods, it requires many sample episodes to do so. We present an enhancement to evolutionary function approximation that makes it much more sample-efficient by exploiting the off-policy nature of certain TD methods. Empirical results in a server job scheduling domain demonstrate that the enhanced method can learn better policies than evolution or TD methods alone and can do so in many fewer episodes than standard evolutionary function approximation.

#index 1250590
#* Mixtures of predictive linear Gaussian models for nonlinear stochastic dynamical systems
#@ David Wingate;Satinder Singh
#t 2006
#c 10
#% 361100
#% 876072
#% 1759704
#! The Predictive Linear Gaussian model (or PLG) improves upon traditional linear dynamical system models by using a predictive representation of state, which makes consistent parameter estimation possible without any loss of modeling power and while using fewer parameters. This work extends the PLG to model non-linear dynamical systems through the use of a kernelized, nonlinear mixture technique. The resulting generative model has been named the "MPLG," for "Mixture of PLGs." We also develop a novel technique to perform inference in the model, which consists of a hybrid of sigma-point approximations and analytical statistics. We show that the technique leads to fast and accurate approximations, and that it is general enough to be applied in other contexts. We empirically explore the MPLG and demonstrate its viability on several real-world and synthetic tasks.

#index 1250591
#* Decision tree methods for finding reusable MDP homomorphisms
#@ Alicia Peregrin Wolfe;Andrew G. Barto
#t 2006
#c 10
#% 111440
#% 286423
#% 318485
#% 464636
#% 655325
#% 702594
#% 777133
#% 829011
#% 840884
#% 840885
#% 1279356
#% 1289241
#% 1650297
#% 1705353
#! State abstraction is a useful tool for agents interacting with complex environments. Good state abstractions are compact, reuseable, and easy to learn from sample data. This paper combines and extends two existing classes of state abstraction methods to achieve these criteria. The first class of methods search for MDP homomorphisms (Ravindran 2004), which produce models of reward and transition probabilities in an abstract state space. The second class of methods, like the UTree algorithm (McCallum 1995), learn compact models of the value function quickly from sample data. Models based on MDP homomorphisms can easily be extended such that they are usable across tasks with similar reward functions. However, value based methods like UTree cannot be extended in this fashion. We present results showing a new, combined algorithm that fulfills all three criteria: the resulting models are compact, can be learned quickly from sample data, and can be used across a class of reward functions.

#index 1250592
#* Robust support vector machine training via convex outlier ablation
#@ Linli Xu;Koby Crammer;Dale Schuurmans
#t 2006
#c 10
#% 116172
#% 186989
#% 197394
#% 333929
#% 420064
#% 722805
#% 722811
#% 722909
#% 743284
#% 757953
#% 770766
#% 793245
#% 1499584
#% 1776453
#! One of the well known risks of large margin training methods, such as boosting and support vector machines (SVMs), is their sensitivity to outliers. These risks are normally mitigated by using a soft margin criterion, such as hinge loss, to reduce outlier sensitivity. In this paper, we present a more direct approach that explicitly incorporates outlier suppression in the training process. In particular, we show how outlier detection can be encoded in the large margin training principle of support vector machines. By expressing a convex relaxation of the joint training problem as a semide finite program, one can use this approach to robustly train a support vector machine while suppressing outliers. We demonstrate that our approach can yield superior results to the standard soft margin approach in the presence of outliers.

#index 1250593
#* An efficient algorithm for local distance metric learning
#@ Liu Yang;Rong Jin;Rahul Sukthankar;Yi Liu
#t 2006
#c 10
#% 209623
#% 318412
#% 342706
#% 458379
#% 593047
#% 723241
#% 733366
#% 757953
#% 770798
#% 780688
#% 812601
#% 1858013
#! Learning application-specific distance metrics from labeled data is critical for both statistical classification and information retrieval. Most of the earlier work in this area has focused on finding metrics that simultaneously optimize compactness and separability in a global sense. Specifically, such distance metrics attempt to keep all of the data points in each class close together while ensuring that data points from different classes are separated. However, particularly when classes exhibit multimodal data distributions, these goals conflict and thus cannot be simultaneously satisfied. This paper proposes a Local Distance Metric (LDM) that aims to optimize local compactness and local separability. We present an efficient algorithm that employs eigenvector analysis, and bound optimization to learn the LDM from training data in a probabilistic framework. We demonstrate that LDM achieves significant improvements in both classification and retrieval accuracy compared to global distance learning and kernel-based KNN.

#index 1250594
#* Hard constrained semi-Markov decision processes
#@ Wai-Leong Yeow;Chen-Khong Tham;Wai-Choong Wong
#t 2006
#c 10
#% 124691
#% 221770
#% 351419
#% 363744
#% 384911
#% 465915
#% 746746
#% 773335
#% 1279370
#% 1650717
#! In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.

#index 1250595
#* A new approach to estimating the expected first hitting time of evolutionary algorithms
#@ Yang Yu;Zhi-Hua Zhou
#t 2006
#c 10
#% 207195
#% 267458
#% 289619
#% 289622
#% 334205
#% 378259
#% 388665
#% 446653
#% 590625
#% 683563
#% 749382
#% 1022876
#% 1022895
#% 1777088
#! The expected first hitting time is an important issue in theoretical analyses of evolutionary algorithms since it implies the average computational time complexity. In this paper, by exploiting the relationship between the convergence rate and the expected first hitting time, a new approach to estimating the expected first hitting time is proposed. This approach is then applied to four evolutionary algorithms which involve operators of mutation, mutation with population, mutation with recombination, and time-variant mutation, respectively. The results show that the proposed approach is helpful for analyzing a broad range of evolutionary algorithms.

#index 1250596
#* A direct evolutionary feature extraction algorithm for classifying high dimensional data
#@ Qijun Zhao;David Zhang;Hongtao Lu
#t 2006
#c 10
#% 68777
#% 80995
#% 235342
#% 266426
#% 316779
#% 369236
#% 445218
#% 618474
#% 729344
#% 775625
#% 789030
#% 940364
#% 1022958
#% 1391630
#% 1655012
#% 1777121
#% 1781622
#! Among various feature extraction algorithms, those based on genetic algorithms are promising owing to their potential parallelizability and possible applications in large scale and high dimensional data classification. However, existing genetic algorithm based feature extraction algorithms are either limited in searching optimal projection basis vectors or costly in both time and space complexities and thus not directly applicable to high dimensional data. In this paper, a direct evolutionary feature extraction algorithm is proposed for classifying high-dimensional data. It constructs projection basis vectors using the linear combination of the basis of the search space and the technique of orthogonal complement. It also constrains the search space when seeking for the optimal projection basis vectors. It evaluates individuals according to the classification performance on a subset of the training samples and the generalization ability Df the projection basis vectors represented by the individuals. We compared the proposed algorithm with some representative feature extraction algorithms in face recognition, including the evolutionary pursuit algorithm, Eigenfaces, and Fisherfaces. The results on the widely-used Yale and ORL face databases show that the proposed algorithm has an excellent performance in classification while reducing the space complexity by an order of magnitude.

#index 1250597
#* On multi-class cost-sensitive learning
#@ Zhi-Hua Zhou;Xu-Ying Liu
#t 2006
#c 10
#% 280437
#% 310519
#% 342611
#% 349550
#% 443509
#% 765520
#% 769875
#% 843876
#% 1271973
#% 1289281
#! A popular approach to cost-sensitive learning is to rescale the classes according to their misclassification costs. Although this approach is effective in dealing with binary-class problems, recent studies show that it is often not so helpful when being applied to multi-class problems directly. This paper analyzes that why the traditional rescaling approach is often helpless on multi-class problems, which reveals that before applying rescaling, the consistency of the costs must be examined. Based on the analysis. a new approach is presented, which should be the choice if the user wants to use rescaling for multi-class cost-sensitive learning. Moreover, this paper shows that the proposed approach is helpful when unequal misclassification costs and class imbalance occur simultaneously, and can also be used to tackle pure class-imbalance learning. Thus, the preposed approach provides a unified framework for using rescaling to address multi-class cost-sensitive learning as well as multi-class class-imbalance learning.

#index 1250598
#* Optimal unbiased estimators for evaluating agent performance
#@ Martin Zinkevich;Michael Bowling;Nolan Bard;Morgan Kan;Darse Billings
#t 2006
#c 10
#% 416988
#% 804975
#% 1279308
#! Evaluating the performance of an agent or group of agents can be, by itself, a very challenging problem. The stochastic nature of the environment plus the stochastic nature of agents' decisions can result in estimates with intractably large variances This paper examines the problem of finding low variance estimates of agent performance. In particular, we assume that some agent-environment dynamics are known, such as the random outcome of drawing a card or rolling a die. Other dynamics are unknown, such as the reasoning of a human or other black-box agent. Using the known dynamics, we describe the complete set of all unbiased estimators, that is, for any possible unknown dynamics the estimate's expectation is always the agent's expected utility. Then, given a belief abcut the unknown dynamics, we identify the unbiased estimator with minimum variance. If the belief is correct our estimate is optimal, and if the belief is wrong it is at least unbiased. Finally, we apply our unbiased estimator to the game of poker, demonstrating dramatically reduced variance and faster evaluation.

#index 1250599
#* Keeping in touch: maintaining biconnected structure by homogeneous robots
#@ Mazda Ahmadi;Peter Stone
#t 2006
#c 10
#% 418831
#% 418844
#% 483901
#! For many distributed autonomous robotic systems, it is important to maintain communication connectivity among the robots. That is, each robot must be able to communicate with each other robot, perhaps through a series of other robots. Ideally, this property should be robust to the removal of any single robot from the system. In (Ahmadi & Stone 2006a) we define a property of a team's communication graph that ensures this property, called biconnectivity. In that paper, a distributed algorithm to check if a team of robots is biconnected and its correctness proof are also presented. In this paper we provide distributed algorithms to add and remove robots to/from a multi-robot team while maintaining the biconnected property. These two algorithms are implemented and tested in the Player/Stage simulator.

#index 1250600
#* Quantifying incentive compatibility of ranking systems
#@ Alon Altman;Moshe Tennenholtz
#t 2006
#c 10
#% 290830
#% 316798
#% 788101
#% 799636
#% 808358
#% 822003
#% 1289497
#! Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. Such ranking systems do not exist in empty space, and therefore agents' incentives should be carefully considered. In this paper we define three measures for quantifying the incentive compatibility of ranking systems. We apply these measures to several known ranking systems, such as PageRank, and prove tight bounds on the level of incentive compatibility under two basic properties: strong monotonicity and non-imposition. We also introduce two novel nonimposing ranking systems, one general, and the other for the case of systems with three participants. A full axiomatization is provided for the latter.

#index 1250601
#* Impersonation-based mechanisms
#@ Moshe Babaioff;Ron Lavi;Elan Pavlov
#t 2006
#c 10
#% 282658
#% 413867
#% 453488
#% 529175
#% 578713
#% 818584
#% 847152
#% 1269397
#! In this paper we present a general scheme to create mechanisms that approximate the social welfare in the presence of selfish (but rational) behavior of agents. The usual approach is to design truthful mechanisms in which an agent can only lose by impersonating as another agent. In contrast, our approach is to allow an agent to impersonate several different agents. We design the mechanisms such that only a limited set of impersonations are reasonable to rational agents. Our mechanisms make sure that for any choice of such impersonations by the agents, an approximation to the social welfare is achieved. We demonstrate our results on the well studied domain of Combinatorial Auctions (CA). Our mechanisms are algorithmic implementations, a notion recently suggested in (Babaioff, Lavi, & Pavlov 2006).

#index 1250602
#* Algorithms for rationalizability and CURB sets
#@ Michael Benisch;George Davis;Tuomas Sandholm
#t 2006
#c 10
#% 143652
#% 773295
#% 808367
#% 813834
#% 850162
#% 890283
#% 1250223
#% 1269435
#% 1269437
#! Significant work has been done on computational aspects of solving games under various solution concepts, such as Nash equilibrium, subgame perfect Nash equilibrium, correlated equilibrium, and (iterated) dominance. However, the fundamental concepts of rationalizability and CURB (Closed Under Rational Behavior sets have not, to our knowledge, been studied from a computational perspective. First, for rationalizability we describe an LP-based polynomial algorithm that finds all strategies that are rationalizable against a mixture over a given set of opponent strategies. Then, we describe a series of increasingly sophisticated polynomial algorithms for finding all minimal CURB sets, one minimal CURB set, and the smallest minimal CURB set. Finally, we give theoretical results regarding the relationships between CURB sets and Nash equilibria, showing that finding a Nash equilibrium can be exponential only in the size of the smallest CURB set. We show that this can lead to an arbitrarily large reduction in the complexity of finding a Nash equilibrium. On the downside, we also show that the smallest CURB set can be arbitrarily larger than the supports of the enclosed Nash equilibrium.

#index 1250603
#* On strictly competitive multi-player games
#@ Felix Brandt;Felix Fischer;Yoav Shoham
#t 2006
#c 10
#% 404909
#% 576214
#% 805727
#% 808367
#% 836504
#% 866687
#% 868474
#% 891142
#% 1272080
#% 1274948
#% 1740209
#! We embark on an initial study of a new class of strategic (normal-form) games, so-called ranking games, in which the payoff to each agent solely depends on his position in a ranking of the agents induced by their actions. This definition is motivated by the observation that in many strategic situations such as parlor games, competitive economic scenarios, and some social choice settings, players are merely interested in performing optimal relative to their opponents rather than in absolute measures. A simple but important subclass of ranking games are single-winner games where in any outcome one agent wins and all others lose. We investigate the computational complexity of a variety of common game-theoretic solution concepts in ranking games and deliver hardness results for iterated weak dominance and mixed Nash equilibria when there are more than two players and pure Nash equilibria when the number of players is unbounded. This dashes hope that multi-player ranking games can be solved efficiently, despite the structural restrictions of these games.

#index 1250604
#* Computing slater rankings using similarities among candidates
#@ Vincent Conitzer
#t 2006
#c 10
#% 126101
#% 242217
#% 330769
#% 529806
#% 578715
#% 805798
#% 808358
#% 847120
#% 858155
#% 1250228
#% 1272396
#% 1279324
#% 1289497
#% 1677599
#% 1698228
#! Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.

#index 1250605
#* Improved bounds for computing Kemeny rankings
#@ Vincent Conitzer;Andrew Davenport;Jayant Kalagnanam
#t 2006
#c 10
#% 242217
#% 330769
#% 529806
#% 578715
#% 805798
#% 808358
#% 847120
#% 1250228
#% 1272396
#% 1279324
#% 1289497
#% 1677599
#% 1698228
#! Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.

#index 1250606
#* Nonexistence of voting rules that are usually hard to manipulate
#@ Vincent Conitzer;Tuomas Sandholm
#t 2006
#c 10
#% 242217
#% 330769
#% 578703
#% 631051
#% 805798
#% 890278
#% 1250228
#% 1272396
#% 1279324
#% 1677599
#% 1698228
#! Aggregating the preferences of self-interested agents is a key problem for multiagent systems, and one general method for doing so is to vote over the alternatives (candidates). Unfortunately, the Gibbard-Satterthwaite theorem shows that when there are three or more candidates, all reasonable voting rules are manipulable (in the sense that there exist situations in which a voter would benefit from reporting its preferences insincerely). To circumvent this impossibility result, recent research has investigated whether it is possible to make finding a beneficial manipulation computationally hard. This approach has had some limited success, exhibiting rules under which the problem of finding a beneficial manipulation is NP-hard, #P-hard, or even PSPACE-hard. Thus, under these rules, it is unlikely that a computationally efficient algorithm can be constructed that always finds a beneficial manipulation (when it exists). However, this still does not preclude the existence of an efficient algorithm that often finds a successful manipulation (when it exists). There have been attempts to design a rule under which finding a beneficial manipulation is usually hard, but they have failed. To explain this failure, in this paper, we show that it is in fact impossible to design such a rule, if the rule is also required to satisfy another property: a large fraction of the manipulable instances are both weakly monotone, and allow the manipulators to make either of exactly two candidates win. We argue why one should expect voting rules to have this property, and show experimentally that common voting rules clearly satisfy it. We also discuss approaches for potentially circumventing this impossibility result.

#index 1250607
#* Overlapping coalition formation for efficient data fusion in multi-sensor networks
#@ Viet Dung Dang;Rajdeep K. Dash;Alex Rogers;Nicholas R. Jennings
#t 2006
#c 10
#% 70370
#% 252199
#% 284645
#% 643188
#% 719917
#% 773258
#! This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.

#index 1250608
#* The complexity of bribery in elections
#@ Piotr Faliszewski;Edith Hemaspaandra;Lane A. Hemaspaandra
#t 2006
#c 10
#% 242217
#% 330769
#% 578703
#% 578715
#% 631051
#% 1279324
#% 1677599
#! We study the complexity of influencing elections through bribery: How computationally complex is it for an external actor to determine whether by a certain amount of bribing voters a specified candidate can be made the election's winner? We study this problem for election systems as varied as scoring protocols and Dodgson voting, and in a variety of settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters, and succinct-vs.-nonsuccinct input specification. We obtain both polynomial-time bribery algorithms and proofs of the intractability of bribery, and indeed our results show that the complexity of bribery is extremely sensitive to the setting. For example, we find settings in which bribery is NP-complete but manipulation (by voters) is in P, and we find settings in which bribing weighted voters is NP-complete but bribing voters with individual bribe thresholds is in P. For the broad class of elections (including plurality, Borda, k-approval, and veto) known as scoring protocols, we prove a dichotomy result for bribery of weighted voters: We find a simple-to-evaluate condition that classifies every case as either NP-complete or in P.

#index 1250609
#* Analysis of privacy loss in distributed constraint optimization
#@ Rachel Greenstadt;Jonathan P. Pearce;Milind Tambe
#t 2006
#c 10
#% 535135
#% 773217
#% 773337
#% 779970
#% 823893
#% 823970
#% 855913
#% 855915
#% 862314
#% 1289393
#! Distributed Constraint Optimization (DCOP) is rapidly emerging as a prominent technique for multi agent coordination. However, despite agent privacy being a key motivation for applying DCOPs in many applications, rigorous quantitative evaluations of privacy loss in DCOP algorithms have been lacking. Recently, [Maheswaran et al. 2005] introduced a framework for quantitative evaluations of privacy in DCOP algorithms, showing that some DCOP algorithms lose more privacy than purely centralized approaches and questioning the motivation for applying DCOPs. This paper addresses the question of whether state-of-the art DCOP algorithms suffer from a similar shortcoming by investigating several of the most efficient DCOP algorithms, including both DPOP and ADOPT. Furthermore, while previous work investigated the impact on efficiency of distributed contraint reasoning design decisions (e.g. constraint-graph topology, asynchrony, message-contents), this paper examines the privacy aspect of such decisions, providing an improved understanding of privacy-efficiency tradeoffs.

#index 1250610
#* From centralized to distributed selective overhearing
#@ Gery Gutnik;Gal A. Kaminka
#t 2006
#c 10
#% 271040
#% 557550
#% 912262
#% 1272346
#! Overhearing is an approach for monitoring open, distributed, multi-agent systems by listening to the routine communications taking place within them. Previous investigations of overhearing assumed that all inter-agent communications are accessible to a single overhearing agent. However, as multi-agent systems grow both in size and distribution two problems arise. First, in large-scale settings, an overhearing agent cannot monitor all agents and their conversations, and must therefore be selective in carefully choosing its targets. Second, a single overhearer would encounter difficulties overhearing agents acting in a geographically-distributed environment. This paper tackles these challenges by addressing distributed teams of overhearing agents involved in selective overhearing. Building on prior work on centralized selective overhearing, we consider the consequences of transitioning from overhearing teams working in a centrally-coordinated manner to distributed overhearing teams. In doing so, we distinguish the various factors influencing the level of distribution within these teams and determine their importance in terms of effective selective overhearing.

#index 1250611
#* A new approach to distributed task assignment using Lagrangian decomposition and distributed constraint satisfaction
#@ Katsutoshi Hirayama
#t 2006
#c 10
#% 29368
#% 443227
#% 643099
#% 855911
#% 1013352
#% 1289393
#! We present a new formulation of distributed task assignment, called Generalized Mutual Assignment Problem (GMAP), which is derived from an NP-hard combinatorial optimization problem that has been studied for many years in the operations research community. To solve the GMAP, we introduce a novel distributed solution protocol using Lagrangian decomposition and distributed constraint satisfaction, where the agents solve their individual optimization problems and coordinate their locally optimized solutions through a distributed constraint satisfaction technique. Next, to produce quick agreement between the agents on a feasible solution with reasonably good quality, we provide a parameter that controls the range of "noise" mixed with an increment/decrement in a Lagrange multiplier. Our experimental results indicate that the parameter may allow us to control tradeoffs between the quality of a solution and the cost of finding it.

#index 1250612
#* Distributed interactive learning in multi-agent systems
#@ Jian Huang;Adrian R. Pearce
#t 2006
#c 10
#% 262678
#% 379347
#% 398869
#% 418731
#% 449587
#% 451031
#% 794074
#! Both explanation-based and inductive learning techniques have proven successful in a variety of distributed domains. However, learning in multi-agent systems does not necessarily involve the participation of other agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately, or single-agent learning. In this paper we present a new framework, named the Multi-Agent Inductive Learning System (MAILS), that tightly integrates processes of induction between agents. The MAILS framework combines inverse entailment with an epistemic approach to reasoning about knowledge in a multi-agent setting, facilitating a systematic approach to the sharing of knowledge and invention of predicates when required. The benefits of the new approach are demonstrated for inducing declarative program fragments in a multi-agent distributed programming system.

#index 1250613
#* Regret-based incremental partial revelation mechanisms
#@ Nathanaël Hyafil;Craig Boutilier
#t 2006
#c 10
#% 314944
#% 460806
#% 578711
#% 580533
#% 643243
#% 754153
#% 818584
#% 819415
#% 830089
#% 1250151
#% 1289499
#% 1289504
#! Classic direct mechanisms suffer from the drawback of requiring full type (or utility function) revelation from participating agents. In complex settings with multi-attribute utility, assessing utility functions can be very difficult, a problem addressed by recent work on preference elicitation. In this work we propose a framework for incremental, partial revelation mechanisms and study the use of minimax regret as an optimization criterion for allocation determination with type uncertainty. We examine the incentive properties of incremental mechanisms when minimax regret is used to determine allocations with no additional elicitation of payment information, and when additional payment information is obtained. We argue that elicitation effort can be focused simultaneously on reducing allocation and payment uncertainty.

#index 1250614
#* A polynomial-time algorithm for action-graph games
#@ Albert Xin Jiang;Kevin Leyton-Brown
#t 2006
#c 10
#% 30034
#% 326878
#% 567883
#% 788040
#% 805727
#% 1279323
#% 1289289
#! Action-Graph Games (AGGs) (Bhat & Leyton-Brown 2004) are a fully expressive game representation which can compactly express strict and context-specific independence and anonymity structure in players' utility functions. We present an efficient algorithm for computing expected payoffs under mixed strategy profiles. This algorithm runs in time polynomial in the size of the AGG representation (which is itself polynomial in the number of players when the in-degree of the action graph is bounded). We also present an extension to the AGG representation which allows us to compactly represent a wider variety of structured utility functions.

#index 1250615
#* Multiparty proactive communication: a perspective for evolving shared mental models
#@ Kaivan Kamali;Xiaocong Fan;John Yen
#t 2006
#c 10
#% 529170
#% 557550
#% 748668
#% 773189
#% 823939
#% 855466
#% 1272346
#% 1289304
#% 1478623
#% 1698526
#! Helping behavior in effective teams is enabled by some overlapping "shared mental models" that are developed and maintained by members of the team. In this paper, we take the perspective that multiparty "proactive" communication is critical for establishing and maintaining such a shared mental model among teammates, which is, the basis for agents to offer proactive help and to achieve coherent teamwork. We first provide formal semantics for multiparty proactive performatives within a team setting. We then examine how such performatives result in updates to mental model of teammates, and how such updates can trigger helpful behaviors from other teammates.

#index 1250616
#* Strong mediated equilibrium
#@ Dov Monderer;Moshe Tennenholtz
#t 2006
#c 10
#% 808385
#% 1272023
#! Providing agents with strategies that will be robust against deviations by coalitions is central to the design of multi-agent agents. However, such strategies, captured by the notion of strong equilibrium, rarely exist. This paper suggests the use of mediators in order to enrich the set of situations where we can obtain stability against deviations by coalitions. A mediator is a reliable entity, which can ask the agents for the right to play on their behalf, and is guaranteed to behave in a pre-specified way based on messages received from the agents. However, a mediator can not enforce behavior; that is, agents can play in the game directly without the mediator's help. We prove some general results about mediators, and concentrate on the notion of strong mediated equilibrium; we show that desired behaviors, which are stable against deviations by coalitions, can be obtained using mediators in a rich class of settings.

#index 1250617
#* A compact representation scheme for coalitional games in open anonymous environments
#@ Naoki Ohta;Atsushi Iwasaki;Makoto Yokoo;Kohki Maruono;Vincent Conitzer;Tuomas Sandholm
#t 2006
#c 10
#% 165011
#% 171142
#% 235988
#% 252199
#% 808378
#% 1250153
#% 1269439
#% 1279301
#! Coalition formation is an important capability of automated negotiation among self-interested agents. In order for coalitions to be stable, a key question that must be answered is how the gains from cooperation are to be distributed. Recent research has revealed that traditional solution concepts, such as the Shapley value, core, least core, and nucleolus, are vulnerable to various manipulations in open anonymous environments such as the Internet. These manipulations include submitting false names, collusion, and hiding some skills. To address this, a solution concept called the anonymity-proof core, which is robust against such manipulations, was developed. However, the representation size of the outcome function in the anonymity-proof core (and similar concepts) requires space exponential in the number of agents/skills. This paper proposes a compact representation of the outcome function, given that the characteristic function is represented using a recently introduced compact language that explicitly specifies only coalitions that introduce synergy. This compact representation scheme can successfully express the outcome function in the anonymity-proof core. Furthermore, this paper develops a new solution concept, the anonymity-proof nucleolus, that is also expressible in this compact representation. We show that the anonymity-proof nucleolus always exists, is unique, and is in the anonymity-proof core (if the latter is nonempty). and assigns the same value to symmetric skills.

#index 1250618
#* ODPOP: an algorithm for open/distributed constraint optimization
#@ Adrian Petcu;Boi Faltings
#t 2006
#c 10
#% 2028
#% 301888
#% 443227
#% 644201
#% 773217
#% 830716
#% 855913
#% 855914
#% 890434
#% 1269429
#% 1275309
#% 1289393
#! We propose ODPOP, a new distributed algorithm for open multiagent combinatorial optimization that feature unbounded domains (Faltings & Macho-Gonzalez 2005). The ODPOP algorithm explores the same search space as the dynamic programming algorithm DPOP (Petcu & Faltings 2005b) or ADOPT (Modi et at. 2005). but does so in an incremental, best-first fashion suitable for open problems. ODPOP has several advantages over DPOP. First, it uses messages whose size only grows linearly with the treewidth of the problem. Second, by letting agents explore values in a best-first order, it avoids incurring always the worst case complexity as DPOP, and on average it saves a significant amount of computation and information exchange. To show the merits of our approach, we report on experiments with practically sized distributed meeting scheduling problems on a multiagent system.

#index 1250619
#* Behaviosites: manipulation of multiagent system behavior through parasitic infection
#@ Amit Shabtay;Zinovi Rabinovich;Jeffrey S. Rosenschein
#t 2006
#c 10
#% 31686
#% 367254
#% 870914
#% 890304
#% 1272348
#% 1784802
#! In this paper we present the Behaviosite Paradigm, a new approach to coordination and control of distributed agents in a multiagent system, inspired by biological parasites with behavior manipulation properties. Behaviosites are code modules that "infect" a system, attaching themselves to agents and altering the sensory activity and actions of those agents. These behavioral changes can be used to achieve altered, potentially improved, performance of the overall system; thus, Behaviosites provide a mechanism for distributed control over a distributed system. Behaviosites need to be designed so that they are intimately familiar with the internal workings of the environment and of the agents operating within it. To demonstrate our approach, we use behaviosites to control the behavior of a swarm of simple agents. With a relatively low infection rate, a few behaviosites can engender desired behavior over the swarm as a whole: keeping it in one place, leading it through checkpoints, or moving the swarm from one stable equilibrium to another. We contrast behaviosites as a distributed swarm control mechanism with alternatives, such as the use of group leaders, herders, or social norms.

#index 1250620
#* Simultaneous team assignment and behavior recognition from spatio-temporal agent traces
#@ Gita Sukthankar;Katia Sycara
#t 2006
#c 10
#% 283197
#% 319464
#% 410837
#% 504942
#% 729437
#% 823974
#% 824907
#% 1271813
#% 1279398
#% 1499477
#! This paper addresses the problem of activity recognition for physically-embodied agent teams. We define team activity recognition as the process of identifying team behaviors from traces of agent positions over time; for many physical domains, military or athletic, coordinated team behaviors create distinctive spatio-temporal patterns that can be used to identify low-level action sequences. This paper focuses on the novel problem of recovering agent-to-team assignments for complex team tasks where team composition, the mapping of agents into teams, changes over time. Without a priori knowledge of current team assignments, the behavior recognition problem is challenging since behaviors are characterized by the aggregate motion of the entire team and cannot generally be determined by observing the movements of a single agent in isolation. To handle this problem, we introduce a new algorithm, Simultaneous Team Assignment and Behavior Recognition (STABR), that generates behavior annotations from spatio-temporal agent traces. The proposed approach is able to perform accurate team behavior recognition without an exhaustive search over the combinatorial space of potential team assignments, as demonstrated on several scenarios of simulated military maneuvers.

#index 1250621
#* Contract enactment in virtual organizations: a commitment-based approach
#@ Yathiraj B. Udupi;Munindar P. Singh
#t 2006
#c 10
#% 378982
#% 581036
#% 764960
#% 773303
#% 791901
#% 822359
#% 842054
#% 1272316
#! A virtual organization (VO) is a dynamic collection of entities (individuals, enterprises, and information resources) collaborating on some computational activity. VOs are an emerging means to model, enact, and manage large-scale computations. VOs consist of autonomous, heterogeneous members, often dynamic exhibiting complex behaviors. Thus, VOs are best modeled via multiagent systems. An agent can be an individual such as a person, business partner, or a resource. An agent may also be a VO. A VO is an agent that comprises other agents. Contracts provide a natural arms-length abstraction for modeling interaction among autonomous and heterogeneous agents. The interplay of contracts and VOs is the subject of this paper. The core of this paper is an approach to formalize VOs and contracts based on commitments. Our main contributions are (1) a formalization of VOs, (2) a discussion of certain key properties of VOs, and (3) an identification of a variety of VO structures and an analysis of how they support contract enactment. We evaluate our approach with an analysis of several scenarios involving the handling of exceptions and conflicts in contracts.

#index 1250622
#* A computational model of logic-based negotiation
#@ Dongmo Zhang;Yan Zhang
#t 2006
#c 10
#% 263126
#% 295006
#% 781207
#% 1250169
#% 1289508
#! This paper presents a computational model of negotiation based on Nebel's syntax-based belief revision. The model guarantees a unique bargaining solution for each bargaining game without using lotteries. Its game-theoretic properties are discussed against the existence and uniqueness of Nash equilibrium and subgame perfect equilibrium. We also study essential computational properties in relation to our negotiation model. In particular, we show that the deal membership checking is DP-complete and the corresponding agreement inference problem is Π2P-hard.

#index 1250623
#* Mechanisms for partial information elicitation: the truth, but not the whole truth
#@ Aviv Zohar;Jeffrey S. Rosenschein
#t 2006
#c 10
#% 24108
#% 187980
#% 382586
#% 393260
#% 959133
#% 1650358
#! We examine a setting in which a buyer wishes to purchase probabilistic information from some agent. The seller must invest effort in order to gain access to the information, and must therefore be compensated appropriately. However, the information being sold is hard to verify and the seller may be tempted to lie in order to collect a higher payment. While it is generally easy to design information elicitation mechanisms that motivate the seller to be truthful, we show that if the seller has additional relevant information it does not want to reveal, the buyer must resort to elicitation mechanisms that work only some of the time. The optimal design of such mechanisms is shown to be computationally hard. We show two different algorithms to solve the mechanism design problem, each appropriate (from a complexity point of view) in different scenarios.

#index 1250624
#* Robust mechanisms for information elicitation
#@ Aviv Zohar;Jeffrey S. Rosenschein
#t 2006
#c 10
#% 44876
#% 959133
#% 1279253
#% 1650358
#% 1845332
#! We study information elicitation mechanisms in which a principal agent attempts to elicit the private information of other agents using a carefully selected payment scheme based on proper scoring rules. Scoring rules, like many other mechanisms set in a probabilistic environment, assume that all participating agents share some common belief about the underlying probability of events. In real-life situations however, the underlying distributions are not known precisely, and small differences in beliefs of agents about these distributions may alter their behavior under the prescribed mechanism. We propose designing elicitation mechanisms that will be robust to small changes in belief. We show how to algorithmically design such mechanisms in polynomial time using tools of stochastic programming and convex programming, and discuss implementation issues for multiagent scenarios.

#index 1250625
#* Societal grounding is essential to meaningful language use
#@ David De Vault;Iris Oved;Matthew Stone
#t 2006
#c 10
#% 68239
#% 85153
#% 301930
#% 406226
#% 578675
#% 741919
#% 766223
#% 943814
#% 1272033
#! Language engineers often point to tight connections between their systems' linguistic representations and accumulated sensor data as a sign that their systems really mean what they say. While we believe such connections are an important piece in the puzzle of meaning, we argue that perceptual grounding alone does not suffice to explain the specific, stable meanings human speakers attribute to each other. Instead, human attributions of meaning depend on a process of societal grounding by which individual language speakers coordinate their perceptual experience and linguistic usage with other members of their linguistic communities. For system builders, this suggests that implementing a strategy of societal grounding would justify the attribution of bona fide linguistic meaning to a system even if it had little perceptual experience and only modest perceptual accuracy. We illustrate the importance and role of societal grounding using an implemented dialogue system that collaboratively identifies visual objects with human users.

#index 1250626
#* Negation, contrast and contradiction in text processing
#@ Sanda Harabagiu;Andrew Hickl;Finley Lacatusu
#t 2006
#c 10
#% 708948
#% 815909
#% 817420
#% 858036
#% 939902
#% 1280242
#! This paper describes a framework for recognizing contradictions between multiple text sources by relying on three forms of linguistic information: (a) negation; (b) antonymy; and (c) semantic and pragmatic information associated with the discourse relations. Two views of contradictions are considered, in which a novel method of recognizing contrast and of finding antonymies are described. Contradictions are used for informing fusion operators in question answering. Our experiments show promising results for the detection of contradictions.

#index 1250627
#* Proposing a new term weighting scheme for text categorization
#@ Man Lan;Chew-Lim Tan;Hwee-Boon Low
#t 2006
#c 10
#% 46803
#% 260001
#% 272995
#% 280817
#% 348303
#% 425047
#% 458379
#% 465754
#% 728350
#% 766436
#% 815171
#! In text categorization, term weighting methods assign appropriate weights to the terms to improve the classification performance. In this study, we propose an effective term weighting scheme, i.e. tf.rf, and investigate several widely-used unsupervised and supervised term weighting methods on two popular data collections in combination with SVM and kNN algorithms. From our controlled experimental results, not all supervised term weighting methods have a consistent superiority over unsupervised term weighting methods. Specifically, the three supervised methods based on the information theory, i.e. tf.χ2, tf.ig and tf.or, perform rather poorly in all experiments. On the other hand, our proposed tf.rf achieves the best performance consistently and outperforms other methods substantially and significantly. The popularly-used tf.idf method has not shown a uniformly good performance with respect to different data corpora.

#index 1250628
#* Script and language identification in degraded and distorted document images
#@ Shijian Lu;Chew Lim Tan
#t 2006
#c 10
#% 224013
#% 227492
#% 255333
#% 493803
#% 493826
#% 832593
#% 1378213
#! This paper reports a statistical identification technique that differentiates scripts and languages in degraded and distorted document images. We identify scripts and languages through document vectorization, which transforms each document image into an electronic document vector that characterizes the shape and frequency of the contained character and word images. We first identify scripts based on the density and distribution of vertical runs between character strokes and a vertical scan line. Latin-based languages are then differentiated using a set of word shape codes constructed using horizontal word runs and character extremum points. Experimental results show that our method is tolerant to noise, document degradation, and slight document skew and attains an average identification rate over 95%.

#index 1250629
#* Corpus-based and knowledge-based measures of text semantic similarity
#@ Rada Mihalcea;Courtney Corley;Carlo Strapparava
#t 2006
#c 10
#% 144031
#% 230530
#% 248221
#% 286069
#% 465914
#% 741083
#% 741891
#% 748600
#% 815902
#% 816060
#% 816173
#% 855269
#% 938688
#% 939405
#% 939699
#% 1275285
#% 1289524
#% 1414358
#% 1672483
#! This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to 13% error rate reduction with respect to the traditional vector-based similarity metric.

#index 1250630
#* Learning noun-modifier semantic relations with corpus-based and WordNet-based features
#@ Vivi Nastase;Jelber Sayyad-Shirabad;Marina Sokolova;Stan Szpakowicz
#t 2006
#c 10
#% 399
#% 529645
#% 708351
#% 747738
#% 747891
#% 747893
#% 748550
#% 756183
#% 756964
#% 786515
#% 815894
#% 817955
#% 858036
#% 939383
#% 1289532
#% 1294844
#! We study the performance of two representations of word meaning in learning noun-modifier semantic relations. One representation is based on lexical resources, in particular WordNet, the other - on a corpus. We experimented with decision trees, instance-based learning and Support Vector Machines. All these methods work well in this learning task. We report high precision, recall and F-score, and small variation in performance across several 10-fold cross-validation runs. The corpus-based method has the advantage of working with data without word-sense annotations and performs well over the baseline. The WordNet-based method, requiring word-sense annotated data, has higher precision.

#index 1250631
#* Planning with first-order temporally extended goals using heuristic search
#@ Jorge A. Baier;Sheila A. Mcilraith
#t 2006
#c 10
#% 100159
#% 172932
#% 224480
#% 417597
#% 417703
#% 479031
#% 578723
#% 867890
#% 1068329
#% 1271962
#% 1389589
#! Temporally extended goals (TEGs) refer to properties that must hold over intermediate and/or final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.

#index 1250632
#* Fast hierarchical goal schema recognition
#@ Nate Blaylock;James Allen
#t 2006
#c 10
#% 567880
#% 1272019
#% 1272356
#% 1704250
#! We present our work on using statistical, corpus-based machine learning techniques to simultaneously recognize an agent's current goal schemas at various levels of a hierarchical plan. Our recognizer is based on a novel type of graphical model, a Cascading Hidden Markov Model, which allows the algorithm to do exact inference and make predictions at each level of the hierarchy in time quadratic to the number of possible goal schemas. We also report results of our recognizer's performance on a plan corpus.

#index 1250633
#* Robust execution of contingent, temporally flexible plans
#@ Stephen A. Block;Andreas F. Wehowsky;Brian C. Williams
#t 2006
#c 10
#% 107137
#% 266108
#% 360802
#% 529337
#% 1289214
#! Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.

#index 1250634
#* Factored planning: how, when, and when not
#@ Ronen I. Brafman;Carmel Domshlak
#t 2006
#c 10
#% 31482
#% 172505
#% 175391
#% 179879
#% 344878
#% 644201
#% 873956
#% 1271885
#% 1271985
#% 1279345
#% 1289543
#% 1290106
#% 1476298
#! Automated domain factoring, and planning methods that utilize them, have long been of interest to planning researchers. Recent work in this area yielded new theoretical insight and algorithms, but left many questions open: How to decompose a domain into factors? How to work with these factors? And whether and when decomposition-based methods are useful? This paper provides theoretical analysis that answers many of these questions: it proposes a novel approach to factored planning; proves its theoretical superiority over previous methods; provides insight into how to factor domains; and uses its novel complexity results to analyze when factored planning is likely to perform well, and when not. It also establishes the key role played by the domain's causal graph in the complexity analysis of planning algorithms.

#index 1250635
#* Adaptive sampling based large-scale stochastic resource control
#@ Balázs Csanád Csáji;László Monostori
#t 2006
#c 10
#% 351419
#% 465910
#% 872759
#% 1147758
#% 1290042
#% 1377489
#! We consider closed-loop solutions to stochastic optimization problems of resource allocation type. They concern with the dynamic allocation of reusable resources over time to non-preemtive interconnected tasks with stochastic durations. The aim is to minimize the expected value of a regular performance measure. First, we formulate the problem as a stochastic shortest path problem and argue that our formulation has favorable properties, e.g., it has finite horizon, it is acyclic, thus, all policies are proper, and moreover, the space of control policies can be safely restricted. Then, we propose an iterative solution. Essentially, we apply a reinforcement learning based adaptive sampler to compute a sub-optimal control policy. We suggest several approaches to enhance this solution and make it applicable to large-scale problems. The main improvements are: (1) the value function is maintained by feature-based support vector regression; (2) the initial exploration is guided by rollout algorithms; (3) the state space is partitioned by clustering the tasks while keeping the precedence constraints satisfied; (4) the action space is decomposed and, consequently, the number of available actions in a state is decreased; and, finally, (5) we argue that the sampling can be effectively distributed among several processors. The effectiveness of the approach is demonstrated by experimental results on both artificial (benchmark) and real-world (industry related) data.

#index 1250636
#* Cost-optimal external planning
#@ Stefan Edelkamp;Shahid Jabbar
#t 2006
#c 10
#% 41684
#% 282771
#% 529516
#% 873948
#% 873954
#% 1250226
#% 1269579
#% 1271962
#% 1272008
#% 1272017
#% 1272098
#% 1712385
#! This paper considers strategies for external memory based optimal planning. An external breadth-first search exploration algorithm is devised that is guaranteed to find the costoptimal solution. We contribute a procedure for finding the upper bound on the locality of the search in planning graphs that dictates the number of layers that have to be kept to avoid re-openings. We also discuss an external variant of Enforced Hill Climbing. Using relaxed-plan heuristic without helpful-action pruning we have been able to perform large explorations on metric planning problems, providing better plan lengths than have been reported earlier. A novel approach to plan reconstruction in external setting with linear I/O complexity is proposed. We provide external exploration results on some recently proposed planning domains.

#index 1250637
#* A two-step hierarchical algorithm for model-based diagnosis
#@ Alexander Feldman;Arjan van Gemund
#t 2006
#c 10
#% 21138
#% 95580
#% 105619
#% 109517
#% 132173
#% 154456
#% 288165
#% 439224
#% 979225
#% 1272329
#% 1290122
#! For many large systems the computational complexity of complete model-based diagnosis is prohibitive. In this paper we investigate the speedup of the diagnosis process by exploiting the hierarchy/locality as is typically present in well-engineered systems. The approach comprises a compile-time and a run-time step. In the first step, a hierarchical CNF representation of the system is compiled to hierarchical DNF of adjustable hierarchical depth. In the second step, the diagnoses are computed from the hierarchical DNF and the actual observations. Our hierarchical algorithm, while sound and complete, allows large models to be diagnosed, where compiletime investment directly translates to run-time speedup. The benefits of our approach are illustrated by using weak-fault models of real-world systems, including the ISCAS-85 combinatorial circuits. Even for these non-optimally partitioned problems the speedup compared to traditional approaches ranges in the hundreds.

#index 1250638
#* Exploration of the robustness of plans
#@ Maria Fox;Richard Howey;Derek Long
#t 2006
#c 10
#% 266125
#% 519060
#% 568467
#% 736897
#% 1269547
#% 1269549
#% 1272008
#% 1272083
#% 1289215
#% 1707652
#! This paper considers the problem of stochastic robustness testing for plans. As many authors have observed, unforeseen execution-time variations, both in the effects of actions and in the times at which they occur, can result in a plan failing to execute correctly even when it is valid with respect to a domain model. In this paper we contrast the validation of a plan with respect to a domain model, confirming soundness, with the validation with respect to an execution model, which we call robustness. We describe a Monte Carlo probing strategy that takes a hypothesis testing approach to confirming the robustness of a plan. An important contribution of the work is that we draw links between the robustness of plans and the notion of the "fuzzy" robustness of traces through timed hybrid automata, introduced by Gupta et al. We show that robustness depends on the metric used to define the set of plans that are probed, and that the most appropriate metric depends on the capabilities of the executive and the way in which it will interpret and execute the plan.

#index 1250639
#* A causal analysis method for concurrent hybrid automata
#@ Michael W. Hofbaur;Franz Wotawa
#t 2006
#c 10
#% 21137
#% 21138
#% 25884
#% 25885
#% 65347
#% 125529
#% 207263
#% 292245
#% 331899
#% 445908
#% 518899
#% 924702
#% 1781501
#! Modern artifacts are typically composed of many system components and exhibit a complex pattern of continuous/discrete behaviors. A concurrent hybrid automaton is a powerful modeling concept to capture such a system's behavior in terms a concurrent composition of hybrid automata for the individual system components. Because of the potentially large number of modes of the concurrent automaton model it is non-trivial to validate the composition such that every possible operational mode leads to a causally valid dynamic model for the overall system. This paper presents a novel model analysis method that validates the automata composition without the necessity to analyze a prohibitively large number of modes. We achieve this by formulating the exhaustive causal analysis of hybrid automata as a diagnosis problem. This provides causal specifications of the component automata and enables us to efficiently calculate the causal relationships for their concurrent composition and thus validate a concurrent automaton model.

#index 1250640
#* Tractable classes of metric temporal problems with domain rules
#@ T. K. Sathish Kumar
#t 2006
#c 10
#% 107137
#% 275221
#% 535149
#% 736897
#% 743461
#% 1675274
#! In this paper, we will deal with some important kinds of metric temporal reasoning problems that arise in many real-life situations. In particular, events X0, X1 ... XN are modeled as time points, and a constraint between the execution times of two events Xi and Xj is either simple temporal (of the form Xi - Xj ∈ [a, b]), or has a connected feasible region that can be expressed using a finite set of domain rules each in turn of the form Xi ∈ [a, b] → Xj ∈ [c, d] (and conversely Xj ∈ [e, f] → Xi ∈ [g, h]). We argue that such rules are useful in capturing important kinds of non-monotonic relationships between the execution times of events when they are governed by potentially complex (external) factors. Our polynomial-time (deterministic and randomized) algorithms for solving such problems therefore enable us to efficiently deal with very expressive representations of time.

#index 1250641
#* A modular action description language
#@ Vladimir Lifschitz;Wanwan Ren
#t 2006
#c 10
#% 3035
#% 26351
#% 28185
#% 341640
#% 400987
#% 405344
#% 763743
#% 763747
#% 934828
#% 1478800
#% 1656405
#! "Toy worlds" involving actions, such as the blocks world and the Missionaries and Cannibals puzzle, are often used by researchers in the areas of commonsense reasoning and planning to illustrate and test their ideas. We would like to create a datahase of general-purpose knowledge about actions that encodes common features of many action domains of this kind. in the same way as abstract algebra and topology represent common features of specific number systems. This paper is a report on the first stage of this project--the design of an action description language in which this database will be written The new language is an extension of the action language C+. Its main distinctive feature is the possibility of referring to other action descriptions in the definition of a new action domain.

#index 1250642
#* PPCP: efficient probabilistic planning with clear preferences in partially-known environments
#@ Maxim Likhachev;Anthony Stentz
#t 2006
#c 10
#% 30037
#% 181627
#% 337981
#% 837649
#% 1279358
#% 1279387
#! For most real-world problems the agent operates in only partially-known environments. Probabilistic planners can reason over the missing information and produce plans that take into account the uncertainty about the environment. Unfortunately though, they can rarely scale up to the problems that are of interest in real-world. In this paper, however, we show that for a certain subset of problems we can develop a very efficient probabilistic planner. The proposed planner, called PPCP, is applicable to the problems for which it is clear what values of the missing information would result in the best plan. In other words, there exists a clear preference for the actual values of the missing information. For example, in the problem of robot navigation in partially-known environments it is always preferred to find out that an initially unknown location is traversable rather than not. The planner we propose exploits this property by using a series of deterministic A*-like searches to construct and refine a policy in anytime fashion. On the theoretical side, we show that once converged, the policy is guaranteed to be optimal under certain conditions. On the experimental side, we show the power of PPCP on the problem of robot navigation in partially-known terrains. The planner can scale up to very large environments with thousands of initially unknown locations. We believe that this is several orders of magnitude more unknowns than what the current probabilistic planners developed for the same problem can handle. Also, despite the fact that the problem we experimented on in general does not satisfy the conditions for the solution optimality, PPCP still produces the solutions that are nearly always optimal.

#index 1250643
#* Reasoning about discrete event sources
#@ Shieu-Hong Lin
#t 2006
#c 10
#% 46272
#% 182941
#% 208046
#% 297770
#% 342119
#% 346653
#% 743353
#% 903341
#% 1269542
#! We investigate the modelling of workflows, plans, and other event-generating processes as discrete event sources and reason about the possibility of having event sequences ending in undesirable states. In previous research, the problem is shown to be NP-Complete even if the number of events to occur is fixed in advance. In this paper, we consider possible events sequences of indefinite length and show that many interesting cases of such reasoning task are solvable in polynomial time.

#index 1250644
#* Optimal scheduling of contract algorithms for anytime problems
#@ Alejandro López-Ortiz;Spyros Angelopoulos;Angèle M. Hamel
#t 2006
#c 10
#% 281808
#% 282477
#% 288975
#% 331356
#% 481220
#% 495928
#% 578760
#% 1279384
#! A contract algorithm is an algorithm which is given, as part of the input, a specified amount of allowable computation time. The algorithm must then compute a solution within the alloted time. An interruptible algorithm, in contrast, can be interrupted at an arbitrary point in time and must produce a solution. It is known that contract algorithms can simulate interruptible algorithms using iterative deepening techniques. This simulation is done at a penalty in the performance of the solution, as measured by the so-called acceleration ratio. In this paper we give matching (i.e. optimal) upper and lower bounds for the acceleration ratio under this simulation. This resolves an open conjecture of Bernstein et al. [IJCAI 2003] who gave an ingenious optimal schedule under the restricted setting of round robin and length-increasing processor schedules, but whose optimality in the general unrestricted case remained open.

#index 1250645
#* Probabilistic temporal planning with uncertain durations
#@  Mausam;Daniel S. Weld
#t 2006
#c 10
#% 179938
#% 181338
#% 181627
#% 194652
#% 495772
#% 840906
#% 1250231
#% 1250235
#% 1250645
#% 1269547
#% 1272008
#% 1289204
#% 1290109
#% 1290110
#% 1650355
#! Few temporal planners handle both concurrency and uncertain durations, but these features commonly co-occur in real-world domains. In this paper, we discuss the challenges caused by concurrent, durative actions whose durations are uncertain. We present five implemented algorithms, including ΔDURprun, a planner guaranteed to find the optimal policy. An empirical comparison reveals that ΔDURexp, our fastest planner, obtains orders of magnitude speed-up compared to ΔDURprun--with little loss in solution quality. Importantly, our algorithms can handle probabilistic effects in addition to stochastic durations, and they are effective even when duration distributions are multi-modal.

#index 1250646
#* Reasoning about partially observed actions
#@ Megan Nance;Adam Vogel;Eyal Amir
#t 2006
#c 10
#% 561712
#% 716892
#% 1273684
#% 1279222
#% 1289445
#% 1476298
#! Partially observed actions are observations of action executions in which we are uncertain about the identity of objects, agents, or locations involved in the actions (e.g., we know that action move(?o, ?x, ?y) occurred, but do not know ?o, ?y). Observed-Action Reasoning is the problem of reasoning about the world state after a sequence of partial observations of actions and states. In this paper we formalize Observed-Action Reasoning, prove intractability results for current techniques, and find tractable algorithms for STRIPS and other actions. Our new algorithms update a representation of all possible world states (the belief state) in logic using new logical constants for unknown objects. A straightforward application of this idea is incorrect, and we identify and add two key amendments. We also present successful experimental results for our algorithm in Blocks-world domains of varying sizes and in Kriegspiel (partially observable chess). These results are promising for relating sensors with symbols, partial-knowledge games, multi-agent decision making, and AI planning.

#index 1250647
#* Approximate compilation for embedded model-based reasoning
#@ Barry O'Sullivan;Gregory M. Provan
#t 2006
#c 10
#% 3460
#% 121397
#% 204396
#% 345434
#% 443070
#% 475714
#% 578749
#% 936786
#% 1272349
#% 1275334
#% 1698020
#! The use of embedded technology has become widespread. Many complex engineered systems comprise embedded features to perform self-diagnosis or self-reconfiguration. These features require fast response times in order to be useful in domains where embedded systems are typically deployed. Researchers often advocate the use of compilation-based approaches to store the set of environments (resp. solutions) to a diagnosis (resp. reconfiguration) problem, in some compact representation. However, the size of a compiled representation may be exponential in the treewidth of the problem. In this paper we propose a novel method for compiling the most preferred environments in order to reduce the large space requirements of our compiled representation. We show that approximate compilation is an effective means of generating the highest-valued environments, while obtaining a representation whose size can be tailored to any embedded application. The method also provides a graceful way to tradeoff space requirements with the completeness of our coverage of the environment space.

#index 1250648
#* Compiling uncertainty away: solving conformant planning problems using a classical planner (sometimes)
#@ Héctor Palacios;Héctor Geffner
#t 2006
#c 10
#% 167629
#% 188086
#% 244412
#% 266386
#% 318489
#% 495996
#% 544923
#% 789560
#% 1269552
#% 1271962
#! Even under polynomial restrictions on plan length, conformant planning remains a very hard computational problem as plan verification itself can take exponential time. This heavy price cannot be avoided in general although in many cases conformant plans are verifiable efficiently by means of simple forms of disjunctive inference. This raises the question of whether it is possible to identify and use such forms of inference for developing an efficient but incomplete planner capable of solving non-trivial problems quickly. In this work, we show that this is possible by mapping conformant into classical problems that are then solved by an off-the-shelf classical planner. The formulation is sound as the classical plans obtained are all conformant, but it is incomplete as the inverse relation does not always hold. The translation accommodates 'reasoning by cases' by means of an 'split-protect-and-merge' strategy; namely, atoms L/Xi that represent conditional beliefs 'if Xi then L' are introduced in the classical encoding, that are combined by suitable actions to yield the literal L when the disjunction X1 ∨ ... ∨ Xn holds and certain invariants in the plan are verified. Empirical results over a wide variety of problems illustrate the power of the approach.

#index 1250649
#* Sensor-based understanding of daily life via large-scale use of common sense
#@ William Pentney;Ana-Maria Popescu;Shiaokai Wang;Henry Kautz;Matthai Philipose
#t 2006
#c 10
#% 44876
#% 308747
#% 405391
#% 509695
#% 754115
#% 788954
#% 812412
#% 843359
#% 850430
#% 1250181
#% 1250214
#% 1269362
#% 1289473
#% 1650633
#! The use of large quantities of common sense has long been thought to be critical to the automated understanding of the world. To this end, various groups have collected repositories of common sense in machine-readable form. However, efforts to apply these large bodies of knowledge to enable correspondingly large-scale sensor-based understanding of the world have been few. Challenges have included semantic gaps between facts in the repositories and phenomena detected by sensors, fragility of reasoning in the face of noise, incompleteness of repositories, and slowness of reasoning with these large repositories. We show how to address these problems with a combination of novel sensors, probabilistic representation, web-scale information retrieval and approximate reasoning. In particular, we show how to use the 50,000-fact hand-entered Open-Mind Indoor Common Sense database to interpret sensor traces of day-to-day activities with 88% accuracy (which is easy) and 32/53% precision/recall (which is not).

#index 1250650
#* Learning partially observable action schemas
#@ Dafna Shahaf;Eyal Amir
#t 2006
#c 10
#% 3035
#% 333786
#% 384911
#% 496116
#% 703709
#% 1289461
#% 1289577
#% 1650580
#! We present an algorithm that derives actions' effects and preconditions in partially observable, relational domains. Our algorithm has two unique features: an expressive relational language, and an exact tractable computation. An action-schema language that we present permits learning of preconditions and effects that include implicit objects and unstated relationships between objects. For example, we can learn that replacing a blown fuse turns on all the lights whose switch is set to on. The algorithm maintains and outputs a relational-logical representation of all possible action-schema models after a sequence of executed actions and partial observations. Importantly, our algorithm takes polynomial time in the number of time steps and predicates. Time dependence on other domain parameters varies with the action-schema language. Our experiments show that the relational structure speeds up both learning and generalization, and outperforms propositional learning methods. It also allows establishing apriori-unknown connections between objects (e.g. light bulbs and their switches), and permits learning conditional effects in realistic and complex situations. Our algorithm takes advantage of a DAG structure that can be updated efficiently and preserves compactness of representation.

#index 1250651
#* Learning partially observable action models: efficient algorithms
#@ Dafna Shahaf;Allen Chang;Eyal Amir
#t 2006
#c 10
#% 246836
#% 336874
#% 1250650
#% 1271828
#% 1289212
#% 1289577
#% 1650276
#! We present tractable, exact algorithms for learning actions' effects and preconditions in partially observable domains. Our algorithms maintain a propositional logical representation of the set of possible action models after each observation and action execution. The algorithms perform exact learning of preconditions and effects in any deterministic action domain. This includes STRIPS actions and actions with conditional effects. In contrast, previous algorithms rely on approximations to achieve tractability, and do not supply approximation guarantees. Our algorithms take time and space that are polynomial in the number of domain features, and can maintain a representation that stays compact indefinitely. Our experimental results show that we can learn efficiently and practically in domains that contain over 1000's of features (more than 21000 states).

#index 1250652
#* Contingent planning with goal preferences
#@ Dmitry Shaparau;Marco Pistore;Paolo Traverso
#t 2006
#c 10
#% 590623
#% 655322
#% 1250208
#% 1271992
#% 1272399
#% 1289550
#! The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.

#index 1250653
#* Motion-based autonomous grounding: inferring external world properties from encoded internal sensory states alone
#@ Yoonsuck Choe;Noah H. Smith
#t 2006
#c 10
#% 85153
#% 97619
#% 124691
#% 179940
#% 229084
#% 274349
#% 361663
#% 376266
#% 722460
#% 840843
#! How can we build artificial agents that can autonomously explore and understand their environments? An immediate requirement for such an agent is to learn how its own sensory state corresponds to the external world properties: It needs to learn the semantics of its internal state (i.e., grounding). In principle, we as programmers can provide the agents with the required semantics, but this will compromise the autonomy of the agent. To overcome this problem, we may fall back on natural agents and see how they acquire meaning of their own sensory states, their neural firing patterns. We can learn a lot about what certain neural spikes mean by carefully controlling the input stimulus while observing how the neurons fire. However, neurons embedded in the brain do not have direct access to the outside stimuli, so such a stimulus-to-spike association may not be learnable at all. How then can the brain solve this problem? (We know it does.) We propose that motor interaction with the environment is necessary to overcome this conundrum. Further, we provide a simple yet powerful criterion, sensory invariance, for learning the meaning of sensory states. The basic idea is that a particular form of action sequence that maintains invariance of a sensory state will express the key property of the environmental stimulus that gave rise to the sensory state. Our experiments with a sensorimotor agent trained on natural images show that sensory invariance can indeed serve as a powerful objective for semantic grounding.

#index 1250654
#* Efficient triangulation-based pathfinding
#@ Douglas Demyen;Michael Buro
#t 2006
#c 10
#% 170198
#% 1269581
#% 1499544
#! In this paper we present a method for abstracting an environment represented using constrained Delaunay triangulations in a way that significantly reduces pathfinding search effort, as well as better representing the basic structure of the environment. The techniques shown here are ideal for objects of varying sizes and environments that are not axis-aligned or that contain many dead-ends, long corridors, or jagged walls that complicate other search techniques. In fact, the abstraction simplifies pathfinding to deciding to which side of each obstacle to go. This technique is suited to real-time computation both because of its speed and because it lends itself to an anytime algorithm, allowing it to work when varying amounts of resources are assigned to pathfinding. We test search algorithms running on both the base triangulation (Triangulation A* - TA*) and our abstraction (Triangulation Reduction A* - TRA*) against A* and PRA* on grid-based maps from the commercial games Baldur's Gate and WarCraft III. We find that in these cases almost all paths are found much faster using TA*, and more so using TRA*.

#index 1250655
#* Exploiting spatial and temporal flexibility for plan execution of hybrid, under-actuated systems
#@ Andreas G. Hofmann;Brian C. Williams
#t 2006
#c 10
#% 266108
#% 518903
#% 715056
#% 1271885
#! Robotic devices, such as rovers and autonomous spacecraft, have been successfully controlled by plan execution systems that use plans with temporal flexibility to dynamically adapt to temporal disturbances. To date these execution systems apply to discrete systems that abstract away the detailed dynamic constraints of the controlled device. To control dynamic, under-actuated devices, such as agile bipedal walking machines, we extend this execution paradigm to incorporate detailed dynamic constraints. Building upon prior work on dispatchable plan execution, we introduce a novel approach to flexible plan execution of hybrid under-actuated systems that achieves robustness by exploiting spatial as well as temporal plan flexibility. To accomplish this, we first transform the high-dimensional system into a set of low dimensional, weakly coupled systems. Second, to coordinate these systems such that they achieve the plan in real-time, we compile a plan into a concurrent timed flow tube description. This description represents all feasible control trajectories and their temporal coordination constraints, such that each trajectory satisfies all plan and dynamic constraints. Finally, the problem of runtime plan dispatching is reduced to maintaining state trajectories in their associated flow tubes, while satisfying the coordination constraints. This is accomplished through an efficient local search algorithm that adjusts a small number of control parameters in real-time. The first step has been published previously; this paper focuses on the last two steps. The approach is validated on the execution of a set of bipedal walking plans, using a high fidelity simulation of a biped.

#index 1250656
#* Object boundary detection in images using a semantic ontology
#@ Anthony Hoogs;Roderic Collins
#t 2006
#c 10
#% 349208
#% 378065
#% 405391
#% 443972
#% 444004
#% 635730
#% 722927
#% 744799
#% 812359
#% 836836
#% 836847
#! We present a novel method for detecting the boundaries between objects in images that uses a large, hierarchical, semantic ontology - WordNet. The semantic object hierarchy in WordNet grounds this ill-posed segmentation problem, so that true boundaries are defined as edges between instances of different classes, and all other edges are clutter. To avoid fully classifying each pixel, which is very difficult in generic images, we evaluate the semantic similarity of the two regions bounding each edge in an initial oversegmentation. Semantic similarity is computed using WordNet enhanced with appearance information, and is largely orthogonal to visual similarity. Hence two regions with very similar visual attributes, but from different categories, can have a large semantic distance and therefore evidence of a strong boundary between them, and vice versa. The ontology is trained with images from the UC Berkeley image segmentation benchmark, extended with manual labeling of the semantic content of each image segment. Results on boundary detection against the benchmark images show that semantic similarity computed through WordNet can significantly improve boundary detection compared to generic segmentation.

#index 1250657
#* Bayesian calibration for Monte Carlo localization
#@ Armita Kaboli;Michael Bowling;Petr Musilek
#t 2006
#c 10
#% 283140
#% 337494
#% 770835
#% 1760897
#! Localization is a fundamental challenge for autonomous robotics. Although accurate and efficient techniques now exist for solving this problem, they require explicit probabilistic models of the robot's motion and sensors. These models are usually obtained from time-consuming and error-prone measurement or tedious manual tuning. In this paper we examine automatic calibration of sensor and motion models from a Bayesian perspective. We introduce an efficient MCMC procedure for sampling from the posterior distribution of the model parameters. We also present a novel extension of particle filters to make use of our posterior parameter samples. Finally, we demonstrate our approach both in simulation and on a physical robot. Our results demonstrate effective inference of model parameters as well as a paradoxical result that using posterior parameter samples can produce more accurate position estimates than the true parameters.

#index 1250658
#* Diagnosis of multi-robot coordination failures using distributed CSP algorithms
#@ Meir Kalech;Gal A. Kaminka;Amnon Meisels;Yehuda Elmaliach
#t 2006
#c 10
#% 360802
#% 431523
#% 443227
#% 643161
#% 855910
#% 1269375
#! With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis; however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.

#index 1250659
#* Probabilistic self-localization for sensor networks
#@ Dimitri Marinakis;Gregory Dudek
#t 2006
#c 10
#% 401228
#% 608491
#% 724279
#% 751038
#% 783725
#% 879222
#% 1756864
#% 1831268
#% 1851033
#! This paper describes a technique for the probabilistic self-localization of a sensor network based on noisy inter-sensor range data. Our method is based on a number of parallel instances of Markov Chain Monte Carlo (MCMC). By combining estimates drawn from these parallel chains, we build up a representation of the underlying probability distribution function (PDF) for the network pose. Our approach includes sensor data incrementally in order to avoid local minima and is shown to produce meaningful results efficiently. We return a distribution over sensor locations rather than a single maximum likelihood estimate. This can then be used for subsequent exploration and validation.

#index 1250660
#* Winning the DARPA grand challenge with an AI robot
#@ Michael Montemerlo;Sebastian Thrun;Hendrik Dahlkamp;David Stavens;Sven Strohband
#t 2006
#c 10
#% 359129
#% 445051
#% 953058
#! This paper describes the software architecture of Stanley, an autonomous land vehicle developed for high-speed desert driving without human intervention. The vehicle recently won the DARPA Grand Challenge, a major robotics competition. The article describes the software architecture of the robot, which relied pervasively on state-of-the-art AI technologies, such as machine learning and probabilistic reasoning.

#index 1250661
#* A manifold regularization approach to calibration reduction for sensor-network based tracking
#@ Jeffrey Junfeng Pan;Qiang Yang;Hong Chang;Dit-Yan Yeung
#t 2006
#c 10
#% 252011
#% 339204
#% 339218
#% 420077
#% 579678
#% 593047
#% 613383
#% 777786
#% 783725
#% 819455
#% 992680
#% 1113048
#% 1250174
#% 1269558
#% 1289473
#! The ability to accurately detect the location of a mobile node in a sensor network is important for many artificial intelligence (AI) tasks that range from robotics to context-aware computing. Many previous approaches to the location-estimation problem assume the availability of calibrated data. However, to obtain such data requires great effort. In this paper, we present a manifold regularization approach known as LeMan to calibration-effort reduction for tracking a mobile node in a wireless sensor network. We compute a subspace mapping function between the signal space and the physical space by using a small amount of labeled data and a large amount of unlabeled data. This mapping function can be used online to determine the location of mobile nodes in a sensor network based on the signals received. We use Crossbow MICA2 to setup the network and USB camera array to obtain the ground truth. Experimental results show that we can achieve a higher accuracy with much less calibration effort as compared to several previous systems.

#index 1250662
#* Running the table: an AI for computer billiards
#@ Michael Smith
#t 2006
#c 10
#% 1733909
#! Billiards is a game of both strategy and physical skill. To succeed, a player must be able to select strong shots, and then execute them accurately and consistently. Several robotic billiards players have recently been developed. These systems address the task of executing shots on a physical table, but so far have incorporated little strategic reasoning. They require AI to select the 'best' shot taking into account the accuracy of the robotics, the noise inherent in the domain, the continuous nature of the search space, the difficulty of the shot, and the goal of maximizing the chances of winning. This paper develops and compares several approaches to establishing a strong AI for billiards. The resulting program, PickPocket, won the first international computer billiards competition.

#index 1250663
#* Reinforcement learning with human teachers: evidence of feedback and guidance with implications for learning performance
#@ Andrea L. Thomaz;Cynthia Breazeal
#t 2006
#c 10
#% 124691
#% 126853
#% 166352
#% 252816
#% 334536
#% 398420
#% 418632
#% 466419
#% 643109
#% 675614
#% 1650593
#! As robots become a mass consumer product, they will need to learn new skills by interacting with typical human users. Past approaches have adapted reinforcement learning (RL) to accept a human reward signal; however, we question the implicit assumption that people shall only want to give the learner feedback on its past actions. We present findings from a human user study showing that people use the reward signal not only to provide feedback about past actions, but also to provide future directed rewards to guide subsequent actions. Given this, we made specific modifications to the simulated RL robot to incorporate guidance. We then analyze and evaluate its learning performance in a second user study, and we report significant improvements on several measures. This work demonstrates the importance of understanding the human-teacher/robot-learner system as a whole in order to design algorithms that support how people want to teach while simultaneously improving the robot's learning performance.

#index 1269357
#* Proceedings of the 20th national conference on Artificial intelligence - Volume 1
#@ Anthony Cohn
#t 2005
#c 10

#index 1269358
#* AAAI-05 preface
#@ Manuela Veloso;Subbarao Kambhampati
#t 2005
#c 10

#index 1269359
#* Multiple-goal recognition from low-level signals
#@ Xiaoyong Chai;Qiang Yang
#t 2005
#c 10
#% 147680
#% 716892
#% 1250174
#% 1250210
#% 1272356
#% 1279397
#% 1279398
#% 1650293
#! Researchers and practitioners from both the artificial intelligence and pervasive computing communities have been paying increasing attention to the task of inferring users' high-level goals from low-level sensor readings. A common assumption made by most approaches is that a user either has a single goal in mind, or achieves several goals sequentially. However, in real-world environments, a user often has multiple goals that are concurrently carried out, and a single action can serve as a common step towards multiple goals. In this paper, we formulate the multiple-goal recognition problem and exemplify it in an indoor environment where an RF-based wireless network is available. We propose a goal-recognition algorithm based on a dynamic model set and show how goal models evolve over time based on pre-defined states. Experiments with real data demonstrate that our method can accurately and efficiently recognize multiple interleaving goals in a user's trace.

#index 1269360
#* A variational learning algorithm for the abstract hidden Markov model
#@ Jeff Johns;Sridhar Mahadevan
#t 2005
#c 10
#% 115608
#% 246836
#% 303620
#% 592062
#% 716892
#% 724234
#% 1250174
#% 1272356
#% 1650767
#! We present a fast algorithm for learning the parameters of the abstract hidden Markov model, a type of hierarchical activity recognition model. Learning using exact inference scales poorly as the number of levels in the hierarchy increases; therefore, an approximation is required for large models. We demonstrate that variational inference is well suited to solve this problem. Not only does this technique scale. but it also offers a natural way to leverage the context specific independence properties inherent in the model via the fixed point equations. Experiments confirm that the variational approximation significantly reduces the time necessary for learning while estimating parameter values that can be used to make reliable predictions.

#index 1269361
#* Large-scale localization from wireless signal strength
#@ Julia Letchner;Dieter Fox;Anthony LaMarca
#t 2005
#c 10
#% 401172
#% 777788
#% 1113048
#% 1250174
#% 1719088
#! Knowledge of the physical locations of mobile devices such as laptops or PDA's is becoming increasingly important with the rise of location-based services such as specialized web search, navigation and social network applications; furthermore, location information is a key foundation for high-level activity inferencing. In this paper we propose a novel technique for accurately estimating the locations of mobile devices and their wearers from wireless signal strengths. Our technique estimates time-varying device locations on a spatial connectivity graph whose outdoor edges correspond to streets and whose indoor edges represent hallways. staircases, elevators. etc. Use of a hierarchical Bayesian framework for learning a signal strength sensor model allows us not only to achieve higher accuracy than existing approaches. but to overcome many of their limitations. In particular, our technique is able to (1) seamlessly integrate new access points into the model, (2) make use of negative information (not detecting an access point), and (3) bootstrap a sensor model from sparse training data. Experiments demonstrate various properties of our system.

#index 1269362
#* Unsupervised activity recognition using automatically mined common sense
#@ Danny Wyatt;Matthai Philipose;Tanzeem Choudhury
#t 2005
#c 10
#% 169717
#% 252011
#% 266215
#% 279755
#% 405391
#% 509695
#% 722929
#% 746867
#% 754115
#% 756232
#% 788954
#% 854229
#% 1250181
#% 1289474
#! A fundamental difficulty in recognizing human activities is obtaining the labeled data needed to learn models of those activities. Given emerging sensor technology, however, it is possible to view activity data as a stream of natural language terms. Activity models are then mappings from such terms to activity names, and may be extracted from text corpora such as the web. We show that models so extracted are sufficient to automatically produce labeled segmentations of activity data with an accuracy of 42% over 26 activities, well above the 3.89% baseline. The segmentation so obtained is sufficient to bootstrap learning, with accuracy of learned models increasing to 52%. To our knowledge, this is the first human activity inferencing system shown to learn from sensed activity data with no human intervention per activity learned, even for labeling.

#index 1269363
#* Activity recognition through goal-based segmentation
#@ Jie Yin;Dou Shen;Qiang Yang;Ze-Nian Li
#t 2005
#c 10
#% 329537
#% 398425
#% 466506
#% 564281
#% 592134
#% 629629
#% 775606
#% 1250174
#% 1250210
#% 1272356
#% 1279342
#% 1650293
#! A major issue in activity recognition in a sensor network is how to automatically segment the low-level signal sequences in order to optimize the probabilistic recognition models for goals and activities. Past efforts have relied on segmenting the signal sequences by hand, which is both time-consuming and error-prone. In our view, segments should correspond to atomic human activities that enable a goal-recognizer to operate optimally; the two are intimately related. In this paper, we present a novel method for building probabilistic activity models at the same time as we segment signal sequences into motion patterns. We model each motion pattern as a linear dynamic model and the transitions between motion patterns as a Markov process conditioned on goals. Our EM learning algorithm simultaneously learns the motion-pattern boundaries and probabilistic models for goals and activities, which in turn can be used to accurately recognize activities in an online phase. A major advantage of our algorithm is that it can reduce the human effort in segmenting and labeling signal sequences. We demonstrate the effectiveness of our algorithm using the data collected in a real wireless environment.

#index 1269364
#* Team member reallocation via tree pruning
#@ Noa Agmon;Gal A. Kaminka;Sarit Kraus
#t 2005
#c 10
#% 65998
#% 70370
#% 233134
#% 233135
#% 252199
#% 280428
#% 301910
#% 302109
#% 345429
#% 379178
#% 408396
#! This paper considers the task reallocation problem, where k agents are to be extracted from a coordinated group of N agents in order to perform a new task. The interaction between the team members and the cost associated with this interaction are represented by a weighted graph. Consider a group of N robots organized in a formation, the graph is the monitoring graph which represents the sensorial capabilities of the robots, i.e., which robot can sense the other and at what cost. Following this example, the team member reallocation problem this paper deals with is the extraction of k robots from the group in order to acquire a new target, while minimizing the cost of the interaction of the remaining group. In general, the method proposed here shifts the utility from the team member itself to the interaction between the members, and calculates the reallocation according to this interaction utility. We found that this can be done optimally by a deterministic polynomial time algorithm under several constraints, the first constraint is that k = O(log N). We describe several other domains in which this method is applicable.

#index 1269365
#* Efficient no-regret multiagent learning
#@ Bikramjit Banerjee;Jing Peng
#t 2005
#c 10
#% 165663
#% 266286
#% 348821
#% 384911
#% 464283
#% 464437
#% 465913
#% 528018
#% 813744
#% 1250119
#! We present new results on the efficiency of no-regret algorithmsin the context of multiagent learning. We use a known approach to augment a large class of no-regret algorithms to allow stochastic sampling of actions and observation of scalar reward of only the action played. We show that the average actual payoffs of the resulting learner gets (1) close to the best response against (eventually) stationary opponents. (2) close to the asymptotic optimal payoff against opponents that playa converging sequence of policies. and (3) close to at least a dynamic variant of minimax payoff against arbitrary opponents. with a high probability in polynomial time. In addition the polynomial bounds are shown to be significantly better than previously known bounds. Furthermore, we do not need to assume that the learner knows the game matrices and can observe the opponents' actions, unlike previous work.

#index 1269366
#* Solving DisCSPs with penalty driven search
#@ Muhammed Basharu;Ines Arana;Hatem Ahriz
#t 2005
#c 10
#% 1894
#% 3179
#% 286424
#% 431523
#% 534985
#% 855910
#% 855911
#% 1478600
#! We introduce the Distributed, Penalty-driven Local search algorithm (DisPeL) for solving Distributed Constraint Satisfaction Problems. DisPeL is a novel distributed iterative improvement algorithm which escapes local optima by the use of both temporary and incremental penalties and a tabu-like no-good store. We justify the use of these features and provide empirical results which demonstrate the competitiveness of the algorithm.

#index 1269367
#* Coordination and adaptation in impromptu teams
#@ Michael Bowling;Peter McCracken
#t 2005
#c 10
#% 164502
#% 215532
#% 266286
#% 280042
#% 643210
#% 1272316
#! Coordinating a team of autonomous agents is one of the major challenges in building effective multiagcnt systems. Many techniques have been devised for this problem. and coordinated teamwork has been demonstrated even in highly dynamic and adversarial environments. A key assumption of these techniques. though. is that the team members are developed together as a whole. In many multi agent scenarios. this assumption is violated. We study the problem of coordination in impromptu teams, where a team is composed of independent agents each unknown to the others. The team members have their own skills. models. strategies. and coordination mechanisms. and no external organization is imposed upon them. In particular. we propose two techniques. one adaptive and one predictive. for coordinating a single agent that joins an unknown team of existing agents. We experimentally evaluate these mechanisms in the robot soccer domain, while introducing useful baselines for evaluating the performance of impromptu teams. We show some encouraging success while demonstrating this is a very fertile area of research.

#index 1269368
#* Robust and self-repairing formation control for swarms of mobile agents
#@ Jimming Cheng;Winston Cheng;Radhika Nagpal
#t 2005
#c 10
#% 31686
#% 418811
#% 426521
#% 723960
#% 766307
#% 1279304
#! We describe a decentralized algorithm for coordinating a swarm of identically-programmed mobile agents to spatially self-aggregate into arbitrary shapes using only local interactions. Our approach, called SHAPEBUGS, generates a consensus coordinate system by agents continually performing local trilaterations, and achieves shape formation by simultaneously allowing agents to disperse within the defined 2D shape using a Contained Gas Model. This approach has several novel features (1) agents can easily aggregate into arbitrary user-specified shapes, using a formation process that is independent of the number of agents (2) the system automatically adapts to influx and death of agents, as well as accidental displacement. We show that the consensus coordinate system is robust and provides reasonable accuracy in the face of significant sensor and movement error.

#index 1269369
#* An extended protocol for multiple-issue concurrent negotiation
#@ Jiangbo Dang;Michael N. Huhns
#t 2005
#c 10
#% 396661
#% 531907
#% 732831
#% 773330
#% 773331
#% 796070
#% 799161
#! Negotiation is the technique for reaching mutually beneficial agreement among agent via communication. A concurrent negotiation problem occurs when an agent needs to negotiate with multiple agents to reach agreement. In this paper, we present a protocol to support many-to-many bilateral multiple-issue negotiation in a competitive environment. The protocol is presented in the context of service-oriented negotiation, where one or more self-interested parties can provide services to one or more other parties. By extending existing negotiation protocols, our described protocol enables both service requestors and service providers to manage several negotiation processes in parallel. Moreover, this protocol mitigates the situation where most one-to-many negotiations are biased in favor of one participating agent, and allow the negotiation participants to make durable commitments to reduce the decommitment situation. We conclude by discussing additional issues related to concurrent multiple-issue negotiation.

#index 1269370
#* The semantics of potential intentions
#@ Xiaocong Fan;John Yen
#t 2005
#c 10
#% 36815
#% 68239
#% 101955
#% 116625
#% 215532
#% 379069
#% 518805
#! The SharedPlans theory provides an axiomatic framework of collaborative plans based on four types of intentional attitudes. However, there still lacks an adequate semantics for the 'potential intention' operators. In this paper, we give a formal semantics to potential intentions, and examine models that can validate various relations between beliefs, intentions, and potential intentions.

#index 1269371
#* Agent-organized networks for multi-agent production and exchange
#@ Matthew E. Gaston;Marie DesJardins
#t 2005
#c 10
#% 430241
#% 432520
#% 469923
#% 643087
#% 1279455
#% 1837416
#! As multi-agent systems grow in size and complexity, social networks that govern the interactions among the agents will directly impact system behavior at the individual and collective levels. Examples of such large-scale, networked multi-agent systems include peer-to-peer networks, distributed information retrieval, and agent-based supply chains. One way of dealing with the uncertain and dynamic nature of such environments is to endow agents with the ability to modify the agent social network by autonomously adapting their local connectivity structure. In this paper, we present a framework for agent-organized networks (AONs) in the context of multi-agent production and exchange, and experimentally evaluate the feasibility and efficiency of specific AON strategies. We find that decentralized network adaptation can significantly improve organizational performance. Additionally, we analyze several properties of the resulting network structures and consider their relationship to the observed increase in organizational performance.

#index 1269372
#* Supporting collaborative activity
#@ Meirav Hadad;Gilad Armon-Kest;Gal A. Kaminka;Sarit Kraus
#t 2005
#c 10
#% 189698
#% 215532
#% 378986
#% 379163
#% 518658
#% 823892
#% 932181
#% 1272316
#! This paper presents a model--SharedActivity--for collaborative agents acting in a group. The model suggests mental states for agents with different levels of cooperation and permits the formation of groups in which members increase individual benefits. Unlike previous models, the model covers group member behavior where group members do not have a joint goal, but act collaboratively. The model defines key components of a collaborative activity and provides a platform for supporting such activity. We studied the behavior of the model in a simulation environment. Results show how the benefit attained by cooperation is influenced by the complexity of the environment, the number of group members, and the social dependencies between the members. The results demonstrate that the model covers social behavior both in settings previously addressed, as well as in novel settings.

#index 1269373
#* Multiple agent event detection and representation in videos
#@ Asaad Hakeem;Mubarak Shah
#t 2005
#c 10
#% 313956
#% 313957
#% 313959
#% 529330
#% 592328
#% 592369
#% 1250160
#% 1502506
#% 1650580
#! We propose a novel method to detect events involving multiple agents in a video and to learn their structure in terms of temporally related chain of sub-events. The proposed method has three significant contributions over existing frameworks. First, in order to learn the event structure from training videos. we present the concept of a video event graph, which is composed of temporally related sub-events. Using the video event graph, we automatically encode the event dependency graph. The event dependency graph is the learnt event model that depicts the frequency of occurrence of conditionally dependent sub-events. Second. we pose the problem of event detection in novel videos as clustering the maximally correlated sub-events, and use normalized cuts to determine these clusters. The pIincipal assumption made in this work is that the events are composed of highly correlated chain of sub-events. that have high weights (association) within the cluster and relatively low weights (disassociation) between clusters. These weights (between sub-events) are the likelihood estimates obtained from the event models. Last, we recognize the importance of representing the variations in the temporal order of sub-events. occurring in an event, and encode the probabilities directly into our representation. We show results of our learning, detection, and representation of events for videos in the meeting, surveillance, and railroad monitoring domains.

#index 1269374
#* Anyone but him: the complexity of precluding an alternative
#@ Edith Hemaspaandra;Lane A. Hemaspaandra;Jörg Rothe
#t 2005
#c 10
#% 242217
#% 330769
#% 404772
#% 408396
#% 529806
#% 578703
#% 631051
#% 654466
#! Preference aggregation in a multiagent setting is a central issue in both human and computer contexts. In this paper, we study in terms of complexity the vulnerability of preference aggregation to destructive control. That is, we study the ability of an election's chair to, through such mechanisms as voter/candidate addition/suppression/partition, ensure that a particular candidate (equivalently, alternative) does not win. And we study the extent to which election systems can make it impossible, or computationally costly (NP-complete), for the chair to execute such control. Among the systems we study--plurality, Condorcet, and approval voting--we find cases where systems immune or computationally resistant to a chair choosing the winner nonetheless are vulnerable to the chair blocking a victory. Beyond that, we see that among our studied systems no one system offers the best protection against destructive control. Rather, the choice of a preference aggregation system will depend closely on which types of control one wishes to be protected against. We also find concrete cases where the complexity of or susceptibility to control varies dramatically based on the choice among natural tie-handling rules.

#index 1269375
#* Towards model-based diagnosis of coordination failures
#@ Meir Kalech;Gal A. Kaminka
#t 2005
#c 10
#% 3460
#% 21137
#% 21138
#% 90990
#% 121397
#% 334633
#% 517111
#% 643161
#% 1279266
#! With increasing deployment of multi-agent and distributed systems, there is an increasing need for failure diagnosis systems. While successfully tackling key challenges in multi-agent settings, model-based diagnosis has left open the diagnosis of coordination failures, where failures often lie in the boundaries between agents, and thus the inputs to the model--with which the diagnoser simulates the system to detect discrepancies--are not known. However, it is possible to diagnose such failures using a model of the coordination between agents. This paper formalizes model-based coordination diagnosis, using two coordination primitives (concurrence and mutual exclusion). We define the consistency-based and abductive diagnosis problems within this formalization, and show that both are NP-Hard by mapping them to other known problems.

#index 1269376
#* Flexible teamwork in behavior-based robots
#@ Gal A. Kaminka;Inna Frenkel
#t 2005
#c 10
#% 581033
#% 643135
#! A key challenge in deploying teams of robots in real-world applications is to automate the control of teamwork, such that the designer can focus on the taskwork. Existing teamwork architectures seeking to address this challenge are monolithic, in that they commit to interaction protocols at the architectural level, and do not allow the designer to mix and match protocols for a given task. We present BITE, a behavior-based teamwork architecture that automates collaboration in physical robots, in a distributed fashion. BITE separates task behaviors that control a robot's interaction with its task, from interaction behaviors that control a robot's interaction with its teammates. This distinction provides for flexibility and modularity in terms of the interactions used by teammates to collaborate effectively. It also allows BITE to synthesize and significantly extend existing teamwork architectures. BITE also incorporates key lessons learned in applying multi-agent teamwork architectures in physical robot teams. We present empirical results from experiments with teams of Sony AIBO robots executing BITE, and discuss the lessons learned.

#index 1269377
#* Coordinating agile systems through the model-based execution of temporal plans
#@ Thomas Léauté;Brian C. Williams
#t 2005
#c 10
#% 107137
#% 1396030
#% 1781501
#! Agile autonomous systems are emerging, such as unmanned aerial vehicles (UAVs), that must robustly perform tightly coordinated time-critical missions; for example, military surveillance or search-and-rescue scenarios. In the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness. We address the challenge of extending plan execution to underactuated systems that are controlled indirectly through the setting of continuous state variables. Our solution is a novel model-based executive that takes as input a temporally flexible state plan, specifying intended state evolutions, and dynamically generates a near-optimal control sequence. To achieve optimality and safety, the executive plans into the future, framing planning as a disjunctive programming problem. To achieve robustness to disturbances and tractability, planning is folded within a receding horizon, continuous planning framework. Key to performance is a problem reduction method based on constraint pruning. We benchmark performance through a suite of UAV scenarios using a hardware-in-the-loop testbed.

#index 1269378
#* Controversial users demand local trust metrics: an experimental study on Epinions.com community
#@ Paolo Massa;Paolo Avesani
#t 2005
#c 10
#% 316798
#% 523051
#% 753425
#% 754098
#% 795262
#% 810734
#! In today's connected world it is possible and very common to interact with unknown people, whose reliability is unknown. Trust Metrics are a recently proposed technique for answering questions such as "Should I trust this user?". However, most of the current research assumes that every user has a global quality score and that the goal of the technique is just to predict this correct value. We show, on data from a real and large user community, Epinions.com, that such an assumption is not realistic because there is a significant portion of what we call controversial users, users who are trusted and distrusted by many. A global agreement about the trustworthiness value of these users cannot exist. We argue, using computational experiments, that the existence of controversial users (a normal phenomena in societies) demands Local Trust Metrics, techniques able to predict the trustworthiness of an user in a personalized way, depending on the very personal view of the judging user.

#index 1269379
#* Modeling human behavior for virtual training systems
#@ Yohei Murakami;Yuki Sugimoto;Toru Ishida
#t 2005
#c 10
#% 330268
#% 330273
#% 378950
#% 378952
#% 379040
#% 379041
#% 438672
#% 643125
#% 752994
#% 773221
#! Constructing highly realistic agents is essential if agents are to be employed in virtual training systems. In training for collaboration based on face-to-face interaction, the generation of emotional expressions is one key. In training for guidance based on one-to-many interaction such as direction giving for evacuations, emotional expressions must be supplemented by diverse agent behaviors to make the training realistic. To reproduce diverse behavior, we characterize agents by using a various combinations of operation rules instantiated by the user operating the agent. To accomplish this goal, we introduce a user modeling method based on participatory simulations. These simulations enable us to acquire information observed by each user in the simulation and the operating history. Using these data and the domain knowledge including known operation rules, we can generate an explanation for each behavior. Moreover, the application of hypothetical reasoning, which offers consistent selection of hypotheses, to the generation of explanations allows us to use otherwise incompatible operation rules as domain knowledge. In order to validate the proposed modeling method, we apply it to the acquisition of an evacuee's model in a fire-drill experiment. We successfully acquire a subject's model corresponding to the results of an interview with the subject.

#index 1269380
#* Networked distributed POMDPs: a synthesis of distributed constraint optimization and POMDPs
#@ Ranjit Nair;Pradeep Varakantham;Milind Tambe;Makoto Yokoo
#t 2005
#c 10
#% 527987
#% 528006
#% 578694
#% 643099
#% 644201
#% 719917
#% 773304
#% 1250230
#% 1272045
#% 1272052
#% 1279314
#! In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present two novel algorithms for ND-POMDPs: a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal.

#index 1269381
#* New approaches to optimization and utility elicitation in autonomic computing
#@ Relu Patrascu;Craig Boutilier;Rajarshi Das;Jeffrey O. Kephart;Gerald Tesauro;William E. Walsh
#t 2005
#c 10
#% 423108
#% 452359
#% 529348
#% 1279257
#% 1672988
#! Autonomic (self-managing) computing systems face the critical problem of resource allocation to different computing elements. Adopting a recent model, we view the problem of provisioning resources as involving utility elicitation and optimization to allocate resources given imprecise utility information. In this paper, we propose a new algorithm for regret-based optimization that performs significantly faster than that proposed in earlier work. We also explore new regret-based elicitation heuristics that are able to find near-optimal allocations while requiring a very small amount of utility information from the distributed computing elements. Since regret-computation is intensive, we compare these to the more tractable Nelder-Mead optimization technique w.r.t. amount of utility information required.

#index 1269382
#* An ecological approach to agent population management
#@ Maxim D. Peysakhov;Robert N. Lass;William C. Regli;Moshe Kam
#t 2005
#c 10
#% 258420
#% 279818
#% 369236
#% 773224
#% 784395
#% 1279325
#% 1707090
#! The problem of maintaining a desired number of mobile agents on a network is not trivial, especially if what is required is a completely decentralized solution. Decentralized control makes a system more robust and less susceptible to partial failures. The problem of agent population management is exacerbated on wireless ad hoc networks where host mobility can result in significant changes in the network size and topology. System stability is also of critical importance. This paper analyzes the stability of a previously proposed ecology-inspired approach to agent population management, and proposes improvements. The stability of the new ecology based strategy is proved theoretically, and the conclusions are verified with a set of experiments.

#index 1269383
#* Distributing coalitional value calculations among cooperative agents
#@ Talal Rahwan;Nicholas R. Jennings
#t 2005
#c 10
#% 252199
#% 284645
#% 529357
#% 773258
#! The process of forming coalitions of software agents generally requires calculating a value for every possible coalition which indicates how beneficial that coalition would be if it was formed. Now, since the number of possible coalitions increases exponentially with the number of agents involved, having one agent calculate all the values is inefficient. Given this, we present a novel algorithm for distributing this calculation among agents in cooperative environments. Specifically, by using our algorithm, each agent is assigned some part of the calculation such that the agents' shares are exhaustive and disjoint. Moreover, the algorithm is decentralized, requires no communication between the agents, and has minimal memory requirements. To evaluate the effectiveness of our algorithm we compare it with the only other algorithm available in the literature (due to Shehory and Kraus). This shows that for the case of 25 agents, the distribution process of our algorithm took 0.00037% of the time, the values were calculated using 0.000006% of the memory, the calculation redundancy was reduced from 477826101 to 0, and the total number of bytes sent between the agents dropped from 674047872 to 0 (note that for larger numbers of agents, these improvements become exponentially better).

#index 1269384
#* Cooperative exploration in the electronic marketplace
#@ David Sarne;Sarit Kraus
#t 2005
#c 10
#% 252199
#% 253310
#% 284645
#% 334648
#% 431562
#% 659838
#% 725207
#! In this paper we study search strategies of agents that represent buyer agents' coalitions in electronic marketplaces. The representative agents operate in environments where numerous potential complex opportunities can be found. Each opportunity is associated with several different terms and conditions thus differing from other opportunities by its value for the coalition. Given a search cost, the goal of the representative agent is to find the best set of opportunities which fulfills the coalition's demands with the maximum overall utility, to be divided among the coalition members. Given the option of side-payments, this strategy will always be preferred by all coalition members (thus no conflict of interests), regardless of the coalition's payoff division protocol. We analyze the incentive to form such coalitions and extract the optimal search strategy for their representative agents, with a distinction between operating in B2C and C2C markets. Based on our findings we suggest efficient algorithms to be used by the representative agents for calculating a strategy that maximizes their expected utilities. A computational-based example is given, illustrating the achieved performance as a function of the coalition's members' heterogeneity level.

#index 1269385
#* Solving the auction-based task allocation problem in an open environment
#@ David Sarne;Sarit Kraus
#t 2005
#c 10
#% 252199
#% 306505
#% 496266
#% 777669
#! In this paper we analyze the process of allocating tasks to self-interested agents in uncertain changing open environments. The allocator in our model is responsible for the performance of dynamically arriving tasks using a second price reverse auction as the allocation protocol. Since the agents are self-interested (i.e. each agent attempts to maximize its own revenue), previous models concerning cooperative agents aiming for a joint goal are not applicable. Thus the main challenge is to identify a set of equilibrium strategies - a stable solution where no agent can benefit from changing its strategy given the other agents' strategies - for any specific environmental settings. We formulate the model and discuss the difficulty in extracting the agents' equilibrium strategies directly from the model's equations. Consequently we propose an efficient algorithm to accurately approximate the agents' equilibrium strategies. A comparative illustration through simulation of the system performance in a closed and open environments is given, emphasizing the advantage of the allocator operating in the latter environment, reaching results close to those obtained by a central enforceable allocation.

#index 1269386
#* Profit sharing auction
#@ Sandip Sen;Teddy Candale;Susnata Basak
#t 2005
#c 10
#% 124691
#% 252811
#% 302061
#% 342124
#% 713046
#% 773199
#% 786984
#% 1271997
#! Auctions are a class of multi-party negotiation protocols. Classical auctions try to maximize social welfare by selecting the highest bidder as the winner. If bidders are rational, this ensures that the sum of profits for all bidders and the seller is maximized. In all such auctions, however, only the winner and the seller make any profit. We believe that "social welfare distribution" is a desired goal of any multiparty protocol. In the context of auctions, this goal translates into a rather radical proposal of profit sharing between all bidders and the seller. We propose a Profit Sharing Auction (PSA) where a part of the selling price paid by the winner is paid back to the bidders. The obvious criticism of this mechanism is the incentive for the seller to share its profit with nonwinning bidders. We claim that this loss can be compensated by attracting more bidders to such an auction, resulting in an associated increase in selling price. We run several sets of experiments where equivalent items are concurrently sold at a First Price Sealed Bid, a Vickrey, and a PSA auction. A population of learning bidders repeatedly choose to go to one of these auctions based on their valuation for the good being auctioned and their learned estimates of profits from these auctions. Results show that sellers make more or equivalent profits by using PSA as compared to the classical auctions. Additionally, PSA always attracts more bidders, which might create auxiliary revenue streams, and a desirable lower variability in selling prices. Interestingly then, a rational seller has the incentive to share profits and offer an auction like PSA which maximizes and distributes social welfare.

#index 1269387
#* OAR: a formal framework for multi-agent negotiation
#@ Jiaying Shen;Ingo Weber;Victor Lesser
#t 2005
#c 10
#% 334525
#% 445540
#% 566719
#% 643115
#% 724034
#% 773256
#% 823999
#! In Multi-Agent systems, agents often need to make decisions about how to interact with each other when negotiating over task allocation. In this paper, we present OAR, a formal framework to address the question of how the agents should interact in an evolving environment in order to achieve their different goals. The traditional categorization of self-interested and cooperative agents is unified by adopting a utility view. We illustrate mathematically that the degree of cooperativeness of an agent and the degree of its selfdirectness are not directly related. We also show how OAR can be used to evaluate different negotiation strategies and to develop distributed mechanisms that optimize the performance dynamically. This research demonstrates that sophisticated probabilistic modeling can be used to understand the behaviors of a system with complex agent interactions.

#index 1269388
#* Tool use for autonomous agents
#@ Robert St. Amant;Alexander B. Wood
#t 2005
#c 10
#% 126364
#% 199007
#% 400110
#% 432421
#% 1272307
#! The intelligent use of tools is a general and important human competence that AI research has not yet examined in depth. Other fields have studied the topic, however, with results we can compile into a broad characterization of habile (tool-using) agents. In this paper we give an overview of research on the use of physical tools, using this information to motivate the development of artificial habile agents. Specifically, we describe how research goals and methods in animal cognition overlap with those in artificial intelligence. We argue that analysis of activities of tool-using agents offers an informative way to evaluate intelligence.

#index 1269389
#* Observation-based model for BDI-agents
#@ Kaile Su;Abdul Sattar;Kewen Wang;Xiangyu Luo;Guido Governatori;Vineet Padmanabhan
#t 2005
#c 10
#% 3873
#% 68239
#% 131283
#% 136356
#% 188086
#% 214197
#% 379175
#% 413871
#% 431514
#% 431526
#% 480681
#% 643130
#% 659831
#% 751235
#% 766972
#% 773266
#% 1250134
#! We present a new computational model of BDI-agents, called the observation-based BDI-model. The key point of this BDI-model is to express agents' beliefs, desires and intentions as a set of runs (computing paths), which is exactly a system in the interpreted system model, a well-known agent model due to Halpern and his colleagues. Our BDI-model is computationally grounded in that we are able to associate the BDI-agent model with a computer program, and formulas, involving agents' beliefs, desires (goals) and intentions, can be understood as properties of program computations. We present a sound and complete proof system with respect to our BDI-model and explore how symbolic model checking techniques can be applied to model checking BDI-agents. In order to make our BDI-model more flexible and practically realistic, we generalize it so that agents can have multiple sources of beliefs, goals and intentions.

#index 1269390
#* Stable service placement on dynamic peer-to-peer networks: a heuristic for the distributed k-center problem
#@ Evan A. Sultanik;William C. Regli
#t 2005
#c 10
#% 349953
#% 643220
#% 773233
#% 787211
#% 824077
#% 1669632
#% 1711399
#! The proliferation of wireless networks has underscored the need for systems capable of coping with sporadic network connectivity. The restriction of communication to neighboring hosts makes determining the global state especially difficult, if not impractical. This paper addresses the problem of coordinating the positions of an arbitrary number of services, encapsulated by mobile agents, in a dynamic peer-to-peer network. The agents' collective goal is to minimize the distance between hosts and services, even if the topology is changing constantly. We propose a distributed algorithm to efficiently calculate the stationary distribution of the network. This can be used as a hill climbing heuristic for agents to find near-optimal locations at which to provide services. Finally, we show that the agent-based hill climbing approach is temporally-stable relative to the instantaneous optimum.

#index 1269391
#* Analogical learning of visual/conceptual relationships in sketches
#@ Kenneth D. Forbus;Jeffrey Usher;Emmett Tomai
#t 2005
#c 10
#% 1116
#% 136370
#% 168280
#% 175380
#% 239643
#% 342760
#% 359837
#% 529803
#% 529817
#% 558111
#% 578785
#% 694777
#% 1269392
#% 1289199
#! This paper explores the use of analogy to learn about properties of sketches. Sketches often convey conceptual relationships between entities via the visual relationships between their depictions in the sketch. Understanding these conventions is an important part of adapting to a user. This paper describes how learning by accumulating examples can be used to make suggestions about such relationships in new sketches. We describe how sketches are being used in Companion Cognitive Systems to illustrate one context in which this problem arises. We describe how existing cognitive simulations of analogical matching and retrieval are used to generate suggestions for new sketches based on analogies with prior sketches. Two experiments provide evidence as to the accuracy and coverage of this technique.

#index 1269392
#* Solving everyday physical reasoning problems by analogy using sketches
#@ Matthew Klenk;Ken Forbus;Emmett Tomai;Hyeonkyeong Kim;Brian Kyckelhahn
#t 2005
#c 10
#% 1116
#% 46271
#% 109848
#% 175380
#% 179983
#% 303937
#% 342760
#% 405391
#% 529803
#% 558111
#% 694777
#! Understanding common sense reasoning about the physical world is one of the goals of qualitative reasoning research. This paper describes how we combine qualitative mechanics and analogy to solve everyday physical reasoning problems posed as sketches. The problems are drawn from the Bennett Mechanical Comprehension Test, which is used to evaluate technician candidates. We discuss sketch annotations, which define conceptual quantities in terms of visual measurements, how modeling decisions are made by analogy, and how analogy can be used to frame comparative analysis problems. Experimental results support the plausibility of this approach.

#index 1269393
#* Complexity-guided case discovery for case based reasoning
#@ Stewart Massie;Susan Craw;Nirmalie Wiratunga
#t 2005
#c 10
#% 307100
#% 420138
#% 477947
#% 520745
#% 1271849
#% 1275276
#% 1389786
#! The distribution of cases in the case base is critical to the performance of a Case Based Reasoning system. The case author is given little support in the positioning of new cases during the development stage of a case base. In this paper we argue that classification boundaries represent important regions of the problem space. They are used to identify locations where new cases should be acquired. We introduce two complexity-guided algorithms which use a local complexity measure and boundary identification techniques to actively discover cases close to boundaries. The ability of these algorithms to discover new cases that significantly improve the accuracy of case bases is demonstrated on five public domain classification datasets.

#index 1269394
#* Interactive knowledge validation and query refinement in CBR
#@ Monica H. Ou;Geoff A. W. West;Mihai Lazarescu;Chris Clay
#t 2005
#c 10
#% 136350
#% 258186
#% 290482
#% 384416
#% 466154
#% 466336
#% 723412
#% 853020
#% 1718291
#! In most case-based reasoning (CBR) systems there has been little research done on validating new knowledge, specifically on how previous knowledge differs from current knowledge as a result of conceptual change. This paper proposes two methods that enable the domain expert, who is nonexpert in artificial intelligence (AI), to interactively supervise the knowledge validation process in a CBR system, and to enable dynamic updating of the system, to provide the best diagnostic questions. The first method is based on formal concept analysis which involves a graphical representation and comparison of the concepts, and a summary description highlighting the conceptual differences. We propose a dissimilarity metric for measuring the degree of variation between the previous and current concepts when a new case is added to the knowledge base. The second method involves determining unexpected classification-based association rules to form critical questions as the knowledge base gets updated.

#index 1269395
#* Competence driven case-base mining
#@ Rong Pan;Qiang Yang;Jeffrey Junfeng Pan;Lei Li
#t 2005
#c 10
#% 168280
#% 258186
#% 266426
#% 494248
#% 494577
#% 496417
#% 516781
#% 722887
#% 729437
#% 1275276
#% 1478834
#! We present a novel algorithm for extracting a high-quality case base from raw data while preserving and sometimes improving the competence of case-based reasoning. We extend the framework of Smyth and Keane's case-deletion policy with two additional features. First, we build a case base using a statistical distribution that is mined from the input data so that the case-base competence can be preserved or even increased for future problems. Second, we introduce a nonlinear transformation of the data set so that the case-base sizes can be further reduced while ensuring that the competence be preserved and even increased. We show that Smyth and Keane's deletion-based algorithm is sensitive to noisy cases, and that our solution solves this problem more satisfactorily. We show the theoretical foundation and empirical evaluation on several data sets.

#index 1269396
#* A domain-independent system for case-based task decomposition without domain theories
#@ Ke Xu;Hector Muñoz-Avila
#t 2005
#c 10
#% 192708
#% 258186
#% 362441
#% 490597
#% 490927
#% 495942
#% 566460
#% 1250313
#% 1275276
#% 1289285
#! We propose using domain-independent task decomposition techniques for situations in which cases are the sole or the main source for domain knowledge. Our work is motivated by project planning domains, where hierarchical cases are readily available, but neither a planning domain theory nor case adaptation knowledge is available. We present DInCaD (Domain-Independent System for Case-Based Task Decomposition), a system that encompasses case retrieval, refinement, and reuse, following from the idea of reusing generalized cases to solve new problems. DInCaD consists of a case refinement procedure that reduces case over-generalization, and a similarity criterion that takes advantage of the refinement to improve case retrieval precision. We will analyze the properties of the system, and present an empirical evaluation.

#index 1269397
#* Mechanism design for single-value domains
#@ Moshe Babaioff;Ron Lavi;Elan Pavlov
#t 2005
#c 10
#% 413867
#% 578713
#% 656791
#% 723935
#% 805726
#% 806744
#% 808388
#! In "Single-Value domains", each agent has the same private value for all desired outcomes. We formalize this notion and give new examples for such domains. including a "SAT domain" and a "single-value combinatorial auctions" domain. We study two informational models: where the set of desired outcomes is public information (the "known" case). and where it is private information (the "unknown" case). Under the "known" assumption, we present several truthful approximation mechanisms. Additionally, we suggest a general technique to convert any bitonic approximation algorithm for an unweighted domain (where agent values are either zero or one) to a truthful mechanism, with only a small approximation loss. In contrast, we show that even positive results from the "unknown single minded combinatorial auctions" literature fail to extend to the "unknown" single-value case. We give a characterization of truthfulness in this case, demonstrating that the difference is subtle and surprising.

#index 1269398
#* Combinatorial auctions with k-wise dependent valuations
#@ Vincent Conitzer;Tuomas Sandholm;Paolo Santi
#t 2005
#c 10
#% 267752
#% 301588
#% 314918
#% 314920
#% 341943
#% 345429
#% 378898
#% 413867
#% 529664
#% 578711
#% 580529
#% 580533
#% 590624
#% 754153
#% 754177
#% 763719
#% 773226
#% 781210
#% 823925
#% 912341
#% 1250152
#! We analyze the computational and communication complexity of combinatorial auctions from a new perspective: the degree of interdependency between the items for sale in the bidders' preferences. Denoting by Gk the class of valuations displaying up to k-wise dependencies, we consider the hierarchy G1 ⊂ G2 ⊂ ... ⊂ Gm, where m is the number of items for sale. We show that the minimum non-trivial degree of interdependency (2-wise dependency) is sufficient to render NP-hard the problem of computing the optimal allocation (but we also exhibit a restricted class of such valuations for which computing the optimal allocation is easy). On the other hand, bidders' preferences can be communicated efficiently (i.e., exchanging a polynomial amount of information) as long as the interdependencies between items are limited to sets of cardinality up to k, where k is an arbitrary constant. The amount of communication required to transmit the bidders' preferences becomes super-polynomial (under the assumption that only value queries are allowed) when interdependencies occur between sets of cardinality g(m), where g(m) is an arbitrary function such that g(m) → ∞ as m → ∞. We also consider approximate elicitation, in which the auctioneer learns, asking polynomially many value queries, an approximation of the bidders' actual preferences.

#index 1269399
#* Expressive negotiation in settings with externalities
#@ Vincent Conitzer;Thomas Sandholm
#t 2005
#c 10
#% 267752
#% 314944
#% 345429
#% 378898
#% 397940
#% 413867
#% 453488
#% 496094
#% 578713
#% 631040
#% 754139
#% 1289307
#! In recent years, certain formalizations of combinatorial negotiation settings, most notably combinatorial auctions, have become an important research topic in the AI community. A pervasive assumption has been that of no externalities: the agents deciding on a variable (such as whether a trade takes place between them) are the only ones affected by how this variable is set. To date, there has been no widely studied formalization of combinatorial negotiation settings with externalities. In this paper, we introduce such a formalization. We show that in a number of key special cases, it is NP-complete to find a feasible nontrivial solution (and therefore the maximum social welfare is completely inapproximable). However, for one important special case, we give an algorithm which converges to the solution with the maximal concession by each agent (in a linear number of rounds for utility functions that decompose into piecewise constant functions). Maximizing social welfare, however, remains NP-complete even in this setting. We also demonstrate a special case which can be solved in polynomial time by linear programming.

#index 1269400
#* A new strategy-proof greedy-allocation combinatorial auction protocol and its extension to open ascending auction protocol
#@ Takayuki Ito;Makoto Yokoo;Atsushi Iwasaki;Shigeo Matsubara
#t 2005
#c 10
#% 496094
#% 529651
#% 580532
#% 631040
#% 723935
#% 806738
#% 870914
#% 1279318
#! This paper proposes a new combinatorial auction protocol called Average-Max-Minimal-Bundle (AM-MB) protocol. The characteristics of the AM-MB protocol are as follows: (i) it is strategyproof, i.e., truth-telling is a dominant strategy, (ii) the computational overhead is very low, since it allocates bundles greedily thereby avoiding an explicit combinatorial optimization problem, and (iii) it can obtain higher social surplus and revenue than can the Max-Minimal-Bundle (M-MB) protocol, which also satisfies (i) and (ii). Furthermore, this paper extends the AM-MB protocol to an open ascending-price protocol in which straightforward bidding is an ex-post Nash equilibrium.

#index 1269401
#* Approximating revenue-maximizing combinatorial auctions
#@ Anton Likhodedov;Tuomas Sandholm
#t 2005
#c 10
#% 267752
#% 325589
#% 578713
#% 723935
#% 754177
#% 813836
#% 1250155
#! Designing revenue-maximizing combinatorial auctions (CAs) is a recognized open problem in mechanism design. It is unsolved even for two bidders and two items for sale. Rather than attempting to characterize the optimal auction, we focus on designing approximations (suboptimal auction mechanisms which yield high revenue). Our approximations belong to the family of virtual valuations combinatorial auctions (VVCA). VVCA is a Vickrey-Clarke-Groves (VCG) mechanism run on virtual valuations that are linear transformations of the bidders' real valuations. We pursue two approaches to constructing approximately optimal CAs. The first is to construct a VVCA with worst-case and average-case performance guarantees. We give a logarithmic approximation auction for basic important special cases of the problem: 1) limited supply of items on sale with additive valuations and 2) unlimited supply. The second approach is to search the parameter space of VVCAs in order to obtain high-revenue mechanisms for the general problem. We introduce a series of increasingly sophisticated algorithms that use economic insights to guide the search and thus reduce the computational complexity. Our experiments demonstrate that in many cases these algorithms perform almost as well as the optimal VVCA, yield a substantial increase in revenue over the VCG mechanism and drastically outperform the straightforward algorithms in run-time.

#index 1269402
#* The achilles' heel of QBF
#@ Carlos Ansotegui;Carla P. Gomes;Bart Selman
#t 2005
#c 10
#% 131357
#% 183640
#% 420766
#% 517569
#% 535315
#% 1675277
#% 1675281
#! In recent years we have seen significant progress in the area of Boolean satisfiability (SAT) solving and its applications. As a new challenge, the community is now moving to investigate whether similar advances can be made in the use of Quantified Boolean Formulas (QBF). QBF provides a natural framework for capturing problem solving and planning in multi-agent settings. However, contrarily to single-agent planning, which can be effectively formulated as SAT, we show that a QBF approach to planning in a multi-agent setting leads to significant unexpected computational difficulties. We identify as a key difficulty of the QBF approach the fact that QBF solvers often end up exploring a much larger search space than the natural search space of the original problem. This is in contrast to the experience with SAT approaches. We also show how one can alleviate these problems by introducing two special QBF formulations and a new QBF solution strategy. We present experiments that show the effectiveness of our approach in terms of a significant improvement in performance compared to earlier work in this area. Our work also provides a general methodology for formulating adversarial scenarios in QBF.

#index 1269403
#* Combining stochastic and greedy search in hybrid estimation
#@ Lars Blackmore;Stanislav Funiak;Brian Williams
#t 2005
#c 10
#% 518899
#% 528169
#% 528327
#% 529185
#! Techniques for robot monitoring and diagnosis have been developed that perform state estimation using probabilistic hybrid discrete/continuous models. Exact inference in hybrid dynamic systems is, in general, intractable. Approximate algorithms are based on either 1) greedy search, as in the case of k-best enumeration or 2) stochastic search, as in the case of Rao-Blackwellised Particle Filtering (RBPF). In this paper we propose a new method for hybrid state estimation. The key insight is that stochastic and greedy search methods, taken together, are often particularly effective in practice. The new method combines the stochastic methods of RBPF with the greedy search of k-best in order to create a method that is effective for a wider range of estimation problems than the individual methods alone. We demonstrate this robustness on a simulated acrobatic robot, and show that this benefit comes at only a small performance penalty.

#index 1269404
#* Propositional fragments for knowledge compilation and quantified boolean formulae
#@ Sylvie Coste-Marquis;Daniel Le Berre;Florian Letombe;Pierre Marquis
#t 2005
#c 10
#% 3873
#% 183640
#% 204396
#% 266109
#% 335852
#% 342378
#% 442363
#% 495784
#% 495922
#% 517569
#% 529515
#% 535315
#% 544788
#% 569116
#% 600496
#% 770539
#% 772064
#% 785483
#% 1272349
#% 1272399
#% 1273692
#% 1289185
#% 1499541
#% 1705009
#! Several propositional fragments have been considered so far as target languages for knowledge compilation and used for improving computational tasks from major AI areas (like inference, diagnosis and planning); among them are the (quite influential) ordered binary decision diagrams, prime implicates, prime implicants, "formulae" in decomposable negation normal form. On the other hand, the validity problem QBF for Quantified Boolean Formulae (QBF) has been acknowledged for the past few years as an important issue for AI, and many solvers have been designed for this purpose. In this paper, the complexity of restrictions of QBF obtained by imposing the matrix of the input QBF to belong to such propositional fragments is identified. Both tractability and intractability results (PSPACE-completeness) are obtained.

#index 1269405
#* Axiom schemata as metalevel axioms: model theory
#@ Timothy L. Hinrichs;Michael R. Genesereth
#t 2005
#c 10
#% 1461
#% 3546
#% 26722
#% 90829
#% 1907872
#! Logicians frequently use axiom schemata to encode (potentially infinite) sets of sentences with particular syntactic form. In this paper we examine a first-order language in which it is possible to write expressions that both describe sentences and assert the truth of the sentences so described. The effect of adding such expressions to a knowledge base is the same as directly including the set of described sentences.

#index 1269406
#* On compiling system models for faster and more scalable diagnosis
#@ Jinbo Huang;Adnan Darwiche
#t 2005
#c 10
#% 3873
#% 154456
#% 342378
#% 427657
#% 542255
#% 788050
#% 1272329
#% 1272349
#% 1675285
#! Knowledge compilation is one of the more traditional approaches to model-based diagnosis, where a compiled system model is obtained in an off-line phase, and then used to efficiently answer diagnostic queries on-line. The choice of a suitable representation for the compiled model is critical to the success of this approach, and two of the main proposals have been Decomposable Negation Normal Form (DNNF) and Ordered Binary Decision Diagram (OBDD). The contribution of this paper is twofold. First, we show that in the current state of the art, DNNF dominates OBDD in efficiency and scalability for some typical diagnostic tasks. This result is based on a step-by-step comparison of the complexities of diagnostic algorithms for DNNF and OBDD, together with a known succinctness relation between the two representations. Second, we present a tool for model-based diagnosis, which is based on a state-of-the-art DNNF compiler and our implementations of DNNF diagnostic algorithms. We demonstrate the efficiency of this tool against recent results reported on diagnosis using OBDD.

#index 1269407
#* A discourse planning approach to cinematic camera control for narratives in virtual environments
#@ Arnav Jhala;R. Michael Young
#t 2005
#c 10
#% 129244
#% 145399
#% 302052
#% 319244
#% 400098
#% 678368
#% 748514
#% 1499487
#! As the complexity of narrative-based virtual environments grows, the need for effective communication of information to the users of these systems increase. Effective camera control for narrative-oriented virtual worlds involves decision making at three different levels: choosing cinematic geometric composition, choosing the best camera parameters for conveying affective information, and choosing camera shots and transitions to maintain thetorical coherence. We propose a camera planning system that mirrors the film production pipeline; we describe our formalization of film idioms used to communicate affective information. Our representation of idioms captures their hierarchical nature, represents the causal motivation for selection of shots, and provides a way for the system designer to specify the ranking of candidate shot sequences.

#index 1269408
#* Dependency-directed reconsideration belief base optimization for truth maintenance systems
#@ Frances L. Johnson;Stuart C. Shapiro
#t 2005
#c 10
#% 3460
#% 36784
#% 100157
#% 154456
#% 208205
#% 631048
#% 1250245
#% 1273685
#! We define reconsideration, a non-prioritized belief change operation on a finite set of base beliefs. Reconsideration is a hindsight belief change repair that eliminates negative effects caused by the order of previously executed belief change operations. Beliefs that had previously been removed are returned to the base if there no longer are valid reasons for their removal. This might result in less preferred beliefs being removed, and additional beliefs being returned. The end product is an optimization of the belief base, converting the results of a series of revisions to the very base that would have resulted from a batch revision performed after all base beliefs were entered/added. Reconsideration can be done by examining the entire set of all base beliefs (both currently believed and retracted) -- or, if the believed base is consistent, by examining all retracted beliefs for possible return. This, however, is computationally expensive. We present a more efficient, TMS-friendly algorithm, dependency-directed reconsideration (DDR), which can produce the same results by examining only a dynamically determined subset of base beliefs that are actually affected by changes made since the last base optimization process. DDR is an efficient, anytime, belief base optimizing algorithm that eliminates operation order effects.

#index 1269409
#* Diagnosis as approximate belief state enumeration for probabilistic concurrent constraint automata
#@ Oliver B. Martin;Brian C. Williams;Michel D. Ingham
#t 2005
#c 10
#% 21138
#% 529667
#% 1476265
#! As autonomous spacecraft and other robotic systems grow increasingly complex, there is a pressing need for capabilities that more accurately monitor and diagnose system state while maintaining reactivity. Mode estimation addresses this problem by reasoning over declarative models of the physical plant, represented as a factored variant of Hidden Markov Models (HMMs), called Probabilistic Concurrent Constraint Automata (PCCA). Previous mode estimation approaches track a set of most likely PCCA state trajectories, enumerating them in order of trajectory probability. Although Best-First Trajectory Enumeration (BFTE) is efficient, ignoring the additional trajectories that lead to the same state can significantly underestimate the true state probability and result in misdiagnosis. This paper introduces an innovative belief approximation technique, called Best-First Belief State Enumeration (BFBSE), that addresses this limitation by computing estimate probabilities directly from the HMM belief state update equations. Theoretical and empirical results show that BFBSE significantly increases estimator accuracy, uses less memory, and requires less computation time when enumerating a moderate number of estimates for the approximate belief state of subsystem sized models.

#index 1269410
#* Model-based monitoring and diagnosis of systems with software-extended behavior
#@ Tsoline Mikaelian;Brian C. Williams;Martin Sachenbacher
#t 2005
#c 10
#% 21138
#% 131859
#% 244097
#% 331899
#% 529667
#% 1275309
#% 1289227
#% 1476265
#! Model-based diagnosis has largely operated on hard-ware systems. However, in most complex systems today, hardware is augmented with software functions that influence the system's behavior. In this paper, hard-ware models are extended to include the behavior of associated embedded software, resulting in more comprehensive diagnoses. Prior work introduced probabilistic, hierarchical, constraint-based automata (PHCA) to allow the uniform and compact encoding of both hard-ware and software behavior. This paper focuses on PHCA-based monitoring and diagnosis to ensure the robustness of complex systems. We introduce a novel approach that frames diagnosis over a finite time horizon as a soft constraint optimization problem (COP), allowing us to leverage an extensive body of efficient solution methods for COPs. The solutions to the COP correspond to the most likely evolutions of the complex system. We demonstrate our approach on a vision-based rover navigation system, and models of the SPHERES and Earth Observing One spacecraft.

#index 1269411
#* Recommender systems: attack types and strategies
#@ Michael P. O'Mahony;Neil J. Hurley;Guénolé C. M. Silvestre
#t 2005
#c 10
#% 173879
#% 280852
#% 431273
#% 754097
#% 1650569
#! In the research to date, the performance of recommender systems has been extensively evaluated across various dimensions. Increasingly, the issue of robustness against malicious attack is receiving attention from the research community. In previous work, we have shown that knowledge of certain domain statistics is sufficient to allow successful attacks to be mounted against recommender systems. In this paper, we examine the extent of domain knowledge that is actually required and find that, even when little such knowledge is known, it remains possible to mount successful attacks.

#index 1269412
#* Compact propositional encoding of first-order theories
#@ Deepak Ramachandran;Eyal Amir
#t 2005
#c 10
#% 344481
#% 355076
#% 528171
#% 822227
#% 1250127
#% 1279353
#% 1476298
#! In this paper we present polynomial-time algorithms that translate First-Order Logic (FOL) theories to smaller propositional encodings than achievable before in polynomial time. For example, we can sometimes reduce the number of propositions to O(|P| + |C|), or O(|P|k ċ log |P|), for |P| predicates of arity k and |C| constant symbols. The guarantee depends on availability of some graphical structure in the FOL representation. Our algorithms accept all FOL theories, and preserve soundness and completeness (sometimes requiring the Domain Closure Assumption). Our experiments show significant speedup in inference with a SAT solver on real-world problems. Our results address a common approach that translates inference and decision problems that originate in FOL into propositional logic, later applying efficient SAT solvers. Standard translation techniques result in very large propositional encodings (O(|P||C|k) for predicates of arity k) that are often infeasible to solve. Our approach scales up inference for many objects, and has potential applications in planning, probabilistic reasoning, and formal verification.

#index 1269413
#* Identifying direct causal effects in linear models
#@ Jin Tian
#t 2005
#c 10
#% 297171
#% 578735
#% 578740
#% 1250135
#% 1650356
#! This paper deals with the problem of identifying direct causal effects in recursive linear structural equation models. Using techniques developed for graphical causal models, we show that a model can be decomposed into a set of submodels such that the identification problem can be solved independently in each submodel. We provide a new identification method that identifies causal effects by solving a set of algebraic equations.

#index 1269414
#* Old resolution meets modern SLS
#@ Anbulagan Anbulagan;Duc Nghia Pham;John Slaney;Abdul Sattar
#t 2005
#c 10
#% 160270
#% 283231
#% 288165
#% 288366
#% 529172
#% 535307
#% 564832
#% 578753
#% 1250144
#% 1250149
#% 1289194
#% 1478779
#% 1499515
#! Recent work on Stochastic Local Search (SLS) for the SAT and CSP domains has shown the importance of a dynamic (non-markovian) strategy for weighting clauses in order to escape from local minima. In this paper, we improve the performance of two best contemprorary clause weighting solvers, PAWS and SAPS, by integrating a propositional resolution procedure. We also extend the work to AdaptNovelty+, the best non-weighting SLS solver in the GSAT/WalkSAT series. One outcome is that our systems can solve some highly structured problems such as quasigroup existence and parity learning problems which were previously thought unsuitable for local search and which are completely out of reach of traditional solvers such as GSAT. Here we present empirical results showing that for a range of random and real-world benchmark problems, resolution-enhanced SLS solvers clearly outperform the alternatives.

#index 1269415
#* CSP properties for quantified constraints: definitions and complexity
#@ Lucas Bordeaux;Marco Cadoli;Toni Mancini
#t 2005
#c 10
#% 183640
#% 420766
#% 495922
#% 535134
#% 1250143
#% 1289372
#% 1698694
#! Quantified constraints and Quantified Boolean Formulae are typically much more difficult to reason with than classical constraints, because quantifier alternation makes the simple, classical notion of solution inappropriate. As a consequence, even such essential CSP properties as consistency or substitutability are not completely understood in the quantified case. In this paper, we show that most of the properties which are used by solvers for CSP can be generalized to Quantified CSP. We propose a systematic study of the relations which hold between these properties, as well as complexity results regarding the decision of these properties. Finally, and since these problems are typically intractable, we generalise the approach used in CSP and propose weakenings of these notions based on locality, which allow for a tractable, albeit incomplete detecting of these properties.

#index 1269416
#* Constrained decision diagrams
#@ Kenil C. K. Cheng;Roland H. C. Yap
#t 2005
#c 10
#% 3873
#% 263913
#% 292075
#% 535306
#% 564858
#! A general n-ary constraint is usually represented explicitly as a set of its solution tuples, which may need exponential space. In this paper, we introduce a new representation for general n-ary constraints called Constrained Decision Diagram (CDD). CDD generalizes BDD-style representations and the main feature is that it combines constraint reasoning/consistency techniques with a compact data structure. We present an application of CDD for recording all solutions of a conjunction of constraints. Instead of an explicit representation, we can implicitly encode the solutions by means of constraint propagation. Our experiments confirm the scalability and demonstrate that CDDs can drastically reduce the space needed over explicit and ZBDD representations.

#index 1269417
#* Finding diverse and similar solutions in constraint programming
#@ Emmanuel Hebrard;Brahim Hnich;Barry O'Sullivan;Toby Walsh
#t 2005
#c 10
#% 266125
#% 283228
#% 345434
#% 430192
#% 490928
#% 534984
#% 917906
#% 1289344
#% 1738929
#! It is useful in a wide range of situations to find solutions which are diverse (or similar) to each other. We therefore define a number of different classes of diversity and similarity problems. For example, what is the most diverse set of solutions of a constraint satisfaction problem with a given cardinality? We first determine the computational complexity of these problems. We then propose a number of practical solution methods, some of which use global constraints for enforcing diversity (or similarity) between solutions. Empirical evaluation on a number of problems show promising results.

#index 1269418
#* Weighted super solutions for constraint programs
#@ Alan Holland;Barry O'Sullivan
#t 2005
#c 10
#% 266125
#% 267752
#% 314925
#% 345429
#% 808377
#% 1738933
#! Super solutions to constraint programs guarantee that if a limited number of variables lose their values, repair solutions can be found by modifying a bounded number of assignments. However, in many application domains the classical super solutions framework is not expressive enough since it only reasons about the number of breaks in a solution and the number of changes that are necessary to find a repair. For example, in combinatorial auctions we may wish to guarantee that we can always find a repair solution whose revenue exceeds some threshold while limiting the cost associated with forming such a repair. In this paper we present the weighted super solution framework that involves two important extensions. Firstly, the set of variables that may lose their values is determined using a probabilistic approach enabling us to find repair solutions for assignments that are most likely to fail. Secondly, we include a mechanism for reasoning about the cost of repair. The proposed framework has been successfully used to find robust solutions to combinatorial auctions.

#index 1269419
#* Generating hard satisfiable formulas by hiding solutions deceptiveily
#@ Haixia Jia;Cristopher Moore;Doug Strain
#t 2005
#c 10
#% 17298
#% 75299
#% 338404
#% 347205
#% 460819
#% 529517
#% 580690
#% 769663
#% 1250139
#% 1289196
#! To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the "hidden" assignment A. Last year, (Achlioptas, Jia, & Moore 2004) proposed a problem generator that cancels this effect by hiding both A and its complement A. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time. Here we propose a new method to cancel the attraction to A, by choosing a clause with t 0 literals satisfied by A with probability proportional to qt for some q q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be true or false; we can even cause the formula to "deceptively" point away from A. We present theoretical and experimental results suggesting that these formulas are exponentially hard both for DPLL algorithms and for incomplete algorithms such as WalkSAT.

#index 1269420
#* Generalized nogoods in CSPs
#@ George Katsirelos;Fahiem Bacchus
#t 2005
#c 10
#% 68183
#% 116763
#% 160208
#% 283220
#% 336874
#% 427631
#% 534978
#% 535152
#% 535153
#% 1272049
#% 1478761
#! Although nogood learning in CSPs and clause learning in SAT are formally equivalent, nogood learning has not been as successful a technique in CSP solvers as clause learning has been for SAT solvers. We show that part of the reason for this discrepancy is that nogoods in CSPs (as standardly defined) are too restrictive. In this paper we demonstrate that these restrictions can be lifted so that a CSP solver can learn more general and powerful nogoods. Nogoods generalized in this manner yield a provably more powerful CSP solver. We also demonstrate how generalized nogoods facilitate learning useful nogoods from global constraints. Finally, we demonstrate empirically that generalized nogoods can yield significant improvements in performance.

#index 1269421
#* Neighborhood interchangeability and dynamic bundling for non-binary finite CSPs
#@ Anagh Lal;Berthe Y. Choueiry;Eugene C. Freuder
#t 2005
#c 10
#% 130206
#% 266125
#% 428344
#% 477295
#% 566349
#% 577151
#% 1279419
#% 1478759
#! Neighborhood Interchangeability (NI) identifies the equivalent values in the domain of a variable of a Constraint Satisfaction Problem (CSP) by considering only the constraints that directly apply to the variable. Freuder described an algorithm for efficiently computing NI values in binary CSPs. In this paper, we show that the generalization of this algorithm to non-binary CSPs is not straightforward, and introduce an efficient algorithm for computing. NI values in the presence of non-binary constraints. Further, we show how to interleave this mechanism with search for solving CSPs, thus yielding a dynamic bundling strategy. While the goal of dynamic bundling is to produce multiple robust solutions, we empirically show that it does not increase (but significantly decreases) the cost of search.

#index 1269422
#* A fast arc consistency algorithm for n-ary constraints
#@ Olivier Lhomme;Jean-Charles Régin
#t 2005
#c 10
#% 160389
#% 267574
#% 534637
#% 1289190
#% 1289191
#! The GAC-Scheme has become a popular general purpose algorithm for solving n-ary constraints, although it may scan an exponential number of supporting tuples. In this paper, we develop a major improvement of this scheme. When searching for a support, our new algorithm is able to skip over a number of tuples exponential in the arity of the constraint by exploiting knowledge about the current domains of the variables. We demonstrate the effectiveness of the method for large table constraints.

#index 1269423
#* Quick shaving
#@ Olivier Lhomme
#t 2005
#c 10
#% 3463
#% 160389
#% 265783
#% 266118
#% 1289190
#% 1289191
#! Arc-consistency plays such a key role in constraint programming for solving real life problems that it is almost the only algorithm used for reducing domains. There are a few specific problems for which a stronger form of propagation, often called shaving, is more efficient. Nevertheless, in many cases. shaving at each node of the search tree is not worth doing: arc-consistency filtering is much faster, and the additional domain reductions inferred by shaving do not pay off. In this paper, we propose a new kind of shaving called QuickShaving, which is guided by the search. As QuickShaving may infer some additional domain reductions compared with arc-consistency, it can improve the search for a solution by an exponential ratio. Moreover, the advantage of Quick Shaving is that in practice, unlike a standard form of shaving, the additional domain reductions deduced by QuickShaving come at a very low overhead compared with arc-consistency.

#index 1269424
#* DC-SSAT: a divide-and-conquer approach to solving stochastic satisfiability problems efficiently
#@ Stephen M. Majercik;Byron Boots
#t 2005
#c 10
#% 2837
#% 205391
#% 327779
#% 420743
#% 560768
#% 655324
#% 1289184
#% 1650297
#% 1672980
#! We present DC-SSAT, a sound and complete divide-and-conquer algorithm for solving stochastic satisfiability (SSAT) problems that outperforms the best existing algorithm for solving such problems (ZANDER) by several orders of magnitude with respect to both time and space. DC-SSAT achieves this performance by dividing the SSAT problem into subproblems based on the structure of the original instance, caching the viable partial assignments (VPAs) generated by solving these subproblems, and using these VPAs to construct the solution to the original problem. DC-SSAT does not save redundant VPAs and each VPA saved is necessary to construct the solution. Furthermore, DC-SSAT builds a solution that is already human-comprehensible, allowing it to avoid the costly solution rebuilding phase in ZANDER. As a result, DC-SSAT is able to solve problems using, typically, 1-2 orders of magnitude less space than ZANDER, allowing DC-SSAT to solve problems ZANDER cannot solve due to space constraints. And, in spite of its more parsimonious use of space, DCSSAT is typically 1-2 orders of magnitude faster than ZANDER. We describe the DC-SSAT algorithm and present empirical results comparing its performance to that of ZANDER on a set of SSAT problems.

#index 1269425
#* A constraint satisfaction approach to geospatial reasoning
#@ Martin Michalowski;Craig A. Knoblock
#t 2005
#c 10
#% 56471
#% 276398
#% 283220
#% 348577
#% 644201
#% 784291
#! The large number of data sources on the Internet can be used to augment and verify the accuracy of geospatial sources, such as gazetteers and annotated satellite imagery. Data sources such as satellite imagery, maps, gazetteers and vector data have been traditionally used in geographic infonnation systems (GIS), but nontraditional geospatial data, such as online phone books and property records are more difficult to relate to imagery. In this paper, we present a novel approach to combining extracted information from imagery, road vector data, and online data sources. We represent the problem of identifying buildings in satellite images as a constraint satisfing problem (CSP) and use constraint programming to solve it. We apply this technique to real-world data sources in EI Segundo, CA and our experimental evaluation shows how this approach can accurately identify buildings when provided with both traditional and nontraditional data sources.

#index 1269426
#* A framework for representing and solving NP search problems
#@ David G. Mitchell;Eugenia Ternovska
#t 2005
#c 10
#% 121603
#% 154317
#% 269391
#% 340744
#% 417651
#% 514777
#% 778122
#% 855345
#! NP search and decision problems occur widely in AI, and a number of general-purpose methods for solving them have been developed. The dominant approaches include propositional satisfiability (SAT), constraint satisfaction problems (CSP), and answer set programming (ASP). Here, we propose a declarative constraint programming framework which we believe combines many strengths of these approaches, while addressing weaknesses in each of them. We formalize our approach as a model extension problem, which is based on the classical notion of extension of a structure by new relations. A parameterized version of this problem captures NP. We discuss properties of the formal framework intended to support effective modelling, and prospects for effective solver design.

#index 1269427
#* Generation of hard non-clausal random satisfiability problems
#@ Juan A. Navarro;Andrei Voronkov
#t 2005
#c 10
#% 4599
#% 160270
#% 302719
#% 327779
#% 336874
#% 419990
#% 578753
#% 580690
#% 1289181
#% 1675296
#% 1675297
#% 1675298
#! We present the results from experiments with a new family of random formulas for the satisfiability problem. Our proposal is a generalisation of the random k-SAT model that introduces non-clausal formulas and exhibits interesting features such as experimentally observed sharp phase transition and the easy-hard-easy pattern. The experimental results provide some insights on how the use of different clausal translations can affect the performance of satisfiability solving algorithms. We also expect our model to provide diverse and challenging benchmarks for developers of SAT procedures for non-clausal formulas.

#index 1269428
#* Any time, complete algorithm for finding utilitarian optimal solutions to STPPs
#@ Bart Peintner;Martha E. Pollack
#t 2005
#c 10
#% 107137
#% 262737
#% 644201
#% 1250232
#% 1279395
#% 1289192
#! We present a simple greedy algorithm and a novel complete algorithm for finding utilitarian optimal solutions to Simple Temporal Problems with Preferences. Unlike previous algorithms, ours does not restrict preference functions to be convex. We present experimental results showing that (1) a single iteration of the greedy algorithm produces high-quality solutions, (2) multiple iterations, bounded by the square of the number of constraints, produce near-optimal solutions, and (3) our complete, memory-boundable algorithm has compelling anylime properties and outperforms a branch-and-bound algorithm.

#index 1269429
#* Superstabilizing, fault-containing distributed combinatorial optimization
#@ Adrian Petcu;Boi Faltings
#t 2005
#c 10
#% 159514
#% 223282
#% 299837
#% 321618
#% 644201
#% 773217
#% 1289393
#! Self stabilization in distributed systems is the ability of a system to respond to transient failures by eventually reaching a legal state, and maintaining it afterwards. This makes such systems particularly interesting because they can tolerate faults, and are able to cope with dynamic environments. We propose the first self stabilizing mechanism for multiagent combinatorial optimization, which works on general networks and stabilizes in a state corresponding to the optimal solution of the optimization problem. Our algorithm is based on dynamic programming, and requires a linear number of messages to find the optimal solution in the absence of faults. We show how our algorithm can be made super-stabilizing, in the sense that while transiting from one stable state to the next, our system preserves the assignments from the previous optimal state, until the new optimal solution is found. We offer equal bounds for the stabilization and the superstabilization time. Furthermore, we describe a general scheme for fault containment and fast response time upon low impact failures. Multiple, isolated failures are handled effectively. To show the merits of our approach we report on experiments with practically sized distributed meeting scheduling problems in a multiagent system.

#index 1269430
#* SAT-based versus CSP-based constraint weighting for satisfiability
#@ Duc Nghia Pham;John Thornton;Abdul Sattar;Abdelraouf Ishtaiwi
#t 2005
#c 10
#% 283231
#% 318596
#% 496245
#% 531461
#% 534972
#% 535307
#% 535317
#% 574399
#% 750050
#% 1250149
#% 1289186
#% 1289196
#% 1476298
#! Recent research has focused on bridging the gap between the satisfiability (SAT) and constraint satisfaction problem (CSP) formalisms. One approach has been to develop a many-valued SAT formula (MV-SAT) as an intermediate paradigm between SAT and CSP, and then to translate existing highly efficient SAT solvers to the MV-SAT domain. Experimental results have shown this approach can achieve significant improvements in performance compared with the traditional SAT and CSP approaches. In this paper, we follow a different route, developing SAT solvers that can automatically recognise CSP structure hidden in SAT encodings. This allows us to look more closely at how constraint weighting can be implemented in the SAT and CSP domains. Our experimental results show that a SAT-based approach to handle weights, together with CSP-based approach to variable instantiation, is superior to other combinations of SAT and CSP-based approaches. A further experiment on the round robin scheduling problem indicates that this many-valued constraint weighting approach outperforms other state-of-the-art solvers.

#index 1269431
#* Constraint-based preferential optimization
#@ S. Prestwich;F. Rossi;K. B. Venable;T. Walsh
#t 2005
#c 10
#% 1250234
#% 1272026
#% 1279242
#% 1650274
#% 1650354
#! We first show that the optimal and undominated outcomes of an unconstrained (and possibly cyclic) CP-net are the solutions of a set of hard constraints. We then propose a new algorithm for finding the optimal outcomes of a constrained CP-net which makes use of hard constraint solving. Unlike previous algorithms, this new algorithm works even with cyclic CP-nets. In addition. the algorithm is not tied to CP-nets, but can work with any preference formalism which produces a preorder over the outcomes. We also propose an approximation method which weakens the preference ordering induced by the CP-net, returning a larger set of outcomes, but provides a significant computational advantage. Finally, we describe a weighted constraint approach that allows to find good solutions even when optimals do not exist.

#index 1269432
#* SymChaff: a structure-aware satisfiability solver
#@ Ashish Sabharwal
#t 2005
#c 10
#% 2119
#% 25242
#% 131357
#% 162828
#% 220203
#% 278488
#% 327779
#% 336401
#% 336874
#% 349895
#% 510506
#% 561083
#% 577909
#% 577910
#% 594224
#% 655781
#% 729258
#% 741023
#% 804949
#% 1272027
#% 1272049
#% 1272054
#% 1478761
#% 1478779
#% 1837579
#! We present a novel low-overhead framework for encoding and utilizing structural symmetry in propositional satisfiability algorithms (SAT solvers). We use the notion of complete multi-class symmetry and demonstrate the efficacy of our technique through a solver SymChaff that achieves exponential speedup by using simple tags in the specification of problems from both theory and practice. Efficient implementations of DPLL-based SAT solvers are routinely used in areas as diverse as planning, scheduling, design automation, model checking, verification, testing, and algebra. A natural feature of many application domains is the presence of symmetry, such as that amongst all trucks at a certain location in logistics planning and all wires connecting two switch boxes in an FPGA circuit. Many of these problems turn out to have a concise description in many-sorted first order logic. This description can be easily specified by the problem designer and almost as easily inferred automatically. SymChaff, an extension of the popular SAT solver zChaff, uses information obtained from the "sorts" in the first order logic constraints to create symmetry sets that are used to partition variables into classes and to maintain and utilize symmetry information dynamically. Current approaches designed to handle symmetry include: (A) symmetry breaking predicates (SBPs), (B) pseudo-Boolean solvers with implicit representation for counting, (C) modifications of DPLL that handle symmetry dynamically, and (D) techniques based on ZBDDs. SBPs are prohibitively many, often large, and expensive to compute for problems such as the ones we report experimental results for. Pseudo-Boolean solvers are provably exponentially slow in certain symmetric situations and their implicit counting representation is not always appropriate. Suggested modifications of DPLL either work on limited global symmetry and are difficult to extend, or involve expensive algebraic group computations. Finally, techniques based on ZBDDs often do not compare well even with ordinary DPLL-based solvers. Sym-Chaff addresses and overcomes most of these limitations.

#index 1269433
#* Performing Bayesian inference by weighted model counting
#@ Tian Sang;Paul Bearne;Henry Kautz
#t 2005
#c 10
#% 68183
#% 205391
#% 220203
#% 266400
#% 283232
#% 289947
#% 327779
#% 329486
#% 370075
#% 427631
#% 496111
#% 655781
#% 723877
#% 1271984
#% 1478761
#% 1672978
#% 1672980
#% 1698709
#! Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling (Kautz and Selman 2003). Solving such NP-complete tasks by "compilation to SAT" has turned out to be an approach that is of both practical and theoretical interest. Recently, (Sang et al. 2004) have shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether "compilation to model-counting" could be a practical technique for solving real-world #P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.

#index 1269434
#* Proceedings of the 20th national conference on Artificial intelligence - Volume 2
#@ Anthony Cohn
#t 2005
#c 10

#index 1269435
#* A generalized strategy eliminability criterion and computational methods for applying it
#@ Vincent Conitzer;Tuomas Sandholm
#t 2005
#c 10
#% 143652
#% 338466
#% 567883
#% 580517
#% 580519
#% 631052
#% 773295
#% 785101
#% 808367
#% 1250223
#% 1279321
#% 1279322
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum of eliminability criteria from strict dominance (when the sets are as small as possible) to Nash equilibrium (when the sets are as large as possible). We show that checking whether a strategy is eliminable according to this criterion is coNP-complete (both when all the sets are as large as possible and when the dominator sets each have size 1). We then give an alternative definition of the eliminability criterion and show that it is equivalent using the Minimax Theorem. We show how this alternative definition can be translated into a mixed integer program of polynomial size with a number of (binary) integer variables equal to the sum of the sizes of the eliminee sets, implying that checking whether a strategy is eliminable according to the criterion can be done in polynomial time, given that the eliminee sets are small. Finally, we study using the criterion for iterated elimination of strategies.

#index 1269436
#* Fast and compact: a simple class of congestion games
#@ Samuel Ieong;Robert McGrew;Eugene Nudelman;Yoav Shoham;Qixiang Sun
#t 2005
#c 10
#% 338466
#% 567883
#% 765315
#% 785106
#% 788040
#% 1250223
#% 1279322
#% 1279323
#% 1289289
#% 1388766
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269437
#* Mixed-integer programming methods for finding Nash equilibria
#@ Thomas Sandholm;Andrew Gilpin;Vincent Conitzer
#t 2005
#c 10
#% 36698
#% 338466
#% 580517
#% 773295
#% 785101
#% 1250223
#% 1279322
#! We present, to our knowledge, the first mixed integer program (MIP) formulations for finding Nash equilibria in games (specifically, two-player normal form games). We study different design dimensions of search algorithms that are based on those formulations. Our MIP Nash algorithm outperforms Lemke-Howson but not Porter-Nudelman-Shoham (PNS) on GAMUT data. We argue why experiments should also be conducted on games with equilibria with medium-sized supports only, and present a methodology for generating such games. On such games MIP Nash drastically outperforms PNS but not Lemke-Howson. Certain MIP Nash formulations also yield anytime algorithms for Ε-equilibrium. with provable bounds. Another advantage of MIP Nash is that it can be used to find an optimal equilibrium (according to various objectives). The prior algorithms can be extended to that setting, but they are orders of magnitude slower.

#index 1269438
#* Approximate strategic reasoning through hierarchical reduction of large symmetric games
#@ Michael P. Wellman;Daniel M. Reeves;Kevin M. Lochner;Shih-Fen Cheng;Rahul Suri
#t 2005
#c 10
#% 580517
#% 643138
#% 773295
#% 810182
#% 1250223
#% 1279323
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269439
#* Coalitional games in open anonymous environments
#@ Makoto Yokoo;Vincent Conitzer;Tuomas Sandholm;Naoki Ohta;Atsushi Iwasaki
#t 2005
#c 10
#% 160153
#% 171142
#% 252199
#% 1250153
#% 1279301
#! Coalition formation is a key aspect of automated negotiation among self-interested agents. In order for coalitions to be stable, a key question that must be answered is how the gains from cooperation are to be distributed. Various solution concepts (such as the Shapley value, core, least core, and nucleolus) have been proposed. In this paper, we demonstrate how these concepts are vulnerable to various kinds of manipulations in open anonymous environments such as the Internet. These manipulations include submitting false names (one acting as many), collusion (many acting as one), and the hiding of skills. To address these threats, we introduce a new solution concept called the anonymity-proof core, which is robust to these manipulations. We show that the anonymity-proof core is characterized by certain simple axiomatic conditions. Furthermore, we show that by relaxing these conditions, we obtain a concept called the least anonymity-proof core, which is guaranteed to be non-empty. We also show that computational hardness of manipulation may provide an alternative barrier to manipulation.

#index 1269440
#* Mathematical domain reasoning tasks in natural language tutorial dialog on proofs
#@ Christoph Benzmüller;Quoc Bao Vo
#t 2005
#c 10
#% 289292
#% 560582
#% 587585
#% 655273
#% 1279404
#% 1344116
#% 1389402
#! We study challenges that are imposed to mathematical domain reasoning in the context of natural language tutorial dialog on mathematical proofs. The focus is on proof step evaluation: (i) How can mathematical domain reasoning support the resolution of ambiguities and underspecified parts in proof steps uttered by a student? (ii) How can mathematical domain reasoning support the evaluation of a proof step with respect to the criteria soundness, granularity, and relevance?.

#index 1269441
#* Real-time classification of electromyographic signals for robotic control
#@ Beau Crawford;Kai Miller;Pradeep Shenoy;Rajesh Rao
#t 2005
#c 10
#% 393059
#% 466423
#% 1860941
#! Advances in bioengineering have led to increasingly sophisticated prosthetic devices for amputees and paralyzed individuals. Control of such devices necessitates real-time classification of biosignals, e.g., electromyographic (EMG) signals recorded from intact muscles. In this paper, we show that a 4-degrees-of-freedom robotic arm can be controlled in real-time using non-invasive surface EMG signals recorded from the forearm. The innovative features of our system include a physiologically-informed selection of forearm muscles for recording EMG signals, intelligent choice of hand gestures for easy classification, and fast, simple feature extraction from EMG signals. Our selection of gestures is meant to intuitively map to appropriate degrees of freedom in the robotic arm. These design decisions allow us to build fast accurate classifiers online, and control a 4-DOF robotic arm in real-time. In a study involving 3 subjects, we achieved accuracies of 92-98% on an 8-class classification problem using linear SVMs. These classifiers can be learned on-line in under 10 minutes, including data collection and training. Our study also analyzes the issues and tradeoffs involved in designing schemes for robotic control using EMG. Finally, we present details of online experiments where subjects successfully solved tasks of varying complexity using EMG to control the robotic arm.

#index 1269442
#* A decision theoretic model for stress recognition and user assistance
#@ Wenhui Liao;Weihong Zhang;Zhiwei Zhu;Qiang Ji
#t 2005
#c 10
#% 185079
#% 238395
#% 643490
#% 658781
#% 744535
#% 775495
#% 1425491
#% 1784781
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269443
#* On the evaluation of dynamic critiquing: a large-scale user study
#@ Kevin McCarthy;Lorraine McGinty;Barry Smyth;James Reilly
#t 2005
#c 10
#% 232136
#% 320862
#% 734952
#% 754156
#% 790460
#% 1279496
#% 1289344
#% 1389761
#% 1499534
#! Critiquing is an important form of feedback in conversational recommender systems. However, in these systems the user is usually limited to critiquing a single product feature at a time. Recently dynamic critiquing has been proposed to address this shortcoming, by automatically generating compound critiques over multiple features that may be presented to the user at recommendation time. To date a number of different versions of dynamic critiquing have been evaluated in isolation, and with reference to artificial users. In this paper we bring together the main flavors of dynamic critiquing and perform a large-scale comparative evaluation as part of an extensive real-user trial. This evaluation reveals some interesting facts about the way real users interact with critique-based recommenders.

#index 1269444
#* Optimal recommendation sets: covering uncertainty over user preferences
#@ Bob Price;Paul R. Messinger
#t 2005
#c 10
#% 220711
#% 266281
#% 302715
#% 316787
#% 445152
#% 528176
#% 528182
#% 578692
#% 1272026
#! We propose an approach to recommendation systems that optimizes over possible sets of recommended alternatives in a decision-theoretic manner. Our approach selects the alternative set that maximizes the expected valuation of the user's choice from the recommended set. The set-based optimization explicitly recognizes the opportunity for passing residual uncertainty about preferences back to the user to resolve. Implicitly, the approach chooses a set with a diversity of alternatives that optimally covers the uncertainty over possible user preferences. The approach can be used with several preference representations, including utility theory, qualitative preferences models, and informal scoring. We develop a specific formulation for multi-attribute utility theory, which we call maximization of expected max (MEM). We go on to show that this optimization is NP-complete (when user preferences are described by discrete distributions) and suggest two efficient methods for approximating it. These approximations have complexity of the same order as the traditional k-max operator and, for both synthetic and real-world data, perform better than the approach of recommending the k-individually best alternatives (which is not a surprise) and very close to the optimum set (which is less expected).

#index 1269445
#* Goal-directed site-independent recommendations from passive observations
#@ Tingshao Zhu;Russ Greiner;Gerald Häubl;Kevin Jewell;Bob Price
#t 2005
#c 10
#% 230532
#% 284794
#% 306468
#% 323131
#% 325198
#% 343160
#% 348191
#% 463903
#% 481290
#% 1275346
#% 1389359
#% 1389361
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269446
#* An analysis of procedure learning by instruction
#@ Jim Blythe
#t 2005
#c 10
#% 283115
#% 398947
#% 689510
#% 734963
#% 773276
#% 790462
#% 1272173
#% 1289277
#! Many useful planning tasks are handled by plan execution tools, such as PRS, that expand procedure definitions and keep track of several interacting goals and tasks. Learning by instruction is a promising approach to help users modifY the definitions of the procedures. However, the impact of the set of possible instructions on the performance of such systems is not well understood. We develop a framework in which instruction templates may be characterized in terms of syntactic transforms on task definitions, and use it to explore the properties of coverage, ambiguity and efficiency in the set of instructions that are understood by an implemented task learning system. We determine what kind of ambiguity is affected by the instruction set, and show how context-dependent interpretation can increase efficiency and coverage without increasing ambiguity.

#index 1269447
#* An analysis of knowledge collected from volunteer contributors
#@ Timothy Chklovski;Yolanda Gil
#t 2005
#c 10
#% 198055
#% 283180
#% 509695
#% 723391
#% 723406
#% 731208
#% 756964
#% 786515
#% 790491
#% 816164
#% 995514
#% 1250181
#% 1250214
#% 1279287
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269448
#* Integrating description logics and action formalisms: first results
#@ Franz Baader;Carsten Lutz;Maja Miličic;Ulrike Sattler;Frank Wolter
#t 2005
#c 10
#% 70391
#% 89961
#% 117869
#% 485080
#% 665856
#% 1271815
#% 1499565
#! We propose an action formalism that is based on description logics (DLs) and may be viewed as an instance of the Situation Calculus (SitCalc). In particular, description logic concepts can be used for describing the state of the world, and the pre- and post-conditions of actions. The main advantage of such a combination is that. on the one hand, the expressive power for describing world states and conditions is higher than in other decidable fragments of the Sitcalc, which are usually propositional. On the other hand, in contrast to the full Sitcalc, effective reasoning is still possible. In this paper, we perform a detailed investigation of how the choice of the DL influences the complexity of the standard reasoning tasks executability and projection in the corresponding action formalism. We also discuss semantic and computational problems in natural extensions of our framework.

#index 1269449
#* Using SAT and logic programming to design polynomial-time algorithms for planning in non-deterministic domains
#@ Chitta Baral;Thomas Eiter;Jicheng Zhao
#t 2005
#c 10
#% 131357
#% 342829
#% 400992
#% 578723
#% 655322
#% 880394
#! We show that a Horn SAT and logic programming approach to obtain polynomial time algorithms for problem solving can be fruitfully applied to finding plans for various kinds of goals in a non-deterministic domain. We particularly focus on finding weak, strong, and strong cyclic plans for planning problems, as they are the most studied ones in the literature. We describe new algorithms for these problems and show how non-monotonic logic programming can be used to declaratively compute strong cyclic plans. As a further benefit, preferred plans among alternative candidate plans may be singled out this way. We give complexity results for weak. strong, and strong cyclic planning. Finally, we briefly discuss some of the kinds of goals in non-deterministic domains for which the approach in the paper can be used.

#index 1269450
#* Hybrid possibilistic networks
#@ Salem Benferhat;Salma Smaoui
#t 2005
#c 10
#% 44876
#% 167544
#% 380725
#% 772064
#% 1787871
#! Possibilistic networks are important tools for dealing with uncertain pieces of information. For multiply-connected networks, it is well known that the inference process is a hard problem. This paper studies a new representation of possibilistic networks, called hybrid possibilistic networks. The uncertainty is no longer represented by local conditional possibility distributions, but by their compact representations which are possibilistic knowledge bases. We show that the inference algorithm in hybrid networks is strictly more efficient than the ones of standard propagation algorithm.

#index 1269451
#* Practical first-order argumentation
#@ Philippe Besnard;Anthony Hunter
#t 2005
#c 10
#% 330290
#% 337502
#% 417812
#% 445844
#% 752766
#! There are many frameworks for modelling argumentation in logic. They include a formal representation of individual arguments and techniques for comparing conflicting arguments. A problem with these proposals is that they do not consider arguments for and against first-order formulae. We present a framework for first-order logic argumentation based on argument trees that provide a way of exhaustively collating arguments and counter-arguments. A difficulty with first-order argumentation is that there may be many arguments and counterarguments even with a relatively small knowledgebase. We propose rationalizing the arguments under consideration with the aim of reducing redundancy and highlighting key points.

#index 1269452
#* Prioritized component systems
#@ Gerhard Brewka;Ilkka Niemelä;Mirosław Truszczynski
#t 2005
#c 10
#% 400992
#% 1272026
#% 1289373
#% 1650274
#! We introduce a flexible framework to specify problem solutions (outcomes) and preferences among them. The proposal combines ideas from answer-set programming (ASP), answer-set optimization (ASO) and CP-nets. The problem domain is structured into components. ASP techniques are used to specify values of components, as well as global (intercomponent) constraints among these values. ASO methods are used to describe preferences among the values of a component and CP-net techniques to represent intercomponent dependencies and corresponding preferences.

#index 1269453
#* DL-Lite: tractable description logics for ontologies
#@ Diego Calvanese;Giuseppe De Giacomo;Domenico Lemho;Maurizio Lenzerini;Riccardo Rosati
#t 2005
#c 10
#% 58347
#% 248038
#% 263136
#% 264704
#% 273687
#% 326595
#% 384978
#% 531450
#% 576116
#% 598376
#% 665856
#% 665867
#% 1279213
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269454
#* An axiomatic account of formal argumentation
#@ Martin Caminada;Leila Amgoud
#t 2005
#c 10
#% 198464
#% 362038
#% 431089
#% 752766
#% 782765
#% 834129
#! Argumentation theory has become an important topic in the field of AI. The basic idea is to construct arguments in favor and against a statement, to select the "acceptable" ones and, finally, to determine whether the statement can be accepted or not. Dung's elegant account of abstract argumentation (Dung 1995) may have caused some to believe that defining an argumentation formalism is simply a matter of determining how arguments and their defeat relation can be constructed from a given knowledge base. Unfortunately, things are not that simple; many straightforward instantiations of Dung's theory can lead to very unintuitive results, as is discussed in this paper. In order to avoid such anomalies, in this paper we are interested in defining some rules, called rationality postulates or axioms, that govern the well definition of an argumentation system. In particular, we define two important rationality postulates that any system should satisfy: the consistency and the closeness of the results returned by that system. We then provide a relatively easy way in which these quality postulates can be warranted by our argumentation system.

#index 1269455
#* Merging argumentation systems
#@ Sylvie Coste-Marquis;Caroline Devred;Sébastien Konieczny;Marie-Christine Lagasquie-Schiex;Pierre Marquis
#t 2005
#c 10
#% 196352
#% 198464
#% 348818
#% 361152
#% 428343
#% 431089
#% 503526
#% 557562
#! In this paper, we address the problem of deriving sensible information from a collection of argumentation systems coming from different agents. A general framework for merging argumentation systems from Dung's theory of argumentation is presented. Each argumentation system gives both a set of arguments and the way they interact (i.e. attack or non-attack) according to the corresponding agent. The aim is to define the argument system (or the set of argument systems) that best represents the group. Our framework is general enough to handle the case when agents do not share the same set of arguments. Merging argumentation systems is shown as a valuable approach for defining (sets of) arguments acceptable by the group.

#index 1269456
#* DD-PREF: a language for expressing preferences over sets
#@ Marie DesJardins;Kiri L. Wagstaff
#t 2005
#c 10
#% 314918
#% 414514
#% 578710
#% 769915
#% 805841
#% 818584
#% 1272026
#% 1289345
#! In many application domains, it is useful to be able to represent and reason about a user's preferences over sets of objects. We present a representation language, DD-PREF (for Diversity and Depth PREFerences), for specifying the desired diversity and depth of sets of objects where each object is represented as a vector of feature values. A strong diversity preference for a particular feature indicates that the user would like the set to include objects whose values are evenly dispersed across the range of possible values for that feature. A strong depth preference for a feature indicates that the user is interested in specific target values or ranges. Diversity and depth are complementary, but are not necessarily opposites. We define an objective function that, when maximized, identifies the subset of objects that best satisfies a statement of preferences in DD-PREF. Exhaustively searching the space of all possible subsets is intractable for large problem spaces; therefore, we also present an efficient greedy algorithm for generating preferred object subsets. We demonstrate the expressive power of DD-PREF and the performance of our greedy algorithm by encoding and applying qualitatively different preferences for multiple tasks on a blocks world data set. Finally, we provide experimental results for a collection of Mars rover images, demonstrating that we can successfully capture individual preferences of different users, and use them to retrieve high-quality image subsets.

#index 1269457
#* Cumulative effects of concurrent actions on numeric-valued fluents
#@ Esra Erdem;Alfredo Gabaldon
#t 2005
#c 10
#% 25884
#% 117869
#% 174161
#% 224765
#% 296169
#% 342119
#% 528334
#% 544921
#% 835732
#% 1272014
#% 1279366
#% 1289204
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269458
#* Only-knowing: taking it beyond autoepistemic reasoning
#@ Gerhard Lakemeyer;Hector J. Levesque
#t 2005
#c 10
#% 1146
#% 39263
#% 68240
#% 89976
#% 95265
#% 175359
#% 231748
#% 326595
#% 450455
#% 780339
#! The idea of only-knowing a collection of sentences has been previously shown to have a close connection with autoepistemic logic. Here we propose a more general account of only-knowing that captures not only autoepistemic logic but default logic as well. This allows us not only to study the properties of default logic in terms of an underlying model of belief, but also the relationship among different forms of nonmonotonic reasoning, all within a classical monotonic logic characterized semantically in terms of possible worlds.

#index 1269459
#* Tractable reasoning in first-order knowledge bases with disjunctive information
#@ Yongmei Liu;Hector J. Levesque
#t 2005
#c 10
#% 55926
#% 93634
#% 181328
#% 184793
#% 191611
#% 321058
#% 598376
#% 1279223
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269460
#* Knowledge integration for description logics
#@ Thomas Meyer;Kevin Lee;Richard Booth
#t 2005
#c 10
#% 79502
#% 101435
#% 216989
#% 517280
#% 561740
#% 665859
#% 665863
#% 763750
#% 1279264
#% 1290099
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269461
#* Analysis of strategic knowledge in back of the envelope reasoning
#@ Praveen K. Paritosh;Kenneth D. Forbus
#t 2005
#c 10
#% 25470
#% 109848
#% 175376
#% 300533
#% 578785
#! Back of the envelope (BotE) reasoning involves generating quantitative answers in situations where exact data and models are unavailable and where available data is often incomplete and/or inconsistent. A rough estimate generated quickly is more valuable and useful than a detailed analysis, which might be unnecessary, impractical, or impossible because the situation does not provide enough time, information, or other resources to perform one. Such reasoning is a key component of commonsense reasoning about everyday physical situations. We present an implemented system, BotE-Solver, that can solve about a dozen estimation questions like "What is the annual cost of healthcare in USA?" from different domains using a library of strategies and the Cyc knowledge base. BotE-Solver is a general-purpose problem solving framework that uses strategies represented as suggestions, and keeps track of problem solving progress in an AND/OR tree. A key contribution of this paper is a knowledge level analysis [Newell, 1982] of the strategic knowledge used in BotE reasoning. We present a core collection of seven powerful estimation strategies that provides broad coverage for such problem solving. We hypothesize that this is the complete set of back of the envelope problem solving strategies. We present twofold support for this hypothesis: 1) an empirical analysis of all problems (n=44) on Force and Pressure, Rotation and Mechanics, Heat, and Astronomy from Clifford Swartz's "Back-of-the-Envelope Physics" [Swartz, 2003], and 2) an analysis of strategies used by BotE-Solver.

#index 1269462
#* Generalized link properties for expressive ε-connections of description logics
#@ Bijan Parsia;Bernardo Cuenca Grau
#t 2005
#c 10
#% 539462
#% 763751
#% 1289174
#! ε-Connections are a robust framework for combining in a decidable way several families of decidable logics, including Description Logics (DLs), Modal Logics, and many logics of time and space. ε-Connections have also proved to be useful for supporting modular, distributed modeling such as is becoming common on the Semantic Web. In this paper, we present an extension to ε-Connections of DLs that provides more flexibility in the way link properties can be defined and used in a combination of ontologies. We also provide means for defining transitive relations across domains and for simulating some of the expressivity of the transitive closure operator. Finally, we provide a tableau-based decision procedure for two relevant ε-Connection languages involving the influential DLs SHIQ, SHOQ and SHIO, which are at the hasis of the Web Ontology Language (OWL).

#index 1269463
#* Functional specification of probabilistic process models
#@ Avi Pfeffer
#t 2005
#c 10
#% 75936
#% 246051
#% 292235
#% 773276
#% 1279354
#% 1289246
#% 1650568
#% 1650767
#% 1784146
#! Agents that handle complex processes evolving over a period of time need to be able to monitor the state of the process. Since the evolution of a process is often stochastic, this requires probabilistic monitoring of processes. A probabilistic process modeling language is needed that can adequately capture our uncertainty about the process execution. We present a language for describing probabilistic process models. This language is functional in nature, and the paper argues that a functional language provides a natural way to specify process models. In our framework, processes have both states and values. Processes may execute sequentially or in parallel, and we describe two alternative forms of parallelism. An inference algorithm is presented that constructs a dynamic Bayesian network, containing a variable for every subprocess that is executed during the course of executing a process. We present a detailed example demonstrating the naturalness of the language.

#index 1269464
#* Diagnosing terminologies
#@ Stefan Schlobach
#t 2005
#c 10
#% 21137
#% 65347
#% 70391
#% 184793
#% 496408
#% 529190
#% 531444
#% 561740
#% 935898
#% 1279264
#% 1289423
#% 1702410
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269465
#* Issues in reasoning about interaction networks in cells: necessity of event ordering knowledge
#@ Nam Tran;Chitta Baral;Carron Shankland
#t 2005
#c 10
#% 101955
#% 763743
#% 790576
#% 833024
#! In this paper we discuss several representation issues that we came across while modelling molecular interactions in cells of living organisms. One of the issues was that the triggering of events inside cells, an important modelling component, are not necessarily immediate, leading to multiple evolution models in the absence of additional information. Second, often an action or a trigger at one level of granularity of representation can be elaborated and refined. We show the problem that existing representation and modelling formalisms have in dealing with the above issues. We then present an action language which builds up on a previous language, and has the ability to express event ordering knowledge. We show that our language is able to adequately address the above-mentioned issues.

#index 1269466
#* A theory of forgetting in logic programming
#@ Kewen Wang;Abdul Sattar;Kaile Su
#t 2005
#c 10
#% 77167
#% 305347
#% 400987
#% 752737
#% 752792
#% 752806
#% 772065
#% 1271987
#% 1289451
#! The study of forgetting for reasoning has attracted considerable attention in AI. However, much of the work on forgetting, and other related approaches such as independence, irrelevance and novelty, has been restricted to the classical logics. This paper describes a detailed theoretical investigation of the notion of forgetting in the context of logic programming. We first provide a semantic definition of forgetting under the answer sets for extended logic programs. We then discuss the desirable properties and some motivating examples. An important result of this study is an algorithm for computing the result of forgetting in a logic program. Furthermore, we present a modified version of the algorithm and show that the time complexity of the new algorithm is polynomial with respect to the size of the given logic program if the size of certain rules is fixed. We show how the proposed theory of forgetting can be used to characterize the logic program updates.

#index 1269467
#* Reasoning about intended actions
#@ Chitta Baral;Michael Gelfond
#t 2005
#c 10
#% 342119
#% 398251
#% 499512
#% 752742
#! In most research on reasoning about actions and reasoning about narratives one either reasons about hypothetical execution of actions, or about actions that actually occurred. In this paper we develop a high level language that allows the expression of intended or planned action sequences. Unlike observed action occurrences, planned or intended action occurrences may not actually take place. But often when they do not take place, they persist, and happen at an opportune future time. We give the syntax and semantics for expressing such intentions. We then give a logic programming axiomatization and show the correspondence between the semantics of a description in the high level language, and the answer sets of the corresponding logic programming axiomatization. We illustrate the application of our formalism with respect to reasoning about trips.

#index 1269468
#* Strong and uniform equivalence in answer-set programming: characterizations and complexity results for the non-ground case
#@ Thomas Eiter;Michael Fink;Hans Tompits;Stefan Woltran
#t 2005
#c 10
#% 23898
#% 53383
#% 53399
#% 53400
#% 61406
#% 340738
#% 342389
#% 342829
#% 400992
#% 578673
#% 752748
#! _cf_loadingtexthtml="";_cf_contextpath="";_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";_cf_jsonprefix='//';_cf_clientid='32BBAB3E6C7445A9B244A56AE3456C9A';Strong and uniform equivalence in answer-set programming function settab() { var mytabs = ColdFusion.Layout.getTabLayout('citationdetails'); mytabs.on('tabchange', function(tabpanel,activetab) { document.cookie = 'picked=' + '1619444' + ',' + activetab.id; }) }function letemknow(){ ColdFusion.Window.show('letemknow');}function testthis(){alert('test');}function loadalert(){ alert("I am in the load alert"); }function loadalert2(){ alert("I am in the load alert2"); } google.load('visualization', '1', {packages:['orgchart']}); google.setOnLoadCallback(drawChart); function drawChart() { var data = new google.visualization.DataTable(); data.addColumn('string', 'Name'); data.addColumn('string', 'Manager'); data.addColumn('string', 'ToolTip'); data.addRows([ [{v:'0', f:'CCS for this Article

#index 1269469
#* Properties of programs with monotone and convex constraints
#@ Lengning Liu;Mirosław Truszczynski
#t 2005
#c 10
#% 340738
#% 400986
#% 400992
#% 411814
#% 464918
#% 578673
#% 752744
#% 752748
#% 1250132
#% 1279333
#! We study properties of programs with monotone and convex constraints. We extend to these formalisms concepts and results from normal logic programming. They include tight programs and Fages Lemma, program completion and loop formulas, and the notions of strong and uniform equivalence with their characterizations. Our results form an abstract account of properties of some recent extensions of logic programming with aggregates, especially the formalism of smodels.

#index 1269470
#* A unified framework for representing logic program updates
#@ Yan Zhang;Norman Foo
#t 2005
#c 10
#% 337497
#% 752792
#% 801771
#% 880392
#% 1271987
#% 1289451
#! As a promising formulation to represent and reason about agents' dynamic behavious, logic program updates have been considerably studied recently. While similarities and differences between various approaches were discussed and evaluated by researchers, there is a lack of method to represent different logic program update approaches under a common framework. In this paper, we continue our study on a general framework for logic program conflict solving based on notions of strong and weak forgettings (Zhang, Foo, & Wang 2005). We show that all major logic program update approaches can be transformed into our framework, under which each update approach becomes a specific conflict solving case with certain constraints. We also investigate related computational properties for these transformations.

#index 1269471
#* Robust supervised learning
#@ J. Andrew Bagnell
#t 2005
#c 10
#% 73372
#% 78916
#% 115608
#% 229940
#% 580687
#% 757953
#% 769885
#% 803574
#% 1080960
#! Supervised machine learning techniques developed in the Probably Approximately Correct, Maximum A Posteriori, and Structural Risk Minimiziation frameworks typically make the assumption that the test data a learner is applied to is drawn from the same distribution as the training data. In various prominent applications of learning techniques, from robotics to medical diagnosis to process control, this assumption is violated. We consider a novel framework where a learner may influence the test distribution in a bounded way. From this framework, we derive an efficient algorithm that acts as a wrapper around a broad class of existing supervised learning algorithms while guarranteeing more robust behavior under changes in the input distribution.

#index 1269472
#* Weighted one-against-all
#@ Alina Beygelzimer;John Langford;Bianca Zadrozny
#t 2005
#c 10
#% 276516
#% 290482
#% 722756
#% 727925
#% 763699
#% 1272365
#! The one-against-all reduction from multiclass classification to binary classification is a standard technique used to solve multiclass problems with binary classifiers. We show that modifying this technique in order to optimize its error transformation properties results in a superior technique, both experimentally and theoretically. This algorithm can also be used to solve a more general classification problem "multi-label classification," which is the same as multiclass classification except that it allows multiple correct labels for a given example.

#index 1269473
#* Optimal efficient learning equilibrium: imperfect monitoring in symmetric games
#@ Ronen I. Brafman;Moshe Tennenholtz
#t 2005
#c 10
#% 266286
#% 418731
#% 465913
#% 722895
#% 789557
#% 1272286
#% 1289288
#! Efficient Learning Equilibrium (ELE) is a natural solution concept for multi-agent encounters with incomplete information. It requires the learning algorithms themselves to be in equilibrium for any game selected from a set of (initially unknown) games. In an optimal ELE, the learning algorithms would efficiently obtain the surplus the agents would obtain in an optimal Nash equilibrium of the initially unknown game which is played. The crucial part is that in an ELE deviations from the learning algorithms would become nonbeneficial after polynomial time, although the game played is initially unknown. While appealing conceptually, the main challenge for establishing learning algorithms based on this concept is to isolate general classes of games where an ELE exists. Unfortunately, it has been shown that while an ELE exists for the setting in which each agent can observe all other agents' actions and payoffs, an ELE does not exist in general when the other agents' payoffs cannot be observed. In this paper we provide the first positive results on this problem, constructively proving the existence of an optimal ELE for the class of symmetric games where an agent can not observe other agents' payoffs.

#index 1269474
#* Discovering domain-specific composite kernels
#@ Tom Briggs;Tim Oates
#t 2005
#c 10
#% 340903
#% 425040
#% 464267
#% 722810
#% 722887
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269475
#* A comparison of novel and state-of-the-art polynomial Bayesian network learning algorithms
#@ Laura E. Brown;Ioannis Tsamardinos;Constantin F. Aliferis
#t 2005
#c 10
#% 197387
#% 370075
#% 400980
#% 722900
#% 729990
#% 1650289
#% 1672992
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269476
#* Reducing labeling effort for structured prediction tasks
#@ Aron Culotta;Andrew McCallum
#t 2005
#c 10
#% 170649
#% 464434
#% 466231
#% 549447
#% 648984
#% 855102
#% 1250184
#! A common obstacle preventing the rapid deployment of supervised machine learning algorithms is the lack of labeled training data. This is particularly expensive to obtain for structured prediction tasks, where each training instance may have multiple, interacting labels, all of which must be correctly annotated for the instance to be of use to the learner. Traditional active learning addresses this problem by optimizing the order in which the examples are labeled to increase learning efficiency. However, this approach does not consider the difficulty of labeling each example, which can vary widely in structured prediction tasks. For example, the labeling predicted by a partially trained system may be easier to correct for some instances than for others. We propose a new active learning paradigm which reduces not only how many instances the annotator must label, but also how difficult each instance is to annotate. The system also leverages information from partially correct predictions to efficiently solicit annotations from the user. We validate this active learning framework in an interactive information extraction system, reducing the total number of annotation actions by 22%.

#index 1269477
#* Towards learning stochastic logic programs from proof-banks
#@ Luc De Raedt;Kristian Kersting;Sunna Torge
#t 2005
#c 10
#% 147677
#% 243701
#% 373774
#% 379345
#% 380466
#% 465762
#% 577225
#% 723249
#% 731606
#% 1271907
#% 1393864
#% 1476274
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269478
#* Incremental estimation of discrete hidden Markov models based on a new backward procedure
#@ German Florez-Larrahondo;Susan Bridges;Eric A. Hansen
#t 2005
#c 10
#% 277483
#% 277494
#% 301551
#% 713956
#% 830740
#% 968564
#% 1271901
#! We address the problem of learning discrete hidden Markov models from very long sequences of observations. Incremental versions of the Baum-Welch algorithm that approximate the β-values used in the backward procedure are commonly used for this problem, since their memory complexity is independent of the sequence length. We introduce an improved incremental Baum-Welch algorithm with a new backward procedure that approximates the β-values based on a one-step lookahead in the training sequence. We justify the new approach analytically, and report empirical results that show it converges faster than previous incremental algorithms.

#index 1269479
#* A hybrid generative/discriminative approach to semi-supervised classifier design
#@ Akinori Fujino;Naonori Ueda;Kazumi Saito
#t 2005
#c 10
#% 211044
#% 252011
#% 280817
#% 311027
#% 406493
#% 529685
#! Semi-supervised classifier design that simultaneously utilizes both labeled and unlabeled samples is a major research issue in machine learning. Existing semisupervised learning methods belong to either generative or discriminative approaches. This paper focuses on probabilistic semi-supervised classifier design and presents a hybrid approach to take advantage of the generative and discriminative approaches. Our formulation considers a generative model trained on labeled samples and a newly introduced bias correction model. Both models belong to the same model family. The proposed hybrid model is constructed by combining both generative and bias correction models based on the maximum entropy principle. The parameters of the bias correction model are estimated by using training data, and combination weights are estimated so that labeled samples are correctly classified. We use naive Bayes models as the generative models to apply the hybrid approach to text classification problems. In our experimental results on three text data sets, we confirmed that the proposed method significantly outperformed pure generative and discriminative methods when the classification performances of the both methods were comparable.

#index 1269480
#* Discriminative model selection for belief net structures
#@ Yuhong Guo;Russ Greiner
#t 2005
#c 10
#% 44876
#% 129987
#% 229806
#% 246832
#% 277480
#% 369349
#% 380342
#% 466591
#% 528319
#% 578681
#% 770761
#% 810947
#% 1650277
#% 1650303
#% 1650719
#! Bayesian belief nets (BNs) are often used for classification tasks, typically to return the most likely class label for a specified instance. Many BN-Iearners, however, attempt to find the BN that maximizes a different objective function - viz., likelihood, rather than classification accuracy - typically by first using some model selection criterion to identify an appropriate graphical structure, then finding good parameters for that structure. This paper considers a number of possible criteria for selecting the best structure, both generative (i.e., based on likelihood; BIC, BDe) and discriminative (i.e., Conditional BIC (CBIC), resubstitution Classification Error (CE) and Bias2+Variance (BV)). We empirically compare these criteria against a variety of different "correct BN structures", both real-world and synthetic, over a range of complexities. We also explore different ways to set the parameters, dealing with two issues: (1) Should we seek the parameters that maximize likelihood versus the ones that maximize conditional likelihood? (2) Should we use (i) the entire training sample first to learn the best parameters and then to evaluate the models, versus (ii) only a partition for parameter estimation and another partition for evaluation (cross-validation)? Our results show that the discriminative BV model selection criterion is one of the best measures for identifying the optimal structure, while the discriminative CBIC performs poorly; that we should use the parameters that maximize likelihood; and that it is typically better to use cross-validation here.

#index 1269481
#* Transforming between propositions and features: bridging the gap
#@ Daniel T. Halstead;Kenneth D. Forbus
#t 2005
#c 10
#% 1116
#% 136370
#% 359837
#% 529803
#% 722914
#! It is notoriously difficult to simultaneously deal with both probabilistic and structural representations in A.I., particularly because probability necessitates a uniform representation of the training examples. In this paper, we show how to build fully-specified probabilistic models from arbitrary propositional case descriptions about terrorist activities. Our method facilitates both reasoning and learning. Our solution is to use structural analogy to build probabilistic generalizations about those cases. We use these generalizations as a framework for mapping the structural representations, which are well-suited for reasoning, into features, which are well-suited for learning, and back again. Finally, we demonstrate how probabilistic generalizations are an excellent bridge for joining reasoning and learning by using them to perform a traditional machine learning technique, Bayesian network modeling, over arbitrarily high order structural data about terrorist actions, and further, we discuss how this might be used to facilitate automatic knowledge acquisition.

#index 1269482
#* Effective short-term opponent exploitation in simplified poker
#@ Bret Hoehn;Finnegan Southey;Robert C. Holte;Valeriy Bulitko
#t 2005
#c 10
#% 233137
#% 593734
#% 1279308
#% 1650304
#% 1740191
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269483
#* Non-stationary policy learning in 2-player zero sum games
#@ Steven Jensen;Daniel Boley;Maria Gini;Paul Schrater
#t 2005
#c 10
#% 222437
#% 279755
#% 292004
#% 348821
#% 384911
#% 463903
#% 629487
#% 823854
#% 959477
#! A key challenge in multiagent environments is the construction of agents that are able to learn while acting in the presence of other agents that are simultaneously learning and adapting. These domains require on-line learning methods without the benefit of repeated training examples, as well as the ability to adapt to the evolving behavior of other agents in the environment. The difficulty is further exacerbated when the agents are in an adversarial relationship, demanding that a robust (i.e. winning) non-stationary policy be rapidly learned and adapted. We propose an on-line sequence learning algorithm, ELPH, based on a straightforward entropy pruning technique that is able to rapidly learn and adapt to non-stationary policies. We demonstrate the performance of this method in a non-stationary learning environment of adversarial zero-sum matrix games.

#index 1269484
#* nFOIL: integrating Naïve Bayes and FOIL
#@ Niels Landwehr;Kristian Kersting;Luc De Raedt
#t 2005
#c 10
#% 217072
#% 333797
#% 449508
#% 550417
#% 568785
#% 727834
#% 727912
#% 731606
#% 770761
#% 771945
#! We present the system nFOIL. It tightly integrates the naïve Bayes learning scheme with the inductive logic programming rule-learner FOIL. In contrast to previous combinations, which have employed naïve Bayes only for post-processing the rule sets, nFOIL employs the naïve Bayes criterion to directly guide its search. Experimental evidence shows that nFOIL performs better than both its base line algorithm FOIL or the post-processing approach, and is at the same time competitive with more sophisticated approaches.

#index 1269485
#* Using modified Lasso regression to learn large undirected graphs in a probabilistic framework
#@ Fan Li;Yiming Yang
#t 2005
#c 10
#% 722937
#% 770857
#% 771626
#% 833694
#! Learning the structures of large undirected graphs with thousands of nodes from data has been an open challenge. In this paper, we use graphical Gaussian model (GGM) as the underlying model and propose a novel ARD style Wishart prior for the precision matrix of the GGM. which encodes the graph structure we want to learn. With this prior, we can get the MAP estimation of the precision matrix by solving (a modified version of) Lasso regressions and achieve a sparse solution. We use our approach to learn genetic regulatory networks from genome-wide expression microarray data and protein-binding location analysis data. Evaluated on the basis of consistency with the GO annotations, the experiments show that our approach has a much better performance than the clustering-based approaches and BN learning approaches in discovering gene regulatory modules.

#index 1269486
#* The regularized EM algorithm
#@ Haifeng Li;Keshu Zhang;Tao Jiang
#t 2005
#c 10
#% 115608
#% 232117
#% 234979
#% 345829
#% 1810175
#! The EM algorithm heavily relies on the interpretation of observations as incomplete data but it does not have any control on the uncertainty of missing data. To effectively reduce the uncertainty of missing data, we present a regularized EM algorithm that penalizes the likelihood with the mutual information between the missing data and the incomplete data (or the conditional entropy of the missing data given the observations). The proposed method maintains the advantage of the conventional EM algorithm, such as reliable global convergence, low cost per iteration, economy of storage, and ease of programming. We also apply the regularized EM algorithm to fit the finite mixture model. Our theoretical analysis and experiments show that the new method can efficiently fit the models and effectively simplify over-complicated models.

#index 1269487
#* Semi-supervised sequence modeling with syntactic topic models
#@ Wei Li;Andrew McCallum
#t 2005
#c 10
#% 158687
#% 162505
#% 311027
#% 464434
#% 722904
#% 788043
#% 855182
#! Although there has been significant previous work on semi-supervised learning for classification, there has been relatively little in sequence modeling. This paper presents an approach that leverages recent work in manifold-learning on sequences to discover word clusters from language data, including both syntactic classes and semantic topics. From unlabeled data we form a smooth. low-dimensional feature space, where each word token is projected based on its underlying role as a function or content word. We then use this projection as additional input features to a linear-chain conditional random field trained on limited labeled training data. On standard part-of-speech tagging and Chinese word segmentation data sets we show as much as 14% error reduction due to the unlabeled data, and also statistically-significant improvements over a related semi-supervised sequence tagging method due to Miller et al.

#index 1269488
#* Giving advice about preferred actions to reinforcement learners via knowledge-based kernel regression
#@ Richard Maclin;Jude Shavlik;Lisa Torrey;Trevor Walker;Edward Wild
#t 2005
#c 10
#% 124692
#% 126853
#% 203611
#% 384911
#% 464296
#% 464622
#% 793236
#! We present a novel formulation for providing advice to a reinforcement learner that employs support-vector regression as its function approximator. Our new method extends a recent advice-giving technique, called Knowledge-Based Kernel Regression (KBKR), that accepts advice concerning a single action of a reinforcement learner. In KBKR, users can say that in some set of states, an action's value should be greater than some linear expression of the current state. In our new technique, which we call Preference KBKR (Pref-KBKR), the user can provide advice in a more natural manner by recommending that some action is preferred over another in the specified set of states. Specifying preferences essentially means that users are giving advice about policies rather than Q values, which is a more natural way for humans to present advice. We present the motivation for preference advice and a proof of the correctness of our extension to KBKR. In addition, we show empirical results that our method can make effective use of advice on a novel reinforcement-learning task, based on the RoboCup simulator, which we call Breakaway. Our work demonstrates the significant potential of advice-giving techniques for addressing complex reinforcement learning problems, while further demonstrating the use of support-vector regression for reinforcement learning.

#index 1269489
#* Distribution-free learning of Bayesian network structure in continuous domains
#@ Dimitris Margaritis
#t 2005
#c 10
#% 44876
#% 528170
#! In this paper we present a method for learning the structure of Bayesian networks (BNs) without making any assumptions on the probability distribution of the domain. This is mainly useful for continuous domains, where there is little guidance and many choices for the parametric distribution families to be used for the local conditional probabilities of the Bayesian network, and only a few have been examined analytically. We therefore focus on BN structure learning in continuous domains. We address the problem by developing a conditional independence test for continuous variables, which can be readily used by any existing independence-based BN structure learning algorithm. Our test is non-parametric, making no assumptions on the distribution of the domain. We also provide an effective and computationally efficient method for calculating it from data. We demonstrate the learning of the structure of graphical models in continuous domains from real-world data, to our knowledge for the first time using independence-based methods and without distributional assumptions. We also experimentally show that our test compares favorably with existing statistical approaches which use prediscretization, and verify desirable properties such as statistical consistency.

#index 1269490
#* Online query relaxation via Bayesian causal structures discovery
#@ Ion Muslea;Thomas J. Lee
#t 2005
#c 10
#% 6804
#% 166821
#% 213443
#% 245519
#% 442712
#% 445170
#% 465904
#% 769900
#! We introduce a novel algorithm, TOQR, for relaxing failed queries over databases, that is, over-constrained DNF queries that return an empty result. TOQR uses a small dataset to discover the implicit relationships among the domain attributes, and then it exploits this domain knowledge to relax the failed query. TOQR starts with a relaxed query that does not include any constraint, and it tries to add to it as many as possible of the original constraints or their relaxations. The order in which the constraints are added is derived from the domain's causal structure, which is learned by applying the TAN algorithm to the small training dataset. Our experiments show that TOQR clearly outperforms other approaches: even when trained on a handful of examples, it successfully relaxes more that 97% of the failed queries; furthermore, TOQR'S relaxed queries are highly similar to the original failed query.

#index 1269491
#* Redescription mining: structure theory and algorithms
#@ Laxmi Parida;Naren Ramakrishnan
#t 2005
#c 10
#% 65641
#% 198240
#% 317130
#% 431103
#% 451052
#% 769902
#% 772329
#% 778215
#% 1289265
#! We introduce a new data mining problem--redescription mining--that unifies considerations of conceptual clustering, constructive induction, and logical formula discovery. Redescription mining begins with a collection of sets, views it as a propositional vocabulary, and identifies clusters of data that can be defined in at least two ways using this vocabulary. The primary contributions of this paper are conceptual and theoretical: (i) we formally study the space of redescriptions underlying a dataset and characterize their intrinsic structure, (ii) we identify impossibility as well as strong possibility results about when mining redescriptions is feasible, (iii) we present several scenarios of how we can custom-build redescription mining solutions for various biases, and (iv) we outline how many problems studied in the larger machine learning community are really special cases of redescription mining. By highlighting its broad scope and relevance. we aim to establish the importance of redescription mining and make the case for a thrust in this new line of research.

#index 1269492
#* Spectral clustering of biological sequence data
#@ William Pentney;Marina Meila
#t 2005
#c 10
#% 313959
#% 397654
#% 594009
#! In this paper, we apply spectral techniques to clustering biological sequence data that has proved more difficult to cluster effectively. For this purpose, we have to (1) extend spectral clustering algorithms to deal with asymmetric affinities. like the alignment scores used in the comparison of biological sequences. and (2) devise a hierarchical algorithm that can handle many clusters with imbalanced sizes robustly. We present an algorithm for clustering asymmetric affinity data, and demonstrate the performance of this algorithm at recovering the higher levels of the Structural Classification of Proteins (SCOP) on a data base of highly conserved subsequences.

#index 1269493
#* Enhanced direct linear discriminant analysis for feature extraction on high dimensional data
#@ A. K. Qin;S. Y. M. Shi;P. N. Suganthan;M. Loog
#t 2005
#c 10
#% 235342
#% 342307
#% 1858145
#% 1860915
#% 1861142
#% 1861148
#! We present an enhanced direct linear discriminant analysis (EDLDA) solution to effectively and efficiently extract discriminatory features from high dimensional data. The EDLDA integrates two types of class-wise weighting terms in estimating the average within-class and between-class scatter matrices in order to relate the resulting Fisher criterion more closely to the minimization of classification error. Furthermore, the extracted discriminant features are weighted by mutual information between features and class labels. Experimental results on four biometric datasets demonstrate the promising performance of the proposed method.

#index 1269494
#* A maximum likelihood framework for integrating taxonomies
#@ Suju Rajan;Kunal Punera;Joydeep Ghosh
#t 2005
#c 10
#% 36672
#% 309141
#% 330767
#% 348187
#% 458379
#% 465747
#% 466078
#% 529190
#% 531444
#% 571073
#% 729927
#% 766458
#% 770849
#% 1279215
#% 1289178
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269495
#* Constraint-based entity matching
#@ Warren Shen;Xin Li;AnHai Doan
#t 2005
#c 10
#% 201889
#% 248801
#% 248810
#% 310516
#% 333943
#% 348187
#% 465919
#% 577247
#% 729913
#% 766199
#% 769877
#% 788090
#% 788107
#% 810014
#% 870902
#! Entity matching is the problem of deciding if two given mentions in the data, such as "Helen Hunt" and "H. M. Hunt", refer to the same real-world entity. Numerous solutions have been developed, but they have not considered in depth the problem of exploiting integrity constraints that frequently exist in the domains. Examples of such constraints include "a mention with age two cannot match a mention with salary 200K" and "if two paper citations match, then their authors are likely to match in the same order". In this paper we describe a probabilistic solution to entity matching that exploits such constraints to improve matching accuracy. At the heart of the solution is a generative model that takes into account the constraints during the generation process, and provides well-defined interpretations of the constraints. We describe a novel combination of EM and relaxation labeling algorithms that efficiently learns the model, thereby matching mentions in an unsupervised way, without the need for annotated training data. Experiments on several real-world domains show that our solution can exploit constraints to significantly improve matching accuracy, by 3-12% F-1, and that the solution scales up to large data sets.

#index 1269496
#* Discriminative training of Markov logic networks
#@ Parag Singla;Pedro Domingos
#t 2005
#c 10
#% 26722
#% 197387
#% 205391
#% 226437
#% 226495
#% 246831
#% 406493
#% 464434
#% 496116
#% 729913
#% 854636
#% 1650403
#% 1673026
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269497
#* Representing conditional independence using decision trees
#@ Jiang Su;Harry Zhang
#t 2005
#c 10
#% 44876
#% 73374
#% 101213
#% 136350
#% 169657
#% 246832
#% 290482
#% 466086
#% 580510
#% 756039
#% 799040
#% 1650767
#% 1650783
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269498
#* Value functions for RL-based behavior transfer: a comparative study
#@ Matthew E. Taylor;Peter Stone;Yaxin Liu
#t 2005
#c 10
#% 124694
#% 169359
#% 333786
#% 363744
#% 377895
#% 384911
#% 466230
#% 506610
#% 578674
#% 823852
#% 1271966
#% 1272005
#% 1279355
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269499
#* Online resource allocation using decompositional reinforcement learning
#@ Gerald Tesauro
#t 2005
#c 10
#% 265807
#% 272665
#% 307843
#% 452359
#% 495933
#% 565550
#% 820343
#% 1271827
#! This paper considers a novel application domain for reinforcement learning: that of "autonomic computing," i.e. selfmanaging computing systems. RL is applied to an online resource allocation task in a distributed multi-application computing environment with independent time-varying load in each application. The task is to allocate servers in real time so as to maximize the sum of performance-based expected utility in each application. This task may be treated as a composite MDP, and to exploit the problem structure, a simple localized RL approach is proposed, with better scalability than previous approaches. The RL approach is tested in a realistic prototype data center comprising real servers, real HTTP requests, and realistic time-varying demand. This domain poses a number of major challenges associated with live training in a real system, including: the need for rapid training, exploration that avoids excessive penalties, and handling complex, potentially non-Markovian system effects. The early results are encouraging: in overnight training, RL performs as well as or slightly better than heavily researched model-based approaches derived from queuing theory.

#index 1269500
#* Inducing hierarchical process models in dynamic domains
#@ Ljupčo Todorovski;Will Bridewell;Oren Shiran;Pat Langley
#t 2005
#c 10
#% 1116
#% 109848
#% 135552
#% 348816
#% 451031
#% 466431
#% 1042867
#! Research on inductive process modeling combines background knowledge with time-series data to construct explanatory models, but previous work has placed few constraints on search through the model space. We present an extended formalism that organizes process knowledge in a hierarchical manner, and we describe HIPM, a system that carries out constrained search for hierarchical process models. We report experiments that suggest this approach produces more accurate and plausible models with less effort. We conclude by discussing related research and directions for future work.

#index 1269501
#* Software testing by active learning for commercial games
#@ Gang Xiao;Finnegan Southey;Robert C. Holte;Dana Wilkinson
#t 2005
#c 10
#% 116165
#% 136350
#% 169717
#% 169942
#% 205245
#% 233618
#% 283138
#% 349909
#% 381858
#% 399954
#% 424624
#% 440675
#% 466095
#% 735358
#% 765387
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269502
#* Unsupervised and semi-supervised multi-class support vector machines
#@ Linli Xu;Dale Schuurmans
#t 2005
#c 10
#% 205305
#% 304876
#% 393059
#% 460812
#% 466263
#% 722816
#% 757953
#% 763697
#% 770763
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269503
#* Learning planning rules in noisy stochastic worlds
#@ Luke S. Zettlemoyer;Hanna M. Pasula;Leslie Pack Kaelbling
#t 2005
#c 10
#% 239778
#% 396021
#% 425074
#% 544926
#% 1289577
#! We present an algorithm for learning a model of the effects of actions in noisy stochastic worlds. We consider learning in a 3D simulated blocks world with realistic physics. To model this world, we develop a planning representation with explicit mechanisms for expressing object reference and noise. We then present a learning algorithm that can create rules while also learning derived predicates, and evaluate this algorithm in the blocks world simulator, demonstrating that we can learn rules that effectively model the world dynamics.

#index 1269504
#* Hidden naive Bayes
#@ Harry Zhang;Liangxiao Jiang;Jiang Su
#t 2005
#c 10
#% 136350
#% 246831
#% 246832
#% 290482
#% 321059
#% 771837
#% 799040
#! The conditional independence assumption of naive Bayes essentially ignores attribute dependencies and is often violated. On the other hand, although a Bayesian network can represent arbitrary attribute dependencies, learning an optimal Bayesian network from data is intractable. The main reason is that learning the optimal structure of a Bayesian network is extremely time consuming. Thus, a Bayesian model without structure learning is desirable. In this paper, we propose a novel model, called hidden naive Bayes (HNB). In an HNB, a hidden parent is created for each attribute which combines the influences from all other attributes. We present an approach to creating hidden parents using the average of weighted one-dependence estimators. HNB inherits the structural simplicity of naive Bayes and can be easily learned without structure learning. We propose an algorithm for learning HNB based on conditional mutual information. We experimentally test HNB in terms of classification accuracy, using the 36 UCI data sets recommended by Weka (Witten & Frank 2000), and compare it to naive Bayes (Langley, Iba, & Thomas 1992), C4.5 (Quinlan 1993), SBC (Langley & Sage 1994), NBTree (Kohavi 1996), CL-TAN (Friedman, Geiger, & Goldszmidt 1997), and AODE (Webb, Boughton, & Wang 2005). The experimental results show that HNB outperforms naive Bayes, C4.5, SBC, NBTree, and CL-TAN, and is competitive with AODE.

#index 1269505
#* Finite sample error bound for Parzen windows
#@ Peng Zhang;Jing Peng;Norbert Riedel
#t 2005
#c 10
#% 80995
#% 91872
#% 309208
#% 376266
#% 729437
#% 763698
#! Parzen Windows as a nonparametric method has been applied to a variety of density estimation as well as classification problems. Similar to nearest neighbor methods. Parzen Windows does not involve learning. While it converges to true but unknown probability densities in the asymptotic limit, there is a lack of theoretical analysis on its performance with finite samples. In this paper we establish a finite sample error bound for Parzen Windows. We first show that Parzen Windows is an approximation to regularized least squares (RLS) methods that have been well studied in statistical learning theory. We then derive the finite sample error bound for Parzen Windows, and discuss the properties of the error bound and its relationship to the error bound for RLS. This analysis provides interesting insight to Parzen Windows as well as the nearest neighbor method from the point of view of learning theory. Finally, we provide empirical results on the performance of Parzen Windows and other methods such as nearest neighbors, RLS and SVMs on a number of real data sets. These results corroborate well our theoretical analysis.

#index 1269506
#* Cross-modal clustering
#@ Michael H. Coen
#t 2005
#c 10
#% 60576
#% 237984
#% 252472
#% 370216
#% 855557
#! This paper presents a self-supervised algorithm for learning perceptual structures based upon correlations in different sensory modalities. The brain and cognitive sciences have gathered an enormous body of neurological and phenomenological evidence in the past half century that demonstrates the extraordinary degree of interaction between sensory modalities during the course of ordinary perception. This paper presents a new framework for creating artificial perceptual systems inspired by these findings, where the primary architectural motif is the cross-modal transmission of perceptual information to enhance each sensory channel individually. The basic hypothesis underlying this approach is that the world has regularities - natural laws tend to correlate physical properties - and biological perceptory systems have evolved to take advantage of this. They share information continually and opportunistically across seemingly disparate perceptual channels, not epiphenomenologically, but rather as a fundamental component of normal perception. It is therefore essential that their artificial counterparts be able to share information synergistically within their perceptual channels, if they are to approach degrees of biological sophistication. This paper is a preliminary step in that direction.

#index 1269507
#* A computational model of the cerebral cortex
#@ Thomas Dean
#t 2005
#c 10
#% 3513
#% 6199
#% 191855
#% 227142
#% 277396
#% 292235
#% 431471
#% 450888
#% 812582
#% 1562580
#% 1562581
#! Our current understanding of the primate cerebral cortex (neocortex) and in particular the posterior, sensory association cortex has matured to a point where it is possible to develop a family of graphical models that capture the structure, scale and power of the neocortex for purposes of associative recall, sequence prediction and pattern completion among other functions. Implementing such models using readily available computing clusters is now within the grasp of many labs and would provide scientists with the opportunity to experiment with both hard-wired connection schemes and structure-learning algorithms inspired by animal learning and developmental studies. While neural circuits involving structures external to the neocortex such as the thalamic nuclei are less well understood, the availability of a computational model on which to test hypotheses would likely accelerate our understanding of these circuits. Furthermore, the existence of an agreed-upon cortical substrate would not only facilitate our understanding of the brain but enable researchers to combine lessons learned from biology with state-of-the-art graphical-model and machine-learning techniques to design hybrid systems that combine the best of biological and traditional computing approaches.

#index 1269508
#* Data-driven MCMC for learning and inference in switching linear dynamic systems
#@ Sang Min Oh;James M. Rehg;Tucker Balch;Frank Dellaert
#t 2005
#c 10
#% 315282
#% 334631
#% 349211
#% 528327
#% 529185
#% 592134
#% 857094
#% 1763210
#% 1828385
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269509
#* Function-based classification from 3D data via generic and symbolic models
#@ Michael Pechuk;Octavian Soldea;Ehud Rivlin
#t 2005
#c 10
#% 104479
#% 165460
#% 195101
#% 208702
#% 359641
#% 376266
#% 1272170
#! We propose a novel scheme for function-based classification of objects in 3D images. The classification process calls for constructing a generic multi-level hierarchical description of object classes in terms of functional components. Functionality is derived from a large set of geometric attributes and relationships between object parts. Initially, the input range data describing each object instance is segmented, each object part is labeled as one of a few possible primitives, and each group of primitive parts is tagged by a functional symbol. Connections between primitive parts and functional parts at the same level in the hierarchy are labeled as well. Then, the generic multi-level hierarchical description of object classes is built using the functionalities of a number of object instances. During classification, a search through a finite graph using a probabilistic fitness measure is performed to find the best assignment of object parts to the functional structures of each class. An object is assigned to a class providing the highest fitness value. The scheme does not require a-priori knowledge about any class. We tested the proposed scheme on a database of about one thousand different 3D objects. The results show high accuracy in classification.

#index 1269510
#* Learning static object segmentation from motion segmentation
#@ Michael G. Ross;Leslie Pack Kaelbling
#t 2005
#c 10
#% 44876
#% 323242
#% 444051
#% 457943
#% 592143
#% 669227
#% 744799
#% 792682
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269511
#* Semantic scene concept learning by an autonomous agent
#@ Weiyu Zhu
#t 2005
#c 10
#% 44876
#% 188519
#% 227145
#% 229349
#% 298768
#% 373978
#% 443975
#% 457912
#% 729437
#% 1272033
#! Scene understanding addresses the issue of "what a scene contains". Existing research on scene understanding is typically focused on classifying a scene into classes that are of the same category type. These approaches, although they solve some scene-understanding tasks successfully, in general fail to address the semantics in scene understanding. For example, how does an agent learn the concept label "red" and "ball" without being told that it is a color or a shape label in advance? To cope with this problem, we have proposed a novel research called semantic scene concept learning. Our proposed approach models the task of scene understanding as a "multi-labeling" classification problem. Each scene instance perceived by the agent may receive multiple labels coming from different concept categories, where the goal of learning is to let the agent discover the semantic meanings, i.e., the set of relevant visual features, of the scene labels received. Our preliminary experiments have shown the effectiveness of our proposed approach in solving this special intra- and inter-category mixing learning task.

#index 1269512
#* A particle filtering based approach to approximating interactive POMDPs
#@ Prashant Doshi;Piotr J. Gmytrasiewicz
#t 2005
#c 10
#% 188086
#% 196353
#% 234979
#% 418730
#% 528339
#% 564834
#% 823885
#% 1272071
#! POMDPs provide a principled framework for sequential planning in single agent settings. An extension of POMDPs to multi agent settings, called interactive POMDPs (I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief systems which represent an agent's belief about the physical world, about beliefs of the other agent(s), about their beliefs about others' beliefs, and so on. This modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute. We describe a method for obtaining approximate solutions to IPOMDPs based on particle filtering (PF). We utilize the interactive PF which descends the levels of interactive belief hierarchies and samples and propagates beliefs at each level. The interactive PF is able to deal with the belief space complexity, but it does not address the policy space complexity. We provide experimental results and chart future work.

#index 1269513
#* Efficient maximization in solving POMDPs
#@ Zhengzhu Feng;Shlomo Zilberstein
#t 2005
#c 10
#% 101869
#% 578702
#% 646457
#% 646971
#% 788053
#% 1271954
#% 1650702
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269514
#* Extending continuous time Bayesian networks
#@ Karthik Gopalratnam;Henry Kautz;Daniel S. Weld
#t 2005
#c 10
#% 75936
#% 1250174
#% 1650390
#% 1673032
#! Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller 2002; 2003), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents: an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.

#index 1269515
#* Planning in models that combine memory with predictive representations of state
#@ Michael R. James;Satinder Singh
#t 2005
#c 10
#% 527859
#% 646958
#% 788097
#% 1279471
#% 1289468
#% 1476294
#% 1650702
#! Models of dynamical systems based on predictive state representations (PSRs) use predictions of future observations as their representation of state. A main departure from traditional models such as partially observable Markov decision processes (POMDPs) is that the PSR-model state is composed entirely of observable quantities. PSRs have recently been extended to a class of models called memory-PSRs (mPSRs) that use both memory of past observations and predictions of future observations in their state representation. Thus, mPSRs preserve the PSR-property of the state being composed of observable quantities while potentially revealing structure in the dynamical system that is not exploited in PSRs. In this paper, we demonstrate that the structure captured by mPSRs can be exploited quite naturally for stochastic planning based on value-iteration algorithms. In particular, we adapt the incremental-pruning (IP) algorithm defined for planning in POMDPs to mPSRs. Our empirical results show that our modified IP on mPSRs outperforms, in most cases, IP on both PSRs and POMDPs.

#index 1269516
#* Risk-sensitive planning with one-switch utility functions: value iteration
#@ Yaxin Liu;Sven Koenig
#t 2005
#c 10
#% 47149
#% 105866
#% 299347
#% 363744
#% 532955
#% 564970
#% 578779
#% 739029
#% 828447
#% 1790393
#! Decision-theoretic planning with nonlinear utility functions is important since decision makers are often risk-sensitive in high-stake planning situations. One-switch utility functions are an important class of nonlinear utility functions that can model decision makers whose decisions change with their wealth level. We study how to maximize the expected utility of a Markov decision problem for a given one-switch utility function, which is difficult since the resulting planning problem is not decomposable. We first study an approach that augments the states of the Markov decision problem with the wealth level. The properties of the resulting infinite Markov decision problem then allow us to generalize the standard risk-neutral version of value iteration from manipulating values to manipulating functions that map wealth levels to values. We use a probabilistic blocks-world example to demonstrate that the resulting risk-sensitive version of value iteration is practical.

#index 1269517
#* Samuel meets Amarel: automating value function approximation using global state space analysis
#@ Sridhar Mahadevan
#t 2005
#c 10
#% 313959
#% 384911
#% 393786
#% 451937
#% 527994
#% 578699
#% 593842
#% 695927
#% 715736
#% 732552
#% 734920
#% 765552
#% 770777
#% 1272002
#% 1279356
#! Most work on value function approximation adheres to Samuel's original design: agents learn a task-specific value function using parameter estimation, where the approximation architecture (e.g, polynomials) is specified by a human designer. This paper proposes a novel framework generalizing Samuel's paradigm using a coordinate-free approach to value function approximation. Agents learn both representations and value functions by constructing geometrically customized task-independent basis functions that form an orthonormal set for the Hilbert space of smooth functions on the underlying state space manifold. The approach rests on a technical result showing that the space of smooth functions on a (compact) Riemanian manifold has a discrete spectrum associated with the Laplace-Beltrami operator. In the discrete setting, spectral analysis of the graph Laplacian yields a set of geometrically customized basis functions for approximating and decomposing value functions. The proposed framework generalizes Samuel's value function approximation paradigm by combining it with a formalization of Saul Amarel's paradigm of representation learning through global state space analysis.

#index 1269518
#* Error bounds for approximate value iteration
#@ Rémi Munos
#t 2005
#c 10
#% 229931
#% 363744
#% 384911
#% 393786
#% 464624
#% 527994
#% 734920
#% 739715
#% 1188997
#% 1289239
#! Approximate Value Iteration (AVI) is an method for solving a Markov Decision Problem by making successive calls to a supervised learning (SL) algorithm. Sequence of value representations Vn are processed iteratively by Vn+1 = ATVn where T is the Bellman operator and A an approximation operator. Bounds on the error between the performance of the policies induced by the algorithm and the optimal policy are given as a function of weighted Lp-norms (p ≥ 1) of the approximation errors. The results extend usual analysis in L∞-norm, and allow to relate the performance of AVI to the approximation power (usually expressed in Lp-norm, for p = 1 or 2) of the SL algorithm. We illustrate the tightness of these bounds on an optimal replacement problem.

#index 1269519
#* Geometric variance reduction in Markov chains: application to value function and gradient estimation
#@ Rémi Munos
#t 2005
#c 10
#% 124687
#% 189854
#% 229931
#% 285799
#% 285957
#% 300360
#% 431473
#% 449561
#% 720749
#% 793249
#% 833360
#% 1272385
#! We study a sequential variance reduction technique for Monte Carlo estimation of functionals in Markov Chains. The method is based on designing sequential control variates using successive approximations of the function of interest V. Regular Monte Carlo estimates have a variance of O(1/N), where N is the number of samples. Here, we obtain a geometric variance reduction O(ρN) (with ρ V - AV, where A is an approximation operator linear in the values. Thus, if V belongs to the right approximation space (i.e. AV = V), the variance decreases geometrically to zero. An immediate application is value function estimation in Markov chains, which may be used for policy evaluation in policy iteration for Markov Decision Processes. Another important domain, for which variance reduction is highly needed, is gradient estimation, that is computing the sensitivity ∂α, V of the performance measure V with respect to some parameter α of the transition probabilities. For example, in parametric optimization of the policy, an estimate of the policy gradient is required to perform a gradient optimization method. We show that, using two approximations, the value function and the gradient, a geometric variance reduction is also achieved, up to a threshold that depends on the approximation errors of both of those representations.

#index 1269520
#* Modeling form for on-line following of musical performances
#@ Bryan Pardo;William Birmingham
#t 2005
#c 10
#% 137711
#% 173501
#% 270994
#% 828256
#! We define a generalized strategy eliminability criterion for bimatrix games that considers whether a given strategy is eliminable relative to given dominator & eliminee subsets of the players' strategies. We show that this definition spans a spectrum ...

#index 1269521
#* Improving action selection in MDP's via knowledge transfer
#@ Alexander A. Sherstov;Peter Stone
#t 2005
#c 10
#% 169359
#% 252329
#% 384911
#% 464296
#% 1271827
#% 1279355
#% 1289241
#% 1650589
#! Temporal-difference reinforcement learning (RL) has been successfully applied in several domains with large state sets. Large action sets, however, have received considerably less attention. This paper demonstrates the use of knowledge transfer between related tasks to accelerate learning with large action sets. We introduce action transfer, a technique that extracts the actions from the (near-)optimal solution to the first task and uses them in place of the full action set when learning any subsequent tasks. When optimal actions make up a small fraction of the domain's action set, action transfer can substantially reduce the number of actions and thus the complexity of the problem. However, action transfer between dissimilar tasks can be detrimental. To address this difficulty, we contribute randomized task perturbation (RTP), an enhancement to action transfer that makes it robust to unrepresentative source tasks. We motivate RTP action transfer with a detailed theoretical analysis featuring a formalism of related tasks and a bound on the suboptimality of action transfer. The empirical results in this paper show the potential of RTP action transfer to substantially expand the applicability of RL to problems with large action sets.

#index 1269522
#* Planning and execution with phase transitions
#@ Håkan L. S. Younes
#t 2005
#c 10
#% 36305
#% 147620
#% 233849
#% 284989
#% 314843
#% 363744
#% 788054
#% 788064
#% 1250235
#% 1272002
#% 1650297
#! We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.

#index 1269523
#* Proceedings of the 20th national conference on Artificial intelligence - Volume 3
#@ Anthony Cohn
#t 2005
#c 10

#index 1269524
#* Scaling up word sense disambiguation via parallel texts
#@ Yee Seng Chan;Hwee Tou Ng
#t 2005
#c 10
#% 748703
#% 815895
#% 817476
#% 817596
#% 818059
#% 854634
#% 854641
#% 938689
#% 938691
#% 1250278
#% 1910924
#% 1910925
#% 1910936
#% 1910940
#% 1910951
#! A critical porblem faced by current supervised WSD systems is the lack or manually annotated training data. Tackling this data acquisition bottleneck is crucial, in order to build high-accuracy and wide-coverage WSD systems. In this paper, we show that the approach of automatically gathering training examples from parallel texts is scalable to a large set of nouns. We conducted evaluation on the nouns of SENSEVAL-2 English all-words task, using fine-grained sense scoring. Our evaluation shows that training on examples gathered from 680MB of parallel texts achieves accuracy comparable to the best system of SENSEVAL-2 English all-words task, and significantly outperforms the baseline of always choosing sense 1 of WordNet.

#index 1269525
#* An inference model for semantic entailment in natural language
#@ Rodrigo De Salvo Braz;Roxana Girju;Vasin Punyakanok;Dan Roth;Mark Sammons
#t 2005
#c 10
#% 19913
#% 19914
#% 33376
#% 342630
#% 708948
#% 748860
#% 816175
#% 855160
#% 935898
#% 939845
#% 995463
#% 1250185
#% 1289529
#% 1393854
#! Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.

#index 1269526
#* A probabilistic classification approach for lexical textual entailment
#@ Oren Glickman;Ido Dagan;Moshe Koppel
#t 2005
#c 10
#% 145393
#% 220132
#% 300045
#% 311027
#% 723243
#% 741891
#% 815799
#% 815843
#% 855158
#! The textual entailment task - determining if a given text entails a given hypothesis - provides an abstraction of applied semantic inference. This paper describes first a general generative probabilistic setting for textual entailment. We then focus on the sub-task of recognizing whether the lexical concepts present in the hypothesis are entailed from the text. This problem is recast as one of text categorization in which the classes are the vocabulary words. We make novel use of Naïve Bayes to model the problem in an entirely unsupervised fashion. Empirical tests suggest that the method is effective and compares favorably with state-of-the-art heuristic scoring approaches.

#index 1269527
#* Clustering and classifying person names by origin
#@ Fei Huang;Stephan Vogel;Alex Waibel
#t 2005
#c 10
#% 252011
#% 279755
#% 740915
#% 746879
#% 855302
#% 1271423
#! In natural language processing, information about a person's geographical origin is an important feature for name entity transliteration and question answering. We propose a language-independent name origin clustering and classification framework. Provided with a small amount of bilingual name translation pairs with labeled origins, we measure origin similarities based on the perplexities of name character language and translation models. We group similar origins into clusters, then train a Bayesian classifier with different features. It achieves 84% classification accuracy with source names only, and 91% with both source and target name pairs. We apply the origin clustering and classification technique to a name transliteration task. The cluster-specific transliteration model dramatically improves the transliteration accuracy from 3.8% to 55%, reducing the transliteration character error rate from 50.3 to 13.5. Adding more unlabeled name pairs to the cluster-specific name transliteration model further improves the transliteration accuracy.

#index 1269528
#* Learning to transform natural to formal languages
#@ Rohit J. Kate;Yuk Wah Wong;Raymond J. Mooney
#t 2005
#c 10
#% 196896
#% 396021
#% 428249
#% 452991
#% 722803
#% 843647
#% 939669
#% 1344851
#% 1476277
#! This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal grammar for the target representation language and learns transformation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora. one which maps English instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps English U.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains.

#index 1269529
#* Impact of linguistic analysis on the semantic graph coverage and learning of document extracts
#@ Jure Leskovec;Natasa Milic-Frayling;Marko Grobelnik
#t 2005
#c 10
#% 194251
#% 266370
#% 280838
#% 938761
#! Automatic document summarization is a problem of creating a document surrogate that adequately represents the full document content. We aim at a summarization system that can replicate the quality of summaries created by humans. In this paper we investigate the machine learning method for extracting full sentences from documents based on the document semantic graph structure. In particular, we explore how the Support Vector Machines (SVM) learning method is affected by the quality of linguistic analyses and the corresponding semantic graph representations. We apply two types of linguistic analysis: (1) a simple part-of-speech tagging of noun phrases and verbs and (2) full logical form analysis which identifies Subject-Predicate-Object triples, and then build the semantic graphs. We train the SVM classifier to identify summary nodes and use these nodes to extract sentences. Experiments with the DUC 2002 and CAST datasets show that the SVM based extraction of sentences does not differ significantly for the simple and the sophisticated syntactic analysis. In both cases the graph attributes used in learning are essential for the classifier performance and the quality of extracted summaries.

#index 1269530
#* Unsupervised multilingual word sense disambiguation via an interlingua
#@ Kornél Markó;Stefan Schulz;Udo Hahn
#t 2005
#c 10
#% 132648
#% 144031
#% 184074
#% 239319
#% 495919
#% 741080
#% 741084
#% 748337
#% 748354
#% 748703
#% 756952
#% 818269
#% 854641
#% 1250183
#% 1279329
#! We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.

#index 1269531
#* Supervised ranking for pronoun resolution: some recent improvements
#@ Vincent Ng
#t 2005
#c 10
#% 184073
#% 269217
#% 740418
#% 740994
#% 740995
#% 747815
#% 748191
#% 748541
#% 756121
#% 757292
#% 786555
#% 815292
#% 817441
#% 938669
#% 1290034
#! A recently-proposed machine learning approach to reference resolution -- the twin-candidate approach -- has been shown to be more pormising than the traditional single-candidate approach. This paper presents a pronoun interpretation system that extends the twin-candidate framework by (1) equippmg it with the ability to identify non-referential pronouns. (2) training different models for handling different types of pronouns, and (3) incorporating linguistic knowledge sources that are generally not employed in traditional pronoun resolvers. The resulting system, when evaluated on a standard coreference corpus, outpreforms not only the original twin-candidate approach but also a state-of-the-art pronoun resolver.

#index 1269532
#* Cross-lingual bootstrapping of semantic lexicons: the case of FrameNet
#@ Sebastian Padó;Mirella Lapata
#t 2005
#c 10
#% 283180
#% 452991
#% 579944
#% 747891
#% 815112
#% 815912
#% 816076
#% 816170
#% 817420
#% 817486
#% 939748
#% 939782
#! This paper considers the problem of unsupervised semantic lexicon acquisition. We introduce a fully automatic approach which exploits parallel corpora, relies on shallow text properties, and is relatively inexpensive. Given the English FrameNet lexicon, our method exploits word alignments to generate frame candidate lists for new languages, which are subsequently pruned automatically using a small set of linguistically motivated filters. Evaluation shows that our approach can produce high-precision multilingual FrameNet lexicons without recourse to bilingual dictionaries or deep syntactic and semantic analysis.

#index 1269533
#* Word sense disambiguation with semi-supervised learning
#@ Thanh Phong Pham;Hwee Tou Ng;Wee Sun Lee
#t 2005
#c 10
#% 252011
#% 311027
#% 466263
#% 565545
#% 815908
#% 818059
#% 854641
#! Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on 29 nouns of Senseval-2 (SE2) English lexical sample task and SE2 English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.

#index 1269534
#* Robust textual inference via learning and abductive reasoning
#@ Rajat Raina;Andrew Y. Ng;Christopher D. Manning
#t 2005
#c 10
#% 26722
#% 145393
#% 198058
#% 708948
#% 755834
#% 816175
#% 817472
#% 1250278
#% 1275285
#! We present a system for textual inference (the task of inferring whether a sentence follows from another text) that uses learning and a logical-formula semantic representation of the text. More precisely, our system begins by parsing and then transforming sentences into a logical formula-like representation similar to the one used by (Harabagiu et al., 2000). An abductive theorem prover then tries to find the minimum "cost" set of assumptions necessary to show that one statement follows from the other. These costs reflect how likely different assumptions are, and are learned automatically using information from syntactic/semantic features and from linguistic resources such as WordNet. If one sentence follows from the other given only highly plausible, low cost assumptions, then we conclude that it can be inferred. Our approach can be viewed as combining statistical machine learning and classical logical reasoning, in the hope of marrying the robustness and scalability of learning with the preciseness and elegance of logical theorem proving. We give experimental results from the recent PASCAL RTE 2005 challenge competition on recognizing textual inferences, where a system using this inference algorithm achieved the highest confidence weighted score.

#index 1269535
#* Exploiting subjectivity classification to improve information extraction
#@ Ellen Riloff;Janyce Wiebe;William Phillips
#t 2005
#c 10
#% 278109
#% 376266
#% 531459
#% 577246
#% 577355
#% 613977
#% 722308
#% 727877
#% 747945
#% 757422
#% 769892
#% 817446
#% 843652
#% 855279
#% 855282
#% 938687
#% 1289318
#% 1290067
#% 1476276
#% 1478939
#% 1700552
#! Information extraction (IE) systems are prone to false hits for a variety of reasons and we observed that many of these false hi ts occur in sentences that contain subjective language (e.g., opinions, emotions, and sentiments). Motivated by these observations, we explore the idea of using subjectivity analysis to improve the precision of information extraction systems. In this paper, we describe an IE system that uses a subjective sentence classifier to filter its extractions. We experimented with several different strategies for using the subjectivity classifications, including an aggressive strategy that discards all extractions found in subjective sentences and more complex strategies that selectively discard extractions. We evaluated the performance of these different approaches on the MUC-4 terrorism data set. We found that indiscriminately filtering extractions from subjective sentences was overly aggressive, but more selective filtering strategies improved IE precision with minimal recall loss.

#index 1269536
#* Dependency parsing with dynamic Bayesian network
#@ Virginia Savova;Leonid Peshkin
#t 2005
#c 10
#% 215631
#% 716892
#% 740916
#% 816218
#% 938713
#! Exact parsing with finite state automata is deemed in-apropriate because of the unbounded non-locality languages overwhelmingly exhibit. We propose a way to structure the parsing task in order to make it amenable to local classification methods. This allows us to build a Dynamic Bayesian Network which uncovers the syntactic dependency structure of English sentences. Experiments with the Wall Street Journal demonstrate that the model successfully learns from labeled data.

#index 1269537
#* Spotting subsequences matching a HMM using the average observation probability criteria with application to keyword spotting
#@ Marius Calin Silaghi
#t 2005
#c 10
#% 174161
#% 252472
#% 395470
#! This paper addresses the problem of detecting keywords in unconstrained speech. The proposed algorithms search for the speech segment maximizing the average observation probability along the most likely path in the hypothesized keyword model. As known, this approach (sometimes referred to as sliding model method) requires a relaxation of the begin/endpoints of the Viterbi matching, as well as a time normalization of the resulting score. This makes solutions complex (i.e., LN2/2 basic operations for keyword HMM models with L states and utterances with N frames). We present here two altemative (quite simple and efficient) solutions to this problem. a) First we provide a method that finds the optimal segmentation according to the criteria of maximizing the average observation probability. It uses Dynamic Programming as a step, but does not require scoring for all possible begin/endpoints. While the worst case remains O(LN2), this technique converged in at most 3(L+2)N basic operations in each experiment for two very different applications. b) The second proposed algorithm does not provide a segmentation but can be used for the decision problem of whether the utterance should be classified as containing the keyword or not (provided a predefined threshold on the acceptable average observation probability). This allows the algorithm to be even faster, with fix cost of (L+2)N.

#index 1269538
#* Capturing expression using linguistic information
#@ Özlem Uzuner;Boris Katz
#t 2005
#c 10
#% 94227
#% 290482
#% 423359
#% 724773
#% 733835
#% 742368
#% 756561
#% 828453
#! Recognizing similarities between literary works for copyright infringement detection requires evaluating similarity in the expression of content. Copyright law protects expression of content; similarities in content alone are not enough to indicate infringement. Expression refers to the way people convey particular information; it captures both the information and the manner of its presentation. In this paper, we present a novel set of linguistically informed features that provide a computational definition of expression and that enable accurate recognition of individual titles and their paraphrases more than 80% of the time. In comparison, baseline features, e.g., tfidf-weighted keywords, function words, etc., give an accuracy of at most 53%. Our computational definition of expression uses linguistic features that are extracted from POS-tagged text using context-free grammars, without incurring the computational cost of full parsers. The results indicate that informative linguistic features do not have to be computationally prohibitively expensive to extract.

#index 1269539
#* State agnostic planning graphs and the application to belief-space planning
#@ William Cushing;Daniel Bryce
#t 2005
#c 10
#% 25470
#% 109517
#% 266387
#% 275214
#% 337981
#% 345431
#% 386158
#% 544784
#% 544930
#% 578728
#% 676362
#% 1271962
#% 1272016
#% 1272020
#% 1272066
#% 1272379
#% 1272392
#% 1289212
#% 1289551
#% 1290109
#! Planning graphs have been shown to be a rich source of heuristic information for many kinds of planners. In many cases, planners must compute a planning graph for each element of a set of states. The naive technique enumerates the graphs individually. This is equivalent to solving an all-pairs shortest path problem by iterating a single-source algorithm over each source. We introduce a structure, the state agnostic planning graph, that directly, solves the all-pairs problem for the relaxation introduced by planning graphs. The technique can also be characterized as exploiting the overlap present in sets of planning graphs. For the purpose of exposition, we first present the technique in classical planning. The more prominent application of tnis technique is in belief-space planning, where an optimization results in drastically improved theoretical complexity. Our experimental evaluation quantifies this performance boost. and demonstrates that heuristic belief-space progression planning using our technique is competitive with the state of t the art.

#index 1269540
#* Genome rearrangement and planning
#@ Esra Erdem;Elisabeth Tillier
#t 2005
#c 10
#% 100159
#% 131357
#% 167629
#% 194648
#% 203120
#% 263205
#% 296170
#% 337980
#% 1289217
#! The genome rearrangement problem is to find the most economical explanation for observed differences between the gene orders of two genomes. Such an explanation is provided in terms of events that change the order of genes in a genome. We present a new approach to the genome rearrangement problem, according to which this problem is viewed as the problem of planning rearrangement events that transform one genome to the other. This method differs from the existing ones in that we can put restrictions on the number of events, specify the cost of events with functions, possibly based on the length of the gene fragment involved, and add constraints controlling search. With this approach. We have described genome rearrangements in the action description language ADL. and studied the evolution of Metazoan mitochondrial genomes and the evolution of Campanulaceae chloroplast genomes using the planner TLPLAN. We have observed that the phylogenies reconstructed using this approach conform with the most widely accepted ones.

#index 1269541
#* Quasi-monotonic segmentation of state variable behavior for reactive control
#@ Will Fitzgerald;Daniel Lemire;Martin Brooks
#t 2005
#c 10
#% 241013
#% 420063
#! Real-world agents must react to changing conditions as they execute planned tasks. Conditions are typically monitored through time series representing state variables. While some predicates on these times series only consider one measure at a time, other predicates, sometimes called episodic predicates, consider sets of measures. We consider a special class of episodic predicates based on segmentation of the the measures into quasi-monotonic intervals where each interval is either quasi-increasing, quasi-decreasing, or quasi-fiat. While being scale-based, this approach is also computational efficient and results can be computed exactly without need for approximation algorithms. Our approach is compared to linear spline and regression analysis.

#index 1269542
#* Validating plans in the context of processes and exogenous events
#@ Maria Fox;Richard Howey;Derek Long
#t 2005
#c 10
#% 296170
#% 519060
#% 587369
#% 785511
#% 1250203
#% 1272008
#! Complex planning domains push the boundaries of the expressive power of planning domain modelling languages. Recent extensions to the standard planning languages have included expressions for temporal, metric and resource structures. Other work has also considered how process models can be incorporated into domain models. In this paper we consider the problem of expressing and validating models containing events which are triggered as a consequence of the action of physical processes. We focus, primarily, on the validation of plans in the context of exogenous events, discussing the modelling, semantic and implementation issues that arise. Events impact not only on plans but on domain models as a whole and we also consider the problems that arise in considering the validation of event structures in domain models.

#index 1269543
#* Fast planning in domains with derived predicates: an approach based on rule-action graphs and local search
#@ Alfonso Gerevini;Alessandro Saetti;Ivan Serina;Paolo Toninelli
#t 2005
#c 10
#% 224480
#% 1272008
#% 1272016
#% 1279350
#% 1289210
#! The ability to express "derived predicates" in the formalization of a planning domain is both practically and theoretically important. In this paper, we propose an approach to planning with derived predicates where the search space consists of "Rule-Action Graphs", particular graphs of actions and rules representing derived predicates. We present some techniques for representing rules and reasoning with them, which are integrated into a method for planning through local search and rule-action graphs. We also propose some new heuristics for guiding the search, and some experimental results illustrating the performance of our approach. Our proposed techniques are implemented in a planner that took part in the fourth International Planning Competition showing good performance in many benchmark problems.

#index 1269544
#* New admissible heuristics for domain-independent planning
#@ Patrik Haslum;Blai Bonet;Héctor Geffner
#t 2005
#c 10
#% 224480
#% 348576
#% 529658
#% 533951
#% 544930
#% 544936
#% 1271962
#! Admissible heuristics are critical for effective domain-independent planning when optimal solutions must be guaranteed. Two useful heuristics are the hm heuristics, which generalize the reachability heuristic underlying the planning graph, and pattern database heuristics. These heuristics, however, have serious limitations: reachability heuristics capture only the cost of critical paths in a relaxed problem, ignoring the cost of other relevant paths, while PDB heuristics, additive or not, cannot accommodate too many variables in patterns, and methods for automatically selecting patterns that produce good estimates are not known. We introduce two refinements of these heuristics: First, the additive hm heuristic which yields an admissible sum of hm heuristics using a partitioning of the set of actions. Second, the constrained PDB heuristic which uses constraints from the original problem to strengthen the lower bounds obtained from abstractions. The new heuristics depend on the way the actions or problem variables are partitioned. We advance methods for automatically deriving additive hm and PDB heuristics from STRIPS encodings. Evaluation shows improvement over existing heuristics in several domains, although, not surprisingly, no heuristic dominates all the others over all domains.

#index 1269545
#* Using domain-configurable search control for probabilistic planning
#@ Ugur Kuter;Dana Nau
#t 2005
#c 10
#% 103050
#% 109935
#% 194647
#% 194652
#% 243697
#% 296170
#% 337981
#% 361730
#% 417703
#% 536408
#% 706874
#% 1250200
#% 1271827
#% 1272019
#% 1478845
#% 1650297
#! We describe how to improve the performance of MDP planning algorithms by modifying them to use the search-control mechanisms of planners such as TLPlan, SHOP2, and TALplanner. In our experiments, modified versions of RTDP, LRTDP, and Value Iteration were exponentially faster than the original algorithms. On the largest problems the original algorithms could solve, the modified ones were about 10,000 times faster. On another set. of problems whose state spaces were more than 14,000 times larger than the original algorithms could solve, the modified algorithms took only about 1/3 second.

#index 1269546
#* Lazy approximation for solving continuous finite-horizon MDPs
#@ Lihong Li;Michael L. Littman
#t 2005
#c 10
#% 178906
#% 317313
#% 363744
#% 384911
#% 393786
#% 425080
#% 644560
#% 788054
#% 1650355
#! Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.

#index 1269547
#* Prottle: a probabilistic temporal planner
#@ Iain Little;Douglas Aberdeen;Sylvie Thiébaux
#t 2005
#c 10
#% 224480
#% 495772
#% 544926
#% 1250231
#% 1272008
#% 1289204
#% 1650355
#! Planning with concurrent durative actions and probabilistic effects, or probabilistic temporal planning, is a relatively new area of research. The challenge is to replicate the success of modern temporal and probabilistic planners with domains that exhibit an interaction between time and uncertainty. We present a general framework for probabilistic temporal planning in which effects, the time at which they occur, and action durations are all probabilistic. This framework includes a search space that is designed for solving probabilistic temporal planning problems via heuristic search, an algorithm that has been tailored to work with it and an effective heuristic based on an extension of the planning graph data structure. Prottle is a planner that implements this framework, and can solve problems expressed in an extension of PDDL.

#index 1269548
#* Augmenting disjunctive temporal problems with finite-domain constraints
#@ Michael D. Moffitt;Bart Peintner;Martha E. Pollack
#t 2005
#c 10
#% 3463
#% 107137
#% 266107
#% 572899
#% 644201
#% 736897
#% 1279348
#% 1675274
#! We present a general framework for augmenting instances of the Disjunctive Temporal Problem (DTP) with finite-domain constraints. In this new formalism, the bounds of the temporal constraints become conditional on the finite-domain assignment. This hybridization makes it possible to reason simultaneously about temporal relationships between events as well as their nontemporal properties. We provide a special case of this hybridization that allows reasoning about a limited form of spatial constraints; namely, the travel time induced by the locations of a set of activities. We develop a least-commitment algorithm for efficiently finding solutions to this combined constraint system and provide empirical results demonstrating the effectiveness of our approach.

#index 1269549
#* Temporal dynamic controllability revisited
#@ Paul Morris;Nicola Muscettola
#t 2005
#c 10
#% 70370
#% 107137
#% 262737
#% 1289215
#! An important issue for temporal planners is the ability to handle temporal uncertainty. We revisit the question of how to determine whether a given set of temporal requirements are feasible in the light of uncertain durations of some processes. In particular, we consider how best to determine whether a network is Dynamically Controllable, i.e., whether a dynamic strategy exisls for executing the network that is guaranteed to satisfy the requirements. Previous work has shown the existence of a pseudo-polynomial algorithm for testing Dynamic Controllability. Here, we simplify the previous framework, and present a strongly polynomial algorithm with a termination criterion based on the structure of the network.

#index 1269550
#* Exploiting temporal flexibility to obtain high quality schedules
#@ Nicola Policella;Xiaofang Wang;Stephen F. Smith;Angelo Oddi
#t 2005
#c 10
#% 174102
#% 529677
#% 1289192
#! We consider a schedule optimization problem where each activity to be scheduled has a duration-dependent quality profile, and activity durations must be determined that maximize overall quality within given deadline and resource constraints. To solve this quality maximization problem, prior work has proposed a hybrid search scheme, where a linear programming solver for optimally setting the durations of temporally related activities is embedded within a larger search procedure that incrementally posts sequencing constraints to resolve resource conflicts. Under this approach, dual concerns of establishing feasibility and optimizing quality are addressed in an integrated fashion. In this paper, we propose an alternative approach, where feasibility and optimization concerns are treated separately: first, we establish a resource-feasible partial order schedule, assuming minimum durations for all activities; second, these fixed duration constraints are relaxed and quality optimal durations are determined, Experimental results indicate a tradeoff: when resource capacity constraints are loose, the integrated hybrid approach performs comparably to the separated scheme. However, in problems with tighter capacity constraints we find that separation of concerns enables both better solving capability and higher quality results. Following from these results, we discuss potential synergy between problem objectives of maintaining temporal flexibility and maximizing quality.

#index 1269551
#* Planning for stream processing systems
#@ Anton Riabov;Zhen Liu
#t 2005
#c 10
#% 623611
#% 767443
#% 1271818
#% 1272017
#% 1290109
#! With the advent of compositional programming models in computer Science, applying planning technologies to automatically build workflows for solving large and complex problems in such a paradigm becomes not only technically appealing but also feasible approach. The application areas that will benefit from automatic composition include, among others, Web services, Grid computing and stream processing systems. Although the classical planning formalism is expressive enough to describe planning problems that arise in a large variety of different applications, it can pose significant limitations on planner performance in compositional applications, in particular, in stream processing systems. In this paper we exlend the classical planning formalism by introducing new language constructs that support the structure of stream processing domains. Exposing this structure to the planner can result in dramatic performance improvements: our experiments show exponential planning time reduction in comparison to most recent metric planners.

#index 1269552
#* Conformant planning for domains with constraints: a new approach
#@ Tran Cao Son;Phan Huy Tu;Michael Gelfond;A. Ricardo Morales
#t 2005
#c 10
#% 266386
#% 318489
#% 322911
#% 398251
#% 495996
#% 572371
#% 655323
#% 789560
#% 1271828
#% 1272399
#! The paper presents a pair of new conformant planners, CPApc and CPAph, based on recent developments in theory of action and change. As an input the planners take a domain description D in action language AL which allows state constraints (non-stratified axioms), together with a set of CNF formulae describing the initial state, and a set of literals representing the goal. We propose two approximations of the transition diagram T defined by D. Both approximations are deterministic transition functions and can be computed efficiently. Moreover they are sound (and sometimes complete) with respect to T. In its search for a plan, an approximation based planner analyses paths of an approximation instead of that of T. CPApc and CPAph are forward, best first search planners based on this idea. We compare them with two state-of-the-art conformant planners, KACMBP and Conformant-FF (CFF), over benchmarks in the literature, and over two new domains. One has large number of state constraints and another has a high degree of incompleteness. Our planners perform reasonably well in benchmark domains and outperform KACMBP and CFF in the first domain while still working well with the second one. Our experimental result shows that having an integral part of a conformant planner to deal with state constraints directly can significantly improve its performance extending a similar claim for classical planners in (Thiebaux. Hoffmann, & Nebel 2003).

#index 1269553
#* Learning measures of progress for planning domains
#@ SungWook Yoon;Alan Fern;Robert Givan
#t 2005
#c 10
#% 126926
#% 135539
#% 289949
#% 449559
#% 466729
#% 578730
#% 770806
#% 1271962
#% 1499582
#% 1650413
#! We study an approach to learning heuristics for planning domains from example solutions. There has been little work on learning heuristics for the types of domains used in deterministic and stochastic planning competitions. Perhaps one reason for this is the challenge of providing a compact heuristic language that facilitates learning. Here we introduce a new representation for heuristics based on lists of set expressions described using taxonomic syntax. Next, we review the idea of a measure of progress (parmar 2002), which is any heuristic that is guaranteed to be improvable at every state. We take finding a measure of progress as our learning goal, and describe a simple learning algorithm for this purpose. We evaluate our approach across a range of deterministic and stochastic planning-competition domains. The results show that often greedily following the learned heuristic is highly effective. We also show our heuristic can be combined with learned rule-based policies, producing still stronger results.

#index 1269554
#* Exploiting the structure of hierarchical plans in temporal constraint propagation
#@ Neil Yorke-Smith
#t 2005
#c 10
#% 107137
#% 178934
#% 266108
#% 453072
#% 618512
#% 644201
#% 722503
#% 743461
#% 1272019
#% 1290110
#% 1499522
#! Quantitative temporal constraints are an essential requirement for many planning domains. The HTN planning paradigm has proven to be better suited than other approaches to many applications. To date, however, efficiently integrating temporal reasoning with HTN planning has been little explored. This paper describes a means to exploit the structure of a HTN plan in performing temporal propagation on an associated Simple Temporal Network. By exploiting the natural restriction on permitted temporal constraints, the time complexity of propagation can be sharply reduced, while completeness of the inference is maintained. Empirical results indicate an order of magnitude improvement on real-world plans.

#index 1269555
#* Sensor selection for active information fusion
#@ Yongmian Zhang;Qiang Ji
#t 2005
#c 10
#% 115608
#% 226947
#% 344587
#% 404996
#% 443640
#% 731042
#% 785559
#% 1394388
#% 1762173
#! Active information fusion is to selectively choose the sensors so that the information gain can compensate the cost spent in information gathering. However, determining the most informative and cost-effective sensors requires an evaluation of all possible sensor combinations, which is computationally intractable, particularly, when information-theoretic criterion is used. This paper presents a methodology to actively select a sensor subset with the best tradeoff between information gain and sensor cost by exploiting the synergy among sensors. Our approach includes two aspects: a method for efficient mutual information computation and a graph-theoretic approach to reduce search space. The approach can reduce the time complexity significantly in searching for a near optimal sensor subset.

#index 1269556
#* Simultaneous heuristic search for conjunctive subgoals
#@ Lin Zhu;Robert Givan
#t 2005
#c 10
#% 23012
#% 224480
#% 337980
#% 453074
#% 529180
#% 533951
#% 1271820
#% 1271962
#% 1272016
#% 1272017
#% 1272047
#% 1279387
#% 1289210
#% 1476299
#! We study the problem of building effective heuristics for achieving cunjunctive goals from heuristics for individual goals. We consider a straightforward method for building conjunctive heuristics that smoothly trades off between previous common methods. In addition to first explicitly formulating the problem of designing conjunctive heuristics. our major contribution is the discovery that this straightforward method substantially outperforms previously used methods across a wide range of domains. Based on a single positive real parameter k, our heuristic measure sums the individual heuristic values for the subgoal conjuncts, each raised to the k'th power. Varying k allows loose approximation and combination of the previous min, max. and sum approaches, while mitigating some of the weaknesses in those approaches. Our empirical work shows that for many benchmark planning domains there exist fixed parameter values that perform well-- we give evidence that these values can be found automatically by training. Our method, applied to top-level conjunctive goals, shows dramatic improvements over the heuristic used in the FF planner across a wide range of planning competition benchmarks. Also, our heuristic, without computing landmarks, consistently improves upon the success ratio of a recently published landmark-based planner FF-L.

#index 1269557
#* Reactive planning in a motivated behavioral architecture
#@ Éric Beaudry;Yannick Brosseau;Carle Côté;Clément Raïevsky;Dominic Létourneau;Froduald Kabanza;François Michaud
#t 2005
#c 10
#% 267315
#% 296170
#% 357083
#% 418654
#% 445480
#% 772054
#% 1250216
#% 1272014
#% 1272016
#% 1713135
#! To operate in natural environmental settings, autonomous mobile robots need more than just the ability to navigate in the world, react to perceived situations or follow pre-determined strategies: they must be able to plan and to adapt those plans according to the robot's capabilities and the situations encountered. Navigation, simultaneous localization and mapping, perception, motivations, planning, etc., are capabilities that contribute to the decision-making processes of an autonomous robot. How can they be integrated while preserving their underlying principles, and not make the planner or other capabilities a central element on which everything else relies on? In this paper, we address this question with an architectural methodology that uses a planner along with other independent motivational sources to influence the selection of behavior-producing modules. Influences of the planner over other motivational sources are demonstrated in the context of the AAAI Challenge.

#index 1269558
#* A distributed approach to passive localization for sensor networks
#@ Rahul Biswas;Sebastian Thrun
#t 2005
#c 10
#% 44876
#% 117665
#% 281557
#% 297915
#% 309430
#% 715096
#% 716673
#% 805467
#! Sensors that know their location, from microphones to vibration sensors, can support a wider arena of applications than their location unaware counterparts. We offer a method for sensors to determine their own location relative to one another by using only exogenous sounds and the differences in the arrivals of these sounds at different sensors. We present a distributed and computationally efficient solution that offers accuracy on par with more active and computationally intense methods.

#index 1269559
#* Recovery planning for ambiguous cases in perceptual anchoring
#@ Mathias Broxvall;Silvia Coradeschi;Lars Karlsson;Alessandro Saffiotti
#t 2005
#c 10
#% 194653
#% 194658
#% 418651
#% 529673
#% 544767
#% 544777
#% 1289203
#% 1289206
#! An autonomous robot using symbolic reasoning, sensing and acting in a real environment needs the ability to create and maintain the connection between symbols representing objects in the world and the corresponding perceptual representations given by its sensors. This connection has been named perceptual anchoring. In complex environments, anchoring is not always easy to establish: the situation may often be ambiguous as to which percept actually corresponds to a given symbol. In this paper. we extend perceptual anchoring to deal robustly with ambiguous situations by providing general methods for detecting them and recovering from them. We consider different kinds of ambiguous situations and present planning-based methods to recover from them. We illustrate our approach by showing experiments involving a mobile robot equipped with a color camera and an electronic nose.

#index 1269560
#* A multifrontal QR factorization approach to distributed inference applied to multirobot localization and mapping
#@ Frank Dellaert;Alexander Kipp;Peter Krauthausen
#t 2005
#c 10
#% 82083
#% 126193
#% 156938
#% 216454
#% 388024
#% 421550
#% 478312
#% 751027
#% 788088
#% 879222
#% 1810385
#! QR factorization is most often used as a "black box" algorithm, but is in fact an elegant computation on a factor graph. By computing a rooted clique tree on this graph, the computation can be parallelized across subtrees, which forms the basis of so-called multifrontal QR methods. By judiciously choosing the order in which variables are eliminated in the clique tree computation, we show that one straightforwardly obtains a method for performing inference in distributed sensor networks. One obvious application is distributed localization and mapping with a team of robots. We phrase the problem as inference on a large-scale Gaussian Markov Random Field induced by the measurement factor graph, and show how multifrontal QR on this graph solves for the global map and all the robot poses in a distributed fashion. The method is illustrated using both small and large-scale simulations, and validated in practice through actual robot experiments.

#index 1269561
#* Learning CPG sensory feedback with policy gradient for biped locomotion for a full-body humanoid
#@ Gen Endo;Jun Morimoto;Takamitsu Matsubara;Jun Nakanishi;Gordon Cheng
#t 2005
#c 10
#% 252183
#% 272382
#% 466088
#% 720778
#% 857462
#% 1250217
#! This paper describes a learning framework for a central pattern generator based biped locomotion controller using a policy gradient method. Our goals in this study are to achieve biped walking with a 3D hardware humanoid, and to develop an efficient learning algorithm with CPG by reducing the dimensionality of the state space used for learning. We demonstrate that an appropriate feed-back controller can be acquired within a thousand trials by numerical simulations and the obtained controller in numerical simulation achieves stable walking with a physical robot in the real world. Numerical simulations and hardware experiments evaluated walking velocity and stability. Furthermore, we present the possibility of an additional online learning using a hardware robot to improve the controller within 200 iterations.

#index 1269562
#* Tactic-based motion modeling and multi-sensor tracking
#@ Yang Gu
#t 2005
#c 10
#% 394009
#% 1250213
#% 1760888
#! Tracking in essence consists of using sensory information combined with a motion model to estimate the position of a moving object. Tracking efficiency completely depends on the accuracy of the motion model and of the sensory information. For a vision sensor like a camera, the estimation is translated into a command to guide the camera where to look. In this paper, we contribute a method to achieve efficient tracking through using a tactic-based motion model, combined vision and infrared sensory information. We use a supervised learning technique to map the state being tracked to the commands that lead the camera to consistently track the object. We present the probabilistic algorithms in detail and present empirical results both in simulation experiment and from their effective execution in a Segway RMP robot.

#index 1269563
#* A relational representation for procedural task knowledge
#@ Stephen Hart;Roderic Grupen;David Jensen
#t 2005
#c 10
#% 273644
#% 368766
#% 496116
#% 677685
#% 712581
#% 713072
#% 714426
#% 729982
#% 785353
#! This paper proposes a methodology for learning joint probability estimates regarding the effect of sensorimotor features on the predicated quality of desired behavior. These relationships can then be used to choose actions that will most likely produce success. relational dependency networks are used to learn statistical models of procedural task knowledge. An example task expert for picking up objects is learned through actual experience with a humanoid robot. We believe that this approach is widely applicable and has great potential to allow a robot to autonomously determine which features in the world are salient and should be used to recommend policy for action.

#index 1269564
#* Controlling tiny multi-scale robots for nerve repair
#@ Tad Hogg;David W. Sretavan
#t 2005
#c 10
#% 41233
#% 303361
#! We designed and evaluated multiagent control for microscopic robots ("nanorobots") aiding the surgical repair of damaged nerve cells. This repair operates on both nerves as a whole, at scales of hundreds of microns, and individual nerve cell axons, at scales of about a micron. We match the robots to these sizes using a combination of microelectomechanical (MEMS) machines for the larger operations and nanorobots for operations on individual cells. Muitiagent control allows accurate and rapid repair with such robots, with only modest computational and communication requirements for the nanorobots, a significant benefit due to their physical limitations. Our simulations, using physical parameters dictated by nerve biology and plausible nanorobotic capabilities, show how specific control choices lead to trade-offs in clinical outcome. Beyond the specific example of nerve repair treated here, multi-scale robots could aid a variety of medical and biological tasks involving both the large scale of organs or tissues and the microscopic scale of individual cells.

#index 1269565
#* Heterogeneous multirobot coordination with spatial and temporal constraints
#@ Mary Koes;Illah Nourbakhsh;Katia Sycara
#t 2005
#c 10
#% 70370
#% 252199
#% 643099
#% 773217
#% 796155
#% 823934
#% 827869
#! Existing approaches to multirobot coordination separate scheduling and task allocation, but finding the optimal schedule with joint tasks and spatial constraints requires robots to simultaneously solve the scheduling, task allocation, and path planning problems. We present a formal description of the multirobot joint task allocation problem with heterogeneous capabilities and spatial constraints and an instantiation of the problem for the search and rescue domain. We introduce a novel declarative framework for modeling the problem as a mixed integer linear programming (MILP) problem and present a centralized anytime algorithm with error bounds. We demonstrate that our algorithm can outperform standard MILP solving techniques, greedy heuristics, and a market based approach which separates scheduling and task allocation.

#index 1269566
#* Consciousness: drinking from the firehose of experience
#@ Benjamin Kuipers
#t 2005
#c 10
#% 18600
#% 146317
#% 229084
#% 303954
#% 722460
#% 835999
#% 1023092
#! The problem of consciousness has captured the imagination of philosophers, neuroscientists, and the general public, but has received little attention within AI. However, concepts from robotics and computer vision hold great promise to account for the major aspects of the phenomenon of consciousness, including philosophically problematical aspects such as the vividness of qualia, the first-person character of conscious experience, and the property of intentionality. This paper presents and evaluates such an account against eleven features of consciousness "that any philosophical-scientific theory should hope to explain", according to the philosopher and prominent AI critic John Searle.

#index 1269567
#* Semantic place classification of indoor environments with mobile robots using boosting
#@ Axel Rottmann;Óscar Martínez Mozos;Cyrill Stachniss;Wolfram Burgard
#t 2005
#c 10
#% 225602
#% 263049
#% 302391
#% 520224
#% 578682
#% 724234
#% 1650347
#! Indoor environments can typically be divided into places with different functionalities like kitchens, offices, or seminar rooms. We believe that such semantic information enables a mobile robot to more efficiently accomplish a variety of tasks such as human-robot interaction, path-planning, or localization. This paper presents a supervised learning approach to label different locations using boosting. We train a classifier using features extracted from vision and laser range data. Furthermore, we apply a Hidden Markov Model to increase the robustness of the final classification. Our technique has been implemented and tested on real robots as well as in simulation. The experiments demonstrate that our approach can be utilized to robustly classify places into semantic categories. We also present an example of localization using semantic labeling.

#index 1269568
#* Learning to prevent failure states for a dynamically balancing robot
#@ Jeremy Searock;Brett Browning
#t 2005
#c 10
#% 136350
#% 363744
#! To achieve robust autonomy, robots must avoid getting stuck in states from which they cannot recover without external aid. While this is the role of the robot's control algorithms, these are often imperfect. We examine how to detect failures by observing the robot's internal sensors over time. For such cases, triggering a response when detecting the onset of a failure can increase the operational range of the robot. Concretely, we explore the use of supervised learning techniques to create a classifier that can detect a potential failure and trigger a response for a dynamically balancing robot. We present a fully implemented system, where the results clearly demonstrate an improved safety margin for the robot.

#index 1269569
#* Autonomous color learning on a mobile robot
#@ Mohan Sridharan;Peter Stone
#t 2005
#c 10
#% 349208
#% 418733
#% 505094
#% 760805
#% 818551
#! Color segmentation is a challenging subtask in computer vision. Most popular approaches are computationally expensive, involve an extensive off-line training phase and/or rely on a stationary camera. This paper presents an approach for color learning on-board a legged robot with limited computational and memory resources. A key defining feature of the approach is that it works without any labeled training data. Rather, it trains autonomously from a color-coded model of its environment. The process is fully implemented, completely autonomous, and provides high degree of segmentation accuracy.

#index 1269570
#* Mobile robot mapping and localization in non-static environments
#@ Cyrill Stachniss;Wolfram Burgard
#t 2005
#c 10
#% 578744
#% 729437
#% 1289340
#% 1650347
#! Whenever mobile robots act in the real world, they need to be able to deal with non-static objects. In the context of mapping, a common technique to deal with dynamic objects is to filter out the spurious measurements corresponding to such objects. In this paper, we present a novel approach to estimate typical configurations of dynamic areas in the environment of a mobile robot. Our approach clusters local grid maps to identify the possible configurations. We furthermore describe how these clusters can be utilized within a Rao-Blackwellized particle filter to localize a mobile robot in a non-static environment. In practical experiments carried out with a mobile robot in a typical office environment, we demonstrate the advantages of our approach compared to alternative techniques for mapping and localization in dynamic environments.

#index 1269571
#* Improving simultaneous mapping and localization in 3D using global constraints
#@ Rudolph Triebel;Wolfram Burgard
#t 2005
#c 10
#% 117665
#% 319464
#% 418645
#% 464443
#% 469837
#% 755464
#% 1250191
#! Recently, the problem of learning volumetric maps from three-dimenisional range data has become quite popular in the context of mobile robotics. One of the key challenges in this context is to reduce the overall amount of data. The smaller the namber of data points, however, the fewer information is available to register the scans and to conputer a consistent map. In this paper we present a novel approach that estimates global constaints from the data and utilizes these contraints to improve the registration process. In our current system we simultaneously minimize the distance between scans and the distance of edges from planes extracted from the edges to obtain highly accurate three-dimensional modele of the environment. Several experiments carried out in simulation as well as with three-dimensional data obtained with a mobile robot in an outdoor environment we show that our approach yields seriously more accurate models compared to a standard apporach that does not utilize the global constraints.

#index 1269572
#* Bitbots: simple robots solving complex tasks
#@ Anna Yershova;Benjamín Tovar;Robert Ghrist;Steven M. LaValle
#t 2005
#c 10
#% 252183
#% 337494
#% 342096
#% 578744
#% 1250122
#! Sensing uncertainty is a central issue in robotics. Sensor limitations often prevent accurate state estimation, and robots find themselves confronted with a complicated infonnation (belief) space. In this paper we define and characterize the information spaces of very simple robots, called Bitbots, which have severe sensor limitations. While complete estimation of the robot's state is impossible, careful consideration and management of the uncertainty is presented as a search in the information space. We show that these simple robots can solve several challenging online problems, even though they can neither obtain a complete map of their environment nor exactly localize themselves. However, when placed in an unknown environment, Bitbots can build a topological representation of it and then perform pursuit-evasion (i.e., locate all moving targets inside this environment). This paper introduces Bitbots, and provides both theoretical analysis of their information spaces and simulation results.

#index 1269573
#* An algorithm better than AO*?
#@ Blai Bonet;Héctor Geffner
#t 2005
#c 10
#% 25470
#% 68238
#% 160381
#% 174161
#% 181627
#% 216974
#% 318417
#% 337981
#% 337985
#% 361729
#% 443807
#% 443874
#% 644560
#! Recently there has been a renewed interest in AO* as planning problems involving uncertainty and feedback can he naturally formulated as AND/OR graphs. In this work, we carry out what is prohably the first detailed empirical evaluation of AO* in relation to other AND/OR search algorithms. We compare AO* with two other methods: the well-known Value Iteration (VI) algorithm, and a new algorithm, Learning in Depth-First Search (LDFS). We consider instances from four domains. usc three different heuristic functions, and focus on the optimization of cost in the worst case (Max AND/OR graphs). Roughly we find that while AO* does better than VI in the presence of informed heuristics, VI does better than recent extensions of AO* in the presence of cycles in the AND/OR graph. At the same time, LOFS and its variant Bounded LOFS, which can be regarded as extensions of IDA*, are almost never slower than either AO* or VI, and in many cases, are orders-of-magnitude faster.

#index 1269574
#* Speeding up learning in real-time search via automatic state abstraction
#@ Vadim Bulitko;Nathan Sturtevant;Maryia Kazakevich
#t 2005
#c 10
#% 68238
#% 137995
#% 180110
#% 181627
#% 217077
#% 466066
#% 497318
#% 578727
#% 581812
#% 655327
#% 720089
#% 773293
#% 1275260
#% 1290111
#! Situated agents which use learning real-time search are well poised to address challenges of real-time path-finding in robotic and computer game applications. They interleave a local lookahead search with movement execution, explore an initially unknown map, and converge to better paths over repeated experiences. In this paper, we first investigate how three known extensions of the most popular learning real-time search algorithm (LRTA*) influence its performance in a path-finding domain. Then, we combine automatic state abstraction with learning real-time search. Our scheme of dynamically building a state abstraction allows us to generalize updates to the heuristic function, thereby speeding up learning. The novel algorithm converges up to 80 times faster than LRTA* with only one fifth of the response time of A*.

#index 1269575
#* The max K-armed bandit: a new model of exploration applied to search heuristic selection
#@ Vincent A. Cicirello;Stephen F. Smith
#t 2005
#c 10
#% 114994
#% 196785
#% 384911
#% 416988
#% 421314
#% 425053
#% 574136
#% 808185
#% 1250205
#! The multiarmed bandit is often used as an analogy for the tradeoff between exploration and exploitation in search problems. The classic problem involves allocating trials to the arms of a multiarmed slot machine to maximize the expected sum of rewards. We pose a new variation of the multiarmed bandit--the Max K-Armed Bandit--in which trials must be allocated among the arms to maximize the expected best single sample reward of the series of trials. Motivation for the Max K-Armed Bandit is the allocation of restarts among a set of multistart stochastic search algorithms. We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter. We compare this exploration policy to policies that allocate trials to the observed best arm at rates faster (and slower) than double exponentially. The results confirm, for two scheduling domains, that the double exponential increase in the rate of allocations to the observed best heuristic outperfonns the other approaches.

#index 1269576
#* Cost-algebraic heuristic search
#@ Stefan Edelkamp;Shahid Jabbar;Alberto Lluch Lafuente
#t 2005
#c 10
#% 2194
#% 70370
#% 102372
#% 230551
#% 337980
#% 337989
#% 411432
#% 421310
#% 767929
#% 1279391
#% 1289385
#% 1863085
#! Heuristic search is used to efficiently solve the single-node shortest path problem in weighted graphs. In practice, however, one is not only interested in finding a short path, but an optimal path, according to a certain cost notion. We propose an algebraic formalism that captures many cost notions, like typical Quality of Service attributes. We thus generalize A*, the popular heuristic search algorithm. for solving optimal-path problem. The paper provides an answer to a fundamental question for AI search, namely to which general notion of cost, heuristic search algorithms can be applied. We proof correctness of the algorithms and provide experimental results that validate the feasibility of the approach.

#index 1269577
#* Backbones and backdoors in satisfiability
#@ Philip Kilby;John Slaney;Sylvie Thiébaux;Toby Walsh
#t 2005
#c 10
#% 294983
#% 408396
#% 529517
#% 534971
#% 601159
#% 782010
#% 1250137
#% 1279379
#% 1289182
#% 1478782
#! We study the backbone and the backdoors of propositional satisfiability problems. We make a number of theoretical, algorithmic and experimental contributions. From a theoretical perspective, we prove that backbones are hard even to approximate. From an algorithmic perspective, we present a number of different procedures for computing backdoors. From an empirical perspective. we study the correlation between being in the backbone and in a backdoor. Experiments show that there tends to be very little overlap between backbones and backdoors. We also study problem hardness for the Davis Putnam procedure. Problem hardness appears to be correlated with the size of strong backdoors, and weakly correlated with the size of the backbone, but does not appear to be correlated to the size of weak backdoors nor their number. Finally, to isolate the effect of backdoors, we look at problems with no backbone.

#index 1269578
#* Search versus knowledge for solving life and death problems in Go
#@ Akihiro Kishimoto;Martin Müller
#t 2005
#c 10
#% 159245
#% 304513
#% 306264
#% 877221
#% 1250220
#! In games research, Go is considered the classical board game that is most resistant to current AI techniques. Large-scale knowledge engineering has been considered indispensable for building state of the art programs, even for subproblems such as Life and Death, or tsume-Go. This paper describes the technologies behind TSUMEGO EXPLORER, a high-performance tsume-Go search engine for enclosed problems. In empirical testing. this engine outperforms GoTools. which has been the undisputedly best tsume-Go program for 15 years.

#index 1269579
#* Large-scale parallel breadth-first search
#@ Richard E. Korf;Peter Schultze
#t 2005
#c 10
#% 43172
#% 178285
#% 214065
#% 400543
#% 496261
#% 529516
#% 541474
#% 829310
#% 1250221
#% 1250226
#% 1279478
#! Recently, best-first search algorithms have been introduced that store their nodes on disk, to avoid their inherent memory limitation. We introduce several improvements to the best of these, including parallel processing, to reduce their storage and time requirements. We also present a linear-time algorithm for bijectively mapping permutations to integers in lexicographic order. We use breadth-first searches of sliding-tile puzzles as testbeds. On the 3×5 Fourteen Puzzle, we reduce both the storage and time needed by a factor of 3.5 on two processors. We also performed the first complete breadth-first search of the 4×4 Fifteen Puzzle, with over 1013 states.

#index 1269580
#* Domain-dependent parameter selection of search-based algorithms compatible with user performance criteria
#@ Biplav Srivastava;Anupam Mediratta
#t 2005
#c 10
#% 529661
#% 544927
#% 810027
#! Search-based algorithms, like planners, schedulers and satisfiability solvers, are notorious for having numerous parameters with a wide choice of values that can affect their performance drastically. As a result, the users of these algorithms, who may not be search experts, spend a significant time in tuning the values of the parameters to get acceptable performance on their particular problem domains. In this paper, we present a learning-based approach for automatic tuning of search-based algorithms to help such users. The benefit of our methodology is that it handles diverse parameter types, performs effectively for a broad range of systematic as well as non-systematic search based solvers (the selected parameters could make the algorithms solve up to 100% problems while the bad parameters would lead to none being solved), incorporates user-specified performance criteria (Φ) and is easy to implement. Moreover, the selected parameter will satisfy Φ in the first try or the ranked candidates can be used along with Φ to minimize the number of times the parameter settings need to he adjusted until a problem is solved.

#index 1269581
#* Partial pathfinding using map abstraction and refinement
#@ Nathan Sturtevant;Michael Buro
#t 2005
#c 10
#% 2194
#% 68238
#% 217077
#% 393302
#% 773293
#% 1499544
#! Classical search algorithms such as A* or IDA* are useful for computing optimal solutions in a single pass, which can then be executed. But in many domains agents either do not have the time to compute complete plans before acting, or should not spend the time to do so, due to the dynamic nature of the environment. Extensions to A* such as LRTA* address this problem by gradually learning an exact heuristic function, but the learning process is quite slow. In this paper we introduce Partial-Refinement A* (PRA*), which can fully interleave planning and acting through path abstraction and refinement. We demonstrate the etfectiveness of PRA* in the domain of real-time strategy (RTS) games. In maps taken from popular RTS games. we show that PRA* is not only able to cleanly interleave planning and execution. but it is also able to do so with only minimal losses of optimality.

#index 1269582
#* External-memory pattern databases using structured duplicate detection
#@ Rong Zhou;Eric A. Hansen
#t 2005
#c 10
#% 1156
#% 2194
#% 282771
#% 348576
#% 548494
#% 578766
#% 728026
#% 1250219
#% 1250221
#% 1250225
#% 1250226
#% 1478838
#! A pattern database is a lookup table that stores an exact evaluation function for a relaxed search problem, which provides an admissible heuristic for the original search problem. In general, the larger the pattern database, the more accurate the heuristic function. We consider how to build large pattern databases that are stored in external memory, such as disk, and how to use an external-memory pattern database efficiently in heuristic search. To limit the number of slow disk I/O operations needed to construct and query an external-memory pattern data-base, we adapt an approach to external-memory graph search called structured duplicate detection that localizes memory references by leveraging an abstraction of the state space. We present results that show this approach increases the scalability of heuristic search by allowing larger and more accurate pattern database heuristics.

#index 1269583
#* Selection and ranking of propositional formulas for large-scale service directories
#@ Ion Constantinescu;Walter Binder;Boi Faltings
#t 2005
#c 10
#% 147928
#% 224480
#% 303934
#% 462207
#% 481599
#% 519428
#% 724547
#% 767435
#! In this paper we consider scenarios, such as web service composition, where a planner needs to discover its operators by querying a potentially very large and dynamically changing directory. Our contribution is a directory system that represents service advertisements and requests as propositional formulas and provides a flexible query language allowing complex selection and ranking expressions. The internal structure of the directory enables efficient selection and ranking in the presence of a large number of services thanks to its organization as a balanced tree with an extra "intersection predicate". In order to optimally exploit the index structure of the directory, a transformation scheme is applied to the original query. Experimental results on randomly generated service composition problems illustrate the benefits of our approach.

#index 1269584
#* WebCrow: a WEB-based system for crossword solving
#@ Marco Ernandes;Giovanni Angelini;Marco Gori
#t 2005
#c 10
#% 283091
#% 283238
#% 342398
#% 722816
#% 770763
#! Language games represent one of the most fascinating challenges of research in artificial intelligence. In this paper we give an overview of WebCrow, a system that tackles crosswords using the Web as a knowledge base. This appears to be a novel approach with respect to the available literature. It is also the first solver for non-English crosswords and it has been designed to be potentially multilingual. Although WebCrow has been implemented only in a preliminary version, it already displays very interesting results reaching the performance of a human beginner: crosswords that are "easy" for expert humans are solved, within competition time limits, with 80% of correct words and over 90% of correct letters.

#index 1269585
#* A learning-based term-weighting approach for information retrieval
#@ GuangCan Liu;Yong Yu;Xing Zhu
#t 2005
#c 10
#% 46803
#% 78171
#% 86371
#% 111303
#% 115473
#% 185289
#% 262096
#% 284873
#% 286669
#% 340948
#% 369657
#% 387427
#% 719444
#% 840583
#! One of the core components in information retrieval (IR) is the document-term-weighting scheme. In this paper, we will propose a novel learning-based term-weighting approach to improve the retrieval performance of vector space model in homogeneous collections. We first introduce a simple learning system to weighting the index terms of documents. Then, we deduce a formal computational approach according to some theories of matrix computation and statistical inference. Our experiments on 8 collections will show that our approach out-performs classic tfidf weighting, about 20%-45%.

#index 1269586
#* Query translation disambiguation as graph partitioning
#@ Yi Liu;Rong Jin
#t 2005
#c 10
#% 232656
#% 313959
#% 316881
#% 340895
#% 397145
#% 397146
#% 420472
#% 466675
#% 735135
#% 786536
#! Resolving ambiguity in the process of query translation is crucial to cross-language information retrieval when only a bilingual dictionary is available. In this paper we propose a novel approach for query translation disambiguation, named "spectral query translation model". The proposed approach views the problem of query translation disambiguation as a graph partitioning problem. For a given query, a weighted graph is first created for all possible translations of query words based on the co-occurrence statistics of the translation words. The best translation of the query is then determined by the most strongly connected component within the graph. The proposed approach distinguishes from previous approaches in that the translations of all query words are estimated simultaneously. Furthermore, translation probabilities are introduced in the proposed approach to capture the uncertainty in translating queries. Empirical studies with TREC datasets have shown that the spectral query translation model achieves a relative 20% -50% improvement in cross-language information retrieval, compared to other approaches that also exploit word co-occurrence statistics for query translation disambiguation.

#index 1269587
#* Searching for common sense: populating Cyc™ from the web
#@ Cynthia Matuszek;Michael Witbrock;Robert C. Kahlert;John Cabral;Dave Schneider;Purvesh Shah;Doug Lenat
#t 2005
#c 10
#% 127670
#% 198055
#% 268079
#% 309124
#% 342398
#% 578788
#% 687433
#% 742218
#% 754068
#% 757301
#! The Cyc project is predicated on the idea that effective machine learning depends on having a core of knowledge that provides a context for novel learned information - what is known informally as "common sense." Over the last twenty years, a sufficient core of common sense knowledge has been entered into Cyc to allow it to begin effectively and flexibly supporting its most important task: increasing its own store of world knowledge. In this paper, we present initial work on a method of using a combination of Cyc and the World Wide Web, accessed via Google, to assist in entering knowledge into Cyc. The long-term goal is automating the process of building a consistent, formalized representation of the world in the Cyc knowledge base via machine learning. We present preliminary results of this work and describe how we expect the knowledge acquisition process to become more accurate, faster, and more automated in the future.

#index 1269588
#* Automatic text summarization of newswire: lessons learned from the document understanding conference
#@ Ani Nenkova
#t 2005
#c 10
#! Since 2001, the Document Understanding Conferences have been the forum for researchers in automatic text summarization to compare methods and results on common test sets. Over the years, several types of summarization tasks have been addressed--single document summarization, multi-document summarization, summarization focused by question, and headline generation. This paper is an overview of the achieved results in the different types of summarization tasks. We compare both the broader classes of baselines, systems and humans, as well as individual pairs of summarizers (both human and automatic). An analysis of variance model is fitted, with summarizer and input set as independent variables, and the coverage score as the dependent variable, and simulation-based multiple comparisons were performed. The results document the progress in the field as a whole, rather then focusing on a single system, and thus can serve as a future reference on the work done up to date, as well as a starting point in the formulation of future tasks. Results also indicate that most progress in the field has been achieved in generic multi-document summarization and that the most challenging task is that of producing a focused summary in answer to a question/topic.

#index 1269589
#* A graph theoretical foundation for integrating RDF ontologies
#@ Octavian Udrea;Yu Deng;Edna Ruckhaus;V. S. Subrahmanian
#t 2005
#c 10
#% 33376
#% 1289178
#! RDF ontologies are rapidly increasing in number. We study the problem of integrating two RDF ontologies under a given set H of Horn clauses that specify semantic relationships between terms in the ontology, as well as under a given set of negative constraints. We formally define the notion of a "witness" to the integrability of two RDF ontologies under such constraints. A witness represents a way of integrating the ontologies together. We define a "minimal" witnesses and provide the polynomial CROW (Computing RDF Ontology Witness) algorithm to find a witness. We report on the performance of CROW both on DAML, SchemaWeb and Onto-Broker ontologies as well as on synthetically generated data. The experiments show that CROW works very well on real-life ontologies and scales to massive ontologies.

#index 1269590
#* Proceedings of the 20th national conference on Artificial intelligence - Volume 4
#@ Anthony Cohn
#t 2005
#c 10

#index 1269591
#* Machine learning and its application at Nooksack falls hydroelectric station
#@ Scott Alexander;Jianna Zhang
#t 2005
#c 10
#% 61477

#index 1269592
#* Helicopter routing for maintaining remote sites in Alaska using a genetic algorithm
#@ Nicholas Armstrong-Crews;Kenrick Mock
#t 2005
#c 10
#% 344813
#% 566495

#index 1269593
#* Autonomous subgoal discovery and hierarchical abstraction for reinforcement learning using Monte Carlo method
#@ Mehran Asadi;Manfred Huber
#t 2005
#c 10
#% 318485
#% 464303
#% 494580
#% 655326
#% 706874

#index 1269594
#* Mixed-initiative approach to collaboration in the mathematical domain
#@ Nadya Belov;Joshua Shaffer
#t 2005
#c 10
#% 157314
#% 547437
#! Using smart-phones for ad-hoc mathematical collaboration poses multiple user interface challenges. In this paper, software agents are used to lessen the cognitive load through automatic line labeling. Researchers in the human-computer interaction community have attempted to alleviate the problem of general usability through user interface engineering conventions (Myers 1994). However, these engineering approaches can be improved upon through the application of mixed-initiative principles.

#index 1269595
#* On predicting user intent
#@ Nadya Belov
#t 2005
#c 10
#% 241115
#% 447080
#% 769066

#index 1269596
#* DR-Prolog: a system for reasoning with rules and ontologies on the semantic web
#@ Antonis Bikakis;Grigoris Antoniou
#t 2005
#c 10
#% 103705
#% 208197
#% 330234
#% 464934
#% 529674

#index 1269597
#* Genre classification of web documents
#@ Elizabeth Sugar Boese;Adele Howe
#t 2005
#c 10
#% 413637
#% 608646
#% 733850
#! Retrieving relevant documents over the Web is an overwhelming task when search engines return thousands of Web documents. Sifting through these documents is time-consuming and sometimes leads to an unsuccessful search. One problem is that most search engines rely on matching a query to documents based solely on topical keywords. However, many users of search engines have a particular genre in mind for the desired documents. The genre of a document concerns aspects of the document such as the style or readability, presentation layout, and meta-content such as words in the title or the existence of graphs or photos. By including genre in Web searches, we hypothesize that Web document retrieval could greatly improve accuracy by better matching documents to the user's information needs. Before implementing a search engine capable of discriminating on both genre and topic, a feasibility analysis of genre classification is needed. Our previous research achieved 91% classification accuracy across ten genres, whereas similar research range between 60 and 85% accuracy. However, the ten genres used in our research were mostly distinct and only exemplar Web documents (consisting of only one genre) were chosen. This paper discusses our current work which involves an in-depth analysis of maintaining high accuracy rates among genres that are very similar.

#index 1269598
#* Rule refinement by domain experts in complex knowledge bases
#@ Cristina Boicu;Gheorghe Tecuci;Mihai Boicu
#t 2005
#c 10
#% 356978

#index 1269599
#* Use of expert knowledge for decision tree pruning
#@ Jingfeng Cai;John Durkin
#t 2005
#c 10
#% 136350
#% 169684
#% 1272369

#index 1269600
#* Learning support vector machines from distributed data sources
#@ Cornelia Caragea;Doina Caragea;Vasant Honavar
#t 2005
#c 10
#% 280481
#% 376266
#% 420077
#% 949205
#! In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.

#index 1269601
#* Boosting semantic web data access using Swoogle
#@ Li Ding;Tim Finin
#t 2005
#c 10
#% 240957
#% 348182
#% 442973
#% 535998
#% 577318
#% 655350
#% 729882
#% 783560

#index 1269602
#* Towards exploiting duality in approximate linear programming for MDPs
#@ Dmitri Dolgov;Edmund Durfee
#t 2005
#c 10
#% 496267
#% 578699
#% 739715
#% 778078
#% 1272002
#% 1290041

#index 1269603
#* Manufacturing processes recognition of machined mechanical parts using SVMs
#@ Cheuk Yiu Ip;William C. Regli
#t 2005
#c 10
#% 197394
#% 340371
#% 401848
#% 773116

#index 1269604
#* An automated distributed meeting scheduler for FCVW plug-in
#@ Hsiang-Hwa Koo;Elhadi Shakshuki
#t 2005
#c 10
#% 445153
#! People have faced conflicts for shifting scheduled meetings to other time slots in order to fit incoming meetings or to find a time slot to book a meeting. The goal of this research is to develop a personal distributed meeting scheduler in FCVW (Federated Collaborative Virtual Workspace) that assists users to deal with these situations. FCVW is an extension of CVW [6] developed by MITRE. In our approach to meeting scheduling, is to provide each user with meeting scheduler agent. Each agent is able to manage, negotiate and schedule tasks, meetings, events, appointments for its user. This paper describes the objective of our research to develop an automated distributed meeting scheduler.

#index 1269605
#* Description logic-ground knowledge integration and management
#@ Joseph B. Kopena
#t 2005
#c 10
#% 339241
#% 445383
#% 671238
#% 1289177
#! This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems: integration of knowledge sources; access and retrieval of stored knowledge; scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.

#index 1269606
#* Continuous speech recognition using modified stack decoding algorithm
#@ David C. Lee
#t 2005
#c 10
#% 394014

#index 1269607
#* Qualitative dimensions in question answering: extending the definitional QA task
#@ Lucian Vlad Lita;Andrew Hazen Schlaikjer;WeiChang Hong;Eric Nyberg
#t 2005
#c 10
#% 643071
#% 766459
#% 815066
#% 938687
#! Current question answering tasks handle definitional questions by seeking answers which are factual in nature. While factual answers are a very important component in defining entities, a wealth of qualitative data is often ignored. In this incipient work, we define qualitative dimensions (credibility, sentiment, contradictions etc.) for evaluating answers to definitional questions and we explore potential benefits to users. These qualitative dimensions are leveraged to uncover indirect and implicit answers and can help satisfy the user's information need.

#index 1269608
#* A learning support method in qualitative simulation-based economic education
#@ Tokuro Matsuo;Takayuki Ito;Toramatsu Shintani
#t 2005
#c 10
#% 741446
#% 1250212
#% 1250298

#index 1269609
#* Evolving AI opponents in a first-person-shooter video game
#@ C. Adam Overholtzer;Simon D. Levy
#t 2005
#c 10
#% 207535

#index 1269610
#* A framework for Bayesian network mapping
#@ Rong Pan;Yun Peng
#t 2005
#c 10
#% 409570
#! This research is motivated by the need to support inference across multiple intelligence systems involving uncertainty. Our objective is to develop a theoretical framework and related inference methods to map semantically similar variables between separate Bayesian networks in a principled way. The work is to be conducted in two steps. In the first step, we investigate the problem of formalizing the mapping between variables in two separate BNs with different semantics and distributions as pair-wise linkages. In the second step, we aim to justify the mapping between networks as a set of selected variable linkages, and then conduct inference along it.

#index 1269611
#* Minimizing environmental swings with a recurrent neural network control system
#@ Sam Skrivan;Jianna Zhang;Debra Jusak
#t 2005
#c 10
#% 301361
#% 330810
#% 376266
#% 392416
#! Maintaining environmental stability in a dynamic system is a difficult challenge. In your living room, when you set your thermostat to 68 degrees the actual temperature cycles above and below 68 degrees. We attempt to use a Recurrent Neural Network (RNN) in an Aquarium Control System that reduces such environmental swings.

#index 1269612
#* Autonomous learning of tool affordances by a robot
#@ Alexander Stoytchev
#t 2005
#c 10
#% 357083

#index 1269613
#* Heuristics for agent routing and itinerary optimization on dynamic networks
#@ Evan A. Sultanik
#t 2005
#c 10
#% 171449
#% 334245
#% 643220
#% 773224

#index 1269614
#* Approximate inference of Bayesian networks through edge deletion
#@ Julie Thornton
#t 2005
#c 10
#% 44876
#! In this paper, we introduce two new algorithms for approximate inference of Bayesian networks that use edge deletion techniques. The first reduces a network to its maximal weight spanning tree using the Kullback-Leibler information divergence as edge weights, and then runs Pearl's algorithm on the resulting tree for linear-time inference. The second algorithm deletes edges from the triangulated graph until the biggest clique in the triangulated graph is below a desired bound, thus placing a polynomial time bound on inference. When tested for efficiency, these two algorithms perform up to 10,000 times faster than exact techniques. See www.cis.ksu.edu/~jas3466/research.html for more information.

#index 1269615
#* Towards truthful mechanisms for binary demand games: a general framework
#@ Weizhao Wang;Xiang-Yang Li
#t 2005
#c 10
#% 271160
#% 413867
#% 578713
#% 656791

#index 1269616
#* Leveraging language into learning
#@ Jacob Beal
#t 2005
#c 10
#! I hypothesize that learning a vocabulary to communicate between components of a system is equivalent to general learning. Moreover, I assert that some problems of general learning, such as eliminating bad hypotheses, deepening shallow representations, and generation of near-misses, will become simpler when refactored into communication learning problems.

#index 1269617
#* Dissertation in progress: an empirical analysis of the costs and benefits of naturalness in spoken dialog systems
#@ Ellen Campana
#t 2005
#c 10
#! In this paper I describe work for my Ph.D. dissertation whieh is currently in progress. The overarching goal of the work is to develop a methodology for empirically evaluating the effects of different interface design decisions in spoken dialogue systems. The methodology I will use is the dual-task method, borrowed from cognitive psychology, which is advantageous because it provides fine-grained information about the cognitive load of the user while he/she is engaged in interacting with the system. For my dissertation I will focus specifically on the use of definite referring expressions and the question of whether "natural" or "fully-specified" definite referring expressions are easier for users to generate and/or understand. The answers are important because both strategies are used in systems on the market today. More importantly, I hope my work will provide a tool for software developers, and encourage them to carefully weigh the empirically observed costs and benefits of various design decisions.

#index 1269618
#* Learning source descriptions for web services
#@ Mark James Carman
#t 2005
#c 10
#% 198465
#% 572314
#% 1271981
#% 1272298
#% 1272333
#% 1275347

#index 1269619
#* Computational aspects of mechanism design
#@ Vincent Conitzer
#t 2005
#c 10
#% 267752
#% 345429
#% 578703
#% 578715
#% 631051
#% 725200
#% 754139
#% 754148
#% 754175
#% 754177
#% 773195
#% 808366
#% 808367
#% 951945
#% 1250152
#% 1250153
#% 1279301

#index 1269620
#* On boosting semantic web data access
#@ Li Ding
#t 2005
#c 10
#% 442973
#% 577318
#% 724580
#% 783560

#index 1269621
#* Dynamic regime identification and prediction based on observed behavior in electronic marketplaces
#@ Wolfgang Ketter
#t 2005
#c 10
#% 579934
#% 773402
#% 856399
#! We present a method for an autonomous agent to identify dominant market conditions, such as oversupply or scarcity. The characteristics of economic regimes are learned from historic data and used, together with real-time observable information, to identify the current market regime and to forecast market changes. The approach is validated with data from the Trading Agent Competition for Supply Chain Management.

#index 1269622
#* Adaptive modeling and planning for reactive agents
#@ Mykel J. Kochenderfer
#t 2005
#c 10
#% 90041
#% 160859
#% 201257
#% 266288
#% 384911
#% 644560
#% 702594

#index 1269623
#* Self-emergence of structures in gene expression programming
#@ Xin Li
#t 2005
#c 10
#% 114994
#% 165144
#% 929625
#% 1777281
#! This thesis work aims at improving the problem solving ability of the Gene Expression Programming (GEP) algorithm to fulfill complex data mining tasks by preserving and utilizing the self-emergence of structures during its evolutionary process. The main contributions include the investigation of the constant creation techniques for promoting good functional structures emergent in the evolution, analysis of the limitation with the current implementation scheme of GEP, and introduction of a novel utilization of the emergent structures to achieve a flexible search process for solutions at a higher level.

#index 1269624
#* Concurrent hierarchical reinforcement learning
#@ Bhaskara Marthi
#t 2005
#c 10
#% 1289475

#index 1269625
#* Discourse factors in multi-document summarization
#@ Ani Nenkova
#t 2005
#c 10
#% 816215
#% 939777
#% 995507

#index 1269626
#* Structure learning for statistical relational models
#@ Jennifer Neville
#t 2005
#c 10
#% 248810
#% 464449
#% 727834
#% 727912
#% 729926
#% 729982
#% 769942
#% 785353

#index 1269627
#* Towards competence in autonomous agents
#@ Özgür Simsek
#t 2005
#c 10
#% 272662
#% 286423
#% 297171
#% 384911
#% 458686
#% 464303
#% 711673
#% 770775
#% 770777
#% 1271827

#index 1269628
#* Rover science autonomy: probabilistic planning for science-aware exploration doctoral consortium thesis summary
#@ Trey Smith
#t 2005
#c 10
#% 788098
#% 1045684
#% 1279358
#% 1650702
#! Future Mars rovers will have the ability to autonomously navigate for distances of kilometers. In one sol a traverse may take a rover into unexplored areas beyond its local horizon. The rover can explore these areas more effectively if it is able to detect and react to science opportunities on its own, what we call science autonomy. We are studying science autonomy in two ways: first, by implementing a simple science autonomy system, In a rover in the field, and second, by developing probabilistic planning technology that can enable more principled autonomous decision-making in future systems.

#index 1269629
#* Natural language generation for text-to-text applications using an information-slim representation
#@ Radu Soricut
#t 2005
#c 10
#% 252011
#% 811376
#% 815808
#% 843734
#% 1272029
#! I propose a representation formalism and algorithms to be used in a new language generation mechanism for text-to-text applications. The generation process is driven by both text-specific information encoded via probability distributions over words and phrases derived from the input text, and general language knowledge captured by n-gram and syntactic language models.

#index 1269630
#* Planning for geospatial data integration
#@ Snehal Thakkar
#t 2005
#c 10
#% 398263
#% 707146
#% 762610
#% 784281

#index 1269631
#* Improving reinforcement learning function approximators via neuroevolution
#@ Shimon Whiteson
#t 2005
#c 10
#% 361100
#% 369236
#% 384911
#% 449980
#% 452359
#% 1250215
#! Reinforcement learning problems are commonly tackled with temporal difference methods, which use dynamic programming and statistical sampling to estimate the long-term value of taking each action in each state. In most problems of real-world interest, learning this value function requires a function approximator. which represents the mapping from stateaction pairs to values via a concise, parameterized function and uses supervised learning methods to set its parameters. Function approximators make it possible to use temporal difference methods on large problems but, in practice, the feasibility of doing so depends on the ability of the human designer to select an appropriate representation for the value function. My thesis presents a new approach to function approximation that automates some of these difficult design choices by coupling temporal difference methods with policy search methods such as evolutionary computation. It also presents a particular implementation which combines NEAT, a neuroevolutionary policy search method, and Q-learning, a popular temporal difference method, to yield a new method called NEAT+Q that automatically learns effective representations for neural network function approximators. Empirical results in a server job scheduling task demonstrate that NEAT+Q can outperform both NEAT and Q-learning with manually designed neural networks.

#index 1269632
#* QUONTO: querying ontologies
#@ Andrea Acciarri;Diego Calvanese;Giuseppe De Giacomo;Domenico Lembo;Maurizio Lenzerini;Mattia Palmieri;Riccardo Rosati
#t 2005
#c 10
#% 531450
#% 665856
#% 665867

#index 1269633
#* Building applications using end to end composition of web services
#@ Vikas Agarwal;Girish Chafle;Koustuv Dasgupta;Neeran Karnik;Arun Kumar;Ashish Kundu;Anupam Mediratta;Sumit Mittal;Biplav Srivastava
#t 2005
#c 10
#% 341625
#% 445446
#% 452454
#% 769363
#% 805851

#index 1269634
#* The AI technologies of the Philadelphia area urban wireless network testbed
#@ Gustave Anderson;Andrew Burnheimer;Vincent Cicirello;David Dorsey;Chris Dugan;Iris Howley;Moshe Kam;Joseph Kopena;Rob Lass;Kris Malfettone;Andy Mroczkowski;Gaurav Naik;Max Peysakhov;Brian Pyles;William Regli;Evan Suitanik;James Thiel;Kyle Usbeck;Dan Venutolo;Marc Winners
#t 2005
#c 10
#% 773224
#% 784395
#% 1112902
#% 1279447

#index 1269635
#* Proving theorems of type theory automatically with TPS
#@ Peter B. Andrews
#t 2005
#c 10
#% 411364
#% 456897
#% 456901
#% 559324
#% 561234
#% 561564
#% 579718
#% 708711
#% 744255
#% 788536

#index 1269636
#* A learning and reasoning system for intelligence analysis
#@ Mihai Boicu;Gheorghe Tecuci;Cindy Ayers;Dorin Marcu;Cristina Boicu;Marcel Barbulescu;Bogdan Stanescu;William Wagner;Vu Le;Denitsa Apostolova;Adrian Ciubotariu
#t 2005
#c 10
#% 356978
#! This paper presents a personal cognitive assistant, called Disciple-LTA, that can acquire expertise in intelligence analysis directly from intelligence analysts, can train new analysts, and can help analysts find solutions to complex problems through mixed-initiative reasoning, making possible the synergistic integration of a human's experience and creativity with an automated agent's knowledge and speed, and facilitating the collaboration with complementary experts and their agents.

#index 1269637
#* MADbot: a motivated and goal directed robot
#@ A. Coddington;M. Fox;J. Gough;D. Long;I. Serina
#t 2005
#c 10
#% 445486

#index 1269638
#* Swoogle: searching for knowledge on the semantic web
#@ Tim Finin;Li Ding;Rong Pan;Anupam Joshi;Pranam Kolari;Akshay Java;Yun Peng
#t 2005
#c 10
#% 442973
#% 729882
#% 783560

#index 1269639
#* Optimal Rhode Island Hold'em poker
#@ Andrew Gilpin;Tuomas Sandholm
#t 2005
#c 10
#% 176299
#% 233137
#% 348584
#% 1279308
#! Rhode Island Hold'em is a poker card game that has been proposed as a testbed for AI research. This game, with a tree size larger than 3.1 billion nodes, features many characteristics present in full-scale poker (e.g., Texas Hold'em). Our research advances in equilibrium computation have enabled us to solve for the optimal (equilibrium) strategies for this game. Some features of the equilibrium include poker techniques such as bluffing, slow-playing, check-raising, and semi-bluffing. In this demonstration, participants will compete with our optimal opponent and will experience these strategies firsthand.

#index 1269640
#* Evolution of an empathetic digital entity: phase one
#@ Margaret Manella Kozak
#t 2005
#c 10
#% 238395
#% 575392
#! This demonstration highlights the first of seven segments designed to develop a digital entity that will possess the potential for human empathy. The experiences in the first phase of the digital entity Zoe (Zero-One Entity) parallel a subset of learning and development activities encountered by human beings during their first nine months of existence. A website has been created to provide a window to observe Zoe's experiences and action selection process in order to make her basic learning observable, cumulative, and evolutionary. The human observer is invited to influence her action selection by setting the intensity of Zoe's digital personality traits such as assertiveness, reasoning ability, and disposition. Actions generate body-based and emotive-based feelings which are stored in memory structures. Significantly, these structures serve as a foundation for later stages of learning, understanding and reasoning.

#index 1269641
#* Language independent extractive summarization
#@ Rada Mihalcea
#t 2005
#c 10
#% 268079
#% 290830
#% 816173
#! TextRank is a system for unsupervised extractive summarization that relies on an innovative application of iterative graph-based ranking algorithms to graphs encoding the cohesive structure of texts. An important characteristic of the system is that it does not rely on any language-specific knowledge resources or any manually constructed training data, and thus it is highly portable to new languages or domains.

#index 1269642
#* TIELT: a testbed for gaming environments
#@ Matthew Molineaux;David W. Aha
#t 2005
#c 10
#% 1705996
#! Many AI researchers want to test the utility of their systems in complex task environments defined by (e.g., real-time strategy) gaming simulators and/or simulators of computer-generated forces. Also, many developers of commercial and military gaming simulators seek behaviors that can be supported by these systems. However, these integrations require great effort. We will demonstrate the late Alpha version of TIELT, a testbed designed to fill these needs.

#index 1269643
#* SenseRelate targetword: a generalized framework for word sense disambiguation
#@ Siddharth Patwardhan;Satanjeev Banerjee;Ted Pedersen
#t 2005
#c 10
#% 286069
#% 532186
#% 938688
#% 1250278
#% 1279327
#% 1414358
#! Many words in natural language have different meanings when used in different contexts. Sense Relate: Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet: Similarity measure of relatedness.

#index 1269644
#* Identifying similar words and contexts in natural language with SenseClusters
#@ Ted Pedersen;Anagba Kulkarni
#t 2005
#c 10
#% 741083
#% 816128
#% 1277986
#% 1700523
#! SenseClusters is a freely available intelligent system that clusters together similar contexts in natural language text. Thereafter it assigns identifying labels to these clusters based on their content. It is a purely unsupervised approach that is language independent, and uses no knowledge other than what is available in raw un-annotated corpora. In addition to clustering similar contexts, it can be used to identify synonyms and sets of related words. It has been applied to a diverse range of problems, including proper name disambiguation, word sense discrimination, email organization, and document clustering. SenseClusters is a complete system that supports feature selection from large corpora, several different context representation schemes, various clustering algorithms, the creation of descriptive and discriminating labels for the discovered clusters, and evaluation relative to gold standard data.

#index 1269645
#* Song search and retrieval by tapping
#@ Geoffrey Peters;Caroline Anthony;Michael Schwartz
#t 2005
#c 10
#% 120649
#% 280845
#% 281389
#% 316259
#% 730141
#! We present an interactive, web based system for musical song search and retrieval, using rhythmic tapping as the primary means of query input. Our approach involves encoding the input rhythm as a contour string, and using approximate string matching to determine the most likely match with songs in the database.

#index 1269646
#* The proteome analyst suite of automated function prediction tools
#@ B. Poulin;D. Szafron;P. Lu;R. Greiner;D. S. Wishart;R. Eisner;A. Fyshe;B. Pearcy;L. Pireddu
#t 2005
#c 10
#% 830753
#! Proteome Analyst (PA) is a publicly available, high-throughput, web-based system for automatically predicting the function and properties of proteins. Biologists can use PA to predict, for example, the Gene Ontology (GO) molecular function and subcellular localization of a protein based on sequence information. Using sequence analysis tools and machine learning, PA gives high accuracy and broad coverage for both molecular function and subcellular localization predictions.

#index 1269647
#* DiamondHelp: a collaborative task guidance framework for complex devices
#@ Charles Rich;Candy Sidner;Neal Lesh;Andrew Garland;Shane Booth;Markus Chimani
#t 2005
#c 10
#% 9197
#% 398946
#% 813208
#! DiamondHelp is a reusable Java framework for building collaborative task guidance systems for complex devices, such as digitally enabled home appliances. DiamondHelp combines a generic conversational interface, adapted from online chat programs, with an application-specific direct manipulation interface. DiamondHelp provides "a things to say" mechanism for use without spoken language understanding; it also supports extensions to take advantage of speech technology. DiamondHelp's software architecture factors all application-specific content into two modular plug-ins, one of which includes Collagen and a task model.

#index 1269648
#* Remote supervisory control of a humanoid robot
#@ Michael T. Rosenstein;Andrew H. Fagg;Robert Platt;John D. Sweeney;Roderic A. Grupen
#t 2005
#c 10
#% 119772
#% 1272348
#! For this demonstration, participants have the opportunity to control a humanoid robot located hundreds of miles away. The general task is to reach, grasp, and transport various objects in the vicinity of the robot. Although remote "pick-and-place" operations of this sort form the basis of numerous practical applications, they are frequently error-prone and fatiguing for human operators. Participants can experience the relative difficulty of remote manipulation both with and without the use of an assistive interface. This interface simplifies the task by injecting artificial intelligence in key places without seizing higher-level control from the operator. In particular, we demonstrate the benefits of two key components of the system: a video display of predicted operator intentions, and a haptic-based controller for automated grasping.

#index 1269649
#* MGLAIR agents in virtual and other graphical environments
#@ Stuart C. Shapiro;Josephine Anstey;David E. Pape;Trupti Devdas Nayak;Michael Kandefer;Orkan Telhan
#t 2005
#c 10
#% 174161
#% 208161
#% 722354
#! We are demonstrating several intelligent agents built according to the MGLAIR (Modal Grounded Layered Architecture with Integrated Reasoning) agent architecture. The top layer of MGLAIR is implemented in SNePS and its acting subsystem, SNeRE (the SNePS Rational Engine). The major demonstration will be act 3 of The Trial The Trail, an interactive drama running on an immersive Virtual Reality system, in which a human participant interacts with several MGLAIR actor-agents. We will also demonstrate several olher MGLAIR agents that operate in non-VR graphical environments. All these agents illustrate our approach to building agents with integrated first-person, on-line reasoning and acting.

#index 1269650
#* Solo: a cognitive orthosis
#@ Richard Simpson;Edmund LoPresti;Debra Schreckenghost;Ned Kirsch;Steve Hayashi
#t 2005
#c 10
#% 266682
#% 579927
#% 1478923
#! Solo is a cognitive assistive device which provides support in remembering when to perform tasks, executing the steps in a task, and recovering from unexpected events. The system includes an interface for clients to receive reminders, an interface for caregivers to enter information about the client's scheduled tasks, and a Cognition Manager which provides reminders and task guidance at appropriate times.

#index 1269651
#* SAGA-ML: an active learning system for semi-automated gameplay analysis
#@ Finnegan Southey;Robert C. Holte
#t 2005
#c 10
#% 136350
#% 1269501

#index 1269652
#* Using the GEMS system for cancer diagnosis and biomarker discovery from microarray gene expression data
#@ Alexander Statnikov;Ioannis Tsamardinos;Constantin F. Aliferis
#t 2005
#c 10
#% 833529
#! We will demonstrate the GEMS system for automated development and evaluation of high-quality cancer diagnostic models and biomarker discovery from microarray gene expression data. The development of GEMS was informed by the results of an extensive algorithmic evaluation using 11 microarray datasets. The system was further evaluated in two cross-dataset applications and using 5 microarray datasets. The performance of models produced by GEMS is comparable or better than the results obtained by human analysts, and these models generalize well to independent samples in cross-dataset applications. The system is freely available for download from http://www.gems-system.org for noncommercial use.

#index 1269653
#* The TaskTracer system
#@ Simone Stumpf;Xinlong Bao;Anton Dragunov;Thomas G. Dietterich;Jon Herlockel;Kevin Johnsrude;Lida Li;JianQiang Shen
#t 2005
#c 10
#% 19634
#% 286415
#% 297611
#% 339375
#% 438551
#% 452634
#% 452635
#% 766422
#% 790446
#! Knowledge workers spend the majority of their working hours processing and manipulating information. These users face continual costs as they switch between tasks to retrieve and create information. The TaskTracer project at Oregon State University investigates the possibilities of a desktop software system that will record in detail how knowledge workers complete tasks, and intelligently leverage that information to increase efficiency and productivity. Our approach assigns each observed user interface action to a task for which it is likely being performed. In this demonstration we show how we have applied machine learning in this environment.

#index 1269654
#* Low-cost outdoor robot platform for the Penn State Abington mini grand challenge
#@ Robert Avanzato
#t 2005
#c 10

#index 1269655
#* Pyro: an integrated environment for robotics education
#@ Douglas Blank;Deepak Kumar;Lisa Meeden;Holly Yanco
#t 2005
#c 10
#% 432841

#index 1269656
#* Ready or not, here i come ...
#@ Magdalena Bugajska;William Adams;Scott Thomas;J. Gregory Trafton;Alan C. Schultz
#t 2005
#c 10
#% 418689
#% 445427

#index 1269657
#* Robots in an intelligent systems course
#@ Debra Burhans;Andre Nelson;Victoria Steck
#t 2005
#c 10
#% 246973
#% 350804

#index 1269658
#* Scavenging with a laptop robot
#@ Alan Davidson;Mac Mason;Susanna Ricco;Ben Tribelhorn;Zachary Dodds
#t 2005
#c 10
#% 181468
#% 337494
#! This synopsis presents Harvey Mudd College's entry into the 2005 AAAI scavenger hunt competition. We are submiting a lap-controlled robot which uses commodity parts and limited sensors to localize itself and perform arrow following and object recognition.

#index 1269659
#* Social tag: finding the person with the pink hat
#@ Carl DiSalvo;Didac Font;Laura Hiatt;Nik Melchior;Marek Michalowski;Reid Simmons
#t 2005
#c 10

#index 1269660
#* Upending the uncanny valley
#@ David Hanson;Andrew Olney;Steve Prilliman;Eric Mathews;Marge Zielke;Derek Hammons;Raul Fernandez;Harry Stephanou
#t 2005
#c 10
#% 232905
#% 350689
#! Although robotics researchers commonly contend that robots should not look too humanlike, many artforms have successfully depicted people and have come to be accepted as great and important works, with examples such as Rodin's Thinker, Mary Cassat's infants, and Disney's Abe Lincoln simulacrum. Extending this tradition to intelligent robotics, the authors have depicted late sci-fi writer Philip K Dick with an autonomous, intelligent android. In doing so, the authors aspire to bring robotic systems up to the level of great art, while using the technology as a mirror for examining human nature in social AI development and cognitive science experiments.

#index 1269661
#* Catoms: moving robots without moving parts
#@ Brian Kirby;Jason Campbell;Burak Aksak;Padmanabhan Pillai;James Hoburg;Todd Mowry;Seth Copen Goldstein
#t 2005
#c 10
#! We demonstrate modular robot prototypes developed as part of the Claytronics Project (Goldstein et al. 2005). Among the novel features of these robots ("catoms") is their ability to reconfigure (move) relative to one another without moving parts. The absence of moving parts is central to one key aim of our work, namely, plausible manufacturability at smaller and smaller physical scales using high-volume. low-unit-cost techniques such as batch photolithography, multimaterial submicron 3D lithographic processing, and self assembly. Claytronics envisions multi-million-module robot ensembles able to form into three dimensional scenes, eventually with sufficient fidelity so as to convince a human observer the scenes are real. This work presents substantial challenges in mechanical and electronic design, control, programming, reliability, power delivery, and motion planning (among other areas), and holds the promise of radically altering the relationship between computation, humans, and the physical world.

#index 1269662
#* NavBot: the navigational search-and-rescue robot
#@ Matthew Marge;Ayman Sawas;Juan Carlos Liberato;Murtaza M. Karim;Manish Muttreja;Nader Alrawahi;Brian Fink
#t 2005
#c 10
#! The Stony Brook Robot Design Team has focused on two main areas of research in the creation of NavBot, our new robot created for the American Association of Artificial Intelligence's (AAAI) Scavenger Hunt Event: navigation and computer vision. The purpose is to create an intelligent machine that is able to navigate the conference floor for specific objects at the AAAI Conference in Pittsburgh, Pennsylvania. To achieve the desirable speeds required in a rescue robot, NavBot utilizes high-rpm servo motors with a manually adjustable gear train. The two 12V servo motors enhance the maneuverability of the robot by providing minor adjustments in the robot's speed as it navigates an area. The manually adjustable gear trains allow NavBot to function in different environments. This is achieved by controlling the force and the speed required for maneuverability in the terrain. NavBot also utilizes a dual-track motion system to handle rough terrain. To enhance the power of NavBot, it was designed with a light aluminum skeleton. In addition to the large space provided for necessary electrical and computer equipment, the skeleton serves as a secure casing that prevent, damage to vital parts.

#index 1269663
#* A brochette of socially interactive robots
#@ F. Michaud;D. Létourneau;P. Lepage;Y. Morin;F. Gagnon;P. Giguère;É Beaudry;Y. Brosseau;C. Côté;A. Duquette;J.-F. Laplante;M.-A. Legault;P. Moisan;A. Ponchon;C. Raïevsky;M.-A. Roux;T. Salter;J.-M. Valin;S. Caron;P. Masson;F. Kabanza;M. Lauria
#t 2005
#c 10
#% 995786
#% 1269557
#% 1784814
#! The design of interactive mobile robots is a multidisciplinary endeavor that profits from putting robots with people and studying their effects and impacts. To do so, two main issues must be addressed: giving robots capabilities in order to interact in meaningful and efficient ways with people, and the ability to move in human settings. This paper briefly describes four robotic platforms that are going to be demonstrated at the AAAI 2005 Robot Competition.

#index 1269664
#* Indoor aerial robot competition: challenges in search and rescue applications
#@ Paul Y. Oh;William E. Green;Keith W. Sevcik
#t 2005
#c 10
#! Tasks like bomb-detection, search-and-rescue, and reconnaissance in near-Earth environments are time, cost and labor intensive. Aerial robots could assist in such missions and offset the demand in resources and personnel. However, flying in environments rich with obstacles presents many more challenges which have yet to be identified. For example, telephone wire is one obstacle that is known to be hard to detect in mid-flight. This paper describes how a blimp can be used in an aerial robot competition to identify other key challenges when flying in these cluttered environments.

#index 1269665
#* Toward affective cognitive robots for human-robot interaction
#@ M. Scheutz;J. Kramer;C. Middendorff;P. Schermerhorn;M. Heilman;D. Anderson;P. Bui
#t 2005
#c 10
#% 344667
#% 1250125
#! We present a brief overview of an architecture for a complex affective robot for human-robot interaction.

#index 1269666
#* Using a sketch pad interface for interacting with a robot team
#@ Marjorie Skubic;Derek Anderson;Samuel Blisard;Dennis Perzanowski;William Adams;J. Gregory Trafton;Alan C. Schultz
#t 2005
#c 10
#% 1776525

#index 1269667
#* Tekkotsu: a framework for AIBO cognitive robotics
#@ David S. Touretzky;Ethan J. Tira-Thompson
#t 2005
#c 10

#index 1269668
#* Improving human-robot interaction for remote robot operation
#@ Holly A. Yanco;Michael Baker;Robert Casey;Andrew Chanler;Munjal Desai;Dan Hestand;Brenden Keyes;Philip Thoren
#t 2005
#c 10
#% 1137760

#index 1269669
#* Proceedings of the 22nd national conference on Artificial intelligence - Volume 1
#@ Anthony Cohn
#t 2007
#c 10

#index 1269670
#* AAAI-07 preface
#@ Robert C. Holte;Adele E. Howe
#t 2007
#c 10

#index 1269671
#* Uncertainty in preference elicitation and aggregation
#@ Toby Walsh
#t 2007
#c 10
#% 578692
#% 578703
#% 578715
#% 631051
#% 952560
#% 1024741
#% 1274974
#% 1274989
#! Uncertainty arises in preference aggregation in several ways. There may, for example, be uncertainty in the votes or the voting rule. Such uncertainty can introduce computational complexity in determining which candidate or candidates can or must win the election. In this paper, we survey recent work in this area and give some new results. We argue, for example, that the set of possible winners can be computationally harder to compute than the necessary winner. As a second example, we show that, even if the unknown votes are assumed to be single-peaked, it remains computationally hard to compute the possible and necessary winners, or to manipulate the election.

#index 1269672
#* Logic for automated mechanism design: a progress report
#@ Michael Wooldridge;Thomas Agotnes;Paul E. Dunne;Wiebe Van der Hoek
#t 2007
#c 10
#% 52273
#% 68656
#% 101955
#% 162305
#% 188086
#% 233136
#% 271160
#% 274917
#% 342124
#% 379175
#% 392811
#% 413871
#% 422073
#% 521602
#% 541468
#% 587589
#% 593767
#% 773198
#% 782011
#% 818584
#% 819613
#% 823928
#% 823931
#% 879715
#% 890215
#% 890218
#% 890221
#% 944865
#% 1024761
#% 1024805
#% 1086662
#% 1250604
#% 1274943
#% 1274944
#! Over the past half decade, we have been exploring the use of logic in the specification and analysis of computational economic mechanisms. We believe that this approach has the potential to bring the same benefits to the design and analysis of computational economic mechanisms that the use of temporal logics and model checking have brought to the specification and analysis of reactive systems. In this paper, we give a survey of our work. We first discuss the use of cooperation logics such as Alternating-time Temporal Loglc (ATL) for the specification and verification of mechanisms such as social choice procedures. We motivate the approach, and then discuss the work we have done on extensions to ATL to support incomplete information, preferences, and quantification over coalition. We then discuss is the use of ATL-like cooperation logics in the development of social laws.

#index 1269673
#* Learning equilibrium in resource selection games
#@ Itai Ashlagi;Dov Monderer;Moshe Tennenholtz
#t 2007
#c 10
#% 465913
#% 722895
#% 789557
#% 816644
#% 890331
#% 960820
#% 1269473
#% 1289288
#% 1393537
#% 1684572
#! We consider a resource selection game with incomplete information about the resource-cost functions. All the players know is the set of players, an upper bound on the possible costs, and that the cost functions are positive and nondecreasing. The game is played repeatedly and after every stage each player observes her cost, and the actions of all players. For every Ε 0 we prove the existence of a learning Ε-equilibrium, which is a profile of algorithms, one for each player such that a unilateral deviation of a player is, up to ε not beneficial for her regardless of the actual cost functions. Furthermore, the learning eqUilibrium yields an optimal social cost.

#index 1269674
#* Action-based alternating transition systems for arguments about action
#@ Katie Atkinson;Trevor Bench-Capon
#t 2007
#c 10
#% 413871
#% 890250
#% 890271
#% 908933
#% 993108
#% 1221666
#% 1727186
#! This paper presents a formalism to describe practical reasoning in terms of an Action-based Alternating Transition System (AATS). The starting point is a previously specified account of practical reasoning that treats reasoning about what action should be chosen as presumptive argumentation using argument schemes and associated critical questions. This paper describes how this account can be extended to situations where the effect of an action is partially dependent upon the choices of another agent. In this context we see practical reasoning as proceeding in three stages. The first involves determining the representation of the particular problem scenario as an AATS. Next the agent must resolve its uncertainties as to its position in the scenario. Finally, the agent moves to choosing a particular action to achieve its ends, proposing presumptive reasons for particular actions and subjecting them to a critique to establish their suitability, taking into account the choices that can be made by the other agents involved. This account thus provides a well-specified basis for addressing the problems of practical reasoning as presumptive argumentation in a multi-agent context.

#index 1269675
#* Implementing the maximum of monotone algorithms
#@ Liad Blumrosen
#t 2007
#c 10
#% 190611
#% 271160
#% 282658
#% 413867
#% 578713
#% 805790
#% 808380
#% 847156
#% 868452
#% 1269397
#% 1707151
#! Running several sub-optimal algorithms and choosing the optimal one is a common procedure in computer science, most notably in the design of approximation algorithms. This paper deals with one significant flaw of this technique in environments where the inputs are provided by selfish agents: such protocols are not necessarily incentive compatible even when the underlying algorithms are. We characterize sufficient and necessary conditions for such best-outcome protocols to be incentive compatible in a general model for agents with one-dimensional private data. We show how our techniques apply in several settings.

#index 1269676
#* Intention guided belief revision
#@ Timothy William Cleaver;Abdul Sattar
#t 2007
#c 10
#% 1275318
#! This paper aims to investigate methodologies to utilize an agent's intentions as a means to guide the revision of its beliefs. For this purpose, we develop a collection of belief revision operators that employ the effect of the revision on the agent's intentions as the selection criteria. These operators are then assessed for rationality against the traditional AGM postulates. There is a large volume of work concerned with classical beliefrevision, the primary issue of which is the mitigation of the uncertainty inherent in environments in which belief revision is necessary. Traditional approaches attempt to assess the explanatory power of beliefs and utilize this as a heuristic to resolve this ambiguity. We argue that for practical reasoning systems, whose primary focus lies in the maintenance of behavior and not information, an agent's intentions provide a better guide.

#index 1269677
#* The impact of network topology on pure Nash equilibria in graphical games
#@ Bistra Dilkina;Carla P. Gomes;Ashish Sabharwal
#t 2007
#c 10
#% 567883
#% 631052
#% 773295
#% 868455
#! Graphical games capture some of the key aspects relevant to the study and design of multi-agent systems. It is often of interest to find the conditions under which a game is stable, i.e., the players have reached a consensus on their actions. In this paper, we characterize how different topologies of the interaction network affect the probability of existence of a pure Nash equilibrium in a graphical game with random payoffs. We show that for tree topologies with unbounded diameter the probability of a pure Nash equilibrium vanishes as the number of players grows large. On the positive side, we define several families of graphs for which the probability of a pure Nash equilibrium is at least 1-1/e even as the number of players goes to infinity. We also empirically show that adding a small number of connection "shortcuts" can increase the probability of pure Nash.

#index 1269678
#* Potential-aware automated abstraction of sequential games, and holistic equilibrium analysis of Texas Hold'em poker
#@ Andrew Gilpin;Tuomas Sandholm;Troels Bjerre Sørensen
#t 2007
#c 10
#% 348584
#% 826273
#% 868462
#% 997606
#% 1024867
#% 1024868
#% 1250316
#% 1250324
#% 1279308
#% 1650304
#% 1740191
#! We present a new abstraction algorithm for sequential imperfect information games. While most prior abstraction algorithms employ a myopic expected-value computation as a similarity metric, our algorithm considers a higher-dimensional space consisting of histograms over abstracted classes of states from later stages of the game. This enables our bottom-up abstraction algorithm to automatically take into account potential: a hand can become relatively better (or worse) over time and the strength of different hands can get resolved earlier or later in the game. We further improve the abstraction quality by making multiple passes over the abstraction, enabling the algorithm to narrow the scope of analysis to information that is relevant given abstraction decisions made for earlier parts of the game. We also present a custom indexing scheme based on suit isomorphisms that enables one to work on significantly larger models than before. We apply the techniques to heads-up limit Texas Hold'em poker. Whereas all prior game theory-based work for Texas Hold'em poker used generic off-the-shelf linear program solvers for the equilibrium analysis of the abstracted game, we make use of a recently developed algorithm based on the excessive gap technique from convex optimization. This paper is, to our knowledge, the first to abstract and game-theoretically analyze all four betting rounds in one run (rather than splitting the game into phases). The resulting player, GS3, beats BluffBot, GS2, Hyperborean, Monash-BPP, Sparbot, Teddy, and Vexbot, each with statistical significance. To our knowledge, those competitors are the best prior programs for the game.

#index 1269679
#* Automated online mechanism design and prophet inequalities
#@ Mohammad Taghi Hajiaghayi;Robert Kleinberg;Tuomas Sandholm
#t 2007
#c 10
#% 314943
#% 453487
#% 580561
#% 580720
#% 723936
#% 754140
#% 754141
#% 808375
#% 813771
#% 813834
#% 821998
#% 868471
#% 890383
#% 907719
#% 959562
#% 991165
#% 1000451
#% 1269401
#% 1274995
#% 1650358
#! Recent work on online auctions for digital goods has explored the role of optimal stopping theory -- particularly secretary problems -- in the design of approximately optimal online mechanisms. This work generally assumes that the size of the market (number of bidders) is known a priori, but that the mechanism designer has no knowledge of the distribution of bid values. However, in many real-world applications (such as online ticket sales), the opposite is true: the seller has distributional knowledge of the bid values (e.g., via the history of past transactions in the market), but there is uncertainty about market size. Adopting the perspective of automated mechanism design, introduced by Conitzer and Sandholm, we develop algorithms that compute an optimal, or approximately optimal, online auction mechanism given access to this distributional knowledge. Our main results are twofold. First, we show that when the seller does not know the market size, no constant-approximation to the optimum efficiency or revenue is achievable in the worst case, even under the very strong assumption that bid values are i.i.d. samples from a distribution known to the seller. Second, we show that when the seller has distributional knowledge of the market size as well as the bid values, one can do well in several senses. Perhaps most interestingly, by combining dynamic programming with prophet inequalities (a technique from optimal stopping theory) we are able to design and analyze online mechanisms which are temporally strategyproof (even with respect to arrival and departure times) and approximately efficiency (revenue)-maximizing. In exploring the interplay between automated mechanism design and prophet inequalities, we prove new prophet inequalities motivated by the auction setting.

#index 1269680
#* Real arguments are approximate arguments
#@ Anthony Hunter
#t 2007
#c 10
#% 3708
#% 224478
#% 330290
#% 337502
#% 417812
#% 470198
#% 752766
#% 1269451
#! There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments. A common assumption for logic-based argumentation is that an argument is a pair (Φ, α) where Φ is minimal subset of the knowledgebase such that Φ is consistent and Φ entails the claim α. However, real arguments (i.e. arguments presented by humans) usually do not have enough explicitly presented premises for the entailment of the claim. This is because there is some common knowledge that can be assumed by a proponent of an argument and the recipient of it. This allows the proponent of an argument to encode an argument into a real argument by ignoring the common knowledge, and it allows a recipient of a real argument to decode it into an argument by drawing on the common knowledge. If both the proponent and recipient use the same common knowledge, then this process is straightforward. Unfortunately, this is not always the case, and raises the need for an approximation of the notion of an argument for the recipient to cope with the disparities between the different views on what constitutes common knowledge.

#index 1269681
#* Partial revelation automated mechanism design
#@ Nathanaël Hyafil;Craig Boutilier
#t 2007
#c 10
#% 879183
#% 1250613
#% 1272144
#% 1274968
#% 1274995
#% 1650358
#! In most mechanism design settings, optimal general-purpose mechanisms are not known. Thus the automated design of mechanisms tailored to specific instances of a decision scenario is an important problem. Existing techniques for automated mechanism design (AMD) require the revelation of full utility information from agents, which can be very difficult in practice. In this work, we study the automated design of mechanisms that only require partial revelation of utilities. Each agent's type space is partitioned into a finite set of partial types, and agents (should) report the partial type within which their full type lies. We provide a set of optimization routines that can be combined to address the trade-offs between the amount of communication, approximation of incentive properties, and objective value achieved by a mechanism. This allows for the automated design of partial revelation mechanisms with worst-case guarantees on incentive properties for any objective function (revenue, social welfare, etc.).

#index 1269682
#* Computing pure Nash equilibria in symmetric action graph games
#@ Albert Xin Jiang;Kevin Leyton-Brown
#t 2007
#c 10
#% 567883
#% 631052
#% 788040
#% 868455
#% 898290
#% 1250614
#% 1269436
#% 1392272
#! We analyze the problem of computing pure Nash equilibria in action graph games (AGGs), which are a compact game-theoretic representation. While the problem is NP-complete in general, for certain classes of AGGs there exist polynomial time algorithms. We propose a dynamic-programming approach that constructs equilibria of the game from equilibria of restricted games played on subgraphs of the action graph. In particular, if the game is symmetric and the action graph has bounded treewidth, our algorithm determines the existence of pure Nash equilibrium in polynomial time.

#index 1269683
#* A unification of extensive-form games and Markov decision processes
#@ H. Brendan McMahan;Geoffrey J. Gordon
#t 2007
#c 10
#% 176299
#% 847048
#% 997945
#% 1275075
#! We describe a generalization of extensive-form games that greatly increases representational power while still allowing efficient computation in the zero-sum setting. A principal feature of our generalization is that it places arbitrary convex optimization problems at decision nodes, in place of the finite action sets typically considered. The possibly-infinite action sets mean we must "forget" the exact action taken (feasible solution to the optimization problem), remembering instead only some statistic sufficient for playing the rest of the game optimally. Our new model provides an exponentially smaller representation for some games; in particular, we show how to compactly represent (and solve) extensive-form games with outcome uncertainty and a generalization of Markov decision processes to multi-stage adversarial planning games.

#index 1269684
#* An ironing-based approach to adaptive online mechanism design in single-valued domains
#@ David C. Parkes;Quang Duong
#t 2007
#c 10
#% 314943
#% 495927
#% 580720
#% 754141
#% 808375
#% 931104
#% 1269397
#% 1274955
#! Online mechanism design considers the problem of sequential decision making in a multi-agent system with self-interested agents. The agent population is dynamic and each agent has private information about its value for a sequence of decisions. We introduce a method ("ironing") to transform an algorithm for online stochastic optimization into one that is incentive-compatible. Ironing achieves this by canceling decisions that violate a form of monotonicity. The approach is applied to the CONSENSUS algorithm and experimental results in a resource allocation domain show that not many decisions need to be canceled and that the overhead of ironing is manageable.

#index 1269685
#* On the reasoning patterns of agents in games
#@ Avi Pfeffer;Ya'akov Gal
#t 2007
#c 10
#% 297171
#% 370075
#% 567883
#% 578708
#% 1289289
#! What reasoning patterns do agents use to choose their actions in games? This paper studies this question in the context of Multi-Agent Influence Diagrams (MAIDs). It defines several kinds of reasoning patterns, and associates each with a pattern of paths in a MAID. We asks the question, what reasoning patterns have to hold in order for an agent to care about its decision? The answer depends on what strategies are considered for other agents' decisions. We introduce a new solution concept, called well-distinguishing (WD) strategies, that captures strategies in which all the distinctions an agent makes really make a difference. We show that when agents are playing WD strategies, all situations in which an agent cares about its decision can be captured by four reasoning patterns. We furthermore show that when one of these four patterns holds, there are some MAID parameter values such that the agent actually does care about its decision.

#index 1269686
#* Learning voting trees
#@ Ariel D. Procaccia;Aviv Zohar;Yoni Peleg;Jeffrey S. Rosenschein
#t 2007
#c 10
#% 103267
#% 241022
#% 271176
#% 417561
#% 578703
#% 754153
#% 868449
#% 1272142
#! Binary voting trees provide a succinct representation for a large and prominent class of voting rules. In this paper, we investigate the PAC-learnability of this class of rules. We show that, while in general a learning algorithm would require an exponential number of samples, if the number of leaves is polynomial in the size of the set of alternatives then a polynomial training set suffices. We apply these results in an emerging theory: automated design of voting rules by learning.

#index 1269687
#* On the benefits of exploiting underlying goals in argument-based negotiation
#@ Iyad Rahwan;Philippe Pasquier;Liz Sonenberg;Frank Dignum
#t 2007
#c 10
#% 178934
#% 643114
#% 643176
#% 782765
#% 1272101
#! Interest-based negotiation (IBN) is a form of negotiation in which agents exchange information about their underlying goals, with a view to improving the likelihood and quality of a deal. While this intuition has been stated informally in much previous literature, there is no formal analysis of the types of deals that can be reached through IBN and how they differ from those reachable using (classical) alternating offer bargaining. This paper bridges this gap by providing a formal framework for analysing the outcomes of IBN dialogues, and begins by analysing a specific IBN protocol.

#index 1269688
#* Revenue monotonicity in combinatorial auctions
#@ Baharak Rastegari;Anne Condon;Kevin Leyton-Brown
#t 2007
#c 10
#% 314944
#% 341408
#% 341912
#% 413867
#% 578713
#% 580516
#% 631040
#% 813835
#% 818584
#% 847155
#% 1250155
#% 1269401
#! Intuitively, one might expect that a seller's revenue from an auction weakly increases as the number of bidders grows, as this increases competition. However, it is known that for combinatorial auctions that use the VCG mechanism, a seller can sometimes increase revenue by dropping bidders. In this paper we investigate the extent to which this problem can occur under other dominant-strategy combinatorial auction mechanisms. Our main result is that such failures of "revenue monotonicity" are not limited to mechanisms that achieve efficient allocations. Instead, they can occur under any dominant-strategy direct mechanism that sets prices using critical values, and that always chooses an allocation that cannot be augmented to make some bidder better off, while making none worse off.

#index 1269689
#* A multi-dimensional trust model for heterogeneous contract observations
#@ Steven Reece;Stephen Roberts;Alex Rogers;Nicholas R. Jennings
#t 2007
#c 10
#% 33917
#% 394009
#% 804916
#% 823905
#% 856790
#% 1024841
#! In this paper we develop a novel probabilistic model of computational trust that allows agents to exchange and combine reputation reports over heterogeneous, correlated multi-dimensional contracts. We consider the specific case of an agent attempting to procure a bundle of services that are subject to correlated quality of service failures (e.g. due to use of shared resources or infrastructure), and where the direct experience of other agents within the system consists of contracts over different combinations of these services. To this end, we present a formalism based on the Kalman filter that represents trust as a vector estimate of the probability that each service will be successfully delivered, and a covariance matrix that describes the uncertainty and correlations between these probabilities. We describe how the agents' direct experiences of contract outcomes can be represented and combined within this formalism and we empirically demonstrate that our formalism provides significantly better trustworthiness estimates than the alternative of using separate single-dimensional trust models for each separate service (where information regarding the correlations between each estimate is lost).

#index 1269690
#* Reasoning from desires to intentions: a dialectical framework
#@ Nicolás D. Rotstein;Alejandro J. García;Guillermo R. Simari
#t 2007
#c 10
#% 244091
#% 334024
#% 417812
#% 428335
#% 752766
#% 890250
#% 890271
#! Here, we define a framework where defeasible argumentation is used for reasoning about beliefs, desires and intentions. A dialectical filtering process is introduced to obtain a subset of the agent's desires containing only those that are achievable in the current situation. Different agents types can be defined in the framework affecting the way in which current desires are obtained. The agent is provided with a set of intention rules that specifies under what conditions an intention could be achieved. When more than one intention is present, a policy will be used to choose among them. Thus, intention policies provide the agent with a mechanism for deciding which intention is selected in the current situation. Several application examples will be given.

#index 1269691
#* A logic of emotions for intelligent agents
#@ Bas R. Steunebrink;Mehdi Dastani;John-Jules Ch. Meyer
#t 2007
#c 10
#% 68239
#% 238395
#% 289946
#% 643091
#% 1223231
#% 1837240
#! This paper formalizes a well-known psychological model of emotions in an agent specification language. This is done by introducing a logical language and its semantics that are used to specify an agent model in terms of mental attitudes including emotions. We show that our formalization renders a number of intuitive and plausible properties of emotions. We also show how this formalization can be used to specify the effect of emotions on an agent's decision making process. Ultimately, the emotions in this model function as heuristics as they constrain an agent's model.

#index 1269692
#* Valuation uncertainty and imperfect introspection in second-price auctions
#@ David R. M. Thompson;Kevin Leyton-Brown
#t 2007
#c 10
#% 460806
#% 643113
#% 773227
#% 781210
#% 808361
#% 819415
#% 823925
#% 890388
#% 912341
#! In auction theory, agents are typically presumed to have perfect knowledge of their valuations. In practice, though, they may face barriers to this knowledge due to transaction costs or bounded rationality. Modeling and analyzing such settings has been the focus of much recent work, though a canonical model of such domains has not yet emerged. We begin by proposing a taxonomy of auction models with valuation uncertainty and showing how it categorizes previous work. We then restrict ourselves to single-good sealed-bid auctions, in which agents have (uncertain) independent private values and can introspect about their own (but not others') valuations through possibly costly and imperfect queries. We investigate second-price auctions, performing equilibrium analysis for cases with both discrete and continuous valuation distributions. We identify cases where every equilibrium involves either randomized or asymmetric introspection. We contrast the revenue properties of different equilibria, discuss steps the seller can take to improve revenue, and identify a form of revenue equivalence across mechanisms.

#index 1269693
#* Reasoning about bargaining situations
#@ Dongmo Zhang
#t 2007
#c 10
#% 263126
#% 1289508
#! This paper presents a logical axiomatization of bargaining solutions. A bargaining situation is described in propositional logic and the bargainers' preferences are quantified in terms of the logical structure of the bargaining situation. A solution to the n-person bargaining problems is proposed based on the maxmin rule over the degrees of bargainers' satisfaction. We show that the solution is uniquely characterized by four natural and intuitive axioms as well as three other fundamental assumptions. All the axioms and assumptions are represented in logical statements and most of them have a game-theoretic counterpart. The framework would help us to identify the logical and numerical reasoning behind bargaining processes.

#index 1269694
#* On balanced CSPs with high treewidth
#@ Carlos Ansótegui;Ramón Béjar;César Fernàndez;Carles Mateu
#t 2007
#c 10
#% 70508
#% 157302
#% 167643
#% 175378
#% 210191
#% 282175
#% 346005
#% 419990
#% 576386
#% 720669
#% 723931
#% 740242
#% 807603
#% 1016533
#% 1275311
#% 1289196
#% 1700197
#% 1703568
#! Tractable cases of the binary CSP are mainly divided in two classes: constraint language restrictions and constraint graph restrictions. To better understand and identify the hardest binary CSPs, in this work we propose methods to increase their hardness by increasing the balance of both the constraint language and the constraint graph. The balance of a constraint is increased by maximizing the number of domain elements with the same number of occurrences. The balance of the graph is defined using the classical definition from graph theory. In this sense we present two graph models; a first graph model that increases the balance of a graph maximizing the number of vertices with the same degree, and a second one that additionally increases the girth of the graph, because a high girth implies a high treewidth, an important parameter for binary CSPs hardness. Our results show that our more balanced graph models and constraints result in harder instances when compared to typicaI random binary CSP instances, by several orders of magnitude. Also we detect, at least for sparse constraint graphs, a higher treewidth for our graph models.

#index 1269695
#* Inference rules for high-order consistency in weighted CSP
#@ Carlos Ansótegui;María L. Bonet;Jordi Levy;Felip Manyà
#t 2007
#c 10
#% 289947
#% 303681
#% 789556
#% 816233
#% 1250517
#% 1274756
#% 1289364
#% 1289381
#% 1728041
#! Recently defined resolution calculi for Max-SAT and signed Max-SAT have provided a logical characterization of the solving techniques applied by Max-SAT and WCSP solvers. In this paper we first define a new resolution rule, called signed Max-SAT parallel resolution, and prove that it is sound and complete for signed Max-SAT. Second, we define a restriction and a generalization of the previous rule called, respectively, signed Max-SAT i-consistency resolution and signed Max-SAT (i,j)-consistency resolution. These rules have the following property: if a WCSP signed encoding is closed under signed Max-SAT i-consistency, then the WCSP is i-consistent, and if it is closed under signed Max-SAT (i,j)-consistency, then the WCSP is (i,j)-consistent. A new and practical insight derived from the definition of these new rules is that algorithms for enforcing high order consistency should incorporate an efficient and effective component for detecting minimal unsatisfiable cores. Finally, we describe an algorithm that applies directional soft consistency with the previous rules.

#index 1269696
#* Randomized adaptive spatial decoupling for large-scale vehicle routing with time windows
#@ Russell Bent;Pascal Van Hentenryck
#t 2007
#c 10
#% 578726
#% 644201
#% 799036
#% 819824
#% 824875
#% 955848
#% 959996
#% 960070
#% 960071
#! In recent years, the size of combinatorial applications and the need to produce high-quality solutions quickly have increased steadily, providing significant challenges for optimization algorithms. This paper addresses this issue for large-scale vehicle routing problems with time windows, a class of very difficult optimization problems involving complex spatial and temporal dependencies. It proposes a randomized adaptive spatial decoupling (RASD) scheme for vehicle routing with time windows in order to produce high-quality solutions quickly. Experimental results on hard instances with 1,000 customers and 90 vehicles show that the RASD scheme, together with large neighborhood search, significantly improves the quality of the solutions under time constraints. Interestingly, the RASD scheme, when allowed to run longer, also improves the best available solutions in almost all the tested instances.

#index 1269697
#* Search space reduction and Russian Doll search
#@ Kenil K. C. Cheng;Roland H. C. Yap
#t 2007
#c 10
#% 3873
#% 55926
#% 283092
#% 289332
#% 535302
#% 873073
#% 1272086
#! In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.

#index 1269698
#* Using more reasoning to improve #SAT solving
#@ Jessica Davies;Fahiem Bacchus
#t 2007
#c 10
#% 336874
#% 578747
#% 578749
#% 723877
#% 1250342
#% 1250515
#% 1269433
#% 1289375
#% 1289558
#% 1698709
#% 1728026
#% 1728055
#! Many real-world problems, including inference in Bayes Nets, can be reduced to #SAT, the problem of counting the number of models of a propositional theory. This has motivated the need for efficient #SAT solvers. Currently, such solvers utilize a modified version of DPLL that employs decomposition and caching, techniques that significantly increase the time it takes to process each node in the search space. In addition, the search space is significantly larger than when solving SAT since we must continue searching even after the first solution has been found. It has previously been demonstrated that the size of a DPLL search tree can be significantly reduced by doing more reasoning at each node. However, for SAT the reductions gained are often not worth the extra time required. In this paper we verify the hypothesis that for #SAT this balance changes. In particular, we show that additional reasoning can reduce the size of a #SAT solver's search space, that this reduction cannot always be achieved by the already utilized technique of clause learning, and that this additional reasoning can be cost effective.

#index 1269699
#* Data structures for generalised arc consistency for extensional constraints
#@ Ian P. Gent;Chris Jefferson;Ian Miguel;Peter Nightingale
#t 2007
#c 10
#% 326878
#% 420719
#% 838024
#% 1223203
#% 1223207
#% 1269422
#% 1664971
#% 1664978
#! Extensional (table) constraints are an important tool for attacking combinatorial problems with constraint programming. Recently there has been renewed interest in fast propagation algorithms for these constraints. We describe the use of two alternative data structures for maintaining generalised arc consistency on extensional constraints. The first, the Next-Difference list, is novel and has been developed with this application in mind. The second, the trie, is well known but its use in this context is novel. Empirical analyses demonstrate the efficiency of the resulting approaches, both in GAC-schema, and in the watched-literal table constraint in Minion.

#index 1269700
#* Approximate counting by sampling the backtrack-free search space
#@ Vibhav Gogate;Rina Dechter
#t 2007
#c 10
#% 327779
#% 374580
#% 448887
#% 529186
#% 1271825
#% 1275122
#% 1665012
#% 1698716
#% 1799876
#! We present a new estimator for counting the number of solutions of a Boolean satisfiability problem as a part of an importance sampling framework. The estimator uses the recently introduced SampleSearch scheme that is designed to overcome the rejection problem associated with distributions having a substantial amount of determinism. We show here that the sampling distribution of SampleSearch can be characterized as the backtrack-free distribution and propose several schemes for its computation. This allows integrating Sample-Search into the importance sampling framework for approximating the number of solutions and also allows using Sample-Search for computing a lower bound measure on the number of solutions. Our empirical evaluation demonstrates the superiority of our new approximate counting schemes against recent competing approaches.

#index 1269701
#* Counting CSP solutions using generalized XOR constraints
#@ Carla P. Gomes;Willem-Jan Van Hoeve;Ashish Sabharwal;Bart Selman
#t 2007
#c 10
#% 529186
#% 1250515
#% 1289604
#% 1728054
#! We present a general framework for determining the number of solutions of constraint satisfaction problems (CSPs) with a high precision. Our first strategy uses additional binary variables for the CSP, and applies an XOR or parity constraint based method introduced previously for Boolean satisfiability (SAT) problems. In the CSP framework, in addition to the naive individual filtering of XOR constraints used in SAT, we are able to apply a global domain filtering algorithm by viewing these constraints as a collection of linear equalities over the field of two elements. Our most promising strategy extends this approach further to larger domains, and applies the so-called generalized XOR constraints directly to CSP variables. This allows us to reap the benefits of the compact and structured representation that CSPs offer. We demonstrate the effectiveness of our counting framework through experimental comparisons with the solution enumeration approach (which, we believe, is the current best generic solution counting method for CSPs), and with solution counting in the context of SAT and integer programming.

#index 1269702
#* Compressing configuration data for memory limited devices
#@ Esben Rune Hansen;Peter Tiedemann
#t 2007
#c 10
#% 3873
#% 928731
#% 1698020
#! The paper introduces a new type of compression for decision diagram data structures, such as BDDs, MDDs and AOMDDs. The compression takes advantage of repeated substructures within the decision diagram in order to lessen redundancy beyond what is possible using simple subfunction sharing. The resulting compressed data structure allows traversal of the original decision diagram with no significant overhead. Specifically it allows the efficient computation of valid domains, that is, the assignments for each encoded variable that can participate in a solution, which is critical when the decision diagram is used to support an interactive configurator. We relate these results to applications for interactively configurable memory limited devices and give empirical results on the amount of saved space for a wide variety of instances.

#index 1269703
#* Interactive configuration with regular string constraints
#@ Esben Rune Hansen;Henrik Reif Andersen
#t 2007
#c 10
#% 3873
#% 327779
#% 345434
#% 410276
#! In this paper we present a generalization of the problem of interactive configuration. The usual interactive configuration problem is the problem of, given some variables on small finite domains and an increasing set of assignment of values to a subnet of the variables, to compute for each of the unassigned variables which values in its domain that participate in some solution for some assignment of values to the other unassigned variables. In this paper we consider how to extend this scheme to handle infinite regular domains using string variables and constraints that involves regular-expression checks on the string variables. We first show how to do this by using one single DFA. Since this approach is vastly space consuming, we construct a data structure that simulates the large DFA and is much more space efficient. As an example a configuration problem on n string variables with only one solution in which each string variable is assigned a value oflength k the former structure will use Ω(kn) space whereas the latter only need O(kn). We also show how this framework can be combined with the recent BDD techniques to allow both boolean, integer and string variables in the configuration problem.

#index 1269704
#* Using expectation maximization to find likely assignments for solving CSP's
#@ Eric I. Hsu;Matthew Kitching;Fahiem Bacchus;Sheila A. Mcllraith
#t 2007
#c 10
#% 44876
#% 160208
#% 277483
#% 644201
#% 819505
#% 1250323
#% 1269577
#% 1272154
#% 1279379
#% 1289196
#% 1665017
#% 1672998
#% 1728048
#% 1810385
#! We present a new probabilistic framework for finding likely variable assignments in difficult constraint satisfaction problems. Finding such assignments is key to efficient search, but practical efforts have largely been limited to random guessing and heuristically designed weighting systems. In contrast, we derive a new version of Belief Propagation (BP) using the method of Expectation Maximization (EM). This allows us to differentiate between variables that are strongly biased toward particular values and those that are largely extraneous. Using EM also eliminates the threat of non-convergence associated with regular BP. Theoretically, the derivation exhibits appealing primal/dual semantics. Empirically, it produces an "EMBP"-based heuristic for solving constraint satisfaction problems, as illustrated with respect to the Quasigroup with Holes domain. EMBP outperforms existing techniques for guiding variable and value ordering during backtracking search on this problem.

#index 1269705
#* Propagating knapsack constraints in sublinear time
#@ Irit Katriel;Meinolf Sellmann;Eli Upfal;Pascal Van Hentenryck
#t 2007
#c 10
#% 808079
#! We develop an efficient incremental version of an existing cost-based filtering algorithm for the knapsack constraint. On a universe of n elements, m invocations of the algorithm require a total of O(n log n+mk log(n/k)) time, where k ≤ n depends on the specific knapsack instance. We show that the expected value of k is significantly smaller than n on several interesting input distributions, hence while keeping the same worst-case complexity, on expectation the new algorithm is faster than the previously best method which runs in amortized linear time. After a theoretical study, we introduce heuristic enhancements and demonstrate the new algorithm's performance experimentally.

#index 1269706
#* Conservative dual consistency
#@ Christophe Lecoutre;Stéphane Cardon;Julien Vion
#t 2007
#c 10
#% 3463
#% 224751
#% 637595
#% 838024
#% 1223218
#% 1271960
#% 1274771
#% 1289359
#! Consistencies are properties of Constraint Networks (CNs) that can be exploited in order to make inferences. When a significant amount of such inferences can be performed, CNs are much easier to solve. In this paper, we interest ourselves in relation filtering consistencies for binary constraints, i.e. consistencies that allow to identify inconsistent pairs of values. We propose a new consistency called Dual Consistency (DC) and relate it to Path Consistency (PC). We show that Conservative DC (CDC, i.e. DC with only relations associated with the constraints of the network considered) is more powerful, in terms of filtering, than Conservative PC (CPC). Following the approach of Mac Gregor, we introduce an algorithm to establish (strong) CDC with a very low worst-case space complexity. Even if the relative efficiency of the algorithm introduced to establish (strong) CDC partly depends on the density of the constraint graph, the experiments we have conducted show that, on many series of CSP instances, CDC is largely faster than CPC (up to more than one order of magnitude). Besides, we have observed that enforcing CDC in a preprocessing stage can significantly speed up the resolution of hard structured instances.

#index 1269707
#* Transposition tables for constraint satisfaction
#@ Christophe Lecoutre;Lakhdar Sais;Sébastien Tabary;Vincent Vidal
#t 2007
#c 10
#% 68183
#% 443807
#% 534837
#% 534978
#% 795186
#% 873074
#% 1137061
#% 1271960
#% 1271962
#! In this paper, a state-based approach for the Constraint Satisfaction Problem (CSP) is proposed. The key novelty is an original use of state memorization during search to prevent the exploration of similar subnetworks. Classical techniques to avoid the resurgence of previously encountered conflicts involve recording conflict sets. This contrasts with our state-based approach which records subnetworks - a snapshot of some selected domains - already explored. This knowledge is later used to either prune inconsistent states or avoid recomputing the solutions of these subnetworks. Interestingly enough, the two approaches present some complementarity: different states can be pruned from the same partial instantiation or conflict set, whereas different partial instantiations can lead to the same state that needs to be explored only once. Also, our proposed approach is able to dynamically break some kinds of symmetries (e.g. neighborhood interchangeability). The obtained experimental results demonstrate the promising prospects of state-based search.

#index 1269708
#* Multi-objective Russian Doll search
#@ Emma Rollon;Javier Larrosa
#t 2007
#c 10
#% 126386
#% 534964
#% 866429
#% 928731
#% 953070
#% 1223211
#% 1223213
#% 1279246
#! Russian Doll Search (RDS) is a well-known algorithm for combinatorial optimization. In this paper we extend it from mono-objective to multi-objective optimization. We demonstrate its practical applicability in the challenging multiple-orbit SPOT5 instances. Besides being much more efficient than any other alternatives, multi-objective RDS can solve an instance which could not have been solved previously.

#index 1269709
#* Learning to solve QBF
#@ Horst Samulowitz;Roland Memisevic
#t 2007
#c 10
#% 327779
#% 336874
#% 517569
#% 864263
#% 1396055
#% 1664973
#% 1728050
#! We present a novel approach to solving Quantified Boolean Formulas (QBF) that combines a search-based QBF solver with marhine learning techniques. We show how classification methods can be used to predict run-times and to choose optimal heuristics both within a portfolio-based, and within a dynamic, online approach. In the dynamic method variables are set to a truth value according to a scheme that tries to maximize the probability of successfully solving the remaining sub-problem efficiently. Since each variable assignment can drastically change the problem-structure, new heuristics are chosen dynamically, and a classifier is used online to predict the usefulness of each heuristic. Experimental results on a large corpus of example problems show the usefulness of our approach in terms of run-time as well as the ability to solve previously unsolved problem instances.

#index 1269710
#* Solving a stochastic queueing design and control problem with constraint programming
#@ Daria Terekhov;J. Christopher Beck;Kenneth N. Brown
#t 2007
#c 10
#% 4185
#% 690812
#% 958611
#% 1190932
#% 1223220
#% 1732239
#! A facility with front room and back room operations has the option of hiring specialized or, more expensive, cross-trained workers. Assuming stochastic customer arrival and service times, we seek a smallest-cost combination of cross-trained and specialized workers satisfying constraints on the expected customer waiting time and expected number of workers in the back room. A constraint programming approach using logic-based Benders' decomposition is presented. Experimental results demonstrate the strong performance of this approach across a wide variety of problem parameters. This paper provides one of the first links between queueing optimization problems and constraint programming.

#index 1269711
#* Population-based simulated annealing for traveling tournaments
#@ Pascal Van Hentenryck;Yannis Vergados
#t 2007
#c 10
#% 217717
#% 422306
#% 534810
#% 862472
#% 948057
#% 1732436
#! This paper reconsiders the travelling tournament problem, a complex sport-scheduling application which has attracted significant interest recently. It proposes a population-based simulated annealing algorithm With both itensification and diversitication. The algorithm is organized as a series of simulated annealing waves, each wave being followed by a macro-intensification. The diversification is obtained through the concept of elite runs that opportunistically survive waves. A parallel implementation of the algorithm on a cluster of workstations exhibits remarkable results. It improves the best known solutions on all considered benchmarks, sometimes reduces the optimality gap by about 60%, and produces novel best solutions on instances that had been stable for several years.

#index 1269712
#* Synthesis of constraint-based local search algorithms from high-level models
#@ Pascal Van Hentenryck;Laurent Michel
#t 2007
#c 10
#% 857142
#! The gap in automation between MIP/SAT solvers and those for constraint programming and constraint-based local search hinders experimentation and adoption of these technologies and slows down scientific progress. This paper addresses this important issue: It shows how effective local search procedures can be automatically synthesized from models expressed in a rich constraint language. The synthesizer analyzes the model and derives the local search algorithm for a specific meta-heuristic by exploiting the structure of the model and the constraint semantics. Experimental results suggest that the synthesized procedures only induce a small loss in efficiency on a variety of realistic applications in sequencing, resource allocation, and facility location.

#index 1269713
#* Learning by reading: a prototype system, performance baseline and lessons learned
#@ Ken Barker;Bhalchandra Agashe;Shaw-Yi Chaw;James Fan;Noah Friedland;Michael Glass;Jerry Hobbs;Eduard Hovy;David Israel;Doo Soon Kim;Rutu Mulkar-Mehta;Sourabh Patwardhan;Bruce Porter;Dan Tecuci;Peter Yeh
#t 2007
#c 10
#% 90188
#% 145393
#% 258600
#% 341640
#% 828974
#% 828977
#% 853907
#% 1250182
#% 1275182
#% 1344851
#% 1713515
#! A traditional goal of Artificial Intelligence research has been a system that can read unrestricted natural language texts on a given topic, build a model of that topic and reason over the model. Natural Language Processing advances in syntax and semantics have made it possible to extract a limited form of meaning from sentences. Knowledge Representation research has shown that it is possible to model and reason over topics in interesting areas of human knowledge. It is useful for these two communities to reunite periodically to see where we stand with respect to the common goal of text understanding. In this paper, we describe a coordinated effort among researchers from the Natural Language and Knowledge Representation and Reasoning communities. We routed the output of existing NL software into existing KR software to extract knowledge from texts for integration with engineered knowledge bases. We tested the system on a suite of roughly 80 small English texts about the form and function of the human heart, as well as a handful of "confuser" texts from other domains. We then manually evaluated the knowledge extracted from novel texts. Our conclusion is that the technology from these fields is mature enough to start producing unified machine reading systems. The results of our exercise provide a performance baseline for systems attempting to acquire models from text.

#index 1269714
#* A tempora mereology for distinguishing between integral objects and portions of stuff
#@ Thomas Bittner;Maureen Donnelly
#t 2007
#c 10
#% 344361
#% 731208
#% 766116
#% 1414319
#! We develop a formal theory of mereology that includes relations that change over time. We show how this theory formalizes reasoning over domains of material objects, which include not only integral objects (my computer, your liver) but also portions of stuff (the water in your glass, the blood in a vial). In particular, we use different mereological summation relations to distinguish between the ways in which i) integral objects, ii) portions of unstructured, homogenous stuffs (e.g. the water in your glass), and iii) mixtures (the blood in a vial) are linked to their parts over time.

#index 1269715
#* A qualitative approach to multiple fault isolation in continuous systems
#@ Matthew Daigle;Xenofon Koutsoukos;Gautam Biswas
#t 2007
#c 10
#% 21138
#% 36561
#% 927441
#% 1476264
#% 1784363
#% 1784383
#% 1784631
#! The multiple fault diagnosis problem is important, since the single fault assumption can lead to incorrect or failed diagnoses when multiple faults occur. It is challenging for continuous systems, because faults can mask or compensate each other's effects, and the solution space grows exponentially with the number of possible faults. We present a qualitative approach to multiple fault isolation in dynamic systems based on analysis of fault transient behavior. Our approach uses the observed measurement deviations and their temporal orderings to generate multiple fault hypotheses. The approach has polynomial space requirements and prunes diagnoses, resulting in an efficient online fault isolation scheme.

#index 1269716
#* Scalable semantic retrieval through summarization and refinement
#@ Julian Dolby;Achille Fokoue;Aditya Kalyanpur;Aaron Kershenbaum;Edith Schonberg;Kavitha Srinivas;Li Ma
#t 2007
#c 10
#% 519438
#% 561419
#% 916864
#% 1269453
#% 1667768
#% 1696308
#! Query processing of OWL-DL ontologies is intractable in the worst case, but we present a novel technique that in practice allows for efficient querying of ontologies with large Aboxes in secondary storage. We focus on the processing of instance retrieval queries, i.e., queries that retrieve individuals in the Abox which are instances of a given concept C. Our technique Uses summarization and refinement to reduce instance retrieval to a small relevant subset of the original Abox. We demonstrate the effectiveness of this technique in Aboxes with up to 7 million assertions. Our results are applicable to the very expressive description logic SHIN, which corresponds to OWL-DL minus nominals and datatypes.

#index 1269717
#* Diagnosis of discrete-event systems using satisfiability algorithms
#@ Alban Grastien;Anbulagan Anbulagan;Jussi Rintanen;Elena Kelareva
#t 2007
#c 10
#% 21137
#% 327779
#% 420713
#% 1274798
#% 1274837
#% 1274838
#% 1476265
#% 1476298
#! The diagnosis of a discrete-event system is the problem of computing possible behaviors of the system given observations of the actual behavior, and testing whether the behaviors are normal or faulty. We show how the diagnosis problems can be translated into the propositional satisfiability problem (SAT) and solved by algorithms for SAT. Our experiments demonstrate that current SAT algorithms can solve much bigger diagnosis problems than traditional diagnosis algorithms can.

#index 1269718
#* On capturing semantics in ontology mapping
#@ Bo Hu;Srinandan Dasmahapatra;Paul Lewis;Nigel Shadbolt
#t 2007
#c 10
#% 348187
#% 375017
#% 445515
#% 572314
#% 665856
#% 830522
#% 975019
#% 1289178
#% 1728979
#! Ontology mapping is a complex and necessary task for many Semantk Web (SW) applications. The perspective users are faced with a number of challenges including the difficulties of capturing semantics. In this paper we present a three-dimensional ontology mapping model. This model reflects the engineering steps needed to materialise a versatile mapping system in order to meet the demands on semantic interoperability in the SW environment. We solidify the formalisation with specialised algorithms and we analyse their effectiveness and performance by way of benchmark tests.

#index 1269719
#* Tablerank: a ranking algorithm for table search and retrieval
#@ Ying Liu;Kun Bai;Prasenjit Mitra;C. Lee Giles
#t 2007
#c 10
#% 46803
#% 218982
#% 262096
#% 268079
#% 387427
#% 728195
#% 818241
#% 869535
#% 869548
#% 874518
#% 879567
#% 879574
#% 879662
#% 967256
#! Tables are ubiquitous in web pages and scientific documents. With the explosive development of the web, tables have become a valuable information repository. Therefore, effectively and efficiently searching tables becomes a challenge. Existing search engines do not provide satisfactory search results largely because the current ranking schemes are inadequate for table search and automatic table understanding and extraction are rather difficult in general. In this work, we design and evaluate a novel table ranking algorithm-TableRank to improve the performance of our table search engine Table-Seer. Given a keyword based table query, TableRank facilities TableSeer to return the most relevant tables by tailoring the classic vector space model. TableRank adopts an innovative term weighting scheme by aggregating multiple weighting factors from three levels: term, table and document. The experimental results show that our table search engine out-performs existing search engines on table search. In addition, incorporating multiple weighting factors can significantly improve the ranking results.

#index 1269720
#* Representative explanations for over-constrained problems
#@ Barry O'Sullivan;Alexandre Papadopoulos;Boi Faltings;Pearl Pu
#t 2007
#c 10
#% 252636
#% 345434
#% 779972
#% 1250145
#% 1275123
#% 1289383
#% 1394352
#% 1499513
#% 1698705
#% 1722417
#! In many interactive decision making scenarios there is often no solution that satisfies all of the user's preferences. The decision process can be helped by providing explanations. Relaxation show sets of consistent preferences and, thus, indicate Which preferences can be enforced, while exclusion sets show which preferences can be relaxed to obtain a solution. We propose a new approach to explanation based on the notion of a representative set of explanations. The size of the set of explanations we compute is exponentially more compact than that found using common approaches from the literature based on finding all minimal conflicts.

#index 1269721
#* L2R: a logical method for reference reconciliation
#@ Fatiha Saïs;Nathalie Pernelle;Marie-Christine Rousset
#t 2007
#c 10
#% 328186
#% 384112
#% 480496
#% 572314
#% 729913
#% 790852
#% 810014
#% 1673578
#! The reference reconciliation problem consists in deciding whether different identifiers refer to the same data, i.e., correspond to the same world entity. The L2R system exploits the semantics of a rich data model, which extends RDFS by a fragment of OWL-DL and SWRL rules. In L2R, the semantics of the schema is translated into a set of logical rules of reconciliation, which are then used to infer correct decisions both of reconciliation and no reconciliation. In contrast with other approaches, the L2R method has a precision of 100% by construction. First experiments show promising results for recall, and most importantly significant increases when rules are added.

#index 1269722
#* A spectrum of symbolic on-line diagnosis approaches
#@ Anika Schumann;Yannick Pencolé;Sylvie Thiébaux
#t 2007
#c 10
#% 3873
#% 257696
#% 275962
#% 496408
#% 819614
#% 873955
#% 1272329
#% 1272349
#% 1274837
#% 1279265
#! This paper deals with the monitoring and diagnosis of large discrete-event systems. The problem is to determine, on-line, all faults and states that explain the flow of observations. Model-based diagnosis approaches that first compile the diagnosis information off-line suffer from space explosion, and those that operate on-line without any prior compilation have poor time performance. Our contribution is a broader spectrum of approaches that suits applications with diverse time and space requirements. Approaches on this spectrum differ in the amount of reasoning and compilation performed off-line and therefore in the way they resolve the tradeoff between the space occupied by the compiled information and the time taken to produce a diagnosis. We tackle the space and time complexity of diagnosis by encoding all approaches in a symbolic framework based on binary decision diagrams. This allows for the compact representation of the compiled diagnosis information, and for its handling across many states at once rather than for each state individually. Our experiments demonstrate the diversity and scalability of our symbolic methods spectrum, as well as its superiority over the corresponding enumerative implementations.

#index 1269723
#* Mining web query hierarchies from clickthrough data
#@ Dou Shen;Min Qin;Weizhu Chen;Qiang Yang;Zheng Chen
#t 2007
#c 10
#% 169729
#% 280849
#% 310567
#% 341006
#% 401405
#% 642982
#% 730051
#% 756964
#% 785375
#% 805878
#% 894253
#% 1077150
#% 1272078
#% 1275208
#! In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages: generating candidate queries and determining "generalization/specialization" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.

#index 1269724
#* Posterior probability profiles for the automated assessment of the recovery of stroke patients
#@ Gert Van Dijck;Jo Van Vaerenbergh;Marc M. Van Hulle
#t 2007
#c 10
#% 115608
#% 243728
#% 246831
#% 778736
#% 1213038
#% 1694687
#! Assessing recovery from stroke has been so far a time consuming procedure in which highly trained clinicians are required. This paper proposes a mechatronic platform which measures low forces and torques exerted by subjects, Class posterior probabilities are used as a quantitative and statistically sound tool to assess motor recovery from these force and torque measurements. The performance of the patients is expressed in terms of the posterior probability to belong to the class of normal subjects. The mechatronic platform together with the class posterior probabilities enables to automate motor recovery assessment without the need for highly trained clinicians. It is shown that the class posterior probability profiles are highly correlated, r ≈ 0.8, with the well-established Fugl-Meyer scale assessment in motor recovery. These results have been obtained through careful feature subset selection procedures in order to prune the large feature set being generated. The overall approach is general and can be applied to many other health monitoring systems where different categories (diseased vs. healthy) can be identified.

#index 1269725
#* Learning causal models for noisy biological data mining: an application to ovarian cancer detection
#@ Ghim-Eng Yap;Ah-Hwee Tan;Hwee-Hwa Pang
#t 2007
#c 10
#% 44876
#% 67866
#% 136350
#% 297490
#% 643688
#% 826260
#% 830837
#! Undetected errors in the expression measurements from high-throughput DNA microarrays and protein spectroscopy could seriously affect the diagnostic reliability in disease detection. In addition to a high resilience against such errors, diagnostic models need to be more comprehensible so that a deeper understanding of the causal interactions among biological entities like genes and proteins may be possible. In this paper, we introduce a robust knowledge discovery approach that addresses these challenges. First, the causal interactions among the genes and proteins in the noisy expression data are discovered automatically through Bayesian network learning. Then, the diagnosis of a disease based on the network is performed using a novel error-handling procedure, which automatically identifies the noisy measurements and accounts for their uncertainties during diagnosis. An application to the problem of ovarian cancer detection shows that the approach effectively discovers causal interactions among cancer-specific proteins. With the proposed error-handling procedure, the network perfectly distinguishes between the cancer and normal patients.

#index 1269726
#* DL-lite in the light of first-order logic
#@ A. Artale;D. Calvanese;R. Kontchakov;M. Zakharyaschev
#t 2007
#c 10
#% 58347
#% 378409
#% 384978
#% 445447
#% 598376
#% 801692
#% 871211
#% 1016250
#% 1250550
#% 1269453
#% 1289408
#% 1440770
#! The use of ontologies in various application domains, such as Data Integration, the Semantic Web, or ontology-based data management, where ontologies provide the access to large amounts of data, is posing challenging requirements w.r.t. a trade-off between expressive power of a DL and efficiency of reasoning. The logics of the DL-Lite family were specifically designed to meet such requirements and optimized w.r.t. the data complexity of answering complex types of queries. In this paper we propose DL-Litebool, an extension of DL-Lite with full Booleans and number restrictions, and study the complexity of reasoning in DL-Litebool and its significant sub-logics. We obtain our results, together with useful insights into the properties of the studied logics, by a novel reduction to the one-variable fragment of first-order logic. We study the computational complexity of satisfiability and subsumption, and the data complexity of answering positive existential queries (which extend unions of conjunctive queries). Notably, we extend the LOGSPACE upper bound for the data complexity of answering unions of conjunctive queries in DL-Lite to positive queries and to the possibility of expressing also number restrictions, and hence local functionality in the TBox.

#index 1269727
#* An egalitarist fusion of incommensurable ranked belief bases under constraints
#@ Salem Benferhat;Sylvain Lagrue;Julien Rossit
#t 2007
#c 10
#% 137868
#% 167544
#% 207782
#% 224753
#% 1250551
#% 1290097
#! In the last decade, several approaches have been proposed for merging multiple and potentially conflicting pieces of information. Egalitarist fusion modes privilege solutions that minimize the (local) dissatisfaction of each agent (source, expert) who is involved in the fusion process. This paper proposes useful strategies for an egalitarist fusion of incommensurable ranked belief bases under constraints. We show that the fusion process can equivalently be characterized either by means of the notion of compatible ranked bases, or by means of a Pareto-like ordering on the set of possible solutions. Lastly, rational postulates for our merging operator are studied.

#index 1269728
#* Possibilistic causal networks for handling interventions: a new propagation algorithm
#@ Salem Benferhat;Salma Smaoui
#t 2007
#c 10
#% 44876
#% 167544
#% 297171
#% 380725
#% 527528
#% 527830
#! This paper contains two important contributions for the development of possibilistic causal networks. The first one concerns the representation of interventions in possibilistic networks. We provide the counterpart of the "DO" operator, recently introduced by Pearl, in possibility theory framework. We then show that interventions can equivalently be represented in different ways in possibilistic causal networks. The second main contribution is a new propagation algorithm for dealing with both observations and interventions. We show that our algorithm only needs a small extra cost for handling interventions and is more appropriate for handling sequences of observations and interventions.

#index 1269729
#* Prime implicates and prime implicants in modal logic
#@ Meghyn Bienvenu
#t 2007
#c 10
#% 57898
#% 338753
#% 1272349
#! The purpose of this paper is to extend the notions of prime implicates and prime implicants to the basic modal logic κ. We consider a number of different potential definitions of clauses and terms for κ, which we evaluate with respect to their syntactic, semantic, and complexity-theoretic properties. We then continue our analysis by comparing the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce. We provide algorithms and complexity results for the prime implicate generation and recognition tasks for the two most satisfactory definitions.

#index 1269730
#* Equilibria in heterogeneous nonmonotonic multi-context systems
#@ Gerhard Brewka;Thomas Eiter
#t 2007
#c 10
#% 1146
#% 28185
#% 160385
#% 245349
#% 333242
#% 1274794
#% 1274828
#% 1289365
#% 1289440
#% 1907873
#! We propose a general framework for multi-context reasoning which allows us to combine arbitrary monotonic and nonmonotonic logics. Nonmonotonic bridge rules are used to specify the information flow among contexts. We investigate several notions of equilibrium representing acceptable belief states for our multi-context systems. The approach generalizes the heterogeneous monotonic multi-context systems developed by F. Giunchiglia and colleagues as well as the homogeneous nonmonotonic multi-context systems of Brewka, Serafini and Roelofsen.

#index 1269731
#* Answering regular path queries in expressive description logics: an automata-theoretic approach
#@ Diego Calvanese;Thomas Eiter;Magdalena Ortiz
#t 2007
#c 10
#% 248026
#% 445447
#% 494344
#% 665856
#% 1274815
#% 1289425
#! Expressive Description Logics (DLs) have been advocated as formalisms for modeling the domain of interest in various application areas. An important requirement is the ability to answer complex queries beyond instance retrieval, taking into account constraints expressed in a knowledge base. We consider this task for positive existential path queries (which generalize conjunctive queries and unions thereof), whose atoms are regular expressions over the roles (and concepts) of a knowledge base in the expressive DL ALCQIbreg. Using techniques based on two-way tree-automata, we first provide an elegant characterization of TBox and ABox reasoning, which gives us also a tight EXPTIME bound. We then prove decidability (more precisely, a 2EXPTIME upper bound) of query answering, thus significantly pushing the decidability frontier, both with respect to the query language and the considered DL. We also show that query answering is Exp-SPACE-hard already in rather restricted settings.

#index 1269732
#* Approximate query answering in locally closed databases
#@ Álvaro Cortés-Calabuig;Marc Denecker;Ofer Arieli;Maurice Bruynooghe
#t 2007
#c 10
#% 67457
#% 224758
#% 248038
#% 378409
#% 384978
#% 481786
#% 1656400
#% 1666181
#! The Closed-World Assumption (CWA) on databases expresses that an atom not in the database is false. A more appropriate assumption for databases that are sound but partially incomplete, is the Local Closed-World Assumption (LCWA), which is a local form of the CWA, expressing that the database is complete in a certain area, called the 'window of expertise'. Databases consisting of a standard database instance augmented with a collection of LCWA's are called locally closed databases. In this paper, we investigate the complexity of certain and possible query answering in such databases. As it tums out that these problems are intractlble, we develop efficient approximate methods to underestimate certain answers and overestimate possible answers. We prove that under certain conditions, our methods produce complete answers.

#index 1269733
#* On the approximation of instance level update and erasure in description logics
#@ Giuseppe De Giacomo;Maurizio Lenzerini;Antonella Poggi;Riccardo Rosati
#t 2007
#c 10
#% 90860
#% 184793
#% 291003
#% 665856
#% 1250357
#% 1269453
#% 1702407
#! A Description Logics knowledge base is constituted by two components, called TBox and ABox, where the former expresses general knowledge about the concepts and their relationships, and the latter describes the properties of instances of concepts. We address the problem of how to deal with changes to a Description Logic knowledge base, when these changes affect only its ABox. We consider two types of changes namely update and erasure, and we characterize the semantics of these operations on the basis of the approaches proposed by Winslett and by Katsuno and Mendelzon. It is well known that, in general, Description Logics are not closed with respect to updates, in the sense that the set of models corresponding to an update applied to a knowledge base in a Description Logic L may not be expressible by ABoxes in L. We show that this is true also for erasure. To deal with this problem, we introduce the notion of best approximation of an update (erasure) in a DL L, with the goal of characterizing the L ABoxes that capture the update (erasure) at best. We then focus on DL-LiteF, a tractable Description Logic, and present polynomial algorithms for computing the best approximation of updates and erasures in this logic, which shows that the nice computational properties of DL-LiteF are retained in dealing with the evolution of the ABox.

#index 1269734
#* Forgetting actions in domain descriptions
#@ Esra Erdem;Paolo Ferraris
#t 2007
#c 10
#% 26351
#% 266241
#% 529665
#% 544782
#% 763743
#% 1250544
#% 1269466
#% 1271987
#% 1289417
#% 1374395
#! Forgetting irrelevant/problematic actions in a domain description can be useful in solving reasoning problems, such as query answering, planning, conftict resolution, prediction, postdiction, etc.. Motivated by such applications, we study what forgetting is, how forgetting can be done, and for which applications forgetting can be useful and how, in the context of reasoning about actions. We study these questions in the action language C (a formalism based on causal explanations), and relate it to forgetting in classical logic and logic programming.

#index 1269735
#* Discovering near symmetry in graphs
#@ Maria Fox;Derek Long;Julie Porteous
#t 2007
#c 10
#% 112912
#% 534506
#% 865293
#% 1289545
#! Symmetry is a widespread phenomenon that can offer opportunities for powerful exploitation in areas as diverse as molecular chemistry, pure mathematics, circuit design, biology and architecture. Graphs are an abstract way to represent relational structures. The search for symmetry in many contexts can thus be reduced to the attempt to find graph automorphisms. Brendan McKay's NAUTY system (McKay 1990) is an example of one of the highly successful products of research into this task. Erdös and Rényi showed that almost all large graphs are asymmetric, but it is readily observed that many graphs representing structures of real Interest contain symmetry. Even more graphs are nearly symmmetric, in the sense that to each graph there is a closely Similar graph that is symmetric. In this paper we explore the problem of finding near symmetries in graphs and describe the techniques we are developing for performing this task.

#index 1269736
#* A logical theory of coordination and joint ability
#@ Hojjat Ghaderi;Hector Levesque;Yves Lespérance
#t 2007
#c 10
#% 342119
#% 484915
#% 572366
#% 781198
#% 944865
#! A team of agents is jointly able to achieve a goal if despite any incomplete knowledge they may have about the world or each other, they still know enough to be able to get to a goal state. Unlike in the single-agent case, the mere existence of a working plan is not enough as there may be several incompatible working plans and the agents may not be able to choose a share that coordinates with those of the others. Some formalizations of joint ability ignore this issue of coordination within a coalition. Others, including those based on game theory, deal with coordination, but require a complete specification of what the agents believe. Such a complete specification is often not available. Here we present a new formalization of joint ability based on logical entailment in the situation calculus that avoids both of these pitfalls.

#index 1269737
#* Belief change and cryptographic protocol verification
#@ Aaron Hunter;James P. Delgrande
#t 2007
#c 10
#% 68144
#% 99831
#% 224753
#% 421961
#% 491457
#% 511949
#% 664686
#% 1275155
#% 1289424
#! Cryptographic protocols are structured sequences of messages that are used for exchanging information in a hostile environment. Many protocols have epistemic goals: a successful run of the protocol is intended to cause a participant to hold certain beliefs. As such, epistemic logics have been employed for the verification of cryptographic protocols. Although this approach to verification is explicitly concerned with changing beliefs, formal belief change operators have not been incorporated in previous work. In this paper, we introduce a new approach to protocol verification by combining a monotonic logic with a non-monotonic belief change operator. In this context, a protocol participant is able to retract beliefs in response to new information and a protocol participant is able to postulate the most plausible event explaining new information. We illustrate that this kind of reasoning is particularly important when protocol participants have incorrect beliefs.

#index 1269738
#* Generality and equivalence relations in default logic
#@ Katsumi Inoue;Chiaki Sakama
#t 2007
#c 10
#% 101951
#% 340738
#% 382569
#% 825990
#% 1289366
#% 1692910
#! Generality or refinement relations between different theories have important applications to generalization in inductive logic programming, refinement of ontologies, and coordination in multi-agent systems. We study generality relations in disjunctive default logic by comparing the amounts of information brought by default theories. Intuitively, a default theory is considered more general than another default theory if the former brings more information than the latter. Using techniques in domain theory, we introduce different types of generality relations over default theories. We show that generality relations based on the Smyth and Hoare orderings reflect orderings on skeptical and credulous consequences, respectively, and that two default theories are equivalent if and only if they are equally general under these orderings. These results naturally extend both generality relations over first-order theories and those for answer set programming.

#index 1269739
#* Mutual belief revision: semantics and computation
#@ Yi Jin;Michael Thielscher;Dongmo Zhang
#t 2007
#c 10
#% 224753
#% 443185
#% 782324
#% 942467
#% 1250165
#% 1250169
#! This paper presents both a semantic and a computational model for multi-agent belief revision. We show that these two models are equivalent but serve different purposes. The semantic model displays the intuition and construction of the belief revision operation in multi-agent environments, especially in case of just two agents. The logical properties of this model provide strong justifications for it. The computational model enables us to reassess the operation from a computational perspective. A complexity analysis reveals that belief revision between two agents is computationally no more demanding than single agent belief revision.

#index 1269740
#* Measuring the level of transfer learning by an AP physics problem-solver
#@ Matthew Klenk;Ken Forbus
#t 2007
#c 10
#% 1116
#% 65345
#% 109848
#% 154456
#% 156189
#% 175376
#% 179983
#% 420653
#% 895697
#% 936982
#% 1250572
#% 1250577
#% 1269392
#% 1274921
#! Transfer learning is the ability of an agent to apply knowledge learned in previous tasks to new problems or domains. We approach this problem by focusing on model formulation, i.e., how to move from the unruly, broad set of concepts used in everyday life to a concise, formal vocabulary of abstractions that can be used effectively for problem solving. This paper describes how the Companions cognitive architecture uses analogical model formulation to learn to solve AP Physics problems. Our system starts with some basic mathematical skills, a broad common sense ontology, and some qualitative mechanics, but no equations. Our system uses worked solutions to learn how to use equations and modeling assumptions to solve AP Physics problems. We show that this process of analogical model formulation can facilitate learning over a range of types of transfer, in an experiment administered by the Educational Testing Service.

#index 1269741
#* Complexity boundaries for horn description logics
#@ Markus Krötzsch;Sebastian Rudolph;Pascal Hitzler
#t 2007
#c 10
#% 289287
#% 577305
#% 869521
#% 1269453
#% 1289408
#% 1289425
#! Horn description logics (Horn-DLs) have recently started to attract attention due to the fact that their (worst-case) data complexities are in general lower than their overall (i.e. combined) complexities, which makes them attractive for reasoning with large ABoxes. However, the natural question whether Horn-DLs also provide advantages for TBox reasoning has hardly been addressed so far. In this paper, we therefore provide a thorough and comprehensive analysis of the combined complexities of Horn-DLs. While the combined complexity for many Horn-DLs turns out to be the same as for their non-Horn counterparts, we identify subboolean DLs where Hornness simplifies reasoning.

#index 1269742
#* Facts do not cease to exist because they are ignored: relativised uniform equivalence with answer-set projection
#@ Johannes Oetsch;Hans Tompits;Stefan Woltran
#t 2007
#c 10
#% 53400
#% 340738
#% 752748
#% 912779
#% 957649
#% 1016490
#% 1223271
#% 1269468
#% 1289366
#% 1289433
#% 1655192
#% 1656431
#% 1705009
#! Recent research in answer-set programming (ASP) focuses on different notions of equivalence between programs which are relevant for program optimisation and modular programming. Prominent among these notions is uniform equivalence, which checks whether two programs have the same semantics when joined with an arbitrary set of facts. In this paper, we study a family of more fine-grained versions of uniform equivalence, where the alphabet of the added facts as well as the projection of answer sets is taken into account. The latter feature, in particular, allows the removal of auxiliary atoms in computation, which is important for practical programming aspects. We introduce novel semantic characterisations for the equivalence problems under consideration and analyse the computational complexity for checking these problems. We furthermore provide efficient reductions to quantified propositional logic, yielding a rapid-prototyping system for equivalence checking.

#index 1269743
#* Learning large scale common sense models of everyday life
#@ William Pentney;Matthai Philipose;Jeff Bilmes;Henry Kautz
#t 2007
#c 10
#% 44876
#% 405391
#% 770844
#% 843359
#% 876066
#% 1250214
#% 1250649
#! Recent work has shown promise in using large, publicly available, hand-contributed commonsense databases as joint models that can be used to infer human state from day-to-day sensor data. The parameters of these models are mined from the web. We show in this paper that learning these parameters using sensor data (with the mined parameters as priors) can improve performance of the models significantly. The primary challenge in learning is scale. Since the model comprises roughly 50,000 irregularly connected nodes in each time slice, it is intractable either to completely label observed data manually or to compute the expected likelihood of even a single lime slice. We show how to solve the resulting semi-supervised learning problem by combining a variety of conventional approximation techniques and a novel technique for simplifying the model called context-based pruning. We show empirically that the learned model is substantially better at interpreting sensor data and an detailed analysis of how various techniques contribute to the performance.

#index 1269744
#* A model-based approach for merging prioritized knowledge bases in possibilistic logic
#@ Guilin Qi
#t 2007
#c 10
#% 167544
#% 417813
#% 763750
#% 772063
#% 1705594
#! This paper presents a new approach for merging prioritized knowledge bases in possibilistic logic. Our approach is semantically defined by a model-based merging operator in propositional logic and the merged result of our approach is a normal possibility distribution. We also give an algorithm to obtain the syntactical counterpart of the semantic approach. The logical properties of our approach are considered. Finally, we analyze the computational complexity of our merging approach.

#index 1269745
#* Description logics for multi-issue bilateral negotiation with incomplete information
#@ Azzurra Ragone;Tommaso Di Noia;Eugenio Di Sciascio;Francesco M. Donini
#t 2007
#c 10
#% 70391
#% 342124
#% 824064
#% 935898
#% 1223234
#% 1250622
#! We propose a framework for multi-issue bilateral negotiation, where issues are expressed and related to each other via Description Logics. Agents' goals are expressed through (complex) concepts, and the worth of goals as weights over concepts. We adopt a very general setting with incomplete information by letting agents keep both goals and worths of goals as private information. We introduce a negotiation protocol for such a setting, and discuss different possible strategies that agents can adopt during the negotiation process. We show that such a protocol converges, if the Description Logic used enjoys the finite implicants property.

#index 1269746
#* A generalized gelfond-lifschitz transformation for logic programs with abstract constraints
#@ Yi-Dong Shen;Jia-Huai You
#t 2007
#c 10
#% 411814
#% 464918
#% 947786
#% 1041019
#% 1250132
#% 1250527
#% 1269469
#% 1279333
#% 1289415
#% 1656398
#% 1656428
#! We present a generalized Gelfond-Lifschitz transformation in order to define stable models for a logic program with arbitrary abstract constraints on sets (c-atoms). The generalization is based on a formal semantics and a novel abstract representation of c-atoms, as opposed to the commonly used power set form representation. In many cases, the abstract representation of a c-atom results in a substantial reduction of size from its power set form representation. We show that any c-atom A = (Ad, Ac) in the body of a clause can be characterized using its satisfiable sets, so that given an interpretation I the c-atom can be handled simply by introducing a special atom θA together with a new clause θA ← A1, ..., An for each satisfiable set {A1, ..., An} of A. We also prove that the latest fixpoint approach presented by Son et al. and our approach using the generalized Gelfond-Lifschitz transformation are semantically equivalent in the sense that they define the same set of stable models.

#index 1269747
#* Probabilistic modal logic
#@ Afsaneh Shirazi;Eyal Amir
#t 2007
#c 10
#% 18327
#% 44876
#% 118746
#% 167194
#% 188086
#% 303620
#% 383110
#% 480202
#% 752494
#% 781175
#% 782337
#% 1810385
#! A modal logic is any logic for handling modalities: concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.

#index 1269748
#* A modal logic for beliefs and pro attitudes
#@ Kaile Su;Abdul Sattar;Han Lin;Mark Reynolds
#t 2007
#c 10
#% 8936
#% 68239
#% 131283
#% 188086
#% 431526
#% 431565
#% 499542
#% 659831
#% 1269389
#% 1693266
#! Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.

#index 1269749
#* Knowledge compilation properties of tree-of-BDDs
#@ Sathiamoorthy Subbarayan;Lucas Bordeaux;Youssef Hamadi
#t 2007
#c 10
#% 3873
#% 159244
#% 342378
#% 543491
#% 644201
#% 787002
#% 1250513
#% 1272349
#% 1289375
#% 1664981
#% 1698020
#! We present a CNF to Tree-of-BDDs (ToB) compiler with complexity at most exponential in the tree width. We then present algorithms for interesting queries on ToB. Although some of the presented query algorithms are in the worst case exponential in the tree width, our experiments show that ToB can answer non-trivial queries like clausal entailment in reasonable time for several realistic instances. While our ToB-tool compiles all the used 91 instances, d-DNNF compilation failed for 12 or 8 of them based on the decomposition heuristic used. Also, on the succeeded instances, a d-DNNF is up to 1000 times larger than the matching ToB. The ToB compilations are often an order of magnitude faster than the d-DNNF compilation. This makes ToB a quite interesting knowledge compilation form.

#index 1269750
#* The modal logic S4F, the default logic, and the logic here-and-there
#@ Mirosław Truszczynski
#t 2007
#c 10
#% 94456
#% 147513
#% 163716
#% 175359
#% 289333
#% 340738
#% 383293
#% 400986
#% 417651
#% 495346
#% 501041
#% 752748
#% 780339
#% 957649
#% 1274822
#! The modal logic S4F provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic S4F concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of S4F-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic S4F. We argue that the logic S4F can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic S4F generalizes the logic here-and-there.

#index 1269751
#* Particle filtering for dynamic agent modelling in simplified poker
#@ Nolan Bard;Michael Bowling
#t 2007
#c 10
#% 103865
#% 266213
#% 466418
#% 1250316
#% 1269482
#% 1272356
#% 1279308
#% 1499474
#% 1760897
#! Agent modelling is a challenging problem in many modern artificial intelligence applications. The agent modelling task is especially difficult when handling stochastic choices, deliberately hidden information, dynamic agents, and the need for fast learning. State estimation techniques, such as Kalman filtering and particle filtering, have addressed many of these challenges, but have received little attention in the agent modelling literature. This paper looks at the use of particle filtering for modelling a dynamic opponent in Kuhn poker, a simplified version of Texas Hold'em poker. We demonstrate effective modelling both against static opponents as well as dynamic opponents, when the dynamics are known. We then examine an application of Rao-Blackwellized particle filtering for doing dual estimation, inferring both the opponent's state as well as a model of its dynamics. Finally, we examine the robustness of the approach to incorrect beliefs about the opponent and compare it to previous work on opponent modelling in Kuhn poker.

#index 1269752
#* A mathematical programming formulation for sparse collaborative computer aided diagnosis
#@ Jinbo Bi;Tao Xiong
#t 2007
#c 10
#% 236497
#% 466750
#% 722932
#% 722943
#% 735256
#% 769886
#% 916788
#! A mathematical programming formulation is proposed to eliminate irrelevant and redundant features for collaborative computer aided diagnosis which requires to detect multiple clinically-related malignant structures from medical images. A probabilistic interpretation is described to justify our formulations. The proposed formulation is optimized through an effective alternating optimization algorithm that is easy to implement and relatively fast to solve. This collaborative prediction approach has been implemented and validated on the automatic detection of solid lung nodules by jointly detecting ground glass opacities.

#index 1269753
#* Isometric projection
#@ Deng Cai;Xiaofei He;Jiawei Han
#t 2007
#c 10
#% 70370
#% 235342
#% 317525
#% 341596
#% 729437
#% 791402
#% 800190
#% 835741
#% 836827
#! Recently the problem of dimensionality reduction has received a lot of interests in many fields of information processing. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. The most popular manifold learning algorithms include Locally Linear Embedding, ISOMAP, and Laplacian Eigenmap. However, these algorithms are nonlinear and only provide the embedding results of training samples. In this paper, we propose a novel linear dimensionality reduction algorithm, called Isometric Projection. Isometric Projection constructs a weighted data graph where the weights are discrete approximations of the geodesic distances on the data manifold. A linear subspace is then obtained by preserving the pairwise distances. In this way, Isometric Projection can be defined everywhere. Comparing to Principal Component Analysis (PCA) which is widely used in data processing, our algorithm is more capable of discovering the intrinsic geometrical structure. Specially, PCA is optimal only when the data space is linear, while our algorithm has no such assumption and therefore can handle more complex data space. Experimental results on two real life data sets illustrate the effectiveness of the proposed method.

#index 1269754
#* Active algorithm selection
#@ Feilong Chen;Rong Jin
#t 2007
#c 10
#% 116165
#% 132697
#% 169717
#% 235377
#% 236729
#% 464268
#% 466095
#% 466231
#% 466419
#% 466576
#% 466887
#% 669513
#% 735358
#% 1478821
#! Most previous studies on active learning focused on the problem of model selection, i.e., how to identify the optimal classification model from a family of predefined models using a small, carefully selected training set. In this paper, we address the problem of active algorithm selection. The goal of this problem is to efficiently identify the optimal learning algorithm for a given dataset from a set of algorithms using a small training set. In this study, we present a general framework for active algorithm selection by extending the idea of the Hedge algorithm. It employs the worst case analysis to identify the example that can effectively increase the weighted loss function defined in the Hedge algorithm. We further extend the framework by incorporating the correlation information among unlabeled examples to accurately estimate the change in the weighted loss function, and Maximum Entropy Discrimination to automatically determine the combination weights used by the Hedge algorithm. Our empirical study with the datasets of WCCI 2006 performance prediction challenge shows promising performance of the proposed framework for active algorithm selection.

#index 1269755
#* Transferring naive bayes classifiers for text classification
#@ Wenyuan Dai;Gui-Rong Xue;Qiang Yang;Yong Yu
#t 2007
#c 10
#% 116149
#% 127850
#% 236497
#% 311027
#% 464641
#% 466263
#% 770847
#% 770858
#% 832331
#% 876034
#% 1272110
#% 1290055
#! A basic assumption in traditional machine learning is that the training and test data distributions should be identical. This assumption may not hold in many situations in practice, but we may be forced to rely on a different-distribution data to learn a prediction model. For example, this may be the case when it is expensive to label the data in a domain of interest, although in a related but different domain there may be plenty of labeled data available. In this paper, we propose a novel transfer-learning algorithm for text classification based on an EM-based Naive Bayes classifiers. Our solution is to first estimate the initial probabilities under a distribution Dl of one labeled data set, and then use an EM algorithm to revise the model for a different distribution Du of the test data which are unlabeled. We show that our algorithm is very effective in several different pairs of domains, where the distances between the different distributions are measured using the Kullback-Leibler (KL) divergence. Moreover, KL-divergence is used to decide the trade-off parameters in our algorithm. In the experiment, our algorithm outperforms the traditional supervised and semi-supervised learning algorithms when the distributions of the training and test sets are increasingly different.

#index 1269756
#* Relationship identification for social network discovery
#@ Christopher P. Diehl;Galileo Namata;Lise Getoor
#t 2007
#c 10
#% 577224
#% 734915
#% 736155
#% 823373
#% 840846
#% 840882
#% 868085
#% 868088
#% 869480
#% 1727324
#! In recent years, informal, online communication has transformed the ways in which we connect and collaborate with friends and colleagues. With millions of individuals communicating online each day, we have a unique opportunity to observe the formation and evolution of roles and relationships in networked groups and organizations. Yet a number of challenges arise when attempting to infer the underlying social network from data that is often ambiguous, incomplete and context-dependent. In this paper, we consider the problem of collaborative network discovery from domains such as intelligence analysis and litigation support where the analyst is attempting to construct a validated representation of the social network. We specifically address the challenge of relationship identification where the objective is to identify relevant communications that substantiate a given social relationship type. We propose a supervised ranking approach to the problem and assess its performance on a manager-subordinate relationship identification task using the Enron email corpus. By exploiting message content, the ranker routinely cues the analyst to relevant communications relationships and message traffic that are indicative of the social relationship.

#index 1269757
#* A reinforcement learning algorithm with polynomial interaction complexity for only-costly-observable MDPs
#@ Roy Fox;Moshe Tennenholtz
#t 2007
#c 10
#% 30037
#% 466075
#% 722895
#% 1289461
#! An Unobservable MDP (UMDP) is a POMDP in which there are no observations. An Only-Costly-Observable MDP (OCOMDP) is a POMDP which extends an UMDP by allowing a particular costly action which completely observes the state. We introduce UR-MAX, a reinforcement learning algorithm with polynomial interaction complexity for unknown OCOMDPs.

#index 1269758
#* Compact spectral bases for value function approximation using Kronecker factorization
#@ Jeff Johns;Sridhar Mahadevan;Chang Wang
#t 2007
#c 10
#% 238374
#% 240005
#% 274612
#% 384911
#% 393786
#% 734920
#% 916799
#% 1250345
#% 1275167
#! A new spectral approach to value function approximation has recently been proposed to automatically construct basis functions from samples. Global basis functions called proto-value functions are generated by diagonalizing a diffusion operator, such as a reversible random walk or the Laplacian, on a graph formed from connecting nearby samples. This paper addresses the challenge of scaling this approach to large domains. We propose using Kronecker factorization coupled with the Metropolis-Hastings algorithm to decompose reversible transition matrices. The result is that the basis functions can be computed on much smaller matrices and combined to form the overall bases. We demonstrate that in several continuous Markov decision processes, compact basis functions can be constructed without significant loss in performance. In one domain, basis functions were compressed by a factor of 36. A theoretical analysis relates the quality of the approximation to the spectral gap. Our approach generalizes to other basis constructions as well.

#index 1269759
#* A method for large-scale l1-regularized logistic regression
#@ Kwangmoo Koh;Seung-Jean Kim;Stephen Boyd
#t 2007
#c 10
#% 1528
#% 238376
#% 757953
#% 770857
#% 803771
#% 829007
#% 906491
#% 1861282
#! Logistic regression with l1 regularization has been proposed as a promising method for feature selection in classification problems. Several specialized solution methods have been proposed for l1-regularized logistic regression problems (LRPs). However, existing methods do not scale well to large problems that arise in many practical settings. In this paper we describe an efficient interior-point method for solving l1-regularized LRPS. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC. A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve large sparse problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few tens of minutes, on a PC. Numerical experiments show that our method outperforms standard methods for solving convex optimization problems as well as other methods specifically designed for l1- regularized LRPs.

#index 1269760
#* Efficient reinforcement learning with relocatable action models
#@ Bethany R. Leffler;Michael L. Littman;Timothy Edmunds
#t 2007
#c 10
#% 162953
#% 174161
#% 183499
#% 384911
#% 425075
#% 449561
#% 464778
#% 495933
#% 722895
#% 1269521
#! Realistic domains for learning possess regularities that make it possible to generalize experience across related states. This paper explores an environment-modeling framework that represents transitions as state-independent outcomes that are common to all states that share the same type. We analyze a set of novel learning problems that arise in this framework, providing lower and upper bounds. We single out one particular variant of practical interest and provide an efficient algorithm and experimental results in both simulated and robotic environments.

#index 1269761
#* Graph partitioning based on link distributions
#@ Bo Long;Mark Zhang;Philip S. Yu
#t 2007
#c 10
#% 148149
#% 202286
#% 274612
#% 313959
#% 342621
#% 342659
#% 466675
#% 578670
#% 769928
#% 823395
#% 881487
#! Existing graph partitioning approaches are mainly based on optimizing edge cuts and do not take the distribution of edge weights (link distribution) into consideration. In this paper, we propose a general model to partition graphs based on link distributions. This model formulates graph partitioning under a certain distribution assumption as approximating the graph affinity matrix under the corresponding distortion measure. Under this model, we derive a novel graph partitioning algorithm to approximate a graph affinity matrix under various Bregman divergences, which correspond to a large exponential family of distributions. We also establish the connections between edge cut objectives and the proposed model to provide a unified view to graph partitioning.

#index 1269762
#* Refining rules incorporated into knowledge-based support vector learners via successive linear programming
#@ Richard Maclin;Edward Wild;Jude Shavlik;Lisa Torrey;Trevor Walker
#t 2007
#c 10
#% 124708
#% 160857
#% 161241
#% 175368
#% 793236
#% 823327
#% 876010
#% 1250574
#% 1269488
#% 1272368
#! Knowledge-based classification and regression methods are especially powerful forms of learning. They allow a system to take advantage of prior domain knowledge supplied either by a human user or another algorithm, combining that knowledge with data to produce accurate models. A limitation of the use of prior knowledge occurs when the provided knowledge is incorrect. Such knowledge likely still contains useful information, but knowledge-based learners might not be able to fully exploit such information. In fact, incorrect knowledge can lead to poorer models than result from knowledge-free learners. We present a support-vector method for incorporating and refining domain knowledge that not only allows the learner to make use of that knowledge, but also suggests changes to the provided knowledge. Our approach is built on the knowledge-based classification and regression methods presented by Fung, Mangasarian, & Shavlik (2002; 2003) and by Mangasarian, Shavlik, & Wild (2004). Experiments on artificial data sets with known properties, as well as on a real-world data set, demonstrate that our method learns more accurate models while also adjusting the provided rules in intuitive ways. Our new algorithm provides an appealing extension to knowledge-based, support-vector learning that is not only able to combine knowledge from rules with data, but is also able to use the data to modify and change those rules to better fit the data.

#index 1269763
#* Improving learning in networked data by combining explicit and mined links
#@ Sofus A. Macskassy
#t 2007
#c 10
#% 44876
#% 248810
#% 266215
#% 266230
#% 406493
#% 420495
#% 496116
#% 549441
#% 565545
#% 729982
#% 769942
#% 770851
#% 876068
#% 961278
#% 1289267
#! This paper is about using multiple types of information for classification of networked data in a semi-supervised setting: given a fully described network (nodes and edges) with known labels for some of the nodes, predict the labels of the remaining nodes. One method recently developed for doing such inference is a guilt-by-association model. This method has been independently developed in two different settings-relational learning and semi-supervised learning. In relational learning, the setting assumes that the networked data has explicit links such as hyperlinks between webpages or citations between research papers. The semi-supervised setting assumes a corpus of non-relational data and creates links based on similarity measures between the instances. Both use only the known labels in the network to predict the remaining labels but use very different information sources. The thesis of this paper is that if we combine these two types of links, the resulting network will carry more information than either type of link by itself. We test this thesis on six benchmark data sets, using a within-network learning algorithm, where we show that we gain significant improvements in predictive performance by combining the links. We describe a principled way of combining mUltiple types of edges with different edge-weights and semantics using an objective graph measure called node-based assortativity. We investigate the use of this measure to combine text-mined links with explicit links and show that using our approach significantly improves performance of our classifier over naively combining these two types of links.

#index 1269764
#* Cautious inference in collective classification
#@ Luke K. McDowell;Kalyan Moy Gupta;David W. Aha
#t 2007
#c 10
#% 248810
#% 266215
#% 420495
#% 769942
#% 785353
#% 844322
#% 1393858
#% 1650403
#! Collective classification can significantly improve accuracy by exploiting relationships among instances. Although several collective inference procedures have been reported, they have not been thoroughly evaluated for their commonalities and differences. We introduce novel generalizations of three existing algorithms that allow such algorithmic and empirical comparisons. Our generalizations permit us to examine how cautiously or aggressively each algorithm exploits intermediate relational data, which can be noisy. We conjecture that cautious approaches that identify and preferentially exploit the more reliable intermediate data should outperform aggressive approaches. We explain why caution is useful and introduce three parameters to control the degree of caution. An empirical evaluation of collective classification algorithms, using two base classifiers on three data sets, supports our conjecture.

#index 1269765
#* Nonmyopic informative path planning in spatio-temporal models
#@ Alexandra Meliou;Andreas Krause;Carlos Guestrin;Joseph M. Hellerstein
#t 2007
#c 10
#% 76270
#% 256685
#% 808255
#% 836516
#% 840868
#% 862545
#% 867029
#% 891549
#% 1016178
#% 1061585
#% 1275108
#! In many sensing applications we must continuously gather information to provide a good estimate of the state of the environment at every point in time. A robot may tour an environment, gathering information every hour. In a wireless sensor network, these tours correspond to packets being transmitted. In these settings, we are often faced with resource restrictions, like energy constraints. The users issue queries with certain expectations on the answer quality. Thus, we must optimize the tours to ensure the satisfaction of the user constraints, while at the same time minimize the cost of the query plan. For a single timestep, this optimization problem is NP-hard, but recent approximation algorithms with theoretical guarantees provide good solutions. In this paper, we present a new efficient algorithm, exploiting dynamic programming and submodularity of the information collected, that efficiently plans data collection tours for an entire (finite) horizon. Our algorithm can use any single step procedure as a black box, and, based on its properties, provides strong theoretical guarantees for the solution. We also provide an extensive empirical analysis demonstrating the benefits of nonmyopic planning in two real world sensing applications.

#index 1269766
#* Mapping and revising Markov logic networks for transfer learning
#@ Lilyana Mihalkova;Tuyen Huynh;Raymond J. Mooney
#t 2007
#c 10
#% 44876
#% 65345
#% 174161
#% 186063
#% 236497
#% 266215
#% 465930
#% 840890
#% 850430
#% 876034
#% 1024713
#% 1250579
#% 1274868
#% 1699609
#% 1718465
#! Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.

#index 1269767
#* Discovering multivariate motifs using subsequence density estimation and greedy mixture learning
#@ David Minnen;Charles L. Isbell;Irfan Essa;Thad Starner
#t 2007
#c 10
#% 137711
#% 349208
#% 629629
#% 727900
#% 729960
#% 795273
#% 799397
#% 844297
#% 905914
#% 952013
#% 1275205
#! The problem of locating motifs in real-valued, multivariate time series data involves the discovery of sets of recurring patterns embedded in the time series. Each set is composed of several non-overlapping subsequences and constitutes a motif because all of the included subsequences are similar. The ability to automatically discover such motifs allows intelligent systems to form endogenously meaningful representations of their environment through unsupervised sensor analysis. In this paper, we formulate a unifying view of motif discovery as a problem of locating regions of high density in the space of all time series subsequences. Our approach is efficient (sub-quadratic in the length of the data), requires fewer user-specified parameters than previous methods, and naturally allows variable length motif occurrences and non-linear temporal warping. We evaluate the performance of our approach using four data sets from different domains including on-body inertial sensors and speech.

#index 1269768
#* M2ICAL analyses HC-gammon
#@ Wee-Chong Oon;Martin Henz
#t 2007
#c 10
#% 183499
#% 214391
#% 268952
#% 1209760
#! We analyse Pollack and Blair's HC-Gammon backgammon program using a new technique that performs Monte Carlo simulations to derive a Markov Chain model for Imperfect Comparison ALgorithms, called the M2ICAL method, which models the behavior of the algorithm using a Markov chain, each of whose states represents a class of players of similar strength. The Markov chain transition matrix is populated using Monte Carlo simulations. Once generated, the matrix allows fairly accurate predictions of the expected solution quality, standard deviation and time to convergence of the algorithm. This allows us to make some observations on the validity of Pollack and Blair's conclusions, and also shows the application of the M2ICAL method on a previously published work.

#index 1269769
#* A randomized string kernel and its application to RNA interference
#@ Shibin Qiu;Terran Lane;Ljubomir Buturovic
#t 2007
#c 10
#% 249321
#% 269207
#% 722803
#% 731607
#% 743284
#% 830744
#% 1046882
#! String kernels directly model sequence similarities without the necessity of extracting numerical features in a vector space. Since they better capture complex traits in the sequences, string kernels often achieve better prediction performance. RNA interference is an important biological mechanism with many therapeutical applications, where strings can be used to represent target messenger RNAs and initiating short RNAs and string kernels can be applied for learning and prediction. However, existing string kernels are not particularly developed for RNA applications. Moreover, most existing string kernels are n-gram based and suffer from high dimensionality and inability of preserving subsequence orderings. We propose a randomized string kernel for use with support vector regression with a purpose of better predicting silencing efficacy scores for the candidate sequences and eventually improving the efficiency of biological experiments. We show the positive definiteness of this kernel and give an analysis of randomization error rates. Empirical results on biological data demonstrate that the proposed kernel performed better than existing string kernels and achieved significant improvements over kernels computed from numerical descriptors extracted according to structural and thermodynamic rules. In addition, it is computationally more efficient.

#index 1269770
#* COD: online temporal clustering for outbreak detection
#@ Tomáš Šingliar;Denver H. Dash
#t 2007
#c 10
#% 75936
#% 246834
#% 302479
#% 788047
#% 790040
#% 963783
#% 963799
#% 963818
#% 1050863
#% 1250333
#! We present Cluster Onset Detection (COD), a novel algorithm to aid in detection of epidemic outbreaks. COD employs unsupervised learning techniques in an online setting to partition the population into subgroups, thus increasing the ability to make a detection over the population as a whole by decreasing the signal-to-noise ratio. The method is adaptive and able to alter its clustering in real-time without the need for detailed background knowledge of the population. COD attempts to detect a cluster made up primarily of infected hosts. We argue that this technique is largely complementary to the existing methods for outbreak detection and can generally be combined with one or more of them. We show empirical results applying COD to the problem of detecting a worm attack on a system of networked computers, and show that thIs method results in approximately 40% lower infection rate at a false positive rate of 1 per week than the best previously reported results on this data set achieved using an HMM model customized for the outbreak detection task.

#index 1269771
#* Abstraction in predictive state representations
#@ Vishal Soni;Satinder Singh
#t 2007
#c 10
#% 286423
#% 477304
#% 1279356
#% 1478746
#! Most work on Predictive Representations of State (PSRs) focuses on learning a complete model of the system that can be used to answer any question about the future. However, we may be interested only in answering certain kinds of abstract questions. For instance, we may only care about the presence of objects in an image rather than pixel level details. In such cases, we may be able to learn substantially smaller models that answer only such abstract questions. We present the framework of PSR homomorphisms for model abstraction in PSRs. A homomorphism transforms a given PSR into a smaller PSR that provides exact answers to abstract questions in the original PSR. As we shall show, this transformation captures structural and temporal abstractions in the original PSR.

#index 1269772
#* Efficient structure learning in factored-state MDPs
#@ Alexander L. Strehl;Carlos Diuk;Michael L. Littman
#t 2007
#c 10
#% 384911
#% 425075
#% 464778
#% 495933
#% 722895
#% 875977
#% 961197
#% 1271827
#! We consider the problem of reinforcement learning in factored-state MDPs in the setting in which learning is conducted in one long trial with no resets allowed. We show how to extend existing efficient algorithms that learn the conditional probability tables of dynamic Bayesian networks (DBNs) given their structure to the case in which DBN structure is not known in advance. Our method learns the DBN structures as part of the reinforcement-learning process and provably provides an efficient learning algorithm when combined with factored Rmax.

#index 1269773
#* Semi-supervised learning by mixed label propagation
#@ Wei Tong;Rong Jin
#t 2007
#c 10
#% 173879
#% 528156
#% 564279
#% 757953
#% 840852
#% 840873
#% 840892
#% 840924
#! Recent studies have shown that graph-based approaches are effective for semi-supervised learning. The key idea behind many graph-based approaches is to enforce the consistency between the class assignment of unlabeled examples and the pairwise similarity between examples. One major limitation with most graph-based approaches is that they are unable to explore dissimilarity or negative similarity. This is because the dissimilar relation is not transitive, and therefore is difficult to be propagated. Furthermore, negative similarity could result in unbounded energy functions, which makes most graph-based algorithms unapplicable. In this paper, we propose a new graph-based approach, termed as "mixed label propagation" which is able to effectively explore both similarity and dissimilarity simultaneously. In particular, the new framework determines the assignment of class labels by (1) minimizing the energy function associated with positive similarity, and (2) maximizing the energy function associated with negative similarity. Our empirical study with collaborative filtering shows promising performance of the proposed approach.

#index 1269774
#* Clustering with local and global regularization
#@ Fei Wang;Changshui Zhang;Tao Li
#t 2007
#c 10
#% 36672
#% 143194
#% 190581
#% 313959
#% 466675
#% 593047
#% 722902
#% 724227
#% 729437
#% 765552
#% 875968
#% 987204
#% 1269778
#% 1705532
#% 1705533
#! Clustering is an old research topic in data mining and machine learning communities. Most of the traditional clustering methods can be categorized local or global ones. In this paper, a novel clustering method that can explore both the local and global information in the dataset is proposed. The method, Clustering with Local and Global Consistency (CLGR), aims to minimize a cost function that properly trades off the local and global costs. We will show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix, which can be done efficiently by some iterative methods. Finally the experimental results on several datasets are presented to show the effectiveness of our method.

#index 1269775
#* Probabilistic community discovery using hierarchical latent Gaussian mixture model
#@ Haizheng Zhang;C. Lee Giles;Henry C. Foley;John Yen
#t 2007
#c 10
#% 310514
#% 722904
#% 773234
#% 836843
#% 869480
#% 876017
#% 907511
#% 1250581
#% 1707851
#! Complex networks exist in a wide array of diverse domains, ranging from biology, sociology, and computer science. These real-world networks, while disparate in nature, often comprise of a set of loose clusters(a.k.a communities), whose members are better connected to each other than to the rest of the network. Discovering such inherent community structures can lead to deeper understanding about the networks and therefore has raised increasing interests among researchers from various disciplines. This paper describes GWN-LDA (Generic weighted network-Latent Dirichlet Allocation) model, a hierarchical Bayesian model derived from the widely-received LDA model, for discovering probabilistic community profiles in social networks. In this model, communities are modeled as latent variables and defined as distributions over the social actor space. In addition, each social actor belongs to every community with different probability. This paper also proposes two different network encoding approaches and explores the impact of these two approaches to the community discovery performance. This model is evaluated on two research collaborative networks: CiteSeer and NanoSCI. The experimental results demonstrate that this approach is promising for discovering community structures in large-scale networks.

#index 1269776
#* Multi-label learning by instance differentiation
#@ Min-Ling Zhang;Zhi-Hua Zhou
#t 2007
#c 10
#% 5182
#% 132779
#% 311034
#% 318412
#% 344447
#% 397142
#% 458379
#% 770783
#% 783478
#% 785359
#% 818234
#% 818236
#% 838412
#% 840928
#% 861434
#% 889101
#% 906025
#% 950571
#% 1223287
#% 1250573
#% 1274865
#% 1388992
#! Multi-label learning deals with ambiguous examples each may belong to several concept classes simultaneously, In this learning framework, the inherent ambiguity of each example is explicitly expressed in the output space by being associated with multiple class labels. While on the other hand, its ambiguity is only implicitly encoded in the input space by being represented by only a single instance. Based on this recognition, we hypothesize that if the inherent ambiguity can be explicitly expressed in the input space appropriately, the problem of multi-label learning can be solved more effectively. We justify this hypothesis by proposing a novel multi-label learning approach named INS-DIF. The core of INSDIF is instance differentiation that transforms an example into a bag of instances each of which reflects the example's relationship with one of the possible classes. In this way, INSDIF directly addresses the inherent ambiguity of each example in the input space. A two-level classification strategy is employed to learn from the transformed examples. Applications to automatic web page categorization, natural scene classification and gene functional analysis show that our approach outperforms several well-established multi-label learning algorithms.

#index 1269777
#* Semi-supervised learning with very few labeled training examples
#@ Zhi-Hua Zhou;De-Chuan Zhan;Qiang Yang
#t 2007
#c 10
#% 252011
#% 271060
#% 311027
#% 466888
#% 565545
#% 765552
#% 816079
#% 832574
#% 836778
#% 855563
#% 879447
#% 906525
#% 1269479
#! In semi-supervised learning, a number of labeled examples are usually required for training an initial weakly useful predictor which is in turn used for exploiting the unlabeled examples. However, in many real-world applications there may exist very few labeled training examples, which makes the weakly useful predictor difficult to generate, and therefore these semisupervised learning methods cannot be applied. This paper proposes a method working under a two-view setting. By taking advantages of the correlations between the views using canonical component analysis, the proposed method can perform semi-supervised learning with only one labeled training example. Experiments and an application to content-based image retrieval validate the effectiveness of the proposed method.

#index 1269778
#* Kernel regression with order preferences
#@ Xiaojin Zhu;Andrew B. Goldberg
#t 2007
#c 10
#% 269217
#% 466084
#% 466263
#% 577224
#% 722932
#% 768632
#% 793236
#% 829028
#% 840846
#% 840853
#% 875962
#% 876081
#% 939346
#% 961195
#% 967004
#! We propose a novel kernel regression algorithm which takes into account order preferences on unlabeled data. Such preferences have the form that point x1 has a larger target value than that of x2, although the target values for x1, x2 are unknown. The order preferences can be viewed as side information or a form of weak labels, and our algorithm can be related to semi-supervised learning. Learning consists of formulating the order preferences as additional regularization in a risk minimization framework. We define a linear program to effectively solve the optimization problem. Experiments on benchmark datasets, sentiment analysis, and housing price problems show that the proposed algorithm outperforms standard regression, even when the order preferences are noisy.

#index 1269779
#* Agent influence as a predictor of difficulty for decentralized problem-solving
#@ Martin Allen;Shlomo Zilberstein
#t 2007
#c 10
#% 450852
#% 458688
#% 643287
#% 890282
#% 1250230
#% 1272052
#% 1279314
#% 1650376
#! We study the effect of problem structure on the practical performance of optimal dynamic programming for decentralized decision problems. It is shown that restricting agent influence over problem dynamics can make the problem easier to solve. Experimental results establish that agent influence correlates with problem difficulty: as the gap between the influence of different agents grows, problems tend to become much easier to solve. The measure thus provides a general-purpose, automatic characterization of decentralized problems, identifying those for which optimal methods are more or less likely to work. Such a measure is also of possible use as a heuristic in the design of algorithms that create task decompositions and control hierarchies in order to simplify multiagent problems.

#index 1269780
#* Computational aspects of covering in dominance graphs
#@ Felix Brandt;Felix Fischer
#t 2007
#c 10
#% 101922
#% 198464
#% 813834
#% 892736
#% 1021248
#% 1250602
#% 1250604
#% 1250605
#! Various problems in AI and multi agent systems can be tackled by finding the "most desirable" elements of a set given some binary relation. Examples can be found in areas as diverse as voting theory, game theory, and argumentation theory. Some particularly attractive solution sets are defined in terms of a covering relation--a transitive subrelation of the original relation. We consider three different types of covering (upward, downward, and bidirectional) and the corresponding solution concepts known as the uncovered set and the minimal covering set. We present the first polynomial-time algorithm for finding the minimal bidirectional covering set (an acknowledged open problem) and prove that deciding whether an alternative is in a minimal upward or downward covering set is NP-hard. Furthermore, we obtain various set-theoretical inclusions, which reveal a strong connection between von Neumann-Morgenstern stable sets and upward covering on the one hand, and the Banks set and downward covering on the other hand. In particular, we show that every stable set is also a minimal upward covering set.

#index 1269781
#* Allocating goods on a graph to eliminate envy
#@ Yann Chevaleyre;Ulle Endriss;Nicolas Maudet
#t 2007
#c 10
#% 267752
#% 430241
#% 443227
#% 754147
#% 819611
#% 1272101
#% 1274953
#% 1289500
#% 1850973
#! We introduce a distributed negotiation framework for multi-agent resource allocation where interactions between agents are limited by a graph defining a negotiation topology. A group of agents may only contract a deal if that group is fully connected according to the negotiation topology. An important criterion for assessing the quality of an allocation of resources, in terms of fairness, is envy-freeness: an agent is said to envy another agent if it would prefer to swap places with that other agent. We analyse under what circumstances a sequence of deals respecting the negotiation topology may be expected to converge to a state where no agent envies any of the agents it is directly connected to. We also analyse the computational complexity of a related decision problem, namely the problem of checking whether a given negotiation state admits any deal that would both be beneficial to every agent involved and reduce envy in the agent society.

#index 1269782
#* Evolutionary and lifetime learning in varying NK fitness landscape changing environments: an analysis of both fitness and diversity
#@ Dara Curran;Colm O'Riordan;Humphrey Sorensen
#t 2007
#c 10
#% 235941
#% 457033
#% 465706
#% 686757
#% 984651
#% 1777288
#! This paper examines the effects of lifetime learning on populations evolving genetically in a series of changing environmets. The analysis of both fitness and diversity of the populations provides an insight into the improved performance provided by lifetime learning. The NK fitness landscape model is employed as the problem task, which has the advantage of being able to generate a variety of fitness landscapes of varying difficulty. Experiments observe the response of populations in an environment where problem difficulty increases and decreases with varying frequency. Results show that lifetime learning is capable of overall higher fitness levels and, in addition, that lifetime learning stimulates the diversity of the population. This increased diversity allows lifetime learning a greater level of recovery and stability than evolutionary learning alone.

#index 1269783
#* Improved state estimation in multiagent settings with continuous or large discrete state spaces
#@ Prashant Doshi
#t 2007
#c 10
#% 303620
#% 823885
#! State estimation in multiagent settings involves updating an agent's belief over the physical states and the space of other agents' models. Performance of the previous approach to state estimation, the interactive particle filter, degrades with large state spaces because it distributes the particles over both, the physical state space and the other agents' models. We present an improved method for estimating the state in a class of multiagent settings that are characterized in part by continuous or large discrete state spaces. We factor out the models of the other agents and update the agent's belief over these models, as exactly as possible. Simultaneously, we sample particles from the distribution over the large physical state space and project the particles in time. This approach is equivalent to Rao-Blackwellising the interactive particle filter. We focus our analysis on the special class of problems where the nested beliefs are represented using Gaussians, the problem dynamics using conditional linear Gaussians (CLGs) and the observation functions using softmax or CLGs. These distributions adequately represent many realistic applications.

#index 1269784
#* Computational complexity of weighted threshold games
#@ Edith Elkind;Leslie Ann Goldberg;Paul Goldberg;Michael Wooldridge
#t 2007
#c 10
#% 80804
#% 165011
#% 341523
#% 388196
#% 404719
#% 808378
#% 873944
#! Weighted threshold games are coalitional games in which each player has a weight (intuitively corresponding to its voting power), and a coalition is successful if the sum of its weights exceeds a given threshold. Key questions in coalitional games include finding coalitions that are stable (in the sense that no member of the coalition has any rational incentive to leave it), and finding a division of payoffs to coalition members (an imputation) that is fair. We investigate the computational complexity of such questions for weighted threshold games. We study the core, the least core, and the nucleolus, distinguishing those problems that are polynomial-time computable from those that are NP-hard, and providing pseudopolynomial and approximation algorithms for the NP-hard problems.

#index 1269785
#* Llull and copeland voting broadly resist bribery and control
#@ Piotr Faliszewski;Edith Hemaspaandra;Lane A. Hemaspaandra;Jörg Rothe
#t 2007
#c 10
#% 330769
#% 578703
#% 578715
#% 631051
#% 890278
#% 940734
#% 953322
#% 1024742
#% 1250606
#% 1250608
#% 1274964
#% 1274991
#% 1279324
#% 1677599
#% 1703609
#! Control of elections refers to attempts by an agent to, via such actions as addition/deletion/partition of candidates or voters, ensure that a given candidate wins (Bartholdi, Tovey, & Trick 1992). An election system in which such an agent's computational task is NP-hard is said to be resistant to the given type of control. Aside from election systems with an NP-hard winner problem, the only systems known to be resistant to all the standard control types are highly artificial election systems created by hybridization (Hemaspaandra, Hemaspaandra, & Rothe 2007b). In this paper, we prove that an election system developed by the 13th century mystic Ramon Llull and the well-studied Copeland election system are both resistant to all the standard types of (constructive) electoral control other than one variant of addition of candidates. This is the most comprehensive resistance to control yet achieved by any natural election system whose winner problem is in P. In addition, we show that Llull and Copeland voting are very broadly resistant to bribery attacks, and we integrate the potential irrationality of voter preferences into many of our results.

#index 1269786
#* Towards a cognitive model of crowd behavior based on social comparison theory
#@ Natalie Fridman;Gal A. Kaminka
#t 2007
#c 10
#% 75896
#% 708124
#! Models of crowd behavior facilitate analysis and prediction of human group behavior, where people are affected by each other's presence. Unfortunately, existing models leave many open challenges. In particular, psychology models often offer only qualitative description, while computer science models are often simplistic, and are not reusable from one simulated phenomenon to the next. We propose a novel model of crowd behavior, based on Festinger's Social Companson Theory (SCT). We propose a concrete algorithmic framework for SCT and evaluate its implementation in several crowd behavior scenarios. Results from task measures and human judges evaluation shows that the SCT model produces improved results compared to base models from the literature.

#index 1269787
#* Centralized, distributed or something else? making timely decisions in multi-agent systems
#@ Tim Harbers;Rajiv T. Maheswaran;Pedro Szekely
#t 2007
#c 10
#% 4185
#% 408396
#% 443227
#% 773232
#% 823973
#% 855913
#% 890289
#% 1289393
#! In multi-agent systems, agents need to share information in order to make good decisions. Who does what in order to achieve this matters a lot. The assignment of responsibility influences delay and consequently affects agents' abilities to make timely decisions. It is often unclear which approaches are best. We develop a model where one can easily test the impact of different assignments and information sharing protocols by focusing only on the delays caused by computation and communication. Using the model, we obtain interesting results that provide insight about the types of assignments that perform well in various domains and how slight variations in protocols can make great differences in feasibility.

#index 1269788
#* An α-approximation protocol for the generalized mutual assignment problem
#@ Katsutoshi Hirayama
#t 2007
#c 10
#% 643099
#% 855911
#% 1250611
#! This paper presents a new distributed solution protocol, called DisLRPα, for the Generalized Mutual Assignment Problem (GMAP). The GMAP is a typical distributed combinatorial optimization problem whose goal is to maximize social welfare of the agents. Unlike the previous protocol for the GMAP, DisLRPα can provide a theoretical guarantee on global solution quality. In DisLRPα, as with in the previous protocol, the agents repeatedly solve their local problems while coordinating their local solutions using a distributed constraint satisfaction technique. The key difference is that, in DisLRPα, each agent is required to produce a feasible solution whose local objective value is not lower than α (0 α can certainly find a solution whose global objective value is higher than that theoretically guaranteed. Furthermore, they also show that, while spending extra communication and computation costs, DisLRPα can produce a significantly better solution than the previous protocol if we set α appropriately.

#index 1269789
#* Anytime coordination using separable bilinear programs
#@ Marek Petrik;Shlomo Zilberstein
#t 2007
#c 10
#% 92301
#% 206118
#% 363744
#% 773196
#% 1272052
#% 1272129
#% 1275075
#% 1275077
#! Developing scalable coordination algorithms for multi-agent systems is a hard computational challenge. One useful approach, demonstrated by the Coverage Set Algorithm (CSA), exploits structured interaction to produce significant computational gains. Empirically, CSA exhibits very good anytime performance, but an error bound on the results has not been established. We reformulate the algorithm and derive both online and offline error bounds for approximate solutions. Moreover, we propose an effective way to automatically reduce the complexity of the interaction. Our experiments show that this is a promising approach to solve a broad class of decentralized decision problems. The general formulation used by the algorithm makes it both easy to implement and widely applicable to a variety of other AI problems.

#index 1269790
#* Active imitation learning
#@ Aaron P. Shon;Deepak Verma;Rajesh P. N. Rao
#t 2007
#c 10
#% 266287
#% 466242
#% 761636
#% 770824
#% 770852
#% 1279315
#% 1425507
#% 1650283
#! Imitation learning, also called learning by watching or programming by demonstration, has emerged as a means of accelerating many reinforcement learning tasks. Previous work has shown the value of imitation in domains where a single mentor demonstrates execution of a known optimal policy for the benefit of a learning agent. We consider the more general scenario of learning from mentors who are themselves agents seeking to maximize their own rewards. We propose a new algorithm based on the concept of transferable utility for ensuring that an observer agent can learn efficiently in the context of a selfish, not necessarily helpful, mentor. We also address the questions of when an imitative agent should request help from a mentor, and when the mentor can be expected to acknowledge a request for help. In analogy with other types of active learning, we call the proposed approach active imitation learning.

#index 1269791
#* Dynamic DFS tree in ADOPT-ing
#@ Marius C. Silaghi;Makoto Yokoo
#t 2007
#c 10
#% 637512
#% 773232
#% 823893
#% 823971
#% 855908
#% 855909
#% 855913
#% 890433
#% 890438
#% 890439
#% 1250618
#% 1274778
#! Several distributed constraint reasoning algorithms employ Depth First Search (DFS) trees on the constraint graph that spans involved agents. In this article we show that it is possible to dynamically detect a minimal DFS tree, compatible with the current order on agents, during the distributed constraint reasoning process of the ADOPT algorithm. This also allows for shorter DFS trees during the initial steps of the algorithms, while some constraints did not yet prove useful given visited combinations of assignments. Earlier distributed algorithms for finding spanning trees on agents did not look to maintain compatibility with an order already used. We also show that announcing a nogood to a single optional agent is bringing significant improvements in the total number of messages. The dynamic detection of the DFS tree brings improvements in simulated time.

#index 1269792
#* Efficient statistical methods for evaluating trading agent performance
#@ Eric Sodomka;John Collins;Maria Gini
#t 2007
#c 10
#% 890387
#% 1024869
#% 1250444
#% 1388353
#! Market simulations, like their real-world counterparts, are typically domains of high complexity, high variability, and incomplete information. The performance of autonomous agents in these markets depends both upon the strategies of their opponents and on various market conditions, such as supply and demand. Because the space for possible strategies and market conditions is very large, empirical analysis in these domains becomes exceedingly difficult. Researchers who wish to evaluate their agents must run many test games across multiple opponent sets and market conditions to verify that agent performance has actually improved. Our approach is to improve the statistical power of market simulation experiments by controlling their complexity, thereby creating an environment more conducive to structured agent testing and analysis. We develop a tool that controls variability across games in one such market environment, the Trading Agent Competition for Supply Chain Management (TAC SCM), and demonstrate how it provides an efficient, systematic method for TAC SCM researchers to analyze agent performance.

#index 1269793
#* Strongly decomposable voting rules on multiattribute domains
#@ Lirong Xia;Jérôme Lang;Mingsheng Ying
#t 2007
#c 10
#% 1021271
#% 1272026
#% 1274973
#! Sequential composition of voting rules, by making use of structural properties of the voters' preferences, provide computationally economical ways for making a common decision over a Cartesian product of finite local domains. A sequential composition is usually defined on a set of legal profiles following a fixed order. In this paper, we generalize this by order-independent sequential composition and strong decomposability, which are independent of the chosen order. We study to which extent some usual properties of voting rules transfer from the local rules to their order-independent sequential composition. Then, to capture the idea that a voting rule is neutral or decomposable on a slightly smaller domain, we define nearly neutral, nearly decomposable rules for both sequential composition and order-independent sequential composition, which leads us to defining and studying decomposable permutations. We prove that any sequential composition of neutral local rules and any order-independent sequential composition of neutral local rules satisfying a necessary condition are nearly neutral.

#index 1269794
#* Approximate solutions of interactive dynamic influence diagrams using model clustering
#@ Yifeng Zeng;Prashant Doshi;Qiongyu Chen
#t 2007
#c 10
#% 252183
#% 643112
#% 1024893
#% 1269512
#% 1272071
#% 1289289
#! Interactive dynamic influence diagrams (I-DIDs) offer a transparent and semantically clear representation for the sequential decision-making problem over multiple time steps in the presence of other interacting agents. Solving I-DlDs exactly involves knowing the solutions of possible models of the other agents, which increase exponentially with the number of time steps. We present a method of solving I-DlDs approximately by limiting the number of other agents' candidate models at each time step to a constant. We do this by clustering the models and selecting a representative set from the clusters. We discuss the error bound of the approximation technique and demonstrate its empirical performance.

#index 1269795
#* A new algorithm for generating equilibria in massive zero-sum games
#@ Martin Zinkevich;Michael Bowling;Neil Burch
#t 2007
#c 10
#% 404712
#% 426646
#% 890449
#% 1279308
#! In normal scenarios, computer scientists often consider the number of states in a game to capture the difficulty of learning an equilibrium. However, players do not see games in the same light: most consider Go or Chess to be more complex than Monopoly. In this paper, we discuss a new measure of game complexity that links existing state-of-the-art algorithms for computing approximate equilibria to a more human measure. In particular, we consider the range of skill in a game, i.e. how many different skill levels exist. We then modify existing techniques to design a new algorithm to compute approximate equilibria whose performance can be captured by this new measure. We use it to develop the first near Nash equilibrium for a four round abstraction of poker, and show that it would have been able to win handily the bankroll competition from last year's AAAI poker competition.

#index 1269796
#* A logic of agent programs
#@ N. Alechina;M. Dastani;B. S. Logan;J.-J. Ch. Meyer
#t 2007
#c 10
#% 378885
#% 390685
#% 517430
#% 689575
#% 856791
#% 1227292
#% 1386456
#% 1740348
#% 1741896
#! We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language 3APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.

#index 1269797
#* Acquiring visibly intelligent behavior with example-guided neuroevolution
#@ Bobby D. Bryant;Risto Miikkulainen
#t 2007
#c 10
#% 92148
#% 126926
#% 136350
#% 160836
#% 341661
#% 466418
#% 495781
#% 540908
#% 746674
#% 761985
#% 1025195
#% 1777111
#% 1777370
#! Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.

#index 1269798
#* Actively exploring creation of face space(s) for improved face recognition
#@ Nitesh V. Chawla;Kevin W. Bowyer
#t 2007
#c 10
#% 116165
#% 169717
#% 170649
#% 310503
#% 464268
#% 466095
#% 592305
#% 735358
#% 812543
#% 884285
#% 1022958
#% 1391224
#% 1391319
#% 1502432
#% 1861639
#! We propose a learning framework that actively explores creation of face space(s) by selecting images that are complementary to the images already represented in the face space. We also construct ensembles of classifiers learned from such actively sampled image sets, which further provides improvement in the recognition rates. We not only signicantly reduce the number of images required in the training set but also improve the accuracy over learning from all the images. We also show that the single face space or ensemble of face spaces, thus constructed, has a higher generalization performance across different illumination and expression conditions.

#index 1269799
#* Modeling reciprocal behavior in human bilateral negotiation
#@ Ya'akov Gal;Avi Pfeffer
#t 2007
#c 10
#% 662423
#% 773284
#% 890253
#% 1250154
#! Reciprocity is a key determinant of human behavior and has been well documented in the psychological and behavioral economics literature. This paper shows that reciprocity has significant implications for computer agents that interact with people over time. It proposes a model for predicting people's actions in multiple bilateral rounds of interactions. The model represents reciprocity as a tradeoff between two social factors: the extent to which players reward and retaliate others' past actions (retrospective reasoning), and their estimate about the future ramifications of their actions (prospective reasoning). The model is trained and evaluated over a series of negotiation rounds that vary players' possible strategies as well as their benefit from potential strategies at each round. Results show that reasoning about reciprocal behavior significantly improves the predictive power of the model, enabling it to outperform alternative models that do not reason about reciprocity, or that play various game theoretic equilibria. These results indicate that computers that interact with people need to represent and to learn the social factors that affect people's play when they interact over time.

#index 1269800
#* Gender-sensitive automated negotiators
#@ Ron Katz;Sarit Kraus
#t 2007
#c 10
#% 266213
#% 890309
#! This paper introduces an innovative approach for automated negotiating using the gender of human opponents. Our approach segments the information acquired from previous opponents, stores it in two databases, and models the typical behavior of males and of females. The two models are used in order to match an optimal strategy to each of the two subpopulations. In addition to the basic separation, we propose a learning algorithm which supplies an online indicator for the gender separability-level of the population, which tunes the level of separation the algorithm activates. The algorithm we present can be generally applied in different environments with no need for configuration of parameters. Experiments in 4 different one-shot domains, comparing the performance of the gender based separation approach with a basic approach which is not gender sensitive, revealed higher payoffs of the former in almost all the domains. Moreover, using the proposed learning algorithm further improved the results.

#index 1269801
#* A connectionist cognitive model for temporal synchronisation and learning
#@ Luís C. Lamb;Rafael V. Borges;Artur S. d'Avila Garcez
#t 2007
#c 10
#% 92148
#% 175368
#% 189699
#% 191753
#% 317108
#% 356892
#% 378009
#% 409507
#% 418103
#% 427296
#% 798723
#% 891066
#% 926265
#% 1275318
#! The importance of the efforts towards integrating the symbolic and connectionist paradigms of artificial intelligence has been widely recognised. Integration may lead to more effective and richer cognitive computational models, and to a better understanding of the processes of artificial intelligence across the field. This paper presents a new model for the representation, computation, and learning of temporal logic in connectionist systems. The model allows for the encoding of past and future temporal logic operators in neural networks, through a neural-symbolic translation algorithms introduced in the paper. The networks are relatively simple and can be used for reasoning about time and for learning by examples with the use of standard neural learning algorithms. We validate the model in a well-known application dealing WIth temporal synchronisation in distributed knowledge systems. This opens several interesting research paths in cognitive modelling, with potential applications in agent technology, learning and reasoning.

#index 1269802
#* Enabling domain-awareness for a generic natural language interface
#@ Yunyao Li;Ishan Chaudhuri;Huahai Yang;Satinder Singh;H. V. Jagadish
#t 2007
#c 10
#% 428249
#% 445448
#% 810101
#% 811958
#% 1688287
#% 1733302
#! In this paper, we present a learning-based approach for enabling domain-awareness for a generic natural language interface. Our approach automatically acquires domain knowledge from user Interactions and Incorporates the knowledge learned to improve the generic system. We have embedded our approach in a generic natural language interface and evaluated the extended system against two benchmark datasets. We found that the performance of the original generic system can be substantially improved through automatic domain knowledge extraction and incorporation. We also show that the generic system with domain-awareness enabled by our approach can achieve performance similar to that of previous learning-based domain-specific systems.

#index 1269803
#* A corpus-based hybrid approach to music analysis and composition
#@ Bill Manaris;Patrick Roos;Penousal Machado;Dwight Krehbiel;Luca Pellicoro;Juan Romero
#t 2007
#c 10
#% 179713
#% 310057
#% 849862
#% 919208
#% 1394160
#! We present a corpus-based hybrid approach to music analysis and composition, which incorporates statistical, connectionist, and evolutionary components. Our framework employs artificial music critics, which may be trained on large music corpora, and then pass aesthetic judgment on music artifacts. Music artifacts are generated by an evolutionary music composer, which utilizes music critics as fitness functions. To evaluate this approach we conducted three experiments. First, using music features based on Zipf's law, we trained artificial neural networks to predict the popularity of 992 musical pieces with 87.85% accuracy, Then, assuming that popularity correlates with aesthetics, we incorporated such neural networks into a genetic-programming system, called NEvMuse. NEvMuse autonomously "composed" novel variations of J.S. Bach's Invention #13 in A minor (BWV 784), variations which many listeners found to be aesthetically pleasing. Finally, we compared aesthetic judgments from an artificial music critic with emotional responses from 23 human subjects. Significant correlations were found. We provide evaluation results and samples of generated music. These results have implications for music information retrieval and computer-aided music composition.

#index 1269804
#* Recognition of hand drawn chemical diagrams
#@ Tom Y. Ouyang;Randall Davis
#t 2007
#c 10
#% 190581
#% 782260
#% 790481
#% 910274
#% 1279280
#% 1297972
#% 1910061
#! Chemists often use hand-drawn structural diagrams to capture and communicate ideas about organic compounds. However, the software available today for specifying these structures to a computer relies on a traditional mouse and keyboard interface, and as a result lacks the ease of use, naturalness, and speed of drawing on paper. In response, we have developed a novel sketch-based system capable of interpreting hand-drawn organic chemistry diagrams, allowing users to draw molecules with a pen-based input device in much the same way that they would on paper. The system's ability to interpret a sketch is based on knowledge about both chemistry and chemical drawing conventions. The system employs a trainable symbol recognizer incorporating both feature-based and image-based methods to locate and identify symbols in the sketch. Analysis of the spatial context around each symbol allows the system to choose among competing interpretations and determine an initial structure for the molecule. Finally, knowledge of chemistry (in particular chemical valence) enables the system to check the validity of its interpretation and, when necessary, refine it to recover from inconsistencies. We demonstrate that the system is capable of recognizing diagrams of common organic molecules and show that using domain knowledge produces a noticeable improvement in recognition accuracy.

#index 1269805
#* Authorial idioms for target distributions in TTD-MDPs
#@ David L. Roberts;Sooraj Bhat;Kenneth St. Clair;Charles L. Isbell
#t 2007
#c 10
#% 705161
#% 890319
#% 1024875
#% 1250348
#! In designing Markov Decision Processes (MDP), one must define the world, its dynamics, a set of actions, and a reward function. MDPs are often applied in situations where there is a clear choice of reward functions and in these cases significant care must be taken to construct a reward function that induces the desired behavior. In this paper, we consider an analogous design problem: crafting a target distribution in Targeted Trajectory Distribution MDPs (TTD-MDPs). TTD-MDPs produce probabilistic policies that minimize divergence from a target distribution of trajectories from an underlying MDP. They are an extension of MDPs that provide variety of experience during repeated execution. Here, we present a brief overview of TTD-MDPs with approaches for constructing target distributions. Then we present a novel authorial idiom for creating target distributions using prototype trajectories. We evaluate these approaches on a drama manager for an interactive game.

#index 1269806
#* Visualization and adjustment of evaluation functions based on evaluation values and win probability
#@ Shogo Takeuchi;Tomoyuki Kaneko;Kazunori Yamaguchi;Satoru Kawai
#t 2007
#c 10
#% 183499
#% 314787
#% 722929
#% 1665148
#! We present a method of visualizing and adjusting the evaluation functions in game programming in this paper. It is widely recognized that an evaluation function should assign a higher evaluation value to a position with greater probability of a win. However, this relation has not been utilized directly to tune evaluation functions because of the difficulty of measuring the probability of wins in deterministic games. We present the use of win percentage to utilize this relation in positions having the same evaluation value as win probability, where the positions we used were stored in a large database of game records. We introduce an evaluation curve formed by evaluation values and win probabilities, to enable evaluation functions to be visualized. We observed that evaluation curves form a sigmoid in various kinds of games and that these curves may split depending on the properties of positions. Because such splits indicate that an evaluation function that is visualized misestimates positions with less probability of winning, we can improve this by fitting evaluation curves to one. Our experiments with Chess and Shogi revealed that deficiencies in evaluation functions could be successfully visualized, and that improvements by automatically adjusting their weights were confirmed by self-plays.

#index 1269807
#* Humans perform semi-supervised classification too
#@ Xiaojin Zhu;Timothy Rogers;Ruichen Qian;Chuck Kalish
#t 2007
#c 10
#% 203341
#% 305016
#% 311027
#% 1808946
#! We explore the connections between machine learning and human learning in one form of semi-supervised classification. 22 human subjects completed a novel 2-class categorization task in which they were first taught to categorize a single labeled example from each category, and subsequently were asked to categorize, without feedback, a large set of additional items. Stimuli were visually complex and unrecognizable shapes. The unlabeled examples were sampled from a bimodal distribution with modes appearing either to the left (left-shift condition) or right (right-shift condition) of the two labeled examples. Results showed that, although initial decision boundaries were near the middle of the two labeled examples, after exposure to the unlabeled examples, they shifted in different directions in the two groups. In this respect, the human behavior conformed well to the predictions of a Gaussian mixture model for semi-supervised learning. The human behavior differed from model predictions in other interesting respects, suggesting some fruitful avenues for future inquiry.

#index 1269808
#* Semantic inference at the lexical-syntactic level
#@ Roy Bar-Haim;Ido Dagan;Iddo Greental;Eyal Shnarch
#t 2007
#c 10
#% 741891
#% 939902
#% 983625
#% 1269525
#% 1269534
#% 1672483
#! Semantic inference is an important component in many natural language understanding applications. Classical approaches to semantic inference rely on complex logical representations. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, but lack a principled inference framework. We propose a generic semantic inference framework that operates directly on syntactic trees. New trees are infened by applying entailment rules, which provide a unified representation for varying types of inferences. Rules were generated by manual and automatic methods, Covering generic linguistic structures as well as specific lexical-based inferences. Initial empirical evaluation in a Relation Extraction setting supports the validity of our approach.

#index 1269809
#* Turning lectures into comic books using linguistically salient gestures
#@ Jacob Eisenstein;Regina Barzilay;Randall Davis
#t 2007
#c 10
#% 73441
#% 286920
#% 340842
#% 387791
#% 451652
#% 529486
#% 592070
#% 592094
#% 939857
#% 939866
#% 1265017
#! Creating video recordings of events such as lectures or meetings is increasingly inexpensive and easy. However, reviewing the content of such video may be time-consuming and difficult. Our goal is to produce a "comic book" summary, in which a transcript is augmented with keyframes that disambiguate and clarify accompanying text. Unlike most previous keyframe extraction systems which rely primarily on visual cues, we present a linguistically-motivated approach that selects keyframes that contain salient gestures. Rather than learning gesture salience directly, it is estimated by measuring the contribution of gesture to understanding other discourse phenomena. More specifically, we bootstrap from multimodal coreference resolution to identify gestures that improve performance. We then select keyframes that capture these gestures. Our model predicts gesture salience as a hidden variable in a conditional framework, with observable features from both the visual and textual modalities. This approach significantly outperforms competitive baselines that do not use gesture information.

#index 1269810
#* A robot that uses existing vocabulary to infer non-visual word meanings from observation
#@ Kevin Gold;Brian Scassellati
#t 2007
#c 10
#% 252472
#% 279755
#% 406265
#% 669288
#% 706604
#% 736300
#% 766223
#! The authors present TWIG, a visually grounded word-learning system that uses its existing knowledge of vocabulary, grammar, and action schemas to help it learn the meanings of new words from its environment. Most systems built to learn word meanings from sensory data focus on the "base case" of learning words when the robot knows nothing, and do not incorporate grammatical knowledge to aid the process of inferring meaning. The present study shows how using existing language knowledge can aid the word-learning process in three ways. First, partial parses of sentences can focus the robot's attention on the correct item or relation in the environment. Second, grammatical inference can suggest whether a new word refers to a unary or binary relation. Third, the robot's existing predicate schemas can suggest possibilities for a new predicate. The authors demonstrate that TWIG can use its understanding of the phrase '"got the ball" while watching a game of catch to learn that "I" refers to the speaker, "you" refers to the addressee, and the names refer to particular people. The robot then uses these new words to learn that "am" and "are" refer to the identity relation.

#index 1269811
#* ASKNet: automated semantic knowledge network
#@ Brian Harrington;Stephen Clark
#t 2007
#c 10
#% 198055
#% 297229
#% 747791
#% 752077
#% 783633
#% 855113
#% 855154
#% 938666
#% 939515
#% 939689
#% 939828
#! The ASKNet system is an attempt to automatically generate large scale semantic knowledge networks from natural language text. State-of-the-art language processing tools, including parsers and semantic analysers, are used to turn input sentences into fragments of semantic network. These network fragments are combined using spreading activation-based algorithms which utilise both lexical and semantic information. The emphasis of the system is on wide-coverage and speed of construction. In this paper we show how a network consisting of over 1.5 million nodes and 3.5 million edges, more than twice as large as any network currently available, can be created in less than 3 days. We believe that the methods proposed here will enable the construction of semantic networks on a scale never seen before, and in doing so reduce the knowledge acquisition bottleneck for AI.

#index 1269812
#* Learning language semantics from ambiguous supervision
#@ Rohit J. Kate;Raymond J. Mooney
#t 2007
#c 10
#% 70370
#% 309208
#% 408652
#% 722803
#% 939615
#% 1269528
#% 1344851
#% 1476277
#! This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.

#index 1269813
#* Disambiguating noun compounds
#@ Su Nam Kim;Timothy Baldwin
#t 2007
#c 10
#% 283174
#% 705934
#% 741080
#% 741085
#% 741900
#% 747893
#% 748550
#% 817955
#% 924403
#% 938688
#% 939379
#% 939950
#% 943827
#% 1249579
#% 1250630
#% 1271226
#% 1279327
#% 1712197
#% 1911089
#! This paper is concerned with the interaction between word sense disambiguation and the interpretation of noun compounds (NCs) in English. We develop techniques for disambiguating word sense specifically in NCs, and then investigate whether word sense information can aid in the semantic relation interpretation of NCs. To disambiguate word sense, we combine the one sense per collocation heuristic with the grammatical role of polysemous nouns and analysis of word sense combinatories. We built supervised and unsupervised classifiers for the task and demonstrate that the supervised methods arc superior to a number of baselines and also a benchmark state-of-the-art WSD system. Finally, we show that WSD can significantly improve the accuracy of NC interpretation.

#index 1269814
#* A meta-learning approach for selecting between response automation strategies in a help-desk domain
#@ Yuval Marom;Ingrid Zukerman;Nathalie Japkowicz
#t 2007
#c 10
#% 406493
#% 414514
#% 509533
#% 798820
#% 816157
#% 817578
#% 945223
#% 1275022
#% 1734438
#! We present a corpus-based approach for the automation of help-desk responses to users' email requests. Automation is performed on the basis of the similarity between a request and previous requests, which affects both the content included in a response and the strategy used to produce it. The latter is the focus of this paper, which introduces a meta-learning mechanism that selects between different information-gathering strategies, such as document retrieval and multidocument summarization. Our results show that this mechanism outperforms a random strategy-selection policy, and performs competitively with a gold baseline that always selects the best strategy.

#index 1269815
#* Joint inference in information extraction
#@ Hoifung Poon;Pedro Domingos
#t 2007
#c 10
#% 271128
#% 312860
#% 450888
#% 729913
#% 788107
#% 850430
#% 915340
#% 938708
#% 939377
#% 1000502
#% 1250579
#% 1269496
#% 1289529
#! The goal of information extraction is to extract database records from text or semi-structured sources. Traditionally, information extraction proceeds by first segmenting each candidate record separately, and then merging records that refer to the same entities. While computationally efficient, this approach is suboptimal, because it ignores the fact that segmenting one candidate record can help to segment similar ones. For example, resolving a well-segmented field with a less-clear one can disambiguate the latter's boundaries. In this paper we propose a joint approach to information extraction, where segmentation of all records and entity resolution are performed together in a single integrated inference process. While a number of previous authors have taken steps in this direction (eg., Pasula et al. (2003), Wellner et al. (2004)), to our knowledge this is the first fully joint approach. In experiments on the CiteSeer and Cora citation matching datasets, joint inference improved accuracy, and our approach outperformed previous ones. Further, by using Markov logic and the existing algorithms for it, our solution consisted mainly of writing the appropriate logical formulas, and required much less engineering than previous ones.

#index 1269816
#* Content analysis for proactive intelligence: marshaling frame evidence
#@ A. P. Sanfilippo;A. J. Cowell;S. C. Tratz;A. M. Boek;A. K. Cowell;C. Posse;L. C. Pouchard
#t 2007
#c 10
#% 939378
#% 1265043
#% 1265109
#! Modeling and simulation have great potential as technologies capable of aiding analysts in making accurate predictions of future situations to help provide competitive advantage and avoid strategic surprise. However, to make modeling and simulation effective, an evidence-marshaling process is needed that addresses the information needs of the modeling task, as detailed by subject matter experts. We suggest that such an evidence-marshaling process can be obtained by combining natural language processing and content analysis techniques to provide quantified qualitative content assessments, and describe a case study on the acquisition and marshaling of frames from unstructured text.

#index 1269817
#* Mining sequential patterns and tree patterns to detect erroneous sentences
#@ Guihua Sun;Gao Cong;Xiaohua Liu;Chin-Yew Lin;Ming Zhou
#t 2007
#c 10
#% 316097
#% 729941
#% 742219
#% 746865
#% 747910
#% 810064
#% 815811
#% 817524
#% 879595
#% 939531
#% 939532
#% 939859
#! An important application area of detecting erroneous sentences is to provide feedback for writers of English as a Second Language. This problem is difficult since both erroneous and correct sentences are diversified. In this paper, we propose a novel approach to identifying erroneous sentences. We first mine labeled tree patterns and sequential patterns to characterize both erroneous and correct sentences. Then the discovered patterns are utilized in two ways to distinguish correct sentences from erroneous sentences: (1) the patterns are transformed into sentence features for existing classification models, e.g, SVM; (2) the patterns are used to build a rule-based classification model. Experimental results show that both techniques are promising while the second technique outperforms the first approach. Moreover, the classification model in the second proposal is easy to understand, and we can provide intuitive explanation for classification results.

#index 1269818
#* Single document summarization with document expansion
#@ Xiaojun Wan;Jianwu Yang
#t 2007
#c 10
#% 194251
#% 262112
#% 268079
#% 290830
#% 340884
#% 340885
#% 340971
#% 342828
#% 387427
#% 397136
#% 397137
#% 755863
#% 816173
#% 818266
#% 855043
#% 1275213
#! Existing methods for single document summarization usually make use of only the information contained in the specified document. This paper proposes the technique of document expansion to provide more knowledge to help single document summarization. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and then the graph-ranking based algorithm is applied on the expanded document set for extracting sentences from the single document, by making use of both the within-document relationships between sentences of the specified document and the cross-document relationships between sentences of all documents in the document set. The experimental results on the DUC2002 dataset demonstrate the effectiveness of the proposed approach based on document expansion. The cross-document relationships between sentences in the expanded document set are validated to be very important for single document summarization.

#index 1269819
#* Recognizing textual entailment using a subsequence kernel method
#@ Rui Wang;Günter Neumann
#t 2007
#c 10
#% 290482
#% 939551
#% 939932
#% 1280237
#% 1280243
#! We present a novel approach to recognizing Textual Entailment. Structural features are constructed from abstract tree descriptions, which are automatically extracted from syntactic dependency trees. These features are then applied in a subsequence-kernel-based classifier to learn whether an entailment relation holds between two texts. Our method makes use of machine learning techniques using a limited data set, no external knowledge bases (e.g. WordNet), and no handcrafted inference rules. We achieve an accuracy of 74.5% for text pairs in the Information Extraction and Question Answering task, 63.6% for the RTE-2 test data, and 66.9% for the RET-3 test data.

#index 1269820
#* Proceedings of the 22nd national conference on Artificial intelligence - Volume 2
#@ Anthony Cohn
#t 2007
#c 10

#index 1269821
#* Incorporating observer biases in keyhole plan recognition (efficiently!)
#@ Dorit Avrahami-Zilberbrand;Gal A. Kaminka
#t 2007
#c 10
#% 147680
#% 292235
#% 812412
#% 823974
#% 1250199
#% 1289455
#% 1784833
#! Plan recognition is the process of inferring other agents' plans and goals based on their observable actions. Essentially all previous work in plan recognition has focused on the recognition process itself, with no regard to the use of the information in the recognizing agent. As a result, low-likelihood recognition hypotheses that may imply significant meaning to the observer, are ignored in existing work. In this paper, we present novel efficient algorithms that allows the observer to incorporate her own biases and preferences--in the form of a utility function--into the plan recognition process. This allows choosing recognition hypotheses based on their expected utility to the observer. We call this Utility-based Plan Recognition (UPR). While reasoning about such expected utilities is intractable in the general case, we present a hybrid symbolic/decision-theoretic plan recognizer, whose complexity is O(N DT), where N is the plan library size, D is the depth of the library and T is the number of observations. We demonstrate the efficacy of this approach with experimental results in several challenging recognition tasks.

#index 1269822
#* Concurrent action execution with shared fluents
#@ Michael Buro;Alexander Kovarsky
#t 2007
#c 10
#% 70370
#% 528302
#% 676373
#% 1269457
#% 1271957
#% 1272008
#% 1289204
#! Concurrent action execution is important for plan-length minimization. However, action specifications are often limited to avoid conflicts arising from precondition/effect interactions. PDDL -- the planning domain definition language -- for example, implements the "no moving targets" rule, which means that no two actions can simultaneously make use of a value if one of the two is updating the value. This rule poses problems for resource allocation planning in which resource values are accessed in preconditions and effects. A simple example is construction actions that consume certain amounts of a resource. For speeding up plan execution, we would like to be able to dispatch several construction actions simultaneously. Because action preconditions depend on resource values and action effects change them, the "no moving targets" rule does not allow concurrent execution. However, if sufficient resources are available, executing actions simultaneously poses no problems. This paper addresses the problem of deciding whether a set of actions produced by a planning system can be executed concurrently in the presence of fluent variables that occur in both action preconditions and effects. We first motivate the concurrent action execution problem by introducing a fair action scheduling algorithm for real-time strategy (RTS) games. Then we prove that the general decision problem, when restricting effects and preconditions to polynomial time computations, is co-NP complete. Thereafter, we focus on problem restrictions based on commutative operators which allow us to specify sufficient conditions for concurrent executability that can be checked quickly if the number of shared fluents is small. Finally, we apply these findings to action execution with shared resources in RTS games.

#index 1269823
#* A situation-calculus semantics for an expressive fragment of PDDL
#@ Jens Claßen;Yuxiao Hu;Gerhard Lakemeyer
#t 2007
#c 10
#% 100159
#% 229083
#% 342119
#% 529354
#% 701158
#% 1271818
#% 1272008
#% 1275051
#% 1275052
#% 1289429
#! The Planning Domain Definition Language (PDDL) has become a common language to specify planning problems, facilitating the formulation of benchmarks and a direct comparison of planners. Over the years PDDL has been extended beyond STRIPS and ADL in various directions, for example, by adding time and concurrent actions. The current semantics of PDDL is purely meta-theoretic and quite complex, which makes an analysis difficult. Moreover, relating the language to other action formalisms is also nontrivial. We propose an alternative semantics for an expressive fragment of PDDL within the situation calculus. This yields at least two advantages. For one, the new semantics is purely declarative, making it amenable to an analysis in terms of logical entailments. For another, it facilitates the comparison with and mapping to other formalisms that are defined on top of the same logic, such as the agent control language Golog. In particular we obtain the semantical foundation for embedding efficient PDDL-based planners into the more expressive, yet computationally expensive Golog, thus combining the benefits of both. Other by-products of our investigations are a simpler account of durative actions in the situation calculus and a new notion of compulsory actions.

#index 1269824
#* A modular action description language for protocol composition
#@ Nirmit Desai;Munindar P. Singh
#t 2007
#c 10
#% 378992
#% 422074
#% 636333
#% 763743
#% 799801
#% 846069
#% 890424
#% 1250641
#% 1269881
#% 1704221
#! Protocols are modular abstractions that capture patterns of interaction among agents. The compelling vision behind protocols is to enable creating customized interactions by refining and composing existing protocols. Realizing this vision presupposes (1) maintaining repositories of protocols and (2) refining and composing selected protocols. To this end, this paper synthesizes recent advances on protocols and on the knowledge representation of actions. This paper presents MAD-P, a modular action description language tailored for protocols. MAD-P enables building an aggregation hierarchy of protocols via composition. This paper demonstrates the value of such compositions via a simplified, but realistic, business scenario.

#index 1269825
#* Detecting execution failures using learned action models
#@ Maria Fox;Jonathan Gough;Derek Long
#t 2007
#c 10
#% 284558
#% 529189
#% 873957
#% 1250174
#% 1270128
#% 1476265
#! Planners reason with abstracted models of the behaviours they use to construct plans. When plans are turned into the instructions that drive an executive, the real behaviours interacting with the unpredictable uncertainties of the environment can lead to failure. One of the challenges for intelligent autonomy is to recognise when the actual execution of a behaviour has diverged so far from the expected behaviour that it can be considered to be a failure. In this paper we present an approach by which a trace of the execution of a behaviour is monitored by tracking its most likely explanation through a learned model of how the behaviour is normally executed. In this way, possible failures are identified as deviations from common patterns of the execution of the behaviour. We perform an experiment in which we inject errors into the behaviour of a robot performing a particular task, and explore how well a learned model of the task can detect where these errors occur.

#index 1269826
#* ESP: a logic of only-knowing, noisy sensing and acting
#@ Alfredo Gabaldon;Gerhard Lakemeyer
#t 2007
#c 10
#% 194652
#% 252183
#% 266387
#% 284644
#% 342119
#% 486934
#% 529345
#% 752489
#% 1289429
#! When reasoning about actions and sensors in realistic domains, the ability to cope with uncertainty often plays an essential role. Among the approaches dealing with uncertainty, the one by Bacchus, Halpern and Levesque, which uses the situation calculus, is perhaps the most expressive. However, there are still some open issues. For example, it remains unclear what an agent's knowledge base would actually look like. The formalism also requires second-order logic to represent uncertain beliefs, yet a first-order representation clearly seems preferable. In this paper we show how these issues can be addressed by incorporating noisy sensors and actions into an existing logic of only-knowing.

#index 1269827
#* Action-space partitioning for planning
#@ Natalia H. Gardiol;Leslie Pack Kaelbling
#t 2007
#c 10
#% 496243
#% 1271962
#% 1289207
#% 1289545
#% 1478760
#! For autonomous artificial decision-makers to solve realistic tasks, they need to deal with searching through large state and action spaces under time pressure. We study the problem of planning in such domains and show how structured representations of the environment's dynamics can help partition the action space into a set of equivalence classes at run time. The partitioned action space is then used to produce a reduced set of actions. This technique speeds up search and can yield significant gains in planning efficiency.

#index 1269828
#* Planning as satisfiability with preferences
#@ Enrico Giunchiglia;Marco Maratea
#t 2007
#c 10
#% 131357
#% 266398
#% 271832
#% 344878
#% 496111
#% 829318
#% 1223264
#% 1275063
#! Planning as Satisfiability is one of the most well-known and effective technique for classical planning: SATPLAN has been the winning system in the deterministic track for optimal planners in the 4th International Planning Competition (IPC) and a co-winner in the 5th IPC. In this paper we extend the Planning as Satisfiability approach in order to handle preferences and SATPLAN in order to solve problems with simple preferences. The resulting system, SATPLAN(P) is competitive with SGPLAN, the winning system in the category "simple preferences" at the last IPC. Further, we show that SATPLAN(P) performances are (almost) always comparable to those of SATPLAN when solving the same problems without preferences: in other words, introducing simple preferences in SATPLAN does not affect its performances. This latter result is due both to the particular mechanism we use in order to incorporate preferences in SAT-PLAN and to the relative low number of soft goals (each corresponding to a simple preference) usually present in planning problems. Indeed, if we consider the issue of determining minimal plans (corresponding to problems with thousands of preferences) the performances of SATPLAN(P) are comparable to those of SATPLAN in many cases, but can be significantly worse when the number of preferences is very high compared to the total number of variables in the problem. Our analysis is conducted considering both qualitative and quantitative preferences, different reductions from quantitative to qualitative ones, and most of the propositional planning domains from the IPCs and that SATPLAN can handle.

#index 1269829
#* Filtering, decomposition and search space reduction for optimal sequential planning
#@ Stéphane Grandcolas;Cyril Pain-Barre
#t 2007
#c 10
#% 283220
#% 344878
#% 495768
#% 496111
#% 1269544
#% 1271809
#% 1271962
#% 1279349
#% 1290109
#! We present in this paper a hybrid planning system Which combines constraint satisfaction techniques and planning heuristics to produce optimal sequential plans. It integrates its own consistency rules and filtering and decomposition mechanisms suitable for planning. Given a fixed bound on the plan length, our planner works directly on a structure related to Graphplan's planning graph. This structure is incrementally built: Each time it is extended, a sequential plan is searched. Different search strategies may be employed. Currently, it is a forward chaining search based on problem decomposition with action sets partitioning. Various techniques are used to reduce the search space, such as memorizing nogood states or estimating goals reachability. In addition, the planner implements two different techniques to avoid enumerating some equivalent action sequences. Empirical evaluation shows that our system is very competitive on many problems, especially compared to other optimal sequential planners.

#index 1269830
#* Stochastic filtering in a probabilistic action model
#@ Hannaneh Hajishirzi;Eyal Amir
#t 2007
#c 10
#% 44876
#% 128629
#% 284644
#% 303620
#% 417762
#% 527859
#% 528169
#% 528171
#% 716892
#% 723877
#% 1275173
#% 1279222
#! Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.

#index 1269831
#* Domain-independent construction of pattern database heuristics for cost-optimal planning
#@ Patrik Haslum;Adi Botea;Malte Helmert;Blai Bonet;Sven Koenig
#t 2007
#c 10
#% 337986
#% 337987
#% 939032
#% 1269544
#% 1272048
#% 1275127
#! Heuristic search is a leading approach to domain-independent planning. For cost-optimal planning, however, existing admissible heuristics are generally too weak to effectively guide the search. Pattern database heuristics (PDBs), which are based on abstractions of the search space, are currently one of the most promising approaches to developing better admissible heuristics. The informedness of PDB heuristics depends crucially on the selection of appropriate abstractions (patterns). Although PDBs have been applied to many search problems, including planning, there are not many insights into how to select good patterns, even manually. What constitutes a good pattern depends on the problem domain, making the task even more difficult for domain-independent planning, where the process needs to be completely automatic and generaL We present a novel way of constructing good patterns automatically from the specification of planning problem instances. We demonstrate that this allows a domain-independent planner to solve planning problems optimally in some very challenging domains, including a STRIPS formulation of the Sokoban puzzle.

#index 1269832
#* Web service composition as planning, revisited: in between background theories and initial state uncertainty
#@ Jörg Hoffmann;Piergiorgio Bertoli;Marco Pistore
#t 2007
#c 10
#% 572371
#% 763743
#% 789560
#% 867890
#% 873941
#% 1271962
#% 1272109
#% 1374395
#! Thanks to recent advances, AI Planning has become the underlying technique for several applications. Amongst these, a prominent one is automated Web Service Composition (WSC). One important issue in this context has been hardly addressed so far: WSC requires dealing with background ontologies. The support for those is severely limited in current planning tools. We introduce a planning formalism that faithfully represents WSC. We show that, unsurprisingly, planning in such a formalism is very hard. We then identify an interesting special case that covers many relevant WSC scenarios, and where the semantics are simpler and easier to deal with. This opens the way to the development of effective support tools for WSC. Furthermore, we show that if one additionally limits the amount and form of outputs that can be generated, then the set of possible states becomes static, and can be modelled in terms of a standard notion of initial state uncertainty. For this, effective tools exist; these can realize scalable WSC with powerful background ontologies. In an initial experiment, we show how scaling WSC instances are comfortably solved by a tool incorporating modern planning heuristics.

#index 1269833
#* Understanding performance tradeoffs in algorithms for solving oversubscribed scheduling
#@ Laurence A. Kramer;Laura V. Barbulescu;Stephen F. Smith
#t 2007
#c 10
#% 465855
#% 535156
#% 892076
#% 924615
#% 1250300
#% 1272136
#% 1272400
#% 1499506
#% 1698009
#! In recent years, planning and scheduling research has paid increasing attention to problems that involve resource oversubscription, where cumulative demand for resources outstrips their availability and some subset of goals or tasks must be excluded. Two basic classes of techniques to solve oversubscribed scheduling problems have emerged: searching directly in the space of possible schedules and searching in an alternative space of task permutations (by relying on a schedule builder to provide a mapping to schedule space). In some problem contexts, permutation-based search methods have been shown to outperform schedule-space search methods, while in others the opposite has been shown to be the case. We consider two techniques for which this behavior has been observed: TaskSwap (TS), a schedule-space repair search procedure, and Squeaky Wheel Optimization (SWO), a permutation-space scheduling procedure. We analyze the circumstances under which one can be expected to dominate the other. Starting from a real-world scheduling problem where SWO has been shown to outperform TS, we construct a series of problem instances that increasingly incorporate characteristics of a second real-world scheduling problem, where TS has been found to outperform SWO. Experimental results provide insights into when schedule-space methods and permutation-based methods may be most appropriate.

#index 1269834
#* The semantics of variables in action descriptions
#@ Vladimir Lifschitz;Wanwan Ren
#t 2007
#c 10
#% 3035
#% 7047
#% 243712
#% 763743
#% 1250641
#% 1274811
#% 1478800
#! Action description language C+ is more expressive than ADL in many ways; for instance, it addresses the ramification problem. On the other hand, ADL is based on first-order logic, while C+ is only propositional; expressions with variables, which are frequently used when action domains are described in C+, are merely schemas describing finite sets of causal laws that are formed according to the same pattern. In this paper we propose a new approach to the semantics of action descriptions with variables that combines attractive features of ADL and C+.

#index 1269835
#* On the partial observability of temporal uncertainty
#@ Michael D. Moffitt
#t 2007
#c 10
#% 107137
#% 179940
#% 252826
#% 496406
#% 529337
#% 873947
#% 1250645
#% 1250646
#% 1269549
#% 1289215
#% 1664984
#! We explore a means to both model and reason about partial observability within the scope of constraint-based temporal reasoning. Prior studies of uncertainty in Temporal CSPs have required the realization of all exogenous processes to be made entirely visible to the agent. We relax this assumption and propose an extension to the Simple Temporal Problem with Uncertainty (STPU), one in which the executing agent is made aware of the occurrence of only a subset of uncontrollable events. We argue that such a formalism is needed to encode those complex environments whose external phenomena share a common, hidden source of temporal causality. After characterizing the levels of controllability in the resulting Partially Observable STPU and various special cases, we generalize a known family of reduction rules to account for this relaxation, introducing the properties of extended contingency and sufficient observability. We demonstrate that these modifications enable a polynomial filtering algorithm capable of determining a local form of dynamic controllability; however, we also show that there do remain some instances whose global controllability cannot yet be correctly identified by existing inference rules, leaving the true computational complexity of dynamic controllability an open problem for future research.

#index 1269836
#* Minimal mental models
#@ David V. Pynadath;Stacy C. Marsella
#t 2007
#c 10
#% 147680
#% 1272071
#% 1272346
#% 1272376
#% 1289539
#% 1650293
#! Agents must form and update mental models about each other in a wide range of domains: team coordination, plan recognition, social simulation, user modeling, games of incomplete information, etc. Existing research typically treats the problem of forming beliefs about other agents as an isolated subproblem, where the modeling agent starts from an initial set of possible models for another agent and then maintains a belief about which of those models applies. This initial set of models is typically a full specification of possible agent types. Although such a rich space gives the modeling agent high accuracy in its beliefs, it will also incur high cost in maintaining those beliefs. In this paper, we demonstrate that by taking this modeling problem out of its isolation and placing it back within the overall decision-making context, the modeling agent can drastically reduce this rich model space without sacrificing any performance. Our approach comprises three methods. The first method clusters models that lead to the same behaviors in the modeling agent's decision-making context. The second method clusters models that may produce different behaviors, but produce equally preferred outcomes with respect to the utility of the modeling agent. The third technique sacrifices a fixed amount of accuracy by clustering models that lead to performance losses that are below a certain threshold. We illustrate our framework using a social simulation domain and demonstrate its value by showing the minimal mental model spaces that it generates.

#index 1269837
#* Asymptotically optimal encodings of conformant planning in QBF
#@ Jussi Rintanen
#t 2007
#c 10
#% 252608
#% 576843
#% 578752
#% 937600
#% 1250202
#% 1272399
#% 1289395
#% 1476298
#% 1675277
#! The world is unpredictable, and acting intelligently requires anticipating possible consequences of actions that are taken. Assuming that the actions and the world are deterministic, planning can be represented in the classical propositional logic. Introducing nondeterminism (but not probabilities) or several initial states increases the complexity of the planning problem and requires the use of quantified Boolean formulae (QBF). The currently leading logic-based approaches to conditional planning use explicitly or implicitly a QBF with the prefix ∃∀∃. We present formalizations of the planning problem as QBF which have an asymptotically optimal linear size and the optimal number of quantifier alternations in the prefix: ∃∀ and ∀∃. This is in accordance with the fact that the planning problem (under the restriction to polynomial size plans) is on the second level of the polynomial hierarchy, not on the third.

#index 1269838
#* Expressiveness of ADL and golog: functions make a difference
#@ Gabriele Röger;Bernhard Nebel
#t 2007
#c 10
#% 342119
#% 948941
#% 1271818
#% 1272008
#% 1275051
#! The main focus in the area of action languages, such as GOLOG, was put on expressive power, while the development in the area of action planning was focused on efficient plan generation. An integration of GOLOG and planning languages would provide great advantages. A user could constrain a system's behavior on a high level using GOLOG, while the actual low-level actions are planned by an efficient planning system. First endeavors have been made by Eyerich et al. by identifying a subset of the situation calculus (which is the basis of GOLOG) with the same expressiveness as the ADL fragment of PDDL. However, it was not proven that the identified restrictions define a maximum subset. The most severe restriction appears to be that functions are limited to constants, We will show that this restriction is indeed necessary in most cases.

#index 1269839
#* Purely epistemic markov decision processes
#@ Régis Sabbadin;Jérôme Lang;Nasolo Ravoanjanahary
#t 2007
#c 10
#% 30037
#% 275929
#% 310835
#% 388196
#% 578692
#% 703709
#% 1279369
#! Planning under uncertainty involves two distinct sources of uncertainty: uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such "purely epistemic" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is "only" NP-complete; however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.

#index 1269840
#* Automatic synthesis of a global behavior from multiple distributed behaviors
#@ Sebastian Sardina;Fabio Patrizi;Giuseppe De Giacomo
#t 2007
#c 10
#% 65904
#% 390685
#% 653685
#% 824702
#% 845433
#% 1081077
#% 1275054
#% 1650314
#% 1693738
#! We consider the problem of synthesizing a team of local behavior controllers to realize a fully controllable target behavior from a set of available partially controllable behaviors that execute distributively within a shared partially predictable, but fully observable, environment. Available behaviors stand for existing distributed components and are represented with (finite) nondeterministic transition systems. The target behavior is assumed to be fully deterministic and stands for the collective behavior that the system as a whole needs to guarantee. We formally define the problem within a general framework, characterize its computational complexity, and propose techniques to actually generate a solution. Also, we investigate the relationship between the distributed solutions and the centralized ones, in which a single global controller is conceivable.

#index 1269841
#* Optimal regression for reasoning about knowledge and actions
#@ Hans Van Ditmarsch;Andreas Herzig;Tiago De Lima
#t 2007
#c 10
#% 117869
#% 188086
#% 284644
#% 284647
#% 342119
#% 480340
#% 529811
#% 572366
#% 823863
#% 890213
#! We show how in the propositional case both Reiter's and Scherl & Levesque's solutions to the frame problem can be modelled in dynamic epistemic logic (DEL), and provide an optimal regression algorithm for the latter. Our method is as follows: we extend Reiter's framework by integrating observation actions and modal operators of knowledge, and encode the resulting formalism in DEL with announcement and assignment operators. By extending Lutz' recent satisfiability-preserving reduction to our logic, we establish optimal decision procedures for both Reiter's and Scherl & Levesque's approaches: satisfiability is NP-complete for one agent, PSPACE-complete for multiple agents and EXPTIME-complete when common knowledge is involved.

#index 1269842
#* A vision-based system for a UGV to handle a road intersection
#@ Javed Ahmed;Mubarak Shah;Andrew Miller;Don Harper;M. N. Jafri
#t 2007
#c 10
#% 131463
#% 1035183
#! We propose a real-time computer vision system that enables a UGV to safely cross urban road-intersections. Specifically, when the UGV approaches the stop sign at a 4-way intersection, it must be aware of the vehicles at the other three roads and adhere to traffic rules by waiting for its turn to proceed. The proposed solution consists of three main components: a vehicle detector, a tracker, and a finite-state-machine to model the traffic. We use an OT-MACH filter to detect the leading vehicle in each of three camera-views of the corresponding roads. Then, we track the vehicles using an edge-enhanced dynamic correlation tracker, which estimates the current and next positions, velocities, and accelerations of the vehicles. Finally, the finite-state-machine describes the traffic in each road with one of four possible states (i.e. No Vehicle Waiting, Arriving, Waiting, and Passing), and signals an autopilot system when it is safe to pass the intersection. We provide the results from an actual intersection with real traffic to demonstrate that the UGV is able to automatically navigate the intersection using the proposed system.

#index 1269843
#* Topological mapping with weak sensory data
#@ Gregory Dudek;Dimitri Marinakis
#t 2007
#c 10
#% 578682
#% 1271848
#% 1768808
#! In this paper, we consider the exploration of topological environments by a robot with weak sensory capabilities. We assume only that the robot can recognize when it has reached a vertex, and can assign a cyclic ordering to the edges leaving a vertex with reference to the edge it arrived from. Given this limited sensing capability, and without the use of any markers or additional information, we will show that the construction of a topological map is still feasible. This is accomplished through both the exploration strategy which is designed to reveal model inconsistencies and by a search process that maintains a bounded set of believable world models throughout the exploration process. Plausible models are selected through the use of a ranking heuristic function based on the principle of Occam's Razor. We conclude with numerical simulations demonstrating the performance of the algorithm.

#index 1269844
#* Hybrid inference for sensor network localization using a mobile robot
#@ Dimitri Marinakis;David Meger;Ioannis Rekleitis;Gregory Dudek
#t 2007
#c 10
#% 82083
#% 984708
#% 1250659
#% 1768660
#% 1851033
#! In this paper, we consider a hybrid solution to the sensor network position inference problem, which combines a real-time filtering system with information from a more expensive, global inference procedure to improve accuracy and prevent divergence. Many online solutions for this problem make use of simplifying assumptions, such as Gaussian noise models and linear system behaviour and also adopt a filtering strategy which may not use available information optimally. These assumptions allow near real-time inference, while also limiting accuracy and introducing the potential for ill-conditioning and divergence. We consider augmenting a particular real-time estimation method, the extended Kalman filter (EKF), with a more complex, but more highly accurate, inference technique based on Markov Chain Monte Carlo (MCMC) methodology. Conventional MCMC techniques applied to this problem can entail significant and time consuming computation to achieve convergence. To address this, we propose an intelligent bootstrapping process and the use of parallel, communicative chains of different temperatures, commonly referred to as parallel tempering. The combined approach is shown to provide substantial improvement in a realistic simulated mapping environment and when applied to a complex physical system involving a robotic platform moving in an office environment instrumented with a camera sensor network.

#index 1269845
#* Autonomous development of a grounded object ontology by a learning robot
#@ Joseph Modayil;Benjamin Kuipers
#t 2007
#c 10
#% 68182
#% 229084
#% 722460
#% 724345
#% 1250653
#% 1269503
#% 1269563
#! We describe how a physical robot can learn about objects from its own autonomous experience in the continuous world. The robot identifies statistical regularities that allow it to represent a physical object with a cluster of sensations that violate a static world model, track that cluster over time, extract percepts from that cluster, form concepts from similar percepts, and learn reliable actions that can be applied to objects. We present a formalism for representing the ontology for objects and actions, a learning algorithm, and the results of an evaluation with a physical robot.

#index 1269846
#* Online co-localization in indoor wireless networks by dimension reduction
#@ Jeffrey Junfeng Pan;Qiang Yang;Sinno Jialin Pan
#t 2007
#c 10
#% 337494
#% 339218
#% 613383
#% 770779
#% 819455
#% 840873
#% 848112
#% 862543
#% 862544
#% 889090
#% 1113048
#% 1250661
#% 1269361
#% 1275102
#% 1275152
#% 1289567
#% 1378394
#! This paper addresses the problem of recovering the locations of both mobile devices and access points from radio signals that come in a stream manner, a problem which we call online co-localization, by exploiting both labeled and unlabeled data from mobile devices and access points. Many tracking systems function in two phases: an offline training phase and an online localization phase. In the training phase, models are built from a batch of data that are collected offline. Many of them can not cope with a dynamic environment in which calibration data may come sequentially. In such case, these systems may gradually become inaccurate without a manually costly re-calibration. To solve this problem, we proposed an online co-localization method that can deal with labeled and unlabeled data stream based on semi-supervised manifold-learning techniques. Experiments conducted in wireless local area networks show that we can achieve high accuracy with less calibration effort as compared to several previous systems. Furthermore, our method can deal with online stream data relatively faster than its two-phase counterpart.

#index 1269847
#* Adaptive localization in a dynamic WiFi environment through multi-view learning
#@ Sinno Jialin Pan;James T. Kwok;Qiang Yang;Jeffrey Junfeng Pan
#t 2007
#c 10
#% 393059
#% 777786
#% 777788
#% 797049
#% 819455
#% 1113048
#% 1250661
#% 1275152
#% 1721311
#! Accurately locating users in a wireless environment is an important task for many pervasive computing and AI applications, such as activity recognition. In a WiFi environment, a mobile device can be localized using signals received from various transmitters, such as access points (APs). Most localization approaches build a map between the signal space and the physical location space in a offline phase, and then using the received-signal-strength (RSS) map to estimate the location in an online phase. However, the map can be outdated when the signal-strength values change with time due to environmental dynamics. It is infeasible or expensive to repeat data calibration for reconstructing the RSS map. In such a case, it is important to adapt the model learnt in one time period to another time period without too much recalibration. In this paper, we present a location-estimation approach based on Manifold co-Regularization, which is a machine learning technique for building a mapping function between data. We describe LeManCoR, a system for adapting the mapping function between the signal space and physical location space over different time periods based on Manifold Co-Regularization. We show that LeManCoR can effectively transfer the knowledge between two time periods without requiring too much new calibration effort. We illustrate LeMan-CoR's effectiveness in a real 802.11 WiFi environment.

#index 1269848
#* Simple robots with minimal sensing: from local visibility to global geometry
#@ Subhash Suri;Elias Vicari;Peter Widmayer
#t 2007
#c 10
#% 235114
#% 297915
#% 367254
#% 934104
#% 1269572
#! We consider problems of geometric exploration and self-deployment for simple robots that can only sense the combinatorial (non-metric) features of their surroundings. Even with such a limited sensing, we show that robots can achieve complex geometric reasoning and perform many non-trivial tasks. Specifically, we show that one robot equipped with a single pebble can decide whether the workspace environment is a simply-connected polygon and, if not, it can also count the number of holes in the environment. Highlighting the subtleties of our sensing model, we show that a robot can decide whether the environment is a convex polygon, yet it cannot resolve whether a particular vertex is convex. Finally, we show that using such local and minimal sensing, a robot can compute a proper triangulation of a polygon, and that the triangulation algorithm can be implemented collaboratively by a group of m such robots, each with Θ(n/m) memory. As a corollary of the triangulation algorithm, we derive a distributed analog of the well-known Art Gallery Theorem: a group of ⌊n/3⌋ (bounded memory) robots in our minimal sensing model can self-deploy to achieve visibility coverage of an n-vertex art gallery (polygon). This resolves an open question raised recently by Ganguli et al.

#index 1269849
#* Photometric and geometric restoration of document images using inpainting and shape-from-shading
#@ Li Zhang;Andy M. Yip;Chew Lim Tan
#t 2007
#c 10
#% 15505
#% 33917
#% 42030
#% 49144
#% 116841
#% 282188
#% 308601
#% 428526
#% 457880
#% 718502
#% 718589
#% 724197
#% 726722
#% 762465
#% 772874
#% 774929
#% 812345
#% 812433
#% 821886
#% 836754
#% 836815
#% 844840
#% 900497
#% 1856060
#! The popularity of current hand-held digital imaging devices such as camera phones, PDAs, camcorders has promoted the use of digital cameras to capture document images for daily information recording purpose. However, the captured images often contain photometric and geometric distortions when the documents are of non-planar shapes, which cause significant problems to various document image analysis (DIA) tasks such as OCR. In this paper, we propose a restoration framework that removes both photometric and geometric distortions in smoothly warped document images to facilitate human perception and machine recognition. First, the photometric distortions are corrected by separating the shading image from the reflectance image using inpainting and surface fitting techniques. Next, a 2-pass Shape-from-Shading (SFS) method is exploited to recover the document's surface shape based on the extracted shading image. Once the document's shape is obtained, the geometric distortions are rectified through a physically-based flattening process. Experiments on real document images show the performance of each sub-task and demonstrate a complete solution to the restoration of physically-distorted document images.

#index 1269850
#* Detection of multiple deformable objects using PCA-SIFT
#@ Stefan Zickler;Alexei Efros
#t 2007
#c 10
#% 241027
#% 336078
#% 443894
#% 635689
#% 812417
#% 1502463
#% 1854406
#! In this paper, we address the problem of identifying and localizing multiple instances of highly deformable objects in real-time video data. We present an approach which uses PCA-SIFT (Scale Invariant Feature Transform) in combination with a clustered voting scheme to achieve detection and localization of multiple objects while providing robustness against rapid shape deformation, partial occlusion, and perspective changes. We test our approach in two highly deformable robot domains and evaluate Its performance using ROC (Receiver Operating Characteristic) statistics.

#index 1269851
#* Heuristic evaluation functions for general game playing
#@ James Clune
#t 2007
#c 10
#% 241
#% 1250387
#! A general game playing program plays games that it has not previously encountered. A game manager program sends the game playing programs a description of a game's rules and objectives in a well-defined game description language. A central challenge in creating effective general game playing programs is that of constructing heuristic evaluation functions from game descriptions. This paper describes a method for constructing evaluation functions that represent exact values of simplified games. The simplified games are abstract models that incorporate the most essential aspects of the original game, namely payoff, control, and termination. Results of applying this method to a sampling of games suggest that heuristic evaluation functions based on our method are both comprehensible and effective.

#index 1269852
#* On the value of good advice: the complexity of A* search with accurate heuristics
#@ Hang Dinh;Alexander Russell;Yuan Su
#t 2007
#c 10
#% 241
#% 1722
#% 2194
#% 190611
#% 217812
#% 266116
#% 337986
#% 341672
#% 453859
#% 765250
#! We study the behavior of the classical A* search algorithm when coupled with a heuristic that provides estimates, accurate to within a small multiplicative factor, of the distance to a solution. We prove general upper bounds on the complexity of A* search, for both admissible and unconstrained heuristic functions, that depend only on the distribution of solution objective values. We go on to provide nearly matching lower bounds that are attained even by non-adversarially chosen solution sets induced by a simple stochastic model.

#index 1269853
#* Best-first search for treewidth
#@ P. Alex Dow;Richard E. Korf
#t 2007
#c 10
#% 208
#% 1722
#% 31482
#% 329486
#% 516650
#% 528338
#% 728026
#% 788060
#% 829310
#% 873948
#% 985940
#% 1250221
#% 1650778
#% 1700197
#! Finding the exact treewidth of a graph is central to many operations in a variety of areas, including probabilistic reasoning and constraint satisfaction. Treewidth can be found by searching over the space of vertex elimination orders. This search space differs from those where best-first search is typically applied, because a solution path is evaluated by its maximum edge cost instead of the sum of its edge costs. We show how to make best-first search admissible on max-cost problem spaces. We also employ breadth-first heuristic search to reduce the memory requirement while still eliminating all duplicate nodes in the search space. Our empirical results show that our algorithms find the exact treewidth an order of magnitude faster than the previous state-of-the-art algorithm on hard benchmark graphs.

#index 1269854
#* Automatic algorithm configuration based on local search
#@ Frank Hutter;Holger H. Hoos;Thomas Stützle
#t 2007
#c 10
#% 283230
#% 421283
#% 490637
#% 535307
#% 750050
#% 846606
#% 903335
#% 959550
#% 1289377
#% 1664973
#! The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variable/value ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile: it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at http://www.cs.ubc.ca/labs/beta/Projects/ParamILS.

#index 1269855
#* Near-optimal search in continuous domains
#@ Samuel Ieong;Nicolas Lambert;Yoav Shoham;Ronen Brafman
#t 2007
#c 10
#% 6603
#% 367254
#% 934104
#% 961216
#% 1250662
#% 1289549
#% 1650355
#! We investigate search problems in continuous state and action spaces with no uncertainty. Actions have costs and can only be taken at discrete time steps (unlike the case with continuous control). Given an admissible heuristic function and a starting state, the objective is to find a minimum-cost plan that reaches a goal state. As the continuous domain does not allow the tight optimality results that are possible in the discrete case (for example by A*), we instead propose and analyze an approximate forward-search algorithm that has the following provable properties. Given a desired accuracy Ε, and a bound d on the length of the plan, the algorithm computes a lower bound L on the cost of any plan. It either (a) returns a plan of cost L that is at most Ε more than the optimal plan, or (b) if, according to the heuristic estimate, there may exist a plan of cost L of length d, returns a partial plan that traces the first d steps of such plan. To our knowledge, this is the first algorithm that provides optimality guarantees in continuous domains with discrete control and without uncertainty.

#index 1269856
#* Analyzing the performance of pattern database heuristics
#@ Richard E. Korf
#t 2007
#c 10
#% 2194
#% 266116
#% 283237
#% 337986
#% 348576
#% 939032
#% 1272048
#% 1275127
#% 1279388
#% 1478838
#! We introduce a model for predicting the performance of IDA* using pattern database heuristics, as a function of the branching factor of the problem, the solution depth, and the size of the pattern databases. While it is known that the larger the pattern database, the more efficient the search, we provide a quantitative analysis of this relationship. In particular, we show that for a single goal state, the number of nodes expanded by IDA* is a fraction of (logb s + 1)/s of the nodes expanded by a brute-force search, where b is the branching factor, and s is the size of the pattern database. We also show that by taking the maximum of at least two pattern databases, the number of node expansions decreases linearly with s compared to a brute-force search. We compare our theoretical predictions with empirical performance data on Rubik's Cube. Our model is conservative, and overestimates the actual number of node expansions.

#index 1269857
#* Best-first AND/OR search for graphical models
#@ Radu Marinescu;Rina Dechter
#t 2007
#c 10
#% 1722
#% 25470
#% 36698
#% 44876
#% 64788
#% 314925
#% 419954
#% 644201
#% 944140
#% 1223215
#% 1250346
#% 1289364
#% 1289386
#% 1732431
#! The paper presents and evaluates the power of best-first search over AND/OR search spaces in graphical models. The main virtue of the AND/OR representation is its sensitivity to the structure of the graphical model, which can translate into significant time savings. Indeed, in recent years depth-first AND/OR Branch-and-Bound algorithms were shown to be very effective when exploring such search spaces, especially when using caching. Since best-first strategies are known to be superior to depth-first when memory is utilized, exploring the best-first control strategy is called for. In this paper we introduce two classes of best-first AND/OR search algorithms; those that explore a context-minimal AND/OR search graph and use static variable orderings, and those that use dynamic variable orderings but explore an AND/OR search tree. The superiority of the best-first search approach is demonstrated empirically on various real-world benchmarks.

#index 1269858
#* Theta*: any-angle path planning on grids
#@ Alex Nash;Kenny Daniel;Sven Koenig;Ariel Feiner
#t 2007
#c 10
#% 90741
#% 319838
#% 534450
#% 1269574
#! Grids with blocked and unblocked cells are often used to represent terrain in computer games and robotics. However, paths formed by grid edges can be sub-optimal and unrealistic looking, since the possible headings are artificially constrained. We present Theta*, a variant of A*, that propagates informati on along grid edges without constraining the paths to grid edges. Theta* is simple, fast and finds short and realistic looking paths. We compare Theta* against both Field D*, the only other variant of A* that propagates information along grid edges without constraining the paths to grid edges, and A* with post-smoothed paths. Although neither path planning method is guaranteed to find shortest paths, we show experimentally that Theta* finds shorter and more realistic looking paths than either of these existing techniques.

#index 1269859
#* Anytime optimal coalition structure generation
#@ Talal Rahwan;Sarvapali D. Ramchurn;Viet D. Dang;Andrea Giovannucci;Nicholas R. Jennings
#t 2007
#c 10
#% 4382
#% 243206
#% 252199
#% 267752
#% 284645
#% 773258
#% 1275134
#! A key problem when forming effective coalitions of autonomous agents is determining the best groupings, or the optimal coalition structure, to select to achieve some goal. To this end, we present a novel, anytime algorithm for this task that is significantly faster than current solutions. Specifically, we empirically show that we are able to find solutions that are optimal in 0.082% of the time taken by the state of the art dynamic programming algorithm (for 27 agents), using much less memory (O(2n) instead of O(3n) for n agents). Moreover, our algorithm is the first to be able to find solutions for more than 17 agents in reasonable time (less than 90 minutes for 27 agents, as opposed to around 2 months for the best previous solution).

#index 1269860
#* Fluxplayer: a successful general game player
#@ Stephan Schiffel;Michael Thielscher
#t 2007
#c 10
#% 60140
#% 126860
#% 337980
#% 676664
#% 1250387
#% 1289397
#! General Game Playing (GGP) is the art of designing programs that are capable of playing previously unknown games of a wide variety by being told nothing but the rules of the game. This is in contrast to traditional computer game players like Deep Blue, which are designed for a particular game and can't adapt automatically to modifications of the rules, let alone play completely different games. General Game Playing is intended to foster the development of integrated cognitive information processing technology. In this article we present an approach to General Game Playing using a novel way of automatically constructing a position evaluation function from a formal game description. Our system is being tested with a wide range of different games. Most notably, it is the winner of the AAAI GGP Competition 2006.

#index 1269861
#* Combining multiple heuristics online
#@ Matthew Streeter;Daniel Golovin;Stephen F. Smith
#t 2007
#c 10
#% 329487
#% 420713
#% 781735
#% 907677
#% 1269862
#% 1669556
#% 1815581
#! We present black-box techniques for learning how to interleave the execution of multiple heuristics in order to improve average-case performance. In our model, a user is given a set of heuristics whose only observable behavior is their running time. Each heuristic can compute a solution to any problem instance, but its running time varies across instances. The user solves each instance by interleaving runs of the heuristics according to a task-switching schedule. We present (i) exact and approximation algorithms for computing an optimal task-switching schedule offline, (ii) sample complexity bounds for learning a task-switching schedule from training data, and (iii) a no-regret strategy for selecting task-switching schedules online. We demonstrate the power of our results using data from recent solver competitions. We outline how to extend our results to the case in which the heuristics are randomized, and the user may periodically restart each heuristic with a fresh random seed.

#index 1269862
#* Restart schedules for ensembles of problem instances
#@ Matthew Streeter;Daniel Golovin;Stephen F. Smith
#t 2007
#c 10
#% 155827
#% 266200
#% 416988
#% 535321
#% 578756
#% 907677
#% 1269861
#% 1274880
#% 1815581
#! The mean running time of a Las Vegas algorithm can often be dramatically reduced by periodically restarting it with a fresh random seed. The optimal restart schedule depends on the Las Vegas algorithm's run length distribution, which in general is not known in advance and may differ across problem instances. We consider the problem of selecting a single restart schedule to use in solving each instance in a set of instances. We present offline algorithms for computing an (approximately) optimal restart schedule given knowledge of each instance's run length distribution, generalization bounds for learning a restart schedule from training data, and online algorithms for selecting a restart schedule adaptively as new problem instances are encountered.

#index 1269863
#* Inconsistent heuristics
#@ Uzi Zahavi;Ariel Felner;Jonathan Schaeffer;Nathan Sturtevant
#t 2007
#c 10
#% 208
#% 174161
#% 337986
#% 348576
#% 529180
#% 939032
#% 1250219
#% 1250327
#% 1289367
#% 1478838
#! In the field of heuristic search it is well-known that improving the quality of an admissible heuristic can significantly decrease the search effort required to find an optimal solution. Existing literature often assumes that admissible heuristics are consistent, implying that consistency is a desirable attribute. To the contrary, this paper shows that an inconsistent heuristic can be preferable to a consistent heuristic. Theoretical and empirical results show that, in many cases, inconsistency can be used to achieve large performance improvements.

#index 1269864
#* Parallel structured duplicate detection
#@ Rong Zhou;Eric A. Hansen
#t 2007
#c 10
#% 167926
#% 214065
#% 443256
#% 873948
#% 1250226
#% 1250321
#% 1250328
#% 1269579
#% 1269582
#% 1275127
#% 1275141
#% 1712385
#! We describe a novel approach to parallelizing graph search using structured duplicate detection. Structured duplicate detection was originally developed as an approach to external-memory graph search that reduces the number of expensive disk I/O operations needed to check stored nodes for duplicates, by using an abstraction of the search graph to localize memory references. In this paper, we show that this approach can also be used to reduce the number of slow synchronization operations needed in parallel graph search. In addition, we describe several techniques for integrating parallel and external-memory graph search in an efficient way. We demonstrate the effectiveness of these techniques in a graph-search algorithm for domain-independent STRIPS planning.

#index 1269865
#* VOILA: efficient feature-value acquisition for classification
#@ Mustafa Bilgic;Lise Getoor
#t 2007
#c 10
#% 44876
#% 443640
#% 447606
#% 785338
#% 788039
#% 829982
#% 876046
#% 1272369
#! We address the problem of efficient feature-value acquisition for classification in domains in which there are varying costs associated with both feature acquisition and misclassification. The objective is to minimize the sum of the information acquisition cost and misclassification cost. Any decision theoretic strategy tackling this problem needs to compute value of information for sets of features. Having calculated this information, different acquisition strategies are possible (acquiring one feature at time, acquiring features in sets, etc.). However, because the value of information calculation for arbitrary subsets of features is computationally intractable, most traditional approaches have been greedy, computing values of features one at a time. We make the problem of value of information calculation tractable in practice by introducing a novel data structure called the Value of Information Lattice (VOILA). VOILA exploits dependencies between missing features and makes sharing of information value computations between different feature subsets possible. To the best of our knowledge, performance differences between greedy acquisition, acquiring features in sets, and a mixed strategy have not been investigated empirically in the past, due to inherit intractability of the problem. With the help of VOILA, we are able to evaluate these strategies on five real world datasets under various cost assumptions. We show that VOILA reduces computation time dramatically. We also show that the mixed strategy outperforms both greedy acquisition and acquisition in sets.

#index 1269866
#* Computing optimal subsets
#@ Maxim Binshtok;Ronen I. Brafman;Solomon E. Shimony;Ajay Martin;Craig Boutilier
#t 2007
#c 10
#% 419951
#% 1250331
#% 1269456
#% 1272026
#% 1272103
#% 1650628
#! Various tasks in decision making and decision support require selecting a preferred subset of items from a given set of feasible items. Recent work in this area considered methods for specifying such preferences based on the attribute values of individual elements within the set. Of these, the approach of (Brafman et al. 2006) appears to be the most general. In this paper, we consider the problem of computing an optimal subset given such a specification. The problem is shown to be NP-hard in the general case, necessitating heuristic search methods. We consider two algorithm classes for this problem: direct set construction, and implicit enumeration as solutions to appropriate CSPs. New algorithms are presented in each class and compared empirically against previous results.

#index 1269867
#* Indefinite-horizon POMDPs with action-based termination
#@ Eric A. Hansen
#t 2007
#c 10
#% 22348
#% 105866
#% 203169
#% 252183
#% 408715
#% 578692
#% 655321
#% 788053
#% 869413
#% 1279358
#% 1290038
#% 1650588
#! For decision-theoretic planning problems with an indefinite horizon, plan execution terminates after a finite number of steps with probability one, but the number of steps until termination (i.e., the horizon) is uncertain and unbounded. In the traditional approach to modeling such problems, called a stochastic shortest-path problem, plan execution terminates when a particular state is reached, typically a goal state. We consider a model in which plan execution terminates when a stopping action is taken. We show that an action-based model of termination has several advantages for partially observable planning problems. It does not require a goal state to be fully observable; it does not require achievement of a goal state to be guaranteed; and it allows a proper policy to be found more easily. This framework allows many partially observable planning problems to be modeled in a more realistic way that does not require an artificial discount factor.

#index 1269868
#* Point-based policy iteration
#@ Shihao Ji;Ronald Parr;Hui Li;Xuejun Liao;Lawrence Carin
#t 2007
#c 10
#% 252183
#% 788098
#% 842579
#% 1272075
#% 1279358
#% 1650313
#% 1650588
#% 1650702
#! We describe a point-based policy iteration (PBPI) algorithm for infinite-horizon POMDPs. PBPI replaces the exact policy improvement step of Hansen's policy iteration with point-based value iteration (PBVI). Despite being an approximate algorithm, PBPI is monotonic: At each iteration before convergence, PBPI produces a policy for which the values increase for at least one of a finite set of initial belief states, and decrease for none of these states. In contrast, PBVI cannot guarantee monotonic improvement of the value function or the policy. In practice PBPI generally needs a lower density of point coverage in the simplex and tends to produce superior policies with less computation. Experiments on several benchmark problems (up to 12,545 states) demonstrate the scalability and robustness of the PBPI algorithm.

#index 1269869
#* Thresholded rewards: acting optimally in timed, zero-sum games
#@ Colin McMillen;Manuela Veloso
#t 2007
#c 10
#% 203604
#% 363744
#% 384911
#% 425074
#% 708571
#% 1650297
#! In timed, zero-sum games, the goal is to maximize the probability of winning, which is not necessarily the same as maximizing our expected reward. We consider cumulative intermediate reward to be the difference between our score and our opponent's score; the "true" reward of a win, loss, or tie is determined at the end of a game by applying a threshold function to the cumulative intermediate reward. We introduce thresholded-rewards problems to capture this dependency of the final reward outcome on the cumulative intermediate reward. Thresholded-rewards problems reflect different real-world stochastic planning domains, especially zero-sum games, in which time and score need to be considered. We investigate the application of thresholded rewards to finite-horizon Markov Decision Processes (MDPs). In general, the optimal policy for a thresholded-rewards MDP will be non-stationary, depending on the number of time steps remaining and the cumulative intermediate reward. We introduce an efficient value iteration algorithm that solves thresholded-rewards MDPs exactly, but with running time quadratic on the number of states in the MDP and the length of the time horizon. We investigate a number of heuristic-based techniques that efficiently find approximate solutions for MDPs with large state spaces or long time horizons.

#index 1269870
#* Macroscopic models of clique tree growth for Bayesian networks
#@ Ole J. Mengshoel
#t 2007
#c 10
#% 31482
#% 36814
#% 61895
#% 80008
#% 322912
#% 329486
#% 739899
#% 939033
#! In clique tree clustering, inference consists of propagation in a clique tree compiled from a Bayesian network. In this paper, we develop an analytical approach to characterizing clique tree growth as a function of increasing Bayesian network connectedness, specifically: (i) the expected number of moral edges in their moral graphs or (ii) the ratio of the number of non-root nodes to the number of root nodes. In experiments, we systematically increase the connectivity of bipartite Bayesian networks, and find that clique tree size growth is well-approximated by Gompertz growth curves. This research improves the understanding of the scaling behavior of clique tree clustering, provides a foundation for benchmarking and developing improved BN inference algorithms, and presents an aid for analytical trade-off studies of tree clustering using growth curves.

#index 1269871
#* Sampling with memoization
#@ Avi Pfeffer
#t 2007
#c 10
#% 363592
#% 495931
#% 529159
#% 798509
#! Memoization is a fundamental technique in computer science, providing the basis for dynamic programming. This paper explores using memoization to improve the performance of rejection sampling algorithms. It is shown that reusing values produced in previous samples and stored in a cache is beneficial. The paper goes on to explore the idea of recursive memoization, in which values are aggressively reused from the cache even in the process of computing a value to store in the cache. This leads to the values in the cache becoming dependent on each other, and therefore produces a biased sampler. However in practice this seems to be quite beneficial. Furthermore, we show that the error in the cache tends to zero in the long run. We demonstrate the usefulness of memoized sampling in a duplicate bridge simulation, and in experiments with probabilistic grammars.

#index 1269872
#* Logical generative models for probabilistic reasoning about existence, roles and identity
#@ David Poole
#t 2007
#c 10
#% 90371
#% 722914
#% 1289565
#% 1478789
#% 1705613
#! In probabilistic reasoning, the problems of existence and identity are important to many different queries; for example, the probability that something that fits some description exists, the probability that some description refers to an object you know about or to a new object, or the probability that an object fulfils some role. Many interesting queries reduce to reasoning about the role of objects. Being able to talk about the existence of parts and sub-parts and the relationships between these parts, allows for probability distributions over complex descriptions. Rather than trying to define a new language, this paper shows how the integration of multiple objects, ontologies and roles can be achieved cleanly. This solves two main problems: reasoning about existence and identity while preserving the clarity principle that specifies that probabilities must be over well defined propositions, and the correspondence problem that means that we don't need to search over all possible correspondences between objects said to exist and things in the world.

#index 1269873
#* Learning graphical model structure using L1-regularization paths
#@ Mark Schmidt;Alexandru Niculescu-Mizil;Kevin Murphy
#t 2007
#c 10
#% 197387
#% 212700
#% 297171
#% 722754
#% 875956
#% 893460
#% 1269485
#% 1650289
#! Sparsity-promoting L1-regularization has recently been succesfully used to learn the structure of undirected graphical models. In this paper, we apply this technique to learn the structure of directed graphical models. Specifically, we make three contributions. First, we show how the decomposability of the MDL score, plus the ability to quickly compute entire regularization paths, allows us to efficiently pick the optimal regularization parameter on a per-node basis. Second, we show how to use L1 variable selection to select the Markov blanket, before a DAG search stage. Finally, we show how L1 variable selection can be used inside of an order search algorithm. The effectiveness of these L1-based approaches are compared to current state of the art methods on 10 datasets.

#index 1269874
#* On the identification of a class of linear models
#@ Jin Tian
#t 2007
#c 10
#% 297171
#% 527989
#% 578735
#% 1250135
#% 1269413
#% 1650356
#! This paper deals with the problem of identifying direct causal effects in recursive linear structural equation models. The paper provides a procedure for solving the identification problem in a special class of models.

#index 1269875
#* Scaling up: solving POMDPs through value based clustering
#@ Yan Virin;Guy Shani;Solomon Eyal Shimony;Ronen Brafman
#t 2007
#c 10
#% 92301
#% 179940
#% 646971
#% 788098
#% 829022
#% 1272075
#% 1289243
#% 1650702
#% 1665157
#% 1713177
#! Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.

#index 1269876
#* Generalized evidence pre-propagated importance sampling for hybrid Bayesian networks
#@ Changhe Yuan;Marek J. Druzdzel
#t 2007
#c 10
#% 44876
#% 424851
#% 527664
#% 527691
#% 528309
#% 1271825
#% 1562580
#% 1650302
#% 1650318
#% 1650732
#% 1673053
#% 1705553
#% 1799876
#! In this paper, we first provide a new theoretical understanding of the Evidence Pre-propagated Importance Sampling algorithm (EPIS-BN) (Yuan & Druzdzel 2003; 2006b) and show that its importance function minimizes the KL-divergence between the function itself and the exact posterior probability distribution in Polytrees. We then generalize the method to deal with inference in general hybrid Bayesian networks consisting of deterministic equations and arbitrary probability distributions. Using a novel technique called soft arc reversal, the new algorithm can also handle evidential reasoning with observed deterministic variables.

#index 1269877
#* A semantic importing approach to knowledge reuse from multiple ontologies
#@ Jie Bao;Giora Slutzki;Vasant Honavar
#t 2007
#c 10
#% 763751
#% 961682
#% 1274799
#% 1274824
#% 1289422
#% 1696289
#% 1713498
#! We present the syntax and semantics of a modular ontology language SHOIQP to support context-specific reuse of knowledge from multiple ontologies. A SHOIQP ontology consists of multiple ontology modules (each of which can be viewed as a SHOIQ ontology) and concept, role and nominal names can be shared by "importing" relations among modules. SHOIQP supports contextualized interpretation, i.e., interpretation from the point of view of a specific package. We establish the necessary and sufficient constraints on domain relations (i.e., the relations between individuals in different local domains) to preserve the satisfiability of concept formulae, monotonicity of inference, and transitive reuse of knowledge.

#index 1269878
#* Modeling contextual factors of click rates
#@ Hila Becker;Christopher Meek;David Maxwell Chickering
#t 2007
#c 10
#% 577224
#% 731615
#% 783482
#% 805200
#% 818206
#% 818221
#% 823348
#% 879565
#% 879567
#% 1250379
#! In this paper, we develop and evaluate several probabilistic models of user click-through behavior that are appropriate for modeling the click-through rates of items that are presented to the user in a list. Potential applications include modeling the click-through rates of search results from a search engine, items ranked by a recommendation system, and search advertisements returned by a search engine. Our models capture contextual factors related to the presentation as well as the underlying relevance or quality of the item. We focus on two types of contextual factors for a given item; the positional context of the item and the quality of the other results. We evaluate our models on a search advertising dataset from Microsoft's Live search engine and demonstrate that modeling contextual factors improves the accuracy of click-through models.

#index 1269879
#* Harvesting relations from the web: quantifiying the impact of filtering functions
#@ Sebastian Blohm;Philipp Cimiano;Egon Stemle
#t 2007
#c 10
#% 301241
#% 504443
#% 754104
#% 756964
#% 815868
#% 830520
#% 939515
#% 1289516
#! Several bootstrapping-based relation extraction algorithms working on large corpora or on the Web have been presented in the literature. A crucial issue for such algorithms is to avoid the introduction of too much noise into further iterations. Typically, this is achieved by applying appropriate pattern and tuple evaluation measures, henceforth called filtering functions, thereby selecting only the most promising patterns and tuples. In this paper, we systematically compare different filtering functions proposed across the literature. Although we also discuss our own implementation of a pattern learning algorithm, the main contribution of the paper is actually the extensive comparison and evaluation of the different filtering functions proposed in the literature with respect to seven datasets. Our results indicate that some of the commonly used filters do not outperform a trivial baseline filter in a statistically significant manner.

#index 1269880
#* KA-CAPTCHA: an opportunity for knowledge acquisition on the web
#@ Bruno Norberto Da Silva;Ana Cristina;Bicharra Garcia
#t 2007
#c 10
#% 198055
#% 220706
#% 268079
#% 379240
#% 509695
#% 723391
#% 801441
#% 860115
#% 896156
#% 1269447
#% 1562518
#! Any Web user is a potential knowledge contributor, but it remains a challenge to make them devote their time contributing to some purpose. In order to align individual with social interests, we selected the CAPTCHA Web resource protection application to embed knowledge elicitation within the users' main task of accessing a Web resource. Consequently, unlike previous knowledge acquisition approaches, no extra effort is expected from users since they are already willing to use a CAPTCHA to perform some particular task. We present an application where we extract pictorial knowledge from Web users, and experiments suggest that our approach enables knowledge acquisition while still satisfying CAPTCHA's security requirements.

#index 1269881
#* Representing and reasoning about commitments in business processes
#@ Nirmit Desai;Amit K. Chopra;Munindar P. Singh
#t 2007
#c 10
#% 314735
#% 346653
#% 643144
#% 763743
#% 799801
#% 846069
#% 852520
#% 890424
#% 912306
#% 987503
#% 1024804
#% 1269824
#! A variety of business relationships in open settings can be understood in terms of the creation and manipulation of commitments among the participants. These include B2C and B2B contracts and processes, as realized via Web services and other such technologies. Business protocols, an interaction-oriented approach for modeling business processes, are formulated in terms of the commitments. Commitments can support other forms of semantic service composition as well. This paper shows how to represent and reason about commitments in a general manner. Unlike previous formalizations, the proposed formalization accommodates complex and nested commitment conditions, and concurrent commitment operations. In this manner, a rich variety of open business scenarios are enabled.

#index 1269882
#* Topic segmentation algorithms for text summarization and passage retrieval: an exhaustive evaluation
#@ Gaël Dias;Elsa Alves;José Gabriel Pereira Lopes
#t 2007
#c 10
#% 340992
#% 448786
#% 504890
#% 577301
#% 607945
#% 643015
#% 740329
#% 742204
#% 748482
#% 748583
#% 748631
#% 815186
#! In order to solve problems of reliability of systems based on lexical repetition and problems of adaptability of language-dependent systems, we present a context-based topic segmentation system based on a new informative similarity measure based on word co-occurrence. In particular, our evaluation with the state-of-the-art in the domain i.e. the c99 and the TextTiling algorithms shows improved results both with and without the identification of multiword units.

#index 1269883
#* The impact of time on the accuracy of sentiment classifiers created from a web log corpus
#@ Kathleen T. Durant;Michael D. Smith
#t 2007
#c 10
#% 180254
#% 272995
#% 290482
#% 577355
#% 723399
#% 746885
#% 769892
#% 854646
#% 938686
#% 938687
#% 1277969
#% 1290045
#% 1409618
#! We investigate the impact of time on the predictability of sentiment classification research for models created from web logs. We show that sentiment classifiers are time dependent and through a series of methodical experiments quantify the size of the dependence. In particular, we measure the accuracies of 25 different time-specific sentiment classifiers on 24 different testing timeframes. We use the Naive Bayes induction technique and the holdout validation technique using equal-sized but separate training and testing data sets. We conducted over 600 experiments and organize our results by the size of the interval (in months) between the training and testing timeframes. Our findings show a significant decrease in accuracy as this interval grows. Using a paired t-test we show classifiers trained on future data and tested on past data significantly outperform classifiers trained on past data and tested on future data. These findings are for a topic-specific corpus created from political web log posts originating from 160 different web logs. We then define concepts that classify months as examplar, infrequent thread, frequent thread or outlier; this classification reveals knowledge on the topic's evolution and the utility of the month's data for the timeframe.

#index 1269884
#* A distributed constraint optimization solution to the P2P video streaming problem
#@ Theodore Elhourani;Nathan Denny;Michael Marefat
#t 2007
#c 10
#% 318291
#% 431523
#% 577332
#% 963648
#% 1279416
#% 1849766
#! The future success of application layer video multicast depends on the availability of video stream distribution methods that can scale in the number of stream senders and receivers. Previous work on the problem of application layer video streaming has not effectively addressed scalability in the number of receivers and senders. Therefore, new solutions that are amenable to analysis and can achieve scalable P2P video streaming are needed. In this work we propose the use of automated negotiation algorithms to construct video streaming trees at the application layer. We show that automated negotiation can effectively solve the problem of distributing a video stream to a large number of receivers.

#index 1269885
#* Analyzing reading behavior by blog mining
#@ Tadanobu Furukawa;Mitsuru Ishizuka;Yutaka Matsuo;Ikki Ohmukai;Koki Uchiyama
#t 2007
#c 10
#% 136350
#% 350002
#% 730089
#% 786841
#% 791728
#% 832271
#% 853532
#% 868089
#! This paper presents a study of the various aspects of blog reading behavior. The analyzed data are obtained from a Japanese weblog hosting service, Doblog. Four kinds of social networks are generated and analyzed: citation, comment, trackback, and blogroll networks. In addition, the user log data are used to identify readership relations among bloggers. After analysis of more than 50,000 users for about two years, we reveal some interactions between social relations and readership relations. We first show that bloggers read other weblogs on a regular basis (50% of weblogs that are read at least three times are read every five times a user logs in). We call this relation a regular reading relation (RR relation). Then, prediction of RR relations is done using features from the four kinds of social networks. Lastly, information diffusion on RR relations is analyzed and characterized. Results of this study show that the blogs in RR relations have an important role in bloggers' activities. We find the features which have a correlation with RR relations.

#index 1269886
#* PhotoSlap: a multi-player online game for semantic annotation
#@ Chien-Ju Ho;Tsung-Hsiang Chang;Jane Yung-Jen Hsu
#t 2007
#c 10
#% 457912
#% 642989
#% 729344
#% 751818
#% 860013
#% 905268
#! Multimedia content presents special challenges for the search engines, and could benefit from semantic annotation of images. Unfortunately, manual labeling is too tedious and time-consuming for humans, whereas automatic image annotation is too difficult for the computers. In this paper, we explore the power of human computation by designing a multi-player online game, PhotoSlap, to achieve the task of annotating metadata for a collection of digital photos. PhotoSlap engages users in an interactive game that capitalizes on human ability in deciphering quickly whether the same person shows up in two consecutive images presented by the computer. The game mechanism supports the objection and trap actions to encourage truthful input from the players. This research extends human computation research in two aspects: game-theoretic design principles and quantitative evaluation metrics. In particular, PhotoSlap can be shown to reach subgame perfect equilibrium with the target strategy when players are rational and without collusion. Experiments involving four focus groups have been conducted, and the preliminary results demonstrated the game to be fun and effective in annotating people metadata for photo collections.

#index 1269887
#* Mobile service for reputation extraction from weblogs: public experiment and evaluation
#@ Takahiro Kawamura;Shinichi Nagano;Masumi Inaba;Yumiko Mizoguchi
#t 2007
#c 10
#% 577355
#% 769424
#% 815915
#% 1275196
#% 1677199
#% 1719691
#% 1721323
#! In this paper, we introduce a mobile service that extracts reputations of a product from weblogs by cellular phones during shopping. If the user takes a photo of a product barcode on the package with a cellular phone camera, Ubiquitous Metadata Scouter first gets the product metadata (name, manufacturer, etc.) from the internet and collects blogs that review the product. Also, it analyzes the blog contents with NLP techniques and ontologies. Then, it indicates the overall reputation (positive or negative), and other related products that are the subject of much discussion in the blogs. This paper illustrates each function of this service and a public experiment and evaluation at a real consumer electronics store and book-store in Tokyo in March 2006.

#index 1269888
#* Extracting influential nodes for information diffusion on a social network
#@ Masahiro Kimura;Kazumi Saito;Ryohei Nakano
#t 2007
#c 10
#% 342596
#% 577217
#% 729923
#% 740301
#% 754107
#% 868469
#% 1289476
#% 1838362
#! We consider the combinatorial optimization problem of finding the most influential nodes on a large-scale social network for two widely-used fundamental stochastic diffusion models. It was shown that a natural greedy strategy can give a good approximate solution to this optimization problem. However, a conventional method under the greedy algorithm needs a large amount of computation, since it estimates the marginal gains for the expected number of nodes influenced by a set of nodes by simulating the random process of each model many times. In this paper, we propose a method of efficiently estimating all those quantities on the basis of bond percolation and graph theory, and apply it to approximately solving the optimization problem under the greedy algorithm. Using real-world large-scale networks including blog networks, we experimentally demonstrate that the proposed method can outperform the conventional method, and achieve a large reduction in computational cost.

#index 1269889
#* SUNNY: a new algorithm for trust inference in social networks using probabilistic confidence models
#@ Ugur Kuter;Jennifer Golbeck
#t 2007
#c 10
#% 44876
#% 174161
#% 655322
#% 724539
#% 842605
#% 1250369
#! In many computing systems, information is produced and processed by many people. Knowing how much a user trusts a source can be very useful for aggregating, filtering, and ordering of information. Furthermore, if trust is used to support decision making, it is important to have an accurate estimate of trust when it is not directly available, as well as a measure of confidence in that estimate. This paper describes a new approach that gives an explicit probabilistic interpretation for confidence in social networks. We describe SUNNY, a new trust inference algorithm that uses a probabilistic sampling technique to estimate our confidence in the trust information from some designated sources. SUNNY computes an estimate of trust based on only those information sources with high confidence estimates. In our experiments, SUNNY produced more accurate trust estimates than the well known trust inference algorithm TIDALTRUST (Golbeck 2005), demonstrating its effectiveness.

#index 1269890
#* Making the difference in semantic web service composition
#@ Freddy Lécue;Alexandre Delteil
#t 2007
#c 10
#% 174161
#% 519428
#% 577335
#% 665859
#% 719298
#% 799157
#% 1374383
#% 1414315
#% 1696311
#! Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.

#index 1269891
#* A planning approach for message-oriented semantic web service composition
#@ Zhen Liu;Anand Ranganathan;Anton Riabov
#t 2007
#c 10
#% 348131
#% 577305
#% 1696311
#% 1713480
#! In this paper, we consider the problem of composing a set of web services, where the requirements are specified in terms of the input and output messages of the composite workflow. We propose a semantic model of messages using RDF graphs that encode OWL ABox assertions. We also propose a model of web service operations where the input message requirements and output message characteristics are modeled using RDF graph patterns. We formulate the message-oriented semantic web service composition problem and show how it can be translated into a planning problem. There are, however, significant challenges in scalably doing planning in this domain, especially since DL reasoning may be performed to check if an operation can be given a certain input message. We propose a two-phase planning algorithm that incorporates DLP reasoning and evaluate the performance of this planning algorithm.

#index 1269892
#* Robust estimation of Google counts for social network extraction
#@ Yutaka Matsuo;Hironori Tomobe;Takuichi Nishimura
#t 2007
#c 10
#% 268114
#% 279755
#% 735133
#% 754104
#% 756964
#% 854664
#% 869499
#% 869502
#% 956570
#% 1077150
#% 1223300
#% 1289532
#% 1374378
#% 1655418
#% 1696335
#! Various studies within NLP and Semantic Web use the so-called Google count, which is the hit count on a query returned by a search engine (not only Google). However, sometimes the Google count is unreliable, especially when the count is large, or when advanced operators such as OR and NOT are used. In this paper, we propose a novel algorithm that estimates the Google count robustly. It (i) uses the co-occurrence of terms as evidence to estimate the occurrence of a given word, and (ii) integrates multiple evidence for robust estimation. We evaluated our algorithm for more than 2000 queries on three datasets using Google, Yahoo! and MSN search engine. Our algorithm also provides estimate counts for any classifier that judges a web page as positive or negative. Consequently, we can estimate the number of documents with included references of a particular person (among namesakes) on the entire web.

#index 1269893
#* Unsupervised shilling detection for collaborative filtering
#@ Bhaskar Mehta
#t 2007
#c 10
#% 734592
#% 754097
#% 783438
#% 1250376
#! Collaborative Filtering systems are essentially social systems which base their recommendation on the judgment of a large number of people. However, like other social systems, they are also vulnerable to manipulation. Lies and Propaganda may be spread by malicious users who may have an interest in promoting an item, or downplaying the popularity of another one. By doing this systematically, with either multiple identities, or by involving more people, malicious shilling user profiles can be injected into a collaborative recommender system which can significantly affect the robustness of a recommender system. While current detection algorithms are able to use certain characteristics of shilling profiles to detect them, they suffer from low precision, and require a large amount of training data. The aim of this work is to explore simpler unsupervised alternatives which exploit the nature of shilling profiles, and can be easily plugged into collaborative filtering framework to add robustness. Two statistical methods are developed and experimentally shown to provide high accuracy in shilling attack detection.

#index 1269894
#* Repairing ontology mappings
#@ C. Meilicke;H. Stuckenschmidt;Andrei Tamilin
#t 2007
#c 10
#% 21137
#% 665856
#% 924747
#% 1289442
#% 1702410
#% 1702419
#! Automatically discovering semantic relations between ontologies is an important task with respect to overcoming semantic heterogeneity on the semantic web. Existing ontology matching systems, however, often produce erroneous mappings. In this paper, we address the problem of errors in mappings by proposing a completely automatic debugging method for ontology mappings. The method uses logical reasoning to discover and repair logical inconsistencies caused by erroneous mappings. We describe the debugging method and report experiments on mappings submitted to the ontology alignment evaluation challenge that show that the proposed method actually improves mappings created by different matching systems without any human intervention.

#index 1269895
#* Relation extraction from wikipedia using subtree mining
#@ Dat P. T. Nguyen;Yutaka Matsuo;Mitsuru Ishizuka
#t 2007
#c 10
#% 269217
#% 301241
#% 504443
#% 577218
#% 740995
#% 815868
#% 817563
#% 818253
#% 869521
#% 938706
#% 940028
#% 1250362
#% 1250378
#% 1250381
#% 1860941
#! The exponential growth and reliability of Wikipedia have made it a promising data source for intelligent systems. The first challenge of Wikipedia is to make the encyclopedia machine-processable. In this study, we address the problem of extracting relations among entities from Wikipedia's English articles, which in turn can serve for intelligent systems to satisfy users' information needs. Our proposed method first anchors the appearance of entities in Wikipedia articles using some heuristic rules that are supported by their encyclopedic style. Therefore, it uses neither the Named Entity Recognizer (NER) nor the Coreference Resolution tool, which are sources of errors for relation extraction. It then classifies the relationships among entity pairs using SVM with features extracted from the web structure and subtrees mined from the syntactic structure of text. The innovations behind our work are the following: a) our method makes use of Wikipedia characteristics for entity allocation and entity classification, which are essential for relation extraction; b) our algorithm extracts a core tree, which accurately reflects a relationship between a given entity pair, and subsequently identifies key features with respect to the relationship from the core tree. We demonstrate the effectiveness of our approach through evaluation of manually annotated data from actual Wikipedia articles.

#index 1269896
#* From whence does your authority come?: utilizing community relevance in ranking
#@ Lan Nie;Brian D. Davison;Baoning Wu
#t 2007
#c 10
#% 281214
#% 290830
#% 309095
#% 309868
#% 310514
#% 340932
#% 348173
#% 438136
#% 766462
#% 800183
#% 869485
#% 879576
#% 880399
#! A web page may be relevant to multiple topics; even when nominally on a single topic, the page may attract attention (and thus links) from multiple communities. Instead of indiscriminately summing the authority provided by all pages, we decompose a web page into separate subnodes with respect to each community pointing to it. Utilizing the relevance of such communities allows us to better model the semantic structure of the Web, leading to better estimates of authority for a given query. We apply a total of eighty queries over two real-world datasets to demonstrate that the use of community decomposition can consistently and significantly improve upon Page-Rank's top-ten results.

#index 1269897
#* Finding related pages using Green measures: an illustration with Wikipedia
#@ Yann Ollivier;Pierre Senellart
#t 2007
#c 10
#% 268079
#% 281209
#% 290830
#% 406493
#% 641979
#% 838486
#% 868096
#! We introduce a new method for finding nodes semantically related to a given node in a hyperlinked graph: the Green method, based on a classical Markov chain tool. It is generic, adjustment-free and easy to implement. We test it in the case of the hyperlink structure of the English version of Wikipedia, the on-line encyclopedia. We present an extensive comparative study of the performance of our method versus several other classical methods in the case of Wikipedia. The Green method is found to have both the best average results and the best robustness.

#index 1269898
#* Approximating OWL-DL ontologies
#@ Jeff Z. Pan;Edward Thomas
#t 2007
#c 10
#% 184793
#% 204396
#% 460912
#% 561740
#% 665856
#% 1269453
#% 1374374
#% 1655409
#% 1666169
#% 1684017
#% 1696306
#% 1702416
#! Efficient query answering over ontologies is one of the most useful and important services to support Semantic Web applications. Approximation has been identified as a potential way to reduce the complexity of query answering over OWL DL ontologies. Existing approaches are mainly based on syntactic approximation of ontological axioms and queries. In this paper, we propose to recast the idea of knowledge compilation into approximating OWL DL ontologies with DL-Lite ontologies, against which query answering has only polynomial data complexity. We identify a useful category of queries for which our approach guarantees also completeness. Furthermore, this paper reports on the implementation of our approach in the ONTOSEARCH2 system and preliminary, but encouraging, benchmark results which compare ONTOSEARCH2's response times on a number of queries with those of existing ontology reasoning systems.

#index 1269899
#* Deriving a large scale taxonomy from Wikipedia
#@ Simone Paolo Ponzetto;Michael Strube
#t 2007
#c 10
#% 78171
#% 268079
#% 325502
#% 405391
#% 708948
#% 723406
#% 741083
#% 741900
#% 742092
#% 748600
#% 756964
#% 786511
#% 786515
#% 786523
#% 848150
#% 853701
#% 896030
#% 896039
#% 939376
#% 939601
#% 956564
#% 1250381
#% 1250507
#% 1269587
#% 1275285
#% 1289530
#! We take the category system in Wikipedia as a conceptual network. We label the semantic relations between categories using methods based on connectivity in the network and lexicosyntactic matching. As a result we are able to derive a large scale taxonomy containing a large amount of subsumption, i.e. isa, relations. We evaluate the quality of the created resource by comparing it with ResearchCyc, one of the largest manually annotated ontologies, as well as computing semantic similarity between words in benchmarking datasets.

#index 1269900
#* Towards large scale argumentation support on the semantic web
#@ Iyad Rahwan;Fouad Zablith;Chris Reed
#t 2007
#c 10
#% 580286
#% 735151
#% 786843
#% 869521
#% 910947
#% 1112388
#% 1221657
#! This paper lays theoretical and software foundations for a World Wide Argument Web (WWAW): a large-scale Web of inter-connected arguments posted by individuals to express their opinions in a structured manner. First, we extend the recently proposed Argument Interchange Format (AIF) to express arguments with a structure based on Walton's theory of argumentation schemes. Then, we describe an implementation of this ontology using the RDF Schema language, and demonstrate how our ontology enables the representation of networks of arguments on the Semantic Web. Finally, we present a pilot Semantic Web-based system, ArgDF, through which users can create arguments using different argumentation schemes and can query arguments using a Semantic Web query language. Users can also attack or support parts of existing arguments, use existing parts of an argument in the creation of new arguments, or create new argumentation schemes. As such, this initial public-domain tool is intended to seed a variety of future applications for authoring, linking, navigating, searching, and evaluating arguments on the Web.

#index 1269901
#* Provisioning heterogeneous and unreliable providers for service workftows
#@ Sebastian Stein;Nicholas R. Jennings;Terry R. Payne
#t 2007
#c 10
#% 326788
#% 433916
#% 577344
#% 793341
#% 800087
#% 856790
#% 1223247
#% 1396227
#! Service-oriented technologies enable software agents to dynamically discover and provision remote services for their workflows. Current work has typically assumed these services to be reliable and deterministic, but this is unrealistic in open systems, such as the Web, where they are offered by autonomous agents and are, therefore, inherently unreliable. To address this potential unreliability (in particular, uncertain service durations and failures), we consider the provisioning of abstract workflows, where many heterogeneous providers offer services at differing levels of quality. More specifically, we show that service provisioning is NP-hard, and then devise two heuristic strategies that use service redundancy in a flexible manner to address uncertainty and failure. In empirical experiments, we show that these heuristic strategies can achieve significant improvements over standard approaches in a wide range of environments.

#index 1269902
#* Partial matchmaking using approximate subsumption
#@ Heiner Stuckenschmidt
#t 2007
#c 10
#% 184793
#% 564572
#% 577334
#% 758324
#% 987502
#% 1702406
#% 1702416
#% 1718489
#! Description Logics, and in particular the web ontology language OWL has been proposed as an appropriate basis for computing matches between structured objects for the sake of information integration and service discovery. A drawback of the direct use of subsumption as a matching criterion is the inability to compute partial matches and qualify the degree of mismatch. In this paper, we describe a method for overcoming these problems that is based on approximate logical reasoning. In particular, we approximate the subsumption relation by defining the notion of subsumption with respect to a certain subset of the concept and relation names. We present the formal semantics of this relation, describe a sound and complete algorithm for computing approximate subsumption and discuss its application to matching tasks.

#index 1269903
#* GRIN: a graph based RDF index
#@ Octavian Udrea;Andrea Pugliese;V. S. Subrahmanian
#t 2007
#c 10
#% 519567
#% 772884
#% 1655412
#% 1655429
#% 1667784
#! RDF ("Resource Description Framework") is now a widely used World Wide Web Consortium standard. However, methods to index large volumes of RDF data are still in their infancy. In this paper, we focus on providing a very lightweight indexing mechanism for certain kinds of RDF queries, namely graph-based queries where there is a need to traverse edges in the graph determined by an RDF database. Our approach uses the idea of drawing circles around selected "center" vertices in the graph where the circle would encompass those vertices in the graph that are within a given distance of the "center" vertex. We come up with methods of finding such "center" vertices and identifying the radius of the circles and then leverage this to build an index called GRIN. We compare GRIN with three existing RDF indexex: Jena, Sesame. and RDFBroker. We compared (i) the time to answer graph based queries, (ii) memory needed to store the index, and (iii) the time to build the index. GRIN outperforms Jena, Sesame and RDFBroker on all three measures for graph based queries (for other types of queries, it may be worth building one of these other indexes and using it), at the expense of using a larger amount of memory when answering queries.

#index 1269904
#* Comprehending and generating apt metaphors: a web-driven, case-based approach to figurative language
#@ Tony Veale;Yanfen Hao
#t 2007
#c 10
#% 65345
#% 78338
#% 740330
#% 1273708
#! Examples of figurative language can range from the explicit and the obvious to the implicit and downright enigmatic. Some simpler forms, like simile, often wear their meanings on their sleeve, while more challenging forms, like metaphor, can make cryptic allusions more akin to those of riddles or crossword puzzles. In this paper we argue that because the same concepts and properties are described in either case, a computational agent can learn from the easy cases (explicit similes) how to comprehend and generate the hard cases (nonexplicit metaphors). We demonstrate that the markedness of similes allows for a large case-base of illustrative examples to be easily acquired from the web, and present a system, called Sardonicus, that uses this case-base both to understand property-attribution metaphors and to generate apt metaphors for a given target on demand. In each case, we show how the text of the web is used as a source of tacit knowledge about what categorizations are allowable and what properties are most contextually appropriate. Overall, we demonstrate that by using the web as a primary knowledge source, a system can achieve a robust and scalable competence with metaphor while minimizing the need for handcrafted resources like WordNet.

#index 1269905
#* Reasoning about attribute authenticity in a web environment
#@ Thomas Wölfl
#t 2007
#c 10
#% 7047
#% 33376
#% 107123
#% 174218
#% 380346
#% 381870
#% 382656
#% 507393
#% 669725
#% 1425500
#% 1718040
#! The reliable authentication of user attributes is an important prerequisite for the security of web based applications. Digital certificates are widely used for that purpose. However, practical certification scenarios can be very complex. Each certiticate carries a validity period and can be revoked during this period. Furthermore, the verifying user has to trust the issuers of certificates and revocations. This work presents a formal model which covers these aspects and provides a theoretical foundation for the decision about attribute authenticity even in complex scenarios. The model is based on the event calculus, an AI technique from the field of temporal reasoning. It uses Clark's completion to address the frame problem. An example illustrates the application of the model.

#index 1269906
#* Towards efficient dominant relationship exploration of the product items on the web
#@ Zhenglu Yang;Lin Li;Botao Wang;Masaru Kitsuregawa
#t 2007
#c 10
#% 2115
#% 288976
#% 289148
#% 310515
#% 320187
#% 427199
#% 452641
#% 463903
#% 464996
#% 465167
#% 654480
#% 769910
#% 824671
#% 824672
#% 875025
#% 957747
#% 993954
#! In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the "best" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this paper can be complementary to traditional search engines.

#index 1269907
#* Improving similarity measures for short segments of text
#@ Wen-Tau Yih;Christopher Meek
#t 2007
#c 10
#% 207677
#% 232713
#% 279755
#% 420487
#% 495937
#% 815864
#% 840846
#% 869484
#% 869500
#% 869501
#% 1279276
#% 1392432
#! In this paper we improve previous work on measuring the similarity of short segments of text in two ways. First, we introduce a Web-relevance similarity measure and demonstrate its effectiveness. This measure extends the Web-kernel similarity function introduced by Sahami and Heilman (2006) by using relevance weighted inner-product of term occurrences rather than TF×IDF. Second, we show that one can further improve the accuracy of similarity measures by using a machine learning approach. Our methods outperform other state-of-the-art methods in a general query suggestion task for multiple evaluation metrics.

#index 1269908
#* Design of a mechanism for promoting honesty in E-marketplaces
#@ Jie Zhang;Robin Cohen
#t 2007
#c 10
#% 314935
#% 773504
#% 897330
#% 943777
#% 959133
#! In this paper, we explore the use of the web as an environment for electronic commerce. In particular, we develop a novel mechanism that creates incentives for honesty in electronic marketplaces where human users are represented by buying and selling agents. In our mechanism, buyers model other buyers and select the most trustworthy ones as their neighbors from which they can ask advice about sellers. In addition, however, sellers model the reputation of buyers. Reputable buyers provide fair ratings of sellers, and are likely to be neighbors of many other buyers. Sellers will provide more attractive products to reputable buyers, in order to build their reputation. We discuss how a marketplace operating with our mechanism leads to better profit both for honest buyers and sellers. With honesty encouraged, our work promotes the acceptance of web-based agent-oriented e-commerce by human users.

#index 1269909
#* Temporal and information flow based event detection from social text streams
#@ Qiankun Zhao;Prasenjit Mitra;Bi Chen
#t 2007
#c 10
#% 262042
#% 313959
#% 448843
#% 577360
#% 581658
#% 766484
#% 818246
#% 818332
#% 869516
#% 869604
#% 876007
#% 879628
#% 881050
#% 881523
#% 907491
#% 907492
#% 993965
#% 1250370
#! Recently, social text streams (e.g., blogs, web forums, and emails) have become ubiquitous with the evolution of the web. In some sense, social text streams are sensors of the real world. Often, it is desirable to extract real world events from the social text streams. However, existing event detection research mainly focused only on the stream properties of social text streams but ignored the contextual, temporal, and social information embedded in the streams. In this paper, we propose to detect events from social text streams by exploring the content as well as the temporal, and social dimensions. We define the term event as the information flow between a group of social actors on a specific topic over a certain time period. We represent social text streams as multi-graphs, where each node represents a social actor and each edge represents the information flow between two actors. The content and temporal associations within the flow of information are embedded in the corresponding edge. Events are detected by combining text based clustering, temporal segmentation, and information flow-based graph cuts of the dual graph of the social networks. Experiments conducted with the Enron email dataset and the political blog dataset from Dailykos show the proposed event detection approach outperforms the other alternatives.

#index 1269910
#* Template-independent news extraction based on visual consistency
#@ Shuyi Zheng;Ruihua Song;Ji-Rong Wen
#t 2007
#c 10
#% 235377
#% 271065
#% 275915
#% 397605
#% 480824
#% 629661
#% 654469
#% 754078
#% 754091
#% 754108
#% 779889
#% 805845
#% 807444
#! Wrapper is a traditional method to extract useful information from Web pages. Most previous works rely on the similarity between HTML tag trees and induced template-dependent wrappers. When hundreds of information sources need to be extracted in a specific domain like news, it is costly to generate and maintain the wrappers. In this paper, we propose a novel template-independent news extraction approach to easily identify news articles based on visual consistency. We first represent a page as a visual block tree. Then, by extracting a series of visual features, we can derive a composite visual feature set that is stable in the news domain. Finally, we use a machine learning approach to generate a template-independent wrapper. Experimental results indicate that our approach is effective in extracting news across websites, even from unseen websites. The performance is as high as around 95% in terms of F1-value.

#index 1269911
#* PLOW: a collaborative task learning agent
#@ James Allen;Nathanael Chambers;George Ferguson;Lucian Galescu;Hyuckchul Jung;Mary Swift;William Taysom
#t 2007
#c 10
#% 262191
#% 266227
#% 341661
#% 379208
#% 398947
#% 734963
#% 790462
#% 1250355
#! To be effective, an agent that collaborates with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies: deep natural language understanding, knowledge representation and reasoning, dialogue systems, planning/agent-based systems and machine learning. A formal evaluation shows the approach has great promise.

#index 1269912
#* An architecture for adaptive algorithmic hybrids
#@ Nicholas Cassimatis;Magdalena Bugajska;Scott Dugas;Arthi Murugesan;Paul Bello
#t 2007
#c 10
#% 23011
#% 168280
#% 706924
#% 850430
#% 1018490
#% 1134790
#% 1250389
#! We describe a cognitive architecture for creating more robust intelligent systems by executing hybrids of algorithms based on different computational formalisms. The architecture is motivated by the belief that (1) most existing computational methods often exhibit some of the characteristics desired of intelligent systems at the cost of other desired characteristics and (2) a system exhibiting robust intelligence can be designed by implementing hybrids of these computational methods. The main obstacle to this approach is that the various relevant computational methods are based on data structures and algorithms that are very difficult to integrate into one system. We describe a new method of executing hybrids of algorithms using the focus of attention of multiple modules. This approach has been embodied in the Polyscheme cognitive architecture. Systems based on Polyscheme can integrate reactive robotic controllers, logical and probabilistic inference algorithms, frame-based formalisms and sensor-processing algorithms into one system. Existing applications involve human-robot interaction, heterogeneous information retrieval and natural language understanding. Systems built using Polyscheme demonstrate that algorithmic hybrids implemented using a focus of attention can (1) exhibit more characteristics of intelligence than individual computational methods alone and (2) deal with problems that have formerly been beyond the reach of synthetic computational intelligence.

#index 1269913
#* Learning to sing like a bird: self-supervised acquisition of birdsong
#@ Michael H. Coen
#t 2007
#c 10
#% 296738
#% 370216
#% 570003
#% 635713
#% 980150
#% 1250386
#% 1269506
#% 1289341
#! This paper presents a new framework for self-supervised sensorimotor learning. We demonstrate this framework with a system that learns to mimic a zebra finch, directly modeled on the dynamics of how male fledglings acquire birdsong from their fathers. Our system first listens to the song of an adult finch. By listening to its own initially nascent attempts at mimicry through an articulatory synthesizer, the system organizes motor maps generating its vocalizations. Our approach is founded on the notion of cross-modal clustering, introduced in (Coen 2005, 2006a), and is unusual for its recursive reuse of perceptual mechanisms in developing motor control. In this paper, we outline this framework, present its results on the unsupervised acquisition of birdsong, and discuss other potential applications.

#index 1269914
#* R-CAST: integrating team intelligence for human-centered teamwork
#@ Xiaocong Fan;John Yen
#t 2007
#c 10
#% 23011
#% 189698
#% 215532
#% 241019
#% 823960
#% 843880
#% 855466
#% 890194
#% 890461
#% 1289304
#% 1478733
#! Developing human-centered agent architectures requires the integral consideration of architectural flexibility, teamwork adaptability, and context reasoning capability. With the integration of various forms of team intelligence including shared teamwork process and progress, dynamic context management and infomation dependency reasoning, and recognition-primed collaborative decision mechanism, R-CAST offers a flexible solution to developing cognitive aids for the support of human-centered teamwork in information and knowledge intensive domains. In this paper, we present the key features of R-CAST. As evidence of its applications in complex real-world problems, we give two experimental evaluations of R-CAST as teammates and decision aids of human Command and Control teams.

#index 1269915
#* Integrating natural language, knowledge representation and reasoning, and analogical processing to learn by reading
#@ Kenneth D. Forbus;Christopher Riesbeck;Lawrence Birnbaum;Kevin Livingston;Abhishek Sharma;Leo Ureel
#t 2007
#c 10
#% 65345
#% 296853
#% 529803
#% 822227
#% 830520
#% 939896
#% 1269481
#% 1269587
#! Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.

#index 1269916
#* Towards an integrated robot with multiple cognitive functions
#@ Nick Hawes;Aaron Sloman;Jeremy Wyatt;Michael Zillich;Henrik Jacobsson;Geert-Jan M. Kruijff;Michael Brenner;Gregor Berginc;Danijel Skočaj
#t 2007
#c 10
#% 18600
#% 23011
#% 1250389
#% 1275087
#% 1732930
#! We present integration mechanisms for combining heterogeneous components in a situated information processing system, illustrated by a cognitive robot able to collaborate with a human and display some understanding of its surroundings. These mechanisms include an architectural schema that encourages parallel and incremental information processing, and a method for binding information from distinct representations that when faced with rapid change in the world can maintain a coherent, though distributed, view of it. Provisional results are demonstrated in a robot combining vision, manipulation, language, planning and reasoning capabilities interacting with a human and manipulable objects.

#index 1269917
#* Spatial representation and reasoning for human-robot collaboration
#@ William G. Kennedy;Magdalena D. Bugajska;Matthew Marge;William Adams;Benjamin R. Fransen;Dennis Perzanowski;Alan C. Schultz;J. Gregory Trafton
#t 2007
#c 10
#% 418689
#% 856893
#% 1784813
#! How should a robot represent and reason about spatial information when it needs to collaborate effectively with a human? The form of spatial representation that is useful for robot navigation may not be useful in higher-level reasoning or working with humans as a team member. To explore this question, we have extended previous work on how children and robots learn to play hide and seek to a human-robot team covertly approaching a moving target. We used the cognitive modeling system, ACT-R, with an added spatial module to support the robot's spatial reasoning. The robot interacted with a team member through voice, gestures, and movement during the team's covert approach of a moving target. This paper describes the new robotic system and its integration of metric, symbolic, and cognitive layers of spatial representation and reasoning for its individual and team behavior.

#index 1269918
#* Extending cognitive architecture with episodic memory
#@ Andrew M. Nuxoll;John E. Laird
#t 2007
#c 10
#% 75896
#% 79997
#% 168280
#% 224475
#! In this paper, we explore the hypothesis that episodic memory is a critical component for cognitive architectures that support general intelligence. Episodic memory overlaps with case-based reasoning (CBR) and can be seen as a task-independent, architectural approach to CBR. We define the design space for episodic memory systems and the criteria any implementation must meet to be useful in a cognitive architecture. We present an implementation and demonstrate how episodic memory, combined with other components of a cognitive architecture, supports a wealth of cognitive capabilities that are difficult to attain without it.

#index 1269919
#* Integrated introspective case-based reasoning for intelligent tutoring systems
#@ Leen-Kiat Soh
#t 2007
#c 10
#% 168280
#% 286419
#% 398948
#% 400438
#% 446573
#% 494248
#% 494260
#% 494577
#% 553608
#% 578669
#% 661318
#% 1220304
#% 1250530
#% 1250532
#% 1250536
#! Many intelligent tutoring systems (ITSs) have been developed, deployed, assessed, and proven to facilitate learning. However, most of these systems do not generally adapt to new circumstances, do not self-evaluate and self-configure their own strategies, and do not monitor the usage history of the learning content being delivered or presented to the students. These shortcomings force ITS developers to often spend much development time in manual revision and fine-tuning of the learning and instructional contents of an ITS. In this paper, we describe an intelligent agent that delivers learning material adaptively to different students, factoring in the usage history of the learning materials and student profiles as observed by the agent. Student-tutor interaction includes the activities of going through learning material, such as a topical tutorial, a set of examples, and a set of problems. Our assumption is that our agent will be able to capture and utilize these student activities as the primer to select the appropriate examples or problems to administer to the student. Using an integrated introspective case-based reasoning approach, our agent further learns from its experience and refines its reasoning process-including the instructional strategies-to adapt to student needs. Moreover, our agent monitors the usage history of the learning materials to improve its performance. We have built an end-to-end ITS using an agent powered by this integrated introspective case-based reasoning engine. We have deployed the ITS in a CS course. Results indicate that the ITS was able to learn to deliver more appropriate examples and problems to the students.

#index 1269920
#* Predicate projection in a bimodal spatial reasoning system
#@ Samuel Wintermute;John E. Laird
#t 2007
#c 10
#% 109857
#% 939431
#! Spatial reasoning is a fundamental aspect of intelligent behavior, which cognitive architectures must address in a problem-independent way. Bimodal systems, employing both qualitative and quantitative representations of spatial information, are efficient and psychologically plausible means for spatial reasoning. Any such system must employ a translation from the qualitative level to the quantitative, where new objects (images) are created through the process of predicate projection. This translation has received little scrutiny. We examine this issue in the context of a bimodal spatial reasoning system integrated with a cognitive architecture (Soar). As part of this system, we define an expressive language for predicate projection that supports general and flexible image creation. We demonstrate this system on multiple spatial reasoning problems in the ORTS real-time strategy game environment.

#index 1269921
#* An intelligent system for Chinese calligraphy
#@ Songhua Xu;Hao Jiang;Francis C. M. Lau;Yunhe Pan
#t 2007
#c 10
#% 2217
#% 85206
#% 157128
#% 159119
#% 238395
#% 286573
#% 371224
#% 443768
#% 594853
#% 616780
#% 641869
#% 813035
#% 871803
#% 899888
#% 1250314
#! Our work links Chinese calligraphy to computer science through an integrated intelligence approach. We first extract strokes of existent calligraphy using a semi-automatic, two-phase mechanism: the first phase tries to do the best possible extraction using a combination of algorithmic techniques; the second phase presents an intelligent user interface to allow the user to provide input to the extraction process for the difficult cases such as those in highly random, cursive, or distorted styles. Having derived a parametric representation of calligraphy, we employ a supervised learning based method to explore the space of visually pleasing calligraphy. A numeric grading method for judging the beauty of calligraphy is then applied to the space. We integrate such a grading unit into an existent constraint-based reasoning system for calligraphy generation, which results in a significant enhancement in terms of visual quality in the automatically generated calligraphic characters. Finally, we construct an intelligent calligraphy tutoring system making use of the above. This work represents our first step towards understanding the human process of appreciating beauty through modeling the process with an integration of available AI techniques. More results and supplementary materials are provided at http://www.cs.hku.hk/~songhua/calligraphy.

#index 1269922
#* An integrated robotic system for spatial understanding and situated interaction in indoor environments
#@ Hendrik Zender;Patric Jensfelt;Óscar Martíinez Mozos;Geert-Jan M. Kruijff;Wolfram Burgard
#t 2007
#c 10
#% 290714
#% 303954
#% 367254
#% 592073
#% 760805
#% 811279
#% 811361
#% 815903
#% 856898
#% 954817
#% 1250390
#! A major challenge in robotics and artificial intelligence lies in creating robots that are to cooperate with people in human-populated environments, e.g. for domestic assistance or elderly care. Such robots need skills that allow them to interact with the world and the humans living and working therein. In this paper we investigate the question of spatial understanding of human-made environments. The functionalities of our system comprise perception of the world, natural language, learning, and reasoning. For this purpose we integrate state-of-the-art components from different disciplines in AI, robotics and cognitive systems into a mobile robot system. The work focuses on the description of the principles we used for the integration, including cross-modal integration, ontology-based mediation, and multiple levels of abstraction of perception. Finally, we present experiments with the integrated "CoSy Explorer" system and list some of the major lessons that were learned from its design, implementation, and evaluation.

#index 1269923
#* A text-to-picture synthesis system for augmenting communication
#@ Xiaojin Zhu;Andrew B. Goldberg;Mohamed Eldawy;Charles R. Dyer;Bradley Strock
#t 2007
#c 10
#% 340405
#% 349208
#% 351983
#% 747668
#% 748636
#% 753907
#% 757148
#% 815902
#% 816173
#% 883897
#% 884178
#% 1112936
#% 1250278
#% 1289522
#% 1395705
#! We present a novel Text-to-Picture system that synthesizes a picture from general, unrestricted natural language text. The process is analogous to Text-to-Speech synthesis, but with pictorial output that conveys the gist of the text. Our system integrates multiple AI components, including natural language processing, computer vision, computer graphics, and machine learning. We present an integration framework that combines these components by first identifying infonnative and 'picturable' text units, then searching for the most likely image parts conditioned on the text, and finally optimizing the picture layout conditioned on both the text and image parts. The effectiveness of our system is assessed in two user studies using children's books and news articles. Experiments show that the synthesized pictures convey as much infonnation about children's stories as the original artists' illustrations, and much more information about news articles than their original photos alone. These results suggest that Text-to-Picture synthesis has great potential in augmenting human-computer and human-human communication modalities, with applications in education and health care, among others.

#index 1269924
#* On the prospects for building a working model of the visual cortex
#@ Thomas Dean;Glenn Carroll;Richard Washington
#t 2007
#c 10
#% 63820
#% 97424
#% 169916
#% 203296
#% 298503
#% 450248
#% 724174
#% 758105
#% 760805
#% 854041
#% 945268
#% 953634
#% 975106
#% 1269507
#! Human visual capability has remained largely beyond the reach of engineered systems despite intensive study and considerable progress in problem understanding, algorithms and computing power. We posit that significant progress can be made by combining existing technologies from computer vision, ideas from theoretical neuroscience and the availability of large-scale computing power for experimentation. From a theoretical standpoint, our primary point of departure from current practice is our reliance on exploiting time in order to turn an otherwise intractable unsupervised problem into a locally semi-supervised, and plausibly tractable, learning problem. From a pragmatic perspective, our system architecture follows what we know of cortical neuroanatomy and provides a solid foundation for scalable hierarchical inference. This combination of features promises to provide a range of robust object-recognition capabilities.

#index 1269925
#* Model-lite planning for the web age masses: the challenges of planning with incomplete and evolving domain models
#@ Subbarao Kambhampati
#t 2007
#c 10
#% 194649
#% 342119
#% 481261
#% 578725
#% 732423
#% 734965
#% 778643
#% 783790
#% 790462
#% 803600
#% 820419
#% 825656
#% 845350
#% 1016160
#% 1250432
#% 1270132
#% 1275078
#% 1275186
#% 1289577
#! The automated planning community has traditionally focused on the efficient synthesis of plans given a complete domain theory. In the past several years, this line of work met with significant successes, and the future course of the community seems to be set on efficient planning with even richer models. While this line of research has its applications, there are also many domains and scenarios where the first bottleneck is getting the domain model at any level of completeness. In these scenarios, the modeling burden automatically renders the planning technology unusable. To counter this, I will motivate model-lite planning technology aimed at reducing the domain-modeling burden (possibly at the expense of reduced functionality), and outline the research challenges that need to be addressed to realize it.

#index 1269926
#* Online collective entity resolution
#@ Indrajit Bhattacharya;Lise Getoor
#t 2007
#c 10
#% 766199
#% 830526
#% 838435
#% 881510
#% 937552
#% 980594
#! Entity resolution is a critical component of data integration where the goal is to reconcile database references corresponding to the same real-world entities. Given the abundance of publicly available databases that have unresolved entities, we motivate the problem of quick and accurate resolution for answering queries over such 'unclean' databases. Since collective entity resolution approaches - where related references are resolved jointly - have been shown to be more accurate than independent attribute-based resolution, we focus on adapting collective resolution for answering queries. We propose a two-stage collective resolution strategy for processing queries. We then show how it can be performed on-the-fly by adaptively extracting and resolving those database references that are the most helpful for resolving the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries. can be answered in real time using our adaptive approach while preserving the gains of collective resolution. This work extends work presented in (Bhattacharya, Licamele, & Getoor 2006).

#index 1269927
#* Learning by combining observations and user edits
#@ Vittorio Castelli;Lawrence Bergman;Daniel Oblinger
#t 2007
#c 10
#% 150990
#% 150994
#% 834614
#% 848654
#% 860034
#! We introduce a new collaborative machine learning paradigm in which the user directs a learning algorithm by manually editing the automatically induced model. We identify a generic architecture that supports seamless interweaving of automated learning from training samples and manual edits of the model, and we discuss the main difficulties that the framework addresses. We describe Augmentation-Based Learning (ABL), the first learning algorithm that supports interweaving of edits and learning from training samples. We use examples based on ABL to outline selected advantages of the approach--dealing with bad data by manually removing their effects from the model, and learning a model with fewer training samples.

#index 1269928
#* Using eye-tracking data for high-level user modeling in adaptive interfaces
#@ Cristina Conati;Christina Merten;Saleema Amershi;Kasia Muldner
#t 2007
#c 10
#% 85672
#% 297575
#% 316163
#% 641790
#% 848633
#% 936916
#% 988199
#% 1219605
#! In recent years, there has been substantial research on exploring how AI can contribute to Human-Computer Interaction by enabling an interface to understand a user's needs and act accordingly. Understanding user needs is especially challenging when it involves assessing the user's high-level mental states not easily reflected by interface actions. In this paper, we present our results on using eye-tracking data to model such mental states during interaction with adaptive educational software. We then discuss the implications of our research for Intelligent User Interfaces.

#index 1269929
#* Informed case base maintenance: a complexity profiling approach
#@ Susan Craw;Stewart Massie;Nirmalie Wiratunga
#t 2007
#c 10
#% 92533
#% 307100
#% 866959
#% 1269393
#% 1275276
#% 1727826
#% 1727842
#! Knowledge maintenance for Case-Based Reasoning systems is an important knowledge engineering task despite the availability of initial case knowledge and new cases to extend it. For classification systems it is essential that different scenarios for the various classes are well represented and decision boundaries are well defined in the case knowledge. A complexity-based competence metric is proposed that identifies redundant and error-causing cases to be deleted. The metric informs a maintenance tool that enables the engineer to experiment and balance conflicting objectives. Complexity-informed maintenance outperforms benchmark algorithms for redundancy and error reduction tasks.

#index 1269930
#* An experimental comparison of constraint logic programming and answer set programming
#@ Agostino Dovier;Andrea Formisano;Enrico Pontelli
#t 2007
#c 10
#% 103704
#% 327779
#% 336874
#% 411814
#% 419969
#% 730003
#% 865743
#% 918148
#% 1655184
#! Answer Set Programming (ASP) and Constraint Logic Programming over finite domains (CLP(FD)) are two declarative progmmming paradigms that have been extensively used to encode applications involving search, optimization, and reasoning (e.g., commonsense reasoning and planning). This paper presents experimental comparisons between the declarative encodings of various computationally hard problems in both frameworks. The objective is to investigate how the solvers in the two domains respond to different problems, highlighting strengths and weaknesses of their implementations, and suggesting criteria for choosing one approach over the other. Ultimately, the work in this paper is expected to lay the foundations for transfer of technology between the two domains, e.g., suggesting ways to use CLP(FD) in the execution of ASP.

#index 1269931
#* Efficient datalog abduction through bounded treewidth
#@ Georg Gottlob;Reinhard Pichler;Fang Wei
#t 2007
#c 10
#% 49315
#% 73005
#% 101922
#% 101944
#% 125557
#% 132173
#% 181220
#% 314762
#% 343623
#% 546670
#% 737759
#% 874889
#% 1250546
#% 1274818
#! Abductive diagnosis is an important method for identifying possible causes which explain a given set of observations. Unfortunately, abduction suffers from the fact that most of the algorithmic problems in this area are intractable. We have recently obtained very promising results for a strongly related problem in the database area. Specifically, the PRIMALITY problem becomes efficiently solvable and highly parallelizable if the underlying functional dependencies have bounded treewidth (Gottlob, Pichler, & Wei 2006b). In the current paper, we show that these favorable results can be carried over to logic-based abduction. In fact, we even show a further generalization of these results.

#index 1269932
#* The pyramid match: efficient learning with partial correspondences
#@ Kristen Grauman
#t 2007
#c 10
#% 347225
#% 724232
#% 734914
#% 760805
#% 762053
#% 836859
#% 883811
#% 883972
#% 883981
#% 961270
#% 997644
#! It is often useful to represent a single example by a set of the local features that comprise it. However, this representation poses a challenge to many conventional learning techniques, since sets may vary in cardinality and the elements are unordered. To compare sets of features, researchers often resort to solving for the least-cost correspondences, but this is computationally expensive and becomes impractical for large set sizes. We have developed a general approximate matching technique called the pyramid match that measures partial match similarity in time linear in the number of feature vectors per set. The matching forms a Mercer kernel, making it valid for use in many existing kernel-based learning methods. We have demonstrated the approach for various learning tasks in vision and text processing, and find that it is accurate and significantly more efficient than previous approaches.

#index 1269933
#* A kernel approach to comparing distributions
#@ Arthur Gretton;Karsten M. Borgwardt;Malte Rasch;Bernhard Scholköpf;Alexander J. Smola
#t 2007
#c 10
#% 164882
#% 722798
#% 833065
#% 906248
#! We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a Reproducing Kernel Hilbert Space. We apply this technique to construct a two-sample test, which is used for determining whether two sets of observations arise from the same distribution. We use this test in attribute matching for databases using the Hungarian marriage method, where it performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist.

#index 1269934
#* A* search via approximate factoring
#@ Aria Haghighi;John DeNero;Dan Klein
#t 2007
#c 10
#% 741071
#% 757953
#% 817596
#% 938736
#% 1261551
#% 1279389
#! We present a novel method for creating A* estimates for structured search problems originally described in Haghighi, DeNero, & Klein (2007). In our approach, we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein & Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and demonstrate its effectiveness.

#index 1269935
#* Manifold denoising as preprocessing for finding natural representations of data
#@ Matthias Hein;Markus Maier
#t 2007
#c 10
#% 196994
#% 257039
#% 593047
#% 732546
#% 1705532
#! A natural representation of data is given by the parameters which generated the data. If the space of parameters is continuous, then we can regard it as a manifold. In practice, we usually do not know this manifold but we just have some representation of the data, often in a very high-dimensional feature space. Since the number of internal parameters does not change with the representation, the data will effectively lie on a low-dimensional submanifold in feature space. However, the data is usually corrupted by noise, which particularly in high-dimensional feature spaces makes it almost impossible to find the manifold structure. This paper reviews a method called Manifold Denoising, which projects the data onto the submanifold using a diffusion process on a graph generated by the data. We will demonstrate that the method is capable of dealing with non-trival high-dimensional noise. Moreover, we will show that using the denoising method as a preprocessing step, one can significantly improve the results of a semi-supervised learning algorithm.

#index 1269936
#* Near-optimal observation selection using submodular functions
#@ Andreas Krause;Carlos Guestrin
#t 2007
#c 10
#% 56558
#% 729923
#% 836516
#% 840868
#% 862540
#% 867029
#% 875997
#% 1275108
#% 1289563
#% 1845813
#! AI problems such as autonomous robotic exploration, automatic diagnosis and activity recognition have in common the need for choosing among a set of informative but possibly expensive observations. When monitoring spatial phenomena with sensor networks or mobile robots, for example, we need to decide which locations to observe in order to most effectively decrease the uncertainty, at minimum cost. These problems usually are NP-hard. Many observation selection objectives satisfy submodularity, an intuitive diminishing returns property - adding a sensor to a small deployment helps more than adding it to a large deployment. In this paper, we survey recent advances in systematically exploiting this submodularity property to efficiently achieve near-optimal observation selections, under complex constraints. We illustrate the effectiveness of our approaches on problems of monitoring environmental phenomena and water distribution networks.

#index 1269937
#* Dominance and equivalence for sensor-based agents
#@ Jason M. O'Kane;Steven M. LaValle
#t 2007
#c 10
#% 18559
#% 252183
#% 360402
#% 789560
#% 862541
#% 934104
#% 1022140
#% 1127694
#% 1272319
#% 1279222
#% 1289395
#! This paper describes recent results from the robotics community that develop a theory, similar in spirit to the theory of computation, for analyzing sensor-based agent systems. The central element to this work is a notion of dominance of one such system over another. This relation is formally based on the agents' progression through a derived information space, but may informally be understood as describing one agent's ability to "simulate" another. We present some basic properties of this dominance relation and demonstrate its usefulness by applying it to a basic problem in robotics. We argue that this work is of interest to a broad audience of artificial intelligence researchers for two main reasons. First, it calls attention to the possibility of studying belief spaces in way that generalizes both probabilistic and nondeterministic uncertainty models. Second, it provides a means for evaluating the information that an agent is able to acquire (via its sensors and via conformant actions), independent of any optimality criterion and of the task to be completed.

#index 1269938
#* Modeling and learning vague event durations for temporal reasoning
#@ Feng Pan;Rutu Mulkar-Mehta;Jerry R. Hobbs
#t 2007
#c 10
#% 399
#% 7047
#% 116335
#% 207677
#% 417715
#% 934828
#% 939550
#% 939595
#% 1289510
#% 1290144
#% 1425500
#% 1787938
#! This paper reports on our recent work on modeling and automatically extracting vague, implicit event durations from text (Pan et al., 2006a, 2006b). It is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problems. We have also proposed a method of using normal distributions to model Judgments that are intervals on a scale and measure their interannotator agreement; this should extend from time to other kinds of vague but substantive information in text and commonsense reasoning.

#index 1269939
#* Learning and inference for hierarchically split PCFGs
#@ Slav Petrov;Dan Klein
#t 2007
#c 10
#% 708948
#% 741360
#% 748810
#% 817472
#% 939341
#% 939353
#% 939555
#% 940033
#% 1250428
#! Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank. We describe a method in which a minimal grammar is hierarchically refined using EM to give accurate, compact grammars. The resulting grammars are extremely compact compared to other high-performance parsers, yet the parser gives the best published accuracies on several languages, as well as the best generative parsing numbers in English. In addition, we give an associated coarse-to-fine inference scheme which vastly improves inference time with no loss in test set accuracy.

#index 1269940
#* Refutation by randomised general resolution
#@ Steven Prestwich;Inês Lynce
#t 2007
#c 10
#% 41220
#% 116559
#% 288165
#% 288366
#% 327779
#% 345059
#% 420720
#% 656782
#% 729052
#% 778165
#% 1250144
#% 1273681
#% 1275116
#% 1728045
#! Local search is widely applied to satisfiable SAT problems, and on some problem classes outperforms backtrack search. An intriguing challenge posed by Selman, Kautz and McAllester in 1997 is to use it instead to prove unsatisfiability. We design a greedy randomised resolution algorithm called RANGER that will eventually refute any unsatisfiable instance while using only bounded memory. RANGER can refute some problems more quickly than systematic resolution or backtracking with clause learning. We believe that non-systematic but greedy inference is an interesting research direction for powerful proof systems such as general resolution.

#index 1269941
#* Beyond individualism: modeling team playing behavior in robot soccer through case-based reasoning
#@ Raquel Ros;Manuela Veloso;Ramon López De Mántaras;Carles Sierra;Josep Lluis Arcos
#t 2007
#c 10
#% 866954
#% 866959
#% 1727823
#! We propose a Case-Based Reasoning approach for action selection in the robot soccer domain presented in the 8th European Conference on Case-Based Reasoning (2006). Based on the current state of a game, the robots retrieve the most similar past situation and then the team reproduces the sequence of actions performed in that occasion. In this domain we have to deal with all the difficulties that a real environment involves.

#index 1269942
#* Temporal difference and policy search methods for reinforcement learning: an empirical comparison
#@ Matthew E. Taylor;Shimon Whiteson;Peter Stone
#t 2007
#c 10
#% 203594
#% 203602
#% 384911
#% 449980
#% 490654
#% 810929
#% 811571
#% 876267
#% 961164
#% 1272024
#% 1693747
#! Reinforcement learning (RL) methods have become popular in recent years because of their ability to solve complex tasks with minimal feedback. Both genetic algorithms (GAs) and temporal difference (TD) methods have proven effective at solving difficult RL problems, but few rigorous comparisons have been conducted. Thus, no general guidelines describing the methods' relative strengths and weaknesses are available. This paper summarizes a detailed empirical comparison between a GA and a TD method in Keepaway, a standard RL benchmark domain based on robot soccer. The results from this study help isolate the factors critical to the performance of each learning method and yield insights into their general strengths and weaknesses.

#index 1269943
#* Making VCG more robust in combinatorial auctions via sub modular approximation
#@ Makoto Yokoo;Atsushi Iwasaki
#t 2007
#c 10
#% 341408
#% 413867
#% 496094
#% 496250
#% 739626
#% 836550
#% 890384
#! The Vickrey-Clarke-Groves (VCG) protocol is a theoretically well-founded protocol that can be used for combinatorial auctions. However, the VCG has several limitations such as (a) vulnerability to false-name bids, (b) vulnerability to loser collusion, and (c) the outcome is not in the core. Yokoo, Matsutani, & Iwasaki (2006) presented a new combinatorial auction protocol called the Groves Mechanism with SubModular Approximation (GM-SMA). This protocol satisfies the following characteristics: (1) it is false-name-proof, (2) each winner is included in a Pareto efficient allocation, and (3) as long as a Pareto efficient allocation is achieved, the protocol is robust against the collusion of losers and the outcome is in the core. The GM-SMA is the first protocol that satisfies all three of these characteristics. The basic ideas of the GM-SMA are as follows: (i) it is based on the VCG protocol, i.e., the payment of a winner in this protocol is identical to the payment in one instance of the Groves mechanism, which is a class of protocols that includes the VCG, (ii) when calculating the payment of a bidder, we approximate the valuations of other bidders by using a submodular valuation function (submodular approximation). This paper shows a high-level presentation of the GM-SMA protocol. and discusses open problems and the relationship to other works in AI.

#index 1269944
#* Data clustering with a relational push-pull model
#@ Adam Anthony;Marie DesJardins
#t 2007
#c 10
#% 578775
#% 722914
#% 844322
#% 1250567
#! Relational data clustering is the task of grouping data objects together when both attributes and relations between objects are present. We present a new generative model for relational data in which relations between objects can have either a binding or separating effect.

#index 1269945
#* UNDERTOW: multi-level segmentation of real-valued time series
#@ Tom Armstrong;Tim Oates
#t 2007
#c 10
#% 313959
#% 326303
#% 629631
#% 662750
#% 1272315
#! The discovery of meaningful change points, finding segments, in both categorical and real-value data time series is a well-studied problem. Prior segmentation algorithms and tasks operate under overly restrictive assumptions (e.g., a priori knowledge of the number of segments, trivial inputs) and in singular domains (e.g., finding common regions in images, speaker change detection). We introduce a domain-independent algorithm, UNDERTOW, which discovers segment boundaries in real-valued time series and constructs hierarchies of segments to form macro segments.

#index 1269946
#* Explanation support for the case-based reasoning tool myCBR
#@ Daniel Bahls;Thomas Roth-Berghofer
#t 2007
#c 10
#% 176887
#! Case-Based Reasoning, in short, is the process of solving new problems based on solutions of similar past problems, much like humans solve many problems. myCBR, an extension of the ontology editor Protégé, provides such similarity-based retrieval functionality. Moreover, the user is supported in modelling appropriate similarity measures by forward and backward explanations.

#index 1269947
#* A Markovian model for dynamic and constrained resource allocation problems
#@ Camille Besse;Brahim Chaib-Draa
#t 2007
#c 10
#% 30037
#% 224762
#% 314843
#% 567883
#% 816232
#% 1250350
#% 1499491

#index 1269948
#* Implementing modal extensions of defeasible logic for the semantic web
#@ Nikos Dimaresis;Grigoris Antoniou
#t 2007
#c 10
#% 330234
#% 340563
#% 346442
#% 894054
#% 904768
#% 913800

#index 1269949
#* Ungreedy methods for Chinese deterministic dependency parsing
#@ Xiangyu Duan;Jun Zhao;Bo Xu
#t 2007
#c 10
#% 827511
#% 939343
#% 939554
#% 939658
#% 1299529
#! Deterministic dependency parsing has often been regarded as an efficient parsing algorithm while its parsing accuracy is a little lower than the best results reported by more complex parsing models. In this paper, we compare deterministic dependency parsers with complex parsing methods such as generative and discriminative parsers on the standard data set of Penn Chinese Treebank. The results show that, for Chinese dependency parsing, deterministic parsers outperform generative and discriminative parsers. Furthermore, based on the observation that deterministic parsing algorithms are greedy algorithms which choose the most probable parsing action at every step, we propose three kinds of ungreedy deterministic dependency parsing algorithms to globally model parsing actions. Results show that ungreedy deterministic dependency parsers perform better than original deterministic dependency parsers while maintaining the same time complexity, and our best parser improves much over all other parsers.

#index 1269950
#* Using multiresolution learning for transfer in image classification
#@ Eric Eaton;Marie DesJardins;John Stevenson
#t 2007
#c 10
#% 784995
#% 812600
#! Our work explores the transfer of knowledge at multiple levels of abstraction to improve learning. By exploiting the similarities between objects at various levels of detail, multiresolution learning can facilitate transfer between image classification tasks. We extract features from images at multiple levels of resolution, then use these features to create models at different resolutions. Upon receiving a new task, the closest-matching stored model can be generalized (adapted to the appropriate resolution) and transferred to the new task.

#index 1269951
#* Robust estimation of 3-D line segments from satellite images for model building and change detection
#@ Ibrahim Eden;David B. Cooper
#t 2007
#c 10
#% 718439
#% 1855336
#% 1855728

#index 1269952
#* Classifiers fusion for EEG signals processing in human-computer interface systems
#@ Maryam Esmaeili
#t 2007
#c 10
#% 80995
#% 170748
#% 991064
#% 1393001
#! In this paper we study the effectiveness of using multiple classifier combination for EEG signals classification aiming to obtain more accurate results than it possible from single classifier system. The developed system employs different features vectors fused at the abstract and measurement levels for integrating information to reach a collective decision. For making decision, the majority voting scheme has been used. While at the measurement level, fuzzy integral, majority vote, decision template and some other types of combination methods have been investigated. The ensemble classification task is completed by feeding the Support Vectors Machines with Redial Basis Kernel functions classifiers with different features extracted from the EEG signal for imagination of right and left hands movements (i.e., at EEG channels C3 and C4). The parameters of SVM classifiers were optimized by genetic algorithm. The results show that using classifier fusion methods improved the overall classification performance.

#index 1269953
#* On policy learning in restricted policy spaces
#@ Robby Goetschalckx;Jan Ramon
#t 2007
#c 10
#% 384911
#% 578697
#% 722923
#% 1274925

#index 1269954
#* Two approaches for building an unsupervised dependency parser and their other applications
#@ Jagadeesh Gorla;Amit Goyal;Rajeev Sangal
#t 2007
#c 10
#% 708948
#% 757329
#% 828246
#% 939609
#% 939919
#% 939966
#! Much work has been done on building a parser for natural languages, but most of this work has concentrated on supervised parsing. Unsupervised parsing is a less explored area, and unsupervised dependency parser has hardly been tried. In this paper we present two approaches for building an unsupervised dependency parser. One approach is based on learning dependency relations and the other on learning subtrees. We also propose some other applications of these approaches.

#index 1269955
#* ASKNet: automated semantic knowledge network
#@ Brian Harrington
#t 2007
#c 10
#% 198055
#% 747791
#% 752077
#% 783633
#% 938666
#! The ASKNet project is an attempt to automatically generate semantic knowledge networks from natural language text. NLP tools such as parsers and semantic analysers are used to turn input sentences into fragments of semantic network, and these network fragments are combined using spreading activation algorithms that utilise both lexical and semantic information. The ultimate goal of the project is to create a semantic resource on a scale that has never before been possible.

#index 1269956
#* Teamtalk: a platform for multi-human-robot dialog research in coherent real and virtual spaces
#@ Thomas K. Harris;Alexander I. Rudnicky
#t 2007
#c 10
#! Performing experiments with human-robot interfaces often requires the allocation of expensive and complex hardware and large physical spaces. Those costs constrain development and research to the currently affordable resources, and they retard the testing-and-redevelopment cycle. In order to explore research free from mundane allocation constraints and speed-up our platform development cycle, we have developed a platform for research of multi-human-robot spoken dialog in coherent real and virtual spaces. We describe the system, and speculate on how it will further research in this domain.

#index 1269957
#* Reputation in the venture games
#@ Philip Hendrix;Barbara J. Grosz
#t 2007
#c 10
#% 659852
#% 808381
#% 823966

#index 1269958
#* Evolutionary rhythm composition with trajectory-based fitness evaluation
#@ John Huddleston;Jianna Zhang
#t 2007
#c 10
#% 310057
#% 749249
#% 872951

#index 1269959
#* Identifying protein interaction abstracts with contextual bag of words
#@ Hsi-Chuan Hung;Richard Tzong-Han Tsai;Wen-Lian Hsu
#t 2007
#c 10
#% 458379
#% 464641
#% 466101
#% 1676554
#! In this paper, we focus on the identification of biomedical abstracts related to protein-protein interactions. We propose a novel feature representation, contextual-bag-of-words, to exploit named entity information. Our method outperforms well-known methods that use named entity information as additional features. Furthermore, we have improved the performance by extracting reliable and informative instances from unlabeled and likely-positive data to provide additional training data.

#index 1269960
#* Modeling user perception of interaction opportunities in collaborative human-computer settings
#@ Ece Kamar;Barbara J. Grosz;David Sarne
#t 2007
#c 10
#% 731040
#% 751812
#% 773284

#index 1269961
#* Towards an adaptive approach for distributed resource allocation in a multi-agent system for solving dynamic vehicle routing problems
#@ Igor Kiselev;Andrey Glaschenko;Alexander Chevelev;Petr Skobelev
#t 2007
#c 10
#% 266286
#% 557569

#index 1269962
#* On possible applications of rough mereology to handling granularity in ontological knowledge
#@ Pavel Klinov;Lawrence J. Mazlack
#t 2007
#c 10
#% 168563
#% 752488
#% 1408592
#% 1730397
#! Representing and reasoning about knowledge is critical in Artificial Intelligence. There is a distinction between factual and ontological knowledge. Factual knowledge represents a set of facts about individual objects that are known or believed whereas ontological (background) knowledge represents implicit concepts and relationships that are assumed to exist in the world. Ontological knowledge is often represented as a hierarchy of concepts because splitting things of the real world into categories and sub-categories is a natural way of human thinking. One example of conceptual hierarchies in AI is ontologies that are widely used in such areas as Natural Language Processing, Semantic Web, etc. Representation of both types of knowledge becomes difficult when the knowledge is imprecise. One example of imprecision is granularity i.e. inability to distinguish between the individual objects. In this case, knowledge cannot be represented precisely but can be approximated with respect to the granularity of the domain. Approximation of factual knowledge has been extensively researched and often employs Rough Set Theory (Pawlak 1982) for dealing with indiscernibility of objects. Similar approach has been applied to ontologies to approximate concepts in the hierarchy (Doherty et al. 2003). The open problem is the approximations of hierarchical relationships (such as "is-a", "part-of') between concepts. This paper addresses this issue using Rough Mereology (Polkowski & Skowron 1996) complemented with Interval Analysis (Moore 1966). The principal contribution is to provide rough mereological functions that can be used for representation and reasoning with formal ontologies in approximation spaces. Specifically, approximate concept membership and approximate concept subsumption functions will be provided. It can be demonstrated that the interval based functions are free of the shortcomings of the previously suggested definitions (Cao, Sui, & Zhang 2003) (Klinov & Mazlack 2007).

#index 1269963
#* Fuzzy set theory-based belief processing for natural language texts
#@ Ralf Krestel;René Witte;Sabine Bergler
#t 2007
#c 10
#% 127341
#% 404608
#% 943529

#index 1269964
#* Knowledge-driven learning and discovery
#@ Benjamin Lambert;Scott E. Fahlman
#t 2007
#c 10
#% 451052
#% 830520
#% 1693246
#! The goal of our current research is machine learning with the help and guidance of a knowledge base (KB). Rather than learning numerical models, our approach generates explicit symbolic hypotheses. These hypotheses are subject to the constraints of the KB and are easily human-readable and verifiable. Toward this end, we have implemented algorithms that hypothesize new relations and new types of entities in a KB by examining structural regularities in the KB that represent implicit knowledge. We evaluate these algorithms on a publications KB and a zoology KB.

#index 1269965
#* Reinforcement using supervised learning for policy generalization
#@ Julien Laumonier
#t 2007
#c 10
#% 174161
#% 183499
#% 876055

#index 1269966
#* Aggregating user-centered rankings to improve web search
#@ Lin Li;Zhenglu Yang;Masaru Kitsuregawa
#t 2007
#c 10
#% 309095
#% 330769
#% 641963
#% 818224
#! This paper is to investigate rank aggregation based on multiple user-centered measures in the context of the web search. We introduce a set of techniques to combine ranking lists in order of user interests termed as a user profile. Moreover, based on the click-history data, a kind of taxonomic hierarchy automatically models the user profile which can include a variety of attributes of user interests. We mainly focus on the topics a user is interested in and the degrees of user interests in these topics. The primary goal of our work is to form a broadly acceptable ranking list, rather than that determined by an individual ranking measure. Experiment results on a real click-history data set show the effectiveness of our aggregation techniques to improve the web search.

#index 1269967
#* Recommending travel packages upon distributed knowledge
#@ Fabiana Lorenzi;Ana L. C. Bazzan;Mara Abel
#t 2007
#c 10
#% 3460
#% 173879
#% 643099

#index 1269968
#* BlogVox: learning sentiment classifiers
#@ Justin Martineau;Akshay Java;Pranam Kolari;Tim Finin;Anupam Joshi;James Mayfield
#t 2007
#c 10
#% 577355
#% 854646

#index 1269969
#* Impromptu teams of heterogeneous mobile robots
#@ Ross Mead;Jerry B. Weinberg
#t 2007
#c 10
#% 412051
#% 1269367
#! As robots become more involved in assisting us in large and hazardous operations, such as search and rescue, we can anticipate that diverse robots will come together with the need to coordinate their efforts. These robots will come from different organizations, creating a heterogeneous team, varying in shape, size, and functionality. How can diverse robots forming such an impromptu team collaborate to accomplish a joint objective? If they are to organize and work together, methods must be developed that allow them to share knowledge in a meaningful way. We propose an ontology-based symbolic communication protocol to provide a shared understanding of physical concepts between units. Coordination is then accomplished through a negotiation of tasks to complete individual and joint goals.

#index 1269970
#* Time-delay neural networks and independent component analysis for EEG-based prediction of epileptic seizures propagation
#@ Piotr W. Mirowski;Deepak Madhavan;Yann LeCun
#t 2007
#c 10
#% 92148
#% 313975
#% 476873
#! This research focuses on the development of a machine learning technique based on Time-Delay Neural Networks (TDNN) and Independent Component Analysis (ICA), to analyze EEG signal dynamics related to the initiation and propagation of epileptic seizures. We aim at designing a generative model to simulate EEG time-series after alteration of specific localized channels (electrodes) in order to explore the effects of brain surgery ex-vivo.

#index 1269971
#* Using iterated best-response to find Bayes-Nash equilibria in auctions
#@ Victor Naroditskiy;Amy Greenwald
#t 2007
#c 10
#% 788092

#index 1269972
#* The marchitecture: a cognitive architecture for a robot baby
#@ Marc Pickett;Tim Oates
#t 2007
#c 10
#% 373996
#% 395698
#% 451399
#% 578675
#% 758105
#% 1705349
#! The Marchitecture is a cognitive architecture for autonomous development of representations. The goals of The Marchitecture are domain independence, operating in the absence of knowledge engineering, learning an ontology of parameterized relational concepts, and elegance of design. To this end, The Marchitecture integrates classification, parsing, reasoning, and explanation. The Marchitecture assumes an ample amount of raw data to develop its representations, and it is therefore appropriate for long lived agents.

#index 1269973
#* Integrative construction and analysis of condition-specific biological networks
#@ Sushmita Roy;Terran Lane;Margaret Werner-Washburne
#t 2007
#c 10
#% 961197

#index 1269974
#* Extracting student models for intelligent tutoring systems
#@ John C. Stamper;Tiffany Barnes;Marvin Croy
#t 2007
#c 10
#% 1499526
#% 1733183

#index 1269975
#* Unscented message passing for arbitrary continuous variables in Bayesian networks
#@ Wei Sun;Kuo-Chu Chang
#t 2007
#c 10
#% 44876
#% 394009
#! Since Bayesian network (BN) was introduced in the field of artificial intelligence in 1980s, a number of inference algorithms have been developed for probabilistic reasoning. However, when continuous variables are present in Bayesian networks, their dependence relationships could be nonlinear and their probability distributions could be arbitrary. So far no efficient inference algorithm could deal with this case except Monte Carlo simulation methods such as Likelihood Weighting. But with unlikely evidence, simulation methods could be very slow to converge. In this paper, we propose an efficient approximate inference algorithm called Unscented Message Passing (UMP-BN) for Bayesian networks with arbitrary continuous variables. UMP-BN combines unscented transformation - a deterministic sampling method, and Pearl's message passing algorithm to provide the estimates of the first two moments of the posterior distributions. We test this algorithm with several networks including the ones with nonlinear and/or non-Gaussian variables. The numerical experiments show that UMP-BN converges very fast and produces promising results.

#index 1269976
#* An investigation into computational recognition of children's jokes
#@ Julia M. Taylor;Lawrence J. Mazlack
#t 2007
#c 10
#% 870887
#! The purpose of this paper is to describe an investigation into an ontology-based computational recognition of children's jokes. While humor has been studied for centuries computational humor has received very little attention: This may in part be due to the difficulty of the task: at the very least, it requires formal methods for humor generation/recognition and "being able to produce/interpret natural language, being capable of subtle and flexible inferences, and having a vast store of knowledge about the real world." [Ritchie, 2004]. There are some humor generators (see Ritchie [2004] for a review) and a handful of humor recognizers. Yet, "If computers are ever going to communicate naturally and effectively with humans, they must be able to use humor." [Binsted, 2006]. We are interested in recognition, not generation of humor. Recognition of all verbally expressed humor is an overly broad task. To narrow the task, only jokes for young children are considered. The reduction of the domain size to young children's jokes is expected to decrease the complexity and sophistication of the language to be analyzed. This in turn decreases the knowledge that needs to be captured for text interpretation.

#index 1269977
#* Representation transfer via elaboration
#@ Matthew E. Taylor;Peter Stone
#t 2007
#c 10
#% 23011
#% 203602
#% 377895
#% 505086
#% 1269498
#% 1705342

#index 1269978
#* Situated conversational agents
#@ Will Thompson
#t 2007
#c 10
#% 252183
#% 940817
#! A Situated Conversational Agent (SCA) is an agent that engages in dialog about the context within which it is embedded. Situated dialog is characterized by its deep connection to the embedding context, and the precise cross-timing of linguistic and non-linguistic actions. This paper describes initial research into the construction of an SCA that engages in dialog about collaborative physical tasks, in which agents engage in dialog with the joint goal of manipulating the physical context in some manner. Constructing an SCA that can interact naturally in such tasks requires an agent with the ability to interleave planning, action, and observation while operating in a partially observable environment. Consequently, I propose to model an SCA as a Partially Observable Markov Decision Process (POMDP).

#index 1269979
#* Scaling up: solving POMDPs through value based clustering
#@ Yan Virin;Guy Shani;Solomon E. Shimony;Ronen I. Brafman
#t 2007
#c 10
#% 788098
#% 1272075
#% 1279358
#% 1665157
#! We present here a point-based value iteration algorithm for solving POMDPs, that orders belief state backups smartly based on a clustering of the underlying MDP states. We show our SCVI algorithm to converge faster than state of the art point-based algorithms.

#index 1269980
#* Learn to compress and restore sequential data
#@ Yi Wang;Jianhua Feng;Shixia Liu
#t 2007
#c 10
#% 874634
#% 910778
#% 915313

#index 1269981
#* Interest-matching comparisons using CP-nets
#@ Andrew W. Wicker;Jon Doyle
#t 2007
#c 10
#% 499542
#% 1272026
#% 1650274
#! The formation of internet-based social networks has revived research on traditional social network models as well as interest-matching, or match-making, systems. In order to automate or augment the process of interest-matching, we describe a method for the comparison of preference orderings represented by CP-nets, which allows one to determine a shared interest level between agents. Empirical results suggest that this distance measure for preference orderings agrees with the intuitive assessment of shared interest levels.

#index 1269982
#* Counting models using extension rules
#@ Mingbao Yin;Hai Lin;Jigui Sun
#t 2007
#c 10
#% 427631
#% 496111
#% 529186
#% 723877
#% 726171
#% 761257
#% 1269433
#% 1272404
#% 1289558

#index 1269983
#* User model and utility based power management
#@ Chih-Han Yu;Shie Mannor;Georgios Theocharous;Avi Pfeffer
#t 2007
#c 10
#% 75936
#% 239725
#% 582393
#% 1838025

#index 1269984
#* Measuring the uncertainty of differences for contrasting groups
#@ Jilian Zhang;Shichao Zhang;Xiaofeng Zhu;Xindong Wu;Chengqi Zhang
#t 2007
#c 10
#% 280477
#% 420126
#% 729935
#% 1692994
#! In this paper, we propose an empirical likelihood (EL) based strategy for building confidence intervals for differences between two contrasting groups. The proposed method can deal with the situations when we know little prior knowledge about the two groups, which are referred to as non-parametric situations. We experimentally evaluate our method on UCI datasets and observe that proposed EL based method outperforms other methods.

#index 1269985
#* Cost-sensitive imputing missing values with ordering
#@ Xiaofeng Zhu;Shichao Zhang;Jilian Zhang;Chengqi Zhang
#t 2007
#c 10
#! Various approaches for dealing with missing data have been developed so far. In this paper, two strategies are proposed for cost-sensitive iterative imputing missing values with optimal ordering. Experimental results demonstrate that proposed strategies outperform the existing methods in terms of imputation cost and accuracy.

#index 1269986
#* Continuous state POMDPs for object manipulation tasks
#@ Emma Brunskill
#t 2007
#c 10
#% 252183
#% 961216
#% 1272075
#! My research focus is on using continuous state partially observable Markov decision processes (POMDPs) to perform object manipulation tasks using a robotic arm. During object manipulation, object dynamics can be extremely complex, non-linear and challenging to specify. To avoid modeling the full complexity of possible dynamics. I instead use a model which switches between a discrete number of simple dynamics models. By learning these models and extending Porta's continuous state POMDP framework (Porta et at. 2006) to incorporate this switching dynamics model, we hope to handle tasks that involve absolute and relative dynamics within a single framework. This dynamics model may be applicable not only to object manipulation tasks, but also to a number of other problems, such as robot navigation. By using an explicit model of uncertainty, I hope to create solutions to object manipulation tasks that more robustly handle the noisy sensory information received by physical robots.

#index 1269987
#* Approximate inference in probabilistic graphical models with determinism
#@ Vibhav Gogate
#t 2007
#c 10
#% 231738
#% 451125
#% 528328
#% 644201
#% 788050
#% 1271825
#% 1664981
#% 1665012
#% 1672982
#% 1799876

#index 1269988
#* Handling non-sentential utterances in a continuous understanding framework
#@ Carlos Gómez Gallo
#t 2007
#c 10
#% 266227
#% 1264260
#! The goal of my research is to understand speech input in a continuous manner by treating the input stream as fragmental utterances. This allows us to use various approaches to predict what comes downstream. Possible interpretations are trimmed by such predictions which in turn also allow us to complete information not readily available in the fragmental utterance. Semantic frames can encode all possible arguments for domain actions. As utterances are processed continuously, appropriate frames can be activated so that fragment interpretations can fill, correct or extend frames under consideration. In turn, feedback can be provided to the parser as the frames are manipulated possibly based on the completeness of the semantic frame construction.

#index 1269989
#* ASKNet: automatically generating semantic knowledge networks
#@ Brian Harrington
#t 2007
#c 10
#% 198055
#% 747791
#% 752077
#% 783633
#% 938666
#! The ASKNet project uses a combination of NLP tools and spreading activation to transform natural language text into semantic knowledge networks. Network fragments are generated from input sentences using a parser and semantic analyser, then these fragments are combined using spreading activation based algorithms. The ultimate goal of the project is to create a semantic resource on a scale that has never before been possible. We have already managed to create networks more than twice as large as any comparable resource(1.5 million nodes, 3.5 million edges) in less than 3 days. This report provides a summary of the project and its current state of development.

#index 1269990
#* A framework for modeling influence, opinions and structure in social media
#@ Akshay Java
#t 2007
#c 10
#% 577355
#% 729923
#% 805873
#% 823367
#% 854646

#index 1269991
#* Thesis summary: empirical game-theoretic methods for strategy design and analysis in complex games
#@ Christopher Kiekintveld
#t 2007
#c 10
#% 806741
#% 868467
#% 868478
#% 1024744
#% 1024869
#% 1343874

#index 1269992
#* Using spatial language in multi-modal knowledge capture
#@ Kate Lockwood
#t 2007
#c 10
#% 320833
#% 539347
#% 895697
#% 1269481
#! The ability to understand and communicate spatial relationships is central to many human-level reasoning tasks. People often describe spatial relationships using prepositions (i.e., in, on, under). Being able to use and interpret spatial prepositions could help create interactive systems for many tasks, including knowledge capture. Here I describe my thesis work modeling the learning and use of spatial prepositions and applying this model to the task of knowledge capture.

#index 1269993
#* Responding to student affect and efficacy through empathetic companion agents in interactive learning environments
#@ Scott W. McQuiggan
#t 2007
#c 10
#% 553935
#% 890353
#% 1704233
#% 1733232
#% 1837240
#! Because many students experience frustration during learning, it is important to develop affective strategies to support students' coping with frustration in interactive learning environments. First, we must devise affect recognition models to detect student affect. Second, we need to determine when to intervene; these conditions are likely to be different for each student. To determine how much frustration a student can persist through, we should utilize models of student self-efficacy to predict a student's frustration threshold. Third, we should devise techniques for responding empathetically before the student reaches her threshold of frustration. We propose an approach to support students' coping with frustration in intelligent tutoring systems that utilizes induced models of affect, self-efficacy and empathetic behavior to effectively reason about precisely when and how to intervene in frustration-ridden learning situations.

#index 1269994
#* The Übercruncher: concept formation by analogy discovery
#@ Marc Pickett
#t 2007
#c 10
#% 451399
#% 464607
#% 1705349

#index 1269995
#* Harnessing algorithm bias in classical planning
#@ Mark Roberts
#t 2007
#c 10
#% 544927
#% 739877
#% 926881
#% 1272083

#index 1269996
#* Reacting to agreement and error in spoken dialogue systems using degrees of groundedness
#@ Antonio Roque
#t 2007
#c 10
#% 678426
#% 939882
#! Computational models of grounding are extended to include representations of degrees of groundedness. These representations are then used for decision-making in dialogue management for spoken dialogue systems. Several domains will be explored with this model, and an implementation will be tested and evaluated.

#index 1269997
#* A framework for ontology-based service selection in dynamic environments
#@ Murat Sensoy
#t 2007
#c 10
#% 378981
#% 379191
#% 890346
#% 1024906
#% 1784807
#! Previous approaches to service selection are mainly based on capturing and exchanging the ratings of consumers to providers. However, ratings reflect tastes of the raters. Therefore, service selection using ratings may mislead the consumers having a taste different than that of the raters. We propose to use experiences instead of the ratings. Experiences are the representation of what is requested by a consumer and what is received at the end. Unlike ratings, experiences do not reflect the opinion of the others, but the actual story between consumers and providers concerning a service demand. Using experiences, the consumer models the services of a provider for a specific service demand and selects the provider that is expected to satisfy the consumer the most. Our simulations show that proposed approach significantly increases the overall satisfaction of the service consumers.

#index 1269998
#* Flexible provisioning of service workflows
#@ Sebastian Stein
#t 2007
#c 10
#% 1024754
#% 1178247
#% 1223247
#% 1269901
#% 1396227

#index 1269999
#* Autonomous inter-task transfer in reinforcement learning domains
#@ Matthew E. Taylor
#t 2007
#c 10
#% 124694
#% 169359
#% 384911
#% 890312
#% 961164
#% 983914
#% 1024713
#% 1250585
#% 1269488
#% 1269498

#index 1270000
#* Predictive exploration for autonomous science
#@ David R. Thompson
#t 2007
#c 10
#% 924022
#! Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.

#index 1270001
#* Spatial reference resolution for an embodied dialogue agent
#@ Timothy Weale
#t 2007
#c 10

#index 1270002
#* An incentive mechanism for promoting honesty in e-marketplaces
#@ Jie Zhang
#t 2007
#c 10
#% 314935
#% 773504
#% 943777

#index 1270003
#* AURA: enabling subject matter experts to construct declarative knowledge bases from science textbooks
#@ Ken Barker;Vinay K. Chaudhri;Shaw-Yi Chaw;Peter E. Clark;Daniel Hansch;Bonnie E. John;Sunil Mishra;John Pacheco;Bruce Porter;Aaron Spaulding;Moritz Weiten
#t 2007
#c 10
#% 341641
#! The long-term goal of Project Halo is to build an application called Digital Aristotle that can answer questions on a variety of science topics and provide user and domain appropriate explanations. As a near-term goal, we are focusing on enabling Subject Matter Experts (SMEs) to construct declarative knowledge bases (KBs) from 50 pages of a science textbook in the domains of Physics (Giancoli 2004), Chemistry (Brown et al. 2003) and Biology (Campbell et al. 2001) in a way that the system can answer questions similar to those on an Advanced Placement (AP) exam. We will demonstrate the current state of a system called AURA that we have been developing as a contributing technology toward the goal of Digital Aristotle. The innovative features of AURA are that it supports knowledge formulation for a mixture of textual and nontextual knowledge, and question formulation using an interactive dialog based on simplified English. The nontextual knowledge may contain tables, chemical reactions, and mathematical equations. In an extensive usability testing of AURA, we have established the basic viability of the approach.

#index 1270004
#* Freebase: a shared database of structured general human knowledge
#@ Kurt Bollacker;Robert Cook;Patrick Tufts
#t 2007
#c 10
#% 198055
#% 252750
#! Freebase is a practical, scalable, graph-shaped database of structured general human knowledge, inspired by Semantic Web research and collaborative data communities such as the Wikipedia. Freebase allows public read and write access through an HTTP-based graph-query API for research, the creation and maintenance of structured data, and application building. Access is free and all data in Freebase has a very open (e.g. Creative Commons, GFDL) license.

#index 1270005
#* Intelligent systems demonstration: disaster evacuation support
#@ Christopher J. Carpenter;Christopher J. Dugan;Joseph B. Kopena;Robert N. Lass;Gaurav Naik;Duc N. Nguyen;Evan Sultanik;Pragnesh Jay Modi;William C. Regli
#t 2007
#c 10
#% 643099
#% 888779
#! This demonstration presents an application of distributed constraint optimization and wireless networking to the task of assigning evacuees to available shelters during an emergency evacuation.

#index 1270006
#* The PhotoSlap game: play to annotate
#@ Tsung-Hsiang Chang;Chien-Ju Ho;Jane Yung-Jen Hsu
#t 2007
#c 10
#% 751818
#! This paper presents PhotoSlap, an intelligent system for semantic annotation of photos. The system contains a semi-automatic face detector, a bulk annotation tool, and a multi-player online game, PhotoSlap. By exploring the design principles of gameplay and applying game theoretic analysis, PhotoSlap is designed as a fun and productive game, which adapts itself to different players to produce the desired output. Experiments involving four focus groups showed the game to be fun and effective in annotating people metadata for personal photo collections.

#index 1270007
#* A demonstration of ScriptEase interruptible and resumable behaviors for CRPGs
#@ Maria Cutumisu;Duane Szafron;Jonathan Schaeffer;Kevin Waugh;Curtis Onuczko;Jeff Siegel;Allan Schumacher
#t 2007
#c 10
#% 902302
#! Intelligent non-player characters that exhibit realistic ambient behaviors produce more captivating and immersive stories for the player. However, the creation of nonrepetitive and entertaining behaviors is challenging, since it involves writing complex custom scripting code for thousands of characters in a common game adventure. This demonstration describes the generation of motivational behavior scripts using generative behavior patterns with ScriptEase. We demonstrate interruptible and resumable motivational ambient and latent behaviors for a tavern scene in a custom Neverwinter Nights game module.

#index 1270008
#* The more the merrier: multi-party negotiation with virtual humans
#@ Patrick Kenny;Arno Hartholt;Jonathan Gratch;David Traum;Stacy Marsella;Bill Swartout
#t 2007
#c 10
#% 445555
#% 895698
#% 1250404
#! The goal of the Virtual Humans Project at the University of Southern California's Institute for Creative Technologies is to enrich virtual training environments with virtual humans - autonomous agents that support face-to-face interaction with trainees in a variety of roles - through bringing together many different areas of research including speech recognition, natural language understanding, dialogue management, cognitive modeling, emotion modeling, nonverbal behavior and speech and knowledge management. The demo at AAAI will focus on our work using virtual humans to train negotiation skills. Conference attendees will negotiate with a virtual human doctor and elder to try to move a clinic out of harm's way in single and multiparty negotiation scenarios using the latest iteration of our Virtual Humans framework. The user will use natural speech to talk to the embodied agents, who will respond in accordance with their internal task model and state. The characters will carry out a multi-party dialogue with verbal and nonverbal behavior. A video of a single-party version of the scenario was shown at AAAI-06. This new interactive demo introduces several new features, including multiparty negotiation, dynamically generated non-verbal behavior and a central ontology.

#index 1270009
#* A deployed semantically-enabled interdisciplinary virtual observatory
#@ Deborah McGuinness;Peter Fox;Luca Cinquini;Patrick West;Jose Garcia;James L. Benedict;Don Middleton
#t 2007
#c 10
#% 1270142
#! We have used semantic technologies to design, implement, and deploy an interdisciplinary virtual observatory. The Virtual Solar-Terrestrial Observatory is a production data framework providing access to observational datasets. It is in use by a community of scientists, students, and data providers interested in the middle and upper Earth's atmosphere, and the Sun. The data sets span upper atmospheric terrestrial physics to solar physics. The observatory allows virtual access to a highly distributed and heterogeneous set of data that appears as if all resources are organized, stored and accessible from a local machine. The system has been operational since the summer of 2006 and has shown registered data access by over 75% of the active community (last count over 600 of the estimated 800 person active research community). This demonstration will highlight how semantic technologies are being used to support data integration and more efficient data access in a multi-disciplinary setting. A full paper on this work is being published in the IAAI 07 'deployed' paper track.

#index 1270010
#* Generating and solving logic puzzles through constraint satisfaction
#@ Barry O'Sullivan;John Horan
#t 2007
#c 10
#% 160208
#% 644201
#! Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a 9 × 9 grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.

#index 1270011
#* An interactive constraint-based approach to Sudoku
#@ Christopher G. Reeson;Kai-Chen Huang;Kenneth M. Bayer;Berthe Y. Choueiry
#t 2007
#c 10

#index 1270012
#* A mixed reality approach to undergraduate robotics education
#@ John Anderson;Jacky Baltes
#t 2007
#c 10
#! Teaching robotics to undergraduate students requires a course framework that allows students to learn about robotics in stages, without being overwhelmed with details. Such a framework must also provide the students with a motivating application environment that challenges them to apply what they have learned. Robotics competitions have proven to be an excellent method for motivating students, so the framework should be portable and robust enough to be used for competitions, and flexible enough to provide a range of environments that can become more challenging as students become more adept. Finally, the framework should provide repeatability and control for evaluating the student's work, as well as for performing research. In this paper, we overview a mixed reality approach that meets these criteria, and describe its use in an advanced undergraduate course.

#index 1270013
#* KSU Willie in semantic vision challenge
#@ David Gustafson;Aaron Chavez;Michael Marlen;Andrew King;Alejandro Alliana;Ondrej Linda
#t 2007
#c 10
#! The KSU Willie entry in the Semantic Vision Challenge will use a variety of classifiers, some standard classifiers and some newly developed classifiers, to learn the classification of images downloaded from the web. KSU Willie will use those classifiers to identify objects in the environment.

#index 1270014
#* The UBC semantic robot vision system
#@ Scott Helmer;David Meger;Per-Erik Forssén;Tristram Southey;Sancho McCann;Pooyan Fazli;Jim Little;David Lowe
#t 2007
#c 10
#% 736300
#% 760805
#% 939007
#% 945194
#% 964935

#index 1270015
#* A robotic weight loss coach
#@ Cory D. Kidd;Cynthia Breazeal
#t 2007
#c 10
#% 345245
#% 350689
#% 856887
#% 946165
#% 946203
#! We present a rationale for studying long-term human-robot interaction and explain why new applications are necessary for this type of experimentation. The design and implementation of a robot that has been implemented is briefly described with the outline of a study that is under way.

#index 1270016
#* OPTIMOL: a framework for online picture collection via incremental model learning
#@ Li-Jia Li;Juan Carlos Niebles;Li Fei-Fei
#t 2007
#c 10
#% 778740
#% 784995
#! OPTIMOL (a framework for Online Picture collection via Incremental MOdel Learning) is a novel, automatic dataset collecting and model learning system for object categorization. Our algorithm mimics the human learning process in such a way that, starting from a few training examples, the more confident data you incorporate in the training data, the more reliahle models can be learnt. Our system uses the Internet as the (nearly) unlimited resource for images. The learning and image collection processes are done via an iterative and incremental scheme. The goal of this work is to use this tremendous web resource to learn robust object category models in order to detect and search for objects in real-world scenes.

#index 1270017
#* An implementation of robot formations using local interactions
#@ Ross Mead;Jerry B. Weinberg;Jeffrey R. Croxell
#t 2007
#c 10
#% 788934
#% 1250461
#! Coordinating a group of robots to work in formation has been suggested for a number of tasks, such as urban search-and-rescue, traffic control, and harvesting solar energy. Algorithms for controlling robot formations have been inspired by biological and organizational systems. In our approach to robot formation control, each robot is treated like a cell in a cellular automaton, where local interactions between robots result in a global organization. The algorithm has been demonstrated in simulation. In this paper, we present a physical implementation.

#index 1270018
#* Proceedings of the 23rd national conference on Artificial intelligence - Volume 1
#@ Anthony Cohn
#t 2008
#c 10

#index 1270019
#* AAAI preface
#@ Dieter Fox;Carla Gomes
#t 2008
#c 10

#index 1270020
#* Memetic networks: analyzing the effects of network properties in multi-agent performance
#@ Ricardo M. Araujo;Luis C. Lamb
#t 2008
#c 10
#% 65643
#% 174161
#% 294751
#% 328908
#% 369236
#% 379974
#% 387861
#% 466354
#% 616104
#% 643246
#% 686757
#% 773391
#% 871315
#% 876290
#% 959904
#% 1024693
#% 1269677
#% 1274785
#% 1831268
#! We explore the relationship between properties of the network defined by connected agents and the global system performance. This is achieved by means of a novel class of optimization algorithms. This new class makes explicit use of an underlying network that structures the information flow between multiple agents performing local searches. We show that this new class of algorithms is competitive with respect to other population-based optimization techniques. Finally, we demonstrate by numerical simulations that changes in the way the network is built leads to variations in the system's performance. In particular, we show how constrained hubs - highly connected agents - can be beneficial in particular optimization problems.

#index 1270021
#* Physical search problems applying economic search models
#@ Yonatan Aumann;Noam Hazon;Sarit Kraus;David Sarne
#t 2008
#c 10
#% 363744
#% 493873
#% 1269384
#! This paper considers the problem of an agent searching for a resource or a tangible good in a physical environment, where at each stage of its search it observes one source where this good can be found. The cost of acquiring the resource or good at a given source is uncertain (a-priori), and the agent can observe its true value only when physically arriving at the source. Sample applications involving this type of search include agents in exploration and patrol missions (e.g., an agent seeking to find the best location to deploy sensing equipment along its path). The uniqueness of these settings is that the expense of observing the source on each step of the process derives from the last source the agent explored. We analyze three variants of the problem, differing in their objective: minimizing the total expected cost, maximizing the success probability given an initial budget, and minimizing the budget necessary to obtain a given success probability. For each variant, we first introduce and analyze the problem with a single agent, either providing a polynomial solution to the problem or proving it is NP-Complete. We also introduce an innovative fully polynomial time approximation scheme algorithm for the minimum budget variant. Finally, the results for the single agent case are generalized to multi-agent settings.

#index 1270022
#* A theory of expressiveness in mechanisms
#@ Michael Benisch;Norman Sadeh;Tuomas Sandholm
#t 2008
#c 10
#% 66937
#% 341922
#% 345429
#% 818584
#% 868452
#% 918842
#% 1269398
#% 1270024
#% 1272153
#% 1407360
#% 1407362
#% 1650358
#! A key trend in (electronic) commerce is a demand for higher levels of expressiveness in the mechanisms that mediate interactions. We develop a theory that ties the expressiveness of mechanisms to their efficiency in a domain-independent manner. We introduce two new expressiveness measures, 1) maximum impact dimension, which captures the number of ways that an agent can impact the outcome, and 2) shatterable outcome dimension, which is based on the concept of shattering from computational learning theory. We derive an upper bound on the expected efficiency of any mechanism under its most efficient Nash equilibrium. Remarkably, it depends only on the mechanism's expressiveness. We prove that the bound increases strictly as we allow more expressiveness. We also show that in some cases a small increase in expressiveness yields an arbitrarily large increase in the bound. Finally, we study channel-based mechanisms, which subsume most combinatorial auctions, multi-attribute mechanisms, and the Vickrey-Clarke-Groves scheme. We show that our domain-independent measures of expressiveness appropriately relate to the natural measure of expressiveness of channel-based mechanisms: the number of channels allowed. Using this bridge, our general results yield interesting implications. For example, any (channel-based) multi-item auction that does not allow rich combinatorial bids can be arbitrarily inefficient--unless agents have no private information.

#index 1270023
#* Multiagent graph coloring: Pareto efficiency, fairness and individual rationality
#@ Yaad Blum;Jeffrey S. Rosenschein
#t 2008
#c 10
#% 693
#% 122318
#% 145874
#% 162305
#% 275929
#% 443227
#% 750574
#! We consider a multiagent extension of single-agent graph coloring. Multiple agents hold disjoint autonomous subgraphs of a global graph, and every color used by the agents in coloring the graph has associated cost. In this multi agent graph coloring scenario, we seek a minimum legal coloring of the global graph's vertices, such that the coloring is also Pareto efficient, socially fair, and individual rational. We analyze complexity of individual-rational solutions in special graph classes where classical coloring algorithms are known. Multiagent graph coloring has application to a wide variety of multi agent coordination problems, including multiagent scheduling.

#index 1270024
#* Expressive banner ad auctions and model-based online optimization for clearing
#@ Craig Boutilier;David C. Parkes;Tuomas Sandholm;William E. Walsh
#t 2008
#c 10
#% 836518
#% 868468
#% 868473
#% 931104
#% 950864
#% 963332
#% 963359
#% 1250198
#% 1269679
#% 1269684
#% 1270022
#% 1407360
#% 1407362
#! We present the design of a banner advertising auction which is considerably more expressive than current designs. We describe a general model of expressive ad contract/bidding and an allocation model that can be executed in real time through the assignment of fractions of relevant ad channels to specific advertiser contracts. The uncertainty in channel supply and demand is addresscd by the formulation of a stochastic combinatorial optimization problem for channel allocation that is rerun periodically. We solve this in two different ways: fast deterministic optimization with respect to expectations; and a novel online sample-based stochastic optimization method-- that can be applied to continuous decision spaces--which exploits the deterministic optimization as a black box. Experiments demonstrate the importance of expressive bidding and the value of stochastic optimization.

#index 1270025
#* A computational analysis of the tournament equilibrium set
#@ Felix Brandt;Felix Fischer;Paul Harrenstein;Maximilian Mair
#t 2008
#c 10
#% 70370
#% 198464
#% 992253
#% 1021248
#% 1250604
#% 1250605
#% 1269780
#! A recurring theme in AI and multiagent systems is how to select the "most desirable" elements given a binary dominance relation on a set of alternatives. Schwartz's tournament equilibrium set (TEQ) ranks among the most intriguing, but also among the most enigmatic, tournament solutions proposed so far in this context. Due to its unwieldy recursive definition, little is known about TEQ. In particular, its monotonicity remains an open problem to date. Yet, if TEQ were to satisfy monotonicity, it would be a very attractive solution concept refining both the Banks set and Dutta's minimal covering set. We show that the problem of deciding whether a given alternative is contained in TEQ is NP-hard. Furthermore, we propose a heuristic that significantly outperforms the naive algorithm for computing TEQ. Early experimental results support the conjecture that TEQ is indeed monotonic.

#index 1270026
#* Approximability of manipulating elections
#@ Eric Brelsford;Piotr Faliszewski;Edith Hemaspaandra;Henning Schnoor;Ilka Schnoor
#t 2008
#c 10
#% 330769
#% 408396
#% 890278
#% 940734
#% 951820
#% 953322
#% 1039608
#% 1084430
#% 1250606
#% 1250608
#% 1269785
#% 1274991
#% 1698228
#! In this paper, we set up a framework to study approximation of manipulation, control, and bribery in elections. We show existence of approximation algorithms (even fully polynomial time approximation schemes) as well as obtain inapproximability results.

#index 1270027
#* Efficient metadeliberation auctions
#@ Ruggiero Cavallo;David C. Parkes
#t 2008
#c 10
#% 28714
#% 781210
#% 782311
#% 819415
#% 823925
#% 890388
#% 1407354
#! Imagine a resource allocation scenario in which the interested parties can, at a cost, individually research ways of using the resource to be allocated, potentially increasing the value they would achieve from obtaining it. Each agent has a private model of its research process and obtains a private realization of its improvement in value, if any. From a social perspective it is optimal to coordinate research in a way that strikes the right tradeoff between value and cost, ultimately allocating the resource to one party- thus this is a problem of multi-agent metadeliberation. We provide a reduction of computing the optimal deliberation-allocation policy to computing Gittins indices in multi-anned bandit worlds, and apply a modification of the dynamic-VCG mechanism to yield truthful participation in an ex post equilibrium. Our mechanism achieves equilibrium implementation ofthe optimal policy even when agents have the capacity to deliberate about other agents' valuations, and thus addresses the problem of strategic deliberation.

#index 1270028
#* Achieving cooperation in a minimally constrained environment
#@ Steven Damer;Maria Gini
#t 2008
#c 10
#% 417720
#% 643115
#% 840855
#% 1665125
#% 1760888
#! We describe a simple environment to study cooperation between two agents and a method of achieving cooperation in that environment. The environment consists of randomly generated normal form games with uniformly distributed pay-offs. Agents play multiple games against each other, each game drawn independently from the random distribution. In this environment cooperation is difficult. Tit-for-Tat cannot be used because moves are not labeled as "cooperate" or "defect", fictitious play cannot be used because the agent never sees the same game twice, and approaches suitable for stochastic games cannot be used because the set of states is not finite. Our agent identifies cooperative moves by assigning an attitude to its opponent and to itself. The attitude determines how much a player values its opponents payoff, i.e how much the player is willing to deviate from strictly selfinterested behavior. To cooperate, our agent estimates the attitude of its opponent by observing its moves and reciprocates by setting its own attitude accordingly. We show how the opponent's attitude can be estimated using a particle filter, even when the opponent is changing its attitude.

#index 1270029
#* Generalized point based value iteration for interactive POMDPs
#@ Prashant Doshi;Dennis Perez
#t 2008
#c 10
#% 252183
#% 1250335
#% 1250351
#% 1269512
#% 1272071
#% 1272075
#% 1272129
#% 1274892
#% 1279314
#! We develop a point based method for solving finitely nested interactive POMDPs approximately. Analogously to point based value iteration (PBVI) in POMDPs, we maintain a set of belief points and form value functions composed of those value vectors that are optimal at these points. However, as we focus on muItiagent settings, the beliefs are nested and computation of the value vectors relies on predicted actions of others. Consequently, we develop a novel interactive gen eralization of PBVI applicable to muItiagent settings.

#index 1270030
#* On the dimensionality of voting games
#@ Edith Elkind;Leslie Ann Goldberg;Paul Goldberg;Michael Wooldridge
#t 2008
#c 10
#% 165011
#% 1269784
#! In a yes/no voting game, a set of voters must determine whether to accept or reject a given alternative. Weighted voting games are a well-studied subclass of yes/no voting games, in which each voter has a weight, and an alternative is accepted if the total weight of its supporters exceeds a certain threshold. Weighted voting games are naturally extended to k-vector weighted voting games, which are intersections of k different weighted voting games: a coalition wins if it wins in every component game. The dimensionality, k, of a k- vector weighted voting game can be understood as a measure of the complexity of the game. In this paper, we analyse the dimensionality of such games from the point of view of complexity theory. We consider the problems of equivalence, (checking whether two given voting games have the same set of winning coalitions), and minimality, (checking whether a given k-vector voting game can be simplified by deleting one of the component games, or, more generally, is equivalent to a k′-weighted voting game with k′ k). We show that these problems are computationally hard, even if k = 1 or all weights are 0 or 1. However, we provide efficient algorithms for cases where both k is small and the weights are polynomially bounded. We also study the notion of monotonicity in voting games, and show that monotone yes/no voting games are essentially as hard to represent and work with as general games.

#index 1270031
#* First-order algorithm with O(ln(1/ε )) convergence for ε -equilibrium in two-person zero-sum games
#@ Andrew Gilpin;Javier Peña;Thomas Sandholm
#t 2008
#c 10
#% 238393
#% 803567
#% 826273
#% 1269678
#% 1269795
#% 1407337
#! We propose an iterated version of Nesterov's first-order smoothing method for the two-person zero-sum game equilibrium problem minx∈Q1 maxy∈Q2 xT Ay = maxy∈Q2 minx∈Q1 xTAy. This formulation applies to matrix games as well as sequential games. Our new algorithmic scheme computes an Ε-equilibrium to this min-max problem in O(κ(A) In(1/Ε)) first-order iterations, where κ(A) is a certain condition measure of the matrix A. This improves upon the previous first-order methods which required O(1/Ε) iterations, and it matches the iteration complexity bound of interior-point methods in terms of the algorithm's dependence on Ε. Unlike the interior-point methods that are inapplicable to large games due to their memory requirements, our algorithm retains the small memory requirements of prior first-order methods. Our scheme supplements Nesterov's algorithm with an outer loop that lowers the target Ε between iterations (this target affects the amount of smoothing in the inner loop). We find it surprising that such a simple modification yields an exponential speed improvement. Finally, computational experiments both in matrix games and sequential games show that a significant speed improvement is obtained in practice as well, and the relative speed improvement increases with the desired accuracy (as suggested by the complexity bounds).

#index 1270032
#* Agent organized networks redux
#@ Robin Glinton;Katia Sycara;Paul Scerri
#t 2008
#c 10
#% 557204
#% 823874
#% 823895
#% 823950
#% 880014
#% 1269371
#% 1275313
#! Individual robots or agents will often need to form coalitions to accomplish shared tasks, e.g., in sensor networks or markets. Furthermore, in most real systems it is infeasible for entities to interact with all peers. The presence of a social network can alleviate this problem by providing a neighborhood system within which entities interact with a reduced number of peers. Previous research has shown that the topology of the underlying social network has a dramatic effect on the quality of coalitions formed and consequently on system performance (Gaston & deslardins 2005a). It has also been shown that it is feasible to develop agents which dynamically alter connections to improve an organization's ability to form coalitions on the network. However those studies have not analysed the network topologies that result from connectivity adaptation strategies. In this paper the resulting network topologies were analysed and it was found that high performance and rapid convergence were attained because scale free networks were being formed. However it was observed that organizational performance is not impacted by limiting the number of links per agent to the total number of skills available within the population. implying that bandwidth was wasted by previous approaches. We used these observations to inform the design of a token based algorithm that attains higher performance using an order of magnitude less messages for both uniform and non-uniform distributions of skills.

#index 1270033
#* Reasoning about the appropriateness of proponents for arguments
#@ Anthony Hunter
#t 2008
#c 10
#% 198464
#% 337502
#% 417812
#% 752766
#% 873958
#% 1269451
#% 1692808
#! Formal approaches to modelling argumentation provide ways to present arguments and counterarguments, and to evaluate which arguments are, in a formal sense, warranted. While these proposals allow for evaluating object-level arguments and counterarguments, they do not give sufficient consideration to evaluating the proponents of the arguments. Yet in everyday life we consider both the contents of an argument and its proponent. So if we do not trust a proponent, we may choose to not trust their arguments. Or if we are faced with an argument that we do not have the expertise to assess (for example when deciding whether to agree to having a particular surgical operation), we tend to agree to an argument by someone who is an expert. In general, we see that for each argument, we need to determine the appropriateness of the proponent for it. So for an argument about our health, our doctor is normally an appropriate proponent, but for an argument about our investments, our doctor is normally not an appropriate proponent. In this way, a celebrity is rarely an appropriate proponent for an argument, and a liar is not necessarily an inappropriate proponent for an argument. In this paper, we provide a logic-based framework for evaluating arguments in terms of the appropriateness of the proponents.

#index 1270034
#* Bayesian coalitional games
#@ Samuel Ieong;Yoav Shoham
#t 2008
#c 10
#% 188086
#% 233135
#% 334648
#% 643080
#% 725207
#% 773332
#% 1024740
#% 1138616
#% 1269439
#% 1274951
#! We introduce Bayesian Coalitional Games (BCGs), a generalization of classical coalitional games to settings with uncertainties. We define the semantics of BCG using the partition model, and generalize the notion of payoffs to contracts among agents. To analyze these games, we extend the solution concept of the core under three natural interpretations-- ex ante, ex interim, and ex post--which coincide with the classical definition of the core when there is no uncertainty. In the special case where agents are risk-neutral, we show that checking for core emptiness under all three interpretations can be simplified to linear feasibility problems similar to that of their classical counterpart.

#index 1270035
#* Agent coordination with regret clearing
#@ Sven Koenig;Xiaoming Zheng;Craig Tovey;Richard Borie;Philip Kilby;Vangelis Markakis;Pinar Keskinocak
#t 2008
#c 10
#% 531449
#% 704123
#% 1250424
#% 1274972
#! Sequential single-item auctions can be used for the distributed allocation of tasks to cooperating agents. We study how to improve the team performance of sequential single-item auctions while still controlling the agents in real time. Our idea is to assign that task to agents during the current round whose regret is large, where the regret of a task is defined as the difference of the second-smallest and smallest team costs resulting from assigning the task to the second-best and best agent, respectively. Our experimental results show that sequential single-item auctions with regret clearing indeed result in smaller team costs than standard sequential single-item auctions for three out of four combinations of two different team objectives and two different capacity constraints (including no capacity constraints).

#index 1270036
#* An expressive auction design for online display advertising
#@ Sébastien Lahaie;David C. Parkes;David M. Pennock
#t 2008
#c 10
#% 314918
#% 578710
#% 963333
#% 1289307
#% 1289313
#! We propose an expressive auction design that allows advertisers to specify the kinds of demographics and websites they wish to target within an advertising network. The design allows the network to differentiate impressions according to relevant attributes (e.g., geographic location of the user, topic of the webpage). Advertisers can place bids for different kinds of impressions according to their attributes, and can also specify volume constraints to control exposure. The novelty of the design is a bidding language that admits scalable allocation and pricing algorithms. We discuss the incentive properties of different pricing approaches. We also propose a bidder feedback mechanism to mitigate the complexity of expressive bidding.

#index 1270037
#* Computer-aided proofs of arrow's and other impossibility theorems
#@ Fangzhen Lin;Pingzhong Tang
#t 2008
#c 10
#% 174161
#% 336874
#% 342119
#! Arrow's Impossibility Theorem is one of the landmark results in social choice theory. Over the years since the theorem was proved in 1950, quite a few alternative proofs have been put forward. In this paper, we propose yet another alternative proof of the theorem. The basic idea is to use induction to reduce the theorem to the base case with 3 alternatives and 2 agents and then use computers to verify the base case. This turns out to be an effective approach for proving other impossibility theorems such as Sen's and Muller-Satterthwaite's theorems as well. Furthermore, we believe this new proof opens an exciting prospect of using computers to discover similar impossibility or even possibility results.

#index 1270038
#* Resource constrained distributed constraint optimization with virtual variables
#@ Toshihiro Matsui;Hiroshi Matsuo;Marius Silaghi;Katsutoshi Hirayama;Makoto Yokoo
#t 2008
#c 10
#% 126386
#% 773217
#% 773232
#% 823971
#% 855913
#% 890436
#% 1250618
#% 1289393
#! Cooperative problem solving with resource constraints are important in practical multi-agent systems. Resource constraints are necessary to handle practical problems including distributed task scheduling with limited resource availability. A dedicated framework called Resource Constrained DCOP (RCDCOP) has recently been proposed. RCDCOP models objective functions and resource constraints separately. A resource constraint is an n-ary constraint that represents the limit on the number of resources of a given type available to agents. Previous research addressing RCDCOPs employs the Adopt algorithm, which is an efficient solver for DCOPs. An important graph structure for Adopt is the pseudo-tree for constraint networks. A pseudo-tree implies a partial ordering of variables. In this variable ordering, n-ary constrained variables are placed on a single path of the tree. Therefore, resource constraints that have large arity augment the depth of the pseudo-tree. This also reduces the parallelism, and therefore the efficiency of Adopt. In this paper we propose another version of the Adopt algorithm for RCDCOP using a pseudo-tree that is generated ignoring resource constraints. The proposed method reduces the previous limitations in the construction of RCDCOP pseudo-trees. The key ideas of our work are as follows: (i) The pseudo-tree is generated ignoring resource constraints. (ii) Virtual variables are introduced, representing the usage of resources. These virtual variables are used to share resources among sub-trees. However, the addition of virtual variables increases the search space. To handle this problem, influence of placement of virtual variables/resources constraints in the pseudo tree is considered. Moreover the search is pruned using the bounds defined by the resource constraints if possible. These ideas are used to extend Adopt. The efficiency of our technique depends on the class of problems being considered, and we describe the obtained experimental results.

#index 1270039
#* Strategyproof classification under constant hypotheses: a tale of two functions
#@ Reshef Meir;Ariel D. Procaccia;Jeffrey S. Rosenschein
#t 2008
#c 10
#% 101898
#% 145156
#% 734918
#% 769885
#% 836548
#% 1013355
#% 1039674
#% 1269686
#! We consider the following setting: a decision maker must make a decision based on reported data points with binary labels. Subsets of data points are controlled by different selfish agents, which might misreport the labels in order to sway the decision in their favor. We design mechanisms (both deterministic and randomized) that reach an approximately optimal decision and are strategyproof, i.e., agents are best off when they tell the truth. We then recast our results into a classical machine learning classification framework, where the decision maker must make a decision (choose between the constant positive hypothesis and the constant negative hypothesis) based only on a sampled subset of the agents' points.

#index 1270040
#* Argument theory change applied to defeasible logic programming
#@ Martín O. Moguillansky;Nicolás D. Rotstein;Marcelo A. Falappa;Alejandro J. García;Guillermo R. Simari
#t 2008
#c 10
#% 173382
#% 428335
#% 752766
#! In this article we work on certain aspects of the belief change theory in order to make them suitable for argumentation systems. This approach is based on Defeasible Logic Programming as the argumentation formalism from which we ground the definitions. The objective of our proposal is to define an argument revision operator that inserts a new argument into a defeasible logic program in such a way that this argument ends up undefeated after the revision, thus warranting its conclusion. In order to ensure this warrant, the defeasible logic program has to be changed in concordance with a minimal change principle. Finally, we present an algorithm that implements the argument revision operation.

#index 1270041
#* The impact of vertical specialization on hierarchical multi-agent systems
#@ Steven Okamoto;Paul Scerri;Katia Sycara
#t 2008
#c 10
#% 741967
#% 758327
#% 774276
#% 823950
#% 839871
#% 1272316
#% 1704157
#! Hierarchies are one of the most common organizational structures observed in multi-agent systems. In this paper we study vertical specialization as a reason for hierarchical structures. In vertically specialized systems, more highly skilled agents are also more costly. By using less capable agents to initially process tasks and forwarding only exceptional tasks to more capable agents, such systems may be able to economize on the number of highly capable agents. The result is a hierarchical structure with least capable agents at the bottom. However, such a structure increases the delay in completing some tasks, because they must pass through multiple levels of control. Thus, vertical specialization presents a tradeoff between economizing on skilled agents and increasing task completion time. We find that for a wide range of settings, vertical specialization induces an optimal hierarchy of height at most three. This suggests that a multi-agent system designer interested in exploiting vertical specialization needs to use at most three levels of specialization in order to reap most of the benefits.

#index 1270042
#* Coordination and multi-tasking using EMT
#@ Zinovi Rabinovich;Nir Pochter;Jeffrey S. Rosenschein
#t 2008
#c 10
#% 165663
#% 232728
#% 251997
#% 357083
#% 379189
#% 384911
#% 465915
#% 823898
#% 824082
#% 890271
#% 1024749
#% 1024797
#% 1275151
#! We introduce a multi-model variant of the EMT-based control algorithm. The new algorithm, MM-EMT, is capable of balancing several control tasks expressed using separate dynamic models with a common action space. Such multiple models are common in both single-agent environments, when the agent has multiple tasks to achieve, and in team activities, when agent actions affect both the local agent's task as well as the overall team's coordination. To demonstrate the behaviour that MM-EMT engenders, several experimental setups were devised. Simulation results support the effectiveness of the approach, which in the multi-agent scenario is expressed in the MM-EMT algorithm's ability to balance local and team-coordinated motion requirements.

#index 1270043
#* Pareto optimality in abstract argumentation
#@ Iyad Rahwan;Kate Larson
#t 2008
#c 10
#% 198464
#% 992251
#% 1083990
#% 1221649
#% 1664525
#! Since its introduction in the mid-nineties, Dung's theory of abstract argumentation frameworks has been influential in artificial intelligence. Dung viewed arguments as abstract entities with a binary defeat relation among them. This enabled extensive analysis of different (semantic) argument acceptance criteria. However, little attention was given to comparing such criteria in relation to the preferences of self-interested agents who may have conflicting preferences over the final status of arguments. In this paper, we define a number of agent preference relations over argumentation outcomes. We then analyse different argument evaluation rules taking into account the preferences of individual agents. Our framework and results inform the mediator (e.g. judge) to decide which argument evaluation rule (i.e. semantics) to use given the type of agent population involved.

#index 1270044
#* Coalition structure generation: dynamic programming meets anytime optimization
#@ Talal Rahwan;Nicholas R. Jennings
#t 2008
#c 10
#% 4382
#% 233135
#% 243206
#% 252199
#% 284645
#% 659853
#% 1084394
#% 1250607
#% 1269859
#% 1275134
#! Coalition structure generation involves partitioning a set of agents into exhaustive and disjoint coalitions so as to maximize the social welfare. What makes this such a challenging problem is that the number of possible solutions grows exponentially as the number of agents increases. To date, two main approaches have been developed to solve this problem, each with its own strengths and weaknesses. The state of the art in the first approach is the Improved Dynamic Programming (IDP) algorithm, due to Rahwan and Jennings, that is guaranteed to find an optimal solution in O(3n), but which cannot generate a solution until it has completed its entire execution. The state of the art in the second approach is an anytime algorithm called IP, due to Rahwan et aI., that provides worst-case guarantees on the quality of the best solution found so far, but which is O(nn). In this paper, we develop a novel algorithm that combines both IDP and IP, resulting in a hybrid performance that exploits the strength of both algorithms and, at the same, avoids their main weaknesses. Our approach is also significantly faster (e.g. given 25 agents, it takes only 28% of the time required by IP, and 0.3% of the time required by IDP).

#index 1270045
#* Partially-synchronized DEC-MDPs in dynamic mechanism design
#@ Sven Seuken;Ruggiero Cavallo;David C. Parkes
#t 2008
#c 10
#% 10253
#% 450852
#% 496272
#% 711934
#% 1090422
#% 1272045
#% 1272052
#! In this paper, we combine for the first time the methods of dynamic mechanism design with techniques from decentralized decision making under uncertainty. Consider a multi-agent system with self-interested agents acting in an uncertain environment, each with private actions, states and rewards. There is also a social planner with its own actions, rewards, and states, acting as a coordinator and able to influence the agents via actions (e.g., resource allocations). Agents can only communicate with the center, but may become inaccessible, e.g., when their communication device fails. When accessible to the center, agents can report their local state (and models) and receive recommendations from the center about local policies to follow for the present period and also, should they become inaccessible, until becoming accessible again. Without self-interest, this poses a new problem class which we call partially-synchronized DEC-MDPs, and for which we establish some positive complexity results under reasonable assumptions. Allowing for self-interested agents, we are able to bridge to methods of dynamic mechanism design, aligning incentives so that agents truthfully report local state when accessible and choose to follow the prescribed "emergency policies" of the center.

#index 1270046
#* Mathematical modeling and convergence analysis of trail formation
#@ Sameena Shah;Ravi Kothari;Jayadeva Jayadeva;Suresh Chandra
#t 2008
#c 10
#% 294103
#% 311680
#% 449998
#% 743016
#% 860400
#% 876089
#% 1734197
#% 1777042
#% 1777223
#! An ant deposits pheromone along the path that it travels and is more likely to choose a path with a higher concentration of pheromone. The sensing and dropping of pheromone makes it easy to understand the trail forming behavior of ants. The reinforcement tendency of pheromone following behavior ensures selection of the shortest path from a set of paths. The reinforcement tendency of pheromone following behavior also ensures a biased selection of the initially followed paths over a path, which is shorter but discovered through chance at a later point in time. Under what conditions and limits can this initial bias be reversed? In this paper, we answer this question based on a theoretical analysis of the trail forming behavior of ants. We believe our results to contribute to the overall area of understanding how to build scalable systems that evolve to solve complex problems (e.g. point covering or the traveling salesman problem) without the necessity of central command-and-control.

#index 1270047
#* Semantical considerations on dialectical and practical commitments
#@ Munindar P. Singh
#t 2008
#c 10
#% 101955
#% 420767
#% 557553
#% 643145
#% 890423
#% 931336
#% 1021221
#% 1083961
#% 1270266
#% 1675011
#% 1704221
#! This paper studies commitments in multi agent systems. A dialectical commitment corresponds to an agent taking a position about a putative fact, including for the sake of argument. A practical commitment corresponds to an agent being obliged to another to bring about a condition. Although commitments have been used in many works, an adequate formal semantics and axiomatization for them does not yet exist. This paper presents a logic of commitments that illustrates the commonalities and differences of the two kinds of commitments. In this manner, it generalizes the developments of previous papers, precisely delineates the meanings of commitments, and identifies important postulates used informally or semiformally in previous work.

#index 1270048
#* Bidding strategies for realistic multi-unit sealed-bid auctions
#@ Ioannis A. Vetsikas;Nicholas R. Jennings
#t 2008
#c 10
#% 643123
#% 781210
#% 808362
#% 1024729
#% 1024779
#% 1084443
#% 1269692
#! When autonomous agents decide on their bidding strategies in real world auctions, they have a number of concerns that go beyond the models that are normally analyzed in traditional auction theory. Oftentimes, the agents have budget constraints and the auctions have a reserve price, both of which restrict the bids the agents can place. In addition, their attitude need not be risk-neutral and they may have uncertainty about the value of the goods they are buying. Some of these issues have been examined individually for single-unit sealed-bid auctions. In this paper, we extend this analysis to the multi-unit case, and also analyze the multi-unit sealed-bid auctions in which a combination of these issues are present, for unit-demand bidders. This analysis constitutes the main contribution of this paper. We then demonstrate the usefulness in practice of this analysis; we show in simulations that taking into account all these issues allows the bidders to maximize their utility. Furthermore, using this analysis allows a seller to improve her revenue, Le. by selecting the optimal reserve price.

#index 1270049
#* Optimal false-name-proof voting rules with costly voting
#@ Liad Wagman;Vincent Conitzer
#t 2008
#c 10
#% 341408
#% 736253
#% 890384
#% 1021250
#% 1269688
#% 1279318
#% 1394505
#% 1650358
#! One way for agents to reach a joint decision is to vote over the alternatives. In open, anonymous settings such as the Internet, an agent can vote more than once without being detected. A voting rule is false-name-proof if no agent ever benefits from casting additional votes. Previous work has shown that all false-name-proof voting rules are unresponsive to agents' preferences. However, that work implicitly assumes that casting additional votes is costless. In this paper, we consider what happens if there is a cost to casting additional votes. We characterize the optimal (most responsive) false-name-proofwith-costs voting rule for 2 alternatives. In sharp contrast to the costless setting, we prove that as the voting population grows larger, the probability that this rule selects the majority winner converges to 1. We also characterize the optimal group false-name-proof rule for 2 alternatives, which is robust to coalitions of agents sharing the costs of additional votes. Unfortunately, the probability that this rule chooses the majority winner as the voting population grows larger is relatively low. We derive an analogous rule in a setting with 3 alternatives, and provide bounding results and computational approaches for settings with 4 or more alternatives.

#index 1270050
#* Determining possible and necessary winners under common voting rules given partial orders
#@ Lirong Xia;Vincent Conitzer
#t 2008
#c 10
#% 242217
#% 578715
#% 808366
#% 951820
#% 1021271
#% 1024741
#% 1039608
#% 1269671
#% 1269793
#% 1274973
#% 1274974
#% 1274989
#% 1650274
#% 1698228
#! Usually a voting rule or correspondence requires agents to give their preferences as linear orders. However, in some cases it is impractical for an agent to give a linear order over all the alternatives. It has been suggested to let agents submit partial orders instead. Then, given a profile of partial orders and a candidate c, two important questions arise: first, is c guaranteed to win, and second, is it still possible for c to win? These are the necessary winner and possible winner problems, respectively. We consider the setting where the number of alternatives is unbounded and the votes are unweighted. We prove that for Copeland, maximin, Bucklin, and ranked pairs, the possible winner problem is NP-complete; also, we give a sufficient condition on scoring rules for the possible winner problem to be NP-complete (Borda satisfies this condition). We also prove that for Copeland and ranked pairs, the necessary winner problem is coNP-complete. All the hardness results hold even when the number of undetermined pairs in each vote is no more than a constant. We also present polynomial-time algorithms for the necessary winner problem for scoring rules, maximin, and Bucklin.

#index 1270051
#* Voting on multiattribute domains with cyclic preferential dependencies
#@ Lirong Xia;Vincent Conitzer;Jérôme Lang
#t 2008
#c 10
#% 879497
#% 1021248
#% 1021271
#% 1024855
#% 1250233
#% 1269793
#% 1272026
#% 1274973
#% 1289373
#% 1650274
#! In group decision making, often the agents need to decide on multiple attributes at the same time, so that there are exponentially many alternatives. In this case, it is unrealistic to ask agents to communicate a full ranking of all the alternatives. To address this, earlier work has proposed decomposing such voting processes by using local voting rules on the individual attributes. Unfortunately, the existing methods work only with rather severe domain restrictions, as they require the voters' preferences to extend acyclic CP-nets compatible with a common order on the attributes. We first show that this requirement is very restrictive, by proving that the number of linear orders extending an acyclic CP-net is exponentially smaller than the number of all linear orders. Then, we introduce a very general methodology that allows us to aggregate preferences when voters express CP-nets that can be cyclic. There does not need to be any common structure among the submitted CP-nets. Our methodology generalizes the earlier, more restrictive methodology. We study whether properties of the local rules transfer to the global rule, and vice versa. We also address how to compute the winning alternatives.

#index 1270052
#* Value-based policy teaching with active indirect elicitation
#@ Haoqi Zhang;David Parkes
#t 2008
#c 10
#% 363744
#% 408396
#% 464274
#% 466230
#% 466418
#% 529348
#% 580515
#% 808370
#% 834611
#% 868447
#% 1275169
#% 1279257
#% 1289499
#! Many situations arise in which an interested party's utility is dependent on the actions of an agent; e.g., a teacher is interested in a student learning effectively and a firm is interested in a consumer's behavior. We consider an environment in which the interested party can provide incentives to affect the agent's actions but cannot otherwise enforce actions. In value-based policy teaching, we situate this within the framework of sequential decision tasks modeled by Markov Decision Processes, and seek to associate limited rewards with states that induce the agent to follow a policy that maximizes the total expected value of the interested party. We show value-based policy teaching is NP-hard and provide a mixed integer program formulation. Focusing in particular on environments in which the agent's reward is unknown to the interested party, we provide a method for active indirect elicitation wherein the agent's reward function is inferred from observations about its response to incentives. Experimental results suggest that we can generally find the optimal incentive provision in a small number of elicitation rounds.

#index 1270053
#* Manipulating the quota in weighted voting games
#@ Michael Zuckerman;Piotr Faliszewski;Yoram Bachrach;Edith Elkind
#t 2008
#c 10
#% 3430
#% 80804
#% 93160
#% 408396
#% 1024833
#% 1083979
#% 1083983
#% 1250153
#% 1269439
#! Weighted voting games provide a popular model of decision making in multiagent systems. Such games are described by a set of players, a list of players' weights, and a quota; a coalition of the players is said to be winning if the total weight of its members meets or exceeds the quota. The power of a player in such games is traditionally identified with her Shapley-Shubik index or her Banzhaf index, two classical power measures that reflect the player's marginal contributions under different coalition formation scenarios. In this paper, we investigate by how much the central authority can change a player's power, as measured by these indices, by modifying the quota. We provide tight upper and lower bounds on the changes in the individual player's power that can result from a change in quota. We also study how the choice of quota can affect the relative power of the players. From the algorithmic perspective, we provide an efficient algorithm for determining whether there is a value of the quota that makes a given player a dummy, i.e., reduces his power (as measured by both indices) to 0. On the other hand, we show that checking which of the two values of the quota makes this player more powerful is computationally hard, namely, complete for the complexity class PP, which is believed to be significantly more powerful than NP.

#index 1270054
#* Measuring the hardness of SAT instances
#@ Carlos Ansótegui;María Luisa Bonet;Jordi Levy;Felip Manyà
#t 2008
#c 10
#% 55926
#% 198885
#% 288927
#% 328082
#% 331899
#% 345059
#% 656686
#% 677662
#% 735860
#% 866733
#% 1250508
#% 1269577
#% 1272049
#% 1279379
#% 1393551
#% 1399077
#! The search of a precise measure of what hardness of SAT instances means for state-of-the-art solvers is a relevant research question. Among others, the space complexity of treelike resolution (also called hardness), the minimal size of strong backdoors and of cycle-cutsets, and the treewidth can be used for this purpose. We propose the use of the tree-like space complexity as a solid candidate to be the best measure for solvers based on DPLL. To support this thesis we provide a comparison with the other mentioned measures. We also conduct an experimental investigation to show how the proposed measure characterizes the hardness of random and industrial instances.

#index 1270055
#* A new incomplete method for CSP inconsistency checking
#@ Belaïd Benhamou;Mohamed Réda Saïdi
#t 2008
#c 10
#% 160208
#% 319789
#% 327779
#% 420712
#% 420758
#% 750050
#% 844118
#% 1273681
#% 1275116
#% 1478763
#% 1728045
#! Checking CSP consistency is shown, in theory, to be an NP-complete problem. There is two families of methods for CSP consistency checking. The first family holds the complete methods which make an exhaustive search on the solution space. These methods have the advantage to prove CSP inconsistency, but their complexity grows exponentially when the problem size increases. The second family includes the incomplete methods that make a local search on the solution space. These methods have been efficiently used to find solutions for large size consistent CSPs that complete methods can not solve. One major drawback of the incomplete methods, is their inability to prove CSP inconsistency. One of the challenges that have been put forward by the CP community (Selman et al. 1997) is to provide incomplete methods that can deal with CSP inconsistency efficiently. The work that we present here, is a contribution towards an answer to this hard challenge. We introduce a new incomplete method for CSP inconsistency checking that is based on both a new notion of dominance between CSPs and a coloration of the CSP micro-structure. We experimented the method on randomly generated CSP instances and the results obtained are very promising.

#index 1270056
#* The parameterized complexity of global constraints
#@ C. Bessiere;E. Hebrard;B. Hnich;Z. Kiziltan;C.-G. Quimper;T. Walsh
#t 2008
#c 10
#% 36814
#% 55926
#% 160208
#% 160382
#% 289332
#% 464907
#% 497307
#% 534497
#% 535153
#% 911010
#% 976748
#% 1065626
#% 1250136
#% 1279379
#% 1399099
#% 1399124
#% 1499496
#% 1664964
#% 1732232
#! We argue that parameterized complexity is a useful tool with which to study global constraints. In particular, we show that many global constraints which are intractable to propagate completely have natural parameters which make them fixed-parameter tractable and which are easy to compute. This tractability tends either to be the result of a simple dynamic program or of a decomposition which has a strong backdoor of bounded size. This strong backdoor is often a cycle cutset. We also show that parameterized complexity can be used to study other aspects of constraint programming like symmetry breaking. For instance, we prove that value symmetry is fixed-parameter tractable to break in the number of symmetries. Finally, we argue that parameterized complexity can be used to derive results about the approximability of constraint propagation.

#index 1270057
#* Protein structure prediction on the face centered cubic lattice by local search
#@ Manuel Cebrián;Ivan Dotú;Pascal Van Hentenryck;Peter Clote
#t 2008
#c 10
#% 125556
#% 420007
#% 862120
#% 1038962
#% 1291557
#! Ab initio protein structure prediction is an important problem for which several algorithms have been developed. Algorithms differ by how they represent 3D protein conformations (on-lattice, off-lattice, coarse-grain or fine-grain model), by the energy model they consider, and whether they are heuristic or exact algorithms. This paper presents a local search algorithm to find the native state for the Hydrophobic-Polar (HP) model on the Face Centered Cubic (FCC) lattice; i.e. a self-avoiding walk on the FCC lattice with maximum number of H-H contacts. The algorithm relies on a randomized, structured initialization, a novel fitness function to guide the search, and efficient data structures to obtain self-avoiding walks. Experimental results on benchmark instances show the efficiency and excellent performance of our algorithm, and illustrate the biological pertinence of the FCC lattice.

#index 1270058
#* Relaxed survey propagation: a sum-product algorithm for Max-SAT
#@ Hai Leong Chieu;Wee Sun Lee
#t 2008
#c 10
#% 534971
#% 819505
#% 1250520
#% 1289364
#% 1289380
#% 1675296
#% 1810385
#! The survey propagation (SP) algorithm has been shown to work well on large instances of the random 3-SAT problem near its phase transition. It was shown that SP estimates marginals over covers, using joker states to represent clusters of configurations. The SP-y algorithm generalizes SP to work on the Max-SAT problem, but the cover interpretation of SP does not generalize to SP-y. Recently, a relaxed survey propagation (RSP) algorithm has been proposed for inference in Markov random fields (MRF). RSP for MRFs assigns zero probability to joker states, and hence the cover interpretation is also inapplicable. We adapt RSP to solve Max-SAT problems, and show that it has an interpretation of estimating marginals over covers violating a minimum number of clauses. This naturally generalizes the cover interpretation of SP. Empirically, we show that RSP out-performs SP-y and other state-of-the-art solvers on random as well as benchmark instances of Max-SAT.

#index 1270059
#* Virtual Arc consistency for weighted CSP
#@ M. Cooper;S. De Givry;M. Sanchez;T. Schiex;M. Zytnicki
#t 2008
#c 10
#% 197431
#% 419942
#% 534981
#% 575676
#% 751442
#% 874125
#% 892923
#% 975169
#% 1095850
#% 1272040
#% 1274762
#% 1275309
#% 1289192
#% 1289364
#% 1396041
#% 1405983
#! Optimizing a combination of local cost functions on discrete variables is a central problem in many formalisms such as in probabilistic networks, maximum satisfiability, weighted CSP or factor graphs. Recent results have shown that maintaining a form of local consistency in a Branch and Bound search provides bounds that are strong enough to solve many practical instances. In this paper, we introduce Virtual Arc Consistency (VAC) which iteratively identifies and applies sequences of cost propagation over rational costs that are guaranteed to transform a WCSP in another WCSP with an improved constant cost. Although not as strong as Optimal Soft Arc Consistency, VAC is faster and powerful enough to solve submodular problems. Maintaining VAC inside branch and bound leads to important improvements in efficiency on large difficult problems and allowed us to close two famous frequency assignment problem instances.

#index 1270060
#* Simulation-based approach to general game playing
#@ Hilmar Finnsson;Yngvi Björnsson
#t 2008
#c 10
#% 60140
#% 425053
#% 443807
#% 983838
#% 1250387
#% 1269851
#% 1269860
#% 1274860
#% 1404135
#% 1665148
#! The aim of General Game Playing (GGP) is to create intelligent agents that automatically learn how to play many different games at an expert level without any human intervention. The most successful GGP agents in the past have used traditional game-tree search combined with an automatically learned heuristic function for evaluating game states. In this paper we describe a GGP agent that instead uses a Monte Carlo/UCT simulation technique for action selection, an approach recently popularized in computer Go. Our GGP agent has proven its effectiveness by winning last year s AAAI GGP Competition. Furthermore, we introduce and empirically evaluate a new scheme for automatically learning search-control knowledge for guiding the simulation playouts, showing that it offers significant benefits for a variety of games.

#index 1270061
#* Phase transitions and complexity of weighted satisfiability and other intractable parameterized problems
#@ Yong Gao
#t 2008
#c 10
#% 338404
#% 414951
#% 419990
#% 835212
#% 895018
#% 1029089
#% 1075151
#% 1272140
#% 1272151
#% 1399077
#% 1413493
#% 1972413
#% 1985061
#! The study of random instances of NP complete and coNP complete problems has had much impact on our understanding of the nature of hard problems. In this work, we initiate an effort to extend this line of research to random instances of intractable parameterized problems. We propose random models for a representative intractable parameterized problem, the weighted d-CNF satisfiability, and its generalization to the constraint satisfaction problem. The exact threshold for the phase transition of the proposed models is determined. Lower bounds on the time complexity of variants of the DPLL algorithm for these parameterized problems are also established. In particularly, we show that random instances of the weighted 2-CNF satisfiability, already an intractable parameterized problem, are typically easy in both of the satisfiable and unsatisfiable regions by exploiting an interesting connection between the unsatisfiability of a weighted 2-CNF formula and the existence of a Hamiltonian-cycle-like global structure.

#index 1270062
#* Studies in solution sampling
#@ Vibhav Gogate;Rina Dechter
#t 2008
#c 10
#% 44876
#% 374580
#% 451125
#% 578658
#% 850430
#% 1250224
#% 1269700
#% 1665012
#% 1698709
#! We introduce novel algorithms for generating random solutions from a uniform distribution over the solutions of a boolean satisfiability problem. Our algorithms operate in two phases. In the first phase, we use a recently introduced SampleSearch scheme to generate biased samples while in the second phase we correct the bias by using either Sampling/lmportance Resampling or the Metropolis-Hastings method. Unlike state-of-the-art algorithms, our algorithms guarantee convergence in the limit. Our empirical results demonstrate the superior performance of our new algorithms over several competing schemes.

#index 1270063
#* On range of skill
#@ Thomas Dueholm Hansen;Peter Bro Miltersen;Troels Bjerre Sørensen
#t 2008
#c 10
#% 104422
#% 176299
#% 925513
#% 1269795
#! At AAAI'07, Zinkevich, Bowling and Burch introduced the Range of Skill measure of a two-player game and used it as a parameter in the analysis of the running time of an algorithm for finding approximate solutions to such games. They suggested that the Range of Skill of a typical natural game is a small number, but only gave heuristic arguments for this. In this paper, we provide the first methods for rigorously estimating the Range of Skill of a given game. We provide some general, asymptotic bounds that imply that the Range of Skill of a perfectly balanced game tree is almost exponential in its size (and doubly exponential in its depth). We also provide techniques that yield concrete bounds for unbalanced game trees and apply these to estimate the Range of Skill of Tic-Tac-Toe and Heads-Up Limit Texas Hold'em Poker. In particular, we show that the Range of Skill of Tic-Tac-Toe is more than 100,000.

#index 1270064
#* Clause learning can effectively P-simulate general propositional resolution
#@ Philipp Hertel;Fahiem Bacchus;Toniann Pitassi;Allen Van Gelder
#t 2008
#c 10
#% 220203
#% 317733
#% 327779
#% 347232
#% 427631
#% 587590
#% 656782
#% 1272049
#% 1396051
#% 1413510
#% 1478761
#% 1674537
#% 1698713
#! Currently, the most effective complete SAT solvers are based on the DPLL algorithm augmented by clause learning. These solvers can handle many real-world problems from application areas like verification, diagnosis, planning, and design. Without clause learning, however, DPLL loses most of its effectiveness on real world problems. Recently there has been some work on obtaining a deeper understanding of the technique of clause learning. In this paper we utilize the idea of effective p-simulation, which is a new way of comparing clause learning with general resolution and other proof systems. We then show that pool proofs, a previously used characterization of clause learning, can effectively p-simulate general resolution. Furthermore, this result holds even for the more restrictive class of greedy, unit propagating, pool proofs, which more accurately characterize clause learning as it is used in practice. This result is surprising and indicates that clause learning is significantly more powerful than was previously known.

#index 1270065
#* Piecewise linear dynamic programming for constrained POMDPs
#@ Joshua D. Isom;Sean P. Meyn;Richard D. Braatz
#t 2008
#c 10
#% 363744
#% 706380
#% 707796
#% 788053
#% 842579
#% 1215050
#% 1269867
#% 1272075
#% 1769968
#% 1845540
#! We describe an exact dynamic programming update for constrained partially observable Markov decision processes (CPOMDPs). State-of-the-art exact solution of unconstrained POMDPs relies on implicit enumeration of the vectors in the piecewise linear value function, and pruning operations to obtain a minimal representation of the updated value function. In dynamic programming for CPOMDPs, each vector takes two valuations, one with respect to the objective function and another with respect to the constraint function. The dynamic programming update consists of finding, for each belief state, the vector that has the best objective function valuation while still satisfying the constraint function. Whereas the pruning operation in an unconstrained POMDP requires solution of a linear program, the pruning operation for CPOMDPs requires solution of a mixed integer linear program.

#index 1270066
#* Efficient memoization for dynamic programming with ad-hoc constraints
#@ Joxan Jaffar;Andrew E. Santosa;Razvan Voicu
#t 2008
#c 10
#% 115193
#% 160253
#% 195444
#% 220203
#% 288882
#% 336874
#% 349117
#% 734797
#% 1274770
#% 1478761
#% 1718203
#! We address the problem of effective reuse of subproblem solutions in dynamic programming. In dynamic programming, a memoed solution of a subproblem can be reused for another if the latter's context is a special case of the former. Our objective is to generalize the context of the memoed subproblem such that more subproblems can be considered subcases and hence enhance reuse. Toward this goal we propose a generalization of context that 1) does not add better solutions than the subproblem's optimal, yet 2) requires that subsumed sub-problems preserve the optimal solution. In addition, we also present a general technique to search for at most k ≥ 1 optimal solutions. We provide experimental results on resource-constrained shortest path (RCSP) benchmarks and program's exact worst-case execution time (WCET) analysis.

#index 1270067
#* On the power of top-down branching heuristics
#@ Matti Järvisalo;Tommi Junttila
#t 2008
#c 10
#% 274131
#% 288165
#% 327779
#% 336402
#% 338406
#% 349898
#% 765945
#% 776981
#% 778165
#% 829318
#% 1272049
#% 1399083
#% 1838754
#! We study the relative best-case performance of DPLL-based structure-aware SAT solvers in terms of the power of the underlying proof systems. The systems result from (i) varying the style of branching and (ii) enforcing dynamic restrictions on the decision heuristics. Considering DPLL both with and without clause learning, we present a relative efficiency hierarchy for refinements of DPLL resulting from combinations of decision heuristics (top-down restricted, justification restricted, and unrestricted heuristics) and branching styles (typical DPLL-style and ATPG-style branching). An an example, for DPLL without clause learning, we establish a strict hierarchy, with the ATPG-style, justification restricted branching variant as the weakest system.

#index 1270068
#* Efficient context-free grammar constraints
#@ Serdar Kadioglu;Meinolf Sellmann
#t 2008
#c 10
#% 421312
#% 534496
#% 903341
#% 911011
#% 1269705
#% 1399099
#% 1664994
#% 1665020
#! With the introduction of constraints based on finite automata a new line of research has opened where constraints are based on formal languages. Recently, constraints based on grammars higher up in the Chomsky hierarchy were introduced. We devise a time- and space-efficient incremental arc-consistency algorithm for context-free grammars. Particularly, we show how to filter a sequence of monotonically tightening problems in cubic time and quadratic space. Experiments on a scheduling problem show orders of magnitude improvements in time and space consumption.

#index 1270069
#* Minimizing disk I/O in two-bit breadth-first search
#@ Richard E. Korf
#t 2008
#c 10
#% 238062
#% 829310
#% 987048
#% 987571
#% 1269579
#% 1272186
#% 1275127
#% 1279478
#! We present a breadth-first search algorithm, two-bit breadth-first search (TBBFS), which requires only two bits for each state in the problem space. TBBFS can be parallelized in several ways, and can store its data on magnetic disk. Using TBBFS, we perform complete breadth-first searches of the original pancake problem with 14 and 15 pancakes, and the burned pancake problem with 11 and 12 pancakes, determining the diameter of these problem spaces for the first time. We also performed a complete breadth-first search of the subspace of Rubik's Cube determined by the edge cubies.

#index 1270070
#* H-DPOP: using hard constraints for search space pruning in DCOP
#@ Akshat Kumar;Adrian Petcu;Boi Faltings
#t 2008
#c 10
#% 2028
#% 281588
#% 314925
#% 443227
#% 644201
#% 855913
#% 1269416
#% 1289393
#% 1664981
#! In distributed constraint optimization problems, dynamic programming methods have been recently proposed (e.g. DPOP). In dynamic programming many valuations are grouped together in fewer messages, which produce much less networking overhead than search. Nevertheless, these messages are exponential in size. The basic DPOP always communicates all possible assignments, even when some of them may be inconsistent due to hard constraints. Many real problems contain hard constraints that significantly reduce the space of feasible assignments. This paper introduces H-DPOP, a hybrid algorithm that is based on DPOP, which uses Constraint Decision Diagrams (CDD) to rule out infeasible assignments, and thus compactly represent UTIL messages. Experimental results show that H-DPOP requires several orders of magnitude less memory than DPOP, especially for dense and tightly-constrained problems.

#index 1270071
#* Online learning with expert advice and finite-horizon constraints
#@ Branislav Kveton;Jia Yuan Yu;Georgios Theocharous;Shie Mannor
#t 2008
#c 10
#% 165663
#% 320081
#% 871302
#% 949834
#% 1270151
#% 1674797
#! In this paper, we study a sequential decision making problem. The objective is to maximize the average reward accumulated over time subject to temporal cost constraints. The novelty of our setup is that the rewards and constraints are controlled by an adverse opponent. To solve our problem in a practical way, we propose an expert algorithm that guarantees both a vanishing regret and a sublinear number of violated constraints. The quality of this solution is demonstrated on a real-world power management problem. Our results support the hypothesis that online learning with convex cost constraints can be performed successfully in practice.

#index 1270072
#* Exploiting causal independence using weighted model counting
#@ Wei Li;Pascal Poupart;Peter Van Beek
#t 2008
#c 10
#% 44876
#% 59918
#% 233849
#% 527688
#% 1250342
#% 1269433
#% 1272302
#% 1279378
#% 1650335
#% 1650767
#% 1698709
#% 1784146
#! Previous studies have demonstrated that encoding a Bayesian network into a SAT-CNF formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference in Bayesian networks. In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations-two relations which are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two space efficient CNF encodings for noisy-OR/MAX and explore alternative search ordering heuristics. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks. On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches and scaled up to networks with larger numbers of random variables.

#index 1270073
#* R* search
#@ Maxim Likhachev;Anthony Stentz
#t 2008
#c 10
#% 241
#% 578810
#% 827731
#! Optimal heuristic searches such as A* search are widely used for planning but can rarely scale to large complex problems. The suboptimal versions of heuristic searches such as weighted A* search can often scale to much larger planning problems by trading off the quality of the solution for efficiency. They do so by relying more on the ability of the heuristic function to guide them well towards the goal. For complex planning problems, however, the heuristic function may often guide the search into a large local minimum and make the search examine most of the states in the minimum before proceeding. In this paper, we propose a novel heuristic search, called R* search, which depends much less on the quality of the heuristic function. The search avoids local minima by solving the whole planning problem with a series of short-range and easy-to-solve searches, each guided by the heuristic function towards a randomly chosen goal. In addition, R* scales much better in terms of memory because it can discard a search state-space after each of its searches. On the theoretical side, we derive probabilistic guarantees on the sub-optimality of the solution returned by R*. On the experimental side, we show that R* can scale to large complex problems.

#index 1270074
#* Within-problem learning for efficient lower bound computation in Max-SAT solving
#@ Han Lin;Kaile Su;Chu-Min Li
#t 2008
#c 10
#% 427631
#% 819612
#% 944137
#% 977574
#% 994697
#% 1023522
#% 1029061
#% 1250517
#% 1250520
#% 1272189
#% 1272198
#% 1275129
#% 1396040
#% 1399075
#% 1406884
#% 1698732
#! This paper focuses on improving branch-and-bound Max-SAT solvers by speeding up the lower bound computation. We notice that the existing propagation-based computing methods and the resolution-based computing methods, which have been studied intensively, both suffer from several drawbacks. In order to overcome these drawbacks, we propose a new method with a nice property that guarantees the increment of lower bounds. The new method exploits within-problem learning techniques. More specifically, at each branch point in the search-tree, the current node is enabled to inherit inconsistencies from its parent and learn information about effectiveness of the lower bound computing procedure from previous nodes. Furthermore, after branching on a new variable, the inconsistencies may shrink by applying unit propagation to them, and such process increases the probability of getting better lower bounds. We graft the new techniques into maxsatz and the experimental results demonstrate that the new solver outperforms the best state-of-the-art solvers on a wide range of instances including random and structured ones.

#index 1270075
#* Learning from multiple heuristics
#@ Mehdi Samadi;Ariel Felner;Jonathan Schaeffer
#t 2008
#c 10
#% 137995
#% 183499
#% 286669
#% 348576
#% 348578
#% 829310
#% 1269581
#% 1272048
#% 1272186
#% 1275127
#! Heuristic functions for single-agent search applications estimate the cost of the optimal solution. When multiple heuristics exist, taking their maximum is an effective way to combine them. A new technique is introduced for combining multiple heuristic values. Inspired by the evaluation functions used in two-player games, the different heuristics in a single-agent application are treated as features of the problem domain. An ANN is used to combine these features into a single heuristic value. This idea has been implemented for the sliding-tile puzzle and the 4-peg Towers of Hanoi, two classic single-agent search domains. Experimental results show that this technique can lead to a large reduction in the search effort at a small cost in the quality of the solution obtained.

#index 1270076
#* Backdoor trees
#@ Marko Samer;Stefan Szeider
#t 2008
#c 10
#% 230778
#% 341672
#% 400366
#% 729052
#% 785504
#% 826011
#% 857282
#% 895018
#% 1023276
#% 1250137
#% 1250546
#% 1269577
#% 1279379
#% 1396056
#% 1399077
#% 1405531
#% 1972413
#% 1985061
#! The surprisingly good performance of modem satisfiability (SAT) solvers is usually explained by the existence of a certain "hidden structure" in real-world instances. We introduce the notion of backdoor trees as an indicator for the presence of a hidden structure. Backdoor trees refine the notion of strong backdoor sets, taking into account the relationship between backdoor variables. We present theoretical and empirical results. Our theoretical results are concerned with the computational complexity of detecting small backdoor trees. With our empirical results we compare the size of backdoor trees against the size of backdoor sets for real-world SAT instances and random 3SAT instances of various density. The results indicate that backdoor trees amplify the properties that have been observed for backdoor sets.

#index 1270077
#* A global constraint for bin-packing with precedences: application to the assembly line balancing problem
#@ Pierre Schaus;Yves Deville
#t 2008
#c 10
#% 410276
#! Assembly line balancing problems (ALBP) are of capital importance for the industry since the first assembly line for the Ford T by Henry Ford. Their objective is to optimize the design of production lines while satisfying the various constraints. Precedence constraints among the tasks are always present in ALBP. The objective is then to place the tasks among various workstations such that the production rate is maximized. This problem can be modeled as a bin packing problem with precedence constraints (BPPC) where the bins are the workstations and the items are the tasks. Paul Shaw introduced a global constraint for bin-packing (without precedence). Unfortunately this constraint does not capture the precedence constraints of BPPC. In this paper, we first introduce redundant constraints for BPPC combining the precedences and the bin-packing, allowing to solve instances which are otherwise intractable in constraint programming. We also design a global constraint for BPPC, introducing even more pruning in the search tree. We finally used our CP model for BPPC to solve ALBP. We propose two search heuristics, and show the efficiency of our approach on standard ALBP benchmarks. Compared to standard non CP approaches, our method is more flexible as it can handle new constraints that might appear in real applications.

#index 1270078
#* Bound consistency for binary length-lex set constraints
#@ Pascal Van Hentenryck;Justin Yip;Carmen Gervet;Grégoire Dooms
#t 2008
#c 10
#% 56471
#% 1026511
#% 1250514
#! The length-lex representation has been recently proposed for representing sets in Constraint Satisfaction Problems. The length-lex representation directly captures cardinality information, provides a total ordering for sets, and allows bound consistency on unary constraints to be enforced in time Õ(c), where c is the cardinality of the set. However, no algorithms were given to enforce bound consistency on binary constraints. This paper addresses this open issue. It presents algorithms to enforce bound consistency on disjointness and cardinality constraints in time O(c3). Moreover, it presents a generic bound-consistency algorithm for any binary constraint S which requires Õ(c2) calls to a feasibility subroutine for S.

#index 1270079
#* Predicting the performance of IDA* with conditional distributions
#@ Uzi Zahavi;Ariel Feiner;Neil Burch;Robert C. Holte
#t 2008
#c 10
#% 2194
#% 266116
#% 337986
#% 348576
#% 1250327
#% 1269863
#% 1272186
#% 1289367
#% 1478838
#! (Korf, Reid, and Edelkamp 2001) introduced a formula to predict the number of nodes IDA* will expand given the static distribution of heuristic values. Their formula proved to be very accurate but it is only accurate under the following limitations: (1) the heuristic must be consistent; (2) the prediction is for a large random sample of start states (or for large thresholds). In this paper we generalize the static distribution to a conditional distribution of heuristic values. We then propose a new formula for predicting the performance of IDA* that works well for inconsistent heuristics (Zahavi et al. 2007) and for any set of start states, not just a random sample. We also show how the formula can be enhanced to work well for single start states. Experimental results demonstrate the accuracy of our method in all these situations.

#index 1270080
#* Reasoning with cardinal directions: an efficient algorithm
#@ Xiaotong Zhang;Weiming Liu;Sanjiang Li;Mingsheng Ying
#t 2008
#c 10
#% 319244
#% 329895
#% 495793
#% 526851
#% 527331
#% 549078
#% 590627
#% 741459
#% 784115
#% 806735
#% 1274821
#% 1274831
#! Direction relations between extended spatial objects are important commonsense knowledge. Recently, Goyal and Egenhofer proposed a formal model, called Cardinal Direction Calculus (CDC), for representing direction relations between connected plane regions. CDC is perhaps the most expressive qualitative calculus for directional information, and has attracted increasing interest from areas such as artificial intelligence, geographical information science, and image retrieval. Given a network of CDC constraints, the consistency problem is deciding if the network is realizable by connected regions in the real plane. This paper provides a cubic algorithm for checking consistency of basic CDC constraint networks. As one by product, we also show that any consistent network of CDC constraints has a canonical realization in digital plane. The cubic algorithm can also been adapted to cope with disconnected regions, in which case the current best algorithm is of time complexity O(n5).

#index 1270081
#* Anytime local search for distributed constraint optimization
#@ Roie Zivan
#t 2008
#c 10
#% 337838
#% 360802
#% 534510
#% 773232
#% 855910
#% 855913
#% 1223208
#% 1274986
#% 1289393
#% 1784342
#! Most former studies of Distributed Constraint Optimization Problems (DisCOPs) search considered only complete search algorithms, which are practical only for relatively small problems. Distributed local search algorithms can be used for solving DisCOPs. However, because of the differences between the global evaluation of a system's state and the private evaluation of states by agents, agents are unaware of the global best state which is explored by the algorithm. Previous attempts to use local search algorithms for solving DisCOPs reported the state held by the system at the termination of the algorithm, which was not necessarily the best state explored. A general framework for implementing distributed local search algorithms for DisCOPs is proposed. The proposed framework makes use of a BFS-tree in order to accumulate the costs of the system's state in its different steps and to propagate the detection of a new best step when it is found. The resulting framework enhances local search algorithms for DisCOPs with the anytime property. The proposed framework does not require additional network load. Agents are required to hold a small (linear) additional space (beside the requirements of the algorithm in use). The proposed framework preserves privacy at a higher level than complete Dis-COP algorithms which make use ofa pseudo-tree (ADOPT, DPOP).

#index 1270082
#* On the decidability of role mappings between modular ontologies
#@ Jie Bao;George Voutsadakis;Giora Slutzki;Vasant Honavar
#t 2008
#c 10
#% 509870
#% 1269877
#% 1289442
#% 1409950
#! Many semantic web applications require support for mappings between roles (or properties) defined in multiple independently developed ontology modules. Distributed Description Logics (DDL) and Package-based Description Logics (P-DL) offer two alternative logical formalisms that support such mappings. We prove that (a) variants of DDL that allow negated roles or cardinality restrictions in bridge rules or inverse bridge rules that connect ALC ontologies are undecidable; (b) a variant of P-DL ALCHIO(¬)P that supports role mappings between ontology modules expressed in ALCHIO(¬) is decidable.

#index 1270083
#* Non-monotonic temporal logics that facilitate elaboration tolerant revision of goals
#@ Chitta Baral;Jicheng Zhao
#t 2008
#c 10
#% 98066
#% 114677
#% 417597
#% 544938
#% 1272094
#% 1274789
#% 1275329
#% 1289213
#% 1476293
#% 1478747
#! Temporal logics are widely used in specifying goals of agents. We noticed that when directing agents, humans often revise their requirements for the agent, especially as they gather more knowledge about the domain. However, all existing temporal logics, except one, do not focus on the revision of goals in an elaboration tolerant manner. Thus formal temporal logics that can allow elaboration tolerant revision of goals are needed. As non-monotonic languages are often used for elaboration tolerant specification, we propose to explore non-monotonic temporal logics for goal specification. Recently, a non-monotonic temporal logic, N-LTL, was proposed with similar aims. In N-LTL, goal specifications could be changed via strong and weak exceptions. However, in NLTL, one had to a-priori declare whether exceptions will be weak or strong exceptions. We propose a new nonmonotonic temporal logic, that not only overcomes this, but is also able to express exception to exceptions, strengthen and weaken preconditions, and revise and replace consequents; all in an elaboration tolerant manner.

#index 1270084
#* Prime implicate normal form for ALC concepts
#@ Meghyn Bienvenu
#t 2008
#c 10
#% 116299
#% 156696
#% 475381
#% 1269729
#% 1271987
#% 1272349
#! In this paper, we present a normal form for concept expressions in the description logic ALC which is based on a recently introduced notion of prime implicate for the modal logic K. We show that concepts in prime implicate normal form enjoy a number of desirable properties which make prime implicate normal form interesting from the viewpoint of knowledge compilation. In particular, we prove that subsumption between ALC concepts in prime implicate normal form can be carried out in polynomial time using a simple structural subsumption algorithm reminiscent of those used for less expressive description logics. Of course, in order to take advantage of these properties, we need a way to transform concepts into equivalent concepts in prime implicate normal form. We provide a sound and complete algorithm for putting concepts into prime implicate normal form, and we investigate the spatial complexity of this transformation, showing there to be an at most doubly-exponential blowup in concept length. At the end of the paper, we compare prime implicate normal form to two other normal forms for ALC, discussing the relative merits of the different approaches.

#index 1270085
#* Credulous resolution for answer set programming
#@ Piero A. Bonatti;Enrico Pontelli;Tran Cao Son
#t 2008
#c 10
#% 877
#% 340740
#% 400992
#% 417651
#% 420735
#% 499514
#% 572371
#% 752745
#% 763752
#% 1274811
#% 1656402
#% 1692897
#! The paper presents a calculus based on resolution for credulous reasoning in Answer Set Programming. The new approach allows a top-down and goal directed resolution, in the same spirit as traditional SLD-resolution. The proposed credulous resolution can be used in query-answering with nonground queries and with non-ground, and possibly infinite, programs. Soundness and completeness results for the resolution procedure are proved for large classes of logic programs. The resolution procedure is also extended to handle some traditional syntactic extensions used in Answer Set Programming, such as choice rules and constraints. The paper also describes an initial implementation of a system for credulous reasoning in Answer Set Programming.

#index 1270086
#* Manifold integration with Markov random walks
#@ Heeyoul Choi;Seungjin Choi;Yoonsuck Choe
#t 2008
#c 10
#% 464615
#% 723241
#% 770839
#% 770865
#% 771840
#% 838734
#% 939132
#! Most manifold learning methods consider only one similarity matrix to induce a low-dimensional manifold embedded in data space. In practice, however, we often use multiple sensors at a time so that each sensory information yields different similarity matrix derived from the same objects. In such a case, manifold integration is a desirable task, combining these similarity matrices into a compromise matrix that faithfully reflects multiple sensory information. A small number of methods exists for manifold integration, including a method based on reproducing kernel Krein space (RKKS) or DISTATIS, where the former is restricted to the case of only two manifolds and the latter considers a linear combination of normalized similarity matrices as a compromise matrix. In this paper we present a new manifold integration method, Markov random walk on multiple manifolds (RAMS), which integrates transition probabilities defined on each manifold to compute a compromise matrix. Numerical experiments confirm that RAMS finds more informative manifolds with a desirable projection property.

#index 1270087
#* Parallel belief revision
#@ James Delgrande;Yi Jin
#t 2008
#c 10
#% 109945
#% 224753
#% 351545
#% 581824
#% 834117
#% 942467
#% 1273687
#% 1290096
#! A recalcitrant problem in approaches to iterated belief revision is that, after first revising by a formula and then by a formula that is inconsistent with the first formula, all information in the original formula is lost. As noted by various researchers, this phenomenon is made explicit in the second postulate (C2) of the well-known Darwiche-Pearl framework, and so this postulate has been a point of criticism of this and related approaches. In contrast, we argue that the true culprit of this problem arises from a basic assumption of the AGM framework, that new information is represented by a single formula. We propose a more general framework for belief revision (called parallel belief revision) in which individual items of new information are represented by a set of formulas. In this framework, if one revises by a set of formulas, and then by the negation of some members of this set, then other members of the set are still believed after the revision. Hence the aforecited problem is discharged. We present first a basic approach to parallel belief revision, and next an approach that combines the basic approach with that of Jin and Thielscher. Postulates and semantic conditions characterizing these approaches are given, and representation results provided.

#index 1270088
#* Efficient haplotype inference with answer set programming
#@ Esra Erdem;Ferhan Türe
#t 2008
#c 10
#% 400992
#% 411814
#% 870978
#% 918148
#% 958596
#% 1041281
#% 1058687
#% 1250523
#% 1274813
#% 1386504
#% 1396817
#! Identifying maternal and paternal inheritance is essential to be able to find the set of genes responsible for a particular disease. However, due to technological limitations, we have access to genotype data (genetic makeup of an individual), and determining haplotypes (genetic makeup of the parents) experimentally is a costly and time consuming procedure. With these biological motivations, we study a computational problem, called Haplotype Inference by Pure Parsimony (HIPP), that asks for the minimal number of haplotypes that form a given set of genotypes. HIPP has been studied using integer linear programming, branch and bound algorithms, SAT-based algorithms, or pseudo-boolean optimization methods. We introduce a new approach to solving HIPP, using Answer Set Programming (ASP). According to our experiments with a large number of problem instances (some automatically generated and some real), our ASP-based approach solves the most number of problems compared with other approaches. Due to the expressivity of the knowledge representation language of ASP, our approach allows us to solve variations of HIPP, e.g., with additional domain specific information, such as patterns/parts of haplotypes observed for some gene family, or with some missing genotype information. In this sense, the ASP-based approach is more general than the existing approaches to haplotype inference.

#index 1270089
#* Extending the knowledge compilation map: Krom, Horn, affine and beyond
#@ Hélène Fargier;Pierre Marquis
#t 2008
#c 10
#% 159058
#% 204396
#% 342378
#% 529171
#% 600496
#% 936786
#% 1250513
#% 1269749
#% 1272349
#% 1273692
#% 1275335
#% 1275338
#% 1499541
#! We extend the knowledge compilation map introduced by Darwiche and Marquis with three influential propositional fragments, the Krom CNF one (also known as the bijunctive fragment), the Hom CNF fragment and the affine fragment (also known as the biconditional fragment) as well as seven additional languages based on them, and composed respectively of Krom or Hom CNF formulas, renamable Hom CNF formulas, disjunctions of Krom CNF formulas, disjunctions of Hom CNF formulas, disjunctions of Krom or Hom CNF formulas, disjunctions of renamable Hom CNF formulas, and disjunction of fine formulas. Each fragment is evaluated w.r.t. several criteria, including the complexity of basic quaries and transformation, and its spatial efficiency is also analysed.

#index 1270090
#* A meta-programming technique for debugging answer-set programs
#@ Martin Gebser;Jörg Pührer;Torsten Schaub;Hans Tompits
#t 2008
#c 10
#% 21137
#% 158171
#% 173127
#% 313688
#% 400992
#% 411814
#% 852149
#% 880394
#% 906658
#% 1250545
#% 1289431
#% 1388121
#% 1692909
#! Answer-set programming (ASP) is widely recognised as a viable tool for declarative problem solving. However, there is currently a lack of tools for developing answer-set programs. In particular, providing tools for debugging answer-set programs has recently been identified as a crucial prerequisite for a wider acceptance of ASP. In this paper, we introduce a meta-programming technique for debugging in ASP. The basic question we address is why interpretations expected to be answer sets are not answer sets of the program to debug. We thus deal with finding semantical errors of programs. The explanations provided by our method are based on an intuitive scheme of errors that relies on a recent characterisation of the answer-set semantics. Furthermore, as we are using a meta-programming technique, debugging queries are expressed in terms of answer-set programs themselves, which has several benefits: For one, we can directly use ASP solvers for processing debugging queries. Indeed, our technique can easily be implemented, and we devised a corresponding prototype debugging system. Also, our approach respects the declarative nature of ASP, and the capabilities of the system can easily be extended to incorporate differing debugging features.

#index 1270091
#* From qualitative to quantitative proofs of security properties using first-order conditional logic
#@ Joseph Y. Halpern
#t 2008
#c 10
#% 77841
#% 320132
#% 339219
#% 342379
#% 443633
#% 664669
#% 923645
#% 947589
#% 1675928
#! A first-order conditional logic is considered, with semantics given by a variant of Ε-semantics (Adams 1975; Goldszmidt & Pearl 1992), where Φ→ψ means that Pr(ψ | Φ) approaches 1 super-polynomially-faster than any inverse polynomial. This type of convergence is needed for reasoning about security protocols. A complete axiomatization is provided for this semantics, and it is shown how a qualitative proof of the correctness of a security protocol can be automatically converted to a quantitative proof appropriate for reasoning about concrete security.

#index 1270092
#* Nonmonotonic modes of inference
#@ Victor Jauregui
#t 2008
#c 10
#% 64371
#% 77841
#% 89975
#% 340472
#% 780340
#% 1406888
#! In this paper we investigate nonmonotonic 'modes of inference'. Our approach uses modal (conditional) logic to establish a uniform framework in which to study nonmonotonic consequence. We consider a particular mode of inference which employs a majority-based account of default reasoning--one which differs from the more familiar preferential accounts--and show how modal logic supplies a framework which facilitates analysis of, and comparison with more traditional formulations of nonmonotonic consequence.

#index 1270093
#* Horn complements: towards horn-to-horn belief revision
#@ Marina Langlois;Robert H. Sloan;Balázs Szörényi;György Thrán
#t 2008
#c 10
#% 131559
#% 307262
#% 341479
#% 363670
#% 464715
#% 595498
#% 763750
#% 875281
#% 993084
#% 1269743
#% 1272324
#% 1289437
#! Horn-to-Horn belief revision asks for the revision of a Horn knowledge base such that the revised knowledge base is also Horn. Horn knowledge bases are important whenever one is concerned with efficiency--of computing inferences, of knowledge acquisition, etc. Horn-to-Horn belief revision could be of interest, in particular, as a component of any efficient system requiring large commonsense knowledge bases that may need revisions because, for example, new contradictory information is acquired. Recent results on belief revision for general logics show that the existence of a belief contraction operator satisfying the generalized AGM postulates is equivalent to the existence of a complement. Here we provide a first step towards efficient Horn-to-Horn belief revision, by characterizing the existence of a complement of a Horn consequence of a Horn knowledge base. A complement exists if and only if the Horn consequence is not the consequence of a modified knowledge base obtained from the original by an operation called body building. This characterization leads to the efficient construction of a complement whenever it exists.

#index 1270094
#* A reductive semantics for counting and choice in answer set programming
#@ Joohyung Lee;Vladimir Lifschitz;Ravi Palla
#t 2008
#c 10
#% 53389
#% 790727
#% 880394
#% 1274811
#% 1388133
#% 1656398
#! In a recent paper, Ferraris, Lee and Lifschitz conjectured that the concept of a stable model of a first-order formula can be used to treat some answer set programming expressions as abbreviations. We follow up on that suggestion and introduce an answer set programming language that defines the meaning of counting and choice by reducing these constructs to first-order formulas. For the new language, the concept of a safe program is defined, and its semantic role is investigated. We compare the new language with the concept of a disjunctive program with aggregates introduced by Faber, Leone and Pfeifer, and discuss the possibility of implementing a fragment of the language by translating it into the input language of the answer set solver DLV. The language is also compared with cardinality constraint programs defined by Syrjänen.

#index 1270095
#* Abductive logic programming by nonground rewrite systems
#@ Fangzhen Lin;Jia-Huai You
#t 2008
#c 10
#% 30096
#% 147511
#% 167174
#% 186896
#% 428362
#! Logic programming with negation offers a compelling approach to abductive reasoning. This paper shows a simple view of abduction in this context for the completion semantics, under which the problem of abduction becomes one of solving quantified equations and disequations. By this way of treating abduction, the problems with nonground negative queries in the previous approaches no longer exist. We show the soundness and completeness results for our approach.

#index 1270096
#* A formalization of program debugging in the situation calculus
#@ Yongmei Liu
#t 2008
#c 10
#% 21137
#% 254512
#% 342119
#% 345432
#% 428316
#% 763198
#% 807094
#% 868126
#% 1274827
#! Program debugging is one of the most time-consuming parts of the software development cycle. In recent years, automatic debugging has been an active research area in software engineering; it has also attracted attention from the AI community. However, existing approaches are mostly expenential; moreover, those model-based approaches are based on abstract models of programs, which lends an experiential flavor to the approaches, due to the heuristic nature of choosing an abstract model. We believe that it is necessary to establish a precise theoretical foundation for debugging from first principles. In this paper, we present a first step towards this foundation: using Reiter's theoretical framework of model-based diagnosis, we give a clean formalization of the program debugging task in the situation calculus, a logical language suitable for describing dynamic worlds. Examples are given to illustrate our formalization.

#index 1270097
#* Minimal contraction of preference relations
#@ Denis Mindolin;Jan Chomicki
#t 2008
#c 10
#% 731407
#% 741458
#% 823654
#% 915803
#% 953599
#% 992635
#% 1272026
#! Changing preferences is very common in real life. The expressive power of the operations of preference change introduced so far in the literature is limited to adding new information about preference and equivalence. Here, we discuss the operation of discarding preferences: preference contraction. We argue that the property of minimality and the preservation of strict partial orders are crucial for contractions. Contractions can be further constrained by specifying which preferences should not be contracted. We provide algorithms for computing minimal and minimal preference-protecting contraction. We also show some preference query optimization techniques which can be used in the presence of contraction.

#index 1270098
#* A first-order theory of Stanislavskian scene analysis
#@ Leora Morgenstern
#t 2008
#c 10
#% 175389
#% 763748
#% 777407
#% 810972
#% 830714
#! At the turn of the last century, Constantin Stanislavski developed a new system of acting, replacing the mannered gestures and forced emotion then popular with a more natural style. The core of his system lay in having actors perform a process of scene analysis, in which an actor would flesh out the circumstances of the play so that the character's motivations and actions would follow logically. This paper is an attempt to ground Stanislavski's method of scene analysis in a formal theory of action. We discuss the relations between Stanislavskian and formal AI theories of action and planning, give a formal definition of the end product of a scene analysis, and characterize the conditions under which a scene analysis is coherent.

#index 1270099
#* Worst-case optimal conjunctive query answering for an expressive description logic without inverses
#@ Magdalena Ortiz;Mantas Šimkus;Thomas Eiter
#t 2008
#c 10
#% 162224
#% 248026
#% 263136
#% 445447
#% 1072656
#% 1269731
#% 1274815
#% 1289425
#% 1405533
#% 1409925
#! Answering conjunctive queries (CQs) has been recognized as a key task for the usage of Description Logics (DLs) in a number of applications, and has thus been studied by many authors. In this paper, we present an algorithm for this problem in the DL ALCH which works in exponential time. It improves over previous algorithms which require double exponential time and is worst-case optimal, as already satisfiability testing in ALC is EXPTIME-complete. Furthermore, it shows that inverse roles cause an exponential jump in complexity; as recently shown, the problem is 2EXPTIME- complete for ALCI. The algorithm is based on a technique that compiles knowledge bases into sets of trees of depth 1. It is in coNP under data complexity (i.e., if the taxonomy part and the query are fixed), thus worst-case optimal. An extension from ALCH to DLs with further constructs is possible.

#index 1270100
#* An AGM-based belief revision mechanism for probabilistic spatio-temporal logics
#@ Austin Parker;Guillaume Infantes;V. S. Subrahmanian;John Grant
#t 2008
#c 10
#% 181340
#% 416007
#% 442755
#% 939431
#% 1013621
#% 1391867
#! There is now extensive interest in reasoning about moving objects. A PST knowledge base is a set of PST-atoms which are statements of the form "Object o is/was/will be at location L at time t with probability in the interval [L,U]". In this paper, we study mechanisms for belief revision in PST-KBs. We propose multiple methods for revising PST-KBs. These methods involve finding maximally consistent subsets, as well as changing the spatial, temporal, and probabilistic components of the atoms. We show that some methods cannot satisfy the AGM axioms for belief revision, while others do but are coNP-hard. Finally we present an algorithm for revision through probability change which runs in polynomial time and satisfies the AGM axioms.

#index 1270101
#* New compilation languages based on structured decomposability
#@ Knot Pipatsrisawat;Adnan Darwiche
#t 2008
#c 10
#% 3873
#% 188397
#% 204396
#% 342378
#% 490419
#% 498538
#% 578749
#% 936786
#% 1250512
#% 1272349
#% 1274845
#% 1275335
#% 1275338
#% 1289541
#% 1349569
#% 1399100
#% 1406884
#% 1499541
#% 1664981
#! We introduce in this paper two new, complete propositional languages and study their properties in terms of (1) their support for polytime operations and (2) their ability to represent boolean functions compactly. The new languages are based on a structured version of decomposability--a property that underlies a number of tractable languages. The key characteristic of structured decomposability is its support for a polytime conjoin operation, which is known to be intractable for unstructured decomposability. We show that any CNF can be compiled into formulas in the new languages, whose size is only exponential in the treewidth of the CNF. Our study also reveals that one of the languages we identify is as powerful as OBDDs in terms of answering key inference queries, yet is more succinct than OBDDs.

#index 1270102
#* A semantic approach for iterated revision in possibilistic logic
#@ Guilin Qi
#t 2008
#c 10
#% 8417
#% 109945
#% 167544
#% 224753
#% 416007
#% 581824
#% 763750
#% 942467
#% 1269744
#! In this paper, we propose a new approach for iterated revision in possibilistic logic by applying a one-step revision operator. We first argue that the set of KM postulates for revision is too strong to define a practical one-step revision operator and some of them should be weakened. We then present a semantic approach for iterated revision in possibilistic logic using a one-step revision operator. The computation of the semantic approach is given. We show that our revision approach satisfies almost all the DP postulates for iterated revision and some other important logical properties.

#index 1270103
#* Terminological reasoning in SHIQ with ordered binary decision diagrams
#@ Sebastian Rudolph;Markus Krötzsch;Pascal Hitzler
#t 2008
#c 10
#% 3873
#% 295866
#% 338753
#% 665856
#% 1068400
#! We present a new algorithm for reasoning in the description logic SHIQ, which is the most prominent fragment of the Web Ontology Language OWL. The algorithm is based on ordered binary decision diagrams (OBDDs) as a datastructure for storing and operating on large model representations. We thus draw on the success and the proven scalability of OBDD-based systems. To the best of our knowledge, we present the very first agorithm for using OBDDs for reasoning with general Tboxes.

#index 1270104
#* A scalable jointree algorithm for diagnosability
#@ Anika Schumann;Jinho Huang
#t 2008
#c 10
#% 644201
#% 844090
#% 1274837
#% 1274844
#% 1279265
#! Diagnosability is an essential property that determines how accurate any diagnostic reasoning can be on a system given any sequence of observations. An unobservable fault event in a discrete-event system is diagnosable iff its occurrence can always be deduced once sufficiently many subsequent observable events have occurred. A classical approach to diagnosability checking constructs a finite state machine known as a twin plant for the system. which has a critical path iff some fault event is not diagnosable. Recent work attempts to avoid the often impractical construction of the global twin plant by exploiting system structure. Specifically, local twin plants are constructed for components of the system, and synchronized with each other until diagnosability is decided. Unfortunately, synchronization of twin plants can remain a bottleneck for large systems; in the worst case, in particular, all local twin plants would be synchronized, again producing the global twin plant. We solve the diagnosability problem in a way that exploits the distributed nature of realistic systems. In our algorithm consistency among twin plants is achieved by message passing on a jointree. Scalability is significantly improved as the messages computed are generally much smaller than the synchronized product of the twin plants involved. Moreover we use an iterative procedure to search for a subset of the jointree that is sufficient to decide diagnosability. Finally, our algorithm is scalable in practice: it provides an approximate and useful solution if the computational resources are not sufficient.

#index 1270105
#* Factored models for probabilistic modal logic
#@ Afsaneh Shirazi;Eyal Amir
#t 2008
#c 10
#% 44876
#% 118746
#% 167194
#% 188086
#% 303620
#% 480202
#% 752494
#% 782337
#% 1269747
#% 1290051
#! Modal logic represents knowledge that agents have about other agents' knowledge. Probabilistic modal logic further captures probabilistic beliefs about probabilistic beliefs. Models in those logics are useful for understanding and decision making in conversations, bargaining situations, and competitions. Unfortunately, probabilistic modal structures are impractical for large real-world applications because they represent their state space explicitly. In this paper we scale up probabilistic modal structures by giving them a factored representation. This representation applies conditional independence for factoring the probabilistic aspect of the structure (as in Bayesian Networks (BN)). We also present two exact and one approximate algorithm for reasoning about the truth value of probabilistic modal logic queries over a model encoded in a factored form. The first exact algorithm applies inference in BNs to answer a limited class of queries. Our second exact method applies a variable elimination scheme and is applicable without restrictions. Our approximate algorithm uses sampling and can be used for applications with very large models. Given a query, it computes an answer and its confidence level efficiently.

#index 1270106
#* AnalogySpace: reducing the dimensionality of common sense knowledge
#@ Robert Speer;Catherine Havasi;Henry Lieberman
#t 2008
#c 10
#% 198055
#% 509695
#% 723391
#% 783633
#% 790480
#% 926881
#% 995514
#% 1279327
#! We are interested in the problem of reasoning over very large common sense knowledge bases. When such a knowledge base contains noisy and subjective data, it is important to have a method for making rough conclusions based on similarities and tendencies, rather than absolute truth. We present Analogy Space, which accomplishes this by forming the analogical closure of a semantic network through dimensionality reduction. It self-organizes concepts around dimensions that can be seen as making distinctions such as "good vs. bad" or "easy vs. hard", and generalizes its knowledge by judging where concepts lie along these dimensions. An evaluation demonstrates that users often agree with the predicted knowledge, and that its accuracy is an improvement over previous techniques.

#index 1270107
#* An extended interpreted system model for epistemic logics
#@ Kaile Su;Abdul Sattar
#t 2008
#c 10
#% 8936
#% 116625
#% 131283
#% 188086
#% 338753
#% 380578
#% 495346
#% 659831
#% 729449
#% 823864
#% 830082
#% 1272393
#! The interpreted system model offers a computationally grounded model, in terms of the states of computer processes, to S5 epistemic logics. This paper extends the interpreted system model, and provides a computationally grounded one, called the interpreted perception system model, to those episternic logics other than S5. It is usually assumed, in the interpreted system model, that those parts of the environment that are visible to an agent are correctly perceived by the agent as a whole. The essential idea of the interpreted perception system model is that an agent may have incorrect perception or observations to the visible parts of the environment and the agent may not be aware of this. The notion of knowledge can be defined so that an agent knows a statement iff the statement holds in those states that the agent can not distinguish (from the current state) by using only her correct observations. We establish a logic of knowledge and certainty, called KC logic, with a sound and complete proof system. The knowledge modality in this logic is S4 valid. It becomes S5 if we assume an agent always has correct observations; and more interestingly, it can be S4.2 or S4.3 under other natural constraints on agents and their sensors to the environment.

#index 1270108
#* Hyperequivalence of logic programs with respect to supported models
#@ Mirosław Truszczynski;Stefan Woltran
#t 2008
#c 10
#% 39263
#% 95265
#% 95266
#% 101949
#% 150822
#% 289052
#% 340738
#% 752748
#% 957649
#% 1041021
#% 1223271
#% 1269742
#% 1289366
#% 1410668
#% 1692919
#! Recent research in nonmonotonic logic programming has focused on program equivalence relevant for program optimization and modular programming. So far, most results concern the stable-model semantics. However, other semantics for logic programs are also of interest, especially the semantics of supported models which, when properly generalized, is closely related to the autoepistemic logic of Moore. In this paper, we consider a framework of equivalence notions for logic programs under the supported (minimal) model-semantics and provide characterizations for this framework in model-theoretic terms. We use these characterizations to derive complexity results concerning testing hyperequivalence of logic programs wrt supported (minimal) models.

#index 1270109
#* Generating application-specific benchmark models for complex systems
#@ Jun Wang;Gregory Provan
#t 2008
#c 10
#% 439276
#% 743408
#% 889640
#% 1272329
#% 1274834
#! Automated generators for synthetic models and data can playa crucial role in designing new algorithms/model-frameworks, given the sparsity of benchmark models for empirical analysis and the cost of generating models by hand. We describe an automated generator for benchmark models that is based on using a compositional modeling framework and employs random-graph models for the system topology. We choose the system topology that best matches the topology of the real-world system using a domain-analysis algorithm. To show the range of models for which this approach is applicable, we demonstrate our model-generation process using two examples of model generation optimized for a specific domain: (1) model-based diagnosis for discrete Boolean circuits, and (2) E.coli TRN networks for simulating gene expression.

#index 1270110
#* Grounding with bounds
#@ Johan Wittocx;Maarten Mariën;Marc Denecker
#t 2008
#c 10
#% 560057
#% 560731
#% 560925
#% 587584
#% 1269426
#% 1274777
#% 1405524
#! Grounding is the task of reducing a first-order theory to an equivalent propositional one. Typical grounders work on a sentence-by-sentence level, substituting variables by domain elements and simplifying where possible. In this work, we propose a method for reasoning on the first-order theory as a whole to optimize the grounding process. Concretely, we develop an algorithm that computes bounds for subformulas. Such bounds indicate for which tuples the subformulas are certainly true and for which they are certainly false. These bounds can then be used by standard grounding algorithms to substantially reduce grounding sizes, and consequently also grounding times. We have implemented the method, and demonstrate its practical applicability.

#index 1270111
#* Towards automatic animated storyboarding
#@ Patrick Ye;Timothy Baldwin
#t 2008
#c 10
#% 375503
#% 452991
#% 823311
#% 916804
#% 939353
#% 1277326
#% 1289522
#! In this paper, we propose a machine learning-based NLP system for automatically creating animated storyboards using the action descriptions of movie scripts. We focus particularly on the importance of verb semantics when generating graphics commands, and find that semantic role labelling boosts performance and is relatively robust to the effects of unseen verbs.

#index 1270112
#* Loop formulas for logic programs with arbitrary constraint atoms
#@ Jia-Huai You;Guohua Liu
#t 2008
#c 10
#% 417651
#% 464918
#% 772065
#% 947786
#% 1250132
#% 1269746
#% 1272128
#% 1272162
#% 1279247
#% 1839540
#! We formulate loop formulas for logic programs with arbitrary constraint atoms, for the semantics based on conditional satisfaction. This provides a method for answer set computation by computing models of completion. One particular attractive candidate for the latter task is pseudo-boolean constraint solvers. To strengthen this connection, we show examples of compact encoding of aggregates and global constraints by pseudo-boolean constraints.

#index 1270113
#* Revising imprecise probabilistic beliefs in the framework of probabilistic logic programming
#@ Anbu Yue;Weiru Liu
#t 2008
#c 10
#% 109945
#% 224753
#% 292510
#% 336009
#% 490492
#% 495759
#% 503981
#% 806734
#% 808860
#% 943104
#% 1271998
#% 1274790
#% 1275150
#% 1650339
#% 1650584
#% 1650720
#! Probabilistic logic programming is a powerful technique to represent and reason with imprecise probabilistic knowledge. A probabilistic logic program (PLP) is a knowledge base which contains a set of conditional events with probability intervals. In this paper, we investigate the issue of revising such a PLP in light of receiving new information. We propose postulates for revising PLPs when a new piece of evidence is also a probabilistic conditional event. Our postulates lead to Jeffrey's rule and Bayesian conditioning when the original PLP defines a single probability distribution. Furthermore, we prove that our postulates are extensions to Darwiche and Pearl (DP) postulates when new evidence is a propositional formula. We also give the representation theorem for the postulates and provide an instantiation of revision operators satisfying the proposed postulates.

#index 1270182
#* Proceedings of the 23rd national conference on Artificial intelligence - Volume 2
#@ Anthony Cohn
#t 2008
#c 10

#index 1270183
#* Distance metric learning vs. Fisher discriminant analysis
#@ Babak Alipanahi;Michael Biggs;Ali Ghodsi
#t 2008
#c 10
#% 757953
#% 769881
#% 770782
#% 770813
#% 1274883
#! There has been much recent attention to the problem of learning an appropriate distance metric, using class labels or other side information. Some proposed algorithms are iterative and computationally expensive. In this paper, we show how to solve one of these methods with a closed-form solution, rather than using semidefinite programming. We provide a new problem setup in which the algorithm performs better or as well as some standard methods, but without the computational complexity. Furthermore, we show a strong relationship between these methods and the Fisher Discriminant Analysis.

#index 1270184
#* Potential-based shaping in model-based reinforcement learning
#@ John Asmuth;Michael L. Littman;Robert Zinkov
#t 2008
#c 10
#% 124691
#% 174161
#% 181627
#% 363744
#% 464778
#% 466230
#% 722895
#% 1269760
#% 1271996
#! Potential-based shaping was designed as a way of introducing background knowledge into model-free reinforcement-learning algorithms. By identifying states that are likely to have high value, this approach can decrease experience complexity--the number of trials needed to find near-optimal behavior. An orthogonal way of decreasing experience complexity is to use a model-based learning approach, building and exploiting an explicit transition model. In this paper, we show how potential-based shaping can be redefined to work in the model-based setting to produce an algorithm that shares the benefits of both ideas.

#index 1270185
#* Sparse projections over graph
#@ Deng Cai;Xiaofei He;Jiawei Han
#t 2008
#c 10
#% 80995
#% 643008
#% 729437
#% 791402
#% 835741
#% 837604
#% 876025
#! Recent study has shown that canonical algorithms such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) can be obtained from graph based dimensionality reduction framework. However, these algorithms yield projective maps which are linear combination of all the original features. The results are difficult to be interpreted psychologically and physiologically. This paper presents a novel technique for learning a sparse projection over graphs. The data in the reduced subspace is represented as a linear combination of a subset of the most relevant features. Comparing to PCA and LDA, the results obtained by sparse projection are often easier to be interpreted. Our algorithm is based on a graph embedding model, which encodes the discriminating and geometrical structure in terms of the data affinity. Once the embedding results are obtained, we then apply regularized regression for learning a set of sparse basis functions. Specifically, by using L1-norm regularizer (e.g. lasso), the sparse projections can be efficiently computed. Experimental results on two document databases demonstrate the effectiveness of our method.

#index 1270186
#* Clustering via random walk hitting time on directed graphs
#@ Mo Chen;Jianzhuang Liu;Xiaoou Tang
#t 2008
#c 10
#% 313959
#% 724227
#% 975021
#! In this paper, we present a general data clustering algorithm which is based on the asymmetric pairwise measure of Markov random walk hitting time on directed graphs. Unlike traditional graph based clustering methods, we do not explicitly calculate the pairwise similarities between points. Instead, we form a transition matrix of Markov random walk on a directed graph directly from the data. Our algorithm constructs the probabilistic relations of dependence between local sample pairs by studying the local distributions of the data. Such dependence relations are asymmetric, which is a more general measure of pairwise relations than the similarity measures in traditional undirected graph based methods in that it considers both the local density and geometry of the data. The probabilistic relations of the data naturally result in a transition matrix of Markov random walk. Based on the random walk viewpoint, we compute the expected hitting time for all sample pairs, which explores the global information of the structure of the underlying directed graph. An asymmetric measure based clustering algorithm, called K-destinations, is proposed for partitioning the nodes of the directed graph into disjoint sets. By utilizing the local distribution information of the data and the global structure information of the directed graph, our method is able to conquer some limitations of traditional pairwise similarity based methods. Experimental results are provided to validate the effectiveness of the proposed approach.

#index 1270187
#* Integrating multiple learning components through Markov logic
#@ Thomas G. Dietterich;Xinlong Bao
#t 2008
#c 10
#% 252011
#% 316509
#% 840890
#% 850430
#% 1250579
#% 1250584
#% 1703524
#! This paper addresses the question of how statistical learning algorithms can be integrated into a larger AI system both from a practical engineering perspective and from the perspective of correct representation, learning, and reasoning. Our goal is to create an integrated intelligent system that can combine observed facts, hand-written rules, learned rules, and learned classifiers to perform joint learning and reasoning. Our solution, which has been implemented in the CALO system, integrates multiple learning components with a Markov Logic inference engine, so that the components can benefit from each other's predictions. We introduce two designs of the learning and reasoning layer in CALO: the MPE Architecture and the Marginal Probability Architecture. The architectures, interfaces, and algorithms employed in our two designs are described, followed by experimental evaluations of the performance of the two designs. We show that by integrating multiple learning components through Markov Logic, the performance of the system can be improved and that the Marginal Probability Architecture performs better than the MPE Architecture.

#index 1270188
#* A case study on the critical role of geometric regularity in machine learning
#@ Jason Gauci;Kenneth O. Stanley
#t 2008
#c 10
#% 384911
#% 449980
#% 506144
#% 643383
#% 976971
#% 986688
#% 986691
#% 1180985
#% 1269760
#% 1272024
#% 1705342
#% 1777179
#! An important feature of many problem domains in machine learning is their geometry. For example, adjacency relationships, symmetries, and Cartesian coordinates are essential to any complete description of board games, visual recognition, or vehicle control. Yet many approaches to learning ignore such information in their representations, instead inputting flat parameter vectors with no indication of how those parameters are situated geometrically. This paper argues that such geometric information is critical to the ability of any machine learning approach to effectively generalize; even a small shift in the configuration of the task in space from what was experienced in training can go wholly unrecognized unless the algorithm is able to learn the regularities in decision-making across the problem geometry. To demonstrate the importance of learning from geometry, three variants of the same evolutionary learning algorithm (NeuroEvolution of Augmenting Topologies), whose representations vary in their capacity to encode geometry, are compared in checkers. The result is that the variant that can learn geometric regularities produces a significantly more general solution. The conclusion is that it is important to enable machine learning to detect and thereby learn from the geometry of its problems.

#index 1270189
#* Semi-supervised ensemble ranking
#@ Steven C. H. Hoi;Rong Jin
#t 2008
#c 10
#% 169777
#% 262096
#% 269217
#% 330769
#% 387427
#% 577224
#% 734915
#% 766414
#% 840583
#% 840846
#% 875948
#% 879588
#% 956542
#% 983805
#% 983820
#% 987226
#% 987227
#% 987241
#% 1455666
#! Ranking plays a central role in many Web search and information retrieval applications. Ensemble ranking, sometimes called meta-search, aims to improve the retrieval performance by combining the outputs from multiple ranking algorithms. Many ensemble ranking approaches employ supervised learning techniques to learn appropriate weights for combining multiple rankers. The main shortcoming with these approaches is that the learned weights for ranking algorithms are query independent. This is suboptimal since a ranking algorithm could perform well for certain queries but poorly for others. In this paper, we propose a novel semi-supervised ensemble ranking (SSER) algorithm that learns query-dependent weights when combining multiple rankers in document retrieval. The proposed SSER algorithm is formulated as an SVM-like quadratic program (QP), and therefore can be solved efficiently by taking advantage of optimization techniques that were widely used in existing SVM solvers. We evaluated the proposed technique on a standard document retrieval testbed and observed encouraging results by comparing to a number of state-of-the-art techniques.

#index 1270190
#* Instance-level semisupervised multiple instance learning
#@ Yangqing Jia;Changshui Zhang
#t 2008
#c 10
#% 224755
#% 272527
#% 565537
#% 576520
#% 771844
#% 876033
#% 902511
#% 983950
#! Multiple instance learning (MIL) is a branch of machine learning that attempts to learn information from bags of instances. Many real-world applications such as localized content-based image retrieval and text categorization can be viewed as MIL problems. In this paper, we propose a new graph-based semi-supervised learning approach for multiple instance learning. By defining an instance-level graph on the data, we first propose a new approach to construct an optimization framework for multiple instance semi-supervised learning, and derive an efficient way to overcome the non-convexity of MIL. We empirically show that our method outperforms state-of-the-art MIL algorithms on several real-world data sets.

#index 1270191
#* Zero-data learning of new tasks
#@ Hugo Larochelle;Dumitru Erhan;Yoshua Bengio
#t 2008
#c 10
#% 236497
#% 397155
#% 716065
#% 723239
#% 829043
#% 983899
#% 1269807
#% 1455666
#% 1861204
#! We introduce the problem of zero-data learning, where a model must generalize to classes or tasks for which no training data are available and only a description of the classes or tasks are provided. Zero-data learning is useful for problems where the set of classes to distinguish or tasks to solve is very large and is not entirely covered by the training data. The main contributions of this work lie in the presentation of a general formalization of zero-data learning, in an experimental analysis of its properties and in empirical evidence showing that generalization is possible and significant in this context. The experimental work of this paper addresses two classification problems of character recognition and a multitask ranking problem in the context of drug discovery. Finally, we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards AI, where an agent can be given a specification for a learning problem before attempting to solve it (with very few or even zero examples).

#index 1270192
#* Dimension amnesic pyramid match kernel
#@ Yi Liu;Xu-Lei Wang;Hongbin Zha
#t 2008
#c 10
#% 325683
#% 378066
#% 443975
#% 722904
#% 734914
#% 743284
#% 760805
#% 765080
#% 812495
#% 836859
#% 866538
#% 883971
#% 919460
#% 961270
#% 1014686
#! With the success of local features in object recognition, feature-set representations are widely used in computer vision and related domains. Pyramid match kernel (PMK) is an efficient approach to quantifying the similarity between two unordered feature-sets, which allows well established kernel machines to learn with such representations. However, the approximation of PMK to the optimal feature matches deteriorates linearly with the dimension of local features, which prohibits the direct use of high dimensional features. In this paper, we propose a general, data-independent kernel to quantify the feature-set similarities, which gives an upper bound of approximation error independent of the dimension of local features. The key idea is to employ the technique of normal random projection to construct a number of low dimensional subspaces, and perform the original PMK algorithm therein. By leveraging on the invariance property of p-stable distributions, our approach achieves the desirable dimension-free property. Extensive experiments on the ETH-80 image database solidly demonstrate the advantage of our approach to high dimensional features.

#index 1270193
#* Clustering on complex graphs
#@ Bo Long;Mark Zhang;Philip S. Yu;Tianbing Xu
#t 2008
#c 10
#% 148149
#% 202286
#% 274612
#% 313959
#% 342621
#% 466675
#% 578670
#% 769928
#% 823328
#% 881487
#% 1117017
#! Complex graphs, in which multi-type nodes are linked to each other, frequently arise in many important applications, such as Web mining, information retrieval, bioinformatics, and epidemiology. In this study, We propose a general framework for clustering on complex graphs. Under this framework, we derive a family of clustering algorithms including both hard and soft versions, which are capable of learning cluster patterns from complex graphs with various structures and statistical properties. We also establish the connections between the proposed framework and the traditional graph partitioning approaches. The experimental evaluation provides encouraging results to validate the proposed framework and algorithms.

#index 1270194
#* From comparing clusterings to combining clusterings
#@ Zhiwu Lu;Yuxin Peng;Jianguo Xiao
#t 2008
#c 10
#% 345829
#% 420081
#% 571905
#% 578670
#% 727903
#% 803762
#% 837616
#% 975154
#! This paper presents a fast simulated annealing framework for combining multiple clusterings (i.e. clustering ensemble) based on some measures of agreement between partitions, which are originally used to compare two clusterings (the obtained clustering vs. a ground truth clustering) for the evaluation of a clustering algorithm. Though we can follow a greedy strategy to optimize these measures as objective functions of clustering ensemble, some local optima may be obtained and simultaneously the computational cost is too large. To avoid the local optima, we then consider a simulated annealing optimization scheme that operates through single label changes. Moreover, for these measures between partitions based on the relationship (joined or separated) of pairs of objects such as Rand index, we can update them incrementally for each label change, which makes sure the simulated annealing optimization scheme is computationally feasible. The simulation and real-life experiments then demonstrate that the proposed framework can achieve superior results.

#index 1270195
#* Trace ratio criterion for feature selection
#@ Feiping Nie;Shiming Xiang;Yangqing Jia;Changshui Zhang;Shuicheng Yan
#t 2008
#c 10
#% 243728
#% 313959
#% 361100
#% 466912
#% 593047
#% 720010
#% 722929
#% 771842
#% 791402
#% 983948
#% 1274913
#! Fisher score and Laplacian score are two popular feature selection algorithms, both of which belong to the general graph-based feature selection framework. In this framework, a feature subset is selected based on the corresponding score (subset-level score), which is calculated in a trace ratio form. Since the number of all possible feature subsets is very huge, it is often prohibitively expensive in computational cost to search in a brute force manner for the feature subset with the maximum subset-level score. Instead of calculating the scores of all the feature subsets, traditional methods calculate the score for each feature, and then select the leading features based on the rank of these feature-level scores. However, selecting the feature subset based on the feature-level score cannot guarantee the optimum of the subset-level score. In this paper, we directly optimize the subset-level score, and propose a novel algorithm to efficiently find the global optimal feature subset such that the subset-level score is maximized. Extensive experiments demonstrate the effectiveness of our proposed algorithm in comparison with the traditional methods for feature selection.

#index 1270196
#* Transfer learning via dimensionality reduction
#@ Sinno Jialin Pan;James T. Kwok;Qiang Yang
#t 2008
#c 10
#% 236497
#% 722798
#% 763697
#% 770767
#% 916788
#% 961218
#% 983828
#% 983899
#% 1261539
#% 1269847
#! Transfer learning addresses the problem of how to utilize plenty of labeled data in a source domain to solve related but different problems in a target domain, even when the training and testing problems have different distributions or features. In this paper, we consider transfer learning via dimensionality reduction. To solve this problem, we learn a low-dimensional latent feature space where the distributions between the source domain data and the target domain data are the same or close to each other. Onto this latent feature space, we project the data in related domains where we can apply standard learning algorithms to train classification or regression models. Thus, the latent feature space can be treated as a bridge of transferring knowledge from the source domain to the target domain. The main contribution of our work is that we propose a new dimensionality reduction method to find a latent space, which minimizes the distance between distributions of the data in different domains in a latent space. The effectiveness of our approach to transfer learning is verified by experiments in two real world applications: indoor WiFi localization and binary text classification.

#index 1270197
#* Active learning for pipeline models
#@ Dan Roth;Kevin Small
#t 2008
#c 10
#% 464268
#% 466231
#% 722797
#% 854636
#% 938727
#% 1261597
#% 1269476
#% 1272282
#% 1665159
#! For many machine learning solutions to complex applications, there are significant performance advantages to decomposing the overall task into several simpler sequential stages, commonly referred to as a pipeline model. Typically, such scenarios are also characterized by high sample complexity, motivating the study of active learning for these situations. While most active learning research examines single predictions, we extend such work to applications which utilize pipelined predictions. Specifically, we present an adaptive strategy for combining local active learning strategies into one that minimizes the annotation requirements for the overall task. Empirical results for a three-stage entity and relation extraction system demonstrate a significant reduction in supervised data requirements when using the proposed method.

#index 1270198
#* Economic hierarchical Q-learning
#@ Erik G. Schultink;Ruggiero Cavallo;David C. Parkes
#t 2008
#c 10
#% 272662
#% 578674
#% 1271827
#% 1290043
#! Hierarchical state decompositions address the curse-of-dimensionality in Q-learning methods for reinforcement learning (RL) but can suffer from suboptimality. In addressing this, we introduce the Economic Hierarchical Q-Learning (EHQ) algorithm for hierarchical RL. The EHQ algorithm uses subsidies to align interests such that agents that would otherwise converge to a recursively optimal policy will instead be motivated to act hierarchically optimally. The essential idea is that a parent will pay a child for the relative value to the rest of the system for "returning the world" in one state over another state. The resulting learning framework is simple compared to other algorithms that obtain hierarchical optimality. Additionally, EHQ encapsulates relevant information about value tradeoffs faced across the hierarchy at each node and requires minimal data exchange between nodes. We provide no theoretical proof of hierarchical optimality but are able demonstrate success with EHQ in empirical results.

#index 1270199
#* Markov blanket feature selection for support vector machines
#@ Jianqiang Shen;Lida Li;Weng-Keen Wong
#t 2008
#c 10
#% 70370
#% 197387
#% 262059
#% 287399
#% 402289
#% 465754
#% 496116
#% 770828
#% 926881
#% 931396
#% 1650289
#! Based on Information Theory, optimal feature selection should be carried out by searching Markov blankets. In this paper, we formally analyze the current Markov blanket discovery approach for support vector machines and propose to discover Markov blankets by performing a fast heuristic Bayesian network structure learning. We give a sufficient condition that our approach will improve the performance. Two major factors that make it prohibitive for learning Bayesian networks from high-dimensional data sets are the large search space and the expensive cycle detection operations. We propose to restrict the search space by only considering the promising candidates and detect cycles using an online topological sorting method. Experimental results show that we can efficiently reduce the feature dimensionality while preserving a high degree of classification accuracy.

#index 1270200
#* On-line case-based plan adaptation for real-time strategy games
#@ Neha Sugandh;Santiago Ontañón;Ashwin Ram
#t 2008
#c 10
#% 120806
#% 176887
#% 399855
#% 803704
#% 1123847
#% 1272367
#% 1705996
#! Traditional artificial intelligence techniques do not perform well in applications such as real-time strategy games because of the extensive search spaces which need to be explored. In addition, this exploration must be carried out on-line during performance time; it cannot be precomputed. We have developed on-line case-based planning techniques that are effective in such domains. In this paper, we extend our earlier work using ideas from traditional planning to inform the real-time adaptation of plans. In our framework, when a plan is retrieved, a plan dependency graph is inferred to capture the relations between actions in the plan. The plan is then adapted in real-time using its plan dependency graph. This allows the system to create and adapt plans in an efficient and effective manner while performing the task. The approach is evaluated using WARGUS, a well-known real-time strategy game.

#index 1270201
#* Adapting ADtrees for high arity features
#@ Robert Van Dam;Irene Langkilde-Geary;Dan Ventura
#t 2008
#c 10
#% 466412
#% 853850
#% 876041
#% 1272326
#! ADtrees, a data structure useful for caching sufficient statistics, have been successfully adapted to grow lazily when memory is limited and to update sequentially with an incrementally updated dataset. For low arity symbolic features, ADtrees trade a slight increase in query time for a reduction in overall tree size. Unfortunately, for high arity features, the same technique can often result in a very large increase in query time and a nearly negligible tree size reduction. In the dynamic (lazy) version of the tree, both query time and tree size can increase for some applications. Here we present two modifications to the ADtree which can be used separately or in combination to achieve the originally intended space-time tradeoff in the ADtree when applied to datasets containing very high arity features.

#index 1270202
#* Efficient learning of action schemas and web-service descriptions
#@ Thomas J. Walsh;Michael L. Littman
#t 2008
#c 10
#% 174161
#% 239778
#% 451055
#% 1250372
#% 1269832
#% 1269891
#% 1272161
#% 1272181
#! This work addresses the problem of efficiently learning action schemas using a bounded number of samples (interactions with the environment). We consider schemas in two languages--traditional STRIPS, and a new language STRIPS+WS that extends STRIPS to allow for the creation of new objects when an action is executed. This modification allows STRIPS+WS to model web services and can be used to describe web-service composition (planning) problems. We show that general STRIPS operators cannot be efficiently learned through raw experience, though restricting the size of action preconditions yields a positive result. We then show that efficient learning is possible without this restriction if an agent has access to a "teacher" that can provide solution traces on demand. We adapt this learning algorithm to efficiently learn web-service descriptions in STRIPS+WS.

#index 1270203
#* On discriminative semi-supervised classification
#@ Fei Wang;Changshui Zhang
#t 2008
#c 10
#% 80995
#% 252011
#% 311027
#% 466263
#% 565545
#% 840938
#% 961218
#! The recent years have witnessed a surge of interests in semi-supervised learning methods. A common strategy for these algorithms is to require that the predicted data labels should be sufficiently smooth with respect to the intrinsic data manifold. In this paper, we argue that rather than penalizing the label smoothness, we can directly punish the discriminality of the classification function to achieve a more powerful predictor, and we derive two specific algorithms: Semi-Supervised Discriminative Regularization (SSDR) and Semi-parametric Discriminative Semi-supervised Classification (SDSC). Finally many experimental results are presented to show the effectiveness of our method.

#index 1270204
#* Semi-supervised classification using local and global regularization
#@ Fei Wang;Tao Li;Gang Wang;Changshui Zhang
#t 2008
#c 10
#% 143194
#% 190581
#% 390723
#% 393059
#% 875968
#% 876068
#% 916790
#% 961218
#% 1269778
#% 1705532
#% 1705533
#! In this paper, we propose a semi-supervised learning (SSL) algorithm based on local and global regularization. In the local regularization part, our algorithm constructs a regularized classifier for each data point using its neighborhood, while the global regularization part adopts a Laplacian regularizer to smooth the data labels predicted by those local classifiers. We show that some existing SSL algorithms can be derived from our framework. Finally we present some experimental results to show the effectiveness of our method.

#index 1270205
#* Learning hidden curved exponential family models to infer face-to-face interaction networks from situated speech data
#@ Danny Wyatt;Tanzeem Choudhury;Jeff Bilmes
#t 2008
#c 10
#% 464434
#% 777522
#% 840577
#% 983844
#% 1650403
#! In this paper, we present a novel probabilistic framework for recovering global, latent social network structure from local, noisy observations. We extend curved exponential random graph models to include two types of variables: hidden variables that capture the structure of the network and observational variables that capture the behavior between actors in the network. We develop a novel combination of informative and intuitive conversational (local) and structural (global) features to specify our model. The model learns, in an unsupervised manner, the relationship between observable behavior and hidden social structure while simultaneously learning properties of the latent structure itself. We present empirical results on both synthetic data and a real world dataset of face-to-face conversations collected from 24 individuals using wearable sensors over the course of 6 months.

#index 1270206
#* Hidden dynamic probabilistic models for labeling sequence data
#@ Xiaofeng Yu;Wai Lam
#t 2008
#c 10
#% 44876
#% 73441
#% 75936
#% 196896
#% 464434
#% 466892
#% 643004
#% 716892
#% 770844
#% 816181
#% 840966
#% 853697
#% 875974
#% 939917
#% 1250184
#! We propose a new discriminative framework, namely Hidden Dynamic Conditional Random Fields (HDCRFs), for building probabilistic models which can capture both internal and external class dynamics to label sequence data. We introduce a small number of hidden state variables to model the sub-structure of a observation sequence and learn dynamics between different class labels. An HDCRF offers several advantages over previous discriminative models and is attractive both, conceptually and computationally. We performed experiments on three well-established sequence labeling tasks in natural language, including part-of-speech tagging, noun phrase chunking, and named entity recognition. The results demonstrate the validity and competitiveness of our model. In addition, our model compares favorably with current state-of-the-art sequence labeling approach, Conditional Random Fields (CRFs), which can only model the external dynamics.

#index 1270207
#* Classification by discriminative regularization
#@ Bin Zhang;Fei Wang;Ta-Hsin Li;Wen Jun Yin;Jin Dong
#t 2008
#c 10
#% 80995
#% 190581
#% 344447
#% 729437
#% 732522
#% 806973
#% 916790
#% 961218
#% 1455666
#% 1861638
#! Classification is one of the most fundamental problems in machine learning, which aims to separate the data from different classes as far away as possible. A common way to get a good classification function is to minimize its empirical prediction loss or structural loss. In this paper, we point out that we can also enhance the discriminality of those classifiers by further incorporating the discriminative information contained in the data set as a prior into the classifier construction process. In such a way, we will show that the constructed classifiers will be more powerful, and this will also be validated by the final empirical study on several benchmark data sets.

#index 1270208
#* Multi-view local learning
#@ Dan Zhang;Fei Wang;Changshui Zhang;Tao Li
#t 2008
#c 10
#% 25998
#% 143194
#% 190581
#% 252011
#% 393059
#% 420495
#% 464267
#% 722902
#% 724227
#% 875962
#% 876068
#% 881557
#% 905823
#% 983934
#% 983949
#% 987253
#% 1269774
#! The idea of local learning, i.e., classifying a particular example based on its neighbors, has been successfully applied to many semi-supervised and clustering problems recently. However, the local learning methods developed so far are all devised for single-view problems. In fact, in many real-world applications, examples are represented by multiple sets of features. In this paper, we extend the idea of local learning to multi-view problem, design a multi-view local model for each example, and propose a Multi-View Local Learning Regularization (MVLL-Reg) matrix. Both its linear and kernel version are given. Experiments are conducted to demonstrate the superiority of the proposed method over several state-of-the-art ones.

#index 1270209
#* Constraint projections for ensemble learning
#@ Daoqiang Zhang;Songcan Chen;Zhi-Hua Zhou;Qiang Yang
#t 2008
#c 10
#% 209021
#% 256615
#% 283145
#% 400847
#% 742990
#% 829025
#% 871632
#% 889165
#% 926881
#% 961245
#% 989616
#% 989642
#% 1781594
#! It is well-known that diversity among base classifiers is crucial for constructing a strong ensemble. Most existing ensemble methods obtain diverse individual learners through resampling the instances or features. In this paper, we propose an alternative way for ensemble construction by resampling pairwise constraints that specify whether a pair of instances belongs to the same class or not. Using pairwise constraints for ensemble construction is challenging because it remains unknown how to influence the base classifiers with the sampled pairwise constraints. We solve this problem with a two-step process. First, we transform the original instances into a new data representation using projections learnt from pairwise constraints. Then, we build the base classifiers with the new data representation. We propose two methods for resampling pairwise constraints following the standard Bagging and Boosting algorithms, respectively. Extensive experiments validate the effectiveness of our method.

#index 1270210
#* Automating to-do lists for users: interpretation of to-dos for selecting and tasking agents
#@ Yolanda Gil;Varun Ratnakar
#t 2008
#c 10
#% 398947
#% 452634
#% 751870
#% 816156
#% 828972
#% 890471
#% 1065188
#% 1269447
#! To-do lists have been found to be the most popular personal information management tools, yet there is no automated system to interpret and act upon them when appropriate on behalf of the user. Automating to-do lists is challenging, not only because they are specified as free text but also because most items contain abbreviated tasks, many do not specify an action to be performed, and often refer to unrelated (personal) items. This paper presents our approach and an implemented system to process to-do list entries and map them to tasks that can be automated for the user by a set of agents. Since the format of to-do entries is not very amenable to natural language processing tools that can parse and create a structured interpretation, our approach is to exploit paraphrases of the target tasks that the agents can perform and that specify how the free-text maps to the task arguments. As users manually assign to-do to agents for automation, our system improves its performance by learning new paraphrases. We show an evaluation of our approach in a corpus of 2100 to-do entries collected from users of an office assistant multi-agent system.

#index 1270211
#* Proactive intrusion detection
#@ Benjamin Liebald;Dan Roth;Neelay Shah;Vivek Srikumar
#t 2008
#c 10
#% 546063
#% 769885
#% 781957
#% 823397
#% 825887
#% 840913
#! Machine learning systems are deployed in many adversarial conditions like intrusion detection, where a classifier has to decide whether a sequence of actions come from a legitimate user or not. However, the attacker, being an adversarial agent, could reverse engineer the classifier and successfully masquerade as a legitimate user. In this paper, we propose the notion of a Proactive Intrusion Detection System (IDS) that can counter such attacks by incorporating feedback into the process. A proactive IDS influences the user's actions and observes them in different situations to decide whether the user is an intruder. We present a formal analysis of proactive intrusion detection and extend the adversarial relationship between the IDS and the attacker to present a game theoretic analysis. Finally, we present experimental results on real and synthetic data that confirm the predictions of the analysis.

#index 1270212
#* Speech-enabled card games for language learners
#@ Ian McGraw;Stephanie Seneff
#t 2008
#c 10
#! This paper debuts a novel application of speech recognition to foreign language learning. We present a generic framework for developing user-customizable card games designed to aid learners in the difficult task of vocabulary acquisition. We also describe a prototype game built on this framework that, using a Mandarin speech recognizer, provides a student of Chinese with opportunities to speak vocabulary items in a meaningful context. The system dynamically loads only the necessary vocabulary for each game in an effort to maintain robust recognition performance without limiting the lexical domain. To assess the Sentence Error Rate (SER) of our prototype, we asked college-age students from various universities in the United States and beyond to participate in a Web-based user study. The three central concepts in the game were recognized with a SER of 16.02%, illustrating the feasibility of deploying this system in a university curriculum via the Internet. Finally, to ensure that our recognizer is behaving appropriately with regard to learner speech, we perform a rigorous analysis of the recognition errors to determine their underlying causes.

#index 1270213
#* Exposing parameters of a trained dynamic model for interactive music creation
#@ Dan Morris;Ian Simon;Sumit Basu
#t 2008
#c 10
#% 296534
#% 350120
#% 428235
#% 452563
#% 581658
#% 735743
#% 751804
#% 783871
#% 790445
#% 848637
#% 848639
#% 860126
#% 916815
#% 926881
#% 936917
#% 936931
#% 954903
#% 954914
#% 954915
#% 1047296
#% 1047379
#% 1650372
#% 1767359
#% 1775675
#! As machine learning (ML) systems emerge in end-user applications, learning algorithms and classifiers will need to be robust to an increasingly unpredictable operating environment. In many cases, the parameters governing a learning system cannot be optimized for every user scenario, nor can users typically manipulate parameters defined in the space and terminology of ML. Conventional approaches to user-oriented ML systems have typically hidden this complexity from users by automating parameter adjustment. We propose a new paradigm, in which model and algorithm parameters are exposed directly to end-users with intuitive labels, suitable for applications where parameters cannot be automatically optimized or where there is additional motivation - such as creative flexibility - to expose, rather than fix or automatically adapt, learning parameters. In our CHI 2008 paper, we introduced and evaluated MySong, a system that uses a Hidden Markov Model to generate chords to accompany a vocal melody. The present paper formally describes the learning underlying MySong and discusses the mechanisms by which MySong's learning parameters are exposed to users, as a case study in making ML systems user-configurable. We discuss the generalizability of this approach, and propose that intuitively exposing ML parameters is a key challenge for the ML and human-computer-interaction communities.

#index 1270214
#* Another look at search-based drama management
#@ Mark J. Nelson;Michael Mateas
#t 2008
#c 10
#% 114760
#% 385483
#% 643172
#% 705161
#% 890319
#% 1024698
#% 1024875
#% 1084365
#% 1250348
#% 1269805
#! A drama manager (DM) monitors an interactive experience, such as a computer game, and intervenes to shape the global experience so it satisfies the author's expressive goals without decreasing a player's interactive agency. In declarative optimization-based drama management (DODM), the author declaratively specifies desired properties of the experience; the DM optimizes its interventions to maximize that metric. The initial DODM approach used online search to optimize an experience-quality function. Subsequent work questioned whether online search could perform well in general, and proposed alternative optimization frameworks such as reinforcement learning. Recent work on targeted trajectory distribution Markov decision processes (TTD-MDPs) replaced the experience-quality metric with a metric and associated algorithm based on targeting experience distributions. We argue that optimizing an experience-quality function does not destroy interactive agency, as has been claimed, and that in fact it can capture that goal directly. We further show that, though apparently quite different on the surface, the original search approach and TTD-MDPs actually use variants of the same underlying search algorithm, and that offline cached search, as is done by the TTD-MDP algorithm, allows the search-based systems to achieve similar results to TTD-MDPs.

#index 1270215
#* Learning to analyze binary computer code
#@ Nathan Rosenblum;Xiaojin Zhu;Barton Miller;Karen Hunt
#t 2008
#c 10
#% 272514
#% 330995
#% 438224
#% 464434
#% 615416
#% 725311
#% 808595
#% 963798
#% 983874
#! We present a novel application of structured classification: identifying function entry points (FEPs, the starting byte of each function) in program binaries. Such identification is the crucial first step in analyzing many malicious, commercial and legacy software, which lack full symbol information that specifies FEPs. Existing pattern-matching FEP detection techniques are insufficient due to variable instruction sequences introduced by compiler and link-time optimizations. We formulate the FEP identification problem as structured classification using Conditional Random Fields. Our Conditional Random Fields incorporate both idiom features to represent the sequence of instructions surrounding FEPs, and control flow structure features to represent the interaction among FEPs. These features allow us to jointly label all FEPs in the binary. We perform feature selection and present an approximate inference method for massive program binaries. We evaluate our models on a large set of real-world test binaries, showing that our models dramatically outperform two existing, standard disassemblers.

#index 1270216
#* Prediction and change detection in sequential data for interactive applications
#@ Jun Zhou;Li Cheng;Walter F. Bischof
#t 2008
#c 10
#% 135968
#% 165663
#% 196761
#% 203297
#% 220706
#% 223576
#% 297164
#% 451055
#% 815974
#% 824952
#% 840853
#% 840875
#% 867760
#% 1016144
#% 1759695
#! We consider the problems of sequential prediction and change detection that arise often in interactive applications: A semi-automatic predictor is applied to a time-series and is expected to make proper predictions and request new human input when change points are detected. Motivated by the Transductive Support Vector Machines (Vapnik 1998), we propose an online framework that naturally addresses these problems in a unified manner. Our empirical study with a synthetic dataset and a road tracking dataset demonstrates the efficacy of the proposed approach.

#index 1270217
#* Using knowledge driven matrix factorization to reconstruct modular gene regulatory network
#@ Yang Zhou;Zheng Li;Xuerui Yang;Linxia Zhang;Shireesh Srivastava;Rong Jin;Christina Chan
#t 2008
#c 10
#% 465754
#% 832997
#% 989616
#! Reconstructing gene networks from micro-array data can provide information on the mechanisms that govern cellular processes. Numerous studies have been devoted to addressing this problem. A popular method is to view the gene network as a Bayesian inference network, and to apply structure learning methods to determine the topology of the gene network. There are, however, several shortcomings with the Bayesian structure learning approach for reconstructing gene networks. They include high computational cost associated with analyzing a large number of genes and inefficiency in exploiting prior knowledge of co-regulation that could be derived from Gene Ontology (GO) information. In this paper, we present a knowledge driven matrix factorization (KMF) framework for reconstructing modular gene networks that addresses these shortcomings. In KMF, gene expression data is initially used to estimate the correlation matrix. The gene modules and the interactions among the modules are derived by factorizing the correlation matrix. The prior knowledge in GO is integrated into matrix factorization to help identify the gene modules. An alternating optimization algorithm is presented to efficiently find the solution. Experiments show that our algorithm performs significantly better in identifying gene modules than several state-of-the-art algorithms, and the interactions among the modules uncovered by our algorithm are proved to be biologically meaningful.

#index 1270218
#* Using answer set programming and lambda calculus to characterize natural language sentences with normatives and exceptions
#@ Chita Baral;Juraj Dzifcak;Tran Cao Son
#t 2008
#c 10
#% 297229
#% 400986
#% 411814
#% 417651
#% 421932
#% 939932
#% 1396564
#% 1404447
#! One way to solve the knowledge acquisition bottleneck is to have ways to translate natural language sentences and discourses to a formal knowledge representation language, especially ones that are appropriate to express domain knowledge in sciences, such as Biology. While there have been several proposals, including by Montague (1970), to give model theoretic semantics for natural language and to translate natural language sentences and discourses to classical logic, none of these approaches use knowledge representation languages that can express domain knowledge involving normative statements and exceptions. In this paper we take a first step to illustrate how one can automatically translate natural language sentences about normative statements and exceptions to representations in the knowledge representation language Answer Set Programming (ASP). To do this, we use λ-calculus representation of words and their composition as dictated by a CCG grammar.

#index 1270219
#* Automatic semantic relation extraction with multiple boundary generation
#@ Brandon Beamer;Alla Rozovskaya;Roxana Girju
#t 2008
#c 10
#% 452994
#% 939965
#% 983584
#% 1250630
#% 1271258
#% 1271340
#! This paper addresses the task of automatic classification of semantic relations between nouns. We present an improved WordNet-based learning model which relies on the semantic information of the constituent nouns. The representation of each noun's meaning captures conceptual features which play a key role in the identification of the semantic relation. We report substantial improvements over previous WordNet-based methods on the 2007 SemEval data. Moreover, our experiments show that WordNet's IS-A hierarchy is better suited for some semantic relations compared with others. We also compute various learning curves and show that our model does not need a large number of training examples.

#index 1270220
#* Importance of semantic representation: dataless classification
#@ Ming-Wei Chang;Lev Ratinov;Dan Roth;Vivek Srikumar
#t 2008
#c 10
#% 252011
#% 311027
#% 466078
#% 876034
#% 1250362
#% 1272110
#% 1275012
#% 1289518
#! Traditionally, text categorization has been studied as the problem of training of a classifier using labeled data. However, people can categorize documents into named categories without any explicit training because we know the meaning of category names. In this paper, we introduce Dataless Classification, a learning protocol that uses world knowledge to induce classifiers without the need for any labeled data. Like humans, a dataless classifier interprets a string of words as a set of semantic concepts. We propose a model for dataless classification and show that the label name alone is often sufficient to induce classifiers. Using Wikipedia as our source of world knowledge, we get 85.29% accuracy on tasks from the 20 Newsgroup dataset and 88.62% accuracy on tasks from a Yahoo! Answers dataset without any labeled or unlabeled data from the data sets. With unlabeled data, we can further improve the results and show quite competitive performance to a supervised learning algorithm that uses 100 labeled examples.

#index 1270221
#* Discourse topic and gestural form
#@ Jacob Eisenstein;Regina Barzilay;Randall Davis
#t 2008
#c 10
#% 277314
#% 399759
#% 724289
#% 748583
#% 788094
#% 826289
#% 871592
#% 876808
#% 891559
#% 1216496
#! Coverbal gesture provides a channel for the visual expression of ideas. While some gestural emblems have culturally predefined forms (e.g., "thumbs up"), the relationship between gesture and meaning is, in general, not conventionalized. It is natural to ask whether such gestures can be interpreted in a speaker-independent way, or whether gestural form is determined by the speaker's idiosyncratic view of the discourse topic. We address this question using an audiovisual dataset across multiple speakers and topics. Our analysis employs a hierarchical Bayesian author-topic model, in which gestural patterns are stochastically generated by a mixture of speaker-specific and topic-specific priors. These gestural patterns are characterized using automatically-extracted visual features, based on spatio-temporal interest points. This framework detects significant cross-speaker patterns in gesture that are governed by the discourse topic, suggesting that even unstructured gesticulation can be interpreted across speakers. In addition, the success of this approach shows that the semantic characteristics of gesture can be detected via a low-level, interest point representation.

#index 1270222
#* Text categorization with knowledge transfer from heterogeneous data sources
#@ Rakesh Gupta;Lev Ratinov
#t 2008
#c 10
#% 200694
#% 311027
#% 448786
#% 465754
#% 466405
#% 722816
#% 741058
#% 770763
#% 817489
#% 869500
#% 983899
#% 1077150
#% 1250214
#% 1250362
#% 1272110
#! Multi-category classification of short dialogues is a common task performed by humans. When assigning a question to an expert, a customer service operator tries to classify the customer query into one of N different classes for which experts are available. Similarly, questions on the web (for example questions at Yahoo Answers) can be automatically forwarded to a restricted group of people with a specific expertise. Typical questions are short and assume background world knowledge for correct classification. With exponentially increasing amount of knowledge available, with distinct properties (labeled vs unlabeled, structured vs unstructured), no single knowledge-transfer algorithm such as transfer learning, multi-task learning or selftaught learning can be applied universally. In this work we show that bag-of-words classifiers performs poorly on noisy short conversational text snippets. We present an algorithm for leveraging heterogeneous data sources and algorithms with significant improvements over any single algorithm, rivaling human performance. Using different algorithms for each knowledge source we use mutual information to aggressively prune features. With heterogeneous data sources including Wikipedia, Open Directory Project (ODP), and Yahoo Answers, we show 89.4% and 96.8% correct classification on Google Answers corpus and Switchboard corpus using only 200 features/class. This reflects a huge improvement over bag of words approaches and 48-65% error reduction over previously published state of art (Gabrilovich et. al. 2006).

#index 1270223
#* Cross-lingual propagation for morphological analysis
#@ Benjamin Snyder;Regina Barzilay
#t 2008
#c 10
#% 815112
#% 817468
#% 828568
#% 910580
#% 939402
#% 939585
#% 939961
#! Multilingual parallel text corpora provide a powerful means for propagating linguistic knowledge across languages. We present a model which jointly learns linguistic structure for each language while inducing links between them. Our model supports fully symmetrical knowledge transfer, utilizing any combination of supervised and unsupervised data across language barriers. The proposed non-parametric Bayesian model effectively combines cross-lingual alignment with target language predictions. This architecture is a potent alternative to projection methods which decompose these decisions into two separate stages. We apply this approach to the task of morphological segmentation, where the goal is to separate a word into its individual morphemes. When tested on a parallel corpus of Hebrew and Arabic, our joint bilingual model effectively incorporates all available evidence from both languages, yielding significant performance gains.

#index 1270224
#* Single document keyphrase extraction using neighborhood knowledge
#@ Xiaojun Wan;Jianguo Xiao
#t 2008
#c 10
#% 281480
#% 303395
#% 309115
#% 342828
#% 420487
#% 495937
#% 729880
#% 818216
#% 853850
#% 855200
#% 855293
#% 869484
#% 874505
#% 1269818
#% 1279276
#% 1406467
#% 1705284
#! Existing methods for single document keyphrase extraction usually make use of only the information contained in the specified document. This paper proposes to use a small number of nearest neighbor documents to provide more knowledge to improve single document keyphrase extraction. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results demonstrate the good effectiveness and robustness of our proposed approach.

#index 1270225
#* Using wiktionary for computing semantic relatedness
#@ Torsten Zesch;Christof Müller;Iryna Gurevych
#t 2008
#c 10
#% 144029
#% 325502
#% 342963
#% 678436
#% 896031
#% 939378
#% 958367
#% 1249577
#% 1275012
#% 1712182
#! We introduce Wiktionary as an emerging lexical semantic resource that can be used as a substitute for expert-made resources in AI applications. We evaluate Wiktionary on the pervasive task of computing semantic relatedness for English and German by means of correlation with human rankings and solving word choice problems. For the first time, we apply a concept vector based measure to a set of different concept representations like Wiktionary pseudo glosses, the first paragraph of Wikipedia articles, English WordNet glosses, and GermaNet pseudo glosses. We show that: (i) Wiktionary is the best lexical semantic resource in the ranking task and performs comparably to other resources in the word choice task, and (ii) the concept vector based approach yields the best results on all datasets in both evaluations.

#index 1270226
#* Optimal scheduling of contract algorithms with soft deadlines
#@ Spyros Angelopoulos;Alejandro López-Ortiz;Angèle M. Hamel
#t 2008
#c 10
#% 331356
#% 495928
#% 578760
#% 590544
#% 765047
#% 1250644
#% 1279384
#! A contract algorithm is an algorithm which is given, as part of its input, a specified amount of allowable computation time. In contrast, interruptible algorithms may be interrupted throughout their execution, at which point they must report their current solution. Simulating interruptible algorithms by means of schedules of executions of contract algorithms in parallel processors is a well-studied problem with significant applications in AI. In the classical case, the interruptions are hard deadlines in which a solution must be reported immediately at the time the interruption occurs. In this paper we study the more general setting of scheduling contract algorithms at the presence of soft deadlines. This is motivated by the observation of practitioners that soft deadlines are as common an occurrence as hard deadlines, if not more common. In our setting, at the time t of interruption the algorithm is given an additional window of time w(t) ≤ c ċ t to continue the contract or, indeed, start a new contract (for some fixed constant c). We explore this variation using the acceleration ratio, which is the canonical measure of performance for these schedules, and derive schedules of optimal acceleration ratio for all functions w.

#index 1270227
#* Optimal metric planning with state sets in automata representation
#@ Björn Ulrich Borowsky;Stefan Edelkamp
#t 2008
#c 10
#% 465089
#% 544788
#% 669644
#% 776160
#% 1272083
#% 1397992
#! This paper proposes an optimal approach to infinite-state action planning exploiting automata theory. State sets and actions are characterized by Presburger formulas and represented using minimized finite state machines. The exploration that contributes to the planning via model checking paradigm applies symbolic images in order to compute the deterministic finite automaton for the sets of successors. A large fraction of metric planning problems can be translated into Presburger arithmetic, while derived predicates are simply compiled away. We further propose three algorithms for computing optimal plans; one for uniform action costs, one for the additive cost model, and one for linear plan metrics. Furthermore, an extension for infinite state sets is discussed.

#index 1270228
#* PBA*: using proactive search to make A* robust to unplanned deviations
#@ Paul Breimyer;Peter R. Wurman
#t 2008
#c 10
#% 337980
#% 757483
#% 837649
#% 952274
#% 1290111
#! Many path planning algorithms leverage A* to determine optimal paths. However, when an actor deviates from the optimal path, a typical application of A* executes a new search from the deviation point to the goal. This approach redundantly calculates paths that may have been examined during the initial search, rather than leveraging previous information. We introduce Plan-B A* (PBA*), which uses A* for the initial search, and substantially reduces the number of searched states during all subsequent searches, while incurring minimal space overhead. PBA* not only remembers certain states it has examined, it proactively creates solution paths for the most likely deviations. In our experiments, PBA* searches only 10% of the A* search space when recovering from execution errors by storing a limited amount of search history.

#index 1270229
#* Fast planning by search in domain transition graph
#@ Yixin Chen;Ruoyun Huang;Weixiong Zhang
#t 2008
#c 10
#% 251783
#% 1271820
#% 1271962
#% 1272113
#% 1275050
#% 1399103
#! Recent advances in classical planning have used the SAS+ formalism, and several effective heuristics have been developed based on the SAS+ formalism. Comparing to the traditional STRIPS/ADL formalism, SAS+ is capable of capturing vital information such as domain transition structures and causal dependencies. In this paper, we propose a new SAS+ based incomplete planning approach. Instead of using SAS+ to derive heuristics within a heuristic search planner, we directly search in domain transition graphs (DTGs) and causal graphs (CGs) derived from the SAS+ formalism. The new method is efficient because the SAS+ representation is often much more compact than STRIPS. The CGs and DTGs provide rich information of domain structures that can effectively guide the search towards solutions. Experimental results show strong performance of the proposed planner on recent international planning competition domains.

#index 1270230
#* Planning with problems requiring temporal coordination
#@ Andrew Coles;Maria Fox;Derek Long;Amanda Smith
#t 2008
#c 10
#% 495772
#% 1136356
#% 1271962
#% 1272008
#% 1272020
#% 1272098
#% 1272116
#% 1275052
#! We present the first planner capable of reasoning with both the full semantics of PDDL2.1 (level 3) temporal planning and with numeric resources. Our planner, CRIKEY3, employs heuristic forward search, using the start-and-end semantics of PDDL2.1 to manage temporal actions. The planning phase is interleaved with a scheduling phase, using a Simple Temporal Network, in order to ensure that temporal constraints are met. To guide search, we introduce a new temporal variant of the Relaxed Planning Graph heuristic that is capable of reasoning with the features of this class of domains, along with the Timed Initial Literals of PDDL2.2. CRIKEY3 extends the state-of-the-art in handling the full temporal expressive power of PDDL2.1, including numeric temporal domains.

#index 1270231
#* Partitioned external-memory value iteration
#@ Peng Dai; Mausam;Daniel S. Weld
#t 2008
#c 10
#% 181627
#% 337981
#% 351418
#% 644560
#% 829022
#% 1191037
#% 1250328
#% 1650297
#% 1650355
#% 1650672
#! Dynamic programming methods (including value iteration, LAO*, RTDP, and derivatives) are popular algorithms for solving Markov decision processes (MDPs). Unfortunately, however, these techniques store the MDP model extensionally in a table and thus are limited by the amount of main memory available. Since the required space is exponential in the number of domain features, these dynamic programming methods are ineffective for large problems. To address this problem, Edelcamp et al. devised the external memory value iteration (EMVI) algorithm, which uses a clever sorting scheme to efficiently move parts of the model between disk and main memory. While EMVI can handle larger problems than previously addressed, the need to repeatedly perform external sorts still limits scalability. This paper proposes a new approach. We partition an MDP into smaller pieces (blocks), keeping just the relevant blocks in memory and performing Bellman backups block by block. Experiments show that our algorithm is able to solve large MDPs an order of magnitude faster than EMVI.

#index 1270232
#* Error classification in action descriptions: a heuristic approach
#@ Thomas Eiter;Michael Fink;Ján Senko
#t 2008
#c 10
#% 266241
#% 752742
#% 820187
#% 1223262
#% 1664554
#% 1692915
#! Action languages allow to formally represent and reason about actions in a highly declarative manner. In recent work, revision and management of conflicts for domain descriptions in such languages wrt. semantic integrity constraints have been considered, in particular their reconciliation. However, merely ad hoc tests and methods have been presented to aid the user in analyzing and correcting a flawed description. We go beyond this and present a methodology on top of such tests for identifying a possible error, which works in several stages. The issue of such a methodology for action languages is novel and has not been addressed before, but is important for building tools and engineering action descriptions in practice.

#index 1270233
#* Computing minimal diagnoses by greedy stochastic search
#@ Alexander Feldman;Gregory Provan;Arjan Van Gemund
#t 2008
#c 10
#% 21137
#% 21138
#% 107135
#% 132173
#% 154456
#% 181220
#% 198885
#% 327779
#% 439224
#% 529186
#% 979225
#% 1250637
#% 1270234
#% 1290158
#% 1707653
#! Most algorithms for computing diagnoses within a model-based diagnosis framework are deterministic. Such algorithms guarantee soundness and completeness, but are Σ2P-hard. To overcome this complexity problem, which prohibits the computation of high-cardinality diagnoses for large systems, we propose a novel approximation approach for multiple-fault diagnosis, based on a greedy stochastic algorithm called SAFARI (StochAstic Fault diagnosis Algo-RIthm). We prove that SAFARI can be configured to compute diagnoses which are of guaranteed minimality under subsumption. We analytically model SAFARI search as a Markov chain, and show a probabilistic bound on the minimality of its minimal diagnosis approximations. We have applied this algorithm to the 74XXX and ISCAS85 suites of benchmark combinatorial circuits, demonstrating order-of-magnitude speedups over two state-of-the-art deterministic algorithms, CDA* and HA*, for multiple-fault diagnoses.

#index 1270234
#* Computing observation vectors for max-fault min-cardinality diagnoses
#@ Alexander Feldman;Gregory Provan;Arjan Van Gemund
#t 2008
#c 10
#% 21138
#% 132173
#% 181220
#% 420719
#% 439224
#% 552528
#% 598489
#% 979225
#% 1250637
#% 1274834
#% 1398242
#! Model-Based Diagnosis (MBD) typically focuses on diagnoses, minimal under some minimality criterion, e.g., the minimal-cardinality set of faulty components that explain an observation α. However, for different α there may be minimal-cardinality diagnoses of differing cardinalities, and several applications (such as test pattern generation and benchmark model analysis) need to identify the α leading to the max-cardinality diagnosis amongst them. We denote this problem as a Max-Fault Min-Cardinality (MFMC) problem. This paper considers the generation of observations that lead to MFMC diagnoses. We present a near-optimal, stochastic algorithm, called MIRANDA (Max-fault mIn-caRdinAlity observatioN Deduction Algorithm), that computes MFMC observations. Compared to optimal, deterministic approaches such as ATPG, the algorithm has very low cost, allowing us to generate observations corresponding to high-cardinality faults. Experiments show that MIRANDA delivers optimal results on the 74XXX circuits, as well as good MFMC cardinality estimates on the larger ISCAS85 circuits.

#index 1270235
#* Finding state similarities for faster planning
#@ Christian Fritz
#t 2008
#c 10
#% 100159
#% 275214
#% 337980
#% 342119
#% 544782
#% 1269539
#% 1272379
#% 1289241
#! In many planning applications one can find actions with overlapping effects. If for optimally reaching the goal all that matters is within this overlap, there is no need to consider all these actions -for the task at hand they are equivalent. Using this structure for speed-up has previously been proposed in the context of least commitment planning. Of a similar spirit is the approach for improving best-first search based planning we present here: intuitively, given a set of start states, reachable from the initial state, we plan in parallel for all of them, exploiting the similarities between them to gain computational savings. Since the similarity of two states is problem specific, we explicitly infer it by regressing all relevant entities, goal, heuristic function, action preconditions and costs, over the action sequences considered in planning. If the resulting formulae mention only fluents whose values the two states have in common, it suffices to evaluate the formulae in one of them. This leads to computational savings over conventional best-first search.

#index 1270236
#* Reasoning about large taxonomies of actions
#@ Yilan Gu;Mikhail Soutchanski
#t 2008
#c 10
#% 211578
#% 342119
#% 531443
#% 882671
#% 1250641
#! We design a representation based on the situation calculus to facilitate development, maintenance and elaboration of very large taxonomies of actions. This representation leads to more compact and modular basic action theories (BATs) for reasoning about actions than currently possible. We compare our representation with Reiter's BATs and prove that our representation inherits all useful properties of his BATs. Moreover, we show that our axioms can be more succinct, but extended Reiter's regression can still be used to solve the projection problem (this is the problem of whether a given logical expression will hold after executing a sequence of actions). We also show that our representation has significant computational advantages. For taxonomies of actions that can be represented as finitely branching trees, the regression operator can work exponentially faster with our theories than it works with Reiter's BATs. Finally, we propose general guidelines on how a taxonomy of actions can be constructed from the given set of effect axioms in a domain.

#index 1270237
#* Accuracy of admissible heuristic functions in selected planning domains
#@ Malte Helmert;Robert Mattmüller
#t 2008
#c 10
#% 167629
#% 337980
#% 496111
#% 1269544
#% 1269831
#% 1272007
#% 1272083
#% 1272087
#% 1414232
#% 1476298
#! The efficiency of optimal planning algorithms based on heuristic search crucially depends on the accuracy of the heuristic function used to guide the search. Often, we are interested in domain-independent heuristics for planning. In order to assess the limitations of domain-independent heuristic planning, we analyze the (in)accuracy of common domain-independent planning heuristics in the IPC benchmark domains. For a selection of these domains, we analytically investigate the accuracy of the h+ heuristic, the hm family of heuristics, and certain (additive) pattern database heuristics, compared to the perfect heuristic h*. Whereas h+and additive pattern database heuristics usually return cost estimates proportional to the true cost, non-additive hm and nonadditive pattern-database heuristics can yield results underestimating the true cost by arbitrarily large factors.

#index 1270238
#* How good is almost perfect?
#@ Malte Helmert;Gabriele Röger
#t 2008
#c 10
#% 241
#% 2194
#% 167629
#% 337986
#% 337987
#% 453074
#% 496111
#% 496243
#% 529508
#% 873948
#% 1223307
#% 1269831
#% 1269852
#% 1271962
#% 1272083
#% 1414232
#! Heuristic search using algorithms such as A* and IDA* is the prevalent method for obtaining optimal sequential solutions for classical planning tasks. Theoretical analyses of these classical search algorithms, such as the well-known results of Pohl, Gaschnig and Pearl, suggest that such heuristic search algorithms can obtain better than exponential scaling behaviour, provided that the heuristics are accurate enough. Here, we show that for a number of common planning benchmark domains, including ones that admit optimal solution in polynomial time, general search algorithms such as A* must necessarily explore an exponential number of search nodes even under the optimistic assumption of almost perfect heuristic estimators, whose heuristic error is bounded by a small additive constant. Our results shed some light on the comparatively bad performance of optimal heuristic search approaches in "simple" planning domains such as GRIPPER. They suggest that in many applications, further improvements in run-time require changes to other parts of the search algorithm than the heuristic estimator.

#index 1270239
#* HTN-MAKER: learning HTNs with minimal additional knowledge engineering required
#@ Chad Hogg;Héctor Muñoz-Avila;Ugur Kuter
#t 2008
#c 10
#% 365189
#% 449587
#% 465761
#% 495942
#% 694516
#% 743353
#% 803704
#% 840877
#% 876028
#% 961150
#% 1269396
#% 1272167
#! We describe HTN-MAKER, an algorithm for learning hierarchical planning knowledge in the form of decomposition methods for Hierarchical Task Networks (HTNs). HTN-MAKER takes as input the initial states from a set of classical planning problems in a planning domain and solutions to those problems, as well as a set of semantically-annotated tasks to be accomplished. The algorithm analyzes this semantic information in order to determine which portions of the input plans accomplish a particular task and constructs HTN methods based on those analyses. Our theoretical results show that HTN-MAKER is sound and complete. We also present a formalism for a class of planning problems that are more expressive than classical planning. These planning problems can be represented as HTN planning problems. We show that the methods learned by HTN-MAKER enable an HTN planner to solve those problems. Our experiments confirm the theoretical results and demonstrate convergence in three well-known planning domains toward a set of HTN methods that can be used to solve nearly any problem expressible as a classical planning problem in that domain, relative to a set of goals.

#index 1270240
#* Generating plans in concurrent, probabilistic, over-subscribed domains
#@ Li Li;Nilufer Onder
#t 2008
#c 10
#% 25470
#% 194652
#% 337981
#% 495772
#% 1269463
#% 1272089
#% 1272092
#% 1272109
#% 1272199
#% 1289543
#% 1289549
#% 1650355
#! Planning in realistic domains involves reasoning under uncertainty, operating under time and resource constraints, and finding the optimal set of goals to be achieved. In this paper, we provide an AO* based algorithm that can deal with durative actions, concurrent execution, over-subscribed goals, and probabilistic outcomes in a unified way. We explore plan optimization by introducing two novel aspects to the model. First, we introduce parallel steps that serve the same goal and increase the probability of success in addition to parallel steps that serve different goals and decrease execution time. Second, we introduce plan steps to terminate concurrent steps that are no longer useful so that resources can be conserved. Our algorithm called CPOAO* (Concurrent, Probabilistic, Oversubscription AO*) can deal with the aforementioned extensions and relies on the AO* framework to reduce the size of the search space using informative heuristic functions. We describe our framework, implementation, the heuristic functions we use, the experimental results, and potential research on heuristics that can further reduce the size of search space.

#index 1270241
#* Unknown rewards in finite-horizon domains
#@ Colin McMillen;Manuela Veloso
#t 2008
#c 10
#% 1250344
#% 1269869
#% 1272286
#% 1394505
#! "Human computation" is a recent approach that extracts information from large numbers of Web users. reCAPTCHA is a human computation project that improves the process of digitizing books by getting humans to read words that are difficult for OCR algorithms to read (von Ahn et al., 2008). In this paper, we address an interesting strategic control problem inspired by the reCAPTCHA project: given a large set of words to transcribe within a time deadline, how can we choose the difficulty level such that we maximize the probability of successfully transcribing a document on time? Our approach is inspired by previous work on timed, zero-sum games, as we face an analogous timed policy decision on the choice of words to present to users. However, our Web-based word transcribing domain is particularly challenging as the reward of the actions is not known; i.e., there is no knowledge if the spelling provided by a human is actually correct. We contribute an approach to solve this problem by checking a small fraction of the answers at execution time, obtaining an estimate of the cumulative reward. We present experimental results showing how the number of samples and time between samples affects the probability of success. We also investigate the choice of aggressive or conservative actions with regard to the bounds produced by sampling. We successfully apply our algorithm to real data gathered by the reCAPTCHA project.

#index 1270242
#* Route planning under uncertainty: the Canadian traveller problem
#@ Evdokia Nikolova;David R. Karger
#t 2008
#c 10
#% 101144
#% 113707
#% 325624
#% 847136
#% 850011
#! The Canadian Traveller problem is a stochastic shortest paths problem in which one learns the cost of an edge only when arriving at one of its endpoints. The goal is to find an optimal policy that minimizes the expected cost of travel. The problem is known to be #P-hard. Since there has been no significant progress on approximation algorithms for several decades, we have chosen to seek out special cases for which exact solutions exist, in the hope of demonstrating techniques that could lead to further progress. Applying a mix of techniques from algorithm analysis and the theory of Markov Decision Processes, we provide efficient exact algorithms for directed acyclic graphs and (undirected) graphs of disjoint paths from source to destination with random two-valued edge costs. We also give worst-case performance analysis and experimental data for two natural heuristics.

#index 1270243
#* Landmarks revisited
#@ Silvia Richter;Malte Helmert;Matthias Westphal
#t 2008
#c 10
#% 251783
#% 875422
#% 1178627
#% 1271962
#% 1272047
#% 1272113
#! Landmarks for propositional planning tasks are variable assignments that must occur at some point in every solution plan. We propose a novel approach for using landmarks in planning by deriving a pseudo-heuristic and combining it with other heuristics in a search framework. The incorporation of landmark information is shown to improve success rates and solution qualities of a heuristic planner. We furthermore show how additional landmarks and orderings can be found using the information present in multi-valued state variable representations of planning tasks. Compared to previously published approaches, our landmark extraction algorithm provides stronger guarantees of correctness for the generated landmark orderings, and our novel use of landmarks during search solves more planning tasks and delivers considerably better solutions.

#index 1270244
#* Fusing procedural and declarative planning goals for nondeterministic domains
#@ Dzmitry Shaparau;Marco Pistore;Paolo Traverso
#t 2008
#c 10
#% 101955
#% 243697
#% 296170
#% 578723
#% 655322
#% 741448
#% 1250200
#% 1272399
#% 1275051
#% 1289213
#% 1289550
#! While in most planning approaches goals and plans are different objects, it is often useful to specify goals that combine declarative conditions with procedural plans. In this paper, we propose a novel language for expressing temporally extended goals for planning in nondeterministic domains. The key feature of this language is that it allows for an arbitrary combination of declarative goals expressed in temporal logic and procedural goals expressed as plan fragments. We provide a formal definition of the language and its semantics, and we propose an approach to planning with this language in nondeterministic domains. We implement the planning framework and perform a set of experimental evaluations that show the potentialities of our approach.

#index 1270245
#* Learning generalized plans using abstract counting
#@ Siddharth Srivastava;Neil Immerman;Shlomo Zilberstein
#t 2008
#c 10
#% 73375
#% 349960
#% 1272095
#% 1289432
#! Given the complexity of planning, it is often beneficial to create plans that work for a wide class of problems. This facilitates reuse of existing plans for different instances drawn from the same problem or from an infinite family of similar problems. We define a class of such planning problems called generalized planning problems and present a novel approach for transforming classical plans into generalized plans. These algorithm-like plans include loops and work for problem instances having varying numbers of objects that must be manipulated to reach the goal. Our approach takes as input a classical plan for a certain problem instance. It outputs a generalized plan along with a classification of the problem instances where it is guaranteed to work. We illustrate the utility of our approach through results of a working implementation on various practical examples.

#index 1270246
#* Hypothesis pruning and ranking for large plan recognition problems
#@ Gita Sukthankar;Katia Sycara
#t 2008
#c 10
#% 179879
#% 283197
#% 379036
#% 812594
#% 1250174
#% 1250199
#% 1250210
#% 1250620
#% 1272316
#% 1289455
#! This paper addresses the problem of plan recognition for multi-agent teams. Complex multi-agent tasks typically require dynamic teams where the team membership changes over time. Teams split into subteams to work in parallel, merge with other teams to tackle more demanding tasks, and disband when plans are completed. We introduce a new multi-agent plan representation that explicitly encodes dynamic team membership and demonstrate the suitability of this formalism for plan recognition. From our multi-agent plan representation, we extract local temporal dependencies that dramatically prune the hypothesis set of potentially-valid team plans. The reduced plan library can be efficiently processed to obtain the team state history. Naive pruning can be inadvisable when low-level observations are unreliable due to sensor noise and classification errors. In such conditions, we eschew pruning in favor of prioritization and show how our scheme can be extended to rank-order the hypotheses. Experiments show that this robust pre-processing approach ranks the correct plan within the top 10%, even under conditions of severe noise.

#index 1270247
#* On the progression of situation calculus basic action theories: resolving a 10-year-old conjecture
#@ Stavros Vassos;Hector J. Levesque
#t 2008
#c 10
#% 34011
#% 155825
#% 229083
#% 284106
#% 284647
#% 578733
#% 873946
#% 1289434
#% 1289445
#! In a seminal paper, Lin and Reiter introduced a model-theoretic definition for the progression of the initial knowledge base of a basic action theory. This definition comes with a strong negative result, namely that for certain kinds of action theories, first-order logic is not expressive enough to correctly characterize this form of progression, and second-order axioms are necessary. However, Lin and Reiter also considered an alternative definition for progression which is always first-order definable. They conjectured that this alternative definition is incorrect in the sense that the progressed theory is too weak and may sometimes lose information. This conjecture, and the status of first-order definable progression, has remained open since then. In this paper we present two significant results about this alternative definition of progression. First, we prove the Lin and Reiter conjecture by presenting a case where the progressed theory indeed does lose information. Second, we prove that the alternative definition is nonetheless correct for reasoning about a large class of sentences, including some that quantify over situations. In this case the alternative definition is a preferred option due to its simplicity and the fact that it is always first-order.

#index 1270248
#* Probabilistic planning via determinization in hindsight
#@ Sungwook Yoon;Alan Fern;Robert Givan;Subbarao Kambhampati
#t 2008
#c 10
#% 527859
#% 1036401
#% 1271962
#% 1275072
#! This paper investigates hindsight optimization as an approach for leveraging the significant advances in deterministic planning for action selection in probabilistic domains. Hindsight optimization is an online technique that evaluates the one-step-reachable states by sampling future outcomes to generate multiple non-stationary deterministic planning problems which can then be solved using search. Hindsight optimization has been successfully used in a number of online scheduling applications; however, it has not yet been considered in the substantially different context of goal-based probabilistic planning. We describe an implementation of hindsight optimization for probabilistic planning based on deterministic forward heuristic search and evaluate its performance on planning-competition benchmarks and other probabilistically interesting problems. The planner is able to outperform a number of probabilistic planners including FF-Replan on many problems. Finally, we investigate conditions under which hindsight optimization is guaranteed to be effective with respect to goal achievement, and also illustrate examples where the approach can go wrong.

#index 1270249
#* CRF-OPT: an efficient high-quality conditional random field solver
#@ Minmin Chen;Yixin Chen;Michael R. Brent
#t 2008
#c 10
#% 464434
#% 1673026
#! Conditional random field (CRF) is a popular graphical model for sequence labeling. The flexibility of CRF poses significant computational challenges for training. Using existing optimization packages often leads to long training time and unsatisfactory results. In this paper, we develop CRFOPT, a general CRF training package, to improve the efficiency and quality for training CRFs. We propose two improved versions of the forward-backward algorithm that exploit redundancy and reduce the time by several orders of magnitudes. Further, we propose an exponential transformation that enforces sufficient step sizes for quasi-Newton methods. The technique improves the convergence quality, leading to better training results. We evaluate CRF-OPT on a gene prediction task on pathogenic DNA sequences, and show that it is faster and achieves better prediction accuracy than both the HMM models and the original CRF model without exponential transformation.

#index 1270250
#* Focusing generalizations of belief propagation on targeted queries
#@ Arthur Choi;Adnan Darwiche
#t 2008
#c 10
#% 44876
#% 115608
#% 272514
#% 303620
#% 448887
#% 528019
#% 715096
#% 788106
#% 819505
#% 1250332
#% 1272120
#% 1272302
#% 1650318
#% 1650361
#% 1650778
#% 1730598
#% 1814768
#% 1815596
#% 1848680
#! A recent formalization of Iterative Belief Propagation (IBP) has shown that it can be understood as an exact inference algorithm on an approximate model that results from deleting every model edge. This formalization has led to (1) new realizations of Generalized Belief Propagation (GBP) in which edges are recovered incrementally to improve approximation quality, and (2) edge-recovery heuristics that are motivated by improving the approximation quality of all node marginals in a graphical model. In this paper, we propose new edge-recovery heuristics, which are focused on improving the approximations of targeted node marginals. The new heuristics are based on newly-identified properties of edge deletion, and in turn IBP, which guarantee the exactness of edge deletion in simple and idealized cases. These properties also suggest new improvements to IBP approximations which are based on performing edge-by-edge corrections on targeted marginals, which are less costly than improvements based on edge recovery.

#index 1270251
#* Many-pairs mutual information for adding structure to belief propagation approximations
#@ Arthur Choi;Adnan Darwiche
#t 2008
#c 10
#% 44876
#% 115608
#% 272514
#% 375029
#% 520726
#% 798967
#% 819505
#% 1250332
#% 1650361
#% 1730598
#% 1814768
#% 1815596
#% 1848680
#! We consider the problem of computing mutual information between many pairs of variables in a Bayesian network. This task is relevant to a new class of Generalized Belief Propagation (GBP) algorithms that characterizes Iterative Belief Propagation (IBP) as a polytree approximation found by deleting edges in a Bayesian network. By computing, in the simplified network, the mutual information between variables across a deleted edge, we can estimate the impact that recovering the edge might have on the approximation. Unfortunately, it is computationally impractical to compute such scores for networks over many variables having large state spaces. So that edge recovery can scale to such networks, we propose in this paper an approximation of mutual information which is based on a soft extension of d-separation (a graphical test of independence in Bayesian networks). We focus primarily on polytree networks, which are sufficient for the application we consider, although we discuss potential extensions of the approximation to general networks as well. Empirically, we show that our proposal is often as effective as mutual information for the task of edge recovery, with orders of magnitude savings in computation time in larger networks. Our results lead to a concrete realization of GBP, admitting improvements to IBP approximations with only a modest amount of computational effort.

#index 1270252
#* Preference aggregation with graphical utility models
#@ Christophe Gonzales;Patrice Perny;Sergio Queiroz
#t 2008
#c 10
#% 380725
#% 388024
#% 424793
#% 528176
#% 830091
#% 963354
#% 1250233
#% 1272026
#% 1650628
#! This paper deals with preference representation and aggregation in the context of multiattribute utility theory. We consider a set of alternatives having a combinatorial structure. We assume that preferences are compactly represented by graphical utility models derived from generalized additive decomposable (GAI) utility functions. Such functions enable to model interactions between attributes while preserving some decomposability property. We address the problem of finding a compromise solution from several GAI utilities representing different points of view on the alternatives. This scheme can be applied both to multicriteria decision problems and to collective decision making problems over combinatorial domains. We propose a procedure using graphical models for the fast determination of a Pareto-optimal solution achieving a good compromise between the conflicting utilities. The procedure relies on a ranking algorithm enumerating solutions according to the sum of all the GAI utilities until a boundary condition is reached. Numerical experiments are provided to highlight the practical efficiency of our procedure.

#index 1270253
#* Exploiting symmetries in POMDPs for point-based algorithms
#@ Kee-Eung Kim
#t 2008
#c 10
#% 252183
#% 318485
#% 408396
#% 450852
#% 677517
#% 836124
#% 1272129
#% 1476294
#% 1478746
#% 1673035
#! We extend the model minimization technique for partially observable Markov decision processes (POMDPs) to handle symmetries in the joint space of states, actions, and observations. The POMDP symmetry we define in this paper cannot be handled by the model minimization techniques previously published in the literature. We formulate the problem of finding the symmetries as a graph automorphism (GA) problem, and although not yet known to be tractable, we experimentally show that the sparseness of the graph representing the POMDP allows us to quickly find symmetries. We show how the symmetries in POMDPs can be exploited for speeding up point-based algorithms. We experimentally demonstrate the effectiveness of our approach.

#index 1270254
#* Towards faster planning with continuous resources in stochastic domains
#@ Janusz Marecki;Milind Tambe
#t 2008
#c 10
#% 314843
#% 734920
#% 788054
#% 788064
#% 890238
#% 1269546
#% 1275161
#% 1289549
#% 1289561
#% 1650355
#! Agents often have to construct plans that obey resource limits for continuous resources whose consumption can only be characterized by probability distributions. While Markov Decision Processes (MDPs) with a state space of continuous and discrete variables are popular for modeling these domains, current algorithms for such MDPs can exhibit poor performance with a scale-up in their state space. To remedy that we propose an algorithm called DPFP. DPFP's key contribution is its exploitation of the dual space cumulative distribution functions. This dual formulation is key to DPFP's novel combination of three features. First, it enables DPFP's membership in a class of algorithms that perform forward search in a large (possibly infinite) policy space. Second, it provides a new and efficient approach for varying the policy generation effort based on the likelihood of reaching different regions of the MDP state space. Third, it yields a bound on the error produced by such approximations. These three features conspire to allow DPFP's superior performance and systematic trade-off of optimality for speed. Our experimental evaluation shows that, when run stand-alone, DPFP outperforms other algorithms in terms of its any-time performance, whereas when run as a hybrid, it allows for a significant speedup of a leading continuous resource MDP solver.

#index 1270255
#* A variance analysis for POMDP policy evaluation
#@ Mahdi Milani Fard;Joelle Pineau;Peng Sun
#t 2008
#c 10
#% 179940
#% 272652
#% 770824
#% 793249
#% 940817
#% 946171
#% 951076
#% 1269868
#% 1650588
#! Partially Observable Markov Decision Processes have been studied widely as a model for decision making under uncertainty, and a number of methods have been developed to find the solutions for such processes. Such studies often involve calculation of the value function of a specific policy, given a model of the transition and observation probabilities, and the reward. These models can be learned using labeled samples of on-policy trajectories. However, when using empirical models, some bias and variance terms are introduced into the value function as a result of imperfect models. In this paper, we propose a method for estimating the bias and variance of the value function in terms of the statistics of the empirical transition and observation model. Such error terms can be used to meaningfully compare the value of different policies. This is an important result for sequential decision-making, since it will allow us to provide more formal guarantees about the quality of the policies we implement. To evaluate the precision of the proposed method, we provide supporting experiments on problems from the field of robotics and medical decision making.

#index 1270256
#* Lifted probabilistic inference with counting formulas
#@ Brian Milch;Luke S. Zettlemoyer;Kristian Kersting;Michael Haimes;Leslie Pack Kaelbling
#t 2008
#c 10
#% 289947
#% 850430
#% 983845
#% 1000502
#% 1250334
#% 1279353
#% 1289560
#% 1650326
#% 1650403
#% 1665139
#! Lifted inference algorithms exploit repeated structure in probabilistic models to answer queries efficiently. Previous work such as de Salvo Braz et al.'s first-order variable elimination (FOVE) has focused on the sharing of potentials across interchangeable random variables. In this paper, we also exploit interchangeability within individual potentials by introducing counting formulas, which indicate how many of the random variables in a set have each possible value. We present a new lifted inference algorithm, C-FOVE, that not only handles counting formulas in its input, but also creates counting formulas for use in intermediate potentials. C-FOVE can be described succinctly in terms of six operators, along with heuristics for when to apply them. Because counting formulas capture dependencies among large numbers of variables compactly, C-FOVE achieves asymptotic speed improvements compared to FOVE.

#index 1270257
#* Optimal testing of structured knowledge
#@ Michael Munie;Yoav Shoham
#t 2008
#c 10
#% 44876
#% 404719
#% 443640
#% 449588
#% 729923
#% 862541
#% 1272077
#% 1289563
#% 1650315
#% 1650712
#% 1861554
#! Adopting a decision-theoretic perspective, we investigate the problem of optimal testing of structured knowledge - the canonical example being a qualifying examination of a graduate student. The setting is characterized by several factors: examinee's knowledge structured around several inter-dependent topics, a limited "budget" of questions available to the examiner, a decision to be made (pass/fail), and an utility for good and bad decisions. The existence of multiple professors brings up additional issues such as committee formation, and the existence of multiple students brings up issues such as fairness.

#index 1270258
#* A general method for reducing the complexity of relational inference and its application to MCMC
#@ Hoifung Poon;Pedro Domingos;Marc Sumner
#t 2008
#c 10
#% 26722
#% 327779
#% 464304
#% 850430
#% 1000452
#% 1000502
#% 1250224
#% 1250334
#% 1250579
#% 1250584
#% 1269815
#! Many real-world problems are characterized by complex relational structure, which can be succinctly represented in first-order logic. However, many relational inference algorithms proceed by first fully instantiating the first-order theory and then working at the propositional level. The applicability of such approaches is severely limited by the exponential time and memory cost of propositionalization. Singla and Domingos (2006) addressed this by developing a "lazy" version of the WalkSAT algorithm, which grounds atoms and clauses only as needed. In this paper we generalize their ideas to a much broader class of algorithms, including other types of SAT solvers and probabilistic inference methods like MCMC. Lazy inference is potentially applicable whenever variables and functions have default values (i.e., a value that is much more frequent than the others). In relational domains, the default is false for atoms and true for clauses. We illustrate our framework by applying it to MC-SAT, a state-of-the-art MCMC algorithm. Experiments on a number of real-world domains show that lazy inference reduces both space and time by several orders of magnitude, making probabilistic relational inference applicable in previously infeasible domains.

#index 1270259
#* Dormant independence
#@ Ilya Shpitser;Judea Pearl
#t 2008
#c 10
#% 44876
#% 297171
#% 578740
#% 716378
#% 1250349
#% 1650407
#! The construction of causal graphs from nonexperimental data rests on a set of constraints that the graph structure imposes on all probability distributions compatible with the graph. These constraints are of two types: conditional independencies and algebraic constraints, first noted by Verma. While conditional independencies are well studied and frequently used in causal induction algorithms, Verma constraints are still poorly understood, and rarely applied. In this paper we examine a special subset of Verma constraints which are easy to understand, easy to identify and easy to apply; they arise from "dormant independencies," namely, conditional independencies that hold in interventional distributions. We give a complete algorithm for determining if a dormant independence between two sets of variables is entailed by the causal graph, such that this independence is identifiable, in other words if it resides in an interventional distribution that can be predicted without resorting to interventions. We further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph.

#index 1270260
#* Symbolic heuristic search value iteration for factored POMDPs
#@ Hyeong Seop Sim;Kee-Eung Kim;Jin Hyung Kim;Du-Seong Chang;Myoung-Wan Koo
#t 2008
#c 10
#% 233849
#% 252183
#% 788098
#% 842579
#% 1045684
#% 1271823
#% 1272075
#% 1476294
#% 1650297
#% 1650568
#% 1650702
#! We propose Symbolic heuristic search value iteration (Symbolic HSVI) algorithm, which extends the heuristic search value iteration (HSVI) algorithm in order to handle factored partially observable Markov decision processes (factored POMDPs). The idea is to use algebraic decision diagrams (ADDs) for compactly representing the problem itself and all the relevant intermediate computation results in the algorithm. We leverage Symbolic Perseus for computing the lower bound of the optimal value function using ADD operators, and provide a novel ADD-based procedure for computing the upper bound. Experiments on a number of standard factored POMDP problems show that we can achieve an order of magnitude improvement in performance over previously proposed algorithms.

#index 1270261
#* Lifted first-order belief propagation
#@ Parag Singla;Pedro Domingos
#t 2008
#c 10
#% 44876
#% 288366
#% 850430
#% 1000502
#% 1250334
#% 1250584
#% 1269496
#% 1279353
#% 1289560
#% 1650326
#% 1810385
#! Unifying first-order logic and probability is a long-standing goal of AI, and in recent years many representations combining aspects of the two have been proposed. However, inference in them is generally still at the level of propositional logic, creating all ground atoms and formulas and applying standard probabilistic inference methods to the resulting network. Ideally, inference should be lifted as in first-order logic, handling whole sets of indistinguishable objects together, in time independent of their cardinality. Poole (2003) and Braz et al. (2005, 2006) developed a lifted version of the variable elimination algorithm, but it is extremely complex, generally does not scale to realistic domains, and has only been applied to very small artificial problems. In this paper we propose the first lifted version of a scalable probabilistic inference algorithm, belief propagation (loopy or not). Our approach is based on first constructing a lifted network, where each node represents a set of ground atoms that all pass the same messages during belief propagation. We then run belief propagation on this network. We prove the correctness and optimality of our algorithm. Experiments show that it can greatly reduce the cost of inference.

#index 1270262
#* Bounding the false discovery rate in local Bayesian network learning
#@ Ioannis Tsamardinos;Laura E. Brown
#t 2008
#c 10
#% 893460
#% 905816
#% 1415214
#% 1650674
#! Modern Bayesian Network learning algorithms are time-efficient, scalable and produce high-quality models; these algorithms feature prominently in decision support model development, variable selection, and causal discovery. The quality of the models, however, has often only been empirically evaluated; the available theoretical results typically guarantee asymptotic correctness (consistency) of the algorithms. This paper describes theoretical bounds on the quality of a fundamental Bayesian Network local-learning task in the finite sample using theories for controlling the False Discovery Rate. The behavior of the derived bounds is investigated across various problem and algorithm parameters. Empirical results support the theory which has immediate ramifications in the design of new algorithms for Bayesian Network learning, variable selection and causal discovery.

#index 1270263
#* Hybrid Markov logic networks
#@ Jue Wang;Pedro Domingos
#t 2008
#c 10
#% 26722
#% 44876
#% 73441
#% 226495
#% 528327
#% 673705
#% 840890
#% 850430
#% 1000452
#% 1000502
#% 1250224
#% 1250579
#% 1289583
#! Markov logic networks (MLNs) combine first-order logic and Markov networks, allowing us to handle the complexity and uncertainty of real-world problems in a single consistent framework. However, in MLNs all variables and features are discrete, while most real-world applications also contain continuous ones. In this paper we introduce hybrid MLNs, in which continuous properties (e.g., the distance between two objects) and functions over them can appear as features. Hybrid MLNs have all distributions in the exponential family as special cases (e.g., multivariate Gaussians), and allow much more compact modeling of non-i.i.d. data than propositional representations like hybrid Bayesian networks. We also introduce inference algorithms for hybrid MLNs, by extending the MaxWalkSAT and MC-SAT algorithms to continuous domains. Experiments in a mobile robot mapping domain--involving joint classification, clustering and regression--illustrate the power of hybrid MLNs as a modeling language, and the accuracy and efficiency of the inference algorithms.

#index 1270264
#* Latent tree models and approximate inference in Bayesian networks
#@ Yi Wang;Nevin L. Zhang;Tao Chen
#t 2008
#c 10
#% 115608
#% 246834
#% 272505
#% 771837
#% 840901
#% 1272279
#% 1650318
#! We propose a novel method for approximate inference in Bayesian networks (BNs). The idea is to sample data from a BN, learn a latent tree model (LTM) from the data offline, and when online, make inference with the LTM instead of the original BN. Because LTMs are tree-structured, inference takes linear time. In the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. Empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost.

#index 1270265
#* A general framework for generating multivariate explanations in Bayesian networks
#@ Changhe Yuan;Tsai-Ching Lu
#t 2008
#c 10
#% 132173
#% 527524
#% 527820
#% 527992
#% 564806
#% 788111
#% 1650391
#% 1650703
#% 1705556
#! Many existing explanation methods in Bayesian networks, such as Maximum a Posteriori (MAP) assignment and Most Probable Explanation (MPE), generate complete assignments for target variables. A priori, the set of target variables is often large, but only a few of them may be most relevant in explaining given evidence. Generating explanations with all the target variables is hence not always desirable. This paper addresses the problem by proposing a new framework called Most Relevant Explanation (MRE), which aims to automatically identify the most relevant target variables. We will also discuss in detail a specific instance of the framework that uses generalized Bayes factor as the relevance measure. Finally we will propose an approximate algorithm based on Reversible Jump MCMC and simulated annealing to solve MRE. Empirical results show that the new approach typically finds much more concise explanations than existing methods.

#index 1270266
#* On the enactability of business protocols
#@ Nirmit Desai;Munindar P. Singh
#t 2008
#c 10
#% 725841
#% 763743
#% 767381
#% 848048
#% 898148
#% 900880
#% 945703
#% 1083961
#% 1279310
#% 1733476
#% 1740811
#! Protocols specifying business interactions among autonomous parties enable reuse and promote interoperability. A protocol is specified from a global viewpoint, but enacted in a distributed manner by (agents playing) different roles. Each role describes a local representation. An ill-specified protocol may yield roles that fail to produce correct enactments of the protocol. Existing approaches lack a formal and comprehensive treatment of this problem. Building on recent work on declaratively specifying a protocol as a set of rules of causal logic, this paper formally defines the enactability of protocols. It presents necessary and sufficient conditions for the enactability of a protocol as well as a decision procedure for extracting correct roles from enactable protocols.

#index 1270267
#* Concept-based feature generation and selection for information retrieval
#@ Ofer Egozi;Evgeniy Gabrilovich;Shaul Markovitch
#t 2008
#c 10
#% 169729
#% 232703
#% 262102
#% 298183
#% 420464
#% 452881
#% 641976
#% 740768
#% 783507
#% 818204
#% 987231
#% 987276
#% 1019084
#% 1019105
#% 1250362
#% 1275012
#% 1682113
#! Traditional information retrieval systems use query words to identify relevant documents. In difficult retrieval tasks, however, one needs access to a wealth of background knowledge. We present a method that uses Wikipedia-based feature generation to improve retrieval performance. Intuitively, we expect that using extensive world knowledge is likely to improve recall but may adversely affect precision. High quality feature selection is necessary to maintain high precision, but here we do not have the labeled training data for evaluating features, that we have in supervised learning. We present a new feature selection method that is inspired by pseudorelevance feedback. We use the top-ranked and bottom-ranked documents retrieved by the bag-of-words method as representative sets of relevant and non-relevant documents. The generated features are then evaluated and filtered on the basis of these sets. Experiments on TREC data confirm the superior performance of our method compared to the previous state of the art.

#index 1270268
#* Linking social networks on the web with FOAF: a semantic web case study
#@ Jennifer Golbeck;Matthew Rothstein
#t 2008
#c 10
#% 220708
#% 415107
#% 452665
#% 654467
#% 729913
#% 730089
#% 754171
#% 766199
#% 791744
#% 835906
#% 869503
#% 1374378
#% 1668087
#% 1696350
#! One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.

#index 1270269
#* Mining translations of web queries from web click-through data
#@ Rong Hu;Weizhu Chen;Jian Hu;Yansheng Lu;Zheng Chen;Qiang Yang
#t 2008
#c 10
#% 280826
#% 283180
#% 397145
#% 420520
#% 466489
#% 735134
#% 747947
#% 815164
#% 939914
#% 987250
#% 1250378
#% 1269723
#! Query translation for Cross-Lingual Information Retrieval (CLIR) has gained increasing attention in the research area. Previous work mainly used machine translation systems, bilingual dictionaries, or web corpora to perform query translation. However, most of these approaches require either expensive language resources or complex language models, and cannot achieve timely translation for new queries. In this paper, we propose a novel solution to automatically acquire query translation pairs from the knowledge hidden in the click-through data, that are represented by the URL a user clicks after submitting a query to a search engine. Our proposed solution consists of two stages: identitying bilingual URL pair patterns in the click-through data and matching query translation pairs based on user click behavior. Experimental results on a real dataset show that our method not only generates existing query translation pairs with high precision, but also generates many timely query translation pairs that could not be obtained by previous methods. A comparative study between our system and two commercial online translation systems shows the advantage of our proposed method.

#index 1270270
#* Hierarchical location and topic based query expansion
#@ Shu Huang;Qiankun Zhao;Prasenjit Mitra;C. Lee Giles
#t 2008
#c 10
#% 249143
#% 340141
#% 342709
#% 348155
#% 722904
#% 730007
#% 783482
#% 785354
#% 818240
#% 838530
#% 838531
#% 838532
#% 879567
#! In this paper, we propose a novel approach to expand queries by exploring both location information and topic information of the queries. Users at different locations tend to have different vocabularies, while the different expressions coming from different vocabularies may relate to the same topics. Thus these expressions are identified as location sensitive and can be used for query expansion. We propose a hierarchical query expansion model, which employs a two-level SVM classification model to classify queries as location sensitive or location non-sensitive, where the former are further classified into same location sensitive and different location sensitive. For the location sensitive queries, we propose an LDA based topic-level query similarity measure to rank the list of similar queries. Experiments with 2G raw log data from CiteSeer and Excite show that our hierarchical classification model predicts the query location sensitivity with more than 80% precision and that the final search result is significantly better than existing query expansion methods.

#index 1270271
#* Semi-supervised learning for blog classification
#@ Daisuke Ikeda;Hiroya Takamura;Manabu Okumura
#t 2008
#c 10
#% 252011
#% 311027
#% 916788
#% 939332
#% 961152
#% 1249453
#% 1261539
#! Blog classification (e.g., identifying bloggers' gender or age) is one of the most interesting current problems in blog analysis. Although this problem is usually solved by applying supervised learning techniques, the large labeled dataset required for training is not always available. In contrast, unlabeled blogs can easily be collected from the web. Therefore, a semi-supervised learning method for blog classification, effectively using unlabeled data, is proposed. In this method, entries from the same blog are assumed to have the same characteristics. With this assumption, the proposed method captures the characteristics of each blog, such as writing style and topic, and uses these characteristics to improve the classification accuracy.

#index 1270272
#* Generating useful network-based features for analyzing social networks
#@ Jun Karamon;Yutaka Matsuo;Mitsuru Ishizuka
#t 2008
#c 10
#% 136350
#% 730089
#% 796142
#% 853532
#% 855601
#% 868089
#% 881460
#% 946524
#% 956578
#% 1655418
#! Recently, many Web services such as social networking services, blogs, and collaborative tagging have become widely popular. Many attempts are being made to investigate user interactions by analyzing social networks among users. However, analyzing a social network with attributional data is often not an easy task because numerous ways exist to define features through aggregation of different tables. In this study, we propose an algorithm to identify important network-based features systematically from a given social network to analyze user behavior efficiently and to expand the services. We apply our method for link-based classification and link prediction tasks with two different datasets, i.e., an @cosme (an online viral marketing site) dataset and a Hatena Bookmark (collaborative tagging service) dataset, to demonstrate the usefulness of our algorithm. Our algorithm is general and can provide useful network-based features for social network analyses.

#index 1270273
#* Automatic extraction of data points and text blocks from 2-dimensional plots in digital documents
#@ Saurabh Kataria;William Browuer;Prasenjit Mitra;C. Lee Giles
#t 2008
#c 10
#% 49243
#% 58391
#% 212690
#% 219847
#% 221423
#% 237279
#% 251159
#% 263214
#% 291299
#% 297159
#% 321652
#% 629566
#% 658582
#% 840455
#% 1065276
#% 1269719
#! Two dimensional plots (2-D) in digital documents on the web are an important source of information that is largely under-utilized. In this paper, we outline how data and text can be extracted automatically from these 2-D plots, thus eliminating a time consuming manual process. Our information extraction algorithm identifies the axes of the figures, extracts text blocks like axes-labels and legends and identifies data points in the figure. It also extracts the units appearing in the axes labels and segments the legends to identify the different lines in the legend, the different symbols and their associated text explanations. Our algorithm also performs the challenging task of separating out overlapping text and data points effectively. Our experiments indicate that these techniques are computationally efficient and provide acceptable accuracy.

#index 1270274
#* Minimizing the spread of contamination by blocking links in a network
#@ Masahiro Kimura;Kazumi Saito;Hiroshi Motoda
#t 2008
#c 10
#% 309749
#% 342596
#% 577217
#% 729923
#% 754107
#% 1269888
#! We address the problem of minimizing the propagation of undesirable things, such as computer viruses or malicious rumors, by blocking a limited number of links in a network, a dual problem to the influence maximization problem of finding the most influential nodes in a social network for information diffusion. This minimization problem is another approach to the problem of preventing the spread of contamination by removing nodes in a network. We propose a method for efficiently finding a good approximate solution to this problem based on a naturally greedy strategy. Using large real networks, we demonstrate experimentally that the proposed method significantly outperforms conventional link-removal methods. We also show that unlike the strategy of removing nodes, blocking links between nodes with high out-degrees is not necessarily effective.

#index 1270275
#* A utility-theoretic approach to privacy and personalization
#@ Andreas Krause;Eric Horvitz
#t 2008
#c 10
#% 303575
#% 576761
#% 754126
#% 757953
#% 802863
#% 818259
#% 832511
#% 864406
#% 864412
#% 956552
#% 956553
#% 1022266
#% 1029072
#% 1275193
#% 1670071
#% 1734340
#! Online services such as web search, news portals, and e-commerce applications face the challenge of providing high-quality experiences to a large, heterogeneous user base. Recent efforts have highlighted the potential to improve performance by personalizing services based on special knowledge about users. For example, a user's location, demographics, and search and browsing history may be useful in enhancing the results offered in response to web search queries. However, reasonable concerns about privacy by both users, providers, and government agencies acting on behalf of citizens, may limit access to such information. We introduce and explore an economics of privacy in personalization, where people can opt to share personal information in return for enhancements in the quality of an online service. We focus on the example of web search and formulate realistic objective functions for search efficacy and privacy. We demonstrate how we can identify a near-optimal solution to the utility-privacy tradeoff. We evaluate the methodology on data drawn from a log of the search activity of volunteer participants. We separately assess users' preferences about privacy and utility via a large-scale survey, aimed at eliciting preferences about peoples' willingness to trade the sharing of personal data in returns for gains in search efficiency. We show that a significant level of personalization can be achieved using only a small amount of information about users.

#index 1270276
#* Query-URL bipartite based approach to personalized query recommendation
#@ Lin Li;Zhenglu Yang;Ling Liu;Masaru Kitsuregawa
#t 2008
#c 10
#% 194299
#% 218978
#% 310567
#% 320839
#% 342659
#% 342673
#% 342961
#% 346798
#% 405168
#% 641976
#% 838532
#% 915329
#! Query recommendation is considered an effective assistant in enhancing keyword based queries in search engines and Web search software. Conventional approach to query recommendation has been focused on query-term based analysis over the user access logs. In this paper, we argue that utilizing the connectivity of a query-URL bipartite graph to recommend relevant queries can significantly improve the accuracy and effectiveness of the conventional query-term based query recommendation systems. We refer to the Query-URL Bipartite based query reCommendation approach as QUBIC. The QUBIC approach has two unique characteristics. First, instead of operating on the original bipartite graph directly using biclique based approach or graph clustering, we extract an affinity graph of queries from the initial query-URL bipartite graph. The affinity graph consists of only queries as its vertices and its edges are weighted according to a query-URL vector based similarity (distance) measure. By utilizing the query affinity graph, we are able to capture the propagation of similarity from query to query by inducing an implicit topical relatedness between queries. We devise a novel rank mechanism for ordering the related queries based on the merging distances of a hierarchical agglomerative clustering. We compare our proposed ranking algorithm with both naïve ranking that uses the query-URL similarity measure directly, and the single-linkage based ranking method. In addition, we make it possible for users to interactively participate in the query recommendation process, to bridge the gap between the determinacy of actual similarity values and the indeterminacy of users' information needs, allowing the lists of related queries to be changed from user to user and query to query, thus personalizing the query recommendation on demand. The experimental results from two query collections demonstrate the effectiveness and feasibility of our approach.

#index 1270277
#* Extracting relevant snippets for web navigation
#@ Qing Li;K. Selçuk Candan;Qi Yan
#t 2008
#c 10
#% 144012
#% 169809
#% 197837
#% 232677
#% 262096
#% 328532
#% 340901
#% 413592
#% 504890
#% 881050
#! Search engines present fix-length passages from documents ranked by relevance against the query. In this paper, we present and compare novel, language-model based methods for extracting variable length document snippets by real-time processing of documents using the query issued by the user. With this extra level of information, the returned snippets are considerably more informative. Unlike previous work on passage retrieval which relies on searching relevant segments for filtering of preoccupied passages, we focus on query-informed segmentation to extract context-aware relevant snippets with variable length. In particular, we show that, when informed through an appropriate relevance language model, curvature analysis and Hidden Markov model (HMM) based content segmentation techniques can facilitate to extract relevant document snippets.

#index 1270278
#* Intelligent output interface for intelligent medical search engine
#@ Gang Luo
#t 2008
#c 10
#% 956635
#! To facilitate ordinary people to search medical information, we have built an intelligent medical Web search engine called iMed. iMed uses medical knowledge and an interactive questionnaire to find multiple diseases serving as queries. The search results of these queries are combined together and returned to the searcher in a traditional sequential order. Nevertheless, searchers still frequently miss desired information, because the traditional search result output interface cannot capture the internal structures of medical search results. This paper presents a new, intelligent search result output interface devoted to intelligent medical search. The new output interface automatically offers searchers what they want instead of waiting until they ask explicitly. It structures all the search results into a multi-level hierarchy with explicitly marked medical meanings. In this way, searchers can efficiently navigate among all the search results and quickly obtain desired information. We demonstrate the effectiveness of our techniques through an evaluation using USMLE medical exam cases.

#index 1270279
#* Neural network based constraint satisfaction in ontology mapping
#@ Ming Mao;Yefei Peng;Michael Spring
#t 2008
#c 10
#% 43774
#% 109206
#% 156337
#% 248810
#% 660001
#% 728755
#% 735938
#% 742769
#% 790852
#% 869465
#% 924747
#% 993982
#% 1033671
#! Ontology mapping seeks to find semantic correspondences between similar elements of different ontologies. Ontology mapping is critical to achieve semantic interoperability in the WWW. Due to the fact that ubiquitous constraints (e.g., hierarchical restrictions in RDFS) caused by the characteristics of ontologies and their representations exist in ontologies, constraints satisfaction has become an intriguing research problem in ontology mapping area Though different techniques have been examined to find mappings, little work is made to solve constraint satisfaction problem for ontology mapping. Currently most approaches simply validate ontology constraints using isolate heuristic rules instead of comprehensively considering them in a global view. This paper proposes a neural network based approach to search for a global optimal solution that can satisfy ontology constraints as many as possible. Experimental results on OAEI benchmark tests #248-#266 show the approach is promising. It dramatically improves the performance of preliminary mapping results.

#index 1270280
#* Supporting manual mapping revision using logical reasoning
#@ Christian Meilicke;Heiner Stuckenschmidt;Andrei Tamilin
#t 2008
#c 10
#% 924747
#% 1269894
#% 1289442
#% 1374369
#% 1409911
#% 1702410
#% 1702419
#! Finding correct semantic correspondences between ontologies is one of the most challenging problems in the area of semantic web technologies. Experiences with benchmarking matching systems revealed that even the manual revision of automatically generated mappings is a very difficult problem because it has to take the semantics of the ontologies as well as interactions between correspondences into account. In this paper, we propose methods for supporting human experts in the task of reviSing automatically created mappings. In particular, we present non-standard reasoning methods for detecting and propagating implications of expert decisions on the correctness of a mapping. We show that the use of these reasoning methods significantly reduces the effort of mapping revision in terms of the number of decisions that have to be made by the expert.

#index 1270281
#* Decoding wikipedia categories for knowledge acquisition
#@ Vivi Nastase;Michael Strube
#t 2008
#c 10
#% 405391
#% 756964
#% 786515
#% 823312
#% 885438
#% 939383
#% 956503
#% 956564
#% 1019061
#% 1250402
#% 1269895
#% 1269899
#% 1271258
#% 1275182
#% 1409954
#! This paper presents an approach to acquire knowledge from Wikipedia categories and the category network. Many Wikipedia categories have complex names which reflect human classification and organizing instances, and thus encode knowledge about class attributes, taxonomic and other semantic relations. We decode the names and refer back to the network to induce relations between concepts in Wikipedia represented through pages or categories. The category structure allows us to propagate a relation detected between constituents of a category name to numerous concept links. The results of the process are evaluated against ResearchCyc and a subset also by human judges. The results support the idea that Wikipedia category names are a rich source of useful and accurate knowledge.

#index 1270282
#* Turning web text and search queries into factual knowledge: hierarchical class attribute extraction
#@ Marius Pasca
#t 2008
#c 10
#% 742092
#% 756964
#% 938705
#% 939600
#% 939601
#% 956503
#% 956564
#% 987250
#% 1249461
#% 1271271
#% 1275182
#% 1275192
#! A seed-based framework for textual information extraction allows for weakly supervised acquisition of open-domain class attributes over conceptual hierarchies, from a combination of Web documents and query logs. Automatically-extracted labeled classes, consisting of a label (e.g., painkillers) and an associated set of instances (e.g., vicodin, oxycontin), are linked under existing conceptual hierarchies (e.g., brain disorders and skin diseases are linked under the concepts BrainDisorder and SkinDisease respectively). Attributes extracted for the labeled classes are propagated upwards in the hierarchy, to determine the attributes of hierarchy concepts (e.g., Disease) from the attributes of their subconcepts (e.g., BrainDisorder and SkinDisease).

#index 1270283
#* Question utility: a novel static ranking of question search
#@ Young-In Song;Chin-Yew Lin;Yunbo Cao;Hae-Chang Rim
#t 2008
#c 10
#% 268079
#% 279755
#% 282905
#% 324192
#% 340948
#% 474643
#% 676170
#% 818241
#% 838398
#% 838472
#% 869534
#% 879593
#% 939968
#% 1272053
#! In this paper, we propose a notion of 'question utility' for studying usefulness of questions and show how question utility can be integrated into question search as static ranking. To measure question utility, we examine three methods: (a) a method of employing the language model to estimate the probability that a question is generated from a question collection and then using the probability as question utility; (b) a method of using the LexRank algorithm to evaluate centrality of questions and then using the centrality as question utility; and (c) the combination of (a) and (b). To use question utility in question search, we employ a log linear model for combining relevance score in question search and utility score regarding question utility. Our experimental results with the questions about 'travel' from Yahoo! Answers show that question utility can be effective in boosting up ranks of generally useful questions.

#index 1270284
#* Metalevel information in ontology-based applications
#@ Thanh Tran;Peter Haase;Boris Motik;Bernardo Cuenca Grau;Ian Horrocks
#t 2008
#c 10
#% 116625
#% 248026
#% 384978
#% 1055734
#% 1092031
#% 1274795
#% 1274815
#% 1666169
#! Applications of Semantic Web technologies often require the management of metalevel information--that is, information that provides additional detail about domain-level information, such as provenance or access rights policies. Existing OWL-based tools provide little or no support for the representation and management of metalevel information. To fill this gap, we propose a framework based on metaviews--ontologies that describe facts in the application domain. We have implemented our framework in the KAON2 reasoner, and have successfully applied it in a nontrivial scenario.

#index 1270285
#* Finding cars, goddesses and enzymes: parametrizable acquisition of labeled instances for open-domain information extraction
#@ Benjamin Van Durme;Marius Pasca
#t 2008
#c 10
#% 742092
#% 756964
#% 786523
#% 815297
#% 939601
#% 956503
#% 1117028
#% 1250402
#! A method is given for the extraction of large numbers of semantic classes along with their corresponding instances. Based on the recombination of elements clustered through distributional similarity, experimental results show the procedure allows for a parametric trade-off between high precision and expanded recall.

#index 1270286
#* An unsupervised approach for product record normalization across different web sites
#@ Tak-Lam Wong;Tik-Shun Wong;Wai Lam
#t 2008
#c 10
#% 310516
#% 577238
#% 729913
#% 788090
#% 818244
#% 844289
#% 867052
#% 889107
#% 912223
#% 915340
#% 989591
#% 989660
#% 1074055
#! An unsupervised probabilistic learning framework for normalizing product records across different retailer Web sites is presented. Our framework decomposes the problem into two tasks to achieve the goal. The first task aims at extracting attribute values of products from different sites and normalizing them into appropriate reference attributes. This task is challenging because the set of reference attributes is unknown in advance. Besides, the layout formats are different in different Web sites. The second task is to conduct product record normalization aiming at identifying product records referring to the same reference product based on the results of the first task. We develop a graphical model for the generation of text fragments in Web pages to accomplish the two tasks. One characteristic of our model is that the product attributes to be extracted are not required to be specified in advance and an unlimited number of previously unseen product attributes can be handled. We compare our framework with existing methods. Extensive experiments using over 300 Web pages from over 150 real-world Web sites from three different domains have been conducted demonstrating the effectiveness of our framework.

#index 1270287
#* A user-oriented webpage ranking algorithm based on user attention time
#@ Songhua Xu;Yi Zhu;Hao Jiang;Francis C. M. Lau
#t 2008
#c 10
#% 169777
#% 340974
#% 399057
#% 565237
#% 577224
#% 592155
#% 766454
#% 766472
#% 805200
#% 805877
#% 818221
#% 823348
#% 875048
#% 954949
#% 956552
#% 999747
#% 1001310
#% 1275224
#! We propose a new webpage ranking algorithm which is personalized. Our idea is to rely on the attention time spent on a document by the user as the essential clue for producing the user-oriented webpage ranking. The prediction of the attention time of a new webpage is based on the attention time of other previously browsed pages by this user. To acquire the attention time of the latter webpages, we developed a browser plugin which is able to record the time a user spends reading a certain webpage and then automatically send that data to a server. Once the user attention time is acquired, we calibrate it to account for potential repetitive occurrences of the webpage before using it in the prediction process. After the user's attention times of a collection of documents are known, our algorithm can predict the user's attention time of a new document through document content similarity analysis, which is applied to both texts and images. We evaluate the webpage ranking results from our algorithm by comparing them with the ones produced by Google's Pagerank algorithm.

#index 1270288
#* Efficient querying relaxed dominant relationship between product items based on rank aggregation
#@ Zhenglu Yang;Lin Li;Masaru Kitsuregawa
#t 2008
#c 10
#% 2115
#% 232703
#% 288976
#% 289148
#% 330769
#% 340936
#% 463903
#% 464726
#% 464996
#% 465167
#% 879582
#% 1269906
#! Current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data, we propose a new model, Relaxed Dominant Relationship (RDR), which extends the state-of-the-art work by incorporating rank aggregation methods. We propose efficient strategies on building compressed data structure to encode the core part of RDR between items. Efficient querying approaches are devised to facilitate the ranking process and to answer the RDR query. Extensive experiments are conducted and the results illustrate the effectiveness and efficiency of our methods.

#index 1270289
#* Proceedings of the 23rd national conference on Artificial intelligence - Volume 3
#@ Anthony Cohn
#t 2008
#c 10

#index 1270290
#* Spatial scaffolding for sociable robot learning
#@ Cynthia Breazeal;Matt Berlin
#t 2008
#c 10
#% 257585
#% 284558
#% 603592
#% 656622
#% 715299
#% 756356
#% 946172
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable spatial scaffolding cues to learn from human teachers. We present an integrated robotic architecture that combines social attention and machine learning components to learn tasks effectively from natural spatial scaffolding interactions with human teachers. We evaluate the performance of this architecture in comparison to human learning data drawn from a novel study of the use of embodied cues in human task learning and teaching behavior. This evaluation provides quantitative evidence for the utility of spatial scaffolding to learning systems. In addition, this evaluation supported the construction of a novel, interactive demonstration of a humanoid robot taking advantage of spatial scaffolding cues to learn from natural human teaching behavior.

#index 1270291
#* POIROT: integrated learning of web service procedures
#@ Mark Burstein;Robert Laddaga;David McDonald;Michael Cox;Brett Benyo;Paul Robertson;Talib Hussain;Marshall Brinn;Drew McDermott
#t 2008
#c 10
#% 73375
#% 120806
#% 154075
#% 172505
#% 286419
#% 449567
#% 464464
#% 490276
#% 495942
#% 694516
#% 992714
#% 1279469
#% 1415877
#% 1718451
#! POIROT is an integration framework for combining machine learning mechanisms to learn hierarchical models of web services procedures from a single or very small set of demonstration examples. The system is organized around a shared representation language for communications with a central hypothesis blackboard. Component learning systems share semantic representations of their hypotheses (generalizations) and inferences about demonstration traces. To further the process, components may generate learning goals for other learning components. POIROT's learners or hypothesis formers develop workflows that include order dependencies, subgoals, and decision criteria for selecting or prioritizing subtasks and service parameters. Hypothesis evaluators, guided by POIROT's meta-control component, plan experiments to confirm or disconfirm hypotheses extracted from these learning products. Collectively, they create methods that POIROT can use to reproduce the demonstration and solve similar problems. After its first phase of development, POIROT has demonstrated it can learn some moderately complex hierarchical task models from semantic traces of user-generated service transaction sequences at a level that is approaching human performance on the same learning task.

#index 1270292
#* An integrated reasoning approach to moral decision-making
#@ Morteza Dehghani;Emmett Tomai;Ken Forbus;Matthew Klenk
#t 2008
#c 10
#% 65345
#% 179717
#% 888869
#% 964212
#! We present a computational model, MoralDM, which integrates several AI techniques in order to model recent psychological findings on moral decision-making. Current theories of moral decision-making extend beyond pure utilitarian models by relying on contextual factors that vary with culture. MoralDM uses a natural language system to produce formal representations from psychological stimuli, to reduce tailorability. The impacts of secular versus sacred values are modeled via qualitative reasoning, using an order of magnitude representation. MoralDM uses a combination of first-principles reasoning and analogical reasoning to determine consequences and utilities when making moral judgments. We describe how MoralDM works and show that it can model psychological results and improve its performance via accumulating examples.

#index 1270293
#* RADAR: a personal assistant that learns to reduce email overload
#@ Michael Freed;Jaime Carbonell;Geoff Gordon;Jordan Hayes;Brad Myers;Daniel Siewiorek;Stephen Smith;Aaron Steinfeld;Anthony Tomasic
#t 2008
#c 10
#% 54259
#% 214751
#% 340962
#% 452634
#% 818214
#% 870799
#% 905347
#% 913196
#% 954978
#% 955064
#% 956498
#% 1078693
#% 1137778
#% 1221065
#% 1269504
#% 1272137
#% 1289515
#% 1304849
#% 1304850
#% 1739292
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270294
#* The PELA architecture: integrating planning and learning to improve execution
#@ Sergio Jiménez;Fernando Fernández;Daniel Borrajo
#t 2008
#c 10
#% 170768
#% 252221
#% 550562
#% 1272008
#% 1272017
#% 1272092
#% 1272161
#! Building architectures for autonomous rational behavior requires the integration of several AI components, such as planning, learning and execution monitoring. In most cases, the techniques used for planning and learning are tailored to the specific integrated architecture, so they could not be replaced by other equivalent techniques. Also, in order to solve tasks that require lookahead reasoning under uncertainty, these architectures need an accurate domain model to feed the planning component. But the manual definition of these models is a difficult task. In this paper, we propose an architecture that uses off-the-shelf interchangeable planning and learning components to solve tasks that require flexible planning under uncertainty. We show how a relational learning component can be applied to automatically obtain accurate probabilistic action models from executions of plans. These models can be used by any classical planner that handles metric functions, or, alternatively, by any decision theoretic planner. We also show how these components can be integrated to solve tasks continuously, under an online relational learning scheme.

#index 1270295
#* Incorporating mental simulation for a more effective robotic teammate
#@ William G. Kennedy;Magdalena D. Bugajska;William Adams;Alan C. Schultz;J. Gregory Trafton
#t 2008
#c 10
#% 215532
#% 224727
#% 241019
#% 334544
#% 418689
#% 418731
#% 581031
#% 732422
#% 946172
#% 1041901
#% 1269917
#% 1271813
#% 1272316
#% 1784813
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270296
#* Pervasive diagnosis: the integration of diagnostic goals into production plans
#@ Lukas Kuhn;Bob Price;Johan De Kleer;Minh Do;Rong Zhou
#t 2008
#c 10
#% 21138
#% 125533
#% 262737
#% 337980
#% 849353
#% 1271962
#! In model-based control, a planner uses a system description to create a plan that achieves production goals (Fikes & Nilsson 1971). The same description can be used by model-based diagnosis to infer the condition of components in a system from partially informative sensors. Prior work has demonstrated that diagnosis can be used to adapt the control of a system to changes in its components. However diagnosis must either make inferences from passive observations of production, or production must be halted to take diagnostic actions. We observe that the declarative nature of model-based control allows the planner to achieve production goals in multiple ways. This exibility can be exploited with a novel paradigm we call pervasive diagnosis which produces diagnostic production plans that simultaneously achieve production goals while uncovering additional information about component health. We present an efficient heuristic search for these diagnostic production plans and show through experiments on a model of an industrial digital printing press that the theoretical increase in information can be realized on practical real-time systems. We obtain higher long-run productivity than a decoupled combination of planning and diagnosis.

#index 1270297
#* An integrated agent for playing real-time strategy games
#@ Josh McCoy;Michael Mateas
#t 2008
#c 10
#% 445553
#% 1270127
#% 1272316
#% 1279355
#% 1279476
#! We present a real-time strategy (RTS) game AI agent that integrates multiple specialist components to play a complete game. Based on an analysis of how skilled human players conceptualize RTS gameplay, we partition the problem space into domains of competence seen in expert human play. This partitioning helps us to manage and take advantage of the large amount of sophisticated domain knowledge developed by human players. We present results showing that incorporating expert high-level strategic knowledge allows our agent to consistently defeat established scripted AI players. In addition, this work lays the foundation to incorporate tactics and unit micromanagement techniques developed by both man and machine.

#index 1270298
#* Adaptive control for autonomous underwater vehicles
#@ Conor McGann;Frederic Py;Kanna Rajan;John Ryan;Richard Henthorn
#t 2008
#c 10
#% 126385
#% 262737
#% 263059
#% 418711
#% 722503
#% 873957
#% 896457
#! We describe a novel integration of Planning with Probabilistic State Estimation and Execution. The resulting system is a unified representational and computational framework based on declarative models and constraint-based temporal plans. The work is motivated by the need to explore the oceans more cost-effectively through the use of Autonomous Underwater Vehicles (AUV), requiring them to be goal-directed, perceptive, adaptive and robust in the context of dynamic and uncertain conditions. The novelty of our approach is in integrating deliberation and reaction over different temporal and functional scopes within a single model, and in breaking new ground in oceanography by allowing for precise sampling within a feature of interest using an autonomous robot. The system is general-purpose and adaptable to other ocean going and terrestrial platforms.

#index 1270299
#* Achieving far transfer in an integrated cognitive architecture
#@ Dan Shapiro;Tolga Könik;Paul O'Rorke
#t 2008
#c 10
#% 23011
#% 65345
#% 75896
#% 876028
#% 961150
#% 1250572
#% 1289205
#! Transfer is the ability to employ knowledge acquired in one task to improve performance in another. We study transfer in the context of the ICARUS cognitive architecture, which supplies diverse capabilities for execution, inference, planning, and learning. We report on an extension to ICARUS called representation mapping that transfers structured skills and concepts between disparate tasks that may not even be expressed with the same symbol set. We show that representation mapping is naturally integrated into ICARUS' cognitive processing loop, resulting in a system that addresses a qualitatively new class of problems by considering the relevance of past experience to current goals.

#index 1270300
#* Bimodal spatial reasoning with continuous motion
#@ Samuel Wintermute;John E. Laird
#t 2008
#c 10
#% 6200
#% 109857
#% 934104
#% 1222482
#% 1250395
#% 1269920
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270301
#* Planning for human-robot interaction using time-state aggregated POMDPs
#@ Frank Broz;Illah Nourbakhsh;Reid Simmons
#t 2008
#c 10
#% 252183
#% 314843
#% 528302
#% 812412
#% 1250235
#% 1269869
#% 1289556
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270302
#* The hidden permutation model and location-based activity recognition
#@ Hung H. Bui;Dinh Phung;Svetha Venkatesh;Hai Phan
#t 2008
#c 10
#% 578771
#% 905281
#% 1250174
#% 1250210
#% 1272356
#% 1289474
#% 1502512
#% 1719085
#! Permutation modeling is challenging because of the combinatorial nature of the problem. However, such modeling is often required in many real-world applications, including activity recognition where subactivities are often permuted and partially ordered. This paper introduces a novel Hidden Permutation Model (HPM) that can learn the partial ordering constraints in permuted state sequences. The HPM is parameterized as an exponential family distribution and is flexible so that it can encode constraints via different feature functions. A chain-flipping Metropolis-Hastings Markov chain Monte Carlo (MCMC) is employed for inference to overcome the O(n!) complexity. Gradient-based maximum likelihood parameter learning is presented for two cases when the permutation is known and when it is hidden. The HPM is evaluated using both simulated and real data from a location-based activity recognition domain. Experimental results indicate that the HPM performs far better than other baseline models, including the naive Bayes classifier, the HMM classifier, and Kirshner's multinomial permutation model. Our presented HPM is generic and can potentially be utilized in any problem where the modeling of permuted states from noisy data is needed.

#index 1270303
#* Adaptive importance sampling with automatic model selection in value function approximation
#@ Hirotaka Hachiya;Takayuki Akiyama;Masashi Sugiyama;Jan Peters
#t 2008
#c 10
#% 384911
#% 464438
#% 464609
#% 466751
#% 528322
#% 734920
#% 1074038
#! Off-policy reinforcement learning is aimed at efficiently reusing data samples gathered in the past, which is an essential problem for physically grounded AI as experiments are usually prohibitively expensive. A common approach is to use importance sampling techniques for compensating for the bias caused by the difference between data-sampling policies and the target policy. However, existing off-policy methods do not often take the variance of value function estimators explicitly into account and therefore their performance tends to be unstable. To cope with this problem, we propose using an adaptive importance sampling technique which allows us to actively control the trade-off between bias and variance. We further provide a method for optimally determining the trade-off parameter based on a variant of cross-validation. We demonstrate the usefulness of the proposed approach through simulations.

#index 1270304
#* Anticipatory perceptual simulation for human-robot joint practice: theory and application study
#@ Guy Hoffman;Cynthia Breazeal
#t 2008
#c 10
#% 592134
#% 593680
#% 1041876
#% 1056546
#% 1337341
#% 1768978
#! With the aim of fluency and efficiency in human-robot teams, we have developed a cognitive architecture based on the neuro-psychological principles of anticipation and perceptual simulation through top-down biasing. An instantiation of this architecture was implemented on a nonanthropomorphic robotic lamp, performing in a human-robot collaborative task. In a human-subject study, in which the robot works on a joint task with untrained subjects, we find our approach to be significantly more efficient and fluent than in a comparable system without anticipatory perceptual simulation. We also show the robot and the human to be increasingly contributing at a similar rate. Through self-report, we find significant differences between the two conditions in the sense of team fluency, the team's improvement over time, and the robot's contribution to the efficiency and fluency. We also find difference in verbal attitudes towards the robot: most notably, subjects working with the anticipatory robot attribute more positive and more human qualities to the robot, but display increased self-blame and self-deprecation.

#index 1270305
#* CIGAR: concurrent and interleaving goal and activity recognition
#@ Derek Hao Hu;Qiang Yang
#t 2008
#c 10
#% 31919
#% 464434
#% 787098
#% 843360
#% 946811
#% 961269
#% 980239
#% 1024911
#% 1269359
#% 1275160
#% 1289474
#% 1668045
#! In artificial intelligence and pervasive computing research, inferring users' high-level goals from activity sequences is an important task. A major challenge in goal recognition is that users often pursue several high-level goals in a concurrent and interleaving manner, where the pursuit of goals may spread over different parts of an activity sequence and may be pursued in parallel. Existing approaches to recognizing multiple goals often formulate this problem either as a single-goal recognition problem or in a deterministic way, ignoring uncertainty. In this paper, we propose CIGAR (Concurrent and Interleaving Goal and Activity Recognition) - a novel and simple two-level probabilistic framework for multiple-goal recognition where we can recognize both concurrent and interleaving goals. We use skip-chain conditional random fields (SCCRF) for modeling interleaving goals and we model concurrent goals by adjusting inferred probabilities through a correlation graph, which is a major advantage in that we are able to reason about goal interactions explicitly through the correlation graph. The two-level framework also avoids the high training complexity when modeling concurrency and interleaving together in a unified CRF model. Experimental results show that our method can effectively improve recognition accuracies on several real-world datasets collected from various wireless and sensor networks.

#index 1270306
#* Efficient optimization of information-theoretic exploration in SLAM
#@ Thomas Kollar;Nicholas Roy
#t 2008
#c 10
#% 408396
#% 590989
#% 1034796
#% 1279373
#% 1810030
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270307
#* An efficient motion planning algorithm for stochastic dynamic systems with constraints on probability of failure
#@ Masahiro Ono;Brian C. Williams
#t 2008
#c 10
#% 1269377
#% 1792270
#! When controlling dynamic systems, such as mobile robots in uncertain environments, there is a trade off between risk and reward. For example, a race car can turn a corner faster by taking a more challenging path. This paper proposes a new approach to planning a control sequence with a guaranteed risk bound. Given a stochastic dynamic model, the problem is to find a control sequence that optimizes a performance metric, while satisfying chance constraints i.e. constraints on the upper bound of the probability of failure. We propose a two-stage optimization approach, with the upper stage optimizing the risk allocation and the lower stage calculating the optimal control sequence that maximizes reward. In general, the upper-stage is a non-convex optimization problem, which is hard to solve. We develop a new iterative algorithm for this stage that efficiently computes the risk allocation with a small penalty to optimality. The algorithm is implemented and tested on the autonomous underwater vehicle (AUV) depth planning problem, and demonstrates a substantial improvement in computation cost and suboptimality, compared to the prior arts.

#index 1270308
#* Transferring localization models across space
#@ Sinno Jialin Pan;Dou Shen;Qiang Yang;James T. Kwok
#t 2008
#c 10
#% 236497
#% 757953
#% 763697
#% 819455
#% 961218
#% 1269361
#% 1275102
#% 1275152
#! Machine learning approaches to indoor WiFi localization involve an offline phase and an online phase. In the offline phase, data are collected from an environment to build a localization model, which will be applied to new data collected in the online phase for location estimation. However, collecting the labeled data across an entire building would be too time consuming. In this paper, we present a novel approach to transferring the learning model trained on data from one area of a building to another. We learn a mapping function between the signal space and the location space by solving an optimization problem based on manifold learning techniques. A low-dimensional manifold is shared between data collected in different areas in an environment as a bridge to propagate the knowledge across the whole environment. With the help of the transferred knowledge, we can significantly reduce the amount of labeled data which are required for building the localization model. We test the effectiveness of our proposed solution in a real indoor WiFi environment.

#index 1270309
#* Structure learning on large scale common sense statistical models of human state
#@ William Pentney;Matthai Philipose;Jeff Bilmes
#t 2008
#c 10
#% 44876
#% 226495
#% 481290
#% 1250181
#% 1250214
#% 1250649
#% 1269362
#% 1269743
#% 1269873
#% 1289473
#% 1673026
#! Research has shown promise in the design of large scale common sense probabilistic models to infer human state from environmental sensor data. These models have made use of mined and preexisting common sense data and traditional probabilistic machine learning techniques to improve recognition of the state of everyday human life. In this paper, we demonstrate effective techniques for structure learning on graphical models designed for this domain, improving the SRCS system of (Pentney et al. 2006) by learning additional dependencies between variables. Because the models used for common sense reasoning typically involve a large number of variables, issues of scale arise in searching for additional dependencies; we discuss how we use data mining techniques to address this problem. We show experimentally that these techniques improve the accuracy of state prediction, and that, with a good prior model, the use of a common sense model with structure learning provides better prediction of unlabeled variables as well as labeled variables. The results also demonstrate that it is possible to collect new common sense information about daily life using such a statistical model and labeled data.

#index 1270310
#* Reducing particle filtering complexity for 3D motion capture using dynamic Bayesian networks
#@ Cédric Rose;Jamal Saboune;François Charpillet
#t 2008
#c 10
#% 266616
#% 527664
#% 527691
#% 716892
#% 844101
#! Particle filtering algorithms can be used for the monitoring of dynamic systems with continuous state variables and without any constraints on the form of the probability distributions. The dimensionality of the problem remains a limitation of these approaches due to the growing number of particles required for the exploration of the state space. Computer vision problems such as 3D motion tracking are an example of complex monitoring problems which have a high dimensional state space and observation functions with high computational cost. In this article we focus on reducing the required number of particles in the case of monitoring tasks where the state vector and the observation function can be factored. We introduce a particle filtering algorithm based on the Dynamic Bayesian network (DBN) formalism which takes advantage of a factored representation of the state space for efficiently weighting and selecting the particles. We illustrate the approach on a simulated and a realworld 3D motion tracking tasks.

#index 1270311
#* A fast data collection and augmentation procedure for object recognition
#@ Benjamin Sapp;Ashutosh Saxena;Andrew Y. Ng
#t 2008
#c 10
#% 213571
#% 220995
#% 247889
#% 592135
#% 736300
#% 778740
#% 812418
#% 812600
#% 836813
#% 836904
#% 840909
#% 852098
#% 860013
#% 875957
#% 927357
#% 1034798
#% 1042788
#% 1502411
#% 1502498
#% 1672482
#% 1685050
#! When building an application that requires object class recognition, having enough data to learn from is critical for good performance, and can easily determine the success or failure of the system. However, it is typically extremely labor-intensive to collect data, as the process usually involves acquiring the image, then manual cropping and hand-labeling. Preparing large training sets for object recognition has already become one of the main bottlenecks for such emerging applications as mobile robotics and object recognition on the web. This paper focuses on a novel and practical solution to the dataset collection problem. Our method is based on using a green screen to rapidly collect example images; we then use a probabilistic model to rapidly synthesize a much larger training set that attempts to capture desired invariants in the object's foreground and background. We demonstrate this procedure on our own mobile robotics platform, where we achieve 135x savings in the time/effort needed to obtain a training set. Our data collection method is agnostic to the learning algorithm being used, and applies to any of a large class of standard object recognition methods. Given these results, we suggest that this method become a standard protocol for developing scalable object recognition systems. Further, we used our data to build reliable classifiers that enabled our robot to visually recognize an object in an office environment, and thereby fetch an object from an office in response to a verbal request.

#index 1270312
#* Multimodal people detection and tracking in crowded scenes
#@ Luciano Spinello;Rudolph Triebel;Roland Siegwart
#t 2008
#c 10
#% 116149
#% 156131
#% 235377
#% 277314
#% 336055
#% 378067
#% 724288
#% 736300
#% 760805
#% 812417
#% 824956
#% 898309
#! This paper presents a novel people detection and tracking method based on a multi-modal sensor fusion approach that utilizes 2D laser range and camera data. The data points in the laser scans are clustered using a novel graph-based method and an SVM based version of the cascaded AdaBoost classifier is trained with a set of geometrical features of these clusters. In the detection phase, the classified laser data is projected into the camera image to define a region of interest for the vision-based people detector. This detector is a fast version of the Implicit Shape Model (ISM) that learns an appearance codebook of local SIFT descriptors from a set of hand-labeled images of pedestrians and uses them in a voting scheme to vote for centers of detected people. The extension consists in a fast and detailed analysis of the spatial distribution of voters per detected person. Each detected person is tracked using a greedy data association method and multiple Extended Kalman Filters that use different motion models. This way, the filter can cope with a variety of different motion patterns. The tracker is asynchronously updated by the detections from the laser and the camera data. Experiments conducted in real-world outdoor scenarios with crowds of pedestrians demonstrate the usefulness of our approach.

#index 1270313
#* Feature selection for activity recognition in multi-robot domains
#@ Douglas L. Vail;Manuela M. Veloso
#t 2008
#c 10
#% 243727
#% 455258
#% 464434
#% 722937
#% 961269
#% 983808
#% 1024911
#% 1275160
#% 1650318
#% 1673026
#! In multi-robot settings, activity recognition allows a robot to respond intelligently to the other robots in its environment. Conditional random fields are temporal models that are well suited for activity recognition because they can robustly incorporate rich, non-independent features computed from sensory data. In this work, we explore feature selection in conditional random fields for activity recognition to choose which features should be included in the final model. We compare two feature selection methods, grafting, a greedy forward-selection strategy, and l1 regularization, which simultaneously smoothes the model and selects a subset of the features. We use robot data recorded during four games of the Small Size League of the RoboCup'07 robot soccer world championship to empirically compare the performance of the two feature selection algorithms in terms of accuracy of the final model, the number of features selected in the final model, and the time required to train the final model.

#index 1270314
#* Transferring localization models over time
#@ Vincent Wenchen Zheng;Evan Wei Xiang;Qiang Yang;Dou Shen
#t 2008
#c 10
#% 95730
#% 339218
#% 401172
#% 613383
#% 797050
#% 819455
#% 1269847
#% 1272110
#% 1272356
#% 1275152
#% 1290055
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270315
#* Transferring multi-device localization models using latent multi-task learning
#@ Vincent Wenchen Zheng;Sinno Jialin Pan;Qiang Yang;Jeffrey Junfeng Pan
#t 2008
#c 10
#% 236497
#% 723239
#% 735256
#% 768632
#% 769886
#% 777788
#% 819455
#% 916788
#% 1271814
#% 1275152
#! In this paper, we propose a latent multi-task learning algorithm to solve the multi-device indoor localization problem. Traditional indoor localization systems often assume that the collected signal data distributions are fixed, and thus the localization model learned on one device can be used on other devices without adaptation. However, by empirically studying the signal variation over different devices, we found this assumption to be invalid in practice. To solve this problem, we treat multiple devices as multiple learning tasks, and propose a multi-task learning algorithm. Different from algorithms assuming that the hypotheses learned from the original data space for related tasks can be similar, we only require the hypotheses learned in a latent feature space are similar. To establish our algorithm, we employ an alternating optimization approach to iteratively learn feature mappings and multi-task regression models for the devices. We apply our latent multi-task learning algorithm to real-world indoor localization data and demonstrate its effectiveness.

#index 1270316
#* Maximum entropy inverse reinforcement learning
#@ Brian D. Ziebart;Andrew Maas;J. Andrew Bagnell;Anind K. Dey
#t 2008
#c 10
#% 464434
#% 466418
#% 770852
#% 876036
#% 953324
#% 1250109
#% 1275169
#% 1674770
#% 1728806
#! Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.

#index 1270317
#* Interaction structure and dimensionality reduction in decentralized MDPs
#@ Martin Allen;Marek Petrik;Shlomo Zilberstein
#t 2008
#c 10
#% 450852
#% 643287
#% 1269789
#% 1272052
#! Decentralized Markov Decision Processes are a powerful general model of decentralized, cooperative multi-agent problem solving. The high complexity of the general problem leads to a focus on restricted models. While worst-case hardness of such reduced problems is often better, less is known about the actual difficulty of given instances. We show tight connections between the structure of agent interactions and the essential dimensionality of various problems. Bounds are placed on problem difficulty, given restrictions on the type and number of interactions between agents. These bounds arise from a bilinear programming formulation of the problem; from such a formulation, a more compact reduced form can be automatically generated, and the original problem rewritten to take advantage of the reduction.

#index 1270318
#* Generating hard SAT/CSP instances using expander graphs
#@ Carlos Ansótegui;Ramón Béjar;Cèsar Fernández;Carles Mateu
#t 2008
#c 10
#% 741028
#% 895016
#% 1250452
#! In this paper we provide a new method to generate hard k-SAT instances. We incrementally construct a high girth bipartite incidence graph of the k-SAT instance. Having high girth assures high expansion for the graph, and high expansion implies high resolution width. We have extended this approach to generate hard n-ary CSP instances and we have also adapted this idea to increase the expansion of the system of linear equations used to generate XORSAT instances, being able to produce harder satisfiable instances than former generators.

#index 1270319
#* An effective and robust method for short text classification
#@ Victoria Bobicev;Marina Sokolova
#t 2008
#c 10
#% 67454
#% 804807
#% 961230
#% 1663660
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270320
#* Hybrid constraint tightening for solving hybrid scheduling problems
#@ James C. Boerkoel;Edmund H. Durfee
#t 2008
#c 10
#% 107137
#% 1056487
#% 1269548
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270321
#* Data-driven programming and behavior for autonomous virtual characters
#@ Jonathan Dinerstein;Parris K. Egbert;Dan Ventura;Michael Goodrich
#t 2008
#c 10

#index 1270322
#* Limits and possibilities of BDDs in state space search
#@ Stefan Edelkamp;Peter Kissmann
#t 2008
#c 10
#% 711069
#% 1269831
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270323
#* Expectation-based versus potential-aware automated abstraction in imperfect information games: an experimental comparison using poker
#@ Andrew Gilpin;Thomas Sandholm
#t 2008
#c 10
#% 348584
#% 991788
#% 1024868
#% 1083975
#% 1269678
#% 1269795
#% 1270031
#% 1407337
#! Automated abstraction algorithms for sequential imperfect information games have recently emerged as a key component in developing competitive game theory-based agents. The existing literature has not investigated the relative performance of different abstraction algorithms. Instead, agents whose construction has used automated abstraction have only been compared under confounding effects: different granularities of abstraction and equilibrium-finding algorithms that yield different accuracies when solving the abstracted game. This paper provides the first systematic evaluation of abstraction algorithms. Two families of algorithms have been proposed. The distinguishing feature is the measure used to evaluate the strategic similarity between game states. One algorithm uses the probability of winning as the similarity measure. The other uses a potential-aware similarity measure based on probability distributions over future states. We conduct experiments on Rhode Island Hold'em poker. We compare the algorithms against each other, against optimal play, and against each agent's nemesis. We also compare them based on the resulting game's value. Interestingly, for very coarse abstractions the expectation-based algorithm is better, but for moderately coarse and fine abstractions the potential-aware approach is superior. Furthermore, agents constructed using the expectation-based approach are highly exploitable beyond what their performance against the game's optimal strategy would suggest.

#index 1270324
#* Learning to identify reduced passive verb phrases with a shallow parser
#@ Sean Igo;Ellen Riloff
#t 2008
#c 10
#% 269217
#% 708948
#% 742218
#% 853874
#% 926881
#! Our research is motivated by the observation that NLP systems frequently mislabel passive voice verb phrases as being in the active voice when there is no auxiliary verb (e.g., "The man arrested had a long record"). These errors directly impact thematic role recognition and NLP applications that depend on it. We present a learned classifier that can accurately identify reduced passive voice constructions in shallow parsing environments.

#index 1270325
#* The re-representation problem in a logic-based framework for analogy making
#@ Ulf Krumnack;Helmar Gust;Kai-Uwe Kühnberger;Angela Schwering
#t 2008
#c 10
#% 198940
#% 830517
#% 872808

#index 1270326
#* A Bayesian Kernel logistic discriminant model: an improvement to the Kernel Fisher's discriminant
#@ R. Ksantini;D. Ziou;B. Colin;F. Dubeau
#t 2008
#c 10
#% 331916
#% 424806
#! The Kernel Fisher's Discriminant (KFD) has proven to be competitive to several state-of-the-art classifiers. However, it is assuming equal covariance structure for all transformed classes, which is not true in many applications. In this paper, we propose a novel Bayesian Kernel Logistic Discriminant model (BKLD) which goes one step further by representing each transformed class by its own covariance matrix. This can perform better than the KFD. An extensive comparison of the BKLD to the KFD and to other state-of-the-art non-linear classifiers is performed.

#index 1270327
#* Dynamic distributed constraint reasoning
#@ Robert N. Lass;Evan A. Sultanik;William C. Regli
#t 2008
#c 10
#% 3460
#% 168251
#% 321618
#% 443227
#% 564936
#% 643099
#% 773306
#% 1150824
#% 1269429
#% 1830870
#! What local action can agents take, without the benefit of global knowledge, to produce the best global solution? Many dynamic distributed systems can be modeled using techniques from distributed constraint reasoning, however, existing work in the distributed constraint reasoning community does not address the true dynamism inherent in many real-world systems.

#index 1270328
#* Ensemble forecasting for disease outbreak detection
#@ Thomas H. Lotze;Galit Shmueli
#t 2008
#c 10
#! We describe a method to improve detection of disease outbreaks in pre-diagnostic time series data. The method uses multiple forecasters and learns the linear combination to minimize the expected squared error of the next day's forecast. This combination adaptively changes over time. This adaptive ensemble combination is used to generate a disease alert score for each day, using a separate multiday combination method learned from examples of different disease outbreak patterns. These scores are used to generate an alert for the epidemiologist practitioner. Several variants are also proposed and compared. Results from the International Society for Disease Surveillance (ISDS) technical contest are given, evaluating this method on three syndromic series with representative outbreaks.

#index 1270329
#* Fast spectral learning using Lanczos eigenspace projections
#@ Sridhar Mahadevan
#t 2008
#c 10
#% 384911
#% 1014677
#% 1275167
#! The core computational step in spectral learning - finding the projection of a function onto the eigenspace of a symmetric operator, such as a graph Laplacian - generally incurs a cubic computational complexity O(N3). This paper describes the use of Lanczos eigenspace projections for accelerating spectral projections, which reduces the complexity to O(nTop + n2N) operations, where n is the number of distinct eigenvalues, and Top is the complexity of mUltiplying T by a vector. This approach is based on diagonalizing the restriction of the operator to the Krylov space spanned by the operator and a projected function. Even further savings can be accrued by constructing an approximate Lanczos tridiagonal representation of the Krylov-space restricted operator. A key novelty of this paper is the use of Krylov-subspace modulated Lanczos acceleration for multi-resolution wavelet analysis. A challenging problem of learning to control a robot arm is used to test the proposed approach.

#index 1270330
#* Efficiently exploiting dependencies in local search for SAT
#@ Duc Nghia Pham;John Thornton;Abdul Sattar
#t 2008
#c 10
#% 303154
#% 520743
#% 535147
#% 535317
#% 578753
#% 724946
#% 1275133
#% 1396047
#% 1406883
#% 1675284
#! We propose a new local search platform that splits a CNF formula into three sub-components: i) a minimal dependency lattice (representing the core connections between logic gates), ii) a conjunction of equivalence clauses, and iii) the remaining clauses. We also adopt a new hierarchical cost function that focuses on solving the core components of the problem first. We then show experimentally that our platform not only significantly outperforms existing local search approaches but is also competitive with modern systematic solvers on highly structured problems.

#index 1270331
#* Towards synthesizing optimal coordination modules for distributed agents
#@ Manh Tung Pham;Kiam Tian Seow
#t 2008
#c 10
#% 29948
#% 31432
#% 39249
#% 729314
#% 926913
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270332
#* A new clause learning scheme for efficient unsatisfiability proofs
#@ Knot Pipatsrisawat;Adnan Darwiche
#t 2008
#c 10
#% 220203
#% 288567
#% 336874
#% 427631
#% 1396060
#! We formalize in this paper a key property of asserting clauses (the most common type of clauses learned by SAT solvers). We show that the formalized property, which is called empowerment, is not exclusive to asserting clauses, and introduce a new class of learned clauses which can also be empowering. We show empirically that (1) the new class of clauses tends to be much shorter and induce further backtracks than asserting clauses and (2) an empowering subset of this new class of clauses significantly improves the performance of the Rsat solver on unsatisfiable problems.

#index 1270333
#* Bayes-relational learning of opponent models from incomplete information in no-limit poker
#@ Marc Ponsen;Jan Ramon;Tom Croonenborghs;Kurt Driessens;Karl Tuyls
#t 2008
#c 10
#% 252221
#% 1740191
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270334
#* Multi-HDP: a non parametric Bayesian model for tensor factorization
#@ Ian Porteous;Evgeniy Bart;Max Welling
#t 2008
#c 10
#% 722904
#% 734592
#% 1117695
#% 1250567
#! Matrix factorization algorithms are frequently used in the machine leaming community to find low dimensional representations of data. We introduce a novel generative Bayesian probabilistic model for unsupervised matrix and tensor factorization. The model consists of several interacting LDA models, one for each modality. We describe an efficient collapsed Gibbs sampler for inference. We also derive the non-parametric form of the model where interacting LDA models are replaced with interacting HDP models. Experiments demonstrate that the model is useful for prediction of missing data with two or more modalities as well as learning the latent structure in the data.

#index 1270335
#* Learning grasp strategies with partial shape information
#@ Ashutosh Saxena;Lawson L. S. Wong;Andrew Y. Ng
#t 2008
#c 10
#% 4615
#% 1034798
#% 1788321
#! We consider the problem of grasping novel objects in cluttered environments. If a full 3-d model of the scene were available, one could use the model to estimate the stability and robustness of different grasps (formalized as form/force-closure, etc); in practice, however, a robot facing a novel object will usually be able to perceive only the front (visible) faces of the object. In this paper, we propose an approach to grasping that estimates the stability of different grasps, given only noisy estimates of the shape of visible portions of an object, such as that obtained from a depth sensor. By combining this with a kinematic description of a robot arm and hand, our algorithm is able to compute a specific positioning of the robot's fingers so as to grasp an object. We test our algorithm on two robots (with very different arms/manipulators, including one with a multifingered hand). We report results on the task of grasping objects of significantly different shapes and appearances than ones in the training set, both in highly cluttered and in uncluttered environments. We also apply our algorithm to the problem of unloading items from a dishwasher.

#index 1270336
#* Incremental algorithms for approximate compilation
#@ Alberto Venturini;Gregory Provan
#t 2008
#c 10
#% 3460
#% 121397
#% 181220
#% 212157
#% 342378
#% 345703
#% 571628
#% 1272329
#% 1274834
#! Compilation is an important approach to a range of inference problems, since it enables linear-time inference in the size S of the compiled representation. However, the main drawback is that S can be exponentially larger than the size of the original function. To address this issue, we propose an incremental, approximate compilation technique that guarantees a sound and space-bounded compilation for weighted boolean functions, at the expense of query completeness. In particular, our approach selectively compiles all solutions exceeding a particular threshold, given a range of weighting functions, without having to perform inference over the full solution-space. We describe incremental, approximate algorithms for the prime implicant and DNNF compilation languages, and provide empirical evidence that these algorithms enable space reductions of several orders-of-magnitude over the full compilation, while losing relatively little query completeness.

#index 1270337
#* Computing reserve prices and identifying the value distribution in real-world auctions with market disruptions
#@ William E. Walsh;David C. Parkes;Tuomas Sandholm;Craig Boutilier
#t 2008
#c 10
#% 403035
#% 868464
#% 951946
#% 1274961

#index 1270338
#* Multi-label dimensionality reduction via dependence maximization
#@ Yin Zhang;Zhi-Hua Zhou
#t 2008
#c 10
#% 818234
#% 855563
#% 950571
#% 983907
#% 1673681
#! Multi-label learning deals with data associated with multiple labels simultaneously. Like other machine learning and data mining tasks, multi-label learning also suffers from the curse of dimensionality. Although dimensionality reduction has been studied for many years, multi-label dimensionality reduction remains almost untouched. In this paper, we propose a multi-label dimensionality reduction method, MDDM, which attempts to project the original data into a lower-dimensional feature space maximizing the dependence between the original feature description and the associated class labels. Based on the Hilbert-Schmidt Independence Criterion, we derive a closed-form solution which enables the dimensionality reduction process to be efficient. Experiments validate the performance of MDDM.

#index 1270339
#* Online learning in monkeys
#@ Xiaojin Zhu;Michael Coen;Shelley Prudom;Ricki Colman;Joseph Kemnitz
#t 2008
#c 10
#% 531952
#% 593737
#% 1759695
#! We examine online learning in the context of the Wisconsin Card Sorting Task (WCST), a task for which the concept acquisition strategies for human and other primates are well documented. We describe a new WCST experiment in rhesus monkeys, comparing the monkeys' behaviors to that of online learning algorithms. Our expectation is that insights gained from this work and future research can lead to improved artificial learning systems.

#index 1270340
#* Beyond classical planning: procedural control knowledge and preferences in state-of-the-art planners
#@ Jorge A. Baier;Christian Fritz;Meghyn Bienvenu;Sheila A McIlraith
#t 2008
#c 10
#% 296170
#% 342119
#% 495942
#% 1250631
#% 1269540
#% 1271962
#% 1272019
#% 1415583
#% 1696326
#% 1715422
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270341
#* Learning and inference with constraints
#@ Ming-Wei Chang;Lev Ratinov;Nicholas Rizzolo;Dan Roth
#t 2008
#c 10
#% 311027
#% 496282
#% 788050
#% 840927
#% 854636
#% 858036
#% 874707
#% 939377
#% 940036
#% 1008125
#% 1084594
#% 1289529
#% 1289530
#% 1344867
#% 1344874
#! Probabilistic modeling has been a dominant approach in Machine Learning research. As the field evolves, thc problems of interest become increasingly challenging and complex. Making complex decisions in real world problems often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate, what assignments are possible. However, incorporating nonlocal depcndencies in a probabilistic model can lead to intractable training and inference. This paper presents Constraints Conditional Models (CCMs), a framework that augments probabilistic models with declarative constraints as a way to support decisions in an expressive output space while maintaining modularity and tractability of training. We further show that declarative constraints can be used to take advantage of unlabeled data when training the probabilistic model.

#index 1270342
#* On-line planning and scheduling: an application to controlling modular printers
#@ Minh B. Do;Wheeler Ruml;Rong Zhou
#t 2008
#c 10
#% 107137
#% 290722
#% 495772
#% 1272008
#% 1272016
#% 1272116
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270343
#* Intelligent email: aiding users with AI
#@ Mark Dredze;Hanna M. Wallach;Danny Puller;Tova Brooks;Josh Carroll;Joshua Magarick;John Blitzer;Fernando Pereira
#t 2008
#c 10
#% 214751
#% 271083
#% 722904
#% 853885
#% 939727
#% 1065169
#% 1065186
#% 1250426
#% 1546444
#! Email occupies a central role in the modern workplace. This has led to a vast increase in the number of email messages that users are expected to handle daily. Furthermore, email is no longer simply a tool for asynchronous online communication-email is now used for task management, personal archiving, as well both synchronous and asynchronous online communication (Whittaker and Sidner 1996). This explosion can lead to .. email overload"-many users are overwhelmed by the large quantity of information in their mailboxes. In the human--computer interaction community, there has been much research on tackling email overload. Recently, similar efforts have emerged in the artificial intelligence (AI) and machine learning communities to form an area of research known as intelligent email. In this paper, we take a user-oriented approach to applying AI to email. We identify enhancements to email user interfaces and employ machine learning techniques to support these changes. We focus on three tasks-summary keyword generation, reply prediction and attachment prediction-and summarize recent work in these areas.

#index 1270344
#* Magic sets for data integration
#@ Wolfgang Faber;Gianluigi Greco;Nicola Leone
#t 2008
#c 10
#% 11797
#% 101623
#% 171033
#% 190336
#% 235018
#% 464915
#% 763752
#% 810106
#% 949371
#% 1279213
#% 1700138
#! We present a generalization of the Magic Sets technique to Datalog programs with (possibly unstratified) negation under the stable model semantics, originally defined in (Faber, Greco, & Leone 2005; 2007). The technique optimizes Datalog¬ programs by means of a rewriting algorithm that preserves query equivalence, under the proviso that the original program is consistent. The approach is motivated by recently proposed methods for query answering in data integration and inconsistent databases, which use cautious reasoning over consistent Datalog¬ programs under the stable model semantics. In order to prove the correctness of our Magic Sets transformation, we have introduced a novel notion of modularity for Datalog¬ under the stable model semantics, which is more suitable for query answering than previous module definitions, and which is also relevant per se. A module under this definition guarantees independent evaluation of queries if the full program is consistent. Otherwise, it guarantees soundness under cautious and completeness under brave reasoning.

#index 1270345
#* Decision-theoretic user interface generation
#@ Krzysztof Z. Gajos;Daniel S. Weld;Jacob O. Wobbrock
#t 2008
#c 10
#% 157150
#% 240805
#% 402090
#% 424038
#% 445768
#% 464274
#% 734961
#% 735119
#% 751789
#% 790448
#% 834611
#% 860137
#% 867346
#% 998824
#% 1047442
#% 1047444
#% 1048854
#% 1415710
#% 1721308

#index 1270346
#* Achieving master level play in 9×9 computer go
#@ Sylvain Gelly;David Silver
#t 2008
#c 10
#% 60140
#% 348582
#% 449561
#% 983838
#% 1274923
#% 1289220
#% 1404135
#% 1665148
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270347
#* Abduction with bounded treewidth: from theoretical tractability to practically efficient computation
#@ Georg Gottlob;Reinhard Pichler;Fang Wei
#t 2008
#c 10
#% 101944
#% 132173
#% 181220
#% 219474
#% 427161
#% 977000
#% 1250546
#% 1972413
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270348
#* Explicit-state abstraction: a new method for generating heuristic functions
#@ Malte Helmert;Patrik Haslum;Jörg Hoffmann
#t 2008
#c 10
#% 241
#% 743353
#% 816169
#% 1269717
#% 1269831
#% 1272048
#% 1741981
#! Many AI problems can be recast as finding an optimal path in a discrete state space. An abstraction defines an admissible heuristic function as the distances in a smaller state space where arbitrary sets of states are "aggregated" into single states. A special case are pattern database (PDB) heuristics, which aggregate states if they agree on the state variables inside the pattern. Explicit-state abstraction is more flexible, explicitly aggregating selected pairs of states in a process that interleaves composition of abstractions with abstraction of the composites. The increased flexibility gains expressive power: sometimes, the real cost function can be represented concisely as an explicit-state abstraction, but not as a PDB. Explicit-state abstraction has been applied to planning and model checking, with highly promiSing empirical results.

#index 1270349
#* Video activity recognition in the real world
#@ Anthony Hoogs;A. G. Amitha Perera
#t 2008
#c 10
#% 283197
#% 405391
#% 724287
#% 775535
#% 790610
#% 836831
#% 883805
#% 884061
#% 899541
#% 1014847
#% 1209045
#% 1393974
#% 1781566
#! With recent advances in motion detection and tracking in video, more efforts are being directed at higher-level video analysis such as recognizing actions, events and activities. One of the more challenging problems is recognizing activities that involve multiple people and/or vehicles, whose relationships change over time, when motion detection and tracking are unreliable, as commonly occurs in busy scenes. We describe an approach to this problem based on Dynamic Bayesian Networks, and show how DBNs can be extended to compensate for track failures. We also show that defining DBNs with semantic concepts improves robustness vs. direct observable, and discuss implications and ideas for incorporating semantic, symbolic knowledge into the perceptual domain of activity recognition.

#index 1270350
#* An analysis of transformational analogy: general framework and complexity
#@ Vithal Kuchibatla;Héctor Muñoz-Avila
#t 2008
#c 10
#% 89781
#% 194656
#% 490120
#% 490787
#% 1272367
#% 1706036
#! We present TransUCP, a formalism for Transformational Analogy in the context of classical domain-independent planning. TransUCP defines precisely possible plan modification operations for Transformational Analogy and covers a wide range of existing implementations. We use TransUCP to analyze the implications for Transformational Analogy of well-known results about the complexity of general plan adaptation.

#index 1270351
#* Efficient algorithms to solve Bayesian Stackelberg games for security applications
#@ Praveen Paruchuri;Jonathan P. Pearce;Janusz Marecki;Milind Tambe;Fernando Ordonez;Sarit Kraus
#t 2008
#c 10
#% 220808
#% 868454
#% 950869
#% 1024857
#% 1083973
#% 1084332
#% 1269437
#! In a class of games known as Stackelberg games, one agent (the leader) must commit to a strategy that can be observed by the other agent (the adversary/follower) before the adversary chooses its own strategy. We consider Bayesian Stackelberg games, in which the leader is uncertain about the type of the adversary it may face. Such games are important in security domains, where, for example, a security agent (leader) must commit to a strategy of patrolling certain areas, and an adversary (follower) can observe this strategy over time before choosing where to attack. We present here two different MIP-formulations, ASAP (providing approximate policies with controlled randomization) and DOBSS (providing optimal policies) for Bayesian Stackelberg games. DOBSS is currently the fastest optimal procedure for Bayesian Stackelberg games and is in use by police at the Los Angeles International Airport(LAX) to schedule their activities.

#index 1270352
#* Examining difficulties software developers encounter in the adoption of statistical machine learning
#@ Kayur Patel;James Fogarty;James A. Landay;Beverly Harrison
#t 2008
#c 10
#% 290482
#% 308589
#% 428235
#% 786393
#% 1047372
#! Statistical machine learning continues to show promise as a tool for addressing complex problems in a variety of domains. An increasing number of developers are therefore looking to use statistical machine learning algorithms within applications. We have conducted two initial studies examining the difficulties that developers encounter when creating a statistical machine learning component of a larger application. We first interviewed researchers with experience integrating statistical machine learning into applications. We then sought to directly observe and quantify some of the behavior described in our interviews using a laboratory study of developers attempting to build a simple application that uses statistical machine learning. This paper presents the difficulties we observed in our studies, discusses current challenges to developer adoption of statistical machine learning, and proposes potential approaches to better supporting developers creating statistical machine learning components of applications.

#index 1270353
#* Decompositions of grammar constraints
#@ Claude-Guy Quimper;Toby Walsh
#t 2008
#c 10
#% 345434
#% 465089
#% 866428
#% 911011
#% 1223221
#% 1399069
#% 1399099
#% 1664994
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270354
#* Make3D: depth perception from a single still image
#@ Ashutosh Saxena;Min Sun;Andrew Y. Ng
#t 2008
#c 10
#% 232125
#% 282188
#% 325689
#% 409857
#% 424094
#% 424120
#% 444003
#% 723949
#% 748636
#% 815973
#% 836753
#% 840909
#% 883982
#% 883993
#% 883994
#% 884157
#% 990284
#% 1022061
#% 1275107
#! Humans have an amazing ability to perceive depth from a single still image; however, it remains a challenging problem for current computer vision systems. In this paper, we will present algorithms for estimating depth from a single still image. There are numerous monocular cues--such as texture variations and gradients, defocus, color/haze, etc.--that can be used for depth perception. Taking a supervised learning approach to this problem, in which we begin by collecting a training set of single images and their corresponding ground-truth depths, we learn the mapping from image features to the depths. We then apply these ideas to create 3-d models that are visually-pleasing as well as quantitatively accurate from individual images. We also discuss applications of our depth perception algorithm in robotic navigation, in improving the performance of stereovision, and in creating large-scale 3-d models given only a small number of images.

#index 1270355
#* Using signals of human interest to enhance single-document summarization
#@ Krysta M. Svore;Lucy Vanderwende;Christopher J. C. Burges
#t 2008
#c 10
#% 449751
#% 818226
#% 840846
#% 879636
#% 943826
#% 1269588
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270356
#* Adaptive management of air traffic flow: a multiagent coordination approach
#@ Kagan Tumer;Adrian Agogino
#t 2008
#c 10
#% 384911
#% 750280
#% 890462
#% 1024931
#% 1083929
#! This paper summarizes recent advances in the application of multiagent coordination algorithms to air traffic flow management. Indeed, air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace). The multiagent approach assigns an agent to a navigational fix (a specific location in 2D space) and uses three separate actions to control the airspace: setting the separation between airplanes, setting ground holds that delay aircraft departures and rerouting aircraft. Agents then use reinforcement learning to learn the best set of actions. Results based on FACET (a commercial simulator) show that agents receiving personalized rewards reduce congestion by up to 80% over agents receiving a global reward and by up to 85% over a current industry approach (Monte Carlo estimation). These results show that with proper selection of agents, their actions and their reward structures, multiagent coordination algorithms can be successfully applied to complex real world domains.

#index 1270357
#* Breaking value symmetry
#@ Toby Walsh
#t 2008
#c 10
#% 349895
#% 497307
#% 535172
#% 873070
#% 1223221
#% 1250136
#% 1250525
#% 1279252
#% 1289398
#% 1399065
#% 1399088
#% 1399124
#% 1411204
#% 1665002
#% 1665009
#! Symmetry is an important factor in solving many constraint satisfaction problems. One common type of symmetry is when we have symmetric values. In a recent series of papers, we have studied methods to break value symmetries (Walsh 2006a; 2007). Our results identify computational limits on eliminating value symmetry. For instance, we prove that pruning all symmetric values is NP-hard in general. Nevertheless, experiments show that much value symmetry can be broken in practice. These results may be useful to researchers in planning, scheduling and other areas as value symmetry occurs in many different domains.

#index 1270358
#* An interaction-based approach to computational epidemiology
#@ Christopher L. Barrett;Stephen Eubank;Madhav V. Marathe
#t 2008
#c 10
#% 44876
#% 310835
#% 578708
#% 729923
#% 749520
#% 878940

#index 1270359
#* What is answer set programming?
#@ Vladimir Lifschitz
#t 2008
#c 10
#% 1146
#% 26351
#% 340738
#% 411814
#% 417651
#% 544776
#% 772065
#% 865743
#% 1021956
#% 1388119
#% 1656398
#! Answer set programming (ASP) is a form of declarative programming oriented towards difficult search problems. As an outgrowth of research on the use of nonmonotonic reasoning in knowledge representation, it is particularly useful in knowledge-intensive applications. ASP programs consist of rules that look like Prolog rules, but the computational mechanisms used in ASP are different: they are based on the ideas that have led to the creation of fast satisfiability solvers for propositional logic.

#index 1270360
#* Learning to connect language and perception
#@ Raymond J. Mooney
#t 2008
#c 10
#% 85153
#% 206437
#% 224755
#% 279755
#% 722803
#% 722927
#% 741071
#% 773136
#% 829159
#% 840922
#% 850002
#% 939364
#% 939615
#% 940046
#% 1000452
#% 1073888
#% 1250196
#% 1265076
#% 1269812
#% 1272355
#% 1344851
#% 1344863
#% 1476277
#! To truly understand language, an intelligent system must be able to connect words, phrases, and sentences to its perception of objects and events in the world. Current natural language processing and computer vision systems make extensive use of machine learning to acquire the probabilistic knowledge needed to comprehend linguistic and visual input. However, to date, there has been relatively little work on learning the relationships between the two modalities. In this talk, I will review some of the existing work on learning to connect language and perception, discuss important directions for future research in this area, and argue that the time is now ripe to make a concerted effort to address this important, integrative AI problem.

#index 1270361
#* Artificial intelligence needs open-access knowledgebase contents
#@ Erik Sandewall
#t 2008
#c 10

#index 1270362
#* Game theory pragmatics: a challenge for AI
#@ Yoav Shoham
#t 2008
#c 10
#% 68239
#% 176293
#% 282043
#% 1000506
#% 1065101
#% 1221188
#% 1269795
#! Game theory has been playing an increasingly visible role in computer science in general and AI in particular, most notably in the area of multi agent systems. I briefly list the areas where most of the action has been in the past decade or so. I then suggest that going forward, the most dramatic interaction between computer science and game theory - with a special role for AI - could be around what might be called game theory pragmatics.

#index 1270363
#* Intelligence in wikipedia
#@ Daniel S. Weld;Fei Wu;Eytan Adar;Saleema Amershi;James Fogarty;Raphael Hoffmann;Kayur Patel;Michael Skinner
#t 2008
#c 10
#% 240955
#% 249143
#% 342398
#% 452641
#% 464434
#% 466078
#% 830520
#% 936912
#% 939601
#% 998796
#% 1019061
#% 1022235
#% 1055735
#% 1083705
#% 1250362
#% 1269899
#% 1270345
#% 1272185
#% 1275012
#% 1275182
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270364
#* Using reasoning patterns to simplify games
#@ Dimitrios Antos;Avi Pfeffer
#t 2008
#c 10
#% 527993
#% 866687
#% 1289289
#! In complex strategic situations decision-making agents interact with many other agents and have access to many pieces of information throughout their play. This usually leads to game solving being a very complex, almost intractable procedure. Moreover, algorithms for solving games usually fail to explain how the various equilibria come about and how "plausible" they are. Reasoning patterns try to capture the strategic thinking of agents and formalize the usage of the various information or evidence they obtain during their interactions. Identifying reasoning patterns can lead to a significant refinement over the full range of equilibria, as well as considerable computational savings in solving the game. Here we present a polynomial-time algorithm that simplifies the original game by iteratively identifying noneffective (ignorable) decision nodes and removing redundant information edges. In some cases, this can lead to exponential-time savings in computing an equilibrium, yet some -potentially efficient-equilibria may be lost in the process.

#index 1270365
#* Lexical and grammatical inference
#@ Tom Armstrong;Tim Oates
#t 2008
#c 10
#% 278101
#% 466849
#% 629631
#% 1272315
#! Children are facile at both discovering word boundaries and using those words to build higher-level structures in tandem. Current research treats lexical acquisition and grammar induction as two distinct tasks; doing so has led to unreasonable assumptions. State-of-the-art unsupervised results presuppose a perfectly segmented, noise-free lexicon, while largely ignoring how the lexicon is used. This paper combines both tasks in a novel framework for bootstrapping lexical acquisition and grammar induction.

#index 1270366
#* The benefits of an ontological patient model in clinical decision-support
#@ Mark Austin;Matthew Kelly;Sir Michael Brady
#t 2008
#c 10
#% 769356
#! In this paper we discuss an application integrating an ontological data model with an argumentation-based decision-support system, showing how the combination of leading technologies OWL, SPARQL and Jena can be used to achieve this. In the context of improving a decision support tool that is currently being trialled live in a clinical environment, we describe quantitatively how the incorporation of an ontology leads to an improvement over the existing software, highlighting the benefits of incorporating an ontology in medical applications. Data and clinical feedback is being collected from a live trial at the John Radcliffe hospital in Oxford, where we are able to test the original decision support tool, but also the ontology driven version, and thus will be able to demonstrate that any quantitative improvements in the efficacy of the software are a product of the ontological data model alone.

#index 1270367
#* Using clustering methods for discovering event structures
#@ Cosmin Adrian Bejan;Sanda Harabagiu
#t 2008
#c 10
#% 575570
#% 722904

#index 1270368
#* Distributed reasoning with conflicts in a multi-context framework
#@ Antonis Bikakis;Grigoris Antoniou
#t 2008
#c 10
#% 1223259
#% 1274794
#% 1289440
#% 1673662

#index 1270369
#* Conformant planning heuristics based on plan reuse in belief states
#@ Dunbo Cai;Jigui Sun;Mingbao Yin
#t 2008
#c 10
#% 181627
#% 1272109
#% 1272113

#index 1270370
#* Personalized reasoner based on belief strengths of information sources
#@ Cai Shu-Bin;Ming Zhong;Li Shi-Xian
#t 2008
#c 10
#% 157425
#% 992960
#% 1300591
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270371
#* A neuro-fuzzy strategy for web personalization
#@ G. Castellano;A. M. Fanelli;P. Plantamura;M. A. Torsello
#t 2008
#c 10
#% 308766
#% 453320
#% 724979
#% 1345722
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270372
#* Sketch recognition based on manifold learning
#@ Heeyoul Choi;Tracy Hammond
#t 2008
#c 10
#% 109079
#% 297588
#% 939132
#! Current feature-based methods for sketch recognition systems rely on human-selected features. Certain machine learning techniques have been found to be good nonhnear features extractors. In this paper, we apply a manifold learning method, kernel Isomap, with a new algorithm for multi-stroke sketch recognition, which significantly outperforms the standard feature-based techniques.

#index 1270373
#* GLADDER: combining gesture and geometric sketch recognition
#@ Paul Corey;Tracy Hammond
#t 2008
#c 10
#% 109079
#% 251145
#% 259958
#% 1065149
#% 1297970
#! Sketch recognition systems usually recognize strokes either as stylistic gestures or geometric shapes. Both techniques have their advantages. This paper presents a method for integrating gesture-based and geometric recognition techniques, significantly outperforming either technique on its own.

#index 1270374
#* Distinguishing between sketched scribble look alikes
#@ Katie Dahmen;Tracy Hammond
#t 2008
#c 10
#% 109079
#% 259958
#% 740177
#! In hand-sketched drawings, nearly identical strokes may have different meanings to a user. For instance, a scribble could signify either that a shape should be filled in or that it should be deleted. This work describes a method for determining user intention in drawing scribbles in the context of a pen-based computer sketch. Our study shows that given two strokes, a circle and a scribble, two features (bounding ratio and density) can quickly and effectively determine a user's intention.

#index 1270375
#* Perpetual learning for non-cooperative multiple agents
#@ Luke Dickens
#t 2008
#c 10
#% 3084
#% 1272050
#! This paper examines, by argument, the dynamics of sequences of behavioural choices made, when non-cooperative restricted-memory agents learn in partially observable stochastic games. These sequences of combined agent strategies (joint-policies) can be thought of as a walk through the space of all possible joint-policies. We argue that this walk, while containing random elements, is also driven by each agent's drive to improve their current situation at each point, and posit a learning pressure field across policy space to represent this drive. Different learning choices may skew this learning pressure, and affect the simultaneous joint learning of multiple agents.

#index 1270376
#* User identification by means of sketched stroke features
#@ Brian David Eoff;Tracy Hammond
#t 2008
#c 10
#% 109079
#% 790481
#% 1768193
#! We present preliminary results of using physical features of a user's sketching style, such as pen tilt and pressure, to identify a user from their sketched strokes.

#index 1270377
#* Unsupervised categorization (filtering) of Google images based on visual consistency
#@ Pooyan Fazli;Ara Bedrosian
#t 2008
#c 10
#% 635689
#% 836717
#% 836904
#% 852098
#! The objective of this paper is to study the existing methods for unsupervised object recognition and image categorization and propose a model that can learn directly from the output of image search engines, e.g. Google Images, bypassing the need to manually collect large quantities of training data. This model can then be used to refine the quality of the image search, or to search through other sources of images. This integrated scheme has been implemented and optimized to be used in The Semantic Robot Vision Challenge as a new test-bed for research in the areas of image understanding and knowledge retrieval in large unstructured image databases.

#index 1270378
#* Existentially quantified values for queries and updates of facts in transaction logic programs
#@ Paul Fodor
#t 2008
#c 10
#% 169697
#% 248013
#% 384112
#! In several applications of logic programming and Transaction Logic, such as, planning, trust management and independent Semantic Web Services, an action might produce incomplete facts and leave existential values in an incrementally generated data structure. The same action or other producer or consumer actions might read, modify or communicate through these facts, making this technique a powerful communication technique. In this poster, we present a definite semantics for these existentially quantified values that occur only in facts, queries and updates of facts. Although this simple semantics applies only to facts and not to clauses, it is relevant to many applications, including artificial intelligence planning, workflow modeling and verification, and updates of facts in the Semantic Web.

#index 1270379
#* Querying sequential and concurrent horn transaction logic programs using tabling techniques
#@ Paul Fodor
#t 2008
#c 10
#% 8850
#% 116987
#% 169697
#% 248013
#% 769364
#! In this poster we describe the tabling techniques for Sequential and Concurrent Horn Transaction Logic. Horn Transaction Logic is an extension of classical logic programming with state updates and it has a SLD-style evaluation algorithm. This SLD-style algorithm enters into infinite loops when computing answers to many recursive programs when they change the underlying state of the knowledge base. We solve this problem by tabling (caching) the calls, call states and answers (unifications and return states) in a searchable structure for the Sequential Transaction Logic, or building a graph for the query and memoize the "hot" vertices (vertices, currently, possible to execute) for the Propositional Concurrent Transaction Logic, so that the same call is not re-executed ad infinum. With these techniques, we can efficiently compute queries to transaction logic programs, and when the underlying programs have the bounded term-depth property (Transaction Datalog) the techniques are guaranteed to terminate. The applications of these techniques promise termination and great improvements in the uses of transaction logic: state-changing systems, artificial intelligence planning, dynamic constraints on transaction execution, workflow modeling and verification, and systems involving financial transactions.

#index 1270380
#* Predicting appropriate semantic web terms from words
#@ Lushan Han;Tim Finin
#t 2008
#c 10
#% 174161
#% 376266
#% 783560
#% 791744
#% 1655394
#% 1696301
#! The Semantic Web language RDF was designed to unambiguously define and use ontologies to encode data and knowledge on the Web. Many people find it difficult, however, to write complex RDF statements and queries because doing so requires familiarity with the appropriate ontologies and the terms they define. We describe a system that suggests appropriate RDF terms given semantically related English words and general domain and context information. We use the Swoogle Semantic Web search engine to provide RDF term and namespace statistics, the WorldNet lexical ontology to find semantically related words, and a naïve Bayes classifier to suggest terms. A customized graph data structure of related namespaces is constructed from Swoogle's database to speed up the classifier model learning and prediction time.

#index 1270381
#* Improving a plan library for real-time systems using nearly orthogonal Latin hypercube sampling
#@ Robert Holder
#t 2008
#c 10
#% 302901
#% 872833
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270382
#* Text beautifier: an affective-text tool to tailor written text
#@ Fahim Kawsar;Mostafa Al Masum Shaikh;Mitsuru Ishizuka
#t 2008
#c 10
#! We have spelling and grammar checking tools available on today's word processors. But what they are missing is a tool that can recommend several possibilities of a given written sentence to assist a user to write better sentences. Therefore, we aim to develop a linguistic tool to beautify text by applying our developed lexical resources regarding textual affect sensing. The developed tool will allow a user to beautify an input sentence in terms of tuning it on different scales like valence, affect, prospect, and praise. For example using such a tool one may get the recommendations like, "Your lovely email makes me very glad", or "I become glad to read your email", or "I am very happy to obtain your nice email" for the input sentence "I am happy to receive your email" after scaling up the input sentence on affective, or prospective, or valence scale respectively. Such tool will be especially helpful to the non-native English speakers to write better English.

#index 1270383
#* Self-organizing multi-agent system for adaptive continuous unsupervised learning in complex uncertain environments
#@ Igor Kiselev;Reda Alhajj
#t 2008
#c 10
#% 1269961
#% 1673558

#index 1270384
#* Loop Calculus for satisfiability
#@ Lukas Kroc;Michael Chertkov
#t 2008
#c 10
#% 450290
#% 961282
#% 1014671
#% 1650318
#% 1815596
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270385
#* Constrained classification on structured data
#@ Chi-Hoon Lee;Matthew Brown;Russell Greiner;Shoajun Wang;Albert Murtha
#t 2008
#c 10
#% 1663639
#! Most standard learning algorithms, such as Logistic Regression (LR) and the Support Vector Machine (SVM), are designed to deal with i.i.d. (independent and identically distributed) data. They therefore do not work effectively for tasks that involve non-i.i.d. data, such as "region segmentation". (Eg, the "tumor vs non-tumor" labels in a medical image are correlated, in that adjacent pixels typically have the same label.) This has motivated the work in random fields, which has produced classifiers for such non-i.i.d. data that are significantly better than standard i.i.d.-based classifiers. However, these random field methods are often too slow to be trained for the tasks they were designed to solve. This paper presents a novel variant, Pseudo Conditional Random Fields (PCRFs), that is also based on i.i.d. learners, to allow efficient training but also incorporates correlations, like random fields. We demonstrate that this system is as accurate as other random fields variants, but significantly faster to train.

#index 1270386
#* Chatting activity recognition in social occasions using factorial conditional random fields with iterative classification
#@ Chia-Chun Lian;Jane Yung-Jen Hsu
#t 2008
#c 10
#% 464434
#% 828825
#% 863022
#% 961269
#% 1275039
#% 1719084

#index 1270387
#* Discover relevant environment feature using concurrent reinforcement learning
#@ Zhihui Luo;David Bell;Barry McCollum
#t 2008
#c 10
#% 1289475

#index 1270388
#* 2-Dimensional cellular automata approach for robot grid formations
#@ Ross Mead;Jerry B. Weinberg
#t 2008
#c 10
#% 312126
#% 766307
#% 1269368
#% 1768875
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270389
#* Hierarchical voting experts: an unsupervised algorithm for segmenting hierarchically structured sequences
#@ Matthew Miller;Alexander Stoytchev
#t 2008
#c 10
#% 1056091

#index 1270390
#* The swarm application framework
#@ Don Miner;Marie DesJardins;Peter Hamilton
#t 2008
#c 10
#% 31686
#% 258420
#% 329349
#! The Swarm Application Framework (SAF) is a tool that makes the development of swarm applications more intuitive. Traditionally, swarm applications are created by programming several low-level rules. This approach leads to several problems in designing and testing swarms, which serve as inspiration for the features of SAF. SAF encourages a new paradigm for designing swarm applications: engineers can interact with a swarm at the abstract (swarm) level instead of the individual (agent) level. In this paper, we discuss the design of the framework, how agents and rules in SAF operate, and a planned rule abstraction feature.

#index 1270391
#* ADROIT: automatic discourse relation organizer of internet-based text
#@ A. S. M. Mahbub Morshed;Mitsuru Ishizuka
#t 2008
#c 10
#% 9197
#% 177915
#% 352869
#% 729026
#% 741065
#% 816183
#! The ADROIT system that we are developing allows automatic discourse analysis of information rich natural language texts extracted directly from the web. We use guidelines and relations of Rhetorical Structure Theory (RST) to decompose texts into elementary segments and to perform the discourse parsing between them. In this paper, we present version 1.0 of ADROIT and focus on the noble technique of cue-phrase disambiguation and machine learning for identification and organization of discourse relations.

#index 1270392
#* NP-completeness of outcome optimization for partial CP-nets
#@ Keith Purrington;Edmund H. Durfee
#t 2008
#c 10
#% 1250233
#% 1272026

#index 1270393
#* Toward autonomous learning of an ontology of tool affordances by a robot
#@ Jivko Sinapov;Alexander Stoytchev
#t 2008
#c 10
#% 1002195

#index 1270394
#* The validity of providing automated hints in an ITS using a MDP
#@ John C. Stamper;Tiffany Barnes
#t 2008
#c 10
#% 1389393

#index 1270395
#* Using a geometric-based sketch recognition approach to sketch Chinese radicals
#@ Paul Taele;Tracy Hammond
#t 2008
#c 10
#% 734867
#% 740157
#% 1297970
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270396
#* Efficient haplotype inference with answer set programming
#@ Ferhan Türe;Esra Erdem
#t 2008
#c 10
#% 958596
#% 1250523
#% 1386504
#% 1396817
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270397
#* Eliminating false positives during corner finding by merging similar segments
#@ Aaron Wolin;Brandon Paulson;Tracy Hammond
#t 2008
#c 10
#% 740157
#% 782261
#% 989765
#% 1065149
#% 1295696
#! We present a new comer finding algorithm based on merging like stroke segmentations together in order to eliminate false positive comers. We compare our system to two benchmark corner finders with substantial improvements in both polyline and complex fits.

#index 1270398
#* Visualization of large-scale weighted clustered graph: a genetic approach
#@ Jiayu Zhou;Youfang Lin;Xi Wang
#t 2008
#c 10
#% 646815
#% 982284
#% 986678
#! In this paper, a bottom-up hierarchical genetic algorithm is proposed to visualize clustered data into a planar graph. To achieve global optimization by accelerating local optimization process, we introduce subgraph rotating and scaling processes into the genetic algorithm. Compared with existing methods, the proposed approach is more feasible and promising, with more accurate graph layout and more satisfiable computationally efficient performance, as demonstrated by the experimental results.

#index 1270399
#* The relational push-pull model: a generative model for relational data clustering
#@ Adam Anthony
#t 2008
#c 10
#% 844322
#% 937552
#% 1030845
#% 1289267

#index 1270400
#* Towards answer set prolog based architectures for intelligent agents
#@ Sandeep Chintabathina
#t 2008
#c 10
#% 333237
#% 411814
#% 417653
#% 752742
#% 763743

#index 1270401
#* Unstructured audio classification for environment recognition
#@ Selina Chu
#t 2008
#c 10
#% 137711
#% 704183
#% 775108
#% 780081
#% 1767442
#% 1767472
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270402
#* An architecture and formalism for handling modular ontologies
#@ Faezeh Ensan
#t 2008
#c 10
#% 251786
#% 763751
#% 1029723
#% 1269877
#% 1279341
#% 1412722
#! The goal of my ongoing work is to provide an architecture for developing and manipulating modular ontologies in such a way that each ontology module can plug into or unplug from an ontology. This architecture builds on top of a fundamental formalism for modular ontologies. Through this formalism we are able to define mechanisms for integrating different modules and develop algorithms for reasoning over the integrated modules. The resolution of inconsistencies arisen by conflicting axioms in different modules as well as the investigation of the impact of changes in a module on the other ontology modules are two important issues that need to be taken into consideration during the development of the formalism. Here, we briefty review the overall structure of the research work that I intended to conduct.

#index 1270403
#* Optimizations and extensions for the horn transaction logic programs
#@ Paul Fodor
#t 2008
#c 10
#% 8850
#% 116987
#% 169697
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270404
#* Social interaction under uncertainty in multi agent systems
#@ Noam Hazon
#t 2008
#c 10
#% 1083981
#% 1270021

#index 1270405
#* Tightly coupled cooperation among independent agents
#@ Daylond Hooper
#t 2008
#c 10
#% 471095
#% 1784677

#index 1270406
#* Autonomous robot skill acquisition
#@ George Konidaris
#t 2008
#c 10
#% 286423
#% 357083
#% 384911
#% 1274897

#index 1270407
#* Generating plans in concurrent, probabilistic, over-subscribed domains
#@ Li Li
#t 2008
#c 10
#% 194652
#% 1272092
#% 1272199
#% 1289543
#% 1650355
#! My thesis topic is plan generation in temporal, parallel, probabilistic domains with oversubscribed goals. I have defined a framework that includes two novel extensions. First, the plans can include parallel steps that serve the same goal and increase the probability of success in addition to parallel steps that serve different goals and decrease execution time. Second, already executing plan steps can be terminated if doing so saves resources, to achieve more goals. My algorithm called CPOAO (Concurrent, Probabilistic, Oversubscription AO) can deal with these extensions. In this paper, I summarize the design and implementation of CPOAO and its associated heuristics, and propose a plan of research.

#index 1270408
#* Adaptive abstraction of constraint-based models for self-diagnosis and planning
#@ Paul Maier
#t 2008
#c 10
#% 644201
#% 1269410

#index 1270409
#* Distributed robust execution of qualitative state plan with chance constraints
#@ Masahiro Ono
#t 2008
#c 10
#% 1270307

#index 1270410
#* Combining global relevance information with local contextual clues for event-oriented information extraction
#@ Siddharth Patwardhan
#t 2008
#c 10
#% 741058
#% 1288558
#! Existing IE systems tend to focus on a tight window of context surrounding the desired information to be extracted. This research addresses shortcomings of these systems by introducing a two-phase approach to IE that incorporates global relevance information with local contextual evidence, to effectively extract information from free text.

#index 1270411
#* Computational influence for training and entertainment
#@ David L. Roberts
#t 2008
#c 10
#% 705161
#% 890319
#% 1024875
#% 1084365
#% 1250348
#% 1269805

#index 1270412
#* Integrative construction and analysis of condition-specific biological networks
#@ Sushmita Roy;Terran Lane;Margaret Werner-Washburne
#t 2008
#c 10
#% 101221
#% 465762
#% 722753
#% 937080
#% 961197

#index 1270413
#* Managing quality of service with soft constraints
#@ Francesco Santini
#t 2008
#c 10
#% 340691
#% 757932
#% 880395
#% 943777
#% 1052901
#% 1059516
#% 1079018
#% 1412911
#% 1768383

#index 1270414
#* The benefits of an ontological patient model in clinical decision-support
#@ Mark Austin;Matthew Kelly;Sir Michael Brady
#t 2008
#c 10
#! We have developed an application, MDTSuite, designed to support complex group decision making under difficult conditions including time pressure, incomplete information, changing group members and ever expanding guidelines. Taking the colorectal cancer MDT (Multi-Disciplinary-Team) meetings at the Radcliffe Infirmary in Oxford as our test case, we have been trialling the software live in the hospital providing decision support for clinicians discussing real patients. MDTSuite is an application integrating an ontological data model with an argumentation-based decision-support system, showing how the combination of leading technologies OWL (McGuinness & Harmelen 2004), SPARQL (Perez, Arenas, & Gutierrez 2006) and Jena (Carroll et al. 2003) can be used to achieve this.

#index 1270415
#* A hybrid approach to domino portrait generation
#@ Hadrien Cambazard;John Horan;Eoin O'Mahony;Barry O'Sullivan
#t 2008
#c 10
#% 148021
#% 1411169
#% 1683809
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270416
#* A demonstration of the RADAR personal assistant
#@ Andrew Faulring;Brad Myers;Ken Mohnkern;Michael Freed
#t 2008
#c 10
#% 452634
#% 818214
#% 905347
#% 955064
#% 1137778
#% 1304849
#% 1532582
#% 1739292
#! Email clients were not designed to serve as a task management tools, but a high volume of task-relevant information in email leads many people to use email clients for this purpose. Such usage aggravates a user's experience of email overload and reduces productivity. Prior research systems have sought to address this problem by experimentally adding task management capabilities to email client software. RADAR (Reflective Agents with Distributed Adaptive Reasoning) takes a different approach in which a software agent acts like a trusted human assistant. Many RADAR components employ machine learning to improve their performance. Human participant studies showed a clear impact of learning on useI peIformance metrics.

#index 1270417
#* CogSketch
#@ Ken Forbus;Andrew Lovett;Kate Lockwood;Jon Wetzel;Camillia Matuk;Ben Jee;Jeffrey Usher
#t 2008
#c 10
#% 65345
#% 175380
#% 320833
#% 1289334
#! Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable ...

#index 1270418
#* Yoopick: a combinatorial sports prediction market
#@ Sharad Goel;David Pennock;Daniel Reeves;Cong Yu
#t 2008
#c 10
#% 431455
#% 946096
#% 963365
#% 1061612
#% 1407353
#! We describe Yoopick, a combinatorial sports prediction market that implements a flexible betting language, and in tum facilitates fine-grained probabilistic estimation of outcomes.

#index 1270419
#* Prometheus design tool
#@ Lin Padgham;John Thangarajah;Michael Winikoff
#t 2008
#c 10
#% 742993
#% 847922
#% 1345298
#% 1388108
#% 1415552
#! The Prometheus Design Tool (PDT) supports the structured design of intelligent agent systems. It supports the Prometheus methodology, but can also be used more generally. This paper outlines the tool and some of its many features.

#index 1270420
#* ARMOR security for Los Angeles international airport
#@ James Pita;Manish Jain;Fernando Ordóñez;Christopher Portway;Milind Tambe;Craig Western;Praveen Paruchuri;Sarit Kraus
#t 2008
#c 10
#% 868454
#% 1083973
#! Security at major locations of economic or political importance is a key concern around the world, particularly given the threat of terrorism. Limited security resources prevent full security coverage at all times, which allows adversaries to observe and exploit patterns in selective patrolling or monitoring, e.g. they can plan an attack avoiding existing patrols. Hence, randomized patrolling or monitoring is important, but randomization must provide distinct weights to different actions based on their complex costs and benefits. To this end, this demonstration showcases a promising transition of the latest in multi-agent algorithms into a deployed application. In particular, it exhibits a software assistant agent called ARMOR (Assistant for Randomized Monitoring over Routes) that casts this patrolling/monitoring problem as a Bayesian Stackelberg game, allowing the agent to appropriately weight the different actions in randomization, as well as uncertainty over adversary types. ARMOR combines two key features: (i) It uses the fastest known solver for Bayesian Stackelberg games called DOBSS, where the dominant mixed strategies enable randomization; (ii) Its mixed-initiative based interface allows users to occasionally adjust or override the automated schedule based on their local constraints. ARMOR has been successfully deployed since August 2007 at the Los Angeles International Airport (LAX) to randomize checkpoints on the roadways entering the airport and canine patrol routes within the airport terminals.

#index 1270421
#* Human-robot collaboration for remote surveillance
#@ Evan A. Sultanik;Ilya Braude;Peter Thai;Robert N. Lass;Duc N. Nguyen;Joseph B. Kopena;William C. Regli;Sean A. Lisse;Steven N. Furtwangler;Alan J. Vayda
#t 2008
#c 10
#! The demonstration presents an application of multiagent systems and wireless networking to remote robot-based surveillance.

#index 1270422
#* Knowledge-based spatial reasoning for scene generation from text descriptions
#@ Dan Tappan
#t 2008
#c 10
#% 180124
#% 370876
#% 549078
#% 748623
#% 788486
#! This system translates basic English descriptions of a wide range of objects in a simplistic zoo environment into plausible, three-dimensional, interactive visualizations of their positions, orientations, and dimensions. It combines a semantic network and contextually sensitive knowledge base as representations for explicit and implicit spatial knowledge, respectively. Its linguistic aspects address underspecification, vagueness, uncertainty, and context with respect to intrinsic, extrinsic, and deictic frames of spatial reference. The underlying, commonsense reasoning fomlalism is probability-based geometric fields that are solved through constraint satisfaction. The architecture serves as an extensible test-and-evaluation framework for a multitude of linguistic and artificial-intelligence investigations.

#index 1270423
#* IMT: a mixed-initiative data mapping and search toolkit
#@ Michael Zang;Adam Gray;Joe Kriege;Kalyan Moy Gupta;David W. Aha
#t 2008
#c 10
#% 572314
#% 1270168
#% 1727831
#! Interoperability requires the resolution of syntactic and semantic variations among system data models. To address this problem, we developed the Intelligent Mapping Toolkit (IMT), which employs a distributed multi-agent architecture to enable the mixed-initiative mapping of metadata and instances. This architecture includes a novel federation of service-encapsulated matching agents that leverage case-based reasoning methods. We recently used the IMT matching service to develop several domain-specific search applications in addition to the IMT mapping application.

#index 1476253
#* Proceedings of the thirteenth national conference on Artificial intelligence - Volume 2
#@ 
#t 1996
#c 10

#index 1476254
#* Estimating the absolute position of a mobile robot using position probability grids
#@ Wolfram Burgard;Dieter Fox;Daniel Hennig;Timo Schmidt
#t 1996
#c 10
#% 39654
#% 44876
#% 81795
#% 179928
#% 1290038
#! In order to re-use existing models of the environment mobile robots must be able to estimate their position and orientation in such models. Most of the existing methods for position estimation are based on special purpose sensors or aim at tracking the robot's position relative to the known starting point. This paper describes the position probability grid approach to estimating the robot's absolute position and orientation in a metric model of the environment. Our method is designed to work with standard sensors and is independent of any knowledge about the starting point. It is a Bayesian approach based on certainty grids. In each cell of such a grid we store the probability that this cell refers to the current position of the robot. These probabilities are obtained by integrating the likelihoods of sensor readings over time. Results described in this paper show that our technique is able to reliably estimate the position of a robot in complex environments. Our approach has proven to be robust with respect to inaccurate environmental models, noisy sensors, and ambiguous situations.

#index 1476255
#* Navigation for everyday life
#@ Daniel D. Fu;Kristian J. Hammond;Michael J. Swain
#t 1996
#c 10
#% 109402
#% 120270
#% 179928
#% 648731
#% 1275236
#% 1275287
#! Past work in navigation has worked toward the goal of producing an accurate map of the environment. While no one can deny the usefulness of such a map, the ideal of producing a complete map becomes unrealistic when an agent is faced with performing real tasks. And yet an agent accomplishing recurring tasks should navigate more efficiently as time goes by. We present a system which integrates navigation, planning, and vision. In this view, navigation supports the needs of a larger system as opposed to being a task in its own right. Whereas previous approaches assume an unknown and unstructured environment, we assume a structured environment whose organization is known, but whose specifics are unknown. The system is endowed with a wide range of visual capabilities as well as search plans for informed exploration of a simulated store constructed from real visual data. We demonstrate the agent finding items while mapping the world. In repeatedly retrieving items, the agent's performance improves as the learned map becomes more useful.

#index 1476256
#* Guaranteeing safety in spatially situated agents
#@ Robert C. Kohout;James A. Hendler;David J. Musliner
#t 1996
#c 10
#% 672
#% 6451
#% 132072
#% 181338
#% 319838
#% 1012401
#% 1068681
#% 1068780
#! "Mission-critical" systems, which include such diverse applications as nuclear power plant controllers, "fly-by-wire" airplanes, medical care and monitoring systems, and autonomous mobile vehicles, are characterized by the fact that system failure is potentially catastrophic. The high cost of failure justifies the expenditure of considerable effort at design-time in order to guarantee the correctness of system behavior. This paper examines the problem of guaranteeing safety in a well studied class of robot motion problems known as the "asteroid avoidance problem." We establish necessary and sufficient conditions for ensuring safety in the simple version of this problem which occurs most frequently in the literature, as well as sufficient conditions for a more general and realistic case. In doing so, we establish functional relationships between the number, size and speed of obstacles, the robot's maximum speed and the conditions which must be maintained in order to ensure safety.

#index 1476257
#* Recognizing and interpreting gestures on a mobile robot
#@ David Kortenkamp;Eric Huber;R. Peter Bonasso
#t 1996
#c 10
#% 592284
#% 634545
#! Gesture recognition is an important skill for robots that work closely with humans. Gestures help to clarify spoken commands and are a compact means of relaying geometric information. We have developed a real-time, three-dimensional gesture recognition system that resides on-board a mobile robot. Using a coarse three-dimensional model of a human to guide stereo measurements of body parts, the system is capable of recognizing six distinct gestures made by an unadorned human in an unaltered environment. An active vision approach focuses the vision system's attention on small, moving areas of space to allow for frame rate processing even when the person and/or the robot are moving. This paper describes the gesture recognition system, including the coarse model and the active vision approach. This paper also describes how the gesture recognition system is integrated with an intelligent control architecture to allow for complex gesture interpretation and complex robot action. Results from experiments with an actual mobile robot are given.

#index 1476258
#* Classifying and recovering from sensing failures in autonomous mobile robots
#@ Robin R. Murphy;David Hershberger
#t 1996
#c 10
#% 80052
#% 116336
#% 152532
#% 408302
#% 1275459
#! This paper presents a characterization of sensing failures in autonomous mobile robots, a methodology for classification and recovery, and a demonstration of this approach on a mobile robot performing landmark navigation. A sensing failure is any event leading to defective perception, including sensor malfunctions, software errors, environmental changes, and errant expectations. The approach demonstrated in this paper exploits the ability of the robot to interact with its environment to acquire additional information for classification (i.e., active perception). A Generate and Test strategy is used to generate hypotheses to explain the symptom resulting from the sensing failure. The recovery scheme replaces the affected sensing processes with an alternative logical sensor. The approach is implemented as the Sensor Fusion Effects Exception Handling (SFX-EH) architecture. The advantages of SFX-EN are that it requires only a partial causal model of sensing failure, the control scheme strives for a fast response, tests are constructed so as to prevent confounding from collaborating sensors which have also failed, and the logical sensor organization allows SFX-EH to be interfaced with the behavioral level of existing robot architectures.

#index 1476259
#* GARGOYLE: an environment for real-time, context-sensitive active vision
#@ Peter N. Prokopowicz;Michael J. Swain;R. James Firby;Roger E. Kahn
#t 1996
#c 10
#% 97855
#% 101437
#% 120270
#% 634603
#% 1275236
#! Researchers in robot vision have access to several excellent image processing packages (e.g., Khoros, Vista, Susan, MIL, and XVision to name only a few) as a base for any new vision software needed in most navigation and recognition tasks. Our work in automonous robot control and human-robot interaction, however, has demanded a new level of run-time flexibility and performance: on-the-fly configuration of visual routines that exploit up-to-the-second context from the task, image, and environment. The result is Gargoyle: an extendible, on-board, realtime vision software package that allows a robot to configure, parameterize, and execute imageprocessing pipelines at run-time. Each operator in a pipeline works at a level of resolution and over regions of interest that are computed by upstream operators or set by the robot according to task constraints. Pipeline configurations and operator parameters can be stored as a library of visual methods appropriate for different sensing tasks and environmental conditions. Beyond this, a robot may reason about the current task and environmental constraints to construct novel visual routines that are too specialized to work under general conditions, but that are well-suited to the immediate environment and task. We use the RAP reactive plan-execution system to select and configure pre-compiled processing pipelines, and to modify them for specific constraints determined at run-time.

#index 1476260
#* Robot navigation using image sequences
#@ Christopher Rasmussen;Gregory D. Hager
#t 1996
#c 10
#% 42251
#% 131471
#% 137675
#% 150481
#% 194022
#% 199610
#! We describe a framework for robot navigation that exploits the continuity of image sequences. Tracked visual features both guide the robot and provide predictive information about subsequent features to track. Our hypothesis is that image-based techniques will allow accurate motion without a precise geometric model of the world, while using predictive information will add speed and robustness.A basic component of our framework is called a scene, which is the set of image features stable over some segment of motion. When the scene changes, it is appended to a stored sequence. As the robot moves, correspondences and dissimilarities between current, remembered, and expected scenes provide cues to join and split scene sequences, forming a map-like directed graph. Visual servoing on features in successive scenes is used to traverse a path between robot and goal map locations.In our framework, a human guide serves as a scene recognition oracle during a map-learning phase; thereafter, assuming a known starting position, the robot can independently determine its location without general scene recognition ability. A prototype implementation of this framework uses as features color patches, sum-of-squared differences (SSD) subimages, or image projections of rectangles.

#index 1476261
#* Integrating grid-based and topological maps for mobile robot navigation
#@ Sebastian Thrun;Arno Bü
#t 1996
#c 10
#% 25470
#% 39654
#% 44876
#% 174161
#% 179928
#% 179995
#% 367254
#% 669503
#% 675625
#% 679671
#% 1290038
#! Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are considerably difficult to learn in large-scale environments. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms--grid-based and topological--, the approach presented here gains the best of both worlds: accuracy/consistency and efficiency. The paper gives results for autonomously operating a mobile robot equipped with sonar sensors in populated multi-room environments.

#index 1476262
#* Improving model-based diagnosis through algebraic analysis: the Petri net challenge
#@ Luigi Portinale
#t 1996
#c 10
#% 50730
#% 110365
#% 125529
#% 132173
#% 150840
#% 452776
#% 542523
#% 543804
#% 1273475
#! The present paper describes the empirical evaluation of a linear algebra approach to model-based diagnosis, in case the behavioral model of the device under examination is described through a Petri net model. In particular, we show that algebraic analysis based on Pinvariants of the net model, can significantly improve the performance of a model-based diagnostic system, while keeping the integrity of a general framework defined from a formal logical theory. A system called INVADS is described and experimental results, performed on a car fault domain and involving the comparison of different implementations of P-invariant based diagnosis, are then discussed.

#index 1476263
#* A model-based approach to blame assignment: revising the reasoning steps of problem solvers
#@ Eleni Stroulia;Ashok K. Goel
#t 1996
#c 10
#% 111539
#% 200191
#% 444999
#% 449525
#% 696239
#% 1305737
#! Blame assignment is a classical problem in learning and adaptation. Given a problem solver that fails to deliver the behaviors desired of it, the blame-assignment task has the goal of identifying the cause(s) of the failure. Broadly categorized, these causes can be knowledge faults (errors in the organization, content, and representation of the problem-solver's domain knowledge) or processing faults (errors in the content, and control of the problem-solving process). Much of AI research on blame assignment has focused on identifying knowledge and control-of-processing faults based on the trace of the failed problem-solving episode. In this paper, we describe a blame-assignment method for identifying content-of-processing faults, i.e., faults in the specification of the problem-solving operators. This method uses a structure-behavior-function (SBF) model of the problem-solving process, which captures the functional semantics of the overall task and the operators of the problem solver, the compositional semantics of its problem-solving methods that combine the operators' inferences into the outputs of the overall task, and the "causal" inter-dependencies between its tasks, methods and domain knowledge. We illustrate this model-based blame-assignment method with examples from AUTOGNOSTIC.

#index 1476264
#* Qualitative multiple-fault diagnosis of continuous dynamic systems using behavioral modes
#@ Siddarth Subramanian;Raymond J. Mooney
#t 1996
#c 10
#% 1116
#% 1117
#% 3460
#% 21137
#% 21138
#% 85224
#% 90702
#% 125575
#% 127547
#% 166232
#% 679423
#% 679759
#% 1273478
#% 1280088
#! Most model-based diagnosis systems, such as GDE and Sherlock, have concerned discrete, static systems such as logic circuits and use simple constraint propagation to detect inconsistencies. However, sophisticated systems such as QSIM and QPE have been developed for qualitative modeling and simulation of continuous dynamic systems. We present an integration of these two lines of research as implemented in a system called QDOCS for multiple-fault diagnosis of continuous dynamic systems using QSIM models. The main contributions of the algorithm include a method for propagating dependencies while solving a general constraint satisfaction problem and a method for verifying the consistency of a behavior with a model across time. Through systematic experiments on two realistic engineering systems, we demonstrate that QDOCS demonstrates a better balance of generality, accuracy, and efficiency than competing methods.

#index 1476265
#* A model-based approach to reactive self-configuring systems
#@ Brian C. Williams;P. Pandurang Nayak
#t 1996
#c 10
#% 21138
#% 114677
#% 163949
#% 182664
#% 517998
#% 1273478
#% 1279751
#! This paper describes Livingstone, an implemented kernel for a model-based reactive self-configuring autonomous system. It presents a formal characterization of Livingstone's representation formalism, and reports on our experience with the implementation in a variety of domains. Livingstone provides a reactive system that performs significant deduction in the sense/response loop by drawing on our past experience at building fast propositional conflict-based algorithms for model-based diagnosis, and by framing a model-based configuration manager as a propositional feedback controller that generates focused, optimal responses. Livingstone's representation formalism achieves broad coverage of hybrid hardware/software systems by coupling the transition system models underlying concurrent reactive languages with the qualitative representations developed in model-based reasoning. Livingstone automates a wide variety of tasks using a single model and a single core algorithm, thus making significant progress towards achieving a central goal of model-based reasoning. Livingstone, together with the HSTS planning and scheduling engine and the RAPS executive, has been selected as part of the core autonomy architecture for NASA's first New Millennium spacecraft.

#index 1476266
#* Trajectory constraints in qualitative simulation
#@ Giorgio Brajnik;Daniel J. Clancy
#t 1996
#c 10
#% 399
#% 1116
#% 101955
#% 107137
#% 166232
#% 618506
#% 679766
#% 1273470
#% 1290126
#! We present a method for specifying temporal constraints on trajectories of dynamical systems and enforcing them during qualitative simulation. This capability can be used to focus a simulation, simulate non-autonomous and piecewise-continuous systems, reason about boundary condition problems and incorporate observations into the simulation. The method has been implemented in TeQSIM, a qualitative simulator that combines the expressive power of qualitative differential equations with temporal logic. It interleaves temporal logic model checking with the simulation to constrain and refine the resulting predicted behaviors and to inject discontinuous changes into the simulation.

#index 1476267
#* A formal hybrid modeling scheme for handling discontinuities in physical system models
#@ Pieter J. Mosterman;Gautam Biswas
#t 1996
#c 10
#% 6200
#% 166220
#% 407731
#% 1796186
#! Physical systems are by nature continuous, but often exhibit nonhnearities that make behavior generation complex and hard to analyze. Complexity is often reduced by linearizing model constraints and by abstracting the time scale for behavior generation. In either case, the physical components are modeled to operate in multiple modes, with abrupt changes between modes. This paper discusses a hybrid modeling methodology and analysis algorithms that combine continuous energy flow modeling and localized discrete signal flow modeling to generate complex, multi-mode behavior in a consistent and correct manner. Energy phase space analysis is employed to demonstrate the correctness of the algorithm, and the reachability of a continuous mode.

#index 1476268
#* Building steady-state simulators via hierarchical feedback decomposition
#@ Nicolas F. Rouquette
#t 1996
#c 10
#% 152552
#% 175108
#% 215193
#% 375029
#% 1290129
#! In recent years, compositional modeling and self-explanatory simulation techniques have simplified the process of building dynamic simulators of physical systems. Building steady-state simulators is, conceptually, a simpler task consisting in solving a set algebraic equations. This simplicity hides delicate technical issues of convergence and search-space size due to the potentially large number of unknown parameters. We present an automated technique for reducing the dimensionality of the problem by 1) automatically identifying feedback loops (a generally NP-complete problem), 2) hierarchically decomposing the set of equations in terms of feedback loops, and 3) structuring a simulator where equations are solved either serially without search or in isolation within a feedback loop. This paper describes the key algorithms and the results of their implementation on building simulators for a two-phase evaporator loop system across multiple combinations of causal and non-causal approximations.

#index 1476269
#* Managing occurrence branching in qualitative simulation
#@ Lance Tokuda
#t 1996
#c 10
#% 166232
#! Qualitative simulators can produce common sense abstractions of complex behaviors given only partial knowledge about a system. One of the problems which limits the applicability of qualitative simulators is the intractable branching of successor states encountered with model of even modest size. Some branches may be unavoidable due to the complex nature of a system. Other branches may be accidental results of the model chosen. A common source of intractability is occurrence branching. Occurrence branching occurs when the state transitions of two variables are unordered with respect to each other. This paper extends the QSIM model to distinguish between interesting occurrence branching and uninteresting occurrence branching. A representation, algorithm, and simulator for efficiently handling uninteresting branching is presented.

#index 1476270
#* Diagrammatic reasoning and cases
#@ Michael Anderson;Robert McCartney
#t 1996
#c 10
#% 114216
#% 120806
#% 168280
#% 283641
#% 380335
#% 426494
#% 444999
#% 668908
#% 1275340
#% 1279718
#! We believe that many problem domains that lend themselves to a case-based reasoning solution can benefit from an diagrammatic implementation and propose a diagrammatic case-based solution to what we term the n-queens best solution problem where the best solution is defined as that which solves the probfem moving the fewest queens. A working system, based on a novel combination of diagrammatic and case-based reasoning, is described.

#index 1476271
#* Augmenting the diagnostic power of flow-based approaches to functional reasoning
#@ Luca Chittaro;Roberto Ranon
#t 1996
#c 10
#% 21138
#% 125555
#% 206965
#! In this paper, we consider flow-based approaches to functional diagnosis. First, we contrast the existing approaches, pointing out the major limitations of each. Then, we choose one of them and extend it in order to overcome the identified limitations. Finally, we show how the proposed extension can be introduced into the other flow-based approaches.

#index 1476272
#* A qualitative model of physical fields
#@ Monika Lundell
#t 1996
#c 10
#% 1116
#% 47469
#% 166232
#% 546590
#% 1274583
#! A qualitative model of the spatio-temporal behaviour of distributed parameter systems based on physical fields is presented. Field-based models differ from the object-based models normally used in qualitative physics by treating parameters as continuous entities instead of as attributes of discrete objects. This is especially suitable for natural physical systems, e.g. in ecology. The model is divided into a static and a dynamic part. The static model describes the distribution of each parameter as a qualitative physical field. Composite fields are constructed from intersection models of pairs of fields. The dynamic model describes processes acting on the fields, and qualitative relationships between parameters. Spatio-temporal behaviour is modelIed by interacting temporal processes, influencing single points in space, and spatial processes that gradually spread temporal processes over space. We give an example of a qualitative model of a natural physical system and discuss the ambiguities that arise during simulation.

#index 1476273
#* Generating multiple new designs from a sketch
#@ Thomas F. Stahovich;Randall Davis;Howard Shrobe
#t 1996
#c 10
#% 77823
#% 132171
#% 196612
#% 224194
#% 668966
#% 669388
#% 669472
#! We describe a program called SKETCHIT that transforms a single sketch of a mechanical device into multiple families of new designs. It represents each of these families with a "BEP-Model," a parametric model augmented with constraints that ensure the device produces the desired behavior. The program is based on qualitative configuration space (qc-space), a novel representation that captures mechanical behavior while abstracting away its implementation. The program employs a paradigm of abstraction and resynthesis: it abstracts the initial sketch into qc-space then maps from qc-space to new implementations.

#index 1476274
#* Tree-bank grammars
#@ Eugene Charniak
#t 1996
#c 10
#% 646955
#% 646980
#% 740916
#% 748476
#% 748561
#% 748810
#! By a "tree-bank grammar" we mean a context-free grammar created by reading the production rules directly from hand-parsed sentences in a tree bank. Common wisdom has it that such grammars do not perform we & though we know of no published data on the issue. The primary purpose of this paper is to show that the common wisdom is wrong. In particular, we present results on a tree-bank grammar based on the Penn WaII Street Journal tree bank. To the best of our knowledge, this grammar outperforms ah other non-word-based statistical parsers/grammars on this corpus. That is, it outperforms parsers that consider the input as a string of tags and ignore the actual words of the corpus.

#index 1476275
#* Left-corner unification-based natural language processing
#@ Steven L. Lytinen;Noriko Tomuro
#t 1996
#c 10
#% 59280
#% 121996
#% 322318
#% 367853
#% 372552
#% 374794
#% 740934
#% 748222
#% 748621
#% 748680
#! In this paper, we present an efficient algorithm for parsing natural language using unification grammars. The algorithm is an extension of left-corner parsing, a bottom-up algorithm which utilizes top-down expectations. The extension exploits unification grammar's uniform representation of syntactic, semantic, and domain knowledge, by incorporating all types of grammatical knowledge into parser expectations. In particular, we extend the notion of the reochcsbility table, which provides information as to whether or not a top-down expectation can be realized by a potential subconstituent, by including all types of grammatical information in table entries, rather than just phrase structure information. While our algorithm's worstcase computational complexity is no better than that of many other algorithms, we present empirical testing in which average-case linear time performance is achieved. Our testing indicates this to be much improved average-case performance over previous leftcorner techniques.

#index 1476276
#* Automatically generating extraction patterns from untagged text
#@ Ellen Riloff
#t 1996
#c 10
#% 179873
#% 217064
#% 496886
#% 740916
#% 1290067
#! Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting AutoSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUG-4 terrorism domain, AutoSlog-TS created a dictionary of extraction patterns that performed comparably to a dictionary created by AutoSlog, using only preclassified texts as input.

#index 1476277
#* Learning to parse database queries using inductive logic programming
#@ John M. Zelle;Raymond J. Mooney
#t 1996
#c 10
#% 173758
#% 179803
#% 179869
#% 215697
#% 443858
#% 496740
#% 741212
#% 748446
#% 748476
#% 748579
#% 748585
#% 748810
#% 748817
#! This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application.

#index 1476278
#* Hunter-Gatherer: three search techniques integrated for natural language semantics
#@ Stephen Beale;Sergei Nirenburg;Kavi Mahesh
#t 1996
#c 10
#% 1145
#% 3463
#% 131482
#% 320265
#% 406437
#% 748662
#! This work integrates three related AI search techniques - constraint satisfaction, branch-and-bound and solution synthesis - and applies the result to semantic processing in natural language (NL). We summarize the approach as "Hunter-Gatherer:" • branch-and-bound and constraint satisfaction allow us to "hunt down" non-optimal and impossible solutions and prune them from the search space. • solution synthesis methods then "gather" all optimal solutions avoiding exponential complexity. Each of the three techniques is briefly described, as well as their extensions and combinations used in our system. We focus on the combination of solution synthesis and branch-and-bound methods which has enabled near-linear-time processing in our applications. Finally, we illustrate how the use of our technique in a large-scale MT project allowed a drastic reduction in search space.

#index 1476279
#* Semantic interpretation of nominalizations
#@ Richard D. Hull;Fernando Gomez
#t 1996
#c 10
#% 114701
#% 131320
#% 144031
#% 145392
#% 145394
#% 748188
#% 748314
#% 1290074
#! A computational approach to the semantic interpretation of nominalizations is described. Interpretation of nominalizations involves three tasks: deciding whether the nominalization is being used in a verbal or non-verbal sense; disambiguating the nominalized verb when a verbal sense is used; and determining the fillers of the thematic roles of the verbal concept or predicate of the nominalization. A verbal sense can be recognized by the presence of modifiers that represent the arguments of the verbal concept. It is these same modifiers which provide the semantic clues to disambiguate the nominalized verb. In the absence of explicit modifiers, heuristics are used to discriminate between verbal and nonverbal senses. A correspondence between verbs and their nominalizations is exploited so that only a small amount of additional knowledge is needed to handle the nominal form. These methods are tested in the domain of encyclopedic texts and the results are shown.

#index 1476280
#* Building up rhetorical structure trees
#@ Daniel Marcu
#t 1996
#c 10
#! I use the distinction between the nuclei and the satellites that pertain to discourse relations to introduce a compositionality criterion for discourse trees. I provide a first-order formalization of rhetorical structure trees and, on its basis, I derive an algorithm that constructs all the valid rhetorical trees that can be associated with a given discourse.

#index 1476281
#* Using plan reasoning in the generation of plan descriptions
#@ R. Michael Young
#t 1996
#c 10
#% 76415
#% 174685
#% 198948
#% 217068
#% 1272550
#% 1288567
#! Previous work on the generation of natural language descriptions of complex activities has indicated that the unwieldy amount of text needed to describe complete plans makes for ineffective and unnatural descriptions. We argue here that concise and effective text descriptions of plans can be generated by exploiting a model of the hearer's plan reasoning capabilities. We define a computational model of the hearer's interpretation process that views the interpretation of plan descriptions as refinement search through a space of partial plans. This model takes into account the hearer's plan preferences and the resource limitations on her reasoning capabilities to determine the completed plans she will construct from a given partial description.

#index 1476282
#* Interfacing sound stream segregation to automatic speech recognition: preliminary results on listening to several sounds simultaneously
#@ Hiroshi G. Okuno;Tomohiro Nakatani;Takeshi Kawabata
#t 1996
#c 10
#% 173497
#% 257158
#% 968594
#% 1275248
#! This paper reports the preliminary results of experiments on listening to several sounds at once. Two issues are addressed: segregating speech streams from a mixture of sounds, and interfacing speech stream segregation with automatic speech recognition (ASR). Speech stream segregation (SSS) is modeled as a process of extracting harmonic fragments, grouping these extracted harmonic fragments, and substituting some sounds for non-harmonic parts of groups. This system is implemented by extending the harmonic-based stream segregation system reported at AAAI-94 and IJCAI-95. The main problem in interfacing SSS with HMM-based ASR is how to improve the recognition performance which is degraded by spectral distortion of segregated sounds caused mainly by the binaural input, grouping, and residue substitution. Our solution is to re-train the parameters of the HMM with training data binauralized for four directions, to group harmonic fragments according to their directions, and to substitute the residue of harmonic fragments for non-harmonic parts of each group. Experiments with 500 mixtures of two women's utterances of a word showed that the cumulative accuracy of word recognition up to the 10th candidate of each woman's utterance is, on average, 75%.

#index 1476283
#* Motion and color analysis for animat perception
#@ Tamer F. Rabie;Demetri Terzopoulos
#t 1996
#c 10
#% 3169
#% 25442
#% 101437
#% 120270
#% 156777
#% 198367
#% 564183
#% 625110
#% 625167
#% 1275357
#% 1310685
#! We propose novel gaze control algorithms for active perception in mobile autonomous agents with directable, foveated vision sensors. Our agents are realistic artificial animals, or animats, situated in physics-based virtual worlds. Their active perception systems continuously analyze photorealistic retinal image streams to glean information useful for controlling the animal's eyes and body. The vision system computes optical flow and segments moving targets in the low-resolution visual periphery. It then matches segmented targets against mental models of colored objects of interest. The eyes saccade to increase acuity by foveating objects. The resulting sensorimotor control loop supports complex behaviors, such as predation.

#index 1476284
#* Noise and the common sense informatic situation for a mobile robot
#@ Murray Shanahan
#t 1996
#c 10
#% 21137
#% 73518
#% 138037
#% 181340
#% 197283
#% 198462
#% 236024
#% 1273435
#% 1275246
#% 1279751
#% 1290146
#! Any model of the world a robot constructs on the basis of its sensor data is necessarily both incomplete, due to the robot's limited window on the world, and uncertain, due to sensor and motor noise. This paper supplies a logical account of sensor data assimilation in which such models are constructed through an abductive process which hypothesises the existence, locations, and shapes of objects. Noise is treated as a kind of non-determinism, and is dealt with by a consistency-based form of abduction.

#index 1476285
#* A hybrid learning approach for better recognition of visual objects
#@ Ibrahim F. Imam;Srinivas Gutta
#t 1996
#c 10
#% 96699
#% 117653
#% 162225
#% 164038
#% 168855
#! Real world images often contain similar objects but with different rotations, noise, or other visual alterations. Vision systems should be able to recognize objects regardless of these visual alterations. This paper presents a novel approach for learning optimized structures of classifiers for recognizing visual objects regardless of certain types of visual alterations. The approach consists of two phases. The first phase is concerned with learning classifications of a set of standard and altered objects. The second phase is concerned with discovering an optimized structure of classifiers for recognizing objects from unseen images. This paper presents an application of this approach to a domain of 15 classes of hand gestures. The experimental results show significant improvement in the recognition rate rather than using a single classifier or multiple classifiers with thresholds.

#index 1476286
#* Using elimination methods to compute thermophysical algebraic invariants from infrared imagery
#@ J. D. Michel;N. Nandhakumar;Tushar Saxena;Deepak Kapur
#t 1996
#c 10
#% 109393
#% 137661
#% 634537
#! We describe a new approach for computing invariant features in infrared (IR) images. Our approach is unique in the field since it considers not just surface reflection and surface geometry in the specification of invariant features, but it also takes into account internal object composition and thermal state which affect images sensed in the non-visible spectrum. We first establish a non-linear energy balance equation using the principle of conservation of energy at the surface of the imaged object. We then derive features that depend only on material parameters of the object and the sensed radiosity. These features are independent of the scene conditions and the scene-to-scene transformation of the "driving conditions" such as ambient temperature, and wind speed. The algorithm for deriving the invariant features is based on the algebraic elimination of the transformation parameters from the non-linear relationships. The elimination approach is a general method based on the extended Dixon resultant. Results on real IR imagery are shown to illustrate the performance of the features derived in this manner when used for an object recognition system that deals with multiple classes of objects.

#index 1476287
#* Approximate world models: incorporating qualitative and linguistic information into vision systems
#@ Claudio S. Pinhanez;Aaron F. Bobick
#t 1996
#c 10
#% 399
#% 109402
#% 117660
#% 165420
#% 697663
#! Approximate world models are coarse descriptions of the elements of a scene, and are intended to be used in the selection and control of vision routines in a vision system. In this paper we present a control architecture in which the approximate models represent the complex relationships among the objects in the world, allowing the vision routines to be situation or context specific. Moreover, because of their reduced accuracy requirements, approximate world models can employ qualitative information such as those provided by linguistic descriptions of the scene. The concept is demonstrated in the development of automatic cameras for a TV studio - SmartCams. Results are shown where SmartCams use vision processing of real imagery and information written in the script of a TV show to achieve TV-quality framing.

#index 1476288
#* Integrating visual information across camera movements with a visual-motor calibration map
#@ Peter N. Prokopowicz;Paul R. Cooper
#t 1996
#c 10
#% 90709
#% 96663
#% 199909
#% 457375
#! Facing the competing demands for wider field of view and higher spatial resolution, computer vision will evolve toward greater use of foveal sensors and frequent camera movements. Integration of visual information across movements becomes a fundamental problem. We show that integration is possible using a biologically-inspired representation we call the visual-motor calibration map. The map is a memory-based model of the relationship between camera movements and corresponding pixel locations before and after any movement. The map constitutes a self-calibration that can compensate for non-uniform sampling, lens distortion, mechanical misalignments, and arbitrary pixel reordering. Integration takes place entirely in a retinotopic frame, using a short-term, predictive visual memory.

#index 1476289
#* A bias towards relevance: recognizing plans where goal minimization fails
#@ Abigail S. Gertner;Bonnie L. Webber
#t 1996
#c 10
#% 147680
#% 174530
#% 195011
#% 214745
#! Domains such as multiple trauma management, in which there are multiple interacting goals that change over time, are ones in which plan recognition's standard inductive bias towards a single explanatory goal is inappropriate. In this paper we define and argue for an alternative bias based on identifying contextually "relevant" goals. We support this claim by showing how a complementary planning system in Traum AID 2.0, a decision-support system for the management of multiple trauma, allows us to define a four-level scale of relevance and therefore, of measurable deviations from relevance. This in turn allows definition of a bias towards relevance in the incremental recognition of physician plans by Traum-AID's critiquing interface, TraumaTIQ.

#index 1476290
#* What is planning in the presence of sensing?
#@ Hector J. Levesque
#t 1996
#c 10
#% 35335
#% 117869
#% 124592
#% 124601
#% 124604
#% 670210
#% 1290146
#% 1290155
#! Despite the existence of programs that are able to generate so-called conditional plans, there has yet to emerge a clear and general specification of what it is these programs are looking for: what exactly is a plan in this setting, and when is it correct? In this paper, we develop and motivate a specification within the situation calculus of conditional and iterative plans over domains that include binary sensing actions. The account is built on an existing theory of action which includes a solution to the frame problem, and an extension to it that handles sensing actions and the effect they have on the knowledge of a robot. Plans are taken to be programs in a new simple robot program language, and the planning task is to find a program that would be known by the robot at the outset to lead to a final situation where the goal is satisfied. This specification is used to analyze the correctness of a small example plan, as well as variants that have redundant or missing sensing actions. We also investigate whether the proposed robot program language is powerful enough to serve for any intuitively achievable goal.

#index 1476291
#* Opportunity recognition in complex environments
#@ Louise Pryor
#t 1996
#c 10
#% 92706
#% 156190
#% 168422
#% 1273346
#! An agent operating in an unpredictable world must be able to take advantage of opportunities but cannot afford to perform a detailed analysis of the effects of every nuance of the current situation on its goals if it is to respond in a timely manner. This paper describes a filtering mechanism that enables the effective recognition of opportunities. The mechanism is based on a characterization of the world in terms of reference features, features that are both cheap and functional and that appear to be prevalent in everyday life. Its use enables the plan execution system PARETO to recognize types of opportunities that other systems cannot. Reference features can also play a rôle in the detection of threats, and may be involved in the development of expertise.

#index 1476292
#* Generalizing indexical-functional reference
#@ Marcel Schoppers;Richard Shu
#t 1996
#c 10
#% 179941
#% 181620
#% 669402
#% 1272691
#% 1273463
#! The goals of situated agents generally do not specify particular objects: they require only that some suitable object should be chosen and manipulated (e.g. any red block). Situated agents engaged in deictic reference grounding, however, may well track a chosen referent object with such fixity of purpose that an unchosen object may be regarded as an obstacle even though it satisfies the agent's goals. In earlier work this problem was bridged by hand-coding. This paper lifts the problem to the symbol level, endowing agents with perceptual referent selection actions and performing those actions as required to allow or disallow opportunistic re-selection of referents. Our work preserves the ability of situated agents to find and track specific objects, adds an ability to automatically exploit the opportunities allowed by nonspecific references, and provides a starting point for studying how much opportunistic perception is appropriate.

#index 1476293
#* Rewarding behaviors
#@ Fahiem Bacchus;Craig Boutilier;Adam Grove
#t 1996
#c 10
#% 75936
#% 100138
#% 101955
#% 179939
#% 179955
#% 179957
#% 363744
#% 644560
#% 1275454
#% 1290040
#% 1290041
#% 1650672
#! Markov decision processes (MDPs) are a very popular tool for decision theoretic planning (DTP), partly because of the welldeveloped, expressive theory that includes effective solution techniques. But the Markov assumption--that dynamics and rewards depend on the current state only, and not on history-- is often inappropriate. This is especially true of rewards: we frequently wish to associate rewards with behaviors that extend over time. Of course, such reward processes can be encoded in an MDP should we have a rich enough state space (where states encode enough history). However it is often difficult to "hand craft" suitable state spaces that encode an appropriate amount of history. We consider this problem in the case where non-Markovian rewards are encoded by assigning values to formulas of a temporal logic. These formulas characterize the value of temporally extended behaviors. We argue that this allows a natural representation of many commonly encountered non-Markovian rewards. The main result is an algorithm which, given a decision process with non-Markovian rewards expressed in this manner, automatically constructs an equivalent MDP (with Markovian reward structure), allowing optimal policy construction using standard techniques.

#index 1476294
#* Computing optimal policies for partially observable decision processes using compact representations
#@ Craig Boutilier;David Poole
#t 1996
#c 10
#% 44876
#% 75936
#% 92301
#% 103309
#% 124691
#% 147677
#% 179940
#% 644560
#% 646479
#% 1290039
#% 1290041
#% 1650679
#! Partially-observable Markov decision processes provide a general model for decision theoretic planning problems, allowing trade-offs between various courses of actions to be determined under conditions of uncertainty, and incorporating partial observations made by an agent. Dynamic programming algorithms based on the belief state of an agent can be used to construct optimal policies without explicit consideration of past history, but at high computational cost. In this paper, we discuss how structured representations of system dynamics can be incorporated in classic POMDP solution algorithms. We use Bayesian networks with structured conditional probability matrices to represent POMDPs, and use this model to structure the belief space for POMDP algorithms, allowing irrelevant distinctions to be ignored. Apart from speeding up optimal policy construction, we suggest that such representations can be exploited in the development of useful approximation methods.

#index 1476295
#* A qualitative model for temporal reasoning with incomplete information
#@ Hector Geffner
#t 1996
#c 10
#% 3035
#% 44876
#% 75936
#% 100159
#% 103309
#% 107123
#% 159243
#% 459577
#% 527528
#% 1273638
#! We develop a qualitative framework for temporal reasoning with incomplete information that features a modeling language based on rules and a semantics based on infinitesimal probabilities. The framework relates logical and probabilistical models, and accommodates in a natural way features that have been found problematic in other models like nondeterminism, action qualifications, parallel actions, and abduction to actions and fluents.

#index 1476296
#* On the size of reactive plans
#@ Peter Jonsson;Christer Bäckström
#t 1996
#c 10
#% 62653
#% 62654
#% 62655
#% 101922
#% 167629
#% 179991
#% 194646
#% 408396
#% 1275454
#% 1373494
#! One of the most widespread approaches to reactive planning is Schoppers' universal plans. We propose a stricter definition of universal plans which guarantees a weak notion of soundness not present in the original definition. Furthermore, we isolate three different types of completeness which capture different behaviours exhibited by universal plans. We show that universal plans which run in polynomial time and are of polynomial size cannot satisfy even the weakest type of completeness unless the polynomial hierarchy collapses. However, by relaxing either the polynomial time or the polynomial space requirement, the construction of universal plans satisfying the strongest type of completeness becomes trivial.

#index 1476297
#* Is "early commitment" in plan generation ever a good idea?
#@ David Joslin;Martha E. Pollack
#t 1996
#c 10
#% 109935
#% 126397
#% 131357
#% 167631
#% 179935
#% 179938
#% 194651
#% 678135
#% 704599
#! Partial-Order Causal Link planners typically take a "least-commitment" approach to some decisions (notably, step ordering), postponing those decisions until constraints force them to be made. However, these planners rely to some degree on early commitments in making other types of decisions, including threat resolution and operator choice. We show why existing planners cannot support full least-commitment decisionmaking, and present an alternative approach that can. The approach has been implemented in the Descartes system, which we describe. We also provide experimental results that demonstrate that a least-commitment approach to planning can be profitably extended beyond what is done in POCL and similar planners, but that taking a least-commitment approach to every planning decision can be inefficient: early commitment in plan generation is sometimes a good idea.

#index 1476298
#* Pushing the envelope: planning, propositional logic, and stochastic search
#@ Henry Kautz;Bart Selman
#t 1996
#c 10
#% 126390
#% 131357
#% 132174
#% 154075
#% 159113
#% 160270
#% 163715
#% 179960
#% 327779
#% 647206
#% 1272468
#% 1274084
#% 1275356
#% 1279705
#% 1290109
#! Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very effective on a wide range of scheduling problems, this is the first demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.

#index 1476299
#* Finding optimal solutions to the twenty-four puzzle
#@ Richard E. Korf;Larry A. Taylor
#t 1996
#c 10
#% 2194
#% 25998
#% 144817
#% 160388
#% 189701
#! We have found the first optimal solutions to random instances of the Twenty-Four Puzzle, the 5 × 5 version of the well-known sliding-tile puzzles. Our new contribution to this problem is a more powerful admissible heuristic function. We present a general theory for the automatic discovery of such heuristics, which is based on considering multiple subgoals simultaneously. In addition, we apply a technique for pruning duplicate nodes in depth-first search using a finitestate machine. Finally, we observe that as heuristic search problems are scaled up, more powerful heuristic functions become both necessary and cost-effective.

#index 1476300
#* Linear time near-optimal planning in the blocks world
#@ John Slaney;Sylvie Thiébaux
#t 1996
#c 10
#% 131357
#% 132174
#% 167629
#% 179991
#% 1476298
#! This paper reports an analysis of near-optimal Blocks World planning. Various methods are clarified, and their time complexity is shown to be linear in the number of blocks, which improves their known complexity bounds. The speed of the implemented programs (ten thousand blocks are handled in a second) enables us to make empirical observations on large problems. These suggest that the above methods have very close average performance ratios, and yield a rough upper bound on those ratios well below the worst case of 2. Further, they lead to the conjecture that in the limit the simplest linear time algorithm could be just as good on average as the optimal one.

#index 1476301
#* Planning for temporally extended goals
#@ Fahiem Bacchus;Froduald Kabanza
#t 1996
#c 10
#% 99826
#% 100138
#% 100159
#% 101955
#% 109935
#% 114677
#% 179938
#% 179943
#% 1272468
#% 1275329
#% 1275454
#% 1477095
#! In planning, goals have traditionally been viewed as specifying a set of desirable final states. Any plan that transforms the current state to one of these desirable states is viewed to be correct. Goals of this form are limited as they do not allow us to constrain the manner in which the plan achieves its objectives. We propose viewing goals as specifying desirable sequences of states, and a plan to be correct if its execution yields one of these desirable sequences. We present a logical language, a temporal logic, for specifying goals with this semantics. Our language is rich and allows the representation of a range of temporally extended goals, including classical goals, goals with temporal deadlines, quantified goals (with both universal and existential quantification), safety goals, and maintenance goals. Our formalism is simple and yet extends previous approaches in this area. We also present a planning algorithm that can generate correct plans for these goals. This algorithm has been implemented, and we provide some examples of the formalism at work. The end result is a planning system which can generate plans that satisfy a novel and useful set of conditions.

#index 1476302
#* A cost-directed planner: preliminary report
#@ Eithan Ephrati;Martha E. Pollack;Marina Milshtein
#t 1996
#c 10
#% 23012
#% 131557
#% 163715
#% 215151
#% 678135
#! We present a cost-directed heuristic planning algorithm, which uses an A* strategy for node selection. The heuristic evaluation function is computed by a deep lookahead that calculates the cost of complete plans for a set of pre-defined top-level subgoals, under the (generally false) assumption that they do not interact. This approach leads to finding low-cost plans, and in many circumstances it also leads to a significant decrease in total planning time. This is due in part to the fact that generating plans for subgoals individually is often much less costly than generating a complete plan taking interactions into account, and in part to the fact that the heuristic can effectively focus the search. We provide both analytic and experimental results.

#index 1476303
#* Monitoring the progress of anytime problem-solving
#@ Eric A. Hansen;Shlomo Zilberstein
#t 1996
#c 10
#% 22348
#% 95983
#% 98073
#% 110379
#% 159239
#% 174102
#% 179940
#% 527850
#% 1273303
#% 1376414
#! Anytime algorithms offer a tradeoff between solution quality and computation time that has proved useful in applying artificial intelligence techniques to time-critical problems. To exploit this tradeoff, a system must be able to determine the best time to stop deliberation and act on the currently available solution. When the rate of improvement of solution quality is uncertain, monitoring the progress of the algorithm can improve the utility of the system. This paper introduces a technique for run-time monitoring of anytime algorithms that is sensitive to the variance of the algorithm's performance, the time-dependent utility of a solution, the ability of the run-time monitor to estimate the quality of the currently available solution, and the cost of monitoring. The paper examines the conditions under which the technique is optimal and demonstrates its applicability.

#index 1476304
#* A linear-programming approach to temporal reasoning
#@ Peter Jonsson;Christer Bäckström
#t 1996
#c 10
#% 1436
#% 82720
#% 84513
#% 107137
#% 137041
#% 152555
#% 181229
#% 190332
#% 319244
#! We present a new formalism, Horn Disjunctive Linear Relations (Horn DLRs), for reasoning about temporal constraints. We prove that deciding satisfiability of sets of Horn DLRs is polynomial by exhibiting an algorithm based upon linear programming. Furthermore, we prove that most other approaches to tractable temporal constraint reasoning can be encoded as Horn DLRs, including the ORD-Horn algebra and most methods for purely quantitative reasoning.

#index 1476305
#* Production systems need negation as failure
#@ Phan Minh Dung;Paolo Mancarella
#t 1996
#c 10
#% 1146
#% 2079
#% 53385
#% 131385
#% 198464
#% 277342
#! We study action rule based systems with two forms of negation, namely classical negation and "negation as failure to find a course of actions". We show by several examples that adding negation as failure to such systems increase their expressiveness, in the sense that real life problems can be represented in a natural and simple way. Then, we address the problem of providing a formal declarative semantics to these extended systems, by adopting an argumentation based approach, which has been shown to be a simple unifying framework for understanding the declarative semantics of various nonmonotonic formalisms. In this way, we naturally define the grounded (well-founded), stable and preferred semantics for production systems with negation as failure. Next, we characterize the class of stratified production systems, which enjoy the properties that the above mentioned semantics coincide and that negation as failure can be computed by a simple bottom-up operator.

#index 1476306
#* Using constraints to model disjunctions in rule-based reasoning
#@ Bing Liu;Joxan Jaffar
#t 1996
#c 10
#% 3463
#% 35562
#% 56471
#% 118278
#% 126385
#% 131561
#% 406437
#% 520420
#% 1275303
#% 1275304
#! Rule-based systems have long been widely used for building expert systems to perform practical knowledge intensive tasks. One important issue that has not been addressed satisfactorily is the disjunction, and this significantly limits their problem solving power. In this paper, we show that some important types of disjunction can be modeled with Constraint Satisfaction Problem (CSP) techniques, employing their simple representation schemes and efficient algorithms. A key idea is that disjunctions are represented as constraint variables, relations among disjunctions are represented as constraints, and rule chaining is integrated with constraint solving. In this integration, a constraint variable or a constraint is regarded as a special fact, and rules can be written with constraints and information about constraints. Chaining of rules may trigger constraint propagation, and constraint propagation may cause firing of rules. A prototype system (called CFR) based on this idea has been implemented.

#index 1476307
#* A connectionist framework for reasoning: reasoning with examples
#@ Dan Roth
#t 1996
#c 10
#% 697
#% 90392
#% 92146
#% 95578
#% 145393
#% 179693
#% 179788
#% 179972
#% 189699
#% 198461
#% 361471
#% 1273565
#% 1273622
#% 1275268
#% 1290050
#% 1477073
#% 1477155
#% 1478526
#% 1478595
#% 1478599
#! We present a connectionist architecture that supports almost instantaneous deductive and abductive reasoning. The deduction algorithm responds in few steps for single rule queries and in general, takes time that is linear with the number of rules in the query. The abduction algorithm produces an explanation in few steps and the best explanation in time linear with the size of the assumption set. The size of the network is polynomially related to the size of other representations of the domain, and may even be smaller. We base our connectionist model on Valiant's Neuroidal model (Val94) and thus make minimal assumptions about the computing elements, which are assumed to be classical threshold elements with states. Within this model we develop a reasoning framework that utilizes a model-based approach to reasoning (KKS93; KR94b). In particular, we suggest to interpret the connectionist architecture as encoding examples of the domain we reason about and show how to perform various reasoning tasks with this interpretation. We then show that the representations used can be acquired efficiently from interactions with the environment and discuss how this learning process influences the reasoning performance of the network.

#index 1476308
#* Goal oriented symbolic propagation in Bayesian networks
#@ Enrique Castillo;José Manuel Gutiérrez;Ali S. Hadi
#t 1996
#c 10
#% 44876
#% 382160
#% 503690
#% 527664
#% 527691
#% 1273621
#! The paper presents an efficient goal oriented algorithm for symbolic propagation in Bayesian networks. The proposed algorithm performs symbolic propagation using numerical methods. It first takes advantage of the independence relationships among the variables and produce a reduced graph which contains only the relevant nodes and parameters required to compute the desired propagation. Then, the symbolic expression of the solution is obtained by performing numerical propagations associated with specific values of the symbolic parameters. These specific values are called the canonical components. Substantial savings are obtained with this new algorithm. Furthermore, the canonical components allow us to obtain lower and upper bounds for the symbolic expressions resulting from the propagation. An example is used to illustrate the proposed methodology.

#index 1476309
#* A Clinician's tool for analyzing non-compliance
#@ David Maxwell Chickering;Judea Pearl
#t 1996
#c 10
#% 1272178
#! We describe a computer program to assist a clinician with assessing the efficacy of treatments in experimental studies for which treatment assignment is random but subject compliance is imperfect. The major difficulty in such studies is that treatment efficacy is not "identifiable", that is, it cannot be estimated from the data, even when the number of subjects is infinite, unless additional knowledge is provided. Our system combines Bayesian learning with Gibbs sampling using two inputs: (1) the investigator's prior probabilities of the relative sizes of subpopulations and (2) the observed data from the experiment. The system outputs a histogram depicting the posterior distribution of the average treatment effect, that is, the probability that the average outcome (e.g., survival) would attain a given level, had the treatment been taken uniformly by the entire population. This paper describes the theoretical basis for the proposed approach and presents experimental results on both simulated and real data, showing agreement with the theoretical asymptotic bounds.

#index 1476310
#* Building classifiers using Bayesian networks
#@ Nir Friedman;Moises Goldszmidt
#t 1996
#c 10
#% 44876
#% 70370
#% 128623
#% 136350
#% 197387
#% 1290045
#! Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from the U. C. Irvine repository, and compared them against C4.5, naive Bayes, and wrapper-based feature selection methods.

#index 1476311
#* Generalized queries on probabilistic context-free grammars
#@ David V. Pynadath;Michael P. Wellman
#t 1996
#c 10
#% 44876
#% 179803
#% 317974
#% 322318
#% 740899
#% 1650681
#! Probabilistic context-free grammars (PCFGs) provide a simple way to represent a particular class of distributions over sentences in a context-free language. Efficient parsing algorithms for answering particular queries about a PCFG (i.e., calculating the probability of a given sentence, or finding the most likely parse) have been applied to a variety of pattern-recognition problems. We extend the class of queries that can be answered in several ways: (1) allowing missing tokens in a sentence or sentence fragment, (2) supporting queries about intermediate structure, such as the presence of particular nonterminals, and (3) flexible conditioning on a variety of types of evidence. Our method works by constructing a Bayesian network to represent the distribution of parse trees induced by a given PCFG. The network structure mirrors that of the chart in a standard parser, and is generated using a similar dynamic-programming approach. We present an algorithm for constructing Bayesian networks from PCFGs, and show how queries or patterns of queries on the network correspond to interesting queries on PCFGs.

#index 1476312
#* On the foundations of qualitative decision theory
#@ Ronen I. Brafman;Moshe Tennenholtz
#t 1996
#c 10
#% 4332
#% 183083
#% 188086
#% 781175
#% 1290145
#! This paper investigates the foundation of maximin, one of the central qualitative decision criteria, using the approach taken by Savage (Savage 1972) to investigate the foundation and rationality of classical decision theory. This approach asks "which behaviors could result from the use of a particular decision procedure?" The answer to this question provides two important insights: (1) under what conditions can we employ a particular agent model, and (2) how rational is a particular decision procedure. Our main result is a constructive representation theorem in the spirit of Savage's result for expected utility maximization, which uses two choice axioms to characterize the maxapnin criterion. These axioms characterize agent behaviors that can be modeled compactly using the maxcirnin model, and, with some reservations, indicate that rnaxionin is a reasonable decision criterion.

#index 1476313
#* Plausibility measures and default reasoning
#@ Nir Friedman;Joseph Y. Halpern
#t 1996
#c 10
#% 36534
#% 74868
#% 77841
#% 100178
#% 115327
#% 146906
#% 163720
#% 167535
#% 396114
#% 443633
#% 780340
#% 782324
#% 1273615
#% 1279725
#% 1279727
#% 1476314
#% 1650648
#% 1650784
#! In recent years, a number of different semantics for defaults have been proposed, such as preferential structures, ε-semantics, possibilistic structures, and κ-rankings, that have been shown to be characterized by the same set of axioms, known as the KLM properties (for Kraus, Lehmann, and Magidor). While this was viewed as a surprise, we show here that it is almost inevitable. We do this by giving yet another semantics for defaults that uses plausibility measures, a new approach to modeling uncertainty that generalize other approaches, such as probability measures, belief functions, and possibility measures. We show that all the earlier approaches to default reasoning can be embedded in the framework of plausibility. We then provide a necessary and sufficient condition on plausibilities for the KLM properties to be sound, and an additional condition necessary and sufficient for the KLM properties to be complete. These conditions are easily seen to hold for all the earlier approaches, thus explaining why they are characterized by the KLM properties.

#index 1476314
#* First-order conditional logic revisited
#@ Nir Friedman;Joseph Y. Halpern;Daphne Koller
#t 1996
#c 10
#% 23014
#% 42005
#% 74868
#% 77841
#% 89958
#% 90371
#% 100178
#% 107155
#% 163720
#% 366370
#% 443633
#% 780335
#% 1279727
#% 1476313
#% 1650648
#! Conditional Zogics play an important role in recent attempts to investigate default reasoning. This paper investigates first-order conditional logic. We show that, as for first-order probabilistic logic, it is important not to confound statistical conditionals over the domain (such as "most birds fly"), and subjective conditionals over possible worlds (such as "I believe that lweety is unlikely to fly"). We then address the issue of ascribing semantics to first-order conditional logic. As in the propositional case, there are many possible semantics. To study the problem in a coherent way, we use plausibility structures. These provide us with a general framework in which many of the standard approaches can be embedded. We show that while these standard approaches are all the same at the propositional level, they are significantly different in the context of a first-order language. We show that plausibilities provide the most natural extension of conditional logic to the first-order case: We provide a sound and complete axiomatization that contains only the KLM properties and standard axioms of first-order modal logic. We show that most of the other approaches have additional properties, which result in an inappropriate treatment of an infinitary version of the lottery paradox.

#index 1476315
#* A counterexample to theorems of Cox and fine
#@ Joseph Y. Halpern
#t 1996
#c 10
#% 71810
#% 209373
#% 405003
#% 527654
#! Cox's well-known theorem justifying the use of probability is shown not to hold in finite domains. The counterexample also suggests that Cox's assumptions are insufficient to prove the result even in infinite domains. The same counterexample is used to disprove a result of Fine on comparative conditional probability.

#index 1476316
#* Robots with AI: a retrospective on the AAAI robot competitions and exhibitions
#@ Pete Bonasso;Tom Dean
#t 1996
#c 10

#index 1476317
#* Moving up the information food chain: deploying softbots on the world wide web
#@ Oren Etzioni
#t 1996
#c 10
#% 90711
#% 97619
#% 159113
#% 168421
#% 179945
#% 1275347
#! I view the World Wide Web as an information food chain (figure 1). The maze of pages and hyperlinks that comprise the Web are at the very bottom of the chain. The WebCrawlers and Alta Vistas of the world are information herbivores; they graze on Web pages and regurgitate them as searchable indices. Today, most Web users feed near the bottom of the information food chain, but the time is ripe to move up. Since 1991, we have been building information carnivores, which intelligently hunt and feast on herbivores in Unix (Etzioni, Lesh, & Segal 1993), on the Internet (Etzioni & Weld 1994), and on the Web (Doorenbos, Etzioni, & Weld 1996; Selberg & Etzioni 1995; Shakes, Langheinrich, & Etzioni 1996).

#index 1476318
#* Brain dynamics in the genesis of trust as the basis for communication by representations
#@ Walter J. Freeman
#t 1996
#c 10
#! A theory of brain dynamics is proposed according to which brains construct external representations by actions into the world for communication. The prior brain patterns constitute meanings, not representations of meanings. The representations have no meaning in themselves. They are shaped in accordance with meaning inside transmitting brains, and they can elicit the construction of meaning inside receiving brains, provided that trust has been established between the transmitters and the receivers through appropriate neurochemical changes.

#index 1476319
#* Using multi-agent systems to represent uncertainty
#@ Joseph Y. Halpern
#t 1996
#c 10
#% 137786
#% 157172
#% 188086
#! I consider a logical framework for modeling uncertainty, based on the use of possible worlds, that incorporates knowledge, probability, and time. This turns out to be a powerful approach for modeling many problems of interest. I show how it can be used to give insights into (among other things) several wellknown puzzles.

#index 1476320
#* Refinement planning: status and prospectus
#@ Subbarao Kambhampati
#t 1996
#c 10
#% 44836
#% 109935
#% 163715
#% 168288
#% 179932
#% 182491
#% 194651
#% 363939
#% 1272364
#% 1272468
#% 1290109
#% 1476298
#! Most current-day AI planning systems operate by iteratively refining a partial plan until it meets the goal requirements. In the past five years, significant progress has been made in our understanding of the spectrum and capabilities of such refinement planners. In this talk, I will summarize this understanding in terms of a unified framework for refinement planning and discuss several current research directions.

#index 1476321
#* Boosting theory towards practice: recent developments in decision tree induction and the weak learning framework
#@ Michael Kearns
#t 1996
#c 10
#% 697
#% 73372
#% 91386
#% 136350
#% 150153
#% 176017
#% 180945
#% 198701
#% 214236
#% 476546
#% 520224
#% 593735
#% 1081181
#% 1081183
#% 1081284

#index 1476322
#* Challenge problems for artificial intelligence
#@ Bart Selman;Rodney A. Brooks;Thomas Dean;Eric Horvitz;Tom M. Mitchell;Nils J. Nilsson
#t 1996
#c 10
#% 100336
#% 426494
#% 834987
#% 1306101

#index 1476323
#* The database approach to knowledge representation
#@ Jeffrey D. Ullman
#t 1996
#c 10
#% 64424
#% 111826
#% 122396
#% 198465
#% 198466
#% 213982
#% 459241
#% 481128
#% 599549
#! The database theory community, centered around the PODS (Principles of Database Systems) conference has had a long-term interest in logic as a way to represent "data", "information," and "knowledge" (take your pick on the term - it boils down to facts or atoms and rules, usually Horn clauses). The approach of this community has been "slow and steady," preferring to build up carefully from simple special cases to more general ideas, always paying attention to how efficiently we can process queries and perform other operations on the facts and rules. A powerful theory has developed, and it is beginning to have some impact on applications, especially information-integration engines.

#index 1476324
#* A reactive mobile robot based on a formal theory of action
#@ C. Baral;L. Floriano;A. Gabaldon;D. Morales;T. Son;R. Watson
#t 1996
#c 10

#index 1476325
#* CoMRoS: cooperative mobile robots stuttgart
#@ Thomas Bräunl;Martin Kalbacher;Paul Levi;Günter Mamier
#t 1996
#c 10

#index 1476326
#* McMaster university's artificial computing system
#@ Andrew Dawes;Mark Bentley
#t 1996
#c 10

#index 1476327
#* Doing tasks with multiple mini-robots
#@ John Fischer;Paul Rybski;Dirk Edmonds;Maria Gini
#t 1996
#c 10

#index 1476328
#* Lola, the mobile robot from NC state
#@ Ricardo Gutierrez-Osuna;Daniel S. Schudel;Jason A. Janet;Ren C. Luo
#t 1996
#c 10

#index 1476329
#* Clementine: colorado school of mines
#@ Robin R. Murphy
#t 1996
#c 10

#index 1476330
#* Mobile robot navigation and control: a case study
#@ Nicholas Roy;Gregory Dudek;Michael Daum
#t 1996
#c 10

#index 1476331
#* YODA: the young observant discovery agent
#@ Wei-Min Shen;Jafar Adibi;Bonghan Cho;Gal Kaminka;Jihie Kim;Behnam Salemi;Sheila Tejada
#t 1996
#c 10
#% 363500

#index 1476332
#* Amelia
#@ Reid Simmons;Sebastian Thrun;Greg Armstrong;Richard Goodwin;Karen Haigh;Sven Koenig;Shyjan Mahamud;Daniel Nikovski;Joseph O'Sullivan
#t 1996
#c 10
#% 1290038

#index 1476333
#* Selection of passages for information reduction
#@ Jody J. Daniels
#t 1996
#c 10
#% 194282

#index 1476334
#* Towards a unified approach to concept learning
#@ Pedro Domingos
#t 1996
#c 10
#% 209023

#index 1476335
#* A computational theory of turn-taking
#@ Toby Donaldson
#t 1996
#c 10
#% 68239
#% 319244

#index 1476336
#* Learning in multi-agent systems
#@ Claudia V. Goldman
#t 1996
#c 10
#% 496725
#% 496726

#index 1476337
#* Bounding the cost of learned rules: a transformational approach
#@ Jihie Kim
#t 1996
#c 10
#% 1499579

#index 1476338
#* Agent-centered search: situated search with small look-ahead
#@ Sven Koenig
#t 1996
#c 10
#% 1499507

#index 1476339
#* Recurrent expert networks
#@ Cathie LeBlanc
#t 1996
#c 10
#% 142230
#% 197599
#% 363663
#% 404505

#index 1476340
#* Semi-deterministic reasoning
#@ Chengjiang Mao
#t 1996
#c 10
#% 126902

#index 1476341
#* A connectionist model of instructed learning
#@ David C. Noelle
#t 1996
#c 10
#% 179793

#index 1476342
#* Symptom management for schizophrenic agents
#@ Phoebe Sengers
#t 1996
#c 10

#index 1476343
#* Adaptive shared control for an intelligent power wheelchair
#@ Richard C. Simpson;Simon P. Levine
#t 1996
#c 10
#% 109042

#index 1476344
#* Induction of selective Bayesian networks from data
#@ Moninder Singh
#t 1996
#c 10
#% 44876

#index 1476345
#* Why dissect a frog when you can simulate a lion?
#@ Brian K. Smith
#t 1996
#c 10
#% 165826
#% 179994
#% 1499556

#index 1476346
#* Algorithm evolution for signal understanding
#@ Astro Teller
#t 1996
#c 10
#% 240255
#% 243355

#index 1476347
#* The use of knowledge-based systems techniques for risk assessment
#@ Botond Virginas
#t 1996
#c 10
#% 2417

#index 1476348
#* Efficient planning by graph rewriting
#@ José Luis Ambite;Craig A. Knoblock
#t 1996
#c 10
#% 32889
#% 126390
#% 182491
#% 1290113
#% 1290115

#index 1476349
#* Expecting the unexpected: detecting and reacting to unplanned-for world states
#@ Ella M. Atkins;Edmund H. Durfee;Kang G. Shin
#t 1996
#c 10
#% 181338
#% 449588

#index 1476350
#* Experiments in evolutionary synthesis of robotic neurocontrollers
#@ Karthik Balakrishnan;Vasant Honavar
#t 1996
#c 10

#index 1476351
#* A reinforcement learning framework for combinatorial optimization
#@ Justin A. Boyan
#t 1996
#c 10
#% 124689
#% 1290042

#index 1476352
#* Learning topological maps: an alternative approach
#@ Arno Bücken;Sebastian Thrun
#t 1996
#c 10
#% 367254
#! Our goal is autonomous real-time control of a mobile robot. In this paper we want to show a possibility to learn topological maps of a large-scale indoor environment autonomously. In the literature there are two paradigms how to store information on the environment of a robot: as a grid-based (geometric) or as a topological map. While grid-based maps are considerably easy to learn and maintain, topological maps are quite compact and facilitate fast motionplanning.

#index 1476353
#* Computing default logic extensions: an implementation
#@ A. P. Courtney;N. Y. Foo;G. Antonion
#t 1996
#c 10
#% 544418

#index 1476354
#* Characterizing temporal repetition
#@ Diana Cukierman;James Delgrande
#t 1996
#c 10
#% 319244

#index 1476355
#* Achieving agent coordination via distributed preferences
#@ Joseph G. D'Ambrosio;William P. Birmingham
#t 1996
#c 10

#index 1476356
#* Fast discovery of simple rules
#@ Pedro Domingos
#t 1996
#c 10

#index 1476357
#* Multistrategy learning: a case study
#@ Pedro Domingos
#t 1996
#c 10
#% 1290056

#index 1476358
#* Simple Bayesian classifiers do not assume independence
#@ Pedro Domingos;Michael Pazzani
#t 1996
#c 10

#index 1476359
#* CADI: an intelligent, multimedia tutor for cardiac auscultation
#@ Kurt D. Fenstermacher
#t 1996
#c 10
#% 55921

#index 1476360
#* Integration of an expert teaching assistant with distance learning software
#@ Steven P. Fonseca;Nancy E. Reed
#t 1996
#c 10

#index 1476361
#* Self-adaptation of mutation rates and dynamic fitness
#@ Matthew R. Glickman;Katia P. Sycara
#t 1996
#c 10
#% 466542
#! In any search via artificial evolution, the likelihood of stagnation at local optima is determined by the particular choices of representation and search operators. Because the performance impact of these design choices is difficult to predict, it is an attractive option to let the representation and/or operators themselves evolve. However, effective evolution at this meta-level can be difficult to achieve for a number of reasons, including: (1) The complexity of the search space is increased; and (2) selection acts at the level of the fitness function and only indirectly at the meta-level, favoring variations only to the extent to which they are stochastically associated with fitness improvements. The question then becomes: Under what conditions is evolution of the representation and/or operators likely to be most effective?

#index 1476362
#* Heterogeneous and homogeneous robot group behavior
#@ Dani Goldberg
#t 1996
#c 10
#% 103455

#index 1476363
#* Inducing design biases that characterize successful experimentation in weak-theory domains: TIPS
#@ Vanathi Gopalakrishnan
#t 1996
#c 10

#index 1476364
#* Belief network algorithms: a study of performance
#@ Nathalie Jitnah
#t 1996
#c 10
#% 44876

#index 1476365
#* Proposed interestingness measure for characteristic rules
#@ Micheline Kamber;Rajjan Shinghal
#t 1996
#c 10
#% 442814

#index 1476366
#* A transformational analysis of the EBL utility problem
#@ Jihie Kim;Paul S. Rosenbloom
#t 1996
#c 10
#% 97623
#% 449587
#% 1499579

#index 1476367
#* Controlling state-space abstraction in Bayesian networks
#@ Chao-Lin Liu
#t 1996
#c 10
#% 159239

#index 1476368
#* Ad hoc attribute-value prediction
#@ Gabor Melli
#t 1996
#c 10
#% 92533
#% 442814

#index 1476369
#* An incremental interactive algorithm for regular grammar inference
#@ Rajesh Parekh;Vasant Honavar
#t 1996
#c 10

#index 1476370
#* Constructive neural network learning algorithms
#@ Rajesh Parekh;Jihoon Yang;Vasant Honavar
#t 1996
#c 10

#index 1476371
#* A computational model of persistent belief
#@ Sunju Park
#t 1996
#c 10
#% 136356
#% 408396

#index 1476372
#* Contracting strategy based on Markov process modeling
#@ Sunju Park;Edmund W. Durfee
#t 1996
#c 10

#index 1476373
#* Learning procedural planning knowledge in complex environments
#@ Douglas J. Pearson
#t 1996
#c 10

#index 1476374
#* MarketBayes: a distributed, market-based Bayesian network
#@ David M. Pennock
#t 1996
#c 10
#% 1268730

#index 1476375
#* The Kritzel system for handwriting interpretation
#@ Gaofeng Qian
#t 1996
#c 10

#index 1476376
#* SplitNet: a dynamic hierarchical network model
#@ Jürgen Rahmel
#t 1996
#c 10

#index 1476377
#* Symbolic performance & learning in continuous environments
#@ Seth O. Rogers
#t 1996
#c 10

#index 1476378
#* Effects of local information on group behavior
#@ Shounak Roychowdhury;Neeraj Arora;Sandip Sen
#t 1996
#c 10
#% 82808

#index 1476379
#* Automated formulation of constraint satisfaction problems
#@ Mihaela Sabin;Eugene C. Freuder
#t 1996
#c 10

#index 1476380
#* Dynamic constraint-based planning in trauma management
#@ Moninder Singh
#t 1996
#c 10

#index 1476381
#* Blocking as a middle-ground for step-order commitments in planning
#@ Biplav Srivastava;Subbarao Kambhampati
#t 1996
#c 10
#% 163715

#index 1476382
#* Experimentation-driven operator learning
#@ Kang Soo Tae
#t 1996
#c 10

#index 1476383
#* Hybrid knowledge- and databases
#@ Merwyn Taylor
#t 1996
#c 10

#index 1476384
#* Learning models for multi-source integration
#@ Sheila Tejada;Craig A. Knoblock;Steven Minton
#t 1996
#c 10
#% 1275347

#index 1476385
#* Rabbi: exploring the inner world through stories
#@ Marina Umaschi
#t 1996
#c 10
#% 149686
#% 375339

#index 1476386
#* Constructive induction of features for planning
#@ Michael van Lent
#t 1996
#c 10
#% 1273368

#index 1476387
#* Agents modeling agents in information economies
#@ José M. Vidal;Edmund H. Durfee
#t 1996
#c 10
#% 437511

#index 1476388
#* Optimal factory scheduling using stochastic dominance A*
#@ Peter R. Wurman
#t 1996
#c 10
#% 102372
#% 1650688

#index 1476389
#* Dynamic map: representation of interactions between robots
#@ Christian Zanardi
#t 1996
#c 10

#index 1476390
#* Neural network guided search control in partial order planning
#@ Terry Zimmerman;Subbarao Kambhampati
#t 1996
#c 10
#% 216992
#% 376589

#index 1476391
#* The BOEING 777 - concurrent engineering and digital pre-assembly
#@ Bob Abarbanel
#t 1996
#c 10
#! The processes created on the 777 for checking designs were called "digital preassembly". Using FlyThru(tm), a spinoff of a Boeing advanced computing research project, engineers were able to view up to 1500 models (15000 solids) in 3d traversing that data at high speed. FlyThru(tm) was rapidly deployed in 1991 to meet the needs of the 777 for large scale product visualization and verification. The digital pre-assembly process has had fantastic results. The 777 has had far fewer assembly and systems problems compared to previous airplane programs. Today, FlyThru(tm) is installed on hundreds of workstations on almost every airplane program, and is being used on Space Station, F22, AWACS, and other defense projects. It's applications have gone far beyond just design review. In many ways, FlyThru is a Data Warehouse supported by advanced tools for analysis. It is today being integrated with Knowledge Based Engineering geometry generation tools.

#index 1476392
#* Data mining and knowledge discovery in databases: applications in astronomy and planetary science
#@ Usama M. Fayyad
#t 1996
#c 10
#% 216499
#% 216502
#% 232102
#% 232106
#% 232136
#% 232166
#% 412588
#% 535655

#index 1478729
#* Proceedings of the fourteenth national conference on artificial intelligence and ninth conference on Innovative applications of artificial intelligence
#@ 
#t 1997
#c 10

#index 1478730
#* If at first you don't succeed...
#@ Kentaro Toyama;Gregory D. Hager
#t 1997
#c 10
#% 90034
#% 98073
#% 120258
#% 129161
#% 146318
#% 457558
#% 592284
#% 625110
#% 630127
#% 932912
#! One quality that makes biological systems appear intelligent is their robustness to difficult circumstances. Robustness is crucial to intelligent behavior and important to AI research. We distinguish between ante-failure and post-failure robustness for causal tasks. Ante-failure robust systems resist failure, whereas post-failure systems incorporate the ability to recover from failure once it happens. We point out the power of post-failure robustness in AI problems, closely examining one example in visual motion tracking. Finally, we raise theoretical issues and argue for greater effort towards building post-failure robust systems.

#index 1478731
#* Modeling emotions and other motivations in synthetic agents
#@ Juan D. Velásquez
#t 1997
#c 10
#% 18600
#% 127564
#% 159119
#% 170982
#% 198113
#% 367541
#% 1275296
#! We present Cathexis, a distributed, computational model which offers an alternative approach to model the dynamic nature of different affective phenomena, such as emotions, moods and temperaments, and provides a flexible way of modeling their influence on the behavior of synthetic autonomous agents. The model has been implemented as part of an extensible, object-oriented framework which provides enough functionality for agent developers to design emotional agents that can be used in a variety of applications including entertainment (e.g. synthetic agents for interactive drama, video games, etc.), education (e.g. Intelligent Tutoring Systems), and human-computer interfaces.

#index 1478732
#* Coordinating agents by role based social constraints and conversation plans
#@ Mihai Barbuceanu
#t 1997
#c 10
#% 68239
#% 128329
#% 136356
#% 181622
#% 644560
#! We explore the view that coordinated behavior is explained by the social constraints that agents in organizations are subject to. In this framework, agents adopt those goals that are requested by their obligations, knowing that not fulfilling obligations induces a price to payor a loss of utility. Based on this idea we build a coordination system where we represent the organization, the roles played by agents, the obligations imposed among roles, the goals and the plans that agents may adopt. Once a goal adopted, a special brand of plans, called conversation plans, are available to the agents for effectively carrying out coordinated action. Conversation plans explicitly represent interactions by message exchange and their actions are dynamically reordered using the theory of Markov Decision Processes to ensure the optimization of various criteria. The framework is applied to model supply chains of distributed enterprises.

#index 1478733
#* Agent architectures for flexible, practical teamwork
#@ Milind Tambe
#t 1997
#c 10
#% 75896
#% 121991
#% 189698
#% 241019
#% 1275240
#% 1279674
#% 1499469
#% 1499477
#! Teamwork in complex, dynamic, multi-agent domains mandates highly flexible coordination and communication. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability is agent architectures with integrated teamwork capabilities. This fundamental shift in agent architectures is illustrated via an implemented candidate: STEAM. While STEAM is founded on the joint intentions theory, practical operationalization has required it to integrate several key novel concepts: (i) team synchronization to establish joint intentions; (ii) constructs for monitoring joint intentions and repair; and (iii) decision-theoretic communication selectivity (to pragmatically extend the joint intentions theory). Applications in three different complex domains, with empirical results, are presented.

#index 1478734
#* Negotiation on data allocation in multi-agent environments
#@ Rina Schwartz;Sarit Kraus
#t 1997
#c 10
#% 126390
#% 162305
#% 189700
#% 318016
#% 408396
#% 462024
#! We propose a strategic negotiation model that takes into account the passage of time during the negotiation process itself in order to solve the problem of data allocation in environments with self-motivated servers which have no common interest and no central controller. The model considers situations characterized by complete, as well as incomplete, information. Using this negotiation mechanism, the servers have simple and stable negotiation strategies that result in efficient agreements without delays. We provide heuristics for finding the details of the strategies which depend on the specific settings of the environment, and demonstrate the quality of the heuristics, using simulations. We prove that our methods yield better results than the static allocation policy currently used for data allocation for servers in distributed systems.

#index 1478735
#* Benefits of learning in negotiation
#@ Dajun Zeng;Katia Sycara
#t 1997
#c 10
#% 361730
#% 496723
#% 496725
#% 1275313
#! Negotiation has been extensively discussed in game-theoretic, economic, and management science literatures for decades. Recent growing interest in electronic commerce has given increased importance to automated negotiation. Evidence both from theoretical analysis and from observations of human interactions suggests that if decision makers can somehow take into consideration what other agents are thinking and furthermore learn during their interactions how other agents behave, their payoff might increase. In this paper, we propose a sequential decision making model of negotiation, called Bazaar. Within the proposed negotiation framework, we model learning as a Bayesian belief update process. In this paper, we explore the hypothesis that learning is beneficial in sequential negotiation and present initial experimental results.

#index 1478736
#* Representing actions and state constraints in model-based diagnosis
#@ Sheila A. McIlraith
#t 1997
#c 10
#% 33376
#% 58574
#% 117869
#% 125529
#% 132173
#% 192039
#% 533953
#% 701158
#% 707198
#% 1273470
#% 1274639
#% 1290152
#% 1290153
#% 1290154
#! In this paper we examine an important set of representation issues which have not been addressed by the model-based diagnosis community. In particular, we examine the problem of integrating a model-based diagnosis system description, SD, with a theory of action to parsimoniously represent the effect of actions on a system and the effects of system state on performing actions in the world. We employ the situation calculus, a first-order language, as our representation language. In the context of the situation calculus, SD presents an, often complex, set of state constraints. These state constraints implicitly define indirect effects of actions as well as indirectly imposing further preconditions on the performance of actions. As a consequence, SD presents further complications to addressing the frame, ramification and qualification problems. For the purposes of this paper, we examine a syntactically restricted SD, which commonly occurs in the axiomatization of model-based diagnosis domains. The contributions of this paper include: 1) a framework for integrating SD and a theory of action. 2) a procedure for compiling SD into a set of successor state axioms. These axioms capture the intended interpretation of SD, while providing a closed-form solution to the frame and ramification problems.

#index 1478737
#* Fast context switching in real-time propositional reasoning
#@ P. Pandurang Nayak;Brian C. Williams
#t 1997
#c 10
#% 3460
#% 241013
#% 457176
#% 1476265
#% 1499556
#! The trend to increasingly capable and affordable control processors has generated an explosion of embedded real-time gadgets that serve almost every function imaginable. The daunting task of programming these gadgets is greatly alleviated with real-time deductive engines that perform all execution and monitoring functions from a single core model. Fast response times are achieved using an incremental propositional deductive database (an LTMS). Ideally the cost of an LTMS's incremental update should be linear in the number of labels that change between successive contexts. Unfortunately an LTMS can expend a significant percentage of its time working on labels that remain constant between contexts. This is caused by the LTMS's conservative approach: a context switch first removes all consequences of deleted clauses, whether or not those consequences hold in the new context. This paper presents a more aggressive incremental TMS, called the ITMS, that avoids processing a significant number of these consequences that are unchanged. Our empirical evaluation for spacecraft control shows that the overhead of processing unchanged consequences can be reduced by a factor of seven.

#index 1478738
#* Visual prompts and graphical design: a framework for exploring the design space of 2-D charts and graphs
#@ Vibhu O. Mittal
#t 1997
#c 10
#% 28144
#% 172757
#% 175130
#% 214548
#% 834997
#% 837671
#! Graphical presentations can be very effective in communicating large datasets and patterns, trends and relationships in them. Charts and graphs used in reporting data usually tend to highlight certain aspects and suppress others. In fact, a recent study of several hundred annual reports found that more than 30% of charts in these reports were designed to facilitate inferences favorable to the companies while hindering others. Unfortunately, many of the techniques used to achieve these effects may not be obvious to the average user. One solution to this problem is to make design choices explicit to the user. This paper presents a data analysis interface that educates users by enabling them to explore the visualization space and modifying chart design parameters. This interface is based on an analysis of a corpus of charts and graphs and uses knowledge about a variety of techniques for emphasizing specific trends and/or values shown in 2-D charts and graphs.

#index 1478739
#* Navigation and planning in a mixed-initiative user interface
#@ Robert St. Amant
#t 1997
#c 10
#% 21287
#% 21299
#% 69423
#% 114517
#% 150994
#% 166076
#% 190614
#% 214456
#% 677278
#! Mixed-initiative planning is one approach to building an intelligent decision-making environment. A mixed-initiative system shares decision-making responsibility with the user such that it acts sometimes as a tool, to be directly applied to a specific task, and other times as an autonomous problem-solver. In the best case, the user can delegate the details of a task to the automated system without giving up the ability to guide and review the decision-making process. We have developed a simple mixed-initiative planner that incorporates a view of problem-solving as navigation. We have explored this notion in two different applications: exploratory statistical analysis and layout design for user interface dialogs. This paper discusses navigation issues in the context of these two systems, the potential benefits of the approach, and some implications for user interface design.

#index 1478740
#* Possibilistic and standard probabilistic semantics of conditional knowledge
#@ Salem Benferhat;Didier Dubois;Henri Prade
#t 1997
#c 10
#% 44876
#% 77841
#% 160255
#% 160378
#% 218813
#% 780340
#% 1279727
#% 1476313
#% 1650645
#! The authors have proposed in their previous works to view a set of default pieces of information of the form, "generally, from αi deduce βi" as the family of possibility distributions satisfying constraints expressing that the situations where αiΛβi is true are more possible than the situations where αiΛ¬βi is true. A representation theorem in terms of this semantics, for default reasoning obeying the System P of postulates proposed by Kraus, Lehmann and Magidor, has been obtained. This paper offers a detailed analysis of the structure of this family of possibility distributions by exploiting two different orderings between them: Yager's specificity ordering and a new refinement ordering. It is shown that from a representation point of view, it is sufficient to consider the subset of linear possibility distributions which corresponds to all the possible completions of the default knowledge in agreement with the constraints. There also exists a semantics for system P in terms of infinitesimal probabilities. Surprisingly, it is also shown that a standard probabilistic semantics can be equivalently given to System P, without referring to infinitesimals, by using a special family of probability measures, that two of the authors have called acceptance functions, and that has been also recently considered by Snow in that perspective.

#index 1478741
#* On the axiomatization of qualitative decision criteria
#@ Ronen I. Brafman;Moshe Tennenholtz
#t 1997
#c 10
#% 4332
#% 69486
#% 174161
#% 489791
#% 1290145
#% 1290156
#% 1476312
#% 1650798
#! Qualitative decision tools have been used in AI and CS in various contexts, but their adequacy is still unclear. To examine this question, our work employs the axiomatic approach to characterize the properties of various decision rules. In the past, we presented a constructive representation theorem for the maximin decision criterion, and we characterized conditions under which an agent can be viewed as adopting a qualitative decision-making approach (consisting of beliefs, goals, and a qualitative decision criterion). In this paper we show that the maximin representation theorem applies to two additional decision criteria: minmax regret and competitive ratio, and with slight modifications, to a third one, maximax. In addition, we characterize conditions under which an agent with a given qualitative utility function can be ascribed beliefs when we assume it adopts maximin as its decision criterion.

#index 1478742
#* Symbolic nearest mean classifiers
#@ Piew Datta;Dennis Kibler
#t 1997
#c 10
#% 5182
#% 92533
#% 126892
#% 126949
#% 136350
#% 140588
#% 1272304
#% 1290045
#! The minimum-distance classifier summarizes each class with a prototype and then uses a nearest neighbor approach for classification. Three drawbacks of the original minimum-distance classifier are its inability to work with symbolic attributes, weigh attributes, and learn more than a single prototype for each class. The proposed solutions to these problems include defining the mean for symbolic attributes, providing a weighting metric, and learning several possible prototypes for each class. The learning algorithm developed to tackle these problems, SNMC, increases classification accuracy by 10% over the original minimum-distance classifier and has a higher average generalization accuracy than both C4.5 and PEBLS on 20 domains from the UCI data repository.

#index 1478743
#* Classification and reductio-ad-absurdum optimality proofs
#@ Haim Schweitzer
#t 1997
#c 10
#% 170407
#% 190581
#% 217058
#% 1290045
#! Proofs for the optimality of classification in real-world machine learning situations are constructed. The validity of each proof requires reasoning about the probability of certain subsets of feature vectors. It is shown that linear discriminants classify by making the least demanding assumptions on the values of these probabilities. This enables measuring the confidence of classification by linear discriminants. We demonstrate experimentally that when linear discriminants make decisions with high confidence, their performance on real-world data improves significantly, to the point where they beat the best known nonlinear techniques on large portions of the data.

#index 1478744
#* The effect of observations on the complexity of model-based diagnosis
#@ Adnan Darwiche;Gregory Provan
#t 1997
#c 10
#% 1121
#% 85230
#% 132173
#% 1275254
#! This paper shows how to efficiently diagnose systems by making use of observations. In particular, we present two theorems concerning the effect of observations on the complexity of Model-Based Diagnosis. The first theorem shows how the presence of certain observations allows us to decompose a diagnostic reasoning task into independent reasoning tasks on sub-systems. The second theorem shows how the absence of certain observations allows us to ignore parts of a system during diagnostic reasoning. Another main contribution of this paper is an application of these theorems to diagnosing discrete-event systems. In particular, we identify observability and modularity characteristics of discrete-event systems that make them amenable to the presented theorems and, hence, to any diagnostic approach that employs these theorems effectively. This also explains why a particular approach that we have presented elsewhere has proven effective for diagnosing these systems.

#index 1478745
#* Monitoring, prediction, and fault isolation in dynamic physical systems
#@ Pieter J. Mosterman;Gautam Biswas
#t 1997
#c 10
#% 21138
#% 74799
#% 407731
#% 1273464
#% 1784168
#! Diagnosis of dynamic physical systems is complex and requires close interaction of monitoring, fault generation and refinement, and prediction. We establish a methodology for model-based diagnosis of continuous systems in a qualitative reasoning framework. A temporal causal model capturing dynamic system behavior identifies faults from deviant measurements and predicts future system behavior expressed as signatures, i.e., qualitative magnitude changes and higher order time-derivative effects. A comparison of the transient characteristics of the observed variables with the predicted effects helps refine initial fault hypotheses. This allows for quick fault isolation, and circumvents difficulties that arise when interactions caused by feedback and dependent faults. This methodology has been successfully applied to the secondary cooling loop of fast breeder reactors.

#index 1478746
#* Model minimization in Markov decision processes
#@ Thomas Dean;Robert Givan
#t 1997
#c 10
#% 44876
#% 75936
#% 115513
#% 131877
#% 179939
#% 194647
#% 194652
#% 203598
#% 836038
#% 1290041
#% 1476294
#! We use the notion of stochastic bisimulation homogeneity to analyze planning problems represented as Markov decision processes (MDPs). Informally, a partition of the state space for an MDP is said to be homogeneous if for each action, states in the same block have the same probability of being carried to each other block. We provide an algorithm for finding the coarsest homogeneous refinement of any partition of the state space of an MDP. The resulting partition can be used to construct a reduced MDP which is minimal in a well defined sense and can be used to solve the original MDP. Our algorithm is an adaptation of known automata minimization algorithms, and is designed to operate naturally on factored or implicit representations in which the full state space is never explicitly enumerated. We show that simple variations on this algorithm are equivalent or closely similar to several different recently published algorithms for finding optimal solutions to (partially or fully observable) factored Markov decision processes, thereby providing alternative descriptions of the methods and results regarding those algorithms.

#index 1478747
#* Structured solution methods for non-Markovian decision processes
#@ Fahiem Bacchus;Craig Boutilier;Adam Grove
#t 1997
#c 10
#% 75936
#% 100138
#% 101955
#% 103309
#% 179939
#% 179955
#% 363744
#% 1290041
#% 1476293
#! Markov Decision Processes (MDPs), currently a popular method for modeling and solving decision theoretic planning problems, are limited by the Markovian assumption: rewards and dynamics depend on the current state only, and not on previous history. Non-Markovian decision processes (NMDPs) can also be defined, but then the more tractable solution techniques developed for MDP's cannot be directly applied. In this paper, we show how an NMDP, in which temporal logic is used to specify history dependence, can be automatically converted into an equivalent MDP by adding appropriate temporal variables. The resulting MDP can be represented in a structured fashion and solved using structured policy construction methods. In many cases, this offers significant computational advantages over previous proposals for solving NMDPs.

#index 1478748
#* Model decomposition and simulation: a component based qualitative simulation algorithm
#@ Daniel J. Clancy;Benjamin Kuipers
#t 1997
#c 10
#% 1115
#% 1116
#% 132172
#% 152552
#% 166232
#% 214028
#% 231749
#% 375029
#% 1478749
#! Traditionally, qualitative simulation uses a global, state-based representation to describe the behavior of the modeled system. For larger, more complex systems this representation proves extremely inefficient since it provides a complete temporal ordering of all potential distinctions leading to a large, complex behavioral description that obscures relevant distinctions, or even fails to terminate. The model decomposition and simulation algorithm (DecSIM) uses a divide and conquer approach to qualitative simulation. Variables within the system are partitioned into components. Each component is viewed as a separate system and is simulated using a state-based representation limited to the variables within the component. Interactions between components are reasoned about separately. DecSIM provides a promising paradigm for qualitative simulation whose complexity is driven by the complexity of the problem specification rather than the inference mechanism used.

#index 1478749
#* Static and dynamic abstraction solves the problem of chatter in qualitative simulation
#@ Daniel J. Clancy;Benjamin Kuipers
#t 1997
#c 10
#% 1115
#% 1116
#% 109853
#% 166232
#% 231749
#% 1478748
#! One of the major factors hindering the use of qualitative simulation techniques to reason about the behavior of complex dynamical systems is intractable branching due to a phenomenon called chatter. This paper presents two general abstraction techniques that solve the problem of chatter. Eliminating the problem of chatter significantly extends the range of models that can be tractably simulated using qualitative simulation. Chatter occurs when a variable's direction of change is constrained only by continuity within a region of the state space. This results in intractable, potentially infinite branching within the behavioral description due to irrelevant distinctions in the direction of change. While a number of techniques have been proposed to eliminate chatter, none of them provide a general solution that can eliminate all instances of chatter. Chatter box abstraction and dynamic chatter abstraction provide two such solutions to this problem. Both solutions eliminate chatter by abstracting the chattering region of the state space into a single qualitative state with an abstract direction of change. The algorithms differ in the manner in which they identify the chattering region of the state space.

#index 1478750
#* The "Inverse hollywood problem": from video to scripts and storyboards via causal analysis
#@ Matthew Brand
#t 1997
#c 10
#% 457653
#% 457691
#% 592062
#% 627207
#! We address the problem of visually detecting causal events and fitting them together into a coherent story of the action witnessed by the camera. We show that this can be done by reasoning about the motions and collisions of surfaces, using high-level causal constraints derived from psychological studies of infant visual behavior. These constraints are naive forms of basic physical laws governing substantiality, contiguity, momentum, and acceleration. We describe two implementations. One system parses instructional videos, extracting plans of action and key frames suitable for storyboarding. Since learning will play a role in making such systems robust, we introduce a new framework for higher-order hidden Markov models and demonstrate its use in a second system that segments stereo video into actions in near real-time. Rather than attempt accurate low-level vision, both systems use high-level causal analysis to integrate fast but sloppy pixel-based representations over time. The output is suitable for summary, indexing, and automated editing.

#index 1478751
#* Qualitative rigid body mechanics
#@ Thomas F. Stahovich;Randall Davis;Howard Shrobe
#t 1997
#c 10
#% 1119
#% 77823
#% 82637
#% 109857
#% 1275460
#% 1476273
#! We present a theory of qualitative rigid body mechanics and describe a program that uses this theory to compute qualitative dynamic simulations. The program works directly from a qualitative representation of geometry (qc-space). It employs a new qualitative representation for forces that reduces ambiguity in force sums and hence reduces branching.

#index 1478752
#* Integrating a spatial reasoner with a resolution theorem-prover
#@ Thomas R. Ioerger
#t 1997
#c 10
#% 3812
#% 35562
#% 38695
#% 161239
#% 384112
#% 1274719
#! Some spatial reasoning systems use images to solve problems, rather than making formal logical inferences. However, an open question is how to use these systems in contexts where some nonspatial information is also involved. We present a hybrid reasoning method in which we extend the capabilities of a spatial reasoner by integrating it with a resolution theorem-prover. We prove that the hybrid system is refutation-complete, in the sense that, if a domain theory is unsatisfiable, perhaps only because all of its models entail unrealizable images, then our algorithm will halt. We discuss how our approach differs from other hybrid reasoning algorithms in the way it manages the interaction between sub-systems.

#index 1478753
#* Noise, non-determinism and spatial uncertainty
#@ Murray Shanahan
#t 1997
#c 10
#% 22370
#% 236024
#% 1275246
#% 1476284
#! This paper presents a logical account of sensor data assimilation in a mobile robot, based on abduction. Unlike previous work, the present formulation handles sensor noise as well as motor noise. In addition, it incorporates two significant technical advances. The use of determining fluents to deal with nondeterminism obviates the need for a special form of abduction, and the use of uncertain object boundaries alleviates a problem with multiple explanations.

#index 1478754
#* Projective relations for 3D space: computational model, application, and psychological evaluation
#@ Constanze Vorwerg;Gudrun Socher;Thomas Fuhr;Gerhard Sagerer;Gert Rickheit
#t 1997
#c 10
#% 42432
#% 44876
#% 59424
#% 180124
#% 370876
#! We propose a 3D computational model for projective relations which is used in an integrated image and speech understanding system. The image and speech understanding system is being developed within a joint research project focusing on both technical and cognitive aspects of human-computer interaction. Psychological experiments have been carried out to evaluate our computational model as an approximation of the meaning of projective prepositions used by humans in spoken instructions. These experiments investigate the acceptance of the model by subjects as well as the regularities regarding human usage of projective relations. Results of the computational model, the overall system, and the psychological experiments are presented.

#index 1478755
#* A new unification method for temporal reasoning with constraints
#@ Eddie Schwalb
#t 1997
#c 10
#% 21136
#% 36236
#% 56471
#% 107137
#% 135873
#% 212218
#% 216976
#% 231740
#% 319244
#% 1280096
#! In this work we consider using logic programs to perform temporal reasoning. We identify some difficulties of combining constraint propagation and generalized resolution when temporal information is represented using tokens. We show that standard top-down evaluation (i.e. resolution) is incomplete due to the inability to unify constraints and ground terms. We present some syntactic restrictions that enable temporal resolution. Under these restrictions, we propose a new unification method composed of constraint unification and token fusion algorithms. Incorporating them within a generalized resolution scheme render it complete.

#index 1478756
#* Connection based strategies for deciding propositional temporal logic
#@ Subash Shankar;James Slagle
#t 1997
#c 10
#% 1729
#% 6246
#% 31856
#% 72548
#% 288985
#% 467626
#% 556949
#% 1279679
#! Connection methods have proven their value for efficient automated theorem proving in classical logics. However, these methods have not been extended to temporal logics due to the lack of a subformula property in existing proof procedures. We show that a slightly looser generalized subformula property exists for temporal logics. We then exploit this generalized subformula property to develop a temporal notion of polarities and connections, upon which we base an efficient proof procedure for propositional temporal logic. The proof procedure is structured around semantic tableau augmented with connections, and we propose a number of connection-based strategies. The procedure achieves many of the benefits of connection methods. The method is also sufficiently general to be extensible to other temporal logics. Experimental results indicate substantial speedup resulting from this approach.

#index 1478757
#* Bayes networks for estimating the number of solutions to a CSP
#@ Amnon Meisels;Solomon Eyal Shimony;Gadi Solotorevsky
#t 1997
#c 10
#% 36814
#% 44876
#% 68244
#% 136358
#% 443635
#% 1279714
#% 1650810
#! The problem of counting the number of solutions to a constraint satisfaction problem (CSP) is rephrased in terms of probability updating in Bayes networks. Approximating the probabilities in Bayes networks is a problem which has been studied for a while, and may well provide a good approximation to counting the number of solutions. We use a simple approximation based on independence, and show that it is correct for tree-structured CSPs. For other CSPs, it is a less optimistic approximation than those suggested in prior work, and experiments show that it is more accurate on the average. We present empirical evidence that our approximation is a useful search heuristic for finding a single solution to a CSP.

#index 1478758
#* A practical algorithm for finding optimal triangulations
#@ Kirill Shoikhet;Dan Geiger
#t 1997
#c 10
#% 1675
#% 6199
#% 10517
#% 31482
#% 44876
#% 93660
#% 150115
#% 493687
#% 503253
#% 750574
#% 1650763
#% 1650778
#! An algorithm called QUICKTREE is developed for finding a triangulation T of a given undirected graph G such that the size of T's maximal clique is minimum and such that no other triangulation of G is a subgraph of T. We have tested QUICKTREE on graphs of up to 100 nodes for which the maximum clique in an optimal triangulation is of size 11. This is the first algorithm that can optimally triangulate graphs of such size in a reasonable time frame. This algorithm is useful for constraint satisfaction problems and for Bayesian inference through the clique tree inference algorithm.

#index 1478759
#* Interchangeability supports abstraction and reformulation for multi-dimensional constraint satisfaction
#@ Eugene C. Freuder;Daniel Sabin
#t 1997
#c 10
#% 124651
#% 130208
#% 444694
#% 1273663
#! Interchangeability provides a principled approach to abstraction and reformulation of constraint satisfaction problems. Values are interchangeable if exchanging one for the other in any solution produces another solution. Abstracting a problem by simplifying the constraints can increase interchangeability. Multi-dimensional constraint satisfaction problems can provide natural opportunities for this abstraction process. Multi-dimensional problems may involve vectors of values, or conjunctive constraints. Utilizing the interchangeability can permit more efficient solutions of the abstracted problem. These solutions can be expanded into smaller reformulations of the original problem. Solving abstracted and then reformulated problems can be considerably more efficient than solving the original problems. We provide data that demonstrates the potential of this abstraction/reformulation process for multi-dimensional problems, and illuminates how its utility can depend on natural problem parameters.

#index 1478760
#* Exploiting symmetry in lifted CSPs
#@ David Joslin;Amitabha Roy
#t 1997
#c 10
#% 57944
#% 126397
#% 484962
#% 560400
#% 704599
#% 1268731
#% 1273663
#% 1476297
#% 1476298
#! When search problems have large-scale symmetric structure, detecting and exploiting that structure can greatly reduce the size of the search space. Previous work has shown how to find and exploit symmetries in propositional encodings of constraint satisfaction problems (CSPs). Here we consider problems that have more compact "lifted" (quantified) descriptions from which propositional encodings can be generated. We describe an algorithm for finding symmetries in lifted representations of CSPs, and show sufficient conditions under which these symmetries can be mapped to symmetries in the propositional encoding. Using two domains (pigeonhole problems, and a CSP encoding of planning problems), we demonstrate experimentally that the approach of finding symmetries in lifted problem representations is a significant improvement over previous approaches that find symmetnes in propositional encodings.

#index 1478761
#* Using CSP look-back techniques to solve real-world SAT instances
#@ Roberto J. Bayardo, Jr.;Robert C. Schrag
#t 1997
#c 10
#% 160253
#% 160270
#% 179960
#% 198885
#% 210195
#% 327779
#% 1476298
#% 1499510
#! We report on the performance of an enhanced version of the "Davis-Putnam" (DP) proof procedure for propositional satisfiability (SAT) on large instances derived from real-world problems in planning, scheduling, and circuit diagnosis and synthesis. Our results show that incorporating CSP look-back techniques -- especially the relatively new technique of relevance-bounded learning -- renders easy many problems which otherwise are beyond DP's reach. Frequently they make DP, a systematic algorithm, perform as well or better than stochastic SAT algorithms such as GSAT or WSAT. We recommend that such techniques be included as options in implementations of DP, Just as they are in systematic algorithms for the more general constraint satisfaction problem.

#index 1478762
#* Using branch-and-bound with constraint satisfaction in optimization problems
#@ Stephen Beale
#t 1997
#c 10
#% 308
#% 3463
#% 320265
#% 375029
#% 408680
#% 1476278
#! This work integrates three related AI search techniques - constraint satisfaction, branch-and-bound and solution synthesis - and apphes the result to constraint satisfaction problems for which optimal answers are required. This method has already been shown to work well in natural language semantic analysis (Beale, et al, 1996); here we extend the domain to optimizing graph coloring problems, which are abstractions of many common scheduling problems of interest. We demonstrate that the methods used here allow us to determine optimal answers to many types of problems without resorting to heuristic search, and, furthermore can be combined with heuristic search methods for problems with excessive complexity.

#index 1478763
#* Detecting unsatisfiable CSPs by coloring the micro-structure
#@ Daya Ram Gaur;W. Ken Jackson;William S. Havens
#t 1997
#c 10
#% 131561
#% 160208
#% 160389
#% 319789
#% 320265
#% 408396
#% 477221
#% 535336
#% 1499497
#% 1499502
#! Constraint satisfaction research has focussed on consistency checking using k-consistency ilnd its variations such as arc-consistency, and path-consistency. We define a new form of consistency checking that is based on coloring the micro-structure graph of a constraint satisfaction problem (CSP). In our formulation, if the micro-structure graph of a CSP with n variables can be colored with n - 1 colors then the problem is unsatisfiable. This new notion of consistency-by-coloring is compared to arc-consistency. We provide examples that show that neither arc-consistency. nor consistency-by-coloring is more powerful than the other in a theoretical sense. We also describe the results of preliminary computational experiments that compare consistency-by-coloring and arc-consistency.

#index 1478764
#* Problem structure in the presence of perturbations
#@ Carla P. Gomes;Bart Selman
#t 1997
#c 10
#% 41220
#% 179689
#% 210191
#% 210195
#% 1273544
#% 1275297
#% 1279714
#% 1290158
#! Recent progress on search and reasoning procedures has been driven by experimentation on computationally hard problem instances. Hard random problem distributions are an important source of such instances. Challenge problems from the area of finite algebra have also stimulated research on search and reasoning procedures. Nevertheless, the relation of such problems to practical applications is somewhat unclear. Realistic problem instances clearly have more structure than the random problem instances, but, on the other hand, they are not as regular as the structured mathematical problems. We propose a new benchmark domain that bridges the gap between the purely random instances and the highly structured problems, by introducing perturbations into a structured domain. We will show how to obtain interesting search problems in this manner, and how such problems can be used to study the robustness of search control mechanisms. Our experiments demonstrate that the performance of search strategies designed to mimic direct constructive methods degrade surprisingly quickly in the presence of even minor perturbations.

#index 1478765
#* Model-theoretic semantics and tractable algorithm for CNF-BCP
#@ Rahul Roy-Chowdhury;Mukesh Dalal
#t 1997
#c 10
#% 167190
#% 384112
#% 601159
#! CNF-BCP is a well-known propositional reasoner that extends clausal Boolean Constraint Propagation (BCP) to non-clausal theories. Although BCP has efficient linear-time implementations, CNF-BCP requires clausal form transformation that sometimes leads to an exponential increase in the size of a theory. We present a new quadratic-time reasoner, RFP, that infers exactly the same literals as CNF-BCP. Although CNF-BCP has been specified only syntactically, we present a simple model-theoretic semantics for RFP. We also present a convergent term-rewriting system for RFP that is suitable for reasoning with knowledge bases that are built incrementally. Potential applications of RFP include logical truth-maintenance systems and general-purpose knowledge representation systems.

#index 1478766
#* Beyond contention: extending texture-based scheduling heuristics
#@ J. Christopher Beck;Andrew J. Davenport;Edward M. Sitarski;Mark S. Fox
#t 1997
#c 10
#% 63222
#% 73996
#% 111050
#% 188076
#% 408396
#% 1273314
#% 1273569
#% 1275306
#% 1478767
#! In order to apply texture measurement based heuristic commitment techniques beyond the unary capacity resource constraints of job shop scheduling, we extend the contention texture measurement to a measure of the probability that a constraint will be broken. We define three methods for the estimation of this probability and show that they perform as well or better than existing heuristics on job shop scheduling problems. Empirical insight into the performance is provided and we sketch how we have extended probability-based heuristics to more complicated scheduling constraints.

#index 1478767
#* Texture-based heuristics for scheduling revisited
#@ J. Christopher Beck;Andrew J. Davenport;Edward M. Sitarski;Mark S. Fox
#t 1997
#c 10
#% 43515
#% 63222
#% 111050
#% 188076
#% 194657
#% 217501
#% 408396
#% 488955
#% 1273314
#% 1273569
#% 1275306
#% 1478766
#! Recent scheduling work has challenged the need for sophisticated heuristics such as those based on texture measurements. This paper examines these claims in the light of advances in scheduling technology. We compare a number of current heuristic commitment techniques against a texture-based heuristic. Our results demonstrate that texture-based heuristics can outperform these widely-used heuristic commitment techniques.

#index 1478768
#* Query optimization using local completeness
#@ Oliver M. Duschka
#t 1997
#c 10
#% 36181
#% 137873
#% 164369
#% 198465
#% 198466
#% 237190
#% 289266
#% 296931
#% 464203
#% 481786
#% 481923
#% 571216
#% 599549
#% 1499471
#! We consider the problem of query plan optimization in information brokers. Information brokers are programs that facilitate access to collections of information sources by hiding source-specific peculiarities and presenting uniform query interfaces. It is unrealistic to assume that data stored by information sources is complete. Therefore, current implementations of information brokers query all possibly relevant information sources in order not to miss any answers. This approach is very costly. We show how a weaker form of completeness, local completeness, can be used to minimize the number of accesses to information sources.

#index 1478769
#* Template-based information mining from HTML documents
#@ Jane Yung-jen Hsu;Wen-tau Yih
#t 1997
#c 10
#% 55490
#% 161754
#% 216509
#% 240955
#% 442840
#% 1290175
#! Tools for mining information from data can create added value for the Internet. As the majority of electronic documents available over the network are in unstructured textual form, extracting useful information from a document usually involves information retrieval techniques or manual processing. This paper presents a novel approach to mining information from HTML documents using tree-structured templates. In addition to syntactic and semantic descriptions, each template is designed to capture the logical structure of a class of documents. Experiments have been conducted to extract FAQ information automatically from over one hundred HTML documents collected from the Web. Using two basic templates, the prototype FAQ Miner has accurately analyzed 65% of the collection of FAQ documents. With additional processing to handle "near-pass" es, the success rate is approximately 75%. The preliminary results have demonstrated the utility of structural templates for mining information from semi-structured text-based documents.

#index 1478770
#* Local search algorithms for partial MAXSAT
#@ Byungki Cha;Kazuo Iwama;Yahiko Kambayashi;Shuichi Miyazaki
#t 1997
#c 10
#% 95582
#% 116559
#% 593704
#% 1275266
#% 1290158
#% 1476298
#% 1499515
#% 1499516
#! MAXSAT solutions, i.e., near-satisfying assignments for propositional formulas, are sometimes meaningless for real-world problems because such formulas include "mandatory clauses" that must be all satisfied for the solution to be reasonable. In this paper, we introduce Partial MAXSAT and investigate how to solve it using local search algorithms. An instance of Partial MAXSAT consists of two formulas fA and fB, and its solution must satisfy all clauses in fA and as many clauses in fB as possible. The basic idea of our algorithm is to give weight to fA-clauses (the mandatory clauses) and then apply local search. We face two problems; (i) what amount of weight is appropriate and (ii) how to deal with the common action of local search algorithms, giving weight to clauses for their own purpose, which will hide the initial weight as the algorithms proceed.

#index 1478771
#* Solving linear pseudo-Boolean constraint problems with local search
#@ Joachim P. Walser
#t 1997
#c 10
#% 36698
#% 95582
#% 126390
#% 147584
#% 149628
#% 160270
#% 1268742
#% 1476298
#% 1499519
#! Stochastic local search is one of the most successful methods for model finding in propositional satisfiability. However, many combinatorial problems have no concise propositional encoding. In this paper, we show that domain-independent local search for satisfiability (Walksat) can be generalized to handle systems of linear pseudo-Boolean (0-1 integer) constraints, a representation that is widely used in operations research. We introduce the algorithm WSAT (PB) and demonstrate its potential in two case studies. The first application is an optimization problem from radar surveillance. Experiments on problems of realistic size show that WSAT (PB) is an efficient heuristic to find good approximate solutions. For most of the test problems, it found provably optimal solutions. In the second case study, we show that pseudo-Boolean local search can efficiently solve the progressive party problem, a problem that is hard for constraint programming with chronological backtracking, and whose 0-1 encoding is beyond the size limitations of integer linear programming.

#index 1478772
#* Variable-selection heuristics in local search for SAT
#@ Alex S. Fukunaga
#t 1997
#c 10
#% 193810
#% 210195
#% 288165
#% 1268732
#% 1273577
#! One of the important components of a local search strategy for satisfiability testing is the variable selection heuristic, which determines the next variable to be flipped. In a greedy local search such as GSAT, the major decision in variable selection is the strategy for breaking ties between variables that offer the same improvement in the number of unsatisfied clauses. In this paper, we analyze a number of tie-breaking strategies for GSAT and evaluate the strategies empirically using randomly generated 3-SAT instances from a hard distribution of random instances. We find that the property of fairness, which was proposed in the literature as being the critical property of a successful variable strategy, is not a sufficient property, and show that randomness plays a significant role in the success of variable selection heuristics.

#index 1478773
#* Tabu search for SAT
#@ Bertrand Mazure;Lakhdar Saïs;Éric Grégoire
#t 1997
#c 10
#% 41220
#% 92788
#% 116559
#% 288165
#% 601159
#% 1279714
#! In this paper, tabu search for SAT is investigated from an experimental point of view. To this end, TSAT, a basic tabu search algonthm for SAT, is introduced and compared With Selman et al. Random Walk Strategy GSAT procedure, in short RWS-GSAT. TSAT does not involve the additional stochastic process of RWS-GSAT. This should facilitate the understanding of why simple local search methods for SAT work. It is shown that the length of the tabu list plays a critical role in the performance of the algorithm. Moreover surprising properties about the (experimental) optimal length of the tabu list are exhibited, raising interesting issues about the nature of hard random SAT problems.

#index 1478774
#* Models of continual computation
#@ Eric Horvitz
#t 1997
#c 10
#% 68244
#% 101225
#% 103309
#% 109043
#% 110379
#% 136358
#% 527850
#% 1650660
#! Automated problem solving is viewed typically as the expenditure of computation to solve one or more problems passed to a reasoning system. In response to each problem received, effort is applied to generate a solution and problem solving ends when the solution is rendered. We discuss the notion of continual computation that addresses a broader conception of problem by considering the ideal use of the idle time between problem instances. The time is used to develop solutions proactively to one or more expected challenges in the future. We consider analyses for traditional all-or-nothing algorithms as well as more flexible computational procedures. After exploring the allocation of idle time for several settings, we generalize the analysis to consider the case of shifting computation from a current problem to solve future challenges. Finally, we discuss a sample application of the use of continual computation in the setting of diagnostic reasoning.

#index 1478775
#* Complex goal criteria and its application in design-to-criteria scheduling
#@ Thomas Wagner;Alan Garvey;Victor Lesser
#t 1997
#c 10
#% 677329
#% 677345
#% 677350
#% 1273446
#! Difficult real-time AI problems require a means for expressing multi-dimensional and dynamic goal criteria and a principled model for satisficing to best meet the criteria. In the context of the Design-to-Criteria task scheduling paradigm, we define a new general client specification metaphor for describing such complex goal criteria or utility attributes, and couple it with a principled evaluation model for using the criteria. The criteria specification and corresponding evaluation mechanism are used throughout the Design-to-Criteria scheduling process to focus scheduling activities on solutions and partial solutions that are most likely to meet the criteria, i.e., to result in the focused production of custom satisficing schedules. Examples of the power of the approach at reducing the complexity of the scheduling task and designing custom satisficing schedules, are shown.

#index 1478776
#* Effective redundant constraints for online scheduling
#@ Lise Getoor;Greger Ottosson;Markus Fromherz;Björn Carlson
#t 1997
#c 10
#% 56471
#% 168288
#% 172500
#% 359711
#% 1273569
#! The use of heuristics as a means to improve constraint solver performance has been researched widely. However, most work has been on problem-independent heuristics (e.g., variable and value ordering), and has focused on offline problems (e.g., one-shot constraint satisfaction). In this paper, we present an online scheduling problem for which we are developing a real-time scheduling algorithm. While we can and do use generic heuristics in the scheduler, here we focus on the use of domain-specific redundant constraints to effectively approximate optimal offline solutions. We present a taxonomy of redundant domain constraints, and examine their impact on the effectiveness of the scheduler. We also describe several techniques for generating redundant constraints, which can be applied to a large class of job shop scheduling problems.

#index 1478777
#* Stochastic procedures for generating feasible schedules
#@ Angelo Oddi;Stephen F. Smith
#t 1997
#c 10
#% 107137
#% 124593
#% 179958
#% 1499506
#! In this paper, we investigate the use of stochastic variable and value ordering heuristics for solving job shop scheduling problems with non-relaxable deadlines and complex metric constraints. Previous research in constraint satisfaction scheduling has developed highly effective, deterministic heuristics for this class of problems based on simple measures of temporal sequencing flexibility. However, they are not infallible, and the possibility of search failure raises the issue of how to most productively enlarge the search. Backtracking is one alternative, but such systematicity generally implies high computational cost. We instead design an iterative sampling procedure, based on the intuition that it is more productive to deviate from heuristic advice in cases where the heuristic is less informed, and likewise better to follow the heuristic in cases where it is more knowledgeable. We specify stochastic counterparts to previously developed search heuristics, which are parameterized to calibrate degree of randomness to level of discriminatory power. Experimental results on job shop scheduling CSPs of increasing size demonstrate comparative advantage over chronological backtracking. Comparison is also made to another, recently proposed iterative sampling technique called heuristic-biased stochastic sampling (HBSS). Whereas HBSS assumes a statically specified heuristic bias that is utilized at every application of the heuristic, our approach defines bias dynamically according to how well the heuristic discriminates alternatives.

#index 1478778
#* The scaling of search cost
#@ Ian P. Gent;Ewan MacIntyre;Patrick Prosser;Toby Walsh
#t 1997
#c 10
#% 210195
#% 216995
#% 327779
#% 534165
#% 1275263
#% 1279714
#% 1499502
#% 1499519
#! We show that a rescaled constrainedness parameter provides the basis for accurate numerical models of search cost for both backtracking and local search algorithms. In the past, the scaling of performance has been restricted to critically constrained problems at the phase transition. Here, we show how to extend models of search cost to the full width of the phase transition. This enables the direct comparison of algorithms on both under-constrained and over-constrained problems. We illustrate the generality of the approach using three different problem domains (satisfiability, constraint satisfaction and travelling salesperson problems) with both backtracking algorithms like the Davis-Putnam procedure and local search algorithms like GSAT. As well as modelling data from experiments, we give accurate predictions for results beyond the range of the experiments.

#index 1478779
#* Evidence for invariants in local search
#@ David McAllester;Bart Selman;Henry Kautz
#t 1997
#c 10
#% 11720
#% 44982
#% 95673
#% 149626
#% 149628
#% 160270
#% 1476298
#% 1499519
#! It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies.

#index 1478780
#* Summarizing CSP hardness with continuous probability distributions
#@ Daniel Frost;Irina Rish;Lluís Vila
#t 1997
#c 10
#% 15245
#% 160251
#% 167643
#% 175378
#% 289332
#% 321639
#% 327779
#% 520424
#! We present empirical evidence that the distribution of effort required to solve CSPs randomly generated at the 50% satisfiable point, when using a backtracking algorithm, can be approximated by two standard families of continuous probability distribution functions. Solvable problems can be modelled by the Weibull distribution, and unsolvable problems by the lognormal distribution. These distributions fit equally well over a variety of backtracking based algorithms.

#index 1478781
#* Exploiting the deep structure of constraint satisfaction problems with quantum computers
#@ Tad Hogg
#t 1997
#c 10
#% 154465
#% 175367
#% 210191
#% 210195
#% 214114
#% 270754
#% 270757
#% 1081278
#% 1272281
#% 1499502
#! The deep structure of constraint satisfaction problems explains the association of hard search instances with a phase transition in problem solubility. This structure is also the basis of a quantum search algoritbm exhibiting the phase transition. In this paper, this algoritbm is modified to incorporate additional problem structure. This modification is an example of a general method for including heuristics in quantum search. The new algoritbm is evaluated empirically for random 3SAT, illustrating how quantum searches can benefit from using problem structure, on average.

#index 1478782
#* Clustering at the phase transition
#@ Andrew J. Parkes
#t 1997
#c 10
#% 24546
#% 160270
#% 167643
#% 175378
#% 210195
#% 1279714
#% 1499502
#% 1499504
#% 1499519
#! Many problem ensembles exhibit a phase transition that is associated with a large peak in the average cost of solving the problem instances. However, this peak is not necessarily due to a lack of solutions: indeed the average number of solutions is typically exponentially large. Here, we study this situation within the context of the satisfiability transition in Random 3SAT. We find that a significant subclass of instances emerges as we cross the phase transition. These instances are characterized by having about 85-95% of their variables occurring in unary prime implicates (UPIs), with their remaining variables being subject to few constraints. In such instances the models are not randomly distributed but all lie in a cluster that is exponentially large, but still admits a simple description. Studying the effect of UPIs on the local search algorithm WSAT shows that these "single-cluster" instances are harder to solve, and we relate their appearance at the phase transition to the peak in search cost.

#index 1478783
#* Realtime generation of customized 3D animated explanations for knowledge-based learning environments
#@ William H. Bares;James C. Lester
#t 1997
#c 10
#% 21734
#% 31687
#% 47909
#% 131759
#% 145639
#% 152767
#% 202046
#% 240953
#% 243112
#% 1290062
#% 1478924
#% 1499486
#% 1499487
#% 1499527
#% 1499529
#! Rich 3D animated explanations can have a powerful impact on students interacting with immersive knowledge-based learning environments. By generating 3D animated explanations in realtime, a learning environment can create engaging explanations that are tailored to individual students. This paper presents the immersive explanation planning framework for generating pedagogically-customized 3D animated explanations in realtime. In this framework, an explanation system selects 3D models and their relevant behaviors, creates camera shots that most clearly depict complex phenomena, constructs a tern poral organization that synchronizes narrative utterances with visual elements, plans the movement of the virtual camera that "films" the explanation, and incorporates specialized visual effects to focus students' attention on the most salient concepts. The framework has been implemented in RAPID, an explanation system that plans and renders customized 3D animated explanations of dynamic phenomena in realtime. Results of a focus group evaluation of RAPID are encouraging.

#index 1478784
#* The sounds of silence: towards automated evaluation of student learning in a reading tutor that listens
#@ Jack Mostow;Gregory Aist
#t 1997
#c 10
#% 179878
#% 194568
#! We propose a paradigm for ecologically valid, authentic, unobtrusive, automatic, data-rich, fast, robust, and sensitive evaluation of computer-assisted student performance. We instantiate this paradigm in the context of a Reading Tutor that listens to children read aloud, and helps them. We introduce interword latency as a simple prosodic measure of assisted reading performance. Finally, to validate the measure and analyze performance improvement, we report initial experimental results from the first extended in-school deployment of the Reading Tutor.

#index 1478785
#* Presenting and analyzing the results of ai experiments: data averaging and data snooping
#@ C. Lee Giles;Steve Lawrence
#t 1997
#c 10
#% 106561
#% 132927
#% 188076
#% 204708
#% 376589
#! Experimental results reported in the machine learning AI literature can be misleading. This paper investigates the common processes of data averaging (reporting results in terms of the mean and standard deviation of the results from multiple trials) and data snooping in the context of neural networks, one of the most popular AI machine learning models. Both of these processes can result in misleading results and inaccurate conclusions. We demonstrate how easily this can happen and propose techniques for avoiding these very important problems. For data averaging, common presentation assumes that the distribution of individual results is Gaussian. However, we investigate the distribution for common problems and find that it often does not approximate the Gaussian distribution, may not be symmetric, and may be multimodal. We show that assuming Gaussian distributions can significantly affect the interpretation of results, especially those of comparison studies. For a controlled task, we find that the distribution of performance is skewed towards better performance for smoother target functions and skewed towards worse performance for more complex target functions. We propose new guidelines for reporting performance which provide more information about the actual distribution (e.g. box-whiskers plots). For data snooping, we demonstrate that optimization of performance via experimentation with multiple parameters can lead to significance being assigned to results which are due to chance. We suggest that precise descriptions of experimental techniques can be very important to the evaluation of results, and that we need to be aware of potential data snooping biases when formulating these experimental techniques (e.g. selecting the test procedure). Additionally, it is important to only rely on appropriate statistical tests and to ensure that any assumptions made in the tests are valid (e.g. normality of the distribution).

#index 1478786
#* Building concept representations from reusable components
#@ Peter Clark;Bruce Porter
#t 1997
#c 10
#% 16343
#% 105991
#% 108685
#% 109848
#% 116299
#% 121903
#% 231749
#% 556363
#% 679521
#% 741059
#% 1478796
#! Our goal is to build knowledge-based systems capable of answering a wide variety of questions, including questions that are unanticipated when the knowledge base is built. For systems to achieve this level of competence and generality, they require the ability to dynamically construct new concept representations, and to do so in response to the questions arLd tasks posed to them. Our approach to meeting this requirement is to build knowledge bases of generalized, representational components, and to develop methods for automatically composing components on demand. This work extends the normal inheritance approach used in frame-based systems, and imports ideas from several different areas of AI, in particular compositional modeling, terminological reasoning, and ontological engineering. The contribution of this work is a novel integration of these methods that improves the efficiency of building knowledge bases and the robustness of using them.

#index 1478787
#* A script-based approach to modifying knowledge bases
#@ Yolanda Gil;Marcelo Tallis
#t 1997
#c 10
#% 2303
#% 55936
#% 105501
#% 134111
#% 156338
#% 161241
#% 176991
#% 179741
#% 197428
#% 198076
#% 1499535
#% 1499589
#! Our goal is to build knowledge acquisition tools that support users in modifying knowledge-based systems. These modifications may require several individual changes to various components of the knowledge base, which need to be carefully coordinated to prevent users from leaving the knowledge-based system in an unusable state. This paper describes an approach to building knowledge acquisition tools which capture knowledge about commonly occurring modification sequences and support users in completing the modifications they start. These sequences, which we call KA Scripts, relate individual changes and the effects that they have on the knowledge base. We discuss our experience in designing and compiling a library of KA Scripts. We also describe the implementation of a tool that uses them and our preliminary evaluations that demonstrate their usability.

#index 1478788
#* Representing sequences in description logics
#@ Haym Hirsh;Daniel Kudenko
#t 1997
#c 10
#% 58347
#% 79502
#% 153005
#% 289010
#% 556363
#% 1268741
#! Representing and manipulating sequences in description logics (DLs) has typically been achieved through the use of new sequence-specific operators or by relying on host-language functions. This paper shows that it is not necessary to add additional features to a DL to handle sequences, and instead describes an approach for dealing with sequences as first-class entities directly within a DL without the need for extensions or extra-linguistic constructs. The key idea is to represent sequences using suffix trees, then represent the resulting trees in a DL using traditional (tractable) concept and role operators. This approach supports the representation of a variety of information about a sequence, such as the locations and numbers of occurrences of all subsequences of the sequence. Moreover, subsequence testing and pattern matching reduce to subsumption checking in this representation, while computing the least common subsumer of two terms supports the application of inductive learning to sequences.

#index 1478789
#* P-CLASSIC: a tractable probablistic description logic
#@ Daphne Koller;Alon Levy;Avi Pfeffer
#t 1997
#c 10
#% 44876
#% 61219
#% 89958
#% 90371
#% 108697
#% 147677
#% 213437
#% 461605
#% 503345
#% 527848
#% 1268741
#% 1478844
#% 1499471
#% 1650675
#! Knowledge representation languages invariably reflect a trade-off between expressivity and tractability. Evidence suggests that the compromise chosen by description logics is a particularly successful one. However, description logiC (as for all vanants of first-order logic) is severely limited in its ability to express uncertainty. In this paper, we present P-CLASSIC, a probabilistic version of the description logiC CLASSIC. In addition to teoninological knowledge, the language utilizes Bayesian networks to express uncertainty about the basic properties of an individual, the number of fillers for its roles, and the properties of these fillers. We provide a semantics for P-CLASSIC and an effective inference procedure for probabilistic subsumption: computing the probability that a random individual in class C is also in class D. The effectiveness of the algorithm relies on independence assumptions and on our ability to execute lifted inference: reasoning about similar individuals as a group rather than as separate ground teons. We show that the complexity of the inference algorithm is the best that can be hoped for in a language that combines description logic with Bayesian networks. In particular, if we restrict to Bayesian networks that support polynomial time inference, the complexity of our inference procedure is also polynomial time.

#index 1478790
#* A reflective proof system for reasoning in contexts
#@ Pierre E. Bonzon
#t 1997
#c 10
#% 127670
#% 138676
#% 160385
#% 197272
#% 474440
#% 1273614
#% 1499554
#% 1499557
#% 1907872
#% 1907875
#! We consider the problem of building an automated proof system for reasoning in contexts. Towards that goal, we first define a language of contextual implications, and give its operational semantics under the form of a natural deduction system using explicit context assertions. We show that this proof system has an equivalent straightforward logic program, which in tum can be reified, i.e. defined as an outer meta-level context, and thus applied to itself. More powerful reasoning models (e.g. those involving theory lifting) can be then implemented by applying the same logic program on extended meta-level contexts containing specialized axioms. As a theoretical application, we consider the task of concept learning. In order to achieve generality (Le. abstracting solution classes from problem instances), we argllle that concept learning goals should aim at the discovery of meta-level operators representing the sequence of inference steps leading to object-level moves or actions. We illustrate: this idea with the definition of a learning model based on partial deduction with respect to theory lifting.

#index 1478791
#* Obvious properties of computer programs
#@ Robert Givan
#t 1997
#c 10
#% 32321
#% 70391
#% 130853
#% 135539
#% 289245
#% 560416
#% 560915
#! We explore the question of what properties of LISP programs can be made "obvious" to a computer system. We present a polynomial-time algorithm for inferring interesting properties of pure LISP programs. Building on previous work in knowledge representation for rapid inference, we present a language for representing properties of programs. We treat properties as generalized types, i.e., sets of program values. The property language is expressive enough to represent any RE set of LISP values as a property, and can naturally represent a wide variety of useful properties. We then use a general technique to construct a polynomial-time property inference relation and use type-inference style program analysis to integrate this relation into an algorithm for inferring properties of programs. This algorithm is intended to work in the context of a library of background information, most of which is typically derived from previous runs of the algorithm. Due to the expressive representation system, no algorithm can infer every valid property-- so instead of proving completeness we show our algorithm's usefulness by giving examples of properties inferred. These examples include that insertion sort returns a sorted permutation of its input, and that a clique finding program correctly returns a clique.

#index 1478792
#* Applications of rule-base coverage measures to expert system evaluation
#@ Valerie Barr
#t 1997
#c 10
#% 60463
#% 286774
#% 318012
#% 703208
#% 1274686
#! Often a rule-based system is tested by checking its performance on a number of test cases with known solutions, modifying the system until it gives the correct results for all or a sufficiently high proportion of the test cases. This method cannot guarantee that the rule-base has been adequately or completely covered during the testing process. We introduce an approach to testing of rule-based systems which uses coverage measures to guide and evaluate the testing process. In addition, the coverage measures can be used to assist rule-base pruning and identification of class dependencies, and serve as the foundation for a set of test data selection heuristics. We also introduce a complexity metric for rule-bases.

#index 1478793
#* Detecting redundant production rules
#@ James G. Schmolze;Wayne Snyder
#t 1997
#c 10
#% 2079
#% 3460
#% 20561
#% 25470
#% 53400
#% 73005
#% 87369
#% 101945
#% 162637
#% 162638
#% 177010
#% 289275
#% 377066
#% 442922
#% 560763
#% 618439
#! We present a general method for detecting redundant production rules based upon a term rewrite semantics. We present the semantic account, define rule execution over both ground memories and memory schemas, and define redundancy for production rules. From those definitions, an algorithm is developed that detects redundant rules, and which improves upon previously published methods.

#index 1478794
#* A comparison of two approaches to splitting default theories
#@ Grigoris Antoniou
#t 1997
#c 10
#% 171033
#% 175371
#% 499496
#% 544418
#% 1290093
#% 1499558
#% 1499561
#! Default logic is computationally expensive. One of the most promising ways of easing this problem and developing powerful implementations is to split a default theory into smaller parts and compute extensions in a modular, "local" way. This paper compares two recent approaches, Turner's splitting and Cholewinski's stratification. It shows that the approaches are closely related - in fact the former can be viewed as a special case of the latter.

#index 1478795
#* Reasoning with minimal belief and negation as failure: algorithms and complexity
#@ Riccardo Rosati
#t 1997
#c 10
#% 1146
#% 77167
#% 101922
#% 130784
#% 147513
#% 150822
#% 163716
#% 175359
#% 383293
#% 1273694
#% 1279721
#% 1290094
#! We study the computational properties of the propositional fragment of MBNF, the logic of minimal belief and negation as failure introduced by Lifschitz, which can be considered as a unifying framework for several nonmonotonic formalisms, including default logic, autoepistemic logic, circumscription, epistemic queries and logic programming. We characterize the complexity and provide algorithms for reasoning in propositional MBNF. In particular, we show that skeptical entailment in propositional MBNF is Π3p-complete, hence, it is harder than reasoning in all the above mentioned propositional formalisms for nonmonotonic reasoning. We also prove the exact correspondence between negation as failure in MBNF and negative introspection in Moore's autoepistemic logic.

#index 1478796
#* Tools for assembling modular ontologies in ontolingua
#@ Richard Fikes;Adam Farquhar;James Rice
#t 1997
#c 10
#% 214666
#! The Ontolingua ontology development environment provides a suite of ontology authoring tools and a library of modular, reusable ontologies. The environment is available as a World Wide Web service and has a substantial user community. The tools in Ontolingua are oriented toward the authoring of ontologies by assembling and extending ontologies obtained from a library. In this paper, we describe Ontolingua's formalism for combining the axioms, definitions, and words (non-logical symbols) of multiple ontologies. We also describe Ontolingua's facilities that enable renaming of words from multiple component ontologies and that provide unambiguous mapping between words and text strings during input and output. These features support cyclic inclusion graphs and enable users to extend ontologies in multiple ways such as adding simplifying assumptions and extending the domains of polymorphic operators.

#index 1478797
#* Efficient management of very large ontologies
#@ Kilian Stoffel;Merwyn Taylor;Jim Hendler
#t 1997
#c 10
#% 2298
#% 162999
#% 170808
#% 179610
#% 179876
#% 214900
#% 219283
#% 232154
#% 405391
#% 1268741
#% 1275324
#! This paper describes an environment for supporting very large ontologies. The system can be used on smgle PCs, workstations, a cluster of workstations, and high-end parallel supercomputers. The architecture of the system uses the secondary storage of a relational data base system, efficient memory management, and (optionally) parallelism. This allows us to answer complex queries in very large ontologies in a few seconds on a single processor machine and in fractions of a second on parallel super computers. The main contribution of our approach is the open architecture of the system on both the hardware and the software levels allowing us easily to translate existing ontologies for our system's use, and to port the system to a wide range of platforms.

#index 1478798
#* Beyond minimizing change
#@ Tom Costello
#t 1997
#c 10
#% 3035
#% 39262
#% 107123
#% 181391
#% 242576
#% 1274639
#% 1290151
#! We introduce a new methodology for comparing nonmonotonic treatments of change. We consider the elaboration tolerance of various proposals. The elaboration tolerance of a non-monotonic approach is defined as the elaborations, or changes, that can be made to the non-monotonic consequences, by conjoining on new information. The standard problem, the frame assumption, is capturing the tendency of properties to persist over time. We show that almost all approaches allow new effects to be added, and preconditions to be dropped. There are other ways of describing the world, and we investigate one in particular, assuming there are as few preconditions for an action as possible. This is equivalent to assuming that actions change properties as often as possible, if they ever change that property. We show that this assumption is in conflict with the usual frame assumption. We show that this methodology allows new effects to be added, and preconditions to be added. We show that this precondition assumption is naturally opposite to the frame assumption. We then show that this assumption can be naturally captured in a similar way to the frame problem, using circumscription.

#index 1478799
#* Adding knowledge to the action description language A
#@ Jorge Lobo;Gisela Mendez;Stuart R. Taylor
#t 1997
#c 10
#% 1476290
#! We introduce Ak an extension of the action description language A (Gelfond & Lifschitz 1993) to handle actions which affect knowledge. We use sensing actions to increase an agent's knowledge of the world and non-deterministic actions to remove knowledge. We include complex plans involving conditionals and loops in our query language for hypothetical reasoning. Finally, we present a translation of descriptions in Ak to epistemic logic programs.

#index 1478800
#* Causal theories of action and change
#@ Norman McCain;Hudson Turner
#t 1997
#c 10
#% 38672
#% 39262
#% 89961
#% 131357
#% 1290152
#% 1290153
#% 1290154
#% 1290157
#! For many commonsense reasoning tasks associated with action domains, only a relatively simple kind of causal knowledge (previously studied by Geffner and Lin) is required. We define a mathematically simple language for expressing knowledge of this kind and describe a general approach to formalizing action domains in it. The language can be used to express ramification and qualification constraints, explicit definitions, concurrency, nondeterminism, and dynamic domains in which things change by themselves.

#index 1478801
#* Qualified ramifications
#@ Michael Thielscher
#t 1997
#c 10
#% 26351
#% 46270
#% 89961
#% 92771
#% 224765
#% 1272572
#% 1275439
#! We consider the problem of ramifications, i.e., indirect effects of actions, having exceptions. It is argued that straightforward minimization of abnormality is insufficient in this context. Taking a recent causality-based solution to the plain Ramification Problem as starting point, we develop an action theory that is shown to successfully address this amalgamation of Ramification and Qualification Problem.

#index 1478802
#* Ordered semantic hyper linking
#@ David A. Plaisted;Yunshan Zhu
#t 1997
#c 10
#% 5363
#% 27036
#% 288165
#% 288529
#% 384112
#% 420634
#% 560253
#% 560572
#% 560748
#% 1305150
#! In this paper, we present a novel first order theorem proving strategy - ordered semantic hyper linking. Ordered semantic hyper linking (OSHL) is an instance-based refutational theorem proving strategy. It is sound and complete. OSHL has an efficient propositional decision procedure. It solves first order problems by reducing them to propositional problems. It uses natural semantics of an input problem to guide its search. It also incorporates term rewriting to handle equality. The propositional efficiency, semantic guidance and equality support allow OSHL to solve problems that are difficult for many other strategies. The efficiency of OSHL is supported by experimental study as well as complexity analysis.

#index 1478803
#* Extending the regular restriction of resolution to non-linear subdeductions
#@ Bruce Spencer;J. D. Horton
#t 1997
#c 10
#% 45220
#% 145147
#% 229081
#% 288366
#% 384112
#% 560754
#! A binary resolution proof, represented as a binary tree, is irregular if some atom is resolved away and reappears on the same branch. We develop an algorithm, linear in the size of the tree, which detects whether reordering the resolutions in a given proof will generate an irregular proof. If so, the given proof is not minimal. A deduction system that keeps only minimal proofs retains completeness. We report on an initial implementation.

#index 1478804
#* Worst-case absolute loss bounds for linear learning algorithms
#@ Tom Bylander
#t 1997
#c 10
#% 67056
#% 163098
#% 165663
#% 451055
#% 682442
#% 1860015
#! The absolute loss is the absolute difference between the desired and predicted outcome. I demonstrate worst-case upper bounds on the absolute loss for the perceptron algorithm and an exponentiated update algorithm related to the Weighted Majority algorithm. The bounds characterize the behavior of the algorithms over any sequence of trials, where each trial consists of an example and a desired outcome interval (any value in the interval is an acceptable outcome). The worstcase absolute loss of both algorithms is bounded by: the absolute loss of the best linear function in the comparison class, plus a constant dependent on the initial weight vector, plus a per-trial loss. The per-trial loss can be eliminated if the learning algorithm is allowed a tolerance from the desired outcome. For concept learning, the worst-case bounds lead to mistake bounds that are comparable to previous results.

#index 1478805
#* Version spaces without boundary sets
#@ Haym Hirsh;Nina Mishra;Leonard Pitt
#t 1997
#c 10
#% 3893
#% 26125
#% 39702
#% 44625
#% 55066
#% 66937
#% 178515
#% 186915
#% 197754
#% 221328
#% 267085
#% 449515
#% 449559
#% 450973
#% 1280023
#! This paper shows that it is not necessary to maintain boundary sets to reason using version spaces. Rather, most of the operations typically performed on version spaces for a concept class can be tractably executed directly on the training data, as long as it is tractable to solve the consistency problem for that concept class -- to determine whether there exists any concept in the concept class that correctly classifies the data. The equivalence of version-space learning to the consistency problem bridges a gap between empirical and theoretical approaches to machine learning, since the consistency problem is already known to be critical to learning in the PAC (Probably Approximately Correct) sense. By exhibiting this link to the consistency problem, we broaden the class of problems to which version spaces can be applied to include concept classes where boundary sets can have exponential or infinite size and cases where boundary sets are not even well defined.

#index 1478806
#* Representation, search and genetic algorithms
#@ Darrell Whitley;Soraya B. Rana
#t 1997
#c 10
#% 1022809
#! Wolpert and Macready's No Free Lunch theorem proves that no search algorithm is better than any other over all possible discrete functions. The meaning of the No Free Lunch theorem has, however, been the subject of intense debate. We prove that for local neighborhood search on problems of bounded complexity, where complexity is measured In terms of number of basins of attraction in the search space a Gray coded representation is better than Binary in the sense that on average it induces fewer minima in a Hamming distance 1 search neighborhood.

#index 1478807
#* Pattern discovery in distributed databases
#@ Raj Bhatnagar;Sriram Srinivasan
#t 1997
#c 10
#% 998
#% 129979
#% 210173
#% 443039
#% 443065
#% 449531
#% 449568
#% 449588
#! Most algorithms for learning and pattern discovery in data assume that all the needed data is available on one computer at a single site. This assumption does not hold in situations where a number of independent databases reside on geographically distributed nodes of a computer network. These databases cannot be moved to a single site due to size, security, privacy and data-ownership concerns but all of them together constitute the dataset in which patterns must be discovered. Some pattern discovery algorithms can be adapted to such situations and some others become inefficient or inapplicable. In this paper we show how a decision-tree induction algorithm may be adapted for distributed data situations. We also discuss some general issues relating to the adaptability of other pattern discovery algorithms to distributed data situations

#index 1478808
#* More efficient windowing
#@ Johannes Fürnkranz
#t 1997
#c 10
#% 136350
#% 164368
#% 277919
#% 449508
#% 481779
#% 1271838
#! Windowing has been proposed as a procedure for efficient memory use in the ID3 decision tree learning algorithm. However, previous work has shown that windowing may often lead to a decrease in performance. In this work, we try to argue that rule learning algorithms are more appropriate for windowing than decision tree algorithms, because the former typically learn and evaluate rules independently and are thus less susceptible to changes in class distributions. Most importantly, we present a new windowing algorithm that achieves additional gains in efficiency by saving promising rules and removing examples covered by these rules from the learning window. While the presented algorithm is only suitable for redundant, noise-free data sets, we will also briefly discuss the problem of noisy data for windowing algorithms.

#index 1478809
#* Maximally parsimonious discrimination: a generic task from linguistic discovery
#@ Raúl Valdes-Pérez;Vladimir Pericliev
#t 1997
#c 10
#% 24538
#% 1272365
#! Data-driven model building is an important task of scientific discovery that is seeing real success in the development and application of discovery programs. Most efforts have targeted fields of natural science in which the hypothesis spaces are specialized and deal with domains having considerable formal structure. Less work has been directed toward qualitative areas of social science, in which model building also arises. This paper reports the first automation of a modelling task from linguistic anthropology: the analysis of natural-language kinship terminologies in terms of simpler semantic components. Our approach uses three generic simplicity criteria to comprehensively find all the simplest models that are consistent with kinship data. We have reproduced results from the linguistics literature, but have also found simpler models in some cases. The task has strong generic elements: extracts of the code are applied to other data sets to illustrate this potential.

#index 1478810
#* Sparse representations for fast, one-shot learning
#@ Kenneth Yip;Gerald Jay Sussman
#t 1997
#c 10
#! Humans rapidly and reliably learn many kinds of regularities and generalizations. We propose a novel model of fast learning that exploits the properties of sparse representations and the constraints imposed by a plausible hardware mechanism. To demonstrate our approach we describe a computational model of acquisition in the domain of morphophonology. We encapsulate phonological information as bidirectional boolean constraint relations operating on the classical linguistic representations of speech sounds in term of distinctive features. The performance model is described as a hardware mechanism that incrementally enforces the constraints. Phonological behavior arises from the action of this mechanism. Constraints are induced from a corpus of common English nouns and verbs. The induction algorithm compiles the corpus into increasingly sophisticated constraints. The algorithm yields one-shot learning from a few examples. Our model has been implemented as a computer program. The program exhibits phonological behavior similar to that of young children. As a bonus the constraints that are acquired can be interpreted as classical linguistic rules.

#index 1478811
#* Intelligent methods for file system optimization
#@ Leo Kuvayev;C. L. Giles;J. Philbin;H. Cejtin
#t 1997
#c 10
#% 376
#% 107692
#% 114994
#% 287458
#% 594132
#! The speed of I/O components is it major limitation of the speed of all other major components in today's computer systems. Motivated by this, we investigated several algorithms for efficient and intelligent organization of files on a hard disk. Total access time may be decreased if files with temporal locality also have spatial locality. Three intelligent methods based on file type, frequency, and transition probabilities information showed up to 60% savings of total I/O time over the naive placement of files. More computationally intensive hill climbing and genetic algorithms approaches did not outperform statistical methods. The experiments were run on a real and simulated hard drive in single and multiple user environments.

#index 1478812
#* Learning Bayesian networks from incomplete data
#@ Moninder Singh
#t 1997
#c 10
#% 17144
#% 129987
#% 185079
#% 197387
#% 668895
#% 1273439
#! Much of the current research in learning Bayesian Networks fails to effectively deal with missing data. Most of the methods assume that the data is complete, or make the data complete using fairly ad-hoc methods; other methods do deal with missing data but learn only the conditional probabilities, assuming that the structure is known. We present a principled approach to learn both the Bayesian network structure as well as the conditional probabilities from incomplete data. The proposed algorithm is an iterative method that uses a combination of Expectation-Maximization (EM) and Imputation techniques. Results are presented on synthetic data sets which show that the performance of the new algorithm is much better than ad-hoc methods for handling missing data.

#index 1478813
#* Lessons in neural network training: overfitting may be harder than expected
#@ Steve Lawrence;C. Lee Giles;Ah Chung Tsoi
#t 1997
#c 10
#% 61477
#% 92148
#% 106567
#% 190581
#% 376589
#% 1478785
#! For many reasons, neural networks have become very popular AI machine learning models. Two of the most important aspects of machine learning models are how well the model generalizes to unseen data, and how well the model scales with problem complexity. Using a controlled task with known optimal training error, we investigate the convergence of the backpropagation (BP) algorithm. We find that the optimal solution is typically not found. Furthermore, we observe that networks larger than might be expected can result in lower training and generalization error. This result is supported by another real world example. We further investigate the training behavior by analyzing the weights in trained networks (excess degrees of freedom are seen to do little harm and to aid convergence), and contrasting the interpolation characteristics of multi-layer perceptron neural networks (MLPs) and polynomial models. (overfitting behavior is very different - the MLP is often biased towards smoother solutions). Finally, we analyze relevant theory outlining the reasons for significant practical differences. These results bring into question common beliefs about neural network training regarding convergence and optimal network size, suggest alternate guidelines for practical use (lower fear of excess degrees of freedom), and help to direct future work (e.g. methods for creation of more parsimonious solutions, importance of the MLP/BP bias and possibly worse performance of "improved" training algorithms).

#index 1478814
#* An empirical evaluation of bagging and boosting
#@ Richard Maclin;David Opitz
#t 1997
#c 10
#% 96699
#% 132938
#% 174242
#% 208181
#% 209021
#% 443616
#% 1275295
#% 1499573
#! An ensemble consists of a set of independently trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund & Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classification algorithms. Our results clearly show two important facts. The first is that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is a powerful technique that can usually produce better ensembles than Bagging; however, it is more susceptible to noise and can quickly overfit a data set.

#index 1478815
#* A new metric-based approach to model selection
#@ Dale Schuurmans
#t 1997
#c 10
#% 132583
#% 140193
#% 145224
#% 190581
#% 203294
#% 465899
#% 837668
#% 1290045
#! We introduce a new approach to model selection that performs better than the standard complexity-penalization and hold-out error estimation techniques in many cases. The basic idea is to exploit the intrinsic metric structure of a hypothesis space, as determined by the natural distribution of unlabeled training patterns, and use this metric as a reference to detect whether the empirical error estimates derived from a small (labeled) training sample can be trusted in the region around an empirically optimal hypothesis. Using simple metric intuitions we develop new geometric strategies for detecting overfitting and performing robust yet responsive model selection in spaces of candidate functions. These new metric-based strategies dramatically outperform previous approaches in experimental studies of classical polynomial curve fitting. Moreover, the technique is simple, efficient, and can be applied to most function learning tasks. The only requirement is access to an auxiliary collection of unlabeled training data.

#index 1478816
#* Maximizing the benefits of parallel search using machine learning
#@ Diane J. Cook;R. Craig Varnell
#t 1997
#c 10
#% 94782
#% 103574
#% 136350
#% 136824
#% 136825
#% 192215
#! Many of the artificial intelligence techniques developed to date rely on heuristic search through large spaces. Unfortunately, the size of these spaces and corresponding computational effort reduce the applicability of otherwise novel and effective algorithms. A number of parallel and distributed approaches to search have considerably improved the performance of certain aspects of the search process. In this paper we describe the EUREKA system, which combines the benefits of many different approaches to parallel heuristic search. EUREKA uses a machine learning system to decide upon the optimal parallel search strategy for a given problem space. When a new search task is input to the system, EUREKA gathers information about the search space and automatically selects the appropriate search strategy. EUREKA includes diverse approaches to task distribution, load balancing, and tree ordering, and has been tested on a MIMD parallel processor, a distributed network of workstations, and a single workstation using multithreading. Results in the fifteen puzzle domain, robot arm path planning domain, and an artificial domain indicate that EUREKA outperforms any existing strategy used exclusively for all problem instances.

#index 1478817
#* Generating C4.5 production rules in parallel
#@ Richard Kufrin
#t 1997
#c 10
#% 73374
#% 136350
#% 198076
#% 204528
#% 232106
#% 452821
#% 1274561
#% 1499476
#! Induction systems that represent concepts in the form of production rules have proven to be useful in a variety of domains where both accuracy and comprehensibility of the resulting models are important. However, the computational requirements for inducing a set of rules from large, noisy training sets can be enormous, so that techniques for improving the performance of rule induction systems by exploiting parallelism are of considerable interest. Recent work to parallelize the C4.5 rule generator algorithm is described. After presenting an overview of the algorithm and the parallelization strategy employed, empirical results of the parallel implementation that demonstrate substantial speedup over serial execution are provided.

#index 1478818
#* Detecting and reacting to unplanned-for world states
#@ Ella M. Atkins;Edmund H. Durfee;Kang G. Shin
#t 1997
#c 10
#% 23011
#% 62653
#% 181338
#% 371358
#% 449588
#% 1275454
#% 1650759
#! The degree to which a planner succeeds and meets response deadlines depends on the correctness and completeness of its models which describe events and actions that change the world state. It is often unrealistic to expect perfect models, so a planner must detect and respond to states it had not planned to handle. In this paper, we characterize different classes of these "unhandled" states and describe planning algorithms to build tests for, and later respond to them. We have implemented these unhandled state detection and response algorithms in the Cooperative Intelligent Real-time Control Architecture (CIRCA), and present experiments from flight simulation that show how the new algorithm enables a fully-automated aircraft to react appropriately to certain classes of unhandled states, averting failure and giving the aircraft a new chance to achieve its goals.

#index 1478819
#* Reinforcement learning with time
#@ Daishi Harada
#t 1997
#c 10
#% 22348
#% 88370
#% 1051507
#% 1272286
#! This paper steps back from the standard infinite horizon formulation of reinforcement learning problems to consider the simpler case of finite horizon problems. Although finite horizon problems may be solved using infinite horizon learning algorithms by recasting the problem as an infinite horizon problem over a state space extended to include time, we show that such an applica tion of infinite horizon learning algorithms does not make use of what is known about the environment structure, and is therefore inefficient. Preserving a notion of time within the environment allows us to consider extending the environment model to include, for example, random action duration. Such extentions allow us to model non-Markov environments which can be learned using reinforcement learning algorithms.

#index 1478820
#* Transferring and retraining learned information filters
#@ William W. Cohen;Daniel Kudenko
#t 1997
#c 10
#% 136350
#% 159108
#% 159114
#% 165110
#% 219053
#% 449508
#% 458257
#% 476537
#% 1499473
#! Any system that learns how to filter documents will suffer poor performance during an initial training phase. One way of addressing this problem is to exploit filters learned by other users in a collaborative fashion. We investigate "direct transfer" of learned filters in this setting--a limiting case for any collaborative learning system. We evaluate the stability of several different learning methods under direct transfer, and conclude that symbolic learning methods that use negatively correlated features of the data perform poorly in transfer, even when they perform well in more conventional evaluation settings. This effect is robust: it holds for several learning methods, when a diverse set of users is used in training the classifier, and even when the learned classifiers can be adapted to the new user's distribution. Our experiments give rise to several concrete proposals for improving generalization performance in a collaborative setting, including a beneficial variation on a feature selection method that has been widely used in text categorization.

#index 1478821
#* Active learning with committees for text categorization
#@ Ray Liere;Prasad Tadepalli
#t 1997
#c 10
#% 85272
#% 101898
#% 116165
#% 165110
#% 169717
#% 170649
#% 209021
#% 236729
#% 445025
#% 450951
#% 451055
#% 451056
#% 476744
#% 677027
#! In many real-world domains, supervised learning requires a large number of training examples. In this paper, we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning. Our approach is similar to the Query by Committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label. Our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant. We report here on experiments using a committee of Winnow-based learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by 1-2 orders of magnitude.

#index 1478822
#* Statistical parsing with a context-free grammar and word statistics
#@ Eugene Charniak
#t 1997
#c 10
#% 646936
#% 740916
#% 748465
#% 748561
#% 748722
#% 1476274
#! We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance.

#index 1478823
#* A new supervised learning algorithm for word sense disambiguation
#@ Ted Pedersen;Rebecca Bruce
#t 1997
#c 10
#% 136350
#% 140588
#% 197060
#% 209023
#% 211044
#% 449566
#% 742451
#% 748601
#% 748703
#% 817954
#% 1247064
#% 1499533
#! The Naive Mix is a new supervised learning algorithm that is based on a sequential method for selecting probabilistic models. The usual objective of model selection is to find a single model that adequately characterizes the data in a training sample. However, during model selection a sequence of models is generated that consists of the best-fitting model at each level of model complexity. The Naive Mix utilizes this sequence of models to define a probabilistic model which is then used as a probabilistic classifier to perform word-sense disambiguation. The models in this sequence are restricted to the class of decomposable log-linear models. This class of models offers a number of computational advantages. Experiments disambiguating twelve different words show that a Naive Mix formulated with a forward sequential search and Akaike's Information Criteria rivals established supervised learning algorithms such as decision trees (C4.5), rule induction (CN2) and nearest-neighbor classification (PEBLS).

#index 1478824
#* A pragmatic treatment of quantification in natural language
#@ Walid S. Saba;Jean-Pierre Corriveau
#t 1997
#c 10
#% 21804
#% 86622
#% 99025
#% 158686
#% 477427
#% 748508
#% 748552
#% 748853
#! Quantification in natural language is an important phenomena that seems to touch on some pragmatic and inferential aspects of language understanding. In this paper we focus on quantifier scope ambiguity and suggest a cognitively plausible model that resolves a number of problems that have traditionally been addressed in isolation. Our claim here is that the problem of quantifier scope ambiguity can not be adequately addressed at the syntactic and semantic levels, but is an inferencing problem that must be addressed at the pragmatic and discourse levels.

#index 1478825
#* Comparatives in context
#@ Steffen Staab;Udo Hahn
#t 1997
#c 10
#% 81670
#% 190444
#% 740330
#% 744513
#% 748509
#% 757356
#! We propose a model of semantic interpretation of comparatives which is based on a mechanism for semantic copying. Besides common phrasal and clausal forms of comparatives, our model also incorporates the analysis of referential and textual phenomena that interact with the interpretation of comparatives, viz. metonymies and omitted complements. In order to allow for efficient processing, guidance from syntactic, semantic, contextual and world knowledge sources is supplied.

#index 1478826
#* Multi-document summarization by graph search and matching
#@ Inderjeet Mani;Eric Bloedorn
#t 1997
#c 10
#% 174315
#% 179800
#% 194251
#% 194252
#% 198058
#% 219036
#% 648346
#% 815336
#% 815342
#% 817970
#! We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out.

#index 1478827
#* From local to global coherence: a bottom-up approach to text planning
#@ Daniel Marcu
#t 1997
#c 10
#% 1891
#% 129244
#% 145399
#% 158691
#% 365790
#% 491748
#% 708199
#% 708427
#% 740937
#% 741182
#% 756974
#% 762159
#% 1273509
#% 1273951
#% 1274316
#% 1288561
#% 1288578
#% 1476280
#! We present a new, data-driven approach to text planning, which can be used not only to map full knowledge pools into natural language texts, but also to generate texts that satisfy multiple, high-level communicative goals. The approach explains how global coherence can be achieved by exploiting the local coherence constraints of rhetorical relations. The local constraints were derived from a corpus analysis.

#index 1478828
#* Interference as a tool for designing and evaluating multi-robot controllers
#@ Dani Goldberg;Maja J. Matarić
#t 1997
#c 10
#% 154042
#% 154047
#% 171161
#% 199808
#% 634143
#% 668797
#% 669503
#% 1279751
#! Designing and implementing cooperative group behaviors for robots is considered something of a black art involving an extensive amount of reprogramming and parameter adjustment. What seems to be lacking is a pragmatic, practical, general-purpose tool that would both guide the design and structure the evaluation of controllers for distributed real-world multi-robot tasks. In this paper, we propose the use of interference. between robots as one such simple tool for designing and evaluating multi-robot controllers. We explore how key issues in multi-robot control can be addressed using interference, a directly measurable property of a multi-robot system. We discuss how behavior arbitration schemes, i.e., the choice of controllers, can be made and adjusted using interference. As an experimental example, we demonstrate three different implementations of a collection clean-up (foraging) task using four physical mobile robots, and present analyses of the experimental data gathered from trials of all three implementations.

#index 1478829
#* Using communication to reduce locality in multi-robot learning
#@ Maja J. Matarić
#t 1997
#c 10
#% 90036
#% 147295
#% 154046
#% 418632
#% 496732
#! This paper attempts to bridge the fields of machine learning, robotics, and distributed AI. It discusses the use of communication in reducing the undesirable effects of locality in fully distributed multi-agent systems with multiple agents/robots learning in parallel while interacting with each other. Two key problems, hidden state and credit assignment, are addressed by applying local undirected broadcast communication in a dual role: as sensing and as reinforcement. The methodology is demonstrated on two multi-robot learning experiments. The first describes learning a tightly-coupled coordination task with two robots, the second a loosely-coupled task with four robots learning social rules. Communication is used to share sensory data to overcome hidden state and reinforcement to overcome the credit assignment problem between the agents and to bridge the gap between local and global payoff.

#index 1478830
#* Spatial navigation with uncertain deviations
#@ Michel de Rougemont;Christoph Schlieder
#t 1997
#c 10
#% 39652
#% 89749
#% 211672
#% 362928
#% 367254
#% 1068410
#! We consider geometrical scenes with obstacles and landmarks that can't necessarily be distinguished and generalize the notion of panoramas (Sch93; Her94), introduced in the qualitative Spatial Reasoning (QSR) approaches to robot navigation. We study various notions of motion strategies in the accessibility graph associated with the local panoramas under uncertain deviations, a natural model of uncertainty for motion planning and navigation. We show that randomized motion strategies can be better than deterministic ones with a finite memory and stress the usefulness of random decisions for qualitative spatial reasoning.

#index 1478831
#* A color interest operator for landmark-based navigation
#@ Zachary Dodds;Gregory D. Hager
#t 1997
#c 10
#% 85042
#% 120270
#% 131471
#% 163346
#% 199610
#% 212697
#% 443806
#% 443864
#% 443913
#% 1476260
#! Landmark-based approaches to robot navigation require an "interest operator" to estimate the utility of a particular image region as an effective representative for a scene. This paper presents a color interest operator consisting of a weighted combination of heuristic scores. The operator selects those image regions (landmarks) likely to be found again, even under a different viewing geometry and/or different illumination conditions. These salient regions yield a robust representation for recognition of a scene. Experiments showing the reproduceability of the regions selected by this operator demonstrate its use as a hedge against environmental uncertainties.

#index 1478832
#* Combining approximate front end signal processing with selective reprocessing in auditory perception
#@ Frank Klassner;Victor Lesser;Hamid Nawab
#t 1997
#c 10
#% 36638
#% 37906
#% 111299
#% 147043
#% 193506
#% 703623
#% 704183
#% 1273516
#% 1275408
#! When dealing with signals from complex environments, where multiple time-dependent signal signatures can interfere with each other in stochastically unpredictable ways, traditional perceptual systems tend to fall back on a strategy of always performing finely-detailed, costly analysis of the signal with a comprehensive front end set of signal processing algorithms (SPAs), whether or not the current scenario requires the extra detail. Approximate SPAs (ASPAs) - algorithms whose processing time can be limited in order to trade off precision in their outputs for reduced execution time - can playa role in producing adaptive, less-costly front ends, but their outputs tend to require context-dependent analysis for use as evidence in interpretation. This paper examines the IPUS (Integrated Processing and Understanding of Signals) architecture's ability to serve as a support framework for applying ASPAs in interpretation problems. Specifically, our work shows that it is feasible to include an approximate version of the Short-Time Fourier Transform in an IPUS-based sound-understanding testbed.

#index 1478833
#* Analogical replay for efficient conditional planning
#@ Jim Blythe;Manuela Veloso
#t 1997
#c 10
#% 124601
#% 362441
#% 1290043
#! Recently, several planners have been designed that can create conditionally branching plans to solve problems which involve uncertainty. These planners represent an important step in broadening the applicability of AI planning techniques, but they typically must search a larger space than non-branching planners, since they must produce valid plans for each branch considered. In the worst case this can produce an exponential increase in the complexity of planning. If conditional planners are to become usable in real-world domains, this complexity must be controlled by sharing planning effort among branches. Analogical plan reuse should playa fundamental role in this process. We have implemented a conditional probabilistic planner that uses analogical plan replay to derive the maximum benefit from previously solved branches of the plan. This approach provides valuable guidance for when and how to merge different branches of the plan and exploits the high similarity between the different branches in a conditional plan, which have the same goal and typically a very similar state. We present experimental data in which analogical plan replay significantly reduces the complexity of conditional planning. Analogical replay can be applied to a variety of conditional planners, complementing the plan sharing that they may perform naturally.

#index 1478834
#* Case-based similarity assessment: estimating adaptability from experience
#@ David B. Leake;Andrew Kinley;David Wilson
#t 1997
#c 10
#% 55921
#% 198767
#% 366694
#% 566457
#% 1273710
#% 1275278
#% 1499567
#! Case-based problem-solving systems rely on similarity assessment to select stored cases whose solutions are easily adaptable to fit current problems. However, widely-used similarity assessment strategies, such as evaluation of semantic similarity, can be poor predictors of adaptability. As a result, systems may select cases that are difficult or impossible for them to adapt, even when easily adaptable cases are available in memory. This paper presents a new similarity assessment approach which couples similarity judgments directly to a case library containing the system's adaptation knowledge. It examines this approach in the context of a case-based planning system that learns both new plans and new adaptations. Empirical tests of alternative similarity assessment strategies show that this approach enables better case selection and increases the benefits accrued from learned adaptations.

#index 1478835
#* Dynamic abstraction planning
#@ Robert P. Goldman;David J. Musliner;Kurt D. Krebsbach;Mark S. Boddy
#t 1997
#c 10
#% 124601
#% 172505
#% 181338
#% 224762
#! This paper describes Dynamic Abstraction Planning (DAP), an abstraction planning technique that improves the efficiency of state-enumeration planners for real-time embedded systems such as CIRCA. Abstraction is used to remove detail from the state representation, reducing both the size of the state space that must be explored to produce a plan and the size of the resulting plan. The intuition behind this approach is simple: in some situations, certain world features are important, while in other situations those same features are not important. By automatically selecting the appropriate level of abstraction at each step during the planning process, DAP can significantly reduce the size of the search space. Furthermore, the planning process can supply initial plans that preserve safety but might, on further refinement, do a better job of goal achievement. DAP can also terminate with an executable abstract plan, which may be much smaller than the corresponding plan expanded to precisely-defined states. Preliminary results show dramatic improvements in planning speed and scalability.

#index 1478836
#* Abductive completion of plan sketches
#@ Karen L. Myers
#t 1997
#c 10
#% 44836
#% 120806
#% 178934
#% 1476289
#! Most work on AI planning has focused on the development of fully automated methods for generating plans that satisfy user-specified goals. However, users in many domains want the ability to influence the nature of the solutions that are generated. With the objective of fostering increased user participation in the planning process, this paper presents an HTN-based framework for the abductive completion of plan sketches. Within this framework, user-supplied outlines of partial plans (possibly spanning multiple abstraction levels) are interpreted and completed. The processing of plan sketches employs an initial abductive plan recognition phase to formulate candidate sets of intended user goals, followed by a plan refinement stage that generates sketch-compliant final plans for those goals. A prototype sketch-based planner based on this approach has been implemented and applied to a crisis action planning domain.

#index 1478837
#* A linear programming heuristic for optimal planning
#@ Tom Bylander
#t 1997
#c 10
#% 1436
#% 76462
#% 103309
#% 131357
#% 167629
#% 179935
#% 194648
#% 194651
#% 1272297
#% 1272468
#% 1290109
#% 1476298
#% 1476300
#% 1476302
#! I introduce a new search heuristic for propositional STRIPS planning that is based on transforming planning instances to linear programming instances. The linear programming heuristic is admissible for finding minimum length plans and can be used by partial-order planning algorithms. This heuristic appears to be the first non-trivial admissible heuristic for partial-order planning. An empirical study compares Lplan, a partial-order planner incorporating the heuristic, to Graphplan, Satplan, and UCPOP on the tower of Hanoi domain, random blocks-world instances, and random planning instances. Graphplan is far faster in the study than the other algorithms. Lplan is often slower because the heuristic is time-consuming, but Lplan shows promise because it often perfonns a small search.

#index 1478838
#* Finding optimal solutions to Rubik's cube using pattern databases
#@ Richard E. Korf
#t 1997
#c 10
#% 1474
#% 2194
#% 116297
#% 160388
#% 180109
#% 189701
#% 451046
#% 533951
#% 1080963
#% 1275257
#! We have found the first optimal solutions to random instances of Rubik's Cube. The median optimal solution length appears to be 18 moves. The algorithm used is iterative-deepening-A* (IDA*), with a lower-bound heuristic function based on large memory-based lookup tables, or "pattern databases" (Culberson and Schaeffer 1996). These tables store the exact number of moves required to solve various subgoals of the problem, in this case subsets of the individual movable cubies. We characterize the effectiveness of an admissible heuristic function by its expected value, and hypothesize that the overall performance of the program obeys a relation in which the product of the time and space used equals the size of the state space. Thus, the speed of the program increases linearly with the amount of memory available. As computer memories become larger and cheaper, we believe that this approach will become increasingly cost-effective.

#index 1478839
#* Planning by rewriting: efficiently generating high-quality plans
#@ José Luis Ambite;Craig A. Knoblock
#t 1997
#c 10
#% 1758
#% 25998
#% 32889
#% 126390
#% 131557
#% 145388
#% 172505
#% 194648
#% 213437
#% 362441
#% 370528
#% 1272367
#% 1290113
#% 1290115
#% 1476298
#! Domain-independent planning is a hard combinatorial problem. Taking into account plan quality makes the task even more difficult. We introduce a new paradigm for efficient high-quality planning that exploits plan rewriting rules and efficient local search techniques to transform an easy-to-generate, but possibly sub-optimal, initial plan into a low-cost plan. In addition to addressing the issues of efficiency and quality, this framework yields a new anytime planning algorithm. We have implemented this planner and applied it to several existing domains. The results show that this approach provides significant savings in planning effort while generating high-quality plans.

#index 1478840
#* A robust and fast action selection mechanism for planning
#@ Blai Bonet;Gábor Loerincs;Héctor Geffner
#t 1997
#c 10
#% 21145
#% 57545
#% 68238
#% 154075
#% 363744
#% 647206
#% 835098
#% 1268736
#% 1272286
#% 1272468
#% 1290109
#% 1291498
#% 1383855
#% 1383858
#% 1476298
#% 1476299
#% 1476300
#% 1499511
#! The ability to plan and react in dynamic environments is central to intelligent behavior yet few algorithms have managed to combine fast planning with a robust execution. In this paper we develop one such algorithm by looking at planning as real time search. For that we develop a variation of Korf's Learning Real Time A* algorithm together with a suitable heuristic function. The resulting algorithm interleaves lookahead with execution and never builds a plan. It is an action selection mechanism that decides at each time point what to do next. Yet it solves hard planning problems faster than any domain independent planning algorithm known to us, including the powerful SAT planner recently introduced by Kautz and Selman. It also works in the presence of perturbations and noise, and can be given a fixed time window to operate. We illustrate each of these features by running the algorithm on a number of benchmark problems.

#index 1478841
#* Planning with concurrent interacting actions
#@ Craig Boutilier;Ronen I. Brafman
#t 1997
#c 10
#% 117869
#% 160202
#% 217918
#% 536408
#% 782311
#% 1275239
#% 1290109
#% 1476298
#! In order to generate plans for agents with multiple actuators or agent teams, we must be able to represent and plan using concurrent actions with interacting effects. Historically, this has been considered a challenging task that could require a temporal planner. We show that, with simple modifications, the STRIPS action representation language can be used to represent concurrent interacting actions. Moreover, current algorithms for partial-order planning require only small modifications in order to handle this language and produce coordinated multiagent plans. These results open the way to partial order planners for cooperative multiagent systems.

#index 1478842
#* A heuristic variable grid solution method for POMDPs
#@ Ronen I. Brafman
#t 1997
#c 10
#% 92301
#% 101869
#% 644560
#% 695957
#% 1290039
#% 1290265
#! Partially observable Markov decision processes (POMDPs) are an appealing tool for modeling planning problems under uncertainty. They incorporate stochastic action and sensor descriptions and easily capture goal oriented and process onented tasks. Unfortunately, POMDPs are very difficult to solve. Exact methods cannot handle problems with much more than 10 states, so approximate methods must be used. In this paper, we describe a simple variable-grid solution method which yields good results on relatively large problems with modest computational effort.

#index 1478843
#* Incremental methods for computing bounds in partially observable Markov decision processes
#@ Milos Hauskrecht
#t 1997
#c 10
#% 92301
#% 141646
#% 556957
#% 646479
#% 707273
#% 1290039
#! Partially observable Markov decision processes (POMDPs) allow one to model complex dynamic decision or control problems that include both action outcome uncertainty and imperfect observability. The control problem is formulated as a dynamic optimization problem with a value function combining costs or rewards from multiple steps. In this paper we propose, analyse and test various incremental methods for computing bounds on the value function for control problems with infinite discounted horizon criteria. The methods described and tested include novel incremental versions of grid-based linear interpolation method and simple lower bound method with Sondik's updates. Both of these can work with arbitrary points of the belief space and can be enhanced by various heuristic point selection strategies. Also introduced is a new method for computing an initial upper bound - the fast informed bound method. This method is able to improve significantly on the standard and commonly used upper bound computed by the MDP-based method. The quality of resulting bounds are tested on a maze navigation problem with 20 states, 6 actions and 8 observations.

#index 1478844
#* Effective Bayesian inference for stochastic programs
#@ Daphne Koller;David McAllester;Avi Pfeffer
#t 1997
#c 10
#% 44876
#% 75936
#% 147677
#% 183497
#% 1476311
#% 1650731
#% 1650767
#% 1650778
#! In this paper, we propose a stochastic version of a general purpose functional programming language as a method of modeling stochastic processes. The language contains random choices, conditional statements, structured values, defined functions, and recursion. By imagining an experiment in which the program is "run" and the random choices made by sampling, we can interpret a program in this language as encoding a probability distribution over a (potentially infinite) set of objects. We provide an exact algorithm for computing conditional probabilities of the form Pr(P(x) | Q(x)) where x is chosen randomly from this distribution. This algorithm terminates precisely when sampling x and computing P(x) and Q(x) terminates in all possible stochastic executions (under lazy evaluation semantics, in which only values needed to compute the output of the program are evaluated). We demonstrate the applicability of the language and the efficiency of the inference algorithm by encoding both Bayesian networks and stochastic context-free grammars in our language, and showing that our algorithm derives efficient inference algorithms for both. Our language easily supports interesting and useful extensions to these formalisms (e.g., recursive Bayesian networks), to which our inference algorithm will automatically apply.

#index 1478845
#* Probabilistic propositional planning: representations and complexity
#@ Michael L. Littman
#t 1997
#c 10
#% 30037
#% 145332
#% 167629
#% 194646
#% 194652
#% 224762
#% 1068628
#% 1290041
#! Many representations for probabilistic propositional planning problems have been studied. This paper reviews several such representations and shows that, in spite of superficial differences between the representations, they are "expressively equivalent," meaning that planning problems specified in one representation can be converted to equivalent planning problems in any of the other representations with at most a polynomial factor increase in the size of the resulting representation and the number of steps needed to reach the goal with sufficient probability. The paper proves that the computational complexity of determining whether a successful plan exists for planning problems expressed in any of these representations is EXPTIME-complete and PSPACE-complete when plans are restricted to take a polynomial number of steps.

#index 1478846
#* THE AAAI-97 mobile robot competition: Martians, remotes, hors d'oeuvres, and cleaning up the mess afterwards
#@ Ronald C. Arkin;R. James Firby
#t 1997
#c 10

#index 1478847
#* The emergence of spacecraft autonomy
#@ Richard J. Doyle
#t 1997
#c 10
#% 1271893
#% 1476265
#! The challenge of space flight in NASA's future is to enable more frequent and more intensive space exploration missions at lower cost. Nowhere is this challenge more acute than among the planetary exploration missions which JPL conducts for NASA. The launching of a new era of solar system exploration - beyond reconnaissance -- is being designed for the first time around the concept of sustained intelligent presence on the space platforms themselves. Artificial intelligence, spacecraft engineering, mission design, software engineering and systems engineering all have a role to play in this vision, and all are being integrated in new work on spacecraft autonomy.

#index 1478848
#* What does knowledge representation have to say to artificial intelligence?
#@ David W. Etherington
#t 1997
#c 10
#! In recent years, the subarea of Knowledge Representation and Reasoning (KR) has become more and more of a discipline unto itself, focusing on artificial problems while other areas of AI have tended to develop their own representations and algorithms. There are signs that this is changing, however. This talk will explore what the current state of KR has to offer to AI.

#index 1478849
#* Machine learning for intelligent systems
#@ Pat Langley
#t 1997
#c 10
#% 90044
#% 107038
#% 124582
#% 156189
#% 179995
#% 191680
#% 198076
#% 363939
#% 444841
#% 444938
#% 449554
#% 449560
#% 449561
#% 449567
#% 449576
#% 449586
#% 466850
#% 748579
#% 1268733
#% 1290042
#% 1476277
#! Recent research in machine learning has focused on supervised induction for simple classification and reinforcement learning for simple reactive behaviors. In the process, the field has become disconnected from AI's original goal of creating complete intelligent agents. In this paper, I review recent work on machine learning for planning, language, vision, and other topics that runs counter to this trend and thus holds interest for the broader AI research community. I also suggest some steps to encourage further research along these lines.

#index 1478850
#* James Bond and Michael Ovitz: the secret life of agents
#@ Katia P. Sycara
#t 1997
#c 10
#% 159108
#% 159113
#% 159114
#% 162305
#% 174715
#% 445079
#% 701788
#% 702580
#! As agents populate Cyberspace in their many guises and roles, they coordinate and interact in different ways, spanning self-interested, as well as collaborative interactions. Agent coordination should be supported by an agent's internal architecture and agent societal frameworks. We take a micro-economic view of coordination. In this talk we report on our work on adaptive agent architecture and the primitive agent behaviors it supports, agent organizations, contracting protocols among agents and presence of middle agents.

#index 1478851
#* Market-oriented programming (abstract)
#@ Michael P. Wellman
#t 1997
#c 10
#% 1268730
#% 1650801
#! Market-oriented programming is the construction of computational economies, where agents interact through a price system. Markets can provide effective allocation of resources for a variety of distributed environments, and economic analysis a powerful design tool for interaction mechanisms. The spread of electronic commerce puts a premium on market-aware agents, and presents a case for market awareness on the part of agent developers and AI researchers as well.

#index 1478852
#* Hack and kluge
#@ C. P. T. Pete Beim;Ian Horswill;Ivan Yen
#t 1997
#c 10
#% 240974
#% 669402
#% 669414
#% 1275234

#index 1478853
#* Learning in a fuzzy logic robot controller
#@ Douglas S. Blank;J. Oliver Ross
#t 1997
#c 10

#index 1478854
#* Autonomous exploration: an integrated systems approach
#@ Marc Bolduc;Eric Bourque;Gregory Dudek;Nicholas Roy;Robert Sim
#t 1997
#c 10

#index 1478855
#* ServerDroid: a multimedia service robot
#@ Pete Bonasso
#t 1997
#c 10

#index 1478856
#* A situated vacuuming robot
#@ D. Bruemmer;R. Dickson;J. Dilatush;D. Lewis;H. Mateyak;M. Mirarchi;M. Morton;J. Tracy;A. Vorobiev;L. Meeden
#t 1997
#c 10
#% 167408

#index 1478857
#* Teaming up: Georgia tech's multi-robot competition teams
#@ Thomas R. Collins;Tucker R. Balch
#t 1997
#c 10
#% 184278

#index 1478858
#* The dartmouth mobile robot: SK
#@ William Garner;Gregory Friedland;Artyom Lifshits;Daniela Rus;Keith Kotay;Jon Howell
#t 1997
#c 10
#! The Dartmouth Mobile Robot Serial Killer (see Figures 1 and 2) is a minimalist, architectually-Iean autonomous robot that can vacuum your room. The robot is controlled by a Motorola 6811 microcontroller and has 40kb usable memory. The robot has several sensors including sonar, motion detection, contact, and analog IR. The robot's motion is based on a combination of off-line and on-line algorithms that run on-board.

#index 1478859
#* LOBOtomous: an autonomous platform for indoor environments
#@ Ales V. Hvezda;John J. Garcia;Paul R. Klarer;Raymond H. Byrne;Gregory L. Heileman;Chaouki T. Abdallah
#t 1997
#c 10
#! The University of New Mexico's entry in this year's AAAI Mobile Robot competition is LOBOtomous. LOBOtomous was constructed from scratch by UNM students in a senior level design class. Hardware for the project was loaned by Sandia National Labs. LOBOtomous will be entered into the home vacuuming and the hors d'oeuvres event.

#index 1478860
#* Finding life on mars, and other tasks for NCSU's mobile robots
#@ Jason A. Janét;Bruce R. Linnell;Sean M. Scoggins
#t 1997
#c 10

#index 1478861
#* Are you being served?
#@ David P. Miller;Cathryne Stein;Anne Wright;Randy Sargent
#t 1997
#c 10

#index 1478862
#* Intelligent sensor fusion for the 1997 AAAI mobile robot competition
#@ Robin R. Murphy
#t 1997
#c 10
#% 242683
#% 1271910

#index 1478863
#* Kansas state robotics
#@ Todd Prater;Michael Novak;Brian Rectanus;Steven Gustafson;David A. Gustafson
#t 1997
#c 10
#! The robotics team from Kansas State University consists of three undergraduates, one graduate student, and a faculty advisor from the Department of Computing and Information Sciences. The group intends to compete in the "Find the Remote" event at this year's AAAI 97 Mobile Robotics Competition in Providence, Rhode Island. Kansas State University has participated in each of the last four competitions, placing two teams (second and third place) in last year's "Office Delivery" event. This year's team has two principal goals: to win the "Find the Remote" event and to provide a solid foundation on which to build future entries.

#index 1478864
#* A cooperative multi-robot approach to the mapping and exploration of mars
#@ Paul Rybski;Sascha Stoeter;Chris Wyman;Maria Gini
#t 1997
#c 10
#% 39654
#% 1478828
#! In the AAAI 1 "Life on Mars" competition this year, we intend to employ a multi-robot team which combines traditional methods of autonomous navigation with experimental group arbitration strategies to explore and map a simulated extra-terrestrial environment. By communicating with each other to optimize the search, the robots will be able to explore the environment faster than could a single robot. Group strategies will also be used to coordinate the robot's actions to optimize the retrieval of objects in the environment.

#index 1478865
#* Lobokhod: the university of new Mexico's robotic mars rover
#@ Dan Stormont;Jane Canulette;Timothy Eyring;Jose Juste;Salamon Quintana;Chaouki Abdallah;Ray Byrne;Greg Heileman
#t 1997
#c 10

#index 1478866
#* Multiple agents from the bottom up: the interaction lab's robot competition effort
#@ Barry Brian Werger;Miguel Schneider Fontan;Dani Goldberg;Greg Hornby;Maja Mataric;Sen Song
#t 1997
#c 10
#% 668819
#% 1478828

#index 1478867
#* ARIEL: autonomous robot for integrated exploration and localization
#@ Brian Yamauchi;Alan Schultz;William Adams;Kevin Graves;John Grefenstette;Dennis Perzanowski
#t 1997
#c 10
#% 590985
#% 590989
#% 742397

#index 1478868
#* Pragmatic question answering: generic versus specific responses
#@ Debra T. Burhans
#t 1997
#c 10
#% 191655

#index 1478869
#* Adaptive hybrid system architecture for forecasting
#@ Juan M. Corchado
#t 1997
#c 10
#% 241035
#! The aim of the research is to combine Symbolic Artificial Intelligence (AI) (Case Base Reasoning systems) and Connectionist AI (particularly Radial Basis Functions Multi-layer Perceptron and Neuro-fuzzy Algorithms) to develop an improved joint approach to forecasting. New ways to combine Connectionist and Symbolic AI techniques to obtain stronger, more flexible and more adaptive forecasting systems are being investigated.

#index 1478870
#* Iterative refinement of knowledge bases with consistency guarantees
#@ Stephen F. Correl
#t 1997
#c 10
#% 2298
#% 1268741
#% 1499552

#index 1478871
#* Probabilistic learning in Bayesian and stochastic neural networks
#@ William H. Hsu
#t 1997
#c 10
#% 44625
#% 169358
#! The goal of this research is to integrate aspects of artificial neural networks (ANNs) with symbolic machine learning methods in a probabilistic reasoning framework. Improved understanding of the semantics of neural nets supports principled integration efforts between seminumerical (so-called "subsymbolic") and symbolic intelligent systems. My dissertation focuses on learning of spatiotemporal (ST) sequences. In recent work, I have investigated architectures for modeling of ST sequences, and dualities between Bayesian networks and ANNs that expose their probabilistic and information theoretic foundations. In addition, I am developing algorithms for automated construction of Bayesian networks (and hybrid models); metrics for comparison of Bayesian networks across architectures; and a quantitative theory of feature construction (in the spirit of the PAC formalism from computational learning theory) for this learning environment. (Haussler 1988) Such methods for pattern prediction will be useful for building advanced knowledge based systems, with diagnostic applications such as intelligent monitoring tools.

#index 1478872
#* Unified hardware and software models for smart system design
#@ Ravi Kapadia
#t 1997
#c 10
#% 92692
#% 858823

#index 1478873
#* Evaluating the role of background knowledge in enhancing knowledge discovery in databases
#@ Venkateswarlu Kolluri
#t 1997
#c 10

#index 1478874
#* Belief network inference in dynamic environments
#@ Ole J. Mengshoel
#t 1997
#c 10
#% 44876
#% 215927
#% 369236
#% 1476322

#index 1478875
#* Knowledge lean word sense disambiguation
#@ Ted Pedersen
#t 1997
#c 10
#% 742451
#% 1478823

#index 1478876
#* Dynamic organization of search results using a taxonomic domain model
#@ Wanda Pratt
#t 1997
#c 10

#index 1478877
#* Applications of machine learning to information access
#@ Mehran Sahami
#t 1997
#c 10
#% 118771
#% 465747
#% 1478905

#index 1478878
#* Computing discourse information with statistical methods
#@ Kenneth B. Samuel
#t 1997
#c 10
#% 196896

#index 1478879
#* An information-based approach to punctuation
#@ Bilge Say
#t 1997
#c 10

#index 1478880
#* Layered learning in multiagent systems
#@ Peter Stone
#t 1997
#c 10

#index 1478881
#* Avoiding failure via pre-planned responses and time-bounded planning
#@ Ella M. Atkins;Edmund H. Durfee;Kang G. Shin
#t 1997
#c 10
#% 181338

#index 1478882
#* Dynamic prioritization of complex agents in distributed constraint satisfaction problems
#@ Aaron A. Armstrong;Edmund H. Durfee
#t 1997
#c 10
#% 160245

#index 1478883
#* An efficient heuristic search in a large multi-agent system
#@ Cheng-Gang Bian;Wen Cao;Gunnar Hartvigsen
#t 1997
#c 10
#% 543195

#index 1478884
#* Summarizing time-varying data
#@ Sarah Boyd
#t 1997
#c 10
#% 217067
#% 444951

#index 1478885
#* Efficient production match algorithm and its implication for dynamic constraint satisfaction problems
#@ Bonghan Cho;Paul Rosenbloom;Milind Tambe
#t 1997
#c 10
#% 23011
#% 153268

#index 1478886
#* Applying clustering to the classification problem
#@ Piew Datta
#t 1997
#c 10
#% 1478742

#index 1478887
#* Experiments in UNIX command prediction
#@ Brian D. Davison;Haym Hirsch
#t 1997
#c 10
#% 136350
#% 150994
#% 159114
#% 241115
#% 1268733

#index 1478888
#* A comparison of model averaging methods in foreign exchange prediction
#@ Pedro Domingos
#t 1997
#c 10

#index 1478889
#* Learning multiple models without sacrificing comprehensibility
#@ Pedro Domingos
#t 1997
#c 10
#% 136350
#% 209021

#index 1478890
#* Stratification for variants of default logic
#@ Jörg Ernst;Grigoris Antoniou
#t 1997
#c 10
#% 544418

#index 1478891
#* Speeding safely: multi-criteria optimization in probabilistic planning
#@ Michael S. Fulkerson;Michael L. Littman;Greg A. Keim
#t 1997
#c 10
#% 1291498

#index 1478892
#* Information routing using a corpus distribution
#@ Jeffrey A. Goldman
#t 1997
#c 10
#% 67565
#% 406493
#% 1306081

#index 1478893
#* Althea: minimalist representation for robot assembly tasks
#@ Jonathan B. Handler
#t 1997
#c 10
#% 97619
#% 705378

#index 1478894
#* Social comparison for failure detection and recovery in multi-agent settings
#@ Gal A. Kaminka;Milind Tambe
#t 1997
#c 10
#% 75896
#% 1499477

#index 1478895
#* Smart system design using hybrid models
#@ Ravi Kapadia
#t 1997
#c 10
#% 92692
#% 374605

#index 1478896
#* Learning to play hearts
#@ Leo Kuvayev
#t 1997
#c 10
#% 92148
#% 124689
#% 449561

#index 1478897
#* Predicting resource use with case-based plan recognition
#@ Jung-Jin Lee;Robert McCartney
#t 1997
#c 10
#% 159113

#index 1478898
#* Active learning with committees
#@ Ray Liere;Prasad Tadepalli
#t 1997
#c 10

#index 1478899
#* Development of iterative scheduler to planner feedback
#@ Charles B. McVey;Ella M. Atkins;Edmund H. Durfee;Kang G. Shin
#t 1997
#c 10
#% 181338
#% 1650759

#index 1478900
#* A stochastic strategy for multiagent contracts and the impact of deliberation overhead
#@ Sunju Park;Edmund H. Durfee
#t 1997
#c 10

#index 1478901
#* Naive mixes for word sense disambiguation
#@ Ted Pedersen
#t 1997
#c 10
#% 742451
#% 1478823

#index 1478902
#* On the discovery of patterns in medical data
#@ Jorge C. G. Ramirez;Lynn L. Peterson;Dolores M. Peterson;Gretchen K. Cormier
#t 1997
#c 10
#% 459006

#index 1478903
#* Learning goal-decomposition rules using exercises
#@ Chandra Reddy;Prasad Tadepalli
#t 1997
#c 10
#% 82136
#% 465761

#index 1478904
#* Quantification and commonsense reasoning
#@ Walid S. Saba;Jean-Pierre Corriveau
#t 1997
#c 10
#% 21804
#% 748552
#% 748853
#! Traditional approaches to the resolution of quantifier scope ambiguity are based on devising syntactic and semantic rules to eliminate a multitude of otherwise equally valid readings. This approach is neither cognitively nor computationally plausible. Instead we suggest a cognitively plausible model to quantifier scope using a "quantificational restriction" which we assume speakers of ordinary language compute in appropriately defined contexts.

#index 1478905
#* Real-time full-text clustering of networked documents
#@ Mehran Sahami;Salim Yusufali;Michelle Q. W. Baldonado
#t 1997
#c 10
#% 118771
#% 214711
#% 232679

#index 1478906
#* A model of invention
#@ Marin Simina;Ashwin Ram;Janet Kolodner;Michael Gorman
#t 1997
#c 10
#% 156190

#index 1478907
#* Modifying knowledge bases using scripts
#@ Marcelo Tallis;Yolanda Gil
#t 1997
#c 10
#% 105499
#% 134111
#% 179741

#index 1478908
#* Noise sensitivity analysis for shape from focus methods
#@ Jenn-Kwei Tyan
#t 1997
#c 10

#index 1478909
#* Analyzing agents that learn about agents
#@ José M. Vidal;Edmund H. Durfee
#t 1997
#c 10
#% 188086

#index 1478910
#* Matching requests for agent services with differentiated vocabulary
#@ Pete Weinstein;William P. Birmingham
#t 1997
#c 10
#% 368925

#index 1478911
#* Pimtool, an expert system to troubleshoot computer hardware failures
#@ Narendra Dev;Bart Anderson
#t 1997
#c 10
#% 565381
#! This paper describes a tool to diagnose the cause of failure of a HP computer server. We do this by analyzing the dump of Processor Internal Memory (PIM) and maximizing the leverage of expert learning from one hardware failure situation to another. The tool is a rule-based expert system, with some nested rules which translate to decision trees. The rules were implemented using a metalanguage which was customized for the hardware failure analysis problem domain. Pimtool has been deployed to 25 users as of December 1996. We plan to expand usage to over 400 users by end of 1997. Using Pimtool, we expect to save over 15 minutes in Mean-Time-to-Repair (MTIR) per call. We have recognized that knowledge management will be a key issue in the future and are developing tools and strategies to address it.

#index 1478912
#* Case and constraint-based apartment construction project planning system: FASTrak-APT
#@ Kyoung Jun Lee;Hyun Woo Kim;Jae Kyu Lee;Tae Hwan Kim;Chang Gon Kim;Myoung Kyun Yoon;Eui Jun Hwang;Hyun Jeong Park
#t 1997
#c 10
#% 62200
#% 176887
#% 194584
#! To effectively generate a fast and consistent apartment construction project network, Hyundai and KAIST have developed a case and constraint based project planning expert system for apartment domain, FASTrak-APT, inspired by the fact that human expert project planner uses previous cases for planning a new project and modifies them using her/his knowledge on domain constraints. This largescale, case based and mixed-initiative planning system integrated with intensive constraint-based adaptation utilizes semantic level meta-constraints and human decisions for compensating incomplete cases imbedding specific planning knowledge. The case and constraint based architecture inherently supports cross-checking cases with constraints during the system development and maintenance. This system has drastically reduced the time and effort required for initial project planning, improved the quality and completeness of the generated plans, and is expected to give the company the competitive advantage in contract bids for new contracts.

#index 1478913
#* PST: the provider selection tool
#@ Howard Marmorstein;Jayesh Ghia;Sandeep Sathaye;Akshay Gupta;Eva Baron-Vartian
#t 1997
#c 10
#! Oxford Health Plans, Inc. is a managed care organization whose goal is to deliver cost-effective, high-quality health care. Oxford's product lines include traditional health maintenance organizations, point-of service plans, third-party administration of employer funded benefit plans, Medicare and Medicaid plans, and dental plans. A Member's satisfaction with their Primary Care Physician (PCP) is important to their relationship with Oxford. People traditionally choose a doctor (i.e., a PCP) by asking family and friends for recommendations. While this will most likely remain the most trusted method for choosing a Physician, Oxford desires to act as an added supportive resource for helping Members make a wise decision. Oxford aims to complement and also confirm the advice of family and friends. Because of this, Oxford has developed the Provider Selection Tool (PST). PST is a case based reasoning tool, deployed within Oxford and on the World Wide Web, that elicits search criteria from a Member to assess and evaluate a roster of Providers that meet those criteria. These Providers are then evaluated and presented in ranked order based upon how well they match the criteria. By helping Members easily select the Provider that best meets their criteria, Oxford enhances the one-toone Oxford-Member relationship.

#index 1478914
#* Desktop underwriter: Fannie Mae's automated mortgage underwriting expert system
#@ David W. McDonald;Charles O. Pepe;Henry M. Bowers;Edward J. Dombroski
#t 1997
#c 10
#! Fannie Mae, the nation's largest source of conventional mortgage funds, has made a commitment to use technology to improve the efficiency of processing a loan by reducing the time, paperwork and cost associated with loan origination. The Desktop Underwriter (DU) system which was developed as a result of this commitment, is an automated underwriting expert system that applies both heuristics and statistics to the problem. The system supports both the wholesale and retail mortgage environments and is built to reason and underwrite loans with incomplete, unverified and conflicting data. The system generates a credit recommendation based on the loan's conformity to credit standards and an eligibility recommendation based on the loan's conformity to eligibility requirements. DU is already having a major impact on the mortgage industry. The system helps standardize how the Fannie Mae underwriting guidelines are interpreted, reduces discrimination by removing subjective reasoning from the decision process and reduces the cost of manual underwriting for both lenders and Fannie Mae. DU was first released into production in June of 1995 and there have been at least two major releases each year. The system's use and importance both to Fannie Mae and the mortgage industry have been growing steadily. DU is well positioned to lead the way in meeting Fannie Mae's goal of reducing the cost of making a mortgage by $1,000 and reducing the processing time from eight weeks to five days.

#index 1478915
#* Design of high performance help desk application and its implementation results
#@ Charles S. Moon;Thomas A. Shore;Gary Brophy;Dennis Koski
#t 1997
#c 10
#% 68602
#% 405727
#! The primary purpose of this paper is to discuss the design and implementation of an automated help desk application called "Remote Expert System To Optimize Repair Efficiency" (RESTORE) currently deployed in the IBM Corporation at Rochester, Minnesota. It was designed in 1992 mainly to enhance the support of AS/400 business computer systems. In order to familiarize those readers with the topics being discussed, an overview of the product support strategy will be provided as a part of the introduction. The RESTORE application is one of the most successful implementations of Knowledge Based Systems (KBS) and Case-Based Reasoning (CBR) technology in IBM history. It is currently deployed and used very successfully in AS/400 product support, saving tens of millions of dollars in support cost each year. This complex system was designed with a blackboard architecture to enable the use of mUltiple knowledge sources. These knowledge sources include several rule-based subsystems and a large case-based subsystem, all tied together with a blackboard control module. This innovative design takes advantage of the strength of KBS and CBR by using the concept of blackboard architecture that allows these knowledge-based subsystems to coexist and contribute. This paper will discuss the design of the overall RESTORE system along with its successful implementation

#index 1478916
#* CREWS_NS: scheduling train crew in the Netherlands
#@ Ernesto M. Morgado;João P. Martins
#t 1997
#c 10
#% 36784
#! We present a system, CREWS_NS, that is used in the long-term scheduling of drivers and guards of the Dutch Railways. CREWS_NS schedules the work of about 5,000 people. CREWS_NS is built on top of CREWS, a scheduling tool for speeding the development of scheduling applications. CREWS heavily relies on the use of AI techniques and has been built in the perspective of a "white box" system, in the sense that the planner can perceive what is going on, can interact with the system by proposing alternatives or querying decisions, and can adapt the behaviour of the system to changing circumstances. Scheduling can be done in automatic, semi-automatic or manual mode. CREWS has mechanisms for dealing with the constant changes that occur in input data, can identify the consequences of the change and guides the planner in accommodating the changes in the already built schedules (re-scheduling).

#index 1478917
#* The scheduling of rail at union pacific railroad
#@ Kathleen Murphy;Elizabeth Ralston;David Friedlander;Rodney Swab;Paul Steege
#t 1997
#c 10
#% 404772
#! The Union Pacific Railroad (UPRR) has over 31,000 miles of track covering a 24 state region. Planning and scheduling the production, packaging, delivery, and pickup of rail, involved in the maintenance of this network, is a very complex task. Manually scheduling only a subset of the resources required has historically taken several days to accomplish. Moreover, the inability to fully schedule all resources can lead to inefficient resource utilization. This paper describes the Rail Train Scheduler (RTS), designed and developed to capture the expertise of the UPRR scheduler, generate production schedules of all the resources involved, and provide a decision support tool for determining the best mix of resources required. RTS is an expert system that uses constraint satisfaction and domain specific heuristics to produce good, low cost schedules. It has been deployed since January, 1996. UPRR anticipates a savings of about $500,000 per year from the use of RTS.

#index 1478918
#* SunRay V - an intelligent container trucking operations management and control system
#@ Ian Ng;Andrew Gill;Ian Chia;Mei-Leng Koh;Chris Yeung;Lih-Wee Chew
#t 1997
#c 10
#% 56471
#! This paper describes SunRay V, an intelligent system designed to support the container trucking companies in the management and control of their operations, especially during the planning. The purpose of the paper is to describe briefly the business domain, the motivation for computerisation, some of the main design considerations of the system and the constraint programming approach. The paper also presents some of the problems encountered during deployment, how they are overcome and the lessons learned from the process. Finally, it discusses some of the benefits that resulted from the operational use of the system.

#index 1478919
#* DISXPERT: a social security disability screening expert system
#@ James R. Nolan
#t 1997
#c 10
#% 140953
#% 444697
#! This case study paper reports on the development and implementation of DISXPERT, a rule-based expert system tool for referral of social security disability recipients to vocational rehabilitation services. The growing use of paraprofessionals as caseworkers responsible for assessment in the social services area provides fertile domain areas for new and innovative application of expert systems technology. The main function of DISXPERT is to provide support to paraprofessional caseworkers in reaching unbiased and consistent assessment decisions regarding referral of clients to vocational rehabilitation services. The results after three years of use demonstrate that paraprofessionals using DISXPERT can make assessments in less time and with a level of accuracy superior to the vocational rehabilitation domain professionals using manual methods. This case study paper discusses the problem domain, the design and development of the system, uses of AI technology, payoffs, and deployment and maintenance of the system.

#index 1478920
#* STHANA: profitability forecast and situation analysis for automated teller machines
#@ Cyril Way
#t 1997
#c 10
#% 136350
#% 168280
#% 232102
#% 405727
#% 1272280
#! The French credit card system makes it highly profitable for banks to have heavily used Automated Teller Machines (ATM). "La Caisse d'Épargne", one of the major French bank manages 5000 ATMs allover France. The goal of the Sthana system is to capitalize the knowledge spread all over the company into a system capable of issuing recommendations for existing ATM's and capable of forecasting a new ATM's activity. Sthana uses Data Mining and Case-Based-Reasoning techniques so as to extract information from existing data (including economic, geographical and internal bank data) and from the bank's ATM experts. The system builds up classifications on high level descriptors from raw data and eventually indicates a measure of the ATM's activity and profitability, highlights factors which could lead to higher profitability or pinpoints the ATM's vulnerabilities. An objectoriented model coupled with an extremely modular system allows the data and rules to be customized for geographical units of the bank. Sthana has been deployed and customized for different geographical units, but the knowledge base is centralized in the bank headquarters in Paris.

#index 1478921
#* ChemReg: using case-based reasoning to support health and safety compliance in the chemical industry
#@ Kirk D. Wilson
#t 1997
#c 10
#% 168280
#% 405727
#% 461717
#! ChemReg is a large knowledge-based system used by Air Products and Chemicals, Inc. to support compliance with regulatory requirements for communicating health and safety information in the shipping and handling of chemical products. This presentation concentrates on one of the knowledge bases in this system: the case-based reasoner. The case-based reasoner addresses the issue, How can proper communication of public health and safety information be insured while allowing for rapid and costeffective product evaluation in the absence of actual hazard testing of the product? Estimates of hazard data from similar products are generated for new products using an existing relational database as a case library. Implementation of the cased-based reasoner in rules and objects using a commercial KBS shell is described. While some refinements remain, the performance of the casebased reasoner has met expectations.

#index 1478922
#* The role of wordnet in the creation of a trainable message understanding system
#@ Amit Bagga;Joyce Yue Chai;Alan W. Biermann
#t 1997
#c 10
#% 748703
#% 814963
#% 815010
#% 815330
#% 815335
#% 815336
#% 815338
#% 815342
#% 815343
#% 817933
#% 1275285
#! The explosion in the amount of free text materials on the Internet, and the use of this information by people from all walks of life, has made the issue of generalized information extraction a central one in Natural Language Processing. We have built a system that attempts to provide any user with the ability to efficiently create and customize, for his or her own application, an information extraction system with competitive precision and recall statistics. The use of WordNet in the design of the system helps take the computational linguist out of the process of customizing the system to each new domain. This is achieved by using WordNet to minimize the effort on the end user for building the semantic knowledge bases and writing the regular patterns for new domains.

#index 1478923
#* Using a robot control architecture to automate space shuttle operations
#@ R. Peter Bonasso;David Kortenkamp;Troy Whitney
#t 1997
#c 10
#% 97619
#% 263059
#! This paper describes preliminary results from using an AI robot control software architecture, known as 3T as the software framework for a procedure tracking system for the space shuttle Remote Manipulator System (RMS). The system, called 3TPT, is designed to track the expected steps of the crew as they carry out RMS operations, detecting malfunctions in the RMS system from failures or improper configurations as well as improper or incomplete procedures by the crew. Scheduled for a ground demonstration in February 1997, and a test flight the following fall, 3TPT was employed this past fall to track the RMS check~ut procedures on a space shuttle mission. It successfully carried out its task because the reactive nature of the architecture allowed it to stay synchronized with the procedures even in the face of intermittent loss of telemetry and unexpected crew actions.

#index 1478924
#* Anymation with CATHI
#@ Andreas Butz
#t 1997
#c 10
#% 82064
#% 94771
#% 98412
#% 145421
#% 213567
#% 220099
#% 520609
#% 1499487
#! This paper presents an incremental approach to the automated generation of 3D animation clips for the explanation of technical devices. It describes the system CATHI, which is part of the intelligent multimedia presentation system PPP. CATHI generates animation clips in the context of a coordinated multimedia document. The system is able to generate animation scripts and to present the corresponding animations with a minimal delay, i.e. within a few seconds. This allows it to be used for on-line and on-demand generation of presentations. The structure of the generated animation depends not only on the given communicative content. It is also influenced by a number of generation parameters. Furthermore, the design of the final animation depends on resource limitations, such as the available computing capacity and the graphical capacity of the output medium. The more powerful the machine is, the more complex the animation scripts generated by CATHI tend to be, but a fluent output within a given amount of time is achieved on (nearly) any machine - hence the name 'anymation'.

#index 1478925
#* Automated generation of tracking plans for a network of communications antennas
#@ S. Chien;A. Govindjee;T. Estlin;X. Wang;R. Hill, Jr.
#t 1997
#c 10
#% 215888
#% 647206
#! This paper describes the Deep Space Network Antenna Operations Planner (DPLAN) : a system that automatically generates antenna tracking plans for a set of highly sensitive radio science and telecommunications antennas. DPLAN accepts as input an equipment configuration and a set of requested antenna track services. The system then uses a knowledge base of antenna operations procedures to produce a plan of activities that will provide the requested services using the allocated equipment. DPLAN produces this plan using an integration of hierarchical task network (HTN) and operator-based planning. A prototype of the DPLAN system was successfully demonstrated in February 1995 at NASA's experimental DSN station, DSS-13, on a series of Voyager tracks. Based on this successful demonstration, DPLAN is being considered for inclusion in the larger Network Monitor and Control (NMC) upgrade underway projected to save NASA over $9 million per year in operations costs.

#index 1478926
#* Building brains for rooms: designing distributed software agents
#@ Michael H. Coen
#t 1997
#c 10
#% 18600
#% 94227
#% 97619
#% 170961
#% 668819
#% 669494
#% 1476317
#! This paper argues that complex, embedded software agent systems are best constructed with parallel, layered architectures. These systems resemble Minskian Societies of Mind and Brooksian subsumption controllers for robots, and they demonstrate that complex behaviors can be had via the aggregates of relatively simple interacting agents. We illustrate this principle with a distributed software agent system that controls the behavior of our laboratory's Intelligent Room.

#index 1478927
#* Mulitmodal interaction for distributed interactive simulation
#@ Philip R. Cohen;Michael Johnston;David McGee;Sharon Oviatt;Jay Pittman;Ira Smith;Liang Chen;Josh Clow
#t 1997
#c 10
#% 96323
#% 114512
#% 117568
#% 127465
#% 131776
#% 145659
#% 149118
#% 159110
#% 172394
#% 174558
#% 214662
#% 232927
#% 541670
#% 603592
#% 742396
#% 969164
#% 1134782
#% 1499469
#! This paper presents an emerging application of Artificial Intelligence research to distributed interactive simulations, with the goal of reducing exercise generation time and effort, yet maximizing training effectiveness. We have developed the QuickSet prototype, a pen/voice system running on a hand-held PC, communicating via wireless LAN through an agent architecture to NRaD's LeatherNet system, a distributed interactive training simulator built for the US Marine Corps. The paper describes our novel multi modal integration strategy offering mutual compensation among modalities, as well as QuickSet's agent-based infrastructure, and provides an example of multimodal simulation setup. Finally, we discuss our applications experience and lessons learned.

#index 1478928
#* ADDVAC: applying active design documents for the capture, retrieval and use of rationale during offshore platform VAC design
#@ Ana Cristina Bicharra Garcia;Joper Cezar de Andrade;Rogério Ferreira Rodrigues;Ricardo Moura
#t 1997
#c 10
#% 51395
#% 58877
#% 1273489
#! Engineering design is a plan to build an artifact that needs to be communicated and discussed among designers. The concept is an abstract idea that can only be transmitted through a document containing the description of the artifact's form, function and behavior as well as the designer's rationale for them. Most engineering design domains are multidisciplinary. Understanding design decisions and their rationale becomes crucial to allow collaborative work. The Active Design Documents (ADD) approach has been very effective in assisting the capture, retrieval and use of rationale during the design process in various engineering domains. This paper presents the development and results of ADD applied to the domain of Ventilation and Air Conditioning (VAC) design of offshore oil production platforms. We focus our discussion on the way rationale availability improved the design process itself and on the problems encountered during development.

#index 1478929
#* MITA: an information extraction approach to analysis of free-form text in life insurance applications
#@ Barry Glasgow;Alan Mandell;Dan Binney;Lila Ghemri;David Fisher
#t 1997
#c 10
#% 110017
#% 210985
#% 217064
#% 496743
#% 1290067
#! MetLife processes over 300,000 life insurance applications a year. Undernriting of these applications is labor intensive. Automation is difficult since they include many free-form text fields. MITA, MetLife's Intelligent Text Analyzer, uses the Infonnation Extraction --IE-- technique of Natural Language Processing to structure the extensive text fields on a life insurance application. Knowledge engineering, with the help of underwriters as domain experts, was performed to elicit significant concepts for both medical and occupational text fields. A corpus of 20,000 life insurance applications provided the syntactical and semantic patterns in which these underwriting concepts occur. The extracted information can then be analyzed by conventional knowledge based systems. We project that MITA and knowledge based analyzers will increase underwriting productivity by 20 to 30%.

#index 1478930
#* Blackboard agents for mixed-initiative management of integrated process-planning/production-scheduling solutions across the supply chain
#@ David W. Hildum;Norman M. Sadeh;Thomas J. Laliberty;John McA'Nulty;Stephen F. Smith;Dag Kjenstad
#t 1997
#c 10
#% 317964
#! As companies increasingly customize their products, move towards smaller lot production and experiment with more flexible customer/supplier arrangements, they increasingly require the ability to respond quickly, accurately and competitively to customer requests for bids on new products and efficiently work out supplier/subcontractor arrangements for these new products. This in turn requires the ability to rapidly convert standard-based product specifications into process plans and quickly integrate new orders with their process plans into existing production schedules across the supply chain. This paper describes IP3S, a blackboard-based agent for supporting integrated process planning/production scheduling across the supply chain. IP3S agents support concurrent development and dynamic revision of integrated process-planning/production-scheduling solutions across the supply chain, maintenance of multiple problem instances and solutions across the supply chain, flexible user-oriented decision making, declarative representation of control information, the use of a common representation for exchanging information, coordination with other planning/scheduling agents and information sources, and ease of integration with legacy systems. The IP3S agent has been customized for and validated in the context of a large and highly dynamic machine shop at Raytheon's Andover manufacturing facility. Empirical results show an average performance improvement of 23% in solution quality over a decoupled approach to building process-planning/production-scheduling solutions.

#index 1478931
#* Intelligent agents for the synthetic battlefield: a company of rotary wing aircraft
#@ Randall W. Hill, Jr.;Johnny Chen;Jonathan Gratch;Paul Rosenbloom;Milind Tambe
#t 1997
#c 10
#% 75896
#% 120806
#% 121991
#% 179879
#% 189698
#% 395698
#% 449587
#% 451031
#% 1274153
#% 1280065
#% 1478733
#% 1499477
#! We have constructed a team of intelligent agents that perform the tasks of an attack helicopter company for a synthetic battlefield environment used for running large-scale military exercises. We have used the Soar integrated architecture to develop: (1) pilot agents for a company of helicopters, (2) a command agent that makes decisions and plans for the helicopter company, and (3) an approach to teamwork that enables the pilot agents to coordinate their activities in accomplishing the goals of the company. This case study describes the task domain and architecture of our application, as well as the benefits and lessons learned from applying AI technology to this domain.

#index 1478932
#* Information extraction based multiple-category document classification for the global legal information network
#@ Richard D. Holowczak;Nabil R. Adam
#t 1997
#c 10
#% 67565
#% 165115
#% 185289
#% 242735
#% 461691
#% 461692
#% 814984
#% 1290067
#! This paper describes a prototype application of an information extraction (IE) based document classification system in the international law domain. IE is used to determine if a set of concepts for a class are present in a document. The syntactic and semantic constraints that must be satisfied to make this determination are derived automatically from a training corpus. A collection of IE systems are arranged in a classification hierarchy and novel documents are guided down the hierarchy based on the results from the previous level. Experimental results for a research prototype are given on a subset of the Global Legal Information Network domain.

#index 1478933
#* An intelligent control architecture for accelerator beamline tuning
#@ William B. Klein;Carl R. Stern;George F. Luger;Eric T. Olsson
#t 1997
#c 10
#% 10804
#% 146444
#% 146451
#% 1014710
#% 1268736
#! This paper discusses a new architecture for accelerator tuning that combines heuristic and knowledge based methods with traditional approaches to control. Control of particle accelerators requires a hybrid architecture, which includes methodologies for planning, intelligent search, and pattern recognition. Control is distributed and hierarchical to utilize parallel problem-solving in the face of time-sensitive control requirements and to decompose complex control problems into more manageable subtasks. For perspective, we discuss past attempts at accelerator control and why these attempts left many issues unresolved. We describe the details of our control architecture along with its motivation. We then report the results of deploying and testing it at two accelerator facilities. This paper ends with a discussion of the commercial importance of this work.

#index 1478934
#* Attitude and position control using real-time color tracking
#@ David P. Miller;Anne Wright;Randy Sargent;Rob Cohen;Teresa Hunt
#t 1997
#c 10
#! A variety of sensors and positioning methods have been developed over the years. Most methods rely on active sensors (such as sonars or lasers) which have range and power restrictions, or rely on computationally complex (and often slow) methods such as recovering position from stereo or optical flow. This paper describes a system that can determine a robot's position and orientation, in all six degrees of freedom, relative to a simple passive target. The system can determine its position relative to the target from a single image frame, and process the image and calculate the position faster than a camera's frame rate (60Hz). It uses a standard, uncalibrated color video camera as its sensor. The system runs on an inexpensive microprocessor (a Motorola 68332) leaving many cycles left over. By careful design of the target and the vision processing hardware and software, we are able to selectively perceive only the relevant parts of the target, greatly simplifying and speeding all the necessary computations. The system is being developed for autonomous spacecraft station keeping and docking.

#index 1478935
#* A hybrid architecture for real-time mixed-initiative planning and control
#@ Steven W. Mitchell
#t 1997
#c 10
#% 42214
#% 44836
#% 124580
#% 124587
#% 124647
#% 160202
#% 179879
#% 179965
#% 1273423
#! In many mission critical applications current technology is inadequate for fully automatic planning and control. In these applications society insists that planning and control be exercised by human minds. However, many such applications lie at the brittle edge of human capabilities. This has lead to serious incidents such as those involving the USS Stark and the USS Vincennes in the Persian Gulf. In domains like these where full automation is unacceptable and purely human operation is inadequate, a promising approach is one which combines the strengths of humans and computers. This paper describes one architecture for addressing this challenge. Interacting with human domain experts in a mixed initiative mode it combines elements of case-based and model-based reasoning in a hierarchical task network decomposition planner to generate plans, and uses multivariate utility theory to evaluate the plans. The architecture includes real-time monitoring of plan execution, and automatic replanning for plan failure or significant changes in the environment. The planner has been implemented in C and C++, and used as the Tactical Response Planner for the DARPA Ship Systems Automation (SSA) program.

#index 1478936
#* Intelligent command control for VLSI CAD systems
#@ Motohide Otsubo;Satoru Fujita;Toru Yamanouchi
#t 1997
#c 10
#% 68238
#% 205385
#% 1650688
#! High-performance CAD systems help designers of VLSI logic to synthesize optimal circuits. With these, users are able to repeatedly select and invoke the most appropriate-looking command (which corresponds to an algorithm). They are hard for novice users to operate, however, because efficient command-selection requires experience. In the past, novices used a "command script with heuristics" for selecting commands and automating command invocation, but the heuristics needed to be rewritten for any updated version of the CAD system being used, and users were unable to place their own deadlines on the time within which the design results had to be obtained. To cope with these problems, we have developed an Intelligent Command Control Shell (ICCS) which performs logic synthesis tasks by automatically selecting and executing multiple sequences of commands within a pre-set time limit or until it obtains an adequate circuit. ICCS uses easily up datable statistical data as its knowledge base. ICCS also features "time-constrained control", which takes imposed deadlines into account in its selection of commands, so as to produce the best possible circuit within a given time limit. When applied to the design of large-scale practical circuits, the use of ICCS resulted in circuits with 6% shorter delay on average (and 30% shorter delay in the best case) than those obtained with simple optimization command.

#index 1478937
#* A generic knowledge-base browser and editor
#@ Suzanne M. Paley;John D. Lowrance;Peter D. Karp
#t 1997
#c 10
#% 103050
#% 110011
#% 156337
#% 192377
#% 192492
#% 405391
#% 1275324
#% 1275326
#! The GKB Editor is a generic editor and browser of knowledge bases (KBs) and ontologies -- generic in the sense that it is portable across several frame knowledge representation systems (FRSs). This generality is possible because the GKB Editor performs all KB access operations using a generic application programming interface to FRSs called the Generic Frame Protocol (GFP). To adapt the GKB Editor to a new FRS, we need only to create a GFP implementation for that FRS - a task that is usually considerably simpler than implementing a complete KB editor. The GKB Editor also contains several relatively advanced features, including three different viewers of KB relationships, incremental browsing of large graphs, KB analysis tools, extensive customizability, complex selection operations, cut-and-paste operations, and both user- and KB-specific profiles. The GKB Editor is in active use in the development of several ontologies and KBs. This paper discusses the design of the GKB Editor from a graphical user interface point of view, and describes the difficulties encountered in achieving true portability across multiple FRSs.

#index 1478938
#* Multiple fault diagnosis from FMEA
#@ Chris Price;Neil Taylor
#t 1997
#c 10
#% 21138
#% 67569
#! The Failure Mode and Effects Analysis (FMEA) design discipline involves the examination at design time of the consequences of potential component failures on the functionality of a system. It is clear that this type of information could also prove useful for diagnostic purposes. Unfortunately, this information cannot be fully utilised for diagnosis when FMEA has been performed by human engineers, because of inconsistencies in effect descriptions. The FMEA process is also very. time consuming, with the consequence that the engineer can only deal with single point failures. Automation of the electrical FMEA process facilitates information reuse for diagnosis by providing consistent descriptions of failure effects, and by speeding up the FMEA process to such an extent that it becomes feasible to examine multiple failures. This paper introduces the advantages that automated FMEA provides for diagnosis, and describes its use for generating fault trees from the FMEA report. The paper examines the current limitations of FMEA use for diagnosis, and reports on how these limitations may be overcome.

#index 1478939
#* Smokey: automatic recognition of hostile messages
#@ Ellen Spertus
#t 1997
#c 10
#% 124010
#% 127850
#% 136350
#! Abusive messages (flames) can be both a source of frustration and a waste of time for Internet users. This paper . describes some approaches to flame recognition, mcluding a prototype system, Smokey. Smokey builds a 47-element feature vector based on the syntax and semantics of each sentence, combining the vectors for the sentences within each message. A training set of 720 messages was used by Quinlan's C4.5 decision-tree generator to determine feature-based rules that were able to correctly categorize 64% of the flames and 98% of the nonflames in a separate test set of 460 messages. Additional techniques for greater accuracy and user customization are also discussed.

#index 1478940
#* MultiADD: a multiagent active design document model to support group design
#@ Adriana Santarosa Vivacqua;Ana Cristina Bicharra Garcia
#t 1997
#c 10
#% 159109
#% 159110
#% 179004
#% 1133886
#! In this paper we discuss the use of multiagent systems to assist the design task of engineering artifacts. We augmented the active design document approach to assist the design activity when done by a group. Task division information flow and conflict management are the main issues when working in a group. This paper reports initial results on applying our multiagent active design document system (MultiADD) to support conflict mitigation in group design. The discussion focuses mainly on the questions related to information flow: what, when and to whom to inform while in a conflict situation. We present our model and an example taken from our implemented system, which uses this technology to support process plant design of offshore oil platform. The system has been used by the Brazilian Oil Company. In addition to speeding up the process due to the design support tool, meetings and scheduling times have been greatly reduced. Consistency among the design pieces and an increase of alternative checking have also been noticed. Most importantly we expect that the environment encourages cooperation among design participants.

#index 1478941
#* Rationale-supported mixed-initiative case-based planning
#@ Manuela M. Veloso;Alice M. Mulvehill;Michael T. Cox
#t 1997
#c 10
#% 179766
#% 360013
#% 362441
#% 459729
#% 494118
#% 647206
#! Mixed-initiative planning envisions a framework in which automated and human planners interact to jointly construct plans that satisfy specific objectives. In this paper, we report on our work engineering a robust mixed-initiative planning system. Human planners rely strongly on past planning experience to generate new plans. ForMAT is a case-based system that supports human planning through the accumulation of user-built plans, query-driven browsing of past plans, and several plan functionality analysis primitives. Prodigy/Analogy is an automated AI planner that combines generative and case-based planning. Stored plans are annotated with plan rationale and reuse involves adaptation driven by this rationale. Our system, MI-CBP, integrates ForMAT and Prodigy/Analogy into a real-time message-passing mixed-initiative planning system. The main technical approach consists of allowing the user to specify and link objectives that enable the system to capture and reuse plan rationale. We present MI-CBP and its concrete application to the domain of military force deployment planning. This synergistic system increases the planning efficiency of human planners through automated suggestion of similar past plans and plausible plan modifications.

#index 1478942
#* IDS: improving aircraft fleet maintenance
#@ Rob Wylie;Robert Orchard;Michael Halasz;François Dubé
#t 1997
#c 10
#% 51241
#% 360128
#! This paper describes the Integrated Diagnostic System (IDS), an applied AI project concerned with the development of hybrid information systems to diagnose problems and help manage repair processes of commercial aircraft fleets. A study at one major airline indicated that significant benefits could accrue (approximately 2% of overall maintenance budget) through the use of innovative information technology. The IDS prototype (currently in extended field trial) takes as input a stream of messages representing maintenance and diagnostic events. These are filtered and aggregated in order to yield information in an appropriate form for various decision making tasks (and in particular for the maintenance staff while performing fault isolation and repair procedures). IDS was built using ART*Enterprise® and makes extensive use of its rule-based and case-based reasoning facilities in order to apply various sources of knowledge (manuals, heuristics, historical data) to this problem. As well as technical issues, this paper discusses the motivation for and methodology followed in this project.

#index 1499465
#* Proceedings of the thirteenth national conference on Artificial intelligence - Volume 1
#@ 
#t 1996
#c 10

#index 1499466
#* Agent amplified communication
#@ Henry Kautz;Bart Selman
#t 1996
#c 10
#% 55490
#% 146494
#% 159113
#% 159121
#% 173780
#% 405233
#% 978159
#! We propose an agent-based framework for assisting and simplifying person-to-person communication for information gathering tasks. As an example, we focus on locating experts for any specified topic. In our approach, the informal person-to-person networks that exist within an organization are used to "referral chain" requests for expertise. User-agents help automate this process. The agents generate referrals by analyzing records of email communication patterns. Simulation results show that the higher responsiveness of an agent-based system can be effectively traded for the higher accuracy of a completely manual approach. Furthermore, preliminary experience with a group of users on a prototype system has shown that useful automatic referrals can be found in practice. Our experience with actual users has also shown that privacy concerns are central to the successful deployment of personal agents: an advanced agent-based system will therefore need to reason about issues involving trust and authority.

#index 1499467
#* The ContactFinder agent: answering bulletin board questions with referrals
#@ Bruce Krulwich;Chad Burkey
#t 1996
#c 10
#% 166352
#% 1499473
#! ContactFinder is an intelligent agent whose approach to assisting users is valuable and innovative in the following four ways. First, ContactFinder operates proactively in reading and responding to messages on electronic bulletin boards rather than acting in response to user queries. Second, ContactFinder assists users by referring them to other people who can help them, rather than attempting to find information that directly answers the user's specific question. Third, ContactFinder categorizes messages and extracts their topic areas using a set of heuristics that are very efficient and demonstrably highly effective. Fourth, ContactFinder posts its referrals back to the bulletin boards rather than simply communicating with specific users, to increase the information density and connectivity of the system. This paper discusses these aspects of the system and demonstrates their effectiveness in over six months of use on a large-scale internal bulletin board.

#index 1499468
#* Deciding to remind during collaborative problem solving: empirical evidence for agent strategies
#@ Pamela W. Jordan;MariIyn A. Walker
#t 1996
#c 10
#% 9197
#% 164546
#% 168422
#% 177915
#% 179887
#% 188076
#% 199723
#% 207678
#% 780333
#% 1273613
#% 1288579
#! Previous work suggests that reminding a conversational partner of mutually known information depends on the conversants' attentional state, their resource limits and the resource demands of the task. In this paper, we propose and evaluate several models of how an agent decides whether or not to communicate a reminder. We elaborate on previous findings by exploring how attentional state and resource bounds are incorporated into the decision making process so that reminders aid the performance of agents during collaborative problem solving. We test two main hypotheses using a multi-agent problem solving simulation testbed: (1) an agent decides to present salient knowledge only when it reduces overall problem solving effort (2) an agent can use its own attentional state as a model of the attentional state of its partner when assessing the effort trade-offs of communicating a reminder. Our results support both hypotheses, suggesting that the models we propose should be further tested for multi-agent communication in problem solving situations.

#index 1499469
#* Toward a semantics for an agent communications language based on speech0-acts
#@ Ira A. Smith;Philip R. Cohen
#t 1996
#c 10
#% 2856
#% 52299
#% 68239
#% 136356
#% 172392
#% 172394
#% 179887
#% 241287
#% 374945
#% 1273588
#% 1499477
#! Systems based on distributed agent architectures require an agent communications language having a clearly defined semantics. This paper demonstrates that a semantics for an agent communications language can be founded on the premise that agents are building, maintaining, and disbanding teams through their performance of communicative acts. This view requires that definitions of basic communicative acts, such as requesting, be recast in terms of the formation of a joint intention - a mental state that has been suggested underlies team behavior. To illustrate these points, a semantics is developed for a number of communication actions that can form and dissolve teams. It is then demonstrated how much of the structure of popular finite-state dialogue models, such as Winograd and Flores' basic conversation for action, follows as a consequence of the logical relationships that are created by the redefined communicative actions.

#index 1499470
#* Planning to gather inforrnation
#@ Chung T. Kwok;Daniel S. Weld
#t 1996
#c 10
#% 21145
#% 100159
#% 159113
#% 163715
#% 179945
#% 188853
#% 198466
#% 1290115
#! We describe Occam, a query planning algorithm that determines the best way to integrate data from different sources. As input, Occam takes a library of site descriptions and a user query. As output, Occam automatically generates one or more plans that encode alternative ways to gather the requested information. Occam has several important features: (1) it integrates both legacy systems and full relational databases with an efficient, domain-independent, query-planning algorithm, (2) it reasons about the capabilities of different information sources, (3) it handles partial goal satisfaction i.e., gathers as much data as possible when it can't gather exactly all that the user requested, (4) it is both sound and complete, (5) it is efficient. We present empirical results demonstrating Occam's performance on a variety of information gathering tasks.

#index 1499471
#* Query-answering algorithms for information agents
#@ Alon Y. Levy;Anand Rajaraman;Joann J. Ordille
#t 1996
#c 10
#% 36683
#% 85089
#% 111913
#% 111922
#% 159113
#% 179945
#% 188853
#% 198465
#% 198466
#% 213437
#% 213982
#% 1290115
#% 1499470
#! We describe the architecture and query-answering algorithms used in the Information Manifold, an implemented information gathering system that provides uniform access to structured information sources on the World-Wide Web. Our architecture provides an expressive language for describing information sources, which makes it easy to add new sources and to model the fine-grained distinctions between their contents. The query-answering algorithm guarantees that the descriptions of the sources are exploited to access only sources that are relevant to a given query. Accessing only relevant sources is crucial to scale up such a system to large numbers of sources. In addition, our algorithm can exploit run-time information to further prune information sources and to reduce the cost of query planning.

#index 1499472
#* Hybrid hill-climbing and knowledge-based methods for intelligent news filtering
#@ Kenrick J. Mock
#t 1996
#c 10
#% 124010
#% 124012
#% 149383
#% 166352
#% 198058
#% 367036
#% 703126
#% 840583
#! As the size of the Internet increases, the amount of data available to users has dramatically risen, resulting in an information overload for users. This work involved the creation of an intelligent information news filtering system named INFOS (Intelligent News Filtering Organizational System) to reduce the user's search burden by automatically eliminating Usenet news articles predicted to be irrelevant. These predictions are learned automatically by adapting an internal user model that is based upon features taken from articles and collaborative features derived from other users. The features are manipulated through keyword-based techniques and knowledge-based techniques to perform the actual filtering. Knowledge-based systems have the advantage of analyzing input text in detail, but at the cost of computational complexity and the difficulty of scaling up to large domains. In contrast, statistical and keyword approaches scale up readily but result in a shallower understanding of the input. A hybrid system integrating both approaches improves accuracy over keyword approaches, supports domain knowledge, and retains scalability. The system would be enhanced by more robust word disambiguation.

#index 1499473
#* Syskill & webert: Identifying interesting web sites
#@ Michael Pazzani;Jack Muramatsu;Daniel Billsus
#t 1996
#c 10
#% 92148
#% 449588
#! We describe Syskill & Webert, a software agent that learns to rate pages on the World Wide Web (WWW), deciding what pages might interest a user. The user rates explored pages on a three point scale, and Syskill & Webert learns a user profile by analyzing the information on each page. The user profile can be used in two ways. First, it can be used to suggest which links a user would be interested in exploring. Second, it can be used to construct a LYCOS query to find pages that would interest a user. We compare six different algorithms from machine learning and information retrieval on this task. We find that the naive Bayesian classifier offers several advantages over other learning algorithms on this task. Furthermore, we find that an initial portion of a web page is sufficient for making predictions on its interestingness substantially reducing the amount of network transmission required to make predictions.

#index 1499474
#* Learning models of intelligent agents
#@ David Carmel;Shaul Markovitch
#t 1996
#c 10
#% 30037
#% 31215
#% 160327
#% 184055
#% 404772
#% 542161
#! Agents that operate in a multi-agent system need an efficient strategy to handle their encounters with other agents involved. Searching for an optimal interactive strategy is a hard problem because it depends mostly on the behavior of the others. In this work, interaction among agents is represented as a repeated two-player game, where the agents' objective is to look for a strategy that maximizes their expected sum of rewards in the game. We assume that agents' strategies can be modeled as finite automata. A model-based approach is presented as a possible method for learning an effective interactive strategy. First, we describe how an agent should find an optimal strategy against a given model. Second, we present an unsupervised algorithm that infers a model of the opponent's automaton from its input/output behavior. A set of experiments that show the potential merit of the algorithm is reported as well.

#index 1499475
#* Cooperative learning over composite search spaces experiences with a multi-agent design system
#@ M. V. Nagendra Prasad;Susan E. Lander;Victor R. Lesser
#t 1996
#c 10
#% 68352
#% 164502
#% 443113
#% 465704
#% 676637
#! We suggest the use of two learning techniques - short term and long term - to enhance search efficiency in a multi-agent design system by letting the agents learn about non-local requirements on the local search process. The first technique allows an agent to accumulate and apply constraining information about global problem solving, gathered as a result of agent communication, to further problem solving within the same problem instance. The second technique is used to classify problem instances and appropriately index and retrieve constraining information to apply to new problem instances. These techniques will be presented within the context of a multi-agent parametric-design application called STEAM. We show that learning conclusively improves solution quality and processing-time results.

#index 1499476
#* Scaling up: distributed machine learning with cooperation
#@ Foster John Provost;Daniel N. Hennessy
#t 1996
#c 10
#% 89776
#% 90155
#% 90212
#% 94224
#% 156186
#% 179770
#% 197059
#% 204528
#% 319452
#% 449566
#% 449588
#% 458178
#% 465582
#% 1272179
#% 1274561
#% 1290030
#! Machine-learning methods are becoming increasingly popular for automated data analysis. However, standard methods do not scale up to massive scientific and business data sets without expensive hardware. This paper investigates a practical alternative for scaling up: the use of distributed processing to take advantage of the often dormant PCs and workstations available on local networks. Each workstation runs a common rule-learning program on a subset of the data. We first show that for commonly used rule-evaluation criteria, a simple form of cooperation can guarantee that a rule will look good to the set of cooperating learners if and only if it would look good to a single learner operating with the entire data set. We then show how such a system can further capitalize on different perspectives by sharing learned knowledge for significant reduction in search effort. We demonstrate the power of the method by learning from a massive data set taken from the domain of cellular fraud detection. Finally, we provide an overview of other methods for scaling up machine learning.

#index 1499477
#* Tracking dynamic team activity
#@ Milind Tambe
#t 1996
#c 10
#% 69167
#% 75896
#% 125386
#% 191678
#% 394184
#% 518658
#% 1275240
#! AI researchers are striving to build complex multi-agent worlds with intended applications ranging from the RoboCup robotic soccer tournaments, to interactive virtual theatre, to large-scale real-world battlefield simulations. Agent tracking - monitoring other agent's actions and inferring their higher-level goals and intentions - is a central requirement in such worlds. While previous work has mostly focused on tracking individual agents, this paper goes beyond by focusing on agent teams. Team tracking poses the challenge of tracking a team's joint goals and plans. Dynamic, real-time environments add to the challenge, as ambiguities have to be resolved in real-time. The central hypothesis underlying the present work is that an explicit team-oriented perspective enables effective team tracking. This hypothesis is instantiated using the model tracing technology employed in tracking individual agents. Thus, to track team activities, team models are put to service. Team models are a concrete application of the joint intentions framework and enable an agent to track team activities, regardless of the agent's being a collaborative participant or a non-participant in the team. To facilitate real-time ambiguity resolution with team models: (i) aspects of tracking are cast as constraint satisfaction problems to exploit constraint propagation techniques; and (ii) a cost minimality criterion is applied to constrain tracking search. Empirical results from two separate tasks in real-world, dynamic environments - one collaborative and one competitive - are provided.

#index 1499478
#* Nearly monotonic problems: a key to effective FA/C distributed sensor interpretation?
#@ Norman Carver;Victor Lesser;Robert Whitehair
#t 1996
#c 10
#% 32357
#% 44876
#% 161243
#% 215582
#% 677117
#% 693619
#! The functionally-accurate, cooperative (FA/C) distributed problem-solving paradigm is one approach for organizing distributed problem solving among homogeneous, cooperating agents. A key assumption of the FA/C model has been that the agents' local solutions can substitute for the raw data in determining the global solutions. This is not the case in general, however. Does this mean that researchers' intuitions have been wrong and/or that FA/C problem solving is not likely to be effective? We suggest that some domains have a characteristic that can account for the success of exchanging mainly local solutions. We call such problems nearly monotonic. This concept is discussed in the context of FA/C-based distributed sensor interpretation.

#index 1499479
#* Analysis of utility-theoretic heuristics for intelligent adaptive network routing
#@ Armin R. Mikler;Vasant Honavar;Johnny S. K. Wong
#t 1996
#c 10
#% 241
#% 9240
#% 22388
#% 226307
#! Utility theory offers an elegant and powerful theoretical framework for design and analysis of autonomous adaptive communication networks. Routing of messages in such networks presents a real-time instance of a multi-criterion optimization problem in a dynamic and uncertain environment. In this paper, we incrementally develop a set of heuristic decision functions that can be used to guide messages along a near-optimal (e.g., minimum delay) path in a large network. We present an analysis of properties of such heuristics under a set of simplifying assumptions about the network topology and load dynamics and identify the conditions under which they are guaranteed to route messages along an optimal path. The paper concludes with a discussion of the relevance of the theoretical results presented in the paper to the design of intelligent autonomous adaptive communication networks and an outline of some directions of future research.

#index 1499480
#* The use of artificially intelligent agents with bounded rationality in the study of economic markets
#@ Vijay Rajan;James R. Slagle
#t 1996
#c 10
#% 115568
#% 136356
#% 188086
#% 1080811
#% 1268730
#! The concepts of 'knowledge' and 'rationality' are of central importance to fields of science that are interested in human behavior and learning, such as artificial intelligence, economics, and psychology. The similarity between artificial intelligence and economics - both are concerned with intelligent thought, rational behavior, and the use and acquisition of knowledge - has led to the use of economic models as a paradigm for solving problems in distributed artificial intelligence (DAI) and multi agent systems (MAS). What we propose is the opposite; the use of artificial intelligence in the study of economic markets. Over the centuries various theories of market behavior have been advanced. The prevailing theory holds that an asset's current price converges to the risk adjusted value of the rationally expected dividend stream. While this rational expectations model holds in equilibrium or near-equilibrium conditions, it does not sufficiently explain conditions of market disequilibrium. An example of market disequilibrium is the phenomenon of a speculative bubble. We present an example of using artificially intelligent agents with bounded rationality in the study of speculative bubbles.

#index 1499481
#* Total-order multi-agent task-network planning for contract bridge
#@ S. J. J. Smith;D. S. Nau;T. A. Throop
#t 1996
#c 10
#% 171
#% 3358
#% 68273
#% 68274
#% 68275
#% 68408
#% 116297
#% 124587
#% 124601
#% 130237
#% 179955
#% 180122
#% 194648
#% 1272550
#% 1272604
#% 1273216
#% 1274183
#% 1290041
#% 1290100
#% 1291472
#% 1306425
#% 1650653
#! This paper describes the results of applying a modified version of hierarchical task-network (HTN) planning to the problem of declarer play in contract bridge. We represent information about bridge in a task network that is extended to represent multi-agency and uncertainty. Our game-playing procedure uses this task network to generate game trees in which the set of alternative choices is determined not by the set of possible actions, but by the set of available tactical and strategic schemes. This approach avoids the difficulties that traditional game-tree search techniques have with imperfect-information games such as bridge--but it also differs in several significant ways from the planning techniques used in typical HTN planners. We describe why these modifications were needed in order to build a successful planner for bridge. This same modified HTN planning strategy appears to be useful in a variety of application domains--for example, we have used the same planning techniques in a process-planning system for the manufacture of complex electro-mechanical devices (Hebbar et al. 1996). We discuss why the same technique has been successful in two such diverse domains.

#index 1499482
#* Learning other agents' preferences in multiagent negotiation
#@ H. H. Bui;D. Kieronska;S. Venkatesh
#t 1996
#c 10
#% 52273
#% 52301
#% 82810
#% 164502
#% 181537
#% 496723
#% 1013352
#! In multiagent systems, an agent does not usually have complete information about the preferences and decision making processes of other agents. This might prevent the agents from making coordinated choices, purely due to their ignorance of what others want. This paper describes the integration of a learning module into a communication-intensive negotiating agent architecture. The learning module gives the agents the ability to learn about other agents' preferences via past interactions. Over time, the agents can incrementally update their models of other agents' preferences and use them to make better coordinated decisions. Combining both communication and learning, as two complement knowledge acquisition methods, helps to reduce the amount of communication needed on average, and is justified in situations where communication is computationally costly or simply not desirable (e.g. to preserve the individual privacy).

#index 1499483
#* Incorporating opponent models into adversary search
#@ David Carmel;Shaul Markovitch
#t 1996
#c 10
#% 101439
#% 1273317
#% 1279674
#! This work presents a generalized theoretical framework that allows incorporation of opponent models into adversary search. We present the M* algorithm, a generalization of minimax that uses an arbitrary opponent model to simulate the opponent's search. The opponent model is a recursive structure consisting of the opponent's evaluation function and its model of the player. We demonstrate experimentally the potential benefit of using an opponent model. Pruning in M* is impossible in the general case. We prove a sufficient condition for pruning and present the αβ* algorithm which returns the M* value of a tree while searching only necessary branches.

#index 1499484
#* Advantages of a leveled commitment contracting protocol
#@ Tuomas W. Sandholm;Victor R. Lesser
#t 1996
#c 10
#% 174569
#% 677274
#% 1013352
#% 1275313
#% 1275317
#! In automated negotiation systems consisting of self-interested agents, contracts have traditionally been binding. Such contracts do not allow agents to efficiently accommodate future events. Game theory has proposed contingency contracts to solve this problem. Among computational agents, contingency contracts are often impractical due to large numbers of interdependent and unanticipated future events to be conditioned on, and because some events are not mutually observable. This paper proposes a leveled commitment contracting protocol that allows self-interested agents to efficiently accommodate future events by having the possibility of unilaterally decommitting from a contract based on local reasoning. A decommitment penalty is assigned to both agents in a contract: to be freed from the contract, an agent only pays this penalty to the other party. It is shown through formal analysis of several contracting settings that this leveled commitment feature in a contracting protocol increases Pareto efficiency of deals and can make contracts individually rational when no full commitment contract can. This advantage holds even if the agents decommit manipulatively.

#index 1499485
#* A kernel-oriented model for coalition-formation in general environments: implementation and results
#@ Onn Shehory;Sarit Kraus
#t 1996
#c 10
#% 160153
#% 171142
#% 214216
#% 370048
#% 782327
#% 1273605
#% 1273642
#% 1275313
#% 1279673
#! In this paper we present a model for coalition formation and payoff distribution in general environments. We focus on a reduced complexity kernel-oriented coalition formation model, and provide a detailed algorithm for the activity of the single rational agent. The model is partitioned into a social level and a strategic level, to distinguish between regulations that must be agreed upon and are forced by agent-designers, and strategies by which each agent acts at will. In addition, we present an implementation of the model and simulation results. From these we conclude that implementing the model for coalition formation among agents increases the benefits of the agents with reasonable time consumption. It also shows that more coalition formations yield more benefits to the agents.

#index 1499486
#* Coping with temporal constraints in multimedia presentation planning
#@ Elisabeth André;Thomas Rist
#t 1996
#c 10
#% 145421
#% 145628
#% 145638
#% 145641
#% 145646
#% 152047
#% 157706
#% 166352
#% 202046
#% 319244
#% 434717
#% 619781
#% 725467
#! Computer-based presentation systems enable the realization of effective and dynamic presentation styles that incorporate multiple media. Obvious examples are animated user interface agents which verbally comment on multimedia objects displayed on the screen while performing cross-media and cross-window pointing gestures. The design of such presentations must account for the temporal coordination of media output and the agent's behavior. In this paper we describe a new presentation system which not only creates the multimedia objects to be presented, but also generates a script for presenting the material to the user. In our system, this script is forwarded to an animated presentation agent running the presentation. The paper details the kernel of the system which is a component for planning temporally coordinated multimedia.

#index 1499487
#* Declarative camera control for automatic cinematography
#@ David B. Christianson;Sean E. Anderson;Li-wei He;David H. Salesin;Daniel S. Weld;Michael F. Cohen
#t 1996
#c 10
#% 3713
#% 73520
#% 82064
#% 86265
#% 119594
#% 131758
#% 131759
#% 173571
#% 179849
#% 213567
#% 435721
#! Animations generated by interactive 3D computer graphics applications are typically portrayed either from a particular character's point of view or from a small set of strategically-placed viewpoints. By ignoring camera placement, such applications fail to realize important storytelling capabilities that have been explored by cinematographers for many years. In this paper, we describe several of the principles of cinematography and show how they can be formalized into a declarative language, called the Declarative Camera Control Language (OCCL). We describe the application of OCCL within the context of a simple interactive video game and argue that OCCL represents cinematic knowledge at the same level of abstraction as expert directors by encoding 16 idioms from a film textbook. These idioms produce compelling animations, as demonstrated on the accompanying videotape.

#index 1499488
#* A model of poetic comprehension
#@ Kenneth Haase
#t 1996
#c 10
#% 25882
#% 65345
#% 94885
#% 136370
#% 1290075
#! This article introduces an account of aesthetic comprehension and experience together with an implemented miniature which generates analogical interpretations from a semi-automatic parse of Wordsworth's "Lines Written in Early Spring". In our account, a poem serves as an analogy teaching machine by using formal structure to cue the formation of novel analogies. This account builds on an analogical model of comprehension previously applied to large corpora of newspaper summaries. In the miniature, an automatic grammatical and semantic analysis of the text is augmented with information about rhyme and rhythm. These formal cues allow the system to determine analogies which it would not otherwise consider. The article describes the comprehension framework, the annotated piece, and the matcher's performance on the piece. It closes with a discussion of possible objections to aspects of the thesis or experiment and suggested directions for future work.

#index 1499489
#* A framework for plot control in interactive story systems
#@ N. M. Sgouros;G. Papakonstantinou;P. Tsanakas
#t 1996
#c 10
#% 154456
#% 159119
#% 198113
#! This paper presents a framework for plot control in interactive story systems. In this framework, the user takes the place of the main character of the story, the protagonist. The rest of the cast consists of discrete characters, each playing a specific role in the story. A separate module in this system, the plot manager, controls the behavior of the cast and specifies what the protagonist can do. The story plot is dynamically shaped by the interference between cast members and their social interactions. The system accepts as input a story map which provides the main metaphor for organizing the plot and localizes the interaction of the protagonist with the rest of the cast. We are implementing this framework in PEGASUS, an interactive travel story environment for Greek mythology.

#index 1499490
#* Approximate resolution of hard numbering problems
#@ Olivier Bailleux;Jean-Jacques Chabrier
#t 1996
#c 10
#% 92787
#% 92788
#% 121438
#% 136688
#% 288165
#% 288286
#! We present a new method for estimating the number of solutions of constraint satisfaction problems. We use a stochastic forward checking algorithm for drawing a sample of paths from a search tree. With this sample, we compute two values related to the number of solutions of a CSP instance. First, an unbiased estimate, second, a lower bound with an arbitrary low error probability. We will describe applications to the Boolean Satisfiability problem and the Queens problem. We shall give some experimental results for these problems.

#index 1499491
#* Mixed constraint satisfaction: a framework for decision problems under incomplete knowledge
#@ Hélène Fargier;Jérôme Lang;Thomas Schiex
#t 1996
#c 10
#% 121397
#% 160216
#% 1273309
#% 1275298
#% 1275308
#% 1275309
#% 1478528
#% 1650647
#% 1650679
#! Constraint satisfaction is a powerful tool for representing and solving decision problems with complete knowledge about the world. We extend the CSP framework so as to represent decision problems under incomplete knowledge. The basis of the extension consists in a distinction between controllable and uncontrollable variables -- hence the terminology "mixed CSP" -- and a "solution" gives actually a conditional decision. We study the complexity of deciding the consistency of a mixed CSP. As the problem is generally intractable, we propose an algorithm for finding an approximate solution.

#index 1499492
#* Russian doll search for solving constraint optimization problems
#@ Gérard Verfaillie;Michel Lemaître;Thomas Schiex
#t 1996
#c 10
#% 2194
#% 126386
#% 535469
#% 1275308
#% 1275309
#! If the Constraint Satisfaction framework has been extended to deal with Constraint Optimization problems, it appears that optimization is far more complex than satisfaction. One of the causes of the inefficiency of complete tree search methods, like Depth First Branch and Bound, lies in the poor quality of the lower bound on the global valuation of a partial assignment, even when using Forward Checking techniques. In this paper, we introduce the Russian Doll Search algorithm which replaces one search by n successive searches on nested subproblems (n being the number of problem variables), records the results of each search and uses them later, when solving larger subproblems, in order to improve the lower bound on the global valuation of any partial assignment. On small random problems and on large real scheduling problems, this algorithm yields surprisingly good results, which greatly improve as the problems get more constrained and the bandwidth of the used variable ordering diminishes.

#index 1499493
#* Enhancements of branch and bound methods for the maximal constraint satisfaction problem
#@ Richard J. Wallace
#t 1996
#c 10
#% 126386
#% 160245
#% 534311
#% 535469
#% 1273577
#% 1275309
#! Two methods are described for enhancing performance of branch and bound methods for overconstrained CSPs. These methods improve either the upper or lower bound, respectively, during search, so the two can be combined. Upper bounds are improved by using heuristic repair methods before search to find a good solution quickly, whose cost is used as the initial upper bound. The method for improving lower bounds is an extension of directed arc consistency preprocessing, used in conjunction with forward checking. After computing directed arc consistency counts, inferred counts are computed for all values based on minimum counts for values of adjacent variables that are later in the search order. This inference process can be iterated, so that counts are cascaded from the end to the beginning of the search order, to augment the initial counts. Improvements in time and effort are demonstrated for both techniques using random problems.

#index 1499494
#* Path-consistency: when space misses time
#@ Assef Chmeiss;Philippe Jégou
#t 1996
#c 10
#% 1145
#% 3463
#% 42007
#% 160389
#% 319244
#% 542440
#% 1275304
#! Within the framework of constraint programming, particulary concerning the Constraint Satisfaction Problems (CSPs), the techniques of preprocessing based on filtering algorithms were shown to be very important for the search phase. In particular, two filtering methods have been studied, these methods exploit two properties of local consistency: arc-and path-consistency. Concerning the arc-consistency methods, there is a linear time algorithm (in the size of the problem) which is efficient in practice (Bessière, Freuder, & Régin 1995). But the limitations of the arc-consistency algorithms requires often filtering methods with higher order like path-consistency filterings. The best path-consistency algorithm proposed is PC-6, a natural generalization of AC-6 (Bessière 1994) to path-consistency (Chmeiss & Jégou 1995)(Chmeiss 1996). Its time complexity is O(n3 d3) and its space complexity is O(n3 d2), where n is the number of variables and d is the size of domains. We have remarked that PC-6, though it is widely better than PC-4 (Han & Lee 1988), was not very efficient in practice, specialy for those classes of problems that require an important. space to be run. Therefore, we propose here a new path-consistency algorithm called PC-7, its space complexity is O(n2 d2) but its time complexity is O(n3 d4) i.e. worse than that of PC-6. However, the simplicity of PC-7 as well as the data structures used for its implementation offer really a higher performance than PC-6. Furthermore, it turns out that when the size of domains is a constant of the problems, the time complexity of PC-7 becomes, like PC-6, optimal i.e. O(n3).

#index 1499495
#* Neighborhood inverse consistency preprocessing
#@ Eugene C. Freuder;Charles D. Elfe
#t 1996
#c 10
#% 2028
#% 36814
#% 172500
#% 320265
#% 1273573
#% 1279714
#! Constraint satisfaction consistency preprocessing methods are used to reduce search effort. Time and especially space costs limit the amount of preprocessing that will be cost effective. A new form of consistency preprocessing, neighborhood inverse consistency, can achieve more problem pruning than the usual arc consistency preprocessing in a cost effective manner. There are two basic ideas: 1) Common forms of consistency enforcement basically operate by identifying and remembering solutions to subproblems for which a consistent value cannot be found for some additional problem variable. The space required for this memory can quickly become prohibitive. Inverse consistency basically operates by removing values for variables that are not consistent with any solution to some subproblem involving additional variables. The space requirement is at worst linear. 2) Typically consistency preprocessing achieves some level of consistency uniformly throughout the problem. A subproblem solution will be tested against each additional variable that constrains any subproblem variable. Neighborhood consistency focuses attention on the subproblem formed by the variables that are all constrained by the value in question. By targeting highly relevant subproblems we hope to "skim the cream", obtaining a high payoff for a limited cost.

#index 1499496
#* Generalized arc consistency for global cardinality constraint
#@ Jean-Charles Régin
#t 1996
#c 10
#% 1451
#% 3463
#% 42582
#% 131561
#% 160208
#% 160389
#% 1275304
#! A global cardinality constraint (gcc) is specified in terms of a set of variables X = {x1,..., xp} which take their values in a subset of V = {v1,...,vd}. It constrains the number of times a value vi ∈ V is assigned to a variable in X to be in an interval [li, ci. Cardinality constraints have proved very useful in many real-life problems, such as scheduling, timetabling, or resource allocation. A gcc is more general than a constraint of difference, which requires each interval to be [0,1]. In this paper, we present an efficient way of implementing generalized arc consistency for a gcc. The algorithm we propose is based on a new theorem of flow theory. Its space complexity is O(|X| × |V|) and its time complexity is O(|X|2 × |V|). We also show how this algorithm can efficiently be combined with other filtering techniques.

#index 1499497
#* Lazy arc consistency
#@ Thomas Schiex;Jean-Charles Régin;Christine Gaspin;Gérard Verfaillie
#t 1996
#c 10
#% 3463
#% 75817
#% 131561
#% 160389
#% 1273406
#% 1275304
#! Arc consistency filtering is widely used in the framework of binary constraint satisfaction problems: with a low complexity inconsistency may be detected and domains are filtered. In this paper, we show that when detecting inconsistency is the objective, a systematic domain filtering is useless and a lazy approach is more adequate. Whereas usual arc consistency algorithms produce the maximum arc consistent subdomain, when it exists, we propose a method, called LAC7, which only looks for any arc consistent sub-domain. The algorithm is then extended to provide the additional service of locating one variable with a minimum domain cardinality in the maximum arc consistent sub-domain, without necessarily computing all domain sizes. Finally, we compare traditional AC enforcing and lazy AC enforcing using several benchmark problems, both randomly generated CSP and real life problems.

#index 1499498
#* Searching game trees under memory constraints
#@ Subir Bhattacharya;Amitava Hagchi
#t 1996
#c 10
#% 241
#% 20851
#% 155837
#% 177876
#% 181335
#% 1275258
#! The best-first game-tree search algorithm SSS* has greater pruning power than the depth-first algorithm Alpha-Beta. Yet it is seldom used in practice because it is slow in execution and requires substantial memory. Variants of SSS* have been proposed in recent years that overcome some, but not all, of its limitations. The recursive controlled-memory best-first search scheme MemSSS* described here is a new derivative of SSS* that compares favourably with Alpha-Beta in respect of all three major performance measures, namely, pruning power, running time and memory needs. MemSSS* improves upon an earlier controlled-memory algorithm IterSSS* which has most of the desired properties but is slow in execution.

#index 1499499
#* Partition search
#@ Matthew L. Ginsberg
#t 1996
#c 10
#% 319400
#! We introduce a new form of game search called partition search that incorporates dependency analysis, allowing substantial reductions in the portion of the tree that needs to be expanded. Both theoretical results and experimental data are presented. For the game of bridge, partition search provides approximately as much of an improvement over existing methods as α-(β pruning provides over minimax.

#index 1499500
#* Exploiting graph properties of game trees
#@ Aske Plaat;Jonathan Schaeffer;Wim Pijls;Arie de Bruin
#t 1996
#c 10
#% 241
#% 2194
#% 12293
#% 20851
#% 25470
#% 39261
#% 60140
#% 68238
#% 109356
#% 116297
#% 159245
#% 183499
#% 216974
#% 443807
#% 689610
#% 1273214
#% 1275257
#% 1275262
#! The state space of most adversary games is a directed graph. However, due to the success of simple recursive algorithms based on Alpha-Beta, theoreticians and practitioners have concentrated on the traversal of trees, giving the field the name "game-tree search." This paper shows that the focus on trees has obscured some important properties of the underlying graphs. One of the hallmarks of the field of game-tree search has been the notion of the minimal tree, the smallest tree that has to be searched by any algorithm to find the minimax value. In fact, for most games it is a directed graph. As demonstrated in chess and checkers, we show that the minimal graph is significantly smaller than previously thought. proving that there is more room for improvement of current algorithms. We exploit the graph properties of the search space to reduce the size of trees built in practice by at least 25%. For over a decade, fixed-depth Alpha-Beta searching has been considered a closed subject, with research moving on to more application-dependent techniques. This work opens up new avenues of research for further application-independent improvements.

#index 1499501
#* Forward estimation for game-tree search
#@ Weixiong Zhang
#t 1996
#c 10
#% 2194
#% 25887
#% 36816
#% 39261
#% 68274
#% 68279
#% 109356
#% 180122
#% 197425
#% 211586
#% 496304
#% 1273318
#% 1279690
#% 1291451
#! It is known that bounds on the mmnnax values of nodes in a game tree can be used to reduce the computational complexity of minimax search for two-player games. We describe a very simple method to estimate bounds on the minimax values of interior nodes of a game tree, and use the bounds to improve minimax search. The new algorithm, called forward estimation, does not require additional domain knowledge other than a static node evaluation function, and has small constant overhead per node expansion. We also propose a variation of forward estimation, which provides a tradeoff between computational complexity and decision quality. Our experimental results show that forward estimation outperforms alpha-beta pruning on random game trees and the game of Othello.

#index 1499502
#* The constrainedness of search
#@ Ian P. Gent;Ewan MacIntyre;Patrick Prosser;Toby Walsh
#t 1996
#c 10
#% 167643
#% 172500
#% 175367
#% 188076
#% 288896
#% 317283
#% 319789
#% 534165
#% 837639
#% 1275261
#% 1279714
#! We introduce a parameter that measures the "constrainedness" of an ensemble of combinatorial problems. If problems are over-constrained, they are likely to be insoluble. If problems are under-constrained, they are likely to be soluble. This constrainedness parameter generalizes a number of parameters previously used in different NP-complete problem classes. Phase transitions in different NP classes can thus be directly compared. This parameter can also be used in a heuristic to guide search. The heuristic captures the intuition of making the most constrained choice first, since it is often useful to branch into the least constrained subproblem. Many widely disparate heuristics can be seen as minimizing constrainedness.

#index 1499503
#* Exploiting a theory of phase transitions in three-satisfiability problems
#@ David M. Pennock;Quentin F. Stout
#t 1996
#c 10
#% 24546
#% 160272
#% 175367
#% 179914
#% 1275263
#% 1279714
#! In the past few years there have been several empirical discoveries of phase transitions in constraint satisfaction problems (CSPs), and a growth of interest in the area among the artificial intelligence community. This paper extends a simple analytical theory of phase transitions in three-satisfiability (3-SAT) problems in two directions. First, a more accurate, problem-dependent calculation leads to a new polynomial time probabilistic estimate of the satisfiability of 3-SAT problems called PE-SAT (Probabilistic Estimate SATisfiability algorithm). PE-SAT empirically classifies 3-SAT problems with about 70% accuracy at the hardest region (the so-called crossover point or 50% satisfiable region) of random 3-SAT space. Furthermore, the estimate has a meaningful magnitude such that extreme estimates are much more likely to be correct. Second, the same estimate is used to improve the running time of a backtracking search for a solution to 3-SAT by pruning unlikely branches of the search. The speed-up is achieved at the expense of accuracy--the search remains sound but is no longer complete. The trade-off between speed-up and accuracy is shown to improve as the size of problems increases.

#index 1499504
#* A second order parameter for 3SAT
#@ Tuornas W. Sandholrn
#t 1996
#c 10
#% 15245
#% 179914
#% 288165
#% 1273577
#% 1275263
#% 1275266
#% 1279714
#! The 3-satisfiability problem (3SAT) has had a central role in the study of complexity. It was recently found that 3SAT instances transition sharply from satisfiable to nonsatisfiable as the ratio of clauses to variables increases. Because this phase transition is so sharp, the ratio - an order parameter - can be used to predict satisfiability. This paper describes a second order parameter for 3SAT. Like the classical order parameter, it can be computed in linear time, but it analyzes the structure of the problem instance more deeply. We present an analytical method for using this new order parameter in conjunction with the classical one to enhance satisfiability prediction accuracy. The assumptions of the method are verified by rigorous statistical testing. The method significantly increases the satisfiability prediction accuracy over using the classical order parameter alone. Hardness - i.e. how long it takes to determine satisfiability - results for one complete and one incomplete algorithm from the literature are also presented as a function of the two order parameters. The importance of new order parameters lies in the fact that they refine the locating of satisfiable vs. nonsatisfiable and hard vs. easy formulas in the space of all problem instances by adding a new dimension in the analysis.

#index 1499505
#* The very particular structure of the very hard instances
#@ Dan R. Vlasie
#t 1996
#c 10
#% 10129
#% 42011
#% 101924
#% 121464
#% 121507
#% 126390
#% 144520
#% 167643
#% 175367
#% 319789
#% 363652
#% 408396
#% 535327
#% 618429
#% 1279714
#! We show that the algorithms which behave well on average may have difficulty only for highly structured, non-random inputs, except in a finite number of cases. The formal framework is provided by the theory of Kolmogorov complexity. An experimental verification is done for graph 3-colorability with Brelaz's algorithm.

#index 1499506
#* Heuristic-biased stochastic sampling
#@ John L. Bresina
#t 1996
#c 10
#% 124593
#% 160270
#% 198950
#% 1272364
#% 1273553
#% 1273577
#% 1290102
#! This paper presents a search technique for scheduling problems, called Heuristic-Biased Stochastic Sampling (HBSS). The underlying assumption behind the HBSS approach is that strictly adhering to a search heuristic often does not yield the best solution and, therefore, exploration off the heuristic path can prove fruitful. Within the HBSS approach, the balance between heuristic adherence and exploration can be controlled according to the confidence one has in the heuristic. By varying this balance, encoded as a bias function, the HBSS approach encompasses a family of search algorithms of which greedy search and completely random search are extreme members. We present empirical results from an application of HBSS to the real-world problem of observation scheduling. These results show that with the proper bias function, it can be easy to outperform greedy search.

#index 1499507
#* Easy and hard testbeds for real-time search algorithms
#@ Sven Koenig;Reid G. Simmons
#t 1996
#c 10
#% 241
#% 68110
#% 68238
#% 98073
#% 124600
#% 137995
#% 203608
#% 214306
#% 647234
#% 1013437
#% 1081036
#% 1273597
#% 1279695
#% 1290111
#% 1290112
#% 1291498
#! Although researchers have studied which factors influence the behavior of traditional search algorithms, currently not much is known about how domain properties influence the performance of real-time search algorithms. In this paper we demonstrate, both theoretically and experimentally, that Eulerian state spaces (a superset of undirected state spaces) are very easy for some existing real-time search algorithms to solve: even real-time search algorithms that can be intractable, in general, are efficient for Eulerian state spaces. Because traditional real-time search testbeds (such as the eight puzzle and gridworlds) are Eulerian, they cannot be used to distinguish between efficient and inefficient real-time search algorithms. It follows that one has to use non-Eulerian domains to demonstrate the general superiority of a given algorithm. To this end, we present two classes of hard-to-search state spaces and demonstrate the performance of various real-time search algorithms on them.

#index 1499508
#* Improved limited discrepancy search
#@ Richard E. Korf
#t 1996
#c 10
#% 2194
#% 137995
#% 180109
#% 674446
#% 1273553
#% 1275261
#% 1275306
#! We present an improvement to Harvey and Ginsberg's limited discrepancy search algorithm, which eliminates much of the redundancy in the original, by generating each path from the root to the maximum search depth only once. For a complete binary tree of depth d, this reduces the asymptotic complexity from O(d+2/2 2d) to O(2d). The savings is much less in a partial tree search, or in a heavily pruned tree. The overhead of the improved algorithm on a complete b-ary tree is only a factor of b/(b - 1) compared to depth-first search. While this constant factor is greater on a heavily pruned tree, this improvement makes limited discrepancy search a viable alternative to depth-first search, whenever the entire tree may not be searched. Finally, we present both positive and negative empirical results on the utility oflimited discrepancy search, for the problem of number partitioning.

#index 1499509
#* Efficient goal-directed exploration
#@ Yury Smirnov;Sven Koenig;Manuela M. Veloso;Reid G. Simmons
#t 1996
#c 10
#% 68110
#% 68238
#% 90041
#% 91339
#% 109604
#% 124600
#% 154052
#% 160859
#% 184062
#% 214306
#% 647111
#% 1081036
#% 1290111
#! If a state space is not completely known in advance, then search algorithms have to explore it sufficiently to locate a goal state and a path leading to it, performing therefore what we call goal-directed exploration. Two paradigms of this process are pure exploration and heuristic-driven exploitation: the former approaches explore the state space using only knowledge of the physically visited portion of the domain, whereas the latter approaches totally rely on heuristic knowledge to guide the search towards goal states. Both approaches have disadvantages: the first one does not utilize available knowledge to cut down the search effort, and the second one relies too much on the knowledge, even if it is misleading. We have therefore developed a framework for goal-directed exploration, called VECA, that combines the advantages of both approaches by automatically switching from exploitation to exploration on parts of the state space where exploitation does not perform well. VECA provides better performance guarantees than previously studied heuristic-driven exploitation algorithms, and experimental evidence suggests that this guarantee does not deteriorate its average-case performance.

#index 1499510
#* A complexity analysis of space-bounded learning algorithms for the constraint satisfaction problem
#@ Roberto J. Bayardo, Jr.;Daniel P. Miranker
#t 1996
#c 10
#% 1675
#% 2028
#% 36814
#% 52793
#% 58376
#% 68183
#% 160253
#% 175394
#% 408396
#% 1268731
#% 1273223
#% 1275299
#! Learning during backtrack search is a space-intensive process that records information (such as additional constraints) in order to avoid redundant work. In this paper, we analyze the effects of polynomial-space-bounded learning on runtime complexity of backtrack search. One space-bounded learning scheme records only those constramts With limited size, and another records arbitrarily large constraints but deletes those that become irrelevant to the portion of the search space being explored. We find that relevance-bounded learning allows better runtime bounds than size-bounded learning on structurally restricted constraint satisfaction problems. Even when restricted to linear space, our relevance-bounded learning algorithm has runtime complexity near that of unrestricted (exponential space-consuming) learning schemes.

#index 1499511
#* Improving the learning efficiencies of realtime search
#@ Toru Ishida;Masashi Shimbo
#t 1996
#c 10
#% 241
#% 68238
#% 180110
#% 209624
#% 443874
#% 1291498
#! The capability of learning is one of the salient features of realtime search algorithms such as LRTA*. The major impediment is, however, the instability of the solution quality during convergence: (1) they try to find all optimal solutions even after obtaining fairly good solutions, and (2) they tend to move towards unexplored areas thus failing to balance exploration and exploitation. We propose and analyze two new realtime search algorithms to stabilize the convergence process. Ɛ-search (weighted realtime search) allows suboptimal solutions with Ɛ error to reduce the total amount of learning performed. δ-search (realtime search with upper bounds) utilizes the upper bounds of estimated costs, which become available after the problem is solved once. Guided by the upper bounds, δ-search can better control the tradeoff between exploration and exploitation.

#index 1499512
#* Dynamic improvements of heuristic evaluations during search
#@ Gerhard Kainz;Hermann Kaindl
#t 1996
#c 10
#% 241
#% 2194
#% 64788
#% 68238
#% 130194
#% 160388
#% 180112
#% 189701
#% 289063
#% 289396
#% 443807
#% 1273312
#% 1275257
#! Heuristic search algorithms employ evaluation functions that utilize heuristic knowledge of the given domain. We call such functions static evaluation functions when they only make use of knowledge applied to the given state but not resuIts of any search in this state space - neither the search guided by the evaluation nor extra search like look-ahead. Static evaluation functions typically evaluate with some error. An approach to improve on the accuracy of a given static evaluation function is to utilize results of a search. Since this involves dynamic changes, we call resulting functions dynamic evaluation functions. We devised a new approach to dynamic improvements that we named difference approach. It utilizes differences of known costs and their heuristic estimates from a given evaluation function to improve other heuristic estimates from this function. This difference approach can be applied in a wide variety of known search algorithms. We show how it fits into a unifying view of dynamic improvements, that also covers already existing approaches as viewed from this perspective. Finally, we report experimental data for two different domains that represent significant improvements over previously published results.

#index 1499513
#* Inference-based constraint satisfaction supports explanation
#@ Mohammed H. Sqalli;Eugene C. Freuder
#t 1996
#c 10
#% 65348
#% 126385
#% 130210
#% 160208
#% 168349
#% 168350
#% 320265
#% 534182
#% 1273311
#% 1275300
#% 1275305
#! Constraint satisfaction problems are typically solved using search, augmented by general purpose consistency inference methods. This paper proposes a paradigm shift in which inference is used as the primary problem solving method, and attention is focused on special purpose, domain specific inference methods. While we expect this approach to have computational advantages, we emphasize here the advantages of a solution method that is more congenial to human thought processes. Specifically we use inference-based constraint satisfaction to support explanations of the problem solving behavior that are considerably more meaningful than a trace of a search process would be. Logic puzzles are used as a case study. Inference-based constraint satisfaction proves surprisingly powerful and easily extensible in this domain. Problems drawn from commercial logic puzzle booklets are used for evaluation. Explanations are produced that compare well with the explanations provided by these booklets.

#index 1499514
#* Constraint satisfaction using a hybrid evolutionary hill-climbing algorithm that performs opportunistic arc and path revision
#@ James Bowen;Gerry Dozier
#t 1996
#c 10
#% 115881
#% 126390
#% 130206
#% 369236
#% 466688
#% 1279714
#! This paper introduces a hybrid evolutionary hill-climbing algorithm that quickly solves Constraint. Satisfaction Problems (CSPs). This hyhrid uses opportunistic arc and path revision in an interleaved fashion to reduce the size of the search space and to realize when to quit if a CSP is based on an inconsistent constraint network. This hybrid outperforms a well known hill-climbing algorithm, the Iterative Descent Method on a test suite of 750 randomly generated CSPs.

#index 1499515
#* Adding new clauses for faster local search
#@ Byungki Cha;Kazuo Iwama
#t 1996
#c 10
#% 116559
#% 1275266
#! A primary concern when using local search methods for CNF satisfiability is how to get rid of local minimas. Among many other heuristics, Weighting by Morris (1993) and Selman and Kautz (1993) works overwhelmingly better than others (Cha and Iwama 1995). Weighting increases the weight of each clause which is unsatisfied at a local minima. This paper introduces a more sophisticated weighting strategy, i.e., adding new clauses (ANC) that are unsatisfied at the local minima. As those new clauses, we choose resolvents of the clauses unsatisfied at the local minima and randomly selected neighboring clauses. The idea is that ANC is to make the slope of search space more smooth than the simple weighting. Experimental data show that ANC is faster than simple weighting: (i) When the number of variables is 200 or more, ANC is roughly four to ten times as fast as weighting in terms of the number of search steps. (ii) It might be more important that the divergence of computation time for each try is much smaller in ANC than in weighting. (iii) There are several possible reasons for ANC's superiority, one of which is that ANC returns the same local minima much less frequently than weighting.

#index 1499516
#* Weighting for godot: learning heuristics for GSAT
#@ Jeremy Frank
#t 1996
#c 10
#% 160239
#% 1273577
#% 1275266
#! We investigate an improvement to GSAT which associates a weight with each clause. GSAT moves to assignments maximizing the weight of satisfied clauses and this weight is incremented when GSAT moves to an assignment in which this clause is unsatisfied. We present results showing that this algorithm and its variants outperform one of the best known modifications of GSAT to date using two metrics: number of solved problems on a single try, and minimum mean number of flips to solve a test suite of problems.

#index 1499517
#* Duplication of coding segments in genetic programming
#@ Thomas Haynes
#t 1996
#c 10
#% 124073
#% 167410
#% 167411
#% 243358
#% 243380
#% 369236
#% 406731
#% 466357
#% 466370
#% 703199
#% 1022820
#% 1022823
#! Research into the utility of non-eoding segments, or introns, in genetic-based encodings has shown that they expedite the evolution of solutions in domains by protecting building blocks against destructive crossover. We consider a genetic programming system where non-coding segments can be removed, and the resultant chromosomes returned into the population. This parsimonious repair leads to premature convergence, since as we remove the naturally occurring non-coding segments, we strip away their protective backup feature. We then duplicate the coding segments in the repaired chromosomes, and place the modified chromosomes into the population. The duplication method significantly improves the learning rate in the domain we have considered. We also show that this method can be applied to other domains.

#index 1499518
#* A graph-based method for improving GSAT
#@ Kalev Kask;Rina Dechter
#t 1996
#c 10
#% 1145
#% 68183
#% 160270
#% 408680
#% 1272171
#% 1275307
#% 1279712
#! GSAT is a randomized greedy local repair procedure that was introduced for solving propositional satisfiability and constraint satisfaction problems. We present an improvement to GSAT that is sensitive to the problem's structure. When the problem has a tree structure the algorithm is guaranteed to find a solution in linear time. For non-tree networks, the algorithm designates a subset of nodes, called cutset, and executes a regular GSAT algorithm on this set of variables. On all the rest of the variables it executes a specialized local search algorithm for trees. This algorithm finds an assignment that, like GSAT, locally minimizes the sum of unsatisfied constraints and also globally minimizes the number of conflicts in every tree-like subnetwork. We will present results of experiments showing that this new algorithm outperforms regular GSAT on sparse networks whose cycle-cutset size is bounded by 30% of the nodes.

#index 1499519
#* Tuning local search for satisfiability testing
#@ Andrew J. Parkes;Joachim P. Walser
#t 1996
#c 10
#% 116559
#% 121507
#% 126390
#% 160270
#% 162682
#% 210195
#% 1279714
#! Local search algorithms, particularly GSAT and WSAT have attracted considerable recent attention primarily because they are the best known approaches to several hard classes of satisfiability problems. However, replicating reported results has been difficult because the setting of certain key parameters is something of an art, and because details of the algorithms, not discussed in the published papers, can have a large Impact on performance. In this paper we present an efficient probabilistic method for finding the optimal setting for a critical local search parameter, Maxflips, and discuss important details of two differing versions of WSAT. We then apply the optimization method to study performance of WSAT on satisfiable instances of Random 3SAT at the crossover point and present extensive experimental results over a wide range of problem sizes. We find that the results are well described by having the optimal value of Maxflips scale as a simple power of the number of variables, n, and the average run time scale sub-exponentially (basically as nlog(n)) over the range n = 25,...,400.

#index 1499520
#* Tabu search techniques for large high-school timetabling problems
#@ Andrea Schaerf
#t 1996
#c 10
#% 95668
#% 119088
#% 143592
#% 176413
#% 321526
#% 445042
#% 651260
#% 651483
#! The high-school timetabling problem consists in assigning all the lectures of a high school to the time periods in such a way that no teacher (or class) is involved in more than one lecture at a time and other side constraints are satisfied. The problem is NP-complete and is usually tackled using heuristic methods. This paper describes a solution algorithm (and its implementation) based on Tabu Search. The algorithm interleaves different types of moves and makes use of an adaptive relaxation of the hard constraints. The implementation of the algorithm has been successfully experimented in some large high schools with various kinds of side constraints.

#index 1499521
#* Combining local search and backtracking techniques for constraint satisfaction
#@ Jian Zhang;Hantao Zhang
#t 1996
#c 10
#% 108707
#% 125386
#% 126390
#% 159432
#% 288165
#% 1273544
#% 1273577
#% 1275265
#! Backtracking techniques are well-known traditional methods for solving many constraint satisfaction problems (CSPs), including the satisfiability (SAT) problem in the propositional logic. In recent years, it has been reported that local search techniques are very effective in solving some large-scale instances of the SAT problem. In this research, we combine the backtracking and local search techniques into a single method for solving SAT and CSPs. When setting a parameter of the method to either of its two extreme values, we obtain the ordinary backtracking procedure or the local search procedure. For some problems, if the parameter takes values in the middle of the two extremes, the new method is much more effective than either backtracking or local search. We tested the method with classical problems like the n-Queens and random SAT instances, as well as some difficult problems from finite mathematics. In particular, using the new method, we solved four open problems in design theory.

#index 1499522
#* A simple way to improve path consistency processing in interval algebra networks
#@ Christian Bessière
#t 1996
#c 10
#% 3463
#% 75817
#% 107137
#% 121993
#% 126395
#% 131372
#% 160389
#% 319244
#% 557960
#% 618459
#% 1273474
#% 1273627
#% 1275304
#% 1275311
#! Reasoning about qualitative temporal information is essential in many artificial intelligence problems. In particular, many tasks can be solved using the interval-based temporal algebra introduced by Allen (All83). In this framework, one of the main tasks is to compute the transitive closure of a network of relations between intervals (also called path consistency in a CSP-like terminology). Almost all previous path consistency algorithms proposed in the temporal reasoning literature were based on the constraint reasoning algorithms PC-1 and PC-2 (Mac77). In this paper, we first show that the most efficient of these algorithms is the one which stays the closest to PC-2. Afterwards, we propose a new algorithm, using the idea "one support is sufficient" (as AC-3 (Mac77) does for arc consistency in constraint networks). Actually, to apply this idea, we simply changed the way composition-intersection of relations was achieved during the path consistency process in previous algorithms.

#index 1499523
#* A representation for efficient temporal reasoning
#@ James P. Delgrande;Arvind Gupta
#t 1996
#c 10
#% 75812
#% 82720
#% 84513
#% 126395
#% 152555
#% 184792
#% 319244
#% 836124
#% 1273474
#! It has been observed that the temporal reasoning component in a knowledge-based system is frequently a bottleneck. We investigate here a class of graphs appropriate for an interesting class of temporal domains and for which very efficient algorithms for reasoning are obtained, that of series-parallel graphs. These graphs can be used for example to model process execution, as well as various planning or scheduling activities. Events are represented by nodes of a graph and relationships are represented by edges labeled by ≤ or O(n) time pre-processing algorithm that allows us to answer queries about the events in O(1) time. Our results make use of a novel embedding of the graphs on the plane that is of independent interest. Finally we argue that these results may be incorporated in general graphs representing temporal events by extending the approach of Gerevini and Schubert.

#index 1499524
#* Maximal tractable subclasses of Allen's interval algebra: preliminary report
#@ Thomas Drakengren;Peter Jonsson
#t 1996
#c 10
#% 82720
#% 116335
#% 126395
#% 137041
#% 152555
#% 181229
#% 183459
#% 319244
#% 1273473
#! This paper continues Nebel and Bürckert's investigation of Allen's interval algebra by presenting nine more maximal tractable subclasses of the algebra (provided that P ≠ N P), in addition to their previously reported ORD-Horn subclass. Furthermore, twelve tractable subclasses are identified, whose maximality is not decided. Four of these can express the notion of sequentiality between intervals, which is not possible in the ORD-Horn algebra. The satisfiability algorithm, which is common for all the algebras, is shown to be linear.

#index 1499525
#* A new proof of tractability for 0RD-horn relations
#@ Gérard Ligozat
#t 1996
#c 10
#% 160209
#% 191259
#! This paper gives an elementary proof of the tractability of a sub-class of temporal relations in Allen's algebra and related temporal calculi, the class of preconvex relations. In Allen's case, this subclass coincides with the class of ORD-Horn relations. Nebel and Bürckert defined ORD-Horn relations and proved that path-consistency is a sufficient condition for consistency of a network for this sub-class. We prove a stronger result: for each path-consistent network in the sub-class, we give an effective method for constructing a feasible scenario without backtrack.

#index 1499526
#* A novel application of theory refinement to student modeling
#@ Paul T. Baffes;Raymond J. Mooney
#t 1996
#c 10
#% 73373
#% 156830
#% 161241
#% 186063
#% 199274
#% 449508
#% 1275480
#! Theory refinement systems developed in machine learning automatically modify a knowledge base to render it consistent with a set of classified training examples. We illustrate a novel application of these techniques to the problem of constructing a student model for an intelligent tutoring system (ITS). Our approach is implemented in an ITS authoring system called ASSERT which uses theory refinement to introduce errors into an initially correct knowledge base so that it models incorrect student behavior. The efficacy of the approach has been demonstrated by evaluating a tutor developed with ASSERT with 75 students tested on a classification task covering concepts from an introductory course on the C++ programmm. g Ia nguage. The system produced reasonably accurate models and students who received feedback based on these models performed significantly better on a post test than students who received simple reteaching.

#index 1499527
#* A simulation-based tutor that reasons about multiple agents
#@ Christopher Rhodes Eliot, III;Beverly Park Woolf
#t 1996
#c 10
#% 6200
#% 109938
#% 160139
#% 160218
#% 552958
#% 669549
#% 669901
#% 1273210
#! This paper examines the problem of modeling multiple agents within an intelligent simulation-based tutor. Multiple agent and planning technology were used to enable the system to critique a human agent's reasoning about multiple agents. This perspective arises naturally whenever a student must learn to lead and coordinate a team of people. The system dynamically selected teaching goals, instantiated plans and modeled the student and the domain as it monitored the student's progress. The tutor provides one of the first complete integrations of a real-time simulation with knowledge-based reasoning. Other novel techniques of the system are reported, such as common-sense reasoning about plans, reasoning about protocol mechanisms, and using a real-time simulation for training.

#index 1499528
#* Scaling up explanation generation: large-scale knowledge bases and empirical studies
#@ James C. Lester;Bruce W. Porter
#t 1996
#c 10
#% 1891
#% 51055
#% 64787
#% 70387
#% 135335
#% 145399
#% 149389
#% 177915
#% 199036
#% 200854
#% 679521
#% 701452
#! To explain complex phenomena, an explanation system must be able to select information from a formal representation of domain knowledge, organize the selected information into multisentential discourse plans, and realize the discourse plans in text. Although recellit years have witnessed significant progress in the development of sophisticated computational mechanisms for explanation, empirical results have been limited. This paper reports on a seven year effort to empirically study explanation generation from semantically rich, large-scale knowledge bases. We first describe Knight, a robust explanation system that constructs multi-sentential and multi-paragraph explanations from the Biology Knowledge Base, a large-scale knowledge base in the domain of botanical anatomy, physiology, and development. We then introduce the Two Panel evaluation methodology and describe how Knight's performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system. In this evaluation, Knight scored within "half a grade" of domain experts, and its performance exceeded that of one of the domain experts.

#index 1499529
#* Dynamically sequencing an animated pedagogical agent
#@ Brian A. Stone;James C. Lester
#t 1996
#c 10
#% 21734
#% 31687
#% 32599
#% 114522
#% 145639
#% 147050
#% 159119
#% 173183
#% 196930
#% 552929
#% 1290062
#! One of the most promising opportunities introduced by rapid advances in knowledge-based learning environments and multimedia technologies is the possibility of creating animated pedagogical agents. These agents should exhibit three properties: timely domain coverage (they should clearly communicate fundamental concepts and relationships within the allotted time); contextuality (they should provide explanations in appropriate problem-solving contexts); and continuity (their activities and utterances should be pedagogically, visually, and aurally coherent). We have developed the coherence-structured behavior space approach to creating animated pedagogical agents. This is a two-step approach. First, we design a behavior space of animation and audio segments that are structured by prerequisite relationships and a continuity metric. Second, we navigate coherent paths through the space to dynamically sequence behaviors. This creates seamless global behaviors that communicate fundamental knowledge and provide contextualized problem-solving advice. The coherence-structured behavior space approach has been implemented in Herman the Bug, an animated pedagogical agent for Design-A-Plant, a knowledge-based learning environment for botanical anatomy and physiology. Formative evaluations of the agent with middle school students are encouraging.

#index 1499530
#* Machine learning of user profiles: representational issues
#@ Eric Bloedorn;Inderjeet Mani;T. Richard MacMillan
#t 1996
#c 10
#% 65946
#% 118726
#% 124004
#% 124009
#% 136350
#% 144007
#% 159108
#% 166352
#% 169806
#% 187756
#% 194283
#% 239316
#% 406493
#% 618427
#% 817971
#! As more information becomes available electronically, tools for finding information of interest to users becomes increasingly important. The goal of the research described here is to build a system for generating comprehensible user profiles that accurately capture user interest with minimum user interaction. The research described here focuses on the importance of a suitable generalization hierarchy and representation for learning profiles which are predictively accurate and comprehensible. In our experiments we evaluated both traditional features based on weighted term vectors as well as subject features corresponding to categories which could be drawn from a thesaurus. Our experiments, conducted in the context of a content-based profiling system for on-line newspapers on the World Wide Web (the IDD News Browser), demonstrate the importance of a generalization hierarchy and the promise of combining natural language processing techniques with machine learning (ML) to address an information retrieval (IR) problem.

#index 1499531
#* Interactive information retrieval systems with minimalist representation
#@ Eric Domeshek;Smadar Kedar;Andrew Gordon
#t 1996
#c 10
#% 55921
#% 55936
#% 151690
#% 156337
#% 168280
#% 174020
#% 1290163
#! Almost any information you might want is becoming available on-line. The problem is how to find what you need. One strategy to improve access to existing information sources, is intelligent information agents - an approach based on extensive representation and inference. Another alternative is to simply concentrate on better information organization and indexing. Our systems use a form of conceptual indexing sensitive to users' task-specific information needs. We aim for minimalist representation, coding only select aspects of stored items. Rather than supporting reliable automated inference, the primary purpose of our representations is to provide sufficient discrimination and guidance to a user for a given domain and task. This paper argues, using case studies, that minimal representations can make strong contributions to the usefulness and usability of interactive information systems, while minimizing knowledge engineering effort. We demonstrate this approach in several broad spectrum applications including video retrieval and advisory systems.

#index 1499532
#* Learning word meanings by instruction
#@ Kevin Knight
#t 1996
#c 10
#% 31918
#% 117568
#% 288366
#% 300045
#% 691507
#% 748464
#% 748465
#% 748491
#% 748494
#% 748579
#% 748975
#% 817918
#% 1273968
#% 1274522
#% 1290077
#! We develop techniques for learning the meanings of unknown words in context. Working within a compositional semantics framework, we write down equations in which a sentence's meaning is some combination function of the meaning of its words. When one of the words is unknown, we ask for a paraphrase of the sentence. We then compute the meaning of the unknown word by inverting parts of the semantic combination function. This technique can be used to learn word-concept mappings, decomposed meanings, and mappings between syntactic and semantic roles. It works for all parts of speech.

#index 1499533
#* Significant lexical relationships
#@ Ted Pedersen;Mehmet Kayaalp;Rebecca Bruce
#t 1996
#c 10
#% 8202
#% 740900
#% 740916
#! Statistical NLP inevitably deals with a large number of rare events. As a consequence, NLP data often violates the assumptions implicit in traditional statistical procedures such as significance testing. We describe a significance test, an exact conditional test, that is appropriate for NLP data and can be performed using freely available software. We apply this test to the study of lexical relationships and demonstrate that the results obtained using this test are both theoretically more reliable and different from the results obtained using previously applied tests.

#index 1499534
#* Knowledge-based navigation of complex information spaces
#@ Robin D. Burke;Kristian J. Hammond;Benjamin C. Young
#t 1996
#c 10
#% 36683
#% 55921
#% 118771
#% 168280
#% 199794
#% 405727
#% 406493
#% 407980
#% 441058
#! While the explosion of on-line information has brought new opportunities for finding and using electronic data, it has also brought to the forefront the problem of isolating useful information and making sense of large multidimension information spaces. We have built several developed an approach to building data "tour guides," called FINDME systems. These programs know enough about an information space to be able to help a user navigate through it. The user not only comes away with items of useful information but also insights into the structure of the information space itself. In these systems, we have combined ideas of instance-based browsing, structuring retrieval around the critiquing of previously-retrieved examples, and retrieval strategies, knowledge-based heuristics for finding relevant information. We illustrate these techniques with several examples, concentrating especially on the RENTME system, a FINDME system for helping users find suitable rental apartments in the Chicago metropolitan area.

#index 1499535
#* Explicit representations of problem-solving strategies to support knowledge acquisition
#@ Yolanda Gil;Eric Melz
#t 1996
#c 10
#% 2768
#% 37909
#% 55936
#% 105499
#% 134101
#% 134110
#% 134111
#% 153509
#% 156338
#% 176991
#% 179741
#% 444889
#% 449526
#% 459120
#! Role-limiting approaches support knowledge acquisition (KA) by centering knowledge base construction on common types of tasks or domain-independent problem-solving strategies. Within a particular problem-solving strategy, domain-dependent knowledge plays specific roles. A KA tool then helps a user to fill these roles. Although role-limiting approaches are useful for guiding KA, they are limited because they only support users in filling knowledge roles that have been built in by the designers of the KA system. EXPECT takes a different approach to KA by representing problem-solving knowledge explicitly, and deriving from the current knowledge base the knowledge gaps that must be resolved by the user during KA. This paper contrasts role-limiting approaches and EXPECT's approach, using the propose-and-revise strategy as an example. EXPECT not only supports users in filling knowledge roles, but also provides support in making other modifications to the knowledge base, including adapting the problem-solving strategy. EXPECT's guidance changes as the knowledge base changes, providing a more flexible approach to knowledge acquisition. This work provides evidence supporting the need for explicit representations in building knowledge-based systems.

#index 1499536
#* Commonkads models for knowledge-based planning
#@ John Kingston;Nigel Shadbolt;Austin Tate
#t 1996
#c 10
#% 2768
#% 125066
#% 182492
#% 362942
#% 444999
#% 459150
#! The CommonKADS methodology is a collection of structured methods for building knowledge-based systems. A key component of CommonKADS is the library of generic inference models which can be applied to tasks of specified types. These generic models can either be used as frameworks for knowledge acquisition, or to verify the completeness of models developed by analysis of the domain. However, the generic models for some task types, such as knowledge-based planning, are not well-developed. Since knowledge-based planning is an important commercial application of Artificial Intelligence, there is a clear need for the development of generic models for planning tasks. Many of the generic models which currently exist have been derived from modelling of existing AI systems. These models have the strength of proven applicability. There are a number of well-known and well-tried AI planning systems in existence; one of the best known is the Open Planning Architecture (O-Plan). This paper describes the development of a CommonKADS generic inference model for knowledge-based planning tasks, based on the capabilities of the O-Plan system. The paper also describes the verification of this model in the context of a real-life planning task: the assignment and management of Royal Air Force Search and Rescue operations.

#index 1499537
#* Detecting knowledge base inconsistencies using automated generation of text and examples
#@ Vibhu O. Mittal;Johanna D. Moore
#t 1996
#c 10
#% 25110
#% 28163
#% 154456
#% 179610
#% 179741
#% 373774
#% 555583
#% 740937
#% 1013815
#! Verifying the fidelity of domain representation in large knowledge bases (KBs) is a difficult problem: domain experts are typically not experts in knowledge representation languages, and as knowledge bases grow more complex, visual inspection of the various terms and their abstract definitions, their interrelationships and the limiting, boundary cases becomes much harder. This paper presents an approach to help verify and refine abstract term definitions in knowledge bases. It assumes that it is easier for a domain expert to determine the correctness of individual concrete examples than it is to verify and correct all the ramifications of an abstract, intensional specification. To this end, our approach presents the user with an interface in which abstract terms in the KB are described using examples and natural language generated from the underlying domain representation. Problems in the KB are therefore manifested as problems in the generated description. The user can then highlight specific examples or parts of the explanation that seem problematic. The system reasons about the underlying domain model by using the discourse plan generated for the description. This paper briefly describes the working of the system and illustrates three possible types of problem manifestations using an example of a specification of floating-point numbers in Lisp.

#index 1499538
#* Path-based rules in object-oriented programming
#@ James M. Crawford;Daniel Dvorak;Diane Litman;Anil Mishra;Peter F. Patel-Schneider
#t 1996
#c 10
#% 53531
#% 58345
#% 103369
#% 114323
#% 394417
#% 463423
#% 480621
#% 481259
#% 669310
#% 1290131
#! Object-oriented programming has recently emerged as one of the most important programming paradigms. While object-oriented programming clearly owes an intellectual debt to AI, it appears to be displacing some AI techniques, such as rule-based programming, from the marketplace. This need not be so as path-based rules--forward-chaining production rules that are restricted to follow pointers between objects--fit into the object-oriented paradigm in a clean and elegant way. The combination of path-based rules and object-oriented programming should be useful in AI applications, and in the more general problem of transferring AI techniques to the larger computer science community.

#index 1499539
#* Approximate knowledge compilation: the first order case
#@ Alvaro del Val
#t 1996
#c 10
#% 132176
#% 167197
#% 179972
#% 181339
#% 204396
#% 556365
#% 1274089
#% 1275334
#% 1275335
#! Knowledge compilation procedures make a knowledge base more explicit so as make inference with respect to the compiled knowledge base tractable or at least more efficient. Most work to date in this area has been restricted to the propositional case, despite the importance of first order theories for expressing knowledge concisely. Focusing on (LUB) approximate compilation (Selman and Kautz 1991), our contribution is twofold: • We present a new ground algorithm for approximate compilation which can produce exponential savings with respect to the previously known algorithm (Selman and Kautz 1991). • We show that both ground algorithms can be lifted to the first order case preserving their correctness for approximate compilation.

#index 1499540
#* A new algorithm for computing theory prime implicates compilations
#@ Pierre Marquis;Samira Sadaoui
#t 1996
#c 10
#% 78702
#% 121397
#% 154456
#% 175453
#% 1011243
#% 1273543
#% 1275335
#% 1279708
#% 1499541
#! We present a new algorithm (called TPI/BDD) for computing the theory prime implicates compilation of a knowledge base Σ. In contrast to many compilation algorithms, TPI/BDD does not require the prime implicates of Σ to be generated. Since their number can easily be exponential in the size of Σ, TPI/BDD can save a lot of computing. Thanks to TPI/BDD, we can now conceive of compiling knowledge bases impossible to before.

#index 1499541
#* Compilation for critically constrained knowledge bases
#@ Robert Schrag
#t 1996
#c 10
#% 39632
#% 107545
#% 126392
#% 159434
#% 177892
#% 204396
#% 210193
#% 288165
#% 327779
#% 560095
#% 1011243
#% 1279708
#! We show that many "critically constrained" Random 3SAT knowledge bases (KBs) can be compiled into disjunctive normal form easily by using a variant of the "Davis-Putnam" proof procedure. From these compiled KBs we can answer all queries about entailment of conjunctive normal formulas, also easily -- compared to a "brute-force" approach to approximate knowledge compilation into unit clauses for the same KBs. We exploit this fact to develop an aggressive hybrid approach which attempts to compile a KB exactly until a given resource limit is reached, then falls back to approximate compilation into unit clauses. The resulting approach handles all of the critically constrained Random 3SAT KBs with average savings of an order of magnitude over the brute-force approach.

#index 1499542
#* Spatial aggregation: language and applications
#@ Christopher Bailey-Kellogg;Feng Zhao;Kenneth Yip
#t 1996
#c 10
#% 106271
#% 109856
#% 109857
#% 167625
#% 515571
#% 1272294
#! Spatial aggregation is a framework for organizing computations around image-like, analogue representations of physical processes in data interpretation and control tasks. It conceptualizes common computational structures in a class of implemented problem solvers for difficult scientific and engineering problems. It comprises a mechanism, a language, and a programming style. The spatial aggregation mechanism transforms a numerical input field to successively higher-level descriptions by applying a small, identical set of operators to each layer given a metric, neighborhood relation and equivalence relation. This paper describes the spatial aggregation language and its applications. The spatial aggregation language provides two abstract data types - neighborhood graph and field -- and a set of interface operators for constructing the transformations of the field, together with a library of component implementations from which a user can mix-and-match and specialize for a particular application. The language allows users to isolate and express important computational ideas in different problem domains while hiding low-level details. We illustrate the use of the language with examples ranging from trajectory grouping in dynamics interpretation to region growing in image analysis. Programs for these different task domains can be written in a modular, concise fashion in the spatial aggregation language.

#index 1499543
#* Computing abstraction hierarchies by numerical simulation
#@ Alan Bundy;Fausto Giunchiglia;Roberto Sebastiani;Toby Walsh
#t 1996
#c 10
#% 21145
#% 124729
#% 172505
#% 175391
#% 1274325
#! We present a novel method for building ABSTRIPS-style abstraction hierarchies in planning. The aim of this method is to minimize the amount of backtracking between abstraction levels. Previous approaches have determined the criticality of operator preconditions by reasoning about plans directly. Here, we adopt a simpler and faster approach where we use numerical simulation of the planning process. We demonstrate the theoretical advantages of our approach by identifying some simple properties lacking in previous approaches but possessed by our method. We demonstrate the empirical advantages of our approach by a set of four benchmark experiments using the ABTWEAK system. We compare the quality of the abstraction hierarchies generated with those built by the ALPINE and HIGHPOINT algorithms.

#index 1499544
#* Hierarchical A *: searching abstraction hierarchies efficiently
#@ Robert C. Holte;M. B. Perez;R. M. Zimmer;A. J. MacDonald
#t 1996
#c 10
#% 172505
#% 181341
#% 217077
#% 451046
#% 1274430
#! Abstraction, in search, problem solving, and planning, works by replacing one state space by another (the "abstract" space) that is easier to search. The results of the search in the abstract space are used to guide search in the original space. For instance, the length of the abstract solution can be used as a heuristic for A* in searching in the original space. However, there are two obstacles to making this work efficiently. The first is a theorem (Valtorta, 1984) stating that for a large class of abstractions, "embedding abstractions," every state expanded by blind search must also be expanded by A* when its heuristic is computed in this way. The second obstacle arises because in solving a problem A* needs repeatedly to do a full search of the abstract space while computing its heuristic. This paper introduces a new abstraction-induced search technique, "Hierarchical A*," that gets around both of these difficulties: first, by drawing from a different class of abstractions, "homomorphism abstractions," and, secondly, by using novel caching techniques to avoid repeatedly expanding the same states in successive searches in the abstract space. Hierarchical A* outperforms blind search on all the search spaces studied.

#index 1499545
#* Commitment strategies in hierarchical task network planning
#@ Reiko Tsuneto;Kutluhan Erol;James Hendler;Dana Nau
#t 1996
#c 10
#% 21145
#% 109935
#% 125386
#% 163715
#% 179766
#% 192708
#% 194651
#% 214718
#% 647059
#% 1272166
#% 1272550
#% 1279703
#! This paper compares three commitment strategies for HTN planning: (1) a strategy that delays variable bindings as much as possible; (2) a strategy in which no non-primitive task is expanded until all variable constraints are committed; and (3) a strategy that chooses between expansion and variable instantiation based on the number of branches that will be created in the search tree. Our results show that while there exist planning domains in which the first two strategies do well, the third does well over a broader range of planning domains.

#index 1499546
#* A semantic characterization of an algorithm for estimating others' beliefs from observation
#@ Hideki Isozaki;Hirofumi Katsuno
#t 1996
#c 10
#% 36683
#% 154456
#% 188086
#% 748869
#% 781188
#! Human beings often estimate others' beliefs and intentions when they interact with others. Estimation of others' beliefs will be useful also in controlling the behavior and utterances of artificial agents, especially when lines of communication are unstable or slow. But, devising such estimation algorithms and background theories for the algorithms is difficult, because of many factors affecting one's belief. We have proposed an algorithm that estimates others' beliefs from observation in the changing world. Experimental results show that this algorithm returns natural answers to various queries. However, the algorithm is only heuristic, and how the algorithm deals with beliefs and their changes is not entirely clear. We propose certain semantics based on a nonstandard structure for modal logic. By using these semantics, we shed light on a logical meaning of the belief estimation that the algorithm deals with. We also discuss how the semantics and the algorithm can be generalized.

#index 1499547
#* What is believed is what is explained (sometimes)
#@ Renwei Li;Luís Moniz Pereira
#t 1996
#c 10
#% 8417
#% 169166
#% 1290098
#! This paper presents a formal and computational methodology for incorporation of new knowledge into knowledge bases about actions and changes. We employ Gelfond and Lifschitz' action description language A to describe domains of actions. The knowledge bases on domains of actions are defined and obtained by a new translation from domain descriptions in A into abductive normal logic programs, where a time dimension is incorporated. The knowledge bases are shown to be both sound and complete with respect to their domain descriptions. In particular, we propose a possible causes approach (PCA) to belief update based on the slogan: What is believed is what is explained. A possible cause of new knowledge consists of abduced occurrences of actions and value propositions about the initial state of the domain of actions, that would allow to derive the new knowledge. We show how to compute possible causes with abductive logic programming, and present some techniques to improve search efficiency. We use examples to compare our possible causes approach with Ginsberg's possible worlds approach (PWA) and Winslett's possible models approach (PMA).

#index 1499548
#* The complexity of model checking for belief revision and update
#@ Paolo Liberatore;Marco Schaerf
#t 1996
#c 10
#% 2655
#% 8417
#% 90860
#% 101922
#% 117858
#% 123098
#% 131559
#% 198471
#% 416007
#% 1273403
#% 1273470
#% 1273492
#% 1275338
#! One of the main challenges in the formal modeling of common-sense reasoning is the ability to cope with the dynamic nature of the world. Among the approaches put forward to address this problem are belief revision and update. Given a knowledge base T, representing our knowledge of the "state of affairs" of the world of interest, it is possible that we are lead to trust another piece of information P, possibly inconsistent with the old one T. The aim of revision and update operators is to characterize the revised knowledge base T′ that incorporates the new formula P into the old one T while preserving consistency and, at the same time, avoiding the loss of too much information in this process. In this paper we study the computational complexity of one of the main computational problems of belief revision and update: deciding if an interpretation M is a model of the revised knowledge base.

#index 1499549
#* Updating knowledge bases with disjunctive information
#@ Yan Zhang;Norman Y. Foo
#t 1996
#c 10
#% 109945
#% 173565
#! It is well known that the minimal change principle was widely used in knowledge base updates. However, recent research has shown that conventional minimal change methods, eg. the PMA (Winslett 1988), are generally problematic for updating knowledge bases with disjunctive information. In this paper, we propose two different approaches to deal with this problem - one is called the minimal change with exceptions (MCE), the other is called the minimal change with maximal disjunctive inclusions (MCD). The first method is syntax-based, while the second is model-theoretic. We show that these two approaches are equivalent for propositional knowledge base updates, and the second method is also appropriate for first order knowledge base updates. We then prove that our new update approaches still satisfy the standard Katsuno and Mendelzon's update postulates.

#index 1499550
#* Irrelevance and conditioning in first-order probabilistic logic
#@ Daphne Koller;Joseph Y. Halpern
#t 1996
#c 10
#% 22296
#% 44876
#% 89958
#% 90371
#% 1290146
#! First-order probabilistic logic is a powerful knowledge representation language. Unfortunately, deductive reasoning based on the standard semantics for this logic does not support certain desirable patterns of reasoning, such as indifference to irrelevant information or substitution of constants into universal rules. We show that both these patterns rely on a first-order version of probabilistic independence, and provide semantic conditions to capture them. The resulting insight enables us to understand the effect of conditioning on independence, and allows us to describe a procedure for determining when independencies are preserved under conditioning. We apply this procedure in the context of a sound and powerful inference algorithm for reasoning from statistical knowledge bases.

#index 1499551
#* The limits on combining recursive horn rules with description logics
#@ Alon Y. Levy;Marie-Christine Rousset
#t 1996
#c 10
#% 36683
#% 101435
#% 108697
#% 145400
#% 161239
#% 172356
#% 179610
#% 461605
#% 558233
#% 1268735
#% 1274719
#% 1499471
#! Horn rule languages have formed the basis for many Artificial Intelligence application languages, but are not expressive enough to model domains with a rich hierarchical structure. Description logics have been designed especially to model rich hierarchies. Several applications would significantly benefit from combining the expressive power of both formalisms. This paper focuses on combining recursive function-free Horn rules with the expressive description logic ALCNR, and shows exactly when a hybrid language with decidable inference can be obtained. First, we show that several of the core constructors of description logics lead by themselves to undecidability of inference when combined with recursive function-free Horn rules. We then show that without these constructors we obtain a maximal subset of ALCNR that yields a decidable hybrid language. Finally, we describe a restriction on the Horn rules that guarantees decidable inference when combined with all of ALCNR, and covers many of the common usages of recursive rules.

#index 1499552
#* Verification of knowledge bases based on containment checking
#@ Alon Y. Levy;Marie-Christine Rousset
#t 1996
#c 10
#% 583
#% 23898
#% 36181
#% 36683
#% 122396
#% 154067
#% 162636
#% 162638
#% 198473
#% 289266
#% 289384
#% 416042
#% 556363
#% 1268735
#% 1499551
#! Building complex knowledge based applications requires encoding large amounts of domain knowledge. After acquiring knowledge from domain experts, much of the effort in building a knowledge base goes into verifying that the knowledge is encoded correctly. We consider the problem of verifying hybrid knowledge bases that contain both Horn rules and a terminology in a description logic. Our approach to the verification problem is based on showing a close relationship to the problem of query containment. Our first contribution, based on this relationship, is presenting a thorough analysis of the decidability and complexity of the verification problem, for knowledge bases containing recursive rules and the interpreted predicates =, ≤,

#index 1499553
#* Closed terminologies in description logics
#@ Robert A. Weida
#t 1996
#c 10
#% 58347
#% 68410
#% 103865
#% 108693
#% 461605
#% 541029
#% 702958
#! We introduce a predictive concept recognition methodology for description logics based on a new closed terminology assumption. During knowledge engineering, our system adopts the standard open terminology assumption as it automatically classifies concept descriptions into a taxonomy via subsumption inferences. However, for applications like configuration, the terminology becomes fixed during problem solving. Then, closed terminology reasoning is more appropriate. In our interactive configuration application, a user incrementally specifies an individual computer system in collaboration with a configuration engine. Choices can be made in any order and at any level of abstraction. We distinguish between abstract and concrete concepts to formally define when an individual's description may be considered finished. We also take advantage of the closed terminology assumption, together with the terminology's subsumption-based organization, to efficiently track the types of systems and components consistent with current choices, infer additional constraints on current choices, and appropriately guide future choices. Thus, we can help focus the efforts of both user and configuration engine.

#index 1499554
#* Quantificational logic of context
#@ Saša Buvač
#t 1996
#c 10
#% 28185
#% 116625
#% 117871
#% 127670
#% 179974
#% 199820
#% 671917
#% 1273614
#% 1907872
#% 1907875
#! In this paper we extend the Propositional Logic of Context, (Buvač & Mason 1993; Buvač, Buvač, & Mason 1995), to the quantificational (predicate calculus) case. This extension is important in the declarative representation of knowledge for two reasons. Firstly, since contexts are objects in the semantics which can be denoted by terms in the language and which can be quantified over, the extension enables us to express arbitrary first-order properties of contexts. Secondly, since the extended language is no longer only propositional, we can express that an arbitrary predicate calculus formula is true in a context. The paper describes the syntax and the semantics of a quantificational language of context, gives a Hilbert style formal system, and outlines a proof of the system's completeness.

#index 1499555
#* Utilizing knowledge-base semantics in graph-based algorithms
#@ Adnan Darwiche
#t 1996
#c 10
#% 1145
#% 44876
#% 55926
#% 160188
#% 205376
#% 1275254
#% 1275464
#% 1650642
#! Graph-based algorithms convert a knowledge base with a graph structure into one with a tree structure (a join-tree) and then apply tree-inference on the result. Nodes in the join-tree are cliques of variables and tree-inference is exponential in w*, the size of the maximal clique in the join-tree. A central property of join-trees that validates tree-inference is the running-intersection property: the intersection of any two cliques must belong to every clique on the path between them. We present two key results in connection to graph-based algorithms. First, we show that the running-intersection property, although sufficient, is not necessary for validating tree-inference. We present a weaker property for this purpose, called running-interaction, that depends on nonstructural (semantical) properties of a knowledge base. We also present a linear algorithm that may reduce w* of a join-tree, possibly destroying its running-intersection property, while maintaining its running-interaction property and, hence, its validity for tree-inference. Second, we develop a simple algorithm for generating trees satisfying the running-interaction property. The algorithm bypasses triangulation (the standard technique for constructing join-trees) and does not construct a join-tree first. We show that the proposed algorithm may in some cases generate trees that are more efficient than those generated by modifying a join-tree.

#index 1499556
#* Scaling up logic-based truth maintenance systems via fact garbage collection
#@ John O. Everett;Kenneth D. Forbus
#t 1996
#c 10
#% 1797
#% 111947
#% 154456
#% 179980
#% 457176
#% 1476345
#% 1477111
#% 1477235
#% 1477288
#% 1477290
#% 1478579
#% 1478637
#! Truth maintenance systems provide caches of beliefs and inferences that support explanations and search. Traditionally, the cost of using a TMS is monotonic growth in the size of this cache. In some applications this cost is too high; for example, intelligent learning environments may require students to explore many alternatives, which leads to unacceptable performance. This paper describes an algorithm for fact garbage collection that retains the explanation-generating capabilities of a TMS while eliminating the increased storage overhead. We describe the application context that motivated this work and the properties of applications that benefit from this technique. We present the algorithm, showing how to balance the tradeoff between maintaining a useful cache and reclaiming storage, and analyze its complexity. We demonstrate that this algorithm can eliminate monotonic storage growth, thus making it more practical to field large-scale TMS-based systems.

#index 1499557
#* Contextual reasoning is NP-complete
#@ Fabio Massacci
#t 1996
#c 10
#% 68608
#% 84229
#% 116625
#% 117871
#% 127670
#% 160385
#% 179922
#% 188086
#% 560744
#% 601159
#% 1273613
#% 1273614
#% 1275338
#% 1907875
#! The logic of context with the ist (c, p) modality has been proposed by McCarthy as a foundation for contextual reasoning. This paper shows that propositional logic of context is NP-complete and therefore more tractable than multimodal logics or Multi Language hierarchical logics which are PSPACE-complete. This result is given in a proof-theoretical way by providing a tableau calculus, which can be used as a decision procedure for automated reasoning. The computational gap between logic of context and modal logics is analyzed and some indications for the use of either formalisms are drawn on the basis of the tradeoff between compactness of representation and tractability of reasoning.

#index 1499558
#* Toward efficient default reasoning
#@ David W. Etherington;James M. Crawford
#t 1996
#c 10
#% 59340
#% 100131
#% 100146
#% 114323
#% 179922
#% 747439
#% 1275338
#% 1279714
#! Early work on default reasoning aimed to formalize the notion of quickly "jumping to conclusions". Unfortunately, the resulting formalisms have proven more computationally complex than classical logics. This has dramatically limited the applicability of formal methods to real problems involving defaults. The complexity of consistency checking is one of the two problems that must be addressed to reduce the complexity of default reasoning. We propose to approximate consistency checking using a novel synthesis of limited contexts and fast incomplete checks, and argue that this combination overcomes the limitations of its component parts. Our approach trades correctness for speed, but we argue that the nature of default reasoning makes this trade relatively inexpensive and intuitively plausible. We present a prototype implementation of a default reasoner based on these ideas, and a preliminary empirical evaluation.

#index 1499559
#* Situation calculus on a dense flow of time
#@ Akira Fusaoka
#t 1996
#c 10
#% 26351
#% 101943
#% 107123
#% 781175
#% 1273409
#! In this paper, we attempt to reconstruct the situation calculus on a dense flow of time. The proposed system: ISC, which is formulated in the framework of S2S (the monoadic second-order theory of two successor functions), allows to deal with temporal properties of time duration such as the continuity of fluents. Also it incorporates an intensional feature into the situation calculus so that the inferential process itself can be represented in ISC. On the basis of this modification, we define a nonmonotonic schema called epistemological minimization which selects the preferable model with respect to the information order in the inferential process. This method of nonmonotonic reasoning is useful for a temporal explanation problem because a sequence of events is interpreted sometimes depending on the information order in the inferential process rather than the chronological order of the actual process.

#index 1499560
#* Reasoning about continuous processes
#@ Christoph S. Herrmann;Michael Thielscher
#t 1996
#c 10
#% 399
#% 42004
#% 46270
#% 92771
#% 100169
#% 151236
#% 166232
#% 171046
#% 467632
#% 1273638
#% 1273656
#% 1290149
#! Overcoming the disadvantages of equidistant discretization of continuous actions, we introduce an approach that separates time into slices of varying length bordered by certain events. Such events are points in time at which the equations describing the system's behavior--that is, the equations which specify the ongoing processes--change. Between two events the system's parameters stay continuous. A high-level semantics for drawing logical conclusions about dynamic systems with continuous processes is presented, and we have developed an adequate calculus to automate this reasoning process. In doing this, we have combined deduction and numerical calculus, offering logical reasoning about precise, quantitative system information. The scenario of multiple balls moving in 1-dimensional space interacting with a pendulum serves as demonstration example of our method.

#index 1499561
#* Splitting a default theory
#@ Hudson Turner
#t 1996
#c 10
#% 95339
#% 150900
#% 167539
#% 171033
#% 369148
#% 383293
#% 499496
#% 1273656
#% 1290152
#! This paper presents mathematical results that can sometimes be used to simplify the task of reasoning about a default theory, by "splitting it into parts." These so-called Splitting Theorems for default logic are related in spirit to "partial evaluation" in logic programming, in which results obtained from one part of a program are used to simplify the remainder of the program. In this paper we focus primarily on the statement and proof of the Splitting Theorems for default logic. We illustrate the usefulness of the results by applying them to an example default theory for commonsense reasoning about action.

#index 1499562
#* Formalizing narratives using nested circumscription
#@ Chitta Baral;Alfredo Gabaldon;AIessandro Provetti
#t 1996
#c 10
#% 3035
#% 7047
#% 58180
#% 146247
#% 167541
#% 184795
#% 198462
#! The representation of narratives of actions and observations is a current issue in Knowledge Representation, where traditional plan-oriented treatments of action seem to fall short. To address narratives, Pinto and Reiter have extended Situation Calculus axioms, Kowalski and Sergot have introduced the Event Calculus in Logic Programming, and Baral et al. have defined the specification language L which allows to express actual and hypothetical situations in a uniform setting. The L entailment relation can formalize several forms of reasoning about actions and change. In this paper we illustrate a translation of L theories into Nested Abnormality Theories, a novel form of circumscription. The proof of soundness and completeness of the translation is the main technical result of the paper, but attention is also devoted to the features of Nested Abnormality Theories to capture commonsense reasoning in general and to clarify which assumptions a logical formalization forces upon a domain. These results also help clarifying the relationship between L and other recent circumscriptive formalizations for narratives, such as Miller and Shanahan's.

#index 1499563
#* Reasoning about nondeterministic and concurrent actions: a process algebra approach
#@ Giuseppe De Giacomo;Xiao Jun Chen
#t 1996
#c 10
#% 1791
#% 2659
#% 61005
#% 89961
#% 90232
#% 101953
#% 132393
#% 135884
#% 145713
#% 155825
#% 165656
#% 179613
#% 368816
#% 369768
#% 384978
#% 557443
#% 1272746
#% 1279705
#! In this paper, we study reasoning about actions following a model checking approach in contrast to the usual validity checking one. Specifically, we model a dynamic system as a transition graph which represents all the possible system evolutions in terms of state changes caused by actions. Such a transition graph is defined by means of a suitable process algebra associated with an explicit global store. To reason about system properties we introduce an extension of modal µ-calculus. This setting, although directly applicable only when complete information on the system is available, has several interesting features for reasoning about actions. On one hand, it inherits from the vast literature on process algebras tools for dealing with complex systems, treating suitably important aspects like parallelism, communications, interruptions, coordinations among agents. On the other hand, reasoning by model checking is typically much easier than more general logical services such as validity checking.

#index 1499564
#* On the range of applicability of baker's approach to the frame problem
#@ G. Neelakantan Kartha
#t 1996
#c 10
#% 3035
#% 26351
#% 100169
#% 107123
#% 117869
#% 146248
#% 167541
#% 167644
#% 169166
#% 1273638
#! We investigate the range of applicability of Baker's approach to the frame problem using an action language. We show that for temporal projection and deterministic domains, Baker's approach gives the intuitively expected results.

#index 1499565
#* Embracing causality in specifying the indeterminate effects of actions
#@ Fangzhen Lin
#t 1996
#c 10
#% 117869
#% 1274639
#% 1290153
#% 1290157
#! This paper makes the following two contributions to formal theories of actions: Showing that a causal minimization framework can be used effectively to specify the effects of indeterminate actions; and showing that for certain classes of such actions, regression, an effective computational mechanism, can be used to reason about them.

#index 1499566
#* Improving case retrieval by remembering questions
#@ Richard Alterman;Daniel Griffin
#t 1996
#c 10
#% 118031
#% 156189
#% 177701
#% 449588
#% 451051
#% 451052
#% 1275278
#% 1275279
#! This paper discusses techniques that improve the performance of a case retrieval system, after it is deployed, as a result of the continued usage of the system, by remembering previous episodes of question answering. The user generates a request for information and the system responds with the retrieval of relevant case(s). A history of such transactional behavior over a given set of data is maintained by the system and used as a foundation for adapting its future retrieval behavior. With each transaction, the system acquires information about the usage of the system that is subsequently used to adjust the behavior of the system. This notion of a case retrieval system draws on a distinction between the system in isolation and the system as it is used for a particular set of cases. It also draws on distinctions between the designed system, the deployed system, and the system that emerges as it is used.

#index 1499567
#* Acquiring case adaptation knowledge: a hybrid approach
#@ David B. Leake;Andrew Kinley;David Wilson
#t 1996
#c 10
#% 55921
#% 113912
#% 168280
#% 366694
#% 373871
#! The ability of case-based reasoning (CBR) systems to apply cases to novel situations depends on their case adaptation knowledge. However, endowing CBR systems with adequate adaptation knowledge has proven to be a very difficult task. This paper describes a hybrid method for performing case adaptation, using a combination of rule-based and case-based reasoning. It shows how this approach provides a framework for acquiring flexible adaptation knowledge from experiences with autonomous adaptation and suggests its potential as a basis for acquisition of adaptation knowledge from interactive user guidance. It also presents initial experimental results examining the benefits of the approach and comparing the relative contributions of case learning and adaptation learning to reasoning performance.

#index 1499568
#* Detecting discontinuities in case-bases
#@ Hideo Shimazu;Yosuke Takashima
#t 1996
#c 10
#% 119924
#% 168280
#% 179738
#% 373871
#% 680011
#% 1273662
#! This paper describes a discontinuity detection method for case-bases and data bases. A discontinuous case or data record is defined as a case or data record whose specific attribute values are very differen t from those of other records retrieved with identical or similar input specifications. Using the proposed method, when a user gives an input specification, he/she can retrieve not only exactly-matched cases, but also similar cases and discontinuous cases. The proposed method has three steps: (1) Retrieving case records with input specifications which are the same as or similar to a user's input specification (Maybe Similar Case, MSC), (2) Selecting a case record which most closelv matches the user's input specification among MSCs (Base Case, BC), and (3) Detecting cases among MSCs whose output specifications are very different from those of BC. The proposed method has been implemented in the CARET case-based retrieval tool operating on commercial RDBMS. Because case-based reasoning systems rely on the underlying assumption that similar input specifications retrieve similar case records, discontinuity detection in casebases is indispensable, and our proposed method is especially useful.

#index 1499569
#* Source selection for analogical reasoning an empirical approach
#@ William A. Stubblefield;George F. Luger
#t 1996
#c 10
#% 37901
#% 55937
#% 136370
#% 168280
#% 368117
#% 449588
#! The effectiveness of an analogical reasoner depends upon its ability to select a relevant analogical source. In many problem domains, however, too little is known about target problems to support effective source selection. This paper describes the design and evaluation of SCAVENGER, an analogical reasoner that applies two techniques to this problem: (1) An assumption-based approach to matching that allows properties of candidate sources to match unknown target properties in the absence of evidence to the contrary. (2) The use of empirical learning to improve memory organization based on problem solving experience.

#index 1499570
#* An efficient algorithm for finding optimal gain-ratio multiple-split tests on hierarchical attributes in decision tree learning
#@ Hussein Almuallim;Yasuhiro Akiba;Shigeo Kaneda
#t 1996
#c 10
#% 44625
#% 73374
#% 92554
#% 129980
#% 136350
#% 170378
#% 179767
#% 449588
#! Given a set of training examples S and a tree-structured attribute x, the goal in this work is to find a multiple-split test defined on x that maximizes Quinlan's gain-ratio measure. The number of possible such multiple-split tests grows exponentially in the size of the hierarchy associated with the attribute. It is, therefore, impractical to enumerate and evaluate all these tests in order to choose the best one. We introduce an efficient algorithm for solving this problem that guarantees maximizing the gain-ratio over all possible tests. For a training set of m examples and an attribute hierarchy of height d, our algorithm runs in time proportional to dm, which makes it efficient enough for practical use.

#index 1499571
#* Learning trees and rules with set-valued features
#@ William W. Cohen
#t 1996
#c 10
#% 44624
#% 90277
#% 105501
#% 116147
#% 127850
#% 129993
#% 136350
#% 165110
#% 179785
#% 180945
#% 219053
#% 368845
#% 375017
#% 396021
#% 449508
#% 449566
#% 451055
#% 542321
#! In most learning systems examples are represented as fixed-length "feature vectors", the components of which are either real numbers or nominal values. We propose an extension of the feature-vector representation that allows the value of a feature to be a set of strings; for instance, to represent a small white and black dog with the nominal features size and species and the set-valued feature color, one might use a feature vector with size=small, species=canis-familiaris and color-{white, black}. Since we make no assumptions about the number of possible set elements, this extension of the traditional feature-vector representation is closely connected to Blum's "infinite attribute" representation. We argue that many decision tree and rule learning algorithms can be easily extended to set-valued features. We also show by example that many real-world learning problems can be efficiently and naturally represented with set-valued features; in particular, text categorization problems and problems that arise in propositionalizing first-order representations lend themselves to set-valued features.

#index 1499572
#* Lazy decision trees
#@ Jerome H. Friedman
#t 1996
#c 10
#% 73374
#% 92539
#% 115608
#% 132583
#% 136350
#% 156186
#% 160862
#% 169767
#% 442814
#% 700962
#% 1273395
#! Lazy learning algorithms, exemplified by nearest-neighbor algorithms, do not induce a concise hypothesis from a given training set; the inductive process is delayed until a test instance is given. Algorithms for constructing decision trees, such as C4.5, ID3, and CART create a single "best" decision tree during the training phase, and this tree is then used to classify test instances. The tests at the nodes of the constructed tree are good on average, but there may be better tests for classifying a specific instance. We propose a lazy decision tree algorithm--LAZYDT--that conceptually constructs the "best" decision tree for each test instance. In practice, only a path needs to be constructed, and a caching scheme makes the algorithm fast. The algorithm is robust with respect to missing values without resorting to the complicated methods usually seen in induction of decision trees. Experiments on real and artificial problems are presented.

#index 1499573
#* Bagging, boosting, and C4.S
#@ J. R. Quinlan
#t 1996
#c 10
#% 28176
#% 89885
#% 136350
#% 209021
#% 520224
#% 1272365
#% 1290036
#! Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered.

#index 1499574
#* The discovery of the causes of leprosy: a computational analysis
#@ Vincent Corruble;Jean-Gabriel Ganascia
#t 1996
#c 10
#% 65445
#% 115112
#% 136350
#% 214028
#% 449566
#! The role played by the inductive inference has been studied extensively in the field of Scientific Discovery. The work presented here tackles the problem of induction in medical research. The discovery of the causes of leprosy is analyzed and simulated using computational means. An inductive algorithm is proposed, which is successful in simulating some essential steps in the progress of the understanding of the disease. It also allows us to simulate the false reasoning of previous centuries through the introduction of some medical a priori inherited form archaic medicine. Corroborating previous research, this problem illustrates the importance of the social and cultural environment on the way the inductive inference is performed in medicine.

#index 1499575
#* Machine discovery based on numerical data generated in computer experiments
#@ Tsuyoshi Murata;Masamichi Shimura
#t 1996
#c 10
#% 179774
#% 195240
#% 451049
#% 1274425
#! In the discovery of useful theorems or formulas, experimental data acquisition plays a fundamental role. Most of the previous discovery systems which have the abilities for experimentation, however, require much knowledge for evaluating experimental results, or require plans of common experiments which are given to the systems in advance. Only few systems have been attempted to make experiments which enable the discovery based on acquired experimental data without depending on given initial knowledge. This paper proposes a new approach for discovering useful theorems in the domain of plane geometry by employing experimentation. In this domain, drawing a figure and observing it correspond to making experimentation since these two processes are preparations for acquiring geometrical data. EXPEDITION, a discovery system based on experimental data acquisition, generates figures by itself and acquires expressions describing relations among line segments and angles in the figures. Such expressions can be extracted from the numerical data obtained in the computer experiments. By using simple heuristics for drawing and observing figures, the system succeeds in discovering many new useful theorems and formulas as well as rediscovering well-known theorems, such as power theorems and Thales' theorem.

#index 1499576
#* Using a hybrid genetic algorithm and fuzzy logic for metabolic modeling
#@ John Yen;Bogju Lee;James C. Liao
#t 1996
#c 10
#% 46013
#% 114994
#% 369236
#% 589687
#! The identification of metabolic systems is a complex task due to the complexity of the system and limited knowledge about the model. Mathematical equations and ODE's have been used to capture the structure of the model, and the conventional optimization techniques have been used to identify the parameters of the model. In general, however, a pure mathematical formulation of the model is difficult due to parametric uncertainty and incomplete knowledge of mechanisms. In this paper, we propose a modeling approach that (1) uses fuzzy rule-based model to augment algebraic enzyme models that are incomplete, and (2) uses a hybrid genetic algorithm to identify uncertain parameters in the model. The hybrid genetic algorithm (GA) integrates a GA with the simplex method in functional optimization to improve the GA's convergence rate. We have applied this approach to modeling the rate of three enzyme reactions in E. coli central metabolism. The proposed modeling strategy allows (1) easy incorporation of qualitative insights into a pure mathematical model and (2) adaptive identification and optimization of key parameters to fit system behaviors observed in biochemical experiments.

#index 1499577
#* Incremental discovery of hidden structure: applications in theory of elementary particles
#@ Jan M. Żytkow;Paul J. Fischer
#t 1996
#c 10
#% 24538
#% 90185
#% 92556
#% 107249
#% 408302
#% 451040
#! Discovering hidden structure is a challenging, universal research task in Physics, Chemistry, Biology, and other disciplines. Not only must the elements of hidden structure be postulated by the discoverer, but they can only be verified by indirect evidence, at the level of observable objects. In this paper we describe a framework for hidden structure discovery, built on a constructive definition of hidden structure. This definition leads to operators that build models of hidden structure step by step, postulating hidden objects, their combinations and properties, reactions described in terms of hidden objects, and mapping between the hidden and the observed structure. We introduce the operator dependency diagram, which shows the order of operator application and model evaluation. Different observational knowledge supports different evaluation criteria, which lead to different search systems with verifiable sequences of operator applications. Isomorphfree structure generation is another issue critical for efficiency of search. We apply our framework in the system GELL-MANN, that hypothesizes hidden structure for elementary particles and we present the results of a large scale search for quark models.

#index 1499578
#* Formalizing dependency directed backtracking and explanation based learning in refinement search
#@ Subbarao Kambhampati
#t 1996
#c 10
#% 241
#% 25470
#% 65441
#% 68183
#% 68243
#% 160253
#% 170413
#% 174161
#% 194651
#% 216992
#! The ideas of dependency directed backtracking (DDB) and explanation based learning (EBL) have developed independently in constraint satisfaction. planning and problem solving communities. In this paper. I formalize and unify these ideas under the task-independent framework of refinement search. which can model the search strategies used in both planning and constraint satisfaction. I show that both DDB and EBL depend upon the common theory of explaining search failures and regressing them to higher levels of the search tree. The relevant issues of importance include (a) how the failures are explained and (b) how many failure explanations are remembered. This task-independent understanding of DDB and EBL helps support cross-fertilization of ideas among Constraint Satisfaction. Planning and Explanation-Based Learning communities.

#index 1499579
#* Learning efficient rules by maintaining the explanation structure
#@ Jihie Kim;Paul S. Rosenbloom
#t 1996
#c 10
#% 23011
#% 54107
#% 73375
#% 97623
#% 111388
#% 116623
#% 140191
#% 449586
#% 449587
#% 451031
#! Many learning systems suffer from the utility problem; that is, that time after learning is greater than time before learning. Discovering how to assure that learned knowledge will in fact speed up system performance has been a focus of research in explanation-based learning (EBL). One way to analyze the utility problem is by examining the differences between the match process (match search) of the learned rule and the problem-solving process from which it is learned. Prior work along these lines examined one such difference. It showed that if the search-control knowledge used during problem solving is not maintained in the match process for learned rules, then learning can engender a slowdown; but that this slowdown could be eliminated if the match is constrained by the original search-control knowledge. This article examines a second difference - when the structure of the problem solving differs from the structure of the match process for the learned rules, time after learning can be greater than time before learning. This article also shows that this slowdown can be eliminated by making the learning mechanism sensitive to the problem-solving structure; i.e., by reflecting such structure in the match of the learned rule.

#index 1499580
#* Compilation of non-contemporaneous constraints
#@ Robert E. Wray, III;John E. Laird;Randolph M. Jones
#t 1996
#c 10
#% 23011
#% 166347
#% 168422
#% 172505
#% 179879
#% 449587
#% 451031
#% 1290047
#! Hierarchical execution of domain knowledge is a useful approach for intelligent, real-time systems in complex domains. In addition, well-known techniques for knowledge compilation allow the reorganization of knowledge hierarchies into more efficient forms. However, these techniques have been developed in the context of systems that work in static domains. Our investigations indicate that it is not straightforward to apply knowledge compilation methods for hierarchical knowledge to systems that generate behavior in dynamic environments. One particular problem involves the compilation of non-contemporaneous constraints. This problem arises when a training instance dynamically changes during execution. After defining the problem, we analyze several theoretical approaches that address non-contemporaneous constraints. We have implemented the most promising of these alternatives within Soar, a software architecture for performance and learning. Our results demonstrate that the proposed solutions eliminate the problem in some situations and suggest that knowledge compilation methods are appropriate for interactive environments.

#index 1499581
#* Sequential inductive learning
#@ Jonathan Gratch
#t 1996
#c 10
#% 33946
#% 92537
#% 116172
#% 145224
#% 179753
#% 204856
#% 449588
#! This article advocates a new model for inductive learning. Called sequential induction, it helps bridge classical fixed-sample learning techniques (which are efficient but difficult to formally characterize), and worst-case approaches (which provide strong statistical guarantees but are too inefficient for practical use). Learning proceeds as a sequence of decisions which are informed by training data. By analyzing induction at the level of these decisions, and by utilizing the only enough data to make each decision, sequential induction provides statistical guarantees but with substantially less data than worst-case methods require. The sequential inductive model is also useful as a method for determining a sufficient sample size for inductive learning and as such, is relevant to learning problems where the preponderance of data or the cost of gathering data precludes the use of traditional methods.

#index 1499582
#* Learning to take actions
#@ Roni Khardon
#t 1996
#c 10
#% 26125
#% 68243
#% 82136
#% 162953
#% 167629
#% 179788
#% 203292
#% 203328
#% 373996
#% 449515
#% 449559
#% 449561
#% 449586
#% 449587
#% 451031
#% 601159
#% 1272292
#% 1274724
#% 1280016
#! We formalize a model for supervised learning of action strategies in dynamic stochastic domains, and show that pac-learning results on Occam algorithms hold in this model as well. We then identify a particularly useful bias for action strategies based on production rule systems. We show that a subset of production rule systems, including rules in predicate calculus style, smaIl hidden state, and unobserved support predicates, is properly learnable. The bias we introduce enables the learning algorithm to invent the recursive support predicates which are used in the action strategy, and to reconstruct the internal state of the strategy. It is also shown that hierarchical strategies are learnable if a helpful teacher is available, but that otherwise the problem is computationally hard.

#index 1499583
#* Testing the robustness of the genetic algorithm on the floating building block representation
#@ Robert K. Lindsay;Annie S. Wu
#t 1996
#c 10
#% 114994
#% 242799
#! Recent studies on a floating building block representation for the genetic algorithm (GA) suggest that there are many advantages to using the floating representation. This paper investigates the behavior of the GA on floating representation problems in response to three different types of pressures: (1) a reduction in the amount of genetic material available to the GA during the problem solving process, (2) functions which have negative-valued building blocks, and (3) randomizing non-coding segments. Results indicate that the GA's performance on floating representation problems is very robust. Significant reductions in genetic material (genome length) may be made with relatively small decrease in performance. The GA can effectively solve problems with negative building blocks. Randomizing non-coding segments appears to improve rather than harm GA performance.

#index 1499584
#* Identifying and eliminating mislabeled training instances
#@ Carla E. Brodley;Mark A. Friedl
#t 1996
#c 10
#% 92533
#% 132938
#% 136350
#% 182686
#% 443616
#% 449588
#! This paper presents a new approach to identifying and eliminating mislabeled training instances. The goal of this technique is to improve classification accuracies produced by learning algorithms by improving the quality of the training data. The approach employs an ensemble of classifiers that serve as a filter for the training data. Using an n-fold cross validation, the training data is passed through the filter. Only instances that the filter classifies correctly are passed to the final learning algorithm. We present an empirical evaluation of the approach for the task of automated land cover mapping from remotely sensed data. Labeling error arises in these data from a multitude of sources including lack of consistency in the vegetation classification used, variable measurement techniques, and variation in the spatial sampling resolution. Our evaluation shows that for noise levels of less than 40%, filtering results in higher predictive accuracy than not filtering, and for levels of class noise less than or equal to 20% filtering allows the base-line accuracy to be retained. Our empirical results suggest that the ensemble filter approach is an effective method for identifying labeling errors, and further, that the approach will significantly benefit ongoing research to develop accurate and robust remote sensing-based methods to map land cover at global scales.

#index 1499585
#* Generation of attributes for learning algorithms
#@ Yuh-Jyh Hu;Dennis Kibler
#t 1996
#c 10
#% 73374
#% 92148
#% 126860
#% 136350
#% 449566
#% 458178
#% 1273368
#% 1273393
#% 1273667
#% 1273668
#% 1280028
#! Inductive algorithms rely strongly on their representational biases. Constructive induction can mitigate representational inadequacies. This paper introduces the notion of a relative gain measure and describes a new constructive induction algorithm (GALA) which is independent of the learning algorithm. Unlike most previous research on constructive induction, our methods are designed as preprocessing step before standard machine learning algorithms are applied. We present the results which demonstrate the effectiveness of GALA on artificial and real domains for several learners: C4.5, CN2, percept ron and backpropagation.

#index 1499586
#* Structural regression trees
#@ Stefan Kramer
#t 1996
#c 10
#% 90154
#% 99396
#% 131402
#% 136350
#% 169655
#% 172512
#% 179785
#% 449508
#% 458303
#% 1272177
#% 1280038
#% 1290052
#! In many real-world domains the task of machine learning algorithms is to learn a theory for predicting numerical values. In particular several standard test domains used in Inductive Logic Programming (ILP) are concerned with predicting numerical values from examples and relational and mostly non-determinate background knowledge. However, so far no ILP algorithm except one can predict numbers and cope with nondeterminate background knowledge. (The only exception is a covering algorithm called FORS.) In this paper we present Structural Regression Trees (SRT), a new algorithm which can be applied to the above class of problems. SRT integrates the statistical method of regression trees into ILP. It constructs a tree containing a literal (an atomic formula or its negation) or a conjunction of literals in each node, and assigns a numerical value to each leaf. SRT provides more comprehensible results than purely statistical methods, and can be applied to a class of problems most other ILP systems cannot handle. Experiments in several real-world domains demonstrate that the approach is competitive with existing methods, indicating that the advantages are not at the expense of predictive accuracy.

#index 1499587
#* Discovering robust knowledge from dynamic closed-world data
#@ Chun-Nan Hsu;Craig A. Knoblock
#t 1996
#c 10
#% 33376
#% 51936
#% 153040
#% 232151
#% 368248
#% 396021
#% 420074
#% 442851
#% 452824
#% 458250
#% 689733
#% 691040
#! Many applications of knowledge discovery require the knowledge to be consistent with data. Examples include discovering rules for query optimization, database integration, decision support, etc. However, databases usually change over time and make machine-discovered knowledge inconsistent with data. Useful knowledge should be robust against database changes so that it is unlikely to become inconsistent after database changes. This paper defines this notion of robustness, describes how to estimate the robustness of Horn-clause rules in closed-world databases, and describes how the robustness estimation can be applied in rule discovery systems.

#index 1499588
#* Post-analysis of learned rules
#@ Bing Liu;Wynne Hsu
#t 1996
#c 10
#% 124708
#% 136350
#% 179008
#% 212710
#% 449566
#% 1290033
#! Rule induction research implicitly assumes that after producing the rules from a dataset, these rules will be used directly by an expert system or a human user. In real-life applications, the situation may not be as simple as that, particularly, when the user of the rules is a human being. The human user almost always has some previous concepts or knowledge about the domain represented by the dataset. Naturally, he/she wishes to know how the new rules compare with his/her existing knowledge. In dynamic domains where the rules may change over time, it is important to know what the changes are. These aspects of research have largely been ignored in the past. With the increasing use of machine leaming tcclmiques in practical applications such as data mining, this issue of post analysis of rules warrants greater emphasis and attention. In this paper, we propose a technique to deal with this problem. A system has been implemented to perform the post analysis of classification rules genemted by systems such as C4.5. The proposed technique is general and highly interactive. It will be particularly useful in data mining and data analysis.

#index 1499589
#* KI: a tool for knowledge integration
#@ Kenneth S. Murray
#t 1996
#c 10
#% 58319
#% 90188
#% 109848
#% 179982
#% 179983
#% 405391
#% 449587
#% 451035
#% 679215
#% 679521
#! Knowledge integration is the process of incorporating new information into a body of existing knowledge. It involves determining how new and existing knowledge interact and how existing knowledge should be modified to accommodate the new information. KI is a machine learning program that performs knowledge integration. Through actively investigating the interaction of new information with existing knowledge KI is capable of detecting and exploiting a variety of diverse learning opportunities during a single learning episode. Empirical evaluation suggests that KI provides significant assistance to knowledge engineers while integrating new information into a large knowledge base.

#index 1499590
#* Multi-strategy learning of search control for partial-order planning
#@ Tara A. Estlin;Raymond J. Mooney
#t 1996
#c 10
#% 65441
#% 90056
#% 124708
#% 154075
#% 163715
#% 169652
#% 170413
#% 216992
#% 449508
#% 449587
#% 451031
#% 679780
#! Most research in planning and learning has involved linear, state-based planners. This paper presents SCOPE, a system for learning search-control rules that improve the performance of a partial-order planner. SCOPE integrates explanation-based and inductive learning techniques to acquire control rules for a partial-order planner. Learned rules are in the form of selection heuristics that help the planner choose between competing plan refinements. Specifically, SCOPE learns domain-specific control rules for a version of the UCPOP planning algorithm. The resulting system is shown to produce significant speedup in two different planning domains.

#index 1499591
#* Design and implementation of a replay framework based on a partial order planner
#@ Laurie H. Ihrig;Subbarao Kambhampati
#t 1996
#c 10
#% 68243
#% 89781
#% 120806
#% 163715
#% 179932
#% 214287
#% 216992
#% 1274548
#! In this paper we describe the design and implementation of the derivation replay framework, DERSNLP+EBL (Derivational SNLP+EBL), which is based within a partial order planner. DERSNLP+EBL replays previous plan derivations by first repeating its earlier decisions in the context of the new problem situation, then extending the replayed path to obtain a complete solution for the new problem. When the replayed path cannot be extended into a new solution, explanation-based learning (EBL) techniques are employed to identify the features of the new problem which prevent this extension. These features are then added as censors on the retrieval of the stored case. To keep retrieval costs low, DERSNLP+EBL normally stores plan derivations for individual goals, and replays one or more of these derivations in solving multi-goal problems. Cases covering multiple goals are stored only when subplans for individual goals cannot be successfully merged. The aim in constructing the case library is to predict these goal interactions and to store a multi-goal case for each set of negatively interacting goals. We provide empirical results demonstrating the effectiveness of DERSNLP+EBL in improving planning performance on randomly-generated problems drawn from a complex domain.

#index 1499592
#* Is there any need for domain-dependent control information? a reply
#@ Steven Minton
#t 1996
#c 10
#% 65441
#% 126859
#% 145388
#% 159429
#% 170413
#% 179764
#% 370528
#% 408396
#% 652937
#! In this paper, we consider the role that domain-dependent control knowledge plays in problem solving systems. Ginsberg and Geddis (Ginsberg & Geddis 1991) have claimed that domain-dependent control information has no place in declarative systems; instead, they say, such information should be derived from declarative facts about the domain plus domain-independent principles. We dispute their conclusion, arguing that it is impractical to generate control knowledge solely on the basis of logical derivations. We propose that simplifying abstractions are crucial for deriving control knowledge, and, as a result, empirical utility evaluation of the resulting rules will frequently be necessary to validate the utility of derived control knowledge. We illustrate our arguments with examples from two implemented systems.

#index 1499593
#* Searching for planning operators with context-dependent and probabilistic effects
#@ Tim Oates;Paul R. Cohen
#t 1996
#c 10
#% 44836
#% 120808
#% 179955
#% 451047
#! Providing a complete and accurate domain model for an agent situated in a complex environment can be an extremely difficult task. Actions may have different effects depending on the context in which they are taken, and actions mayor may not induce their intended effects, with the probability of success again depending on context. We present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world. Empirical results show that the algorithm successfully finds operators that capture the true structure of an agent's interactions with its environment, and avoids spurious associations between actions and exogenous events.

#index 1499594
#* Learning robust plans for mobile robots from a single trial
#@ Sean P. Engelson
#t 1996
#c 10
#% 110211
#% 168280
#% 197596
#% 214814
#% 217915
#% 363500
#% 669427
#% 693286
#% 695783
#! We address the problem of learning robust plans for robot navigation by observing particular robot behaviors. In this paper we present a method which can learn a robust reactive plan from a single example of a desired behavior. The system operates by translating a sequence of events arising from the effector system into a plan which represents the dependencies among such events. This method allows us to rely on the underlying stability properties of low-level behavior processes in order to produce robust plans. Since the resultant plan reproduces the original behavior of the robot at a high level, it generalizes over small environmental changes and is robust to sensor and effector noise.

#index 1499595
#* An average-reward reinforcement learning algorithm for computing bias-optimal policies
#@ Sridhar Mahadevan
#t 1996
#c 10
#% 179795
#% 203604
#% 363744
#% 670940
#% 1290040
#! Average-reward reinforcement learning (ARL) is an undiscounted optimality framework that is generally applicable to a broad range of control tasks. ARL computes gain-optimal control policies that maximize the expected payoff per step. However, gainoptimality has some intrinsic limitations as an optimality criterion, since for example, it cannot distinguish between different policies that all reach an absorbing goal state, but incur varying costs. A more selective criterion is bias optimality, which can filter gain-optimal policies to select those that reach absorbing goals with the minimum cost. While several ARL algorithms for computing gain-optimal policies have been proposed, none of these algorithms can guarantee bias optimality, since this requires solving at least two nested optimality equations. In this paper, we describe a novel model-based ARL algorithm for computing bias-optimal policies. We test the proposed algorithm using an admission control queuing system, and show that it is able to utilize the queue much more efficiently than a gain-optimal method by learning bias-optimal policies.

#index 1499596
#* Auto-exploratory average reward reinforcement learning
#@ DoKyeong Ok;Prasad Tadepalli
#t 1996
#c 10
#% 120808
#% 124691
#% 124692
#% 135414
#% 179795
#% 203604
#% 203608
#% 361730
#% 363744
#% 670940
#% 1291498
#% 1499595
#! We introduce a model-based average reward Reinforcement Learning method called H-learning and compare it with its discounted counterpart, Adaptive Real-Time Dynamic Programming, in a simulated robot scheduling task. We also introduce an extension to H-learning, which automatically explores the unexplored parts of the state space, while always choosing greedy actions with respect to the current value function. We show that this "Auto-exploratory H-learning" performs better than the original H-learning under previously studied exploration methods such as random, recency-based, or counter-based exploration.

#index 1499597
#* Evolution-based discovery of hierarchical behaviors
#@ Justinian P. Rosca;Dana H. Ballard
#t 1996
#c 10
#% 114994
#% 124073
#% 153991
#% 174161
#% 181337
#% 197056
#% 200223
#% 458268
#% 466540
#% 703199
#% 1275322
#! Procedural representations of control policies have two advantages when facing the scale-up problem in learning tasks. First they are implicit, with potential for inductive generalization over a very large set of situations. Second they facilitate modularization. In this paper we compare several randomized algorithms for learning modular procedural representations. The main algorithm, called Adaptive Representation through Learning (ARL) is a genetic programming extension that relies on the discovery of subroutines. ARL is suitable for learning hierarchies of subroutines and for constructing policies to complex tasks. ARL was successfully tested on a typical reinforcement learning problem of controlling an agent in a dynamic and nondeterministic environment where the discovered subroutines correspond to agent behaviors.

#index 1613041
#* Agent-Mediated Electronic Commerce and Trading Agent Design and Analysis: AAMAS Workshop, AMEC 2008, Estoril, Portugal, May 12-16, 2008, and AAAI ... Notes in Business Information Processing)
#@ Wolfgang Ketter;Han La Poutr;Norman M. Sadeh;Onn Shehory;William Walsh
#t 2011
#c 10
#! This volume contains 13 thoroughly refereed and revised papers detailing recent advances in research on trading agents, negotiating agents, dynamic pricing, and auctions. They were originally presented at the 10th International Workshop on Agent-Mediated Electronic Commerce (AMEC 2008) collocated with AAMAS 2008 in Estoril, Portugal, or the 6th Workshop on Trading Agent Design and Analysis (TADA 2008) collocated with AAAI 2008 in Chicago, IL, USA. The papers originating from AMEC 2008 address agent modeling and multi-agent problems in the context of e-negotiations and e-commerce. The TADA papers stem from the effort to design scenarios where trading agents and market designers can be pitched against each other in applications from supply chain management and procurement. They are all characterized by interdisciplinary research combining fields such as artificial intelligence, distributed systems, game theory, and economics.

