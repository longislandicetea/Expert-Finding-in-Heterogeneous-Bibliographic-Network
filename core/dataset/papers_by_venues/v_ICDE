#index 454187
#* Proceedings of the Twelfth International Conference on Data Engineering
#@ Stanley Y. W. Su
#t 1996
#c 17

#index 454188
#* Proceedings of the Thirteenth International Conference on Data Engineering
#@ Alex Gray;Per-Åke Larson
#t 1997
#c 17

#index 454189
#* Proceedings of the Fourteenth International Conference on Data Engineering
#@ 
#t 1998
#c 17

#index 454190
#* Proceedings of the 17th International Conference on Data Engineering
#@ 
#t 2001
#c 17

#index 461883
#* Auditory Browsing for Acquisition of Information in Cyberspace
#@ Naoto Oki;Kunio Teramoto;Ken-ichi Okada;Yutaka Matsushita
#t 1996
#c 17
#! Today, various novel telecommunication systems have been proposed. The idea of a virtual communication space shared by multiple distributed users via networks, so-called cyberspace, has received considerable communication. In cyberspace, there are tremendously many users, on-line services, and much information, and users have great opportunity to encounter all of these. We think the most serious problem in cyberspace is the great risk that a user might miss a chance to get relevant or important information. In this paper, we propose a strategy for auditory browsing to address this problem, using a spatial sound interface. We implemented VCP (Virtual Cocktail Party), an experimental system for achieving efficient and flexible telecommunication and data retrieval, which takes advantage of human auditory capability. This system can support a number of physically separated users in a single shared sound cyberspace and consists of distributed terminals with a spatial sound interface.

#index 461884
#* SQL3 Update
#@ Jim Melton
#t 1996
#c 17
#! The third major generation of the standard for the SQL database language, known as SQL3, is nearing completion. Many significant new features have been added to SQL, both in areas traditionally addressed by SQL and in pursuit of adding object technology to the language. The standard has been partitioned into a number of distinct parts, each of which may progress at its own rate. Publication of SQL3 as a replacement for the current version of the standard, SQL-92, is expected no sooner than 1998.

#index 461885
#* Approximate Queries and Representations for Large Data Sequences
#@ Hagit Shatkay;Stanley B. Zdonik
#t 1996
#c 17
#! Many new database application domains such as experimental sciences and medicine are characterized by large sequences as their main form of data. Using approximate representation can significantly reduce the required storage and search space. A good choice of representation, can support a broad new class of approximate queries, needed in these domains. These queries are concerned with application-dependent features of the data as opposed to the actual sampled points. We introduce a new notion of generalized approximate queries and a general divide and conquer approach that supports them. This approach uses families of real-valued functions as an approximate representation. We present an algorithm for realizing our technique, and the results of applying it to medical cardiology data.

#index 461886
#* ODMG Update
#@ Dirk Bartels
#t 1996
#c 17
#! The Object Database Management Group (ODMG) is a consortium of the leading Object Database (ODMBS) vendors. The consortium was formed in 1992 with the objective to define a standard for the emerging ODBMS industry. Within 18 months, the first release of the standard, so called ODMG-93 was published in October 1993. The following abstract gives a comprehensive overview of the standard and the extension that have been made since the initial publishing. The overview includes the ODMG Object Model, the ODMG Object Definition Language (ODL), the ODMG Object Query Language (OQL), the ODMG C++ binding and the ODMG Smalltalk binding.

#index 461887
#* Deferred Updates and Data Placement in Distributed Databases
#@ Parvathi Chundi;Daniel J. Rosenkrantz;S. S. Ravi
#t 1996
#c 17
#! Commercial distributed database systems generally support an optional protocol that provides loose consistency of replicas, allowing replicas to be inconsistent for some time. In such a protocol, each replicated data item is assigned a primary copy site. Typically, a transaction updates only the primary copies of data items, with updates to other copies deferred until after the transaction commits. After a transaction commits, its updates to primary copies are sent transactionally to the other sites containing secondary copies. We investigate the transaction model underlying the above protocol. We show that global serializability in such a system is a property of the placement of primary and secondary copies of replicated data items. We present a polynomial time algorithm to assign primary sites to data items so that the resulting topology ensures serializability.

#index 461888
#* Electronic Catalogs - Panel
#@ Arthur M. Keller;Don Brown;Anna-Lena Neches;Sherif Danish;Daniel Barbará
#t 1996
#c 17

#index 461889
#* Consistency and Performance of Concurrent Interactive Database Applications
#@ Konstantinos Stathatos;Stephen Kelley;Nick Roussopoulos;John S. Baras
#t 1996
#c 17
#! In many modern database applications, there is an emerging need for interactive environments where users directly manipulate the contents of the database. Graphical user interfaces (GUIs) display images of the database which must reflect a consistent up--to--date state of the data with minimum perceivable delay to the user. Moreover, the possibility of several applications concurrently displaying different views of the same database increases the overall system complexity. In this paper, we show how design, performance and concurrency issues can be addressed by adapting existing database techniques. We propose the use of suitable display schemas whose instances compose active views of the database, an extended client caching scheme which is expected to yield significant performance benefits and a locking mechanism that maintains consistency between the GUIs and the database.

#index 461890
#* High Availability in Clustered Multimedia Servers
#@ Renu Tewari;Daniel M. Dias;Rajat Mukherjee;Harrick M. Vin
#t 1996
#c 17
#! Clustered multimedia servers, consisting of interconnected nodes and disks, have been proposed for large scale servers, that are capable of supporting multiple concurrent streams which access the video objects stored in the server. As the number of disks and nodes in the cluster increases, so does the probability of a failure. With data striped across all disks in a cluster, the failure of a single disk or node, results in the disruption of many or all streams in the system. Guaranteeing high availability in such a cluster becomes a primary requirement, to ensure continuous service. In this paper, we study mirroring and software RAID schemes with different placement strategies, that guarantee high availability in the event of disk and node failures, while satisfying the real-time requirements of the streams. We examine various declustering techniques for spreading the redundant information across disks and nodes and show that random declustering has good real-time performance. Finally, we compare the overall cost per stream for different system configurations. We derive the parameter space where mirroring and software RAID apply, and determine optimal parity group sizes.

#index 461891
#* Refined Triggering Graphs: A Logic-Based Approach to Termination Analysis in an Active Object-Oriented Database
#@ Anton P. Karadimce;Susan Darling Urban
#t 1996
#c 17
#! We present the notion of refined triggering graphs (RTG) for analyzing termination of active rules in object-oriented databases (OODBs). The RTG method consists of mapping the possibility that one active rule can trigger another to the satisfiability of a well-defined logic formula called a triggering formula. The unsatisfiability of the triggering formula is then an indication that the rule triggering possibility is nil. We identify three increasingly more powerful types of triggering formulae and give pointers to the corresponding satisfiability procedures.

#index 461892
#* Hypermedia Database ``Himotoki'' and Its Applications
#@ Yoshinori Hara;Kyoji Hirata;Hajime Takano;Shigehito Kawasaki
#t 1996
#c 17
#! This paper describes the design concept of a hypermedia database "Himotoki" and its navigational capabilities. A hypermedia database is a system that integrates hypermedia operations with database models. Advantages are: (a) its structured design can improve authoring and browsing capabilities for large hypermedia applications, (b) nodes and links can be automatically generated under a certain condition, (c) navigational data interface for DBMS is obtained, etc.For providing cost-effective operations as hypermedia databases, we introduce a set-to-set linking and the navigational functions, i.e., media-based navigation, schema navigation, moving hot-spot navigation. These functional capabilities make media contents well-organized so as to improve the human-machine interactive interface. Implemented applications such as "Electronic Aquatic Life" and "Hypermedia Museum" demonstrate the usefulness of Himotoki navigational functions and customizability of its architectural design.

#index 461893
#* A Log-Structured Organization for Tertiary Storage
#@ Daniel Alexander Ford;Jussi Myllymaki
#t 1996
#c 17
#! We present the design of a log-structured tertiary storage system (LTS). The advantage of this approach is that it allows the system to hide the details of juke-box robotics and media characteristics behind a uniform, random access, block-oriented interface. It also allows the system to avoid media mount operations for writes, giving write performance similar to that of secondary storage.

#index 461894
#* Are We Moving Toward an Information SuperHighway or a Tower of Babel? The Challenge of Large-Scale Semantic Heterogeneity
#@ Stuart E. Madnick
#t 1996
#c 17
#! The popularity and growth of the "Information Super Highway" have dramatically increased the number of information sources available for use. Unfortunately, there are significant challenges to be overcome. One particular problem is context interchange, whereby each source of information and potential receiver of that information may operate with a different context, leading to large-scale semantic heterogeneity. A context is the collection of implicit assumptions about the context definition (i.e., meaning) and context characteristics (i.e., quality) of the information. This paper describes various forms of context challenges and examples of potential context mediation services, such as data semantics acquisition, data quality attributes, and evolving semantics and quality, that can mitigate the problem.

#index 461895
#* The Microsoft Relational Engine
#@ Goetz Graefe
#t 1996
#c 17

#index 461896
#* Order Preserving Compression
#@ Gennady Antoshenkov;David B. Lomet;James Murray
#t 1996
#c 17
#! Order-preserving compression can improve sorting and searching performance, and hence the performance of database systems. We describe a new parsing (tokenization) technique that can be applied to variable-length "keys", producing substantial compression. It can both compress and decompress data, permitting variable lengths for dictionary entries and compressed forms. The key notion is to partition the space of strings into ranges, encoding the common prefix of each range. We illustrate our method with padding character compression for multi-field keys, demonstrating the dramatic gains possible. A specific version of the method has been implemented in Digital's Rdb relational database system to enable effective multi-field compression.

#index 461897
#* Complex Query Decorrelation
#@ Praveen Seshadri;Hamid Pirahesh;T. Y. Cliff Leung
#t 1996
#c 17
#! Complex queries used in decision support applications use multiple correlated subqueries and table expressions, possibly across several levels of nesting. It is usually inefficient to directly execute a correlated query; consequently, algorithms have been proposed to decorrelate the query, i.e., to eliminate the correlation by rewriting the query. This paper explains the issues involved in decorrelation, and surveys existing algorithms. It presents an efficient and flexible algorithm called magic decorrelation which is superior to existing algorithms both in terms of the generality of application, and the efficiency of the rewritten query. The algorithm is described in the context of its implementation in the Starburst Extensible Database System, and its performance is compared with other decorrelation techniques. The paper also explains why magic decorrelation is not merely applicable, but crucial in a parallel database system.

#index 461898
#* Improving the Performance of Multi-Dimensional Access Structures Based on k-d-Trees
#@ Andreas Henrich
#t 1996
#c 17
#! In recent years, various k-d-tree based multi-dimensional access structures have been proposed. All these structures share an average bucket utilization of at most ln 2 (about 69.3 %). In this paper we present two algorithms which perform local redistributions of objects to improve the storage utilization of these access structures. We show that under fair conditions a good improvement algorithm can save up to 20 % of space and up to 15 % of query processing time. On the other hand we also show that a local redistribution scheme designed without care, can improve the storage utilization and at the same time worsen the performance of range queries drastically. Furthermore we show the dependencies between split strategies and local redistribution schemes and the general limitations which can be derived from these dependencies.

#index 461899
#* Advanced Transaction Models in Workflow Contexts
#@ Gustavo Alonso;Divyakant Agrawal;Amr El Abbadi;Mohan Kamath;Roger Günthör;C. Mohan
#t 1996
#c 17
#! In recent years, numerous transaction models have been proposed to address the problems posed by advanced database applications, but only a few of these models are being used in commercial products. In this paper, we make the case that such models may be too centered around databases to be useful in real environments. Advanced applications raise a variety of issues that are not addressed at all by transaction models. These same issues, however, are the basis for existing workflow systems, which are having considerable success as commercial products in spite of not having a solid theoretical foundation. We explore some of these issues and show that, in many aspects, workflow models are a superset of transaction models and have the added advantage of incorporating a variety of ideas that to this date have remained outside the scope of traditional transaction processing.

#index 461900
#* The Mentor Project: Steps Toward Enterprise-Wide Workflow Management
#@ Dirk Wodtke;Jeanine Weißenfels;Gerhard Weikum;Angelika Kotz Dittrich
#t 1996
#c 17
#! Enterprise-wide workflow management where workflows may span multiple organizational units require particular consideration of scalability, heterogeneity, and availability issues. The Mentor project which is introduced in this paper aims to reconcile a rigorous workflow specification method with a distributed middleware architecture as a step towards enterprise-wide solutions. The project uses the formalism of state and activity charts and a commercial tool, Statemate, for workflow specification. A first prototype of Mentor has been built which allows executing specifications in a distributed manner A major contribution of this paper is the method for transforming a centralized state chart spectfication into a form that is amenable to a distributed execution and to incorporate the necessary synchronization between different processing entities. Fault tolerance issues are addressed by coupling Mentor with the Tuxedo TP monitor.

#index 461901
#* Secure Mediated Databases
#@ K. Selçuk Candan;Sushil Jajodia;V. S. Subrahmanian
#t 1996
#c 17
#! With the evolution of the information superhighway, there is now an immense amount of information available in a wide variety of databases. Furthermore, users often have the ability to access legacy software packages developed by external sources. However, sometimes both the information provided by a data source, as well as one or more of the functions available through a software package may be sensitive-in such cases, organizations require that access by users be controlled. HERMES (HEterogeneous Reasoning and MEdiator System) is a platform that has been developed at the University of Maryland within which mediators may be designed and implemented. HERMES has already been used for a number of applications. In this paper, we provide a formal model of security in mediated systems. We then develop techniques that are sound and complete and respect security constraints of packages/databases participating in the mediated system. The security constraints described an this paper have been implemented, and we describe the existing implementation.

#index 461902
#* Data Replication in Mariposa
#@ Jeff Sidell;Paul M. Aoki;Adam Sah;Carl Staelin;Michael Stonebraker;Andrew Yu
#t 1996
#c 17
#! The Mariposa distributed data manager uses an economic model for managing the allocation of both storage objects and queries to servers. We present extensions to the economic model which support replica management, as well as our mechanisms for propagating updates among replicas. We show how our replica control mechanism can be used to provide consistent, although potentially stale, views of data across many machines without expensive per-transaction synchronization. We present a rule-based conflict resolution mechanism, which can be used to enhance traditional time-stamp serialization. We discuss the effects of our replica system on query processing for both read-only and read-write queries. We further demonstrate how the replication model and mechanisms naturally support name service in Mariposa.

#index 461903
#* Knowledge Discovery from Telecommunication Network Alarm Databases
#@ Kimmo Hätönen;Mika Klemettinen;Heikki Mannila;Pirjo Ronkainen;Hannu Toivonen
#t 1996
#c 17
#! A telecommunication network produces daily large amounts of alarm data. The data contains hidden valuable knowledge about the behavior of the network. This knowledge can be used in filtering redundant alarms, locating problems in, the network, and possibly in predicting severe faults. We describe the TASA (Telecommunication Network Alarm Sequence Analyzer) system for discovering and browsing knowledge from large alarm databases. The system is built on the basis of viewing knowledge discovery as an interactive and iterative process, containing data collection, pattern discovery, rule postprocessing, etc. The system uses a novel framework for locating frequently occurring episodes from sequential data. The TASA system offers a variety of selection and ordering criteria for episodes, and supports iterative retrieval from the discovered knowledge. This means that a large part of the iterative nature of the KDD process can be replaced by iteration in the rule postprocessing stage. The user interface is based on dynamically generated HTML. The system is in experimental use, and the results are encouraging: some of the discovered knowledge is being integrated into the alarm handling software of telecommunication operators.

#index 461904
#* Title, General Chairs' Message, Program Chairs' Message, Committees, Reviewers, Author Index
#@ 
#t 1997
#c 17

#index 461905
#* Databases and the Web: What's in it for Databases? (Panel)
#@ Erich J. Neuhold;Karl Aberer
#t 1997
#c 17

#index 461906
#* Data Integration and Interrogation
#@ J. Verso
#t 1997
#c 17
#! Summary form only given. One major concern of the Verso group at Inria is the development of technology for data integration and interrogation, especially for non traditional data formats such as structured text. The article describes aspects of Verso's technology as partly sponsored by the European Community (AQUARELLE project, Esprit IV projects OPAL and WIRE).

#index 461907
#* Interfacing Parallel Applications and Parallel Databases
#@ Vibby Gottemukkala;Anant Jhingran;Sriram Padmanabhan
#t 1997
#c 17
#! The use of parallel database systems to deliver high performance has become quite common. Although queries submitted to these database systems are executed in parallel, the interaction between applications and current parallel database systems is serial. As the complexity of the applications and the amount of data they access increases, the need to parallelize applications also increases. In this parallel application environment, a serial interface to the database could become the bottleneck in the performance of the application. Hence, parallel database systems should support interfaces that allow the applications to interact with the database system in parallel. We present a taxonomy of such parallel interfaces, namely the Single Coordinator, Multiple Coordinator, Hybrid Parallel, and Pure Parallel interfaces. Furthermore, we discuss how each of these interfaces can be realized and in the process introduce new constructs that enable the implementation of the interfaces. We also qualitatively evaluate each of the interfaces with respect to their restrictiveness and performance impact.

#index 461908
#* Data Mining: Where is it Heading? (Panel)
#@ Jiawei Han
#t 1997
#c 17
#! Data mining is a promising field in which research and development activities are flourishing. It is also a young field with vast, unexplored territories. How can we contribute significantly to this fast expanding, multi-disciplinary field? This panel will bring database researchers together to share different views and insights on the issues in the field.

#index 461909
#* Clustering Association Rules
#@ Brian Lent;Arun N. Swami;Jennifer Widom
#t 1997
#c 17
#! The authors consider the problem of clustering two-dimensional association rules in large databases. They present a geometric-based algorithm, BitOp, for performing the clustering, embedded within an association rule clustering system, ARCS. Association rule clustering is useful when the user desires to segment the data. They measure the quality of the segmentation generated by ARCS using the minimum description length (MDL) principle of encoding the clusters on several databases including noise and errors. Scale-up experiments show that ARCS, using the BitOp algorithm, scales linearly with the amount of data.

#index 461910
#* Media Asset Management: Managing Complex Data as a Re-Engineering Exercise
#@ Peter DeVries
#t 1997
#c 17
#! Building a media asset management application involves the storing, searching, and retrieving of complex data. How this data is managed can be viewed from two perspectives-in terms of the internal representation required to allow for high-speed searching and transferring of these items between systems, but also from the end-user perspective. This paper focuses on the perspective that media asset management is a re-engineering exercise whose fundamental goal is to eliminate the file system and its underlying classification model. The paper discusses the following topics: folder/director classification schemes; file system security; cross platform file transfers; traditional searching techniques and constraints; the data characteristics of a media asset management solution; an architectural mapping of elements required to manage complex data, including source, proxy, and metadata; a media asset management approach to classification including business semantics; content based search algorithms; and the feasibility of a database replacing the file system.

#index 461911
#* New and Forgotten Dreams in Database Research (Panel)
#@ Surajit Chaudhuri;Rakesh Agrawal;Klaus R. Dittrich;Andreas Reuter;Abraham Silberschatz;Gerhard Weikum
#t 1997
#c 17

#index 461912
#* Similarity Based Retrieval of Videos
#@ A. Prasad Sistla;Clement T. Yu;R. Venkatasubrahmanian
#t 1997
#c 17

#index 461913
#* Buffer and I/O Resource Pre-allocation for Implementing Batching and Buffering Techniques for Video-on-Demand Systems
#@ M. Y. Y. Leung;John C. S. Lui;Leana Golubchik
#t 1997
#c 17
#! To design a cost effective VOD server, it is important to carefully manage the system resources so that the number of concurrent viewers can be maximized. Previous research results use data sharing techniques, such as batching, buffering and piggybacking, to reduce the demand for I/O resources in a VOD system. However, these techniques still suffer from the problem that additional I/O resources are needed in the system for providing VCR functionality --- without careful resource management, the benefits of these data sharing techniques can be lost. In this paper, we first introduce a model for determining the amount of resources required for supporting both normal playback and VCR functionality to satisfy predefined performance characteristics. Consequently, this model allows us to maximize the benefits of data sharing techniques. Furthermore, one important application of this model is its use in making system sizing decisions. Proper system sizing will result in a more cost-effective VOD system.

#index 461914
#* Tools to Enable Interoperation of Heterogeneous Databases
#@ Wernher Behrendt;N. J. Fiddian;Ajith P. Madurapperuma
#t 1997
#c 17
#! We demonstrate a prototype toolkit called ITSE (Integrated Translation Support Environment), for interoperation and migration of heterogeneous database systems. The main objective of ITSE is to enable transparency in heterogeneous database environments. Therefore, tools have been developed to support flexible configuration of databases and the wrapping or migrating of legacy systems in intranets. The tools themselves are aimed at MDBMS administrators and MIS analysts. These users can tailor the toolkit's operation so that end users are shielded from the underlying heterogeneity of their information system.

#index 461915
#* A Generic Query-Translation Framework for a Mediator Architecture
#@ Jacques Calmet;Sebastian Jekutsch;Joachim Schü
#t 1997
#c 17
#! A mediator is a domain-specific tool to support uniform access to multiple heterogeneous information sources and to abstract and combine data from different but related databases to gain new information. This middleware product is urgently needed for these frequently occurring tasks in a decision support environment. In order to provide a front end, a mediator usually defines a new language. If an application or a user submits a question to the mediator, it has to be decomposed into several queries to the underlying information sources. Since these sources can only be accessed using their own query language, a query translator is needed. This paper presents a new approach for implementing query translators. It supports conjunctive queries as well as negation. Care is taken to enable information sources of which processing capabilities do not allow conjunctive queries in general. Rapid implementation is guided by reusing previously prepared code. The specification of the translator is done declaratively and domain--independent.

#index 461916
#* Content is King, (If You Can Find It): A New Model for Knowledge Storage and Retrieval
#@ Fred L. Wurden
#t 1997
#c 17
#! The technology for acquiring and storing vast amounts of complex data is accelerating at a much faster rate than the technology for retrieving and analyzing that data. While progress has been made with OODB, OLAP, and knowledge discovery (KD) systems, users of these systems are still required to know and supply missing semantic information. When dealing with complex real-world representations, this is often nearly impossible to do. We discuss a new model that provides significant improvements in storing, correlating, and navigating information. We first provide a brief background looking at other relevant knowledge representation approaches, then describe our patented Contiguous Connection Model. Finally we discuss the impact this technology has had on a large, high-value, digital media knowledge base.

#index 461917
#* System Design for Digital Media Asset Management
#@ Pamela D. Fisher
#t 1997
#c 17
#! Client-server computing is only now able to deliver genuine gains to broadcasters, publishers, creative agencies and production facilities. These new users are entering the distributed computing domain just as media object technologies and commercial broadband services emerge from infancy. This paper discusses the complex process of decision-making and system design for digital media asset management. The Cinebase Digital Media Management System is described and used to illustrate critical points. The digital media server architecture must accommodate extremely large datasets, scaleable content retrieval and rapid network query response. Cinebase installations are currently in place containing 100s of Terabytes of media content, and 100s of local and remote users. Cinebase has recently been ported to ObjectStore by Object Design Inc. (ODI). The presentation discusses how, in combination, the Cinebase application and ODI extensions can be used to deliver a complete object management environment for production-quality content. Examples of the new workflows, and the technical issues complex networks raise for database architecture, are also discussed.

#index 461918
#* Selectivity Estimation in the Presence of Alphanumeric Correlations
#@ Min Wang;Jeffrey Scott Vitter;Balakrishna R. Iyer
#t 1997
#c 17
#! Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Almost all previous work dealt with the estimation of numeric selectivity, i.e., the query contains only numeric variables. The general problem of estimating alphanumeric selectivity is much more difficult and has attracted attention only very recently, and the focus has been on the special case when only one column is involved. In this paper, we consider the more general case when there are two correlated alphanumeric columns. We develop efficient algorithms to build storage structures that can fit in a database catalog. Results from our extensive experiments to test our algorithms, on the basis of error analysis and space requirements, are given to guide DBMS implementors.

#index 461919
#* On Incremental Cache Coherency Schemes in Mobile Computing Environments
#@ J. Cai;Kian-Lee Tan;Beng Chin Ooi
#t 1997
#c 17
#! Re-examines the cache coherency problem in a mobile computing environment in the context of relational operations (i.e. selection, projection and join). We propose a taxonomy of cache coherency schemes, and as case studies, we pick several schemes for further study. These schemes are novel in several ways. First, they are incremental. Second, they are an integration of (and built on) techniques in view maintenance in centralized systems and cache invalidation in client-server computing environments. We conducted extensive studies based on a simulation model. Our study shows the effectiveness of these algorithms in reducing uplink transmission and average access times. Moreover, the class of algorithms that exploit collaboration between the client and server performs best in most cases. We also study extended versions of this class of algorithms to further cut down on the work performed by the server.

#index 461920
#* A Data Model and Semantics of Objects with Dynamic Roles
#@ Raymond K. Wong;H. Lewis Chau;Frederick H. Lochovsky
#t 1997
#c 17
#! Although the concept of roles is becoming a popular research issue in object-oriented databases and has been proven to be useful for dynamic and evolving applications, it has only been described conceptually in most of the previous work. Moreover, the important issues such as the semantics of roles (e.g., message passing) are seldom discussed. Furthermore, none of the previous work has investigated the idea of role player qualification, which models the fact that not every object is qualified to play a particular role. In this paper, we present a data model and the semantics of roles. We discuss each of the above issues and illustrate the ideas with examples. From these examples, we can easily see that the problems we discussed are fundamental and indeed exist in many complex applications.

#index 461921
#* Modeling Multidimensional Databases
#@ Rakesh Agrawal;Ashish Gupta;Sunita Sarawagi
#t 1997
#c 17
#! We propose a data model and a few algebraic operations that provide semantic foundation to multidimensional databases. The distinguishing feature of the proposed model is the symmetric treatment not only of all dimensions but also measures. The model provides support for multiple hierarchies along each dimension and support for adhoc aggregates. The proposed operators are composable, reorderable, and closed in application. These operators are also minimal in the sense that none can be expressed in terms of others nor can any one be dropped without sacrificing functionality. They make possible the declarative specification and optimization of multidimensional database queries that are currently specified operationally. The operators have been designed to be translated to SQL and can be implemented either on top of a relational database system or within a special purpose multidimensional database engine. In effect, they provide an algebraic application programming interface (API) that allows the separation of the frontend from the backend. Finally, the proposed model provides a framework in which to study multidimensional databases and opens several new research problems.

#index 461922
#* Titan: A High-Performance Remote Sensing Database
#@ Chialin Chang;Bongki Moon;Anurag Acharya;Carter Shock;Alan Sussman;Joel H. Saltz
#t 1997
#c 17
#! There are two major challenges for a high performance remote sensing database. First, it must provide low latency retrieval of very large volumes of spatio temporal data. This requires effective declustering and placement of a multidimensional dataset onto a large disk farm. Second, the order of magnitude reduction in data size due to post processing makes it imperative, from a performance perspective, that the post processing be done on the machine that holds the data. This requires careful coordination of computation and data retrieval. The paper describes the design, implementation and evaluation of Titan, a parallel shared nothing database designed for handling remote sensing data. The computational platform for Titan is a 16 processor IBM SP-2 with four fast disks attached to each processor. Titan is currently operational and contains about 24 GB of AVHRR data from the NOAA-7 satellite. The experimental results show that Titan provides good performance for global queries and interactive response times for local queries.

#index 461923
#* Modeling and Querying Moving Objects
#@ A. Prasad Sistla;Ouri Wolfson;Sam Chamberlain;Son Dao
#t 1997
#c 17
#! In this paper we propose a data model for representing moving objects in database systems. It is called the Moving Objects Spatio-Temporal (MOST) data model. We also propose Future Temporal Logic (FTL) as the query language for the MOST model, and devise an algorithm for processing FTL queries in MOST.

#index 461924
#* Subquery Elimination: A Complete Unnesting Algorithm for an Extended Relational Algebra
#@ Pedro Celis;Hansjörg Zeller
#t 1997
#c 17
#! Summary form only given, as follows. Research in the area of subquery unnesting algorithms has mostly focused on the problem of making queries more efficient at run-time by transforming subqueries into joins. Unnesting rules describe a transformation of a nested query tree or a nested SQL query into an equivalent tree or SQL query that is no longer nested. However, it is not possible to express all nested queries in a non-nested form, unless the used language (relational algebra or ISO/ANSI SQL) is extended. This means that a database system must continue to have the ability to process subqueries. When working on a new optimizer and executor design for NonStop SQL, our development team was faced with a slightly different problem: we wanted to eliminate the need for optimization and execution of nested queries altogether and were looking for a complete subquery unnesting process. Such a process would allow us to develop a query optimizer and executor that do not need to process subqueries. Our goal was to make use of the existing unnesting algorithms and to extend them in a way that does not necessarily improve or change the execution characteristics of nested queries, but that leads to complete unnesting of all forms of nested queries, as defined by the "full" level of the ISO/ANSI SQL92 standard. To indicate this different approach we call it "subquery elimination" rather than "subquery unnesting".

#index 461925
#* Indexing OODB Instances based on Access Proximity
#@ Chee Yong Chan;Cheng Hian Goh;Beng Chin Ooi
#t 1997
#c 17
#! Queries in object-oriented databases (OODBs) may be asked with respect to different class scopes: a query may either request for object-instances which belong exclusively to a given class c, or those which belong to any class in the hierarchy rooted at c. To facilitate retrieval of objects both from a single class as well as from multiple classes in a class hierarchy, we propose a multi-dimensional class-hierarchy index called the /spl chi/-tree. The /spl chi/-tree dynamically partitions the data space using both the class and indexed attribute dimensions by taking into account the semantics of the class dimension as well as access patterns of queries. Experimental results show that it is an efficient index.

#index 461926
#* Failure Handling for Transaction Hierarchies
#@ Qiming Chen;Umeshwar Dayal
#t 1997
#c 17
#! Previously, failure recovery mechanisms have been developed separately for nested transactions and for transactional workflows specified as "flat" flow graphs. The paper develops unified techniques for complex business processes modeled as cooperative transaction hierarchies. Multiple cooperative transaction hierarchies often have operational dependencies, thus a failure occurring in one transaction hierarchy may need to be transferred to another. The existing transaction models do not support failure handling across transaction hierarchies. The authors introduce the notion of transaction execution history tree which allows one to develop a unified hierarchical failure recovery mechanism applicable to both nested and flat transaction structures. They also develop a cross-hierarchy undo mechanism for determining failure scopes and supporting backward and forward failure recovery over multiple transaction hierarchies. These mechanisms form a structured and unified approach for handling failures in flat transactional workflows, along a transaction hierarchy, and across transaction hierarchies.

#index 462042
#* Partial Video Sequence Caching Scheme for VOD Systems with Heterogeneous Clients
#@ Y. M. Chiu;K. H. Yeung
#t 1997
#c 17
#! Video on Demand is one of the key application in the information era. An hinge factor to its wide booming is the huge bandwidth required to transmit digitized video to a large group of clients with widely varying requirements. This paper addresses issues due to heterogeneous clients by proposing a program caching scheme called Partial Video Sequence (PVS) Caching Scheme. PVS Caching Scheme decomposes video sequences into a number of parts by using a scalable video compression algorithm. Video parts are selected to be cached in local video servers based on the amount of bandwidth it would be demanded from the distribution network and central video server if it is only kept in central video server. In this paper, we also show that PVS Caching Scheme is suitable for handling vastly varying client requirements.

#index 462043
#* A Cost-Model-Based Online Method for Ditributed Caching
#@ Markus Sinnwell;Gerhard Weikum
#t 1997
#c 17
#! The paper presents a method for distributed caching to exploit the aggregate memory of networks of workstations in data-intensive applications. In contrast to prior work, the approach is based on a detailed cost model as the basis for optimizing the placement of variable-size data objects in a distributed, possibly heterogeneous two-level storage hierarchy. To address the online problem with a priori unknown and evolving workload parameters, the method employs dynamic load tracking procedures and an approximative, low-overhead version of the cost model for continuous reoptimization steps that are embedded in the decisions of the underlying local cache managers. The method is able to automatically find a good tradeoff between an "egoistic" and an "altruistic" behavior of the network nodes, and proves its practical viability in a detailed simulation study under a variety of workload and system configurations.

#index 462044
#* Teaching an OLTP Database Kernel Advanced Data Warehousing Techniques
#@ Clark D. French
#t 1997
#c 17
#! Most, if not all, of the major commercial database products available today were written more than 10 years ago. Their internal designs have always been heavily optimized for OLTP applications. Over the last couple of years as DSS and data warehousing have become more important, database companies have attempted to increase their performance with DSS-type applications. Most of their attempts have been in the form of added features like parallel table scans and simple bitmap indexing techniques. These were chosen because they could be quickly implemented (1-2 years), giving some level of increased query performance. The paper contends that the real performance gains for the DSS application have not yet been realized. The performance gains for DSS will not come from parallel table scans, but from major changes to the low level database storage management used by OLTP systems. One Sybase product, Sybase-IQ has pioneered some of these new techniques. The paper discusses a few of these techniques and how they could be integrated into an existing OLTP database kernel.

#index 462045
#* ULIXES: Building Relational Views over the Web
#@ Paolo Atzeni;Alessandro Masci;Giansalvatore Mecca;Paolo Merialdo;Elena Tabet
#t 1997
#c 17
#! The authors consider structured Web sites, those sites in which structures are so tight and regular that one can assimilate the site, from the logical viewpoint, to a conventional database. They have argued that, with respect to structured Web servers, it is possible to apply ideas from traditional database techniques, specifically with respect to design, query, and update. They focus on the querying process, which consists of associating a scheme with a server and then using this scheme to pose queries in a high level query language. To describe the scheme, they use a specific data model, called the ARANEUS Data Model (ADM). They call ADM a page oriented model, in the sense that the main construct of the model is that of a page scheme, used to describe the structure of sets of homogeneous pages in the server. ADM schemes are then offered to the user, who can query them using the ULIXES language, whose expressions produce relations as results. These are essentially relational views over Web data and can therefore be queried using any relational query language. It should be noted that the approach inherited some ideas from other proposals for query languages for the Web. However, these approaches are mainly based on a loose notion of structure, and tend to view the Web as a huge collection of unstructured objects, organized as a graph. In contrast, the approach explicitly considers structure, both in the information source (the Web) and in the derived information (the relational views).

#index 462046
#* Oracle Parallel Warehouse Server
#@ Gary Hallmark
#t 1997
#c 17
#! Oracle is the leading supplier of data warehouse servers, yet little has been published about Oracle's parallel warehouse architecture. After a brief review of Oracle's market, performance, and platform strengths, we present two novel features of the Oracle parallel database architecture. First, the data flow model achieves scalability while using a fixed number of threads that is independent of the complexity of the query plan. Second, a new "load shipping" architecture combines the best aspects of data shipping and function shipping, and runs on shared everything, shared disk, and shared nothing hardware.

#index 462047
#* NAOS Protyotype - Version 2.2
#@ Christine Collet;Thierry Coupaye;Luc Fayolle;Claudia Roncancio
#t 1997
#c 17
#! Summary form only given. The Native Active Object System (NAOS) incorporates an active behavior within the object-oriented database management system O/sub 2/. NAOS rules are event-condition-action (EGA) rules belonging to an O/sub 2/ database schema. The authors focus on user and temporal event detection as well as composite event detection.

#index 462048
#* Semantic Dictionary Design for Database Interoperability
#@ Silvana Castano;Valeria De Antonellis
#t 1997
#c 17
#! Criteria and techniques to support the establishment of a semantic dictionary for database interoperability are described. The techniques allow the analysis of conceptual schemas of databases in a federation and the definition and maintenance of concept hierarchies. Similarity-based criteria are used to evaluate concept closeness and, consequently, to generate concept hierarchies. Experimentation of the techniques in the public administration domain is discussed.

#index 462049
#* FLORID: A Prototype for F-Logic
#@ Jürgen Frohn;Rainer Himmeröder;Paul-Thomas Kandzia;Georg Lausen;Christian Schlepphorst
#t 1997
#c 17
#! FLORID - F-LOgic Reasoning In Databases - is a deductive object-oriented database system incorporating F-logic as data definition and query language and combining the advantages of deductive databases with the rich modelling capabilities of object oriented concepts. F-logic provides complex objects, uniform handling of data and metadata, rule-defined class hierarchy and signatures, non-monotonic multiple inheritance, equating of objects by rules and variables ranging over methods and classes. Moreover, FLORID extends F-logic by path expressions to facilitate object navigation.

#index 462050
#* Developing and Accessing Scientific Databases with the OPM Data Management Tools
#@ I-Min A. Chen;Anthony Kosky;Victor M. Markowitz;Ernest Szeto
#t 1997
#c 17
#! Summary form only given. The Object-Protocol Model (OPM) data management tools provide facilities for rapid development, documentation, and flexible exploration of scientific databases. The tools are based on OPM, an object-oriented data model which is similar to the ODMG standard, but also supports extensions for modeling scientific data. Databases designed using OPM can be implemented using a variety of commercial relational DBMSs, using schema translation tools that generate complete DBMS database definitions from OPM schemas. Further, OPM schemas can be retrofitted on top of existing databases defined using a variety of notations, such as the relational data model or the ASN.1 data exchange format, using OPM retrofitting tools. Several archival molecular biology databases have been designed and implemented using the OPM tools, including the Genome Database (GDB) and the Protein Data Bank (PDB), while other scientific databases, such as the Genome Sequence Database (GSDB), have been retrofitted with semantically enhanced views using the OPM tools.

#index 462051
#* WOL: A Language for Database Transformations and Constraints
#@ Susan B. Davidson;Anthony Kosky
#t 1997
#c 17
#! The need to transform data between heterogeneous databases arises from a number of critical tasks in data management. These tasks are complicated by schema evolution in the underlying databases and by the presence of non-standard database constraints. We describe a declarative language called WOL (Well-founded Object Logic) for specifying such transformations, and its implementation in a system called Morphase (an "enzyme" for morphing data). WOL is designed to allow transformations between the complex data structures which arise in object-oriented databases as well as in complex relational databases, and to allow for reasoning about the interactions between database transformations and constraints.

#index 462052
#* A Priority Ceiling Protocol with Dynamic Adjustment of Serialization Order
#@ Kwok-Wa Lam;Sang Hyuk Son;Sheung-lun Hung
#t 1997
#c 17
#! The difficulties of providing a guarantee of meeting transaction deadlines in hard real-time database systems lie in the problems of priority inversion and of deadlocks. Priority inversion and deadlock problems ensue when concurrency control protocols are adapted in priority-driven scheduling. The blocking delay due to priority inversion can be unbounded, which is unacceptable in the mission-critical real-time applications. Some priority ceiling protocols have been proposed to tackle these two problems. However, they are too conservative in scheduling transactions for the single-blocking and deadlock-free properties, leading to many unnecessary transaction blockings. In this paper, we analyze the unnecessary transaction blocking problem inherent in these priority ceiling protocols and investigate the conditions for allowing a higher priority transaction to preempt a lower priority transaction using the notion of dynamic adjustment of serialization order. A new priority ceiling protocol is proposed to solve the unnecessary blocking problem, thus enhancing schedulability. We also devise the worst-case schedulability analysis for the new protocol which provides a better schedulability condition than other protocols.

#index 462053
#* A Propagation Mechanism for Populated Schema Versions
#@ Sven-Eric Lautemann
#t 1997
#c 17
#! Object-oriented database systems (OODBMS) offer powerful modeling concepts as required by advanced application domains like CAD/CAM/CAE or office automation. Typical applications have to handle large and complex structured objects which frequently change their value and their structure. As the structure is described in the schema of the database, support for schema evolution is a highly required feature. Therefore, a set of schema update primitives must be provided which can be used to perform the required changes, even in the presence of populated databases and running applications. In this paper, we use the versioning approach to schema evolution to support schema updates as a complex design task. The presented propagation mechanism is based on conversion functions that map objects between different types and can be used to support schema evolution and schema integration.

#index 462054
#* Modeling Business Rules with Situation/Activation Diagrams
#@ Peter Lang;Werner Obermair;Michael Schrefl
#t 1997
#c 17
#! Business rules are statements about business policies and can be formulated according to the event-condition-action structure of rules in active database systems. However, modeling business rules at the conceptual level from an external user's perspective requires a different set of concepts than currently provided by active database systems. This paper identifies requirements on the event language and on the semantics of rule execution for modeling business rules and presents a graphical object-oriented language, called Situation/Activation diagrams, meeting these requirements.

#index 462055
#* Graphical Tools for Rule Development in the Active DBMS SAMOS
#@ Anca Vaduva;Stella Gatziu;Klaus R. Dittrich
#t 1997
#c 17
#! Summary form only given. Active database management systems (active DBMS) support the definition, management and execution of event/condition/action rules specifying reactive application behavior. Although the advantages of active mechanisms are nowadays well known, there is still no wide use in practice. One main problem is that especially for large rule sets, defined by different persons at different points in time, potential conflicts and dependencies between rules are hard to predict and rule behavior is difficult to control. Therefore, tools are needed to assist the development and maintenance of rule bases. These tools should provide for graphical interfaces supporting both, "static" activities (performed during rule specification) such as rule editing, browsing, design, rule analysis, and "dynamic" activities (performed at runtime, during the execution of an application) such as testing, debugging and understanding of rule behavior. The aim of the article is to show the use of three of these tools, namely the rule editor, the browser and the termination analyzer in the process of developing applications for the active object oriented DBMS SAMOS.

#index 462056
#* Object Related Plus: A Practical Tool for Developing Enhanced Object Databases
#@ Bryon K. Ehlmann;Gregory A. Riccardi
#t 1997
#c 17
#! Object Relater Plus is a practical tool currently being used for research and development of enhanced object databases (ODBs). The tool, which is a prototype Object Database Management System (ODBMS), provides two languages that are compatible with the ODMG-93 ODBMS standard yet enhance it in some significant ways. The Object Database Definition Language (ODDL) allows object relationships to be better defined and supported; provides for the specification and separation of external, conceptual, and internal views; and facilitates the implementation of domain specific ODB extensions. The Object Database Manipulation Language (ODML) augments ODDL by providing a C++ interface for database creation, access, and manipulation based on an ODDL specification. In this paper we give an overview of Object Relater Plus, emphasizing its salient features. We also briefly discuss its architecture and implementation and its use in developing scientific databases.

#index 462057
#* Integrated Query Processing Strategies for Spatial Path Queries
#@ Yun-Wu Huang;Ning Jing;Elke A. Rundensteiner
#t 1997
#c 17
#! We investigate optimization strategies for processing path queries with embedded spatial constraints, such as avoiding areas with certain characteristics. To resolve complex spatial constraints during path finding, we consider two decisions: (1) the spatial relation operations (e.g., intersect) between areas and links can be preprocessed or intermixed with path finding and (2) areas satisfying the query constraint can be prefiltered or dynamically selected during path finding. Based on these two decisions, we propose and implement the resulting four integrated query processing strategies, utilizing state-of-the-art technologies such as spatial joins for intersect computation, R-tree access structure for spatial overlap search, and spatial clustering for efficient path search. In this paper, we also report an experimental evaluation to show which strategies perform best in different scenarios.

#index 462058
#* A Rule Engine for Query Transformation in Starburst and IBM DB2 C/S DBMS
#@ Hamid Pirahesh;T. Y. Cliff Leung;Waqar Hasan
#t 1997
#c 17
#! The complexity of queries in relational DBMSs is increasing, particularly in the decision support area and interactive client sewer environments. This calls for a more powerful and flexible optimization of complex queries. H. Pirahesh et al. (1992) introduced query rewrite as a distinct query optimization phase mainly targeted to responding to this requirement. This approach has enabled us to extensively enrich the optimization rules in our system. Further, it has made it easier to incrementally enrich and adapt the system as need arises. Examples of such query optimizations are predicate pushdown, subquery and magic sets transformations, and decorrelating subquery. We describe the design and implementation of a rule engine for query rewrite optimization. Each transformation is implemented as a rule which consists of a pair of rule condition and action. Rules can be grouped into rule classes for higher efficiency, better understandability and more extensibility. The rule engine has a number of novelties in that it supports a full spectrum of control-from totally data driven to totally procedural. Furthermore, it incorporates a budget control scheme for controlling the resources taken for query optimization as well as guaranteeing the termination of rule execution. The rule engine and a suite of query rewrite rules have been implemented in Starburst relational DBMS prototype and a significant portion of this technology has been integrated into IBM DB2 Common Server relational DBMS.

#index 462059
#* STR: A Simple and Efficient Algorithm for R-Tree Packing
#@ Scott T. Leutenegger;J. M. Edgington;Mario A. Lopez
#t 1997
#c 17
#! In this paper we present the results from an extensive comparison study of three R-tree packing algorithms, including a new easy to implement algorithm. The algorithms are evaluated using both synthetic and actual data from various application domains including VLSI design, GIS (tiger), and computational fluid dynamics. Our studies also consider the impact that various degrees of buffering have on query performance. Experimental results indicate that none of the algorithms is best for all types of data. In general, our new algorithm requires up to 50\% fewer disk accesses than the best previously proposed algorithm for point and region queries on uniformly distributed or mildly skewed point and region data, and approximately the same for highly skewed point and region data.

#index 462060
#* Semantic Query Optimization for Object Databases
#@ John Grant;Jarek Gryz;Jack Minker;Louiqa Raschid
#t 1997
#c 17

#index 462061
#* Improving the Quality of Technical Data for Developing Case Based Reasoning Diagnostic Software for Aircraft Maintenance
#@ Richard Heider
#t 1997
#c 17
#! Summary form only given. Time spent by airline maintenance operators to solve engine failures and the related costs (flight delays or cancellations) are a major concern to SNECMA which manufacture engines for civilian aircraft such as BOEING 737s and Airbus A340s. The use of an intelligent diagnostic software contributes to improving customer support and reduces the cost of ownership by improving troubleshooting accuracy and reducing airplane downtime. However, classical rule based or model based expert systems are costly to develop and maintain. Our goal has been to improve the development of troubleshooting systems through case based reasoning (CBR) and data mining. These technologies reason from past cases, whose solution is known, rather than rules. New problems are solved by searching for similar problem solving experiences and by adapting the solutions that worked in the past, Our second objective was to acquire the capacity to produce systems which match the quality standard in the aeronautic industry in the given time frame. We aim at both assuring the quality of the core data mining and CBR software as well as the quality of the technical information that is fed into the system (case knowledge).

#index 462062
#* Representative Objects: Concise Representations of Semistructured, Hierarchial Data
#@ Svetlozar Nestorov;Jeffrey D. Ullman;Janet L. Wiener;Sudarshan S. Chawathe
#t 1997
#c 17
#! Introduces the concept of representative objects, which uncover the inherent schema(s) in semi-structured, hierarchical data sources and provide a concise description of the structure of the data. Semi-structured data, unlike data stored in typical relational or object-oriented databases, does not have a fixed schema that is known in advance and stored separately from the data. With the rapid growth of the World Wide Web, semi-structured hierarchical data sources are becoming widely available to the casual user. The lack of external schema information currently makes browsing and querying these data sources inefficient at best, and impossible at worst. We show how representative objects make schema discovery efficient and facilitate the generation of meaningful queries over the data.

#index 462063
#* A Persistent Hyper-Programming System
#@ Graham N. C. Kirby;Ronald Morrison;David S. Munro;Richard C. H. Connor;Quintin I. Cutts
#t 1997
#c 17
#! We demonstrate the use of a hyper-programming system in building persistent applications. This allows program representations to contain type-safe links to persistent objects embedded directly within the source code. The benefits include improved efficiency and potential for static program checking, reduced programming effort and the ability to display meaningful source-level representations for first-class procedure values. Hyper-programming represents a completely new style of programming which is only possible in a persistent programming system.

#index 462064
#* Designing the Reengineering Services for the DOK Federated Database System
#@ Zahir Tari;John Stokes
#t 1997
#c 17
#! Addresses the design of the reengineering service for the DOK (Distributed Object Kernel) federated database. This service allows the hiding of the heterogeneity of databases involved in a federation by generating object-oriented representations from their corresponding schemata. We propose a complete methodology that supports the identification and the translation of both the explicit and implicit information. The identification of object-oriented constructs is performed by classifying a relational schema into different categories of relations, namely base, dependent and composite relations. The main difficulty in designing the reengineering service relies on the distinction between the different types of relationships amongst classes. Our approach deals with this problem by analysing relations according two types of correlation: (i) the degree of correlation between the external and primary keys, and (ii) the degree of correlation between sets of tuples in the relations. Examining these correlations uncovers implicit relationships contained as well-hidden classes in a relational schema.

#index 462065
#* The Constraint-Based Knowledge Broker System
#@ Jean-Marc Andreoli;Uwe M. Borghoff;Pierre-Yves Chevalier;Boris Chidlovskii;Remo Pareschi;Jutta Willamowski
#t 1997
#c 17
#! The amount of information available from electronic sources on the World Wide Web and other on-line information repositories is highly heterogeneous and increases dramatically. Tools are needed to extract relevant information from these repositories. The Constraint-Based Knowledge Brokers project (CBKB) at RXRC Grenoble realizes sophisticated facilities for efficient information retrieval, schema integration, and knowledge fusion. The current implementation of the CBKB research prototype involves three kinds of agents: a) users, who input queries and process answers (i.e., ranking, fusion) through a GUI; b) wrappers, capable of interrogating heterogeneous information sources, which can provide answers to elementary queries (essentially various public bibliographic catalogues available on the Web, as well as preprint archives and opera information repositories); c) brokers, which can manage complex queries (i.e., decompose a complex query, recompose the partial answers, synthesize a full answer) and which mediate between the GUI and the different wrappers.

#index 462066
#* ODB-QOPTIMIZER: A Tool for Semantic Query Optimization in OODB
#@ Sonia Bergamaschi;Domenico Beneventano;Claudio Sartori;Maurizio Vincini
#t 1997
#c 17
#! ODB-QOptimizer is a ODMG 93 compliant tool for the schema validation and semantic query optimization. The approach is based on two fundamental ingredients. The first one is the OCDL description logics (DLs) proposed as a common formalism to express class descriptions, a relevant set of integrity constraints rules (IC rules) and queries. The second one are DLs inference techniques, exploited to evaluate the logical implications expressed by IC rules and thus to produce the semantic expansion of a given query.

#index 462067
#* Active Customization of GIS User Interfaces
#@ Juliano Lopes de Oliveira;Claudia Bauzer Medeiros;Mariano Cilia
#t 1997
#c 17
#! This paper presents a new approach to user interface customization in Geographic Information Systems (GIS). This approach is based on the integration of three main components: a GIS user interface architecture; an active database mechanism; and a generic interface builder. The GIS interface architecture provides the default interface behavior, while the active system allows customization of interfaces according to the specific context. The generic interface builder relies on a library of interface objects to dynamically construct generic and customized interfaces. The main advantage of this approach is that it decreases the costs associated with developing customized \gis\ interfaces.

#index 462068
#* Distributing Semantic Constraints Between Heterogeneous Databases
#@ Stefan Grufman;Fredrik Samson;Suzanne M. Embury;Peter M. D. Gray;Tore Risch
#t 1997
#c 17
#! In recent years, research on distributing databases over networks has become increasingly important. In this paper, we concentrate on the issues of the interoperability of heterogeneous DBMSs and enforcing integrity across a multi-database made in this fashion. This has been done through a cooperative project between Aberdeen and Linko/spl uml/ping universities, with database modules distributed between the sites. In the process, we have shown the advantage of using DBMSs based on variants of the functional data model (FDM), which has made it remarkably straightforward to interoperate queries and schema definitions. Further, we have used the constraint transformation facilities of P/FDM (Prolog implementation of FDM) to compile global constraints into active rules installed locally on one or more AMOS (Active Mediators Object System) servers. We present the theory behind this, and the conditions for it to improve performance.

#index 462069
#* Data Warehousing: Dealing with the Growing Pains
#@ Robert Armstrong
#t 1997
#c 17
#! A data warehouse provides a customer with information to run and plan their business. It is true that if the data warehouse can not quickly adapt to changes in the environment then the company will lose the advantage that information provides. A warehouse must be built with a solid foundation that is flexible and responsive to business changes. The purpose of this paper is to share experiences in the area of managing the growth within the data warehouse. There are many technical issues that need to be addressed as the data warehouse grows in multiple dimensions. The ideas in this paper should enable you to provide the correct foundation for a long term warehouse. Very few companies are discussing these issues and the lack of discussion leads to a lack of knowledge that will further lead to poor architectural choices. This paper will articulate not only the benefits that are derived from data warehousing today but how to prepare to reap benefits for many tomorrow s. It will also explore the questions to ask, the points to make, and the issues to be addressed to have a long term successful data warehouse project.

#index 462070
#* High-Dimensional Similarity Joins
#@ Kyuseok Shim;Ramakrishnan Srikant;Rakesh Agrawal
#t 1997
#c 17
#! Many emerging data mining applications require a similarity join between points in a high-dimensional domain. We present a new algorithm that utilizes a new index structure, called the epsilon-kdB tree, for fast spatial similarity joins on high-dimensional points. This index structure reduces the number of neighboring leaf nodes that are considered for the join test, as well as the traversal cost of finding appropriate branches in the internal nodes. The storage cost for internal nodes is independent of the number of dimensions. Hence the proposed index structure scales to high-dimensional data. Empirical evaluation, using synthetic and real-life datasets, shows that similarity join using the epsilon-kdB tree is 2 to an order of magnitude faster than the R+ tree, with the performance gap increasing with the number of dimensions.

#index 462071
#* The IDEA Tool Set
#@ Stefano Ceri;Piero Fraternali;Stefano Paraboschi
#t 1997
#c 17

#index 462072
#* Supporting Fine-grained Data Lineage in a Database Visualization Environment
#@ Allison Woodruff;Michael Stonebraker
#t 1997
#c 17
#! The lineage of a datum records its processing history. Because such information can be used to trace the source of anomalies and errors in processed data sets, it is valuable to users for a variety of applications, including the investigation of anomalies and debugging. Traditional data lineage approaches rely on metadata. However, metadata does not scale well to fine-grained lineage, especially in large data sets. For example, it is not feasible to store all of the information that is necessary to trace from a specific floating-point value in a processed data set to a particular satellite image pixel in a source data set. In this paper, we propose a novel method to support fine-grained data lineage. Rather than relying on metadata, our approach lazily computes the lineage using a limited amount of information about the processing operators and the base data. We introduce the notions of weak inversion and verification. While our system does not perfectly invert the data, it uses weak inversion and verification to provide a number of guarantees about the lineage it generates. We propose a design for the implementation of weak inversion and verification in an object-relational database management system.

#index 462073
#* An Argument in Favour of Presumed Commit Protocol
#@ Yousef J. Al-Houmaily;Panos K. Chrysanthis;Steven P. Levitan
#t 1997
#c 17
#! We argue in favor of the presumed commit protocol by proposing two new presumed commit variants that significantly reduce the cost of logging activities associated with the original presumed commit protocol. Furthermore, for read-only transactions, we apply our unsolicited update-vote optimization and show that the cost associated with this type of transactions is the same in both presumed commit and presumed abort protocols, thus, nullifying the basis for the argument that favors the presumed abort protocol. This is especially important for modern distributed environments which are characterized by high reliability and high probability of transactions being committed rather than aborted.

#index 462074
#* Multiple View Consistency for Data Warehousing
#@ Yue Zhuge;Hector Garcia-Molina;Janet L. Wiener
#t 1997
#c 17
#! A data warehouse stores integrated information from multiple distributed data sources. In effect, the warehouse stores materialized views over the source data. The problem of ensuring data consistency at the warehouse can be divided into two components: ensuring that each view reflects a consistent state of the base data, and ensuring that multiple views are mutually consistent. In this paper we study the latter problem, that of guaranteeing multiple view consistency (MVC). We identify and define formally three layers of consistency for materialized views in a distributed environment. We present a scalable architecture for consistently handling multiple views in a data warehouse, which we have implemented in the WHIPS(WareHousing Information Project at Stanford) prototype. Finally, we develop simple, scalable, algorithms for achieving MVC at a warehouse.

#index 462075
#* Periodic Retrieval of Videos from Disk Arrays
#@ Banu Özden;Rajeev Rastogi;Abraham Silberschatz
#t 1997
#c 17
#! A growing number of applications need access to video data stored in digital form on secondary storage devices (e.g., video-on-demand, multimedia messaging). As a result, video servers that are responsible for the storage and retrieval, at fixed rates, of hundreds of videos from disks are becoming increasingly important. Since video data tends to be voluminous, several disks are usually used in order to store the videos. A challenge is to devise schemes for the storage and retrieval of videos that distribute the workload evenly across disks, reduce the cost of the server and at the same time, provide good response times to client requests for video data. In this paper, we present schemes that retrieve videos periodically from disks in order to provide better response times to client requests. We present two schemes that stripe videos across multiple disks in order to distribute the workload uniformly among them. For the two striping schemes, we show that the problem of retrieving videos periodically is equivalent to that of scheduling periodic tasks on a multiprocessor. For the multiprocessor scheduling problems, we present and compare schemes for computing start times for the tasks, if it is determined that they are scheduleable.

#index 462076
#* Adding Full Text Indexing to the Operating System
#@ Kyle Peltonen
#t 1997
#c 17
#! Many challenges must be faced when incorporating full text retrieval into the operating system. The search engine must be a nearly invisible, natural extension to the operating system, just like the file system and the network. The search engine must meet user expectations of an operating system, specifically in areas such as performance, fault tolerance, and security. It must handle a very heterogeneous collection of documents, in many formats, many languages and many styles. The search engine must scale with the operating system, from small laptop computers to large multiprocessor servers. The paper is an overview of the challenges faced when incorporating full text indexing into the Microsoft Windows NT/sup TM/ operating system. Specific solutions used by the Microsoft 'Tripoli' search engine, are offered.

#index 462077
#* Adaptive Broadcast Protocols to Support Power Conservant Retrieval by Mobile Users
#@ Anindya Datta;Aslihan Celik;Jeong G. Kim;Debra E. VanderMeer;Vijay Kumar
#t 1997
#c 17
#! Mobile computing has the potential for managing information globally. Data management issues in mobile computing have received some attention in recent times, and the design of adaptive broadcast protocols has been posed as an important problem. Such protocols are employed by database servers to decide on the content of broadcasts dynamically, in response to client mobility and demand patterns. In this paper we design such protocols and also propose efficient retrieval strategies that may be employed by clients to download information from broadcasts. The goal is to design cooperative strategies between server and client to provide access to information in such a way as to minimize energy expenditure by clients. We evaluate the performance of our protocols analytically.

#index 462078
#* The Multikey Type Index for Persistent Object Sets
#@ Thomas A. Mück;Martin L. Polaschek
#t 1997
#c 17
#! Multikey index structures for type hierarchies are a recently discussed alternative to traditional B/sup +/-tree indexing schemes. We describe an efficient implementation of this alternative called the multikey type index (MT-index). A prerequisite for our approach is an optimal linearization of the type hierarchy that allows us to map queries in object type hierarchies to minimal-volume range queries in multi-attribute search structures. This provides access to an already-existing large and versatile tool-box. The outline of an index implementation by means of a multi-attribute search structure (e.g. the hB-tree or any other structure with comparable performance) is followed by an analytical performance evaluation. Selected performance figures are compared to previous approaches, in particular to the H-tree and the class hierarchy tree. The comparison results allow for practically relevant conclusions with respect to index selection based on query profiles.

#index 462079
#* Physical Database Design for Data Warehouses
#@ Wilburt Labio;Dallan Quass;Brad Adelberg
#t 1997
#c 17
#! Data warehouses collect copies of information from remote sources into a single database. Since the remote data is cached at the warehouse, it appears as local relations to the users of the warehouse. To improve query response time, the warehouse administrator will often materialize views defined on the local relations to support common or complicated queries. Unfortunately, the requirement to keep the views consistent with the local relations creates additional overhead when the remote sources change. The warehouse is often kept only loosely consistent with the sources: it is periodically refreshed with changes sent from the source. When this happens, the warehouse is taken off-line until the local relations and materialized views can be updated. Clearly, the users would prefer as little down time as possible. Often the down time can be reduced by adding carefully selected materialized views or indexes to the physical schema. This paper studies how to select the sets of supporting views and of indexes to materialize to minimize the down time. We call this the view index selection (VIS) problem. We present an A* search based solution to the problem as well as rules of thumb. We also perform additional experiments to understand the space-time tradeoff as it applies to data warehouses.

#index 462080
#* Performance Evaluation of Rule Semantics in Active Databases
#@ Elena Baralis;Andrea Bianco
#t 1997
#c 17
#! Different rule execution semantics may be available in the same active database system. We performe several simulation experiments to evaluate the performance trade-offs yielded by different execution semantics in various operating conditions. In particular, we evaluate the effect of executing transaction and rule statements that affect a varying number of data instances, and applications with different rule triggering breadth and depth. Since references to data changed by the database operation triggering the rules are commonly used in active rule programming, we also analyze the impact of its management on overall performance.

#index 462081
#* Scalable Versioning in Distributed Databases with Commuting Updates
#@ H. V. Jagadish;Inderpal Singh Mumick;Michael Rabinovich
#t 1997
#c 17
#! We present a multiversioning scheme for a distributed system with the workload consisting of read-only transactions and update transactions, (most of) which commute on individual nodes. The scheme introduces a version advancement protocol that is completely asynchronous with user transactions, thus allowing the system to scale to very high transaction rates and frequent version advancements. Moreover, the scheme never creates more than three copies of a data item. Combined with existing techniques to avoid global concurrency control for commuting transactions that execute in a particular version, our multiversioning scheme results in a protocol where no user transaction on a node can be delayed by any activity (either version advancement or another transaction) occurring on another node. Non-commuting transactions are gracefully handled. Our technique is of particular value to distributed recording systems where guaranteeing global serializability is often desirable, but rarely used because of the high performance cost of running distributed transactions. Examples include calls on a telephone network, inventory management in a "point-of-sale'' system, operations monitoring systems in automated factories, and medical information management systems.

#index 462082
#* Delegation: Efficiently Rewriting History
#@ Cris Pedregal Martin;Krithi Ramamritham
#t 1997
#c 17
#! Transaction delegation, as introduced in ACTA, allows a transaction to transfer responsibility for the operations that it has performed on an object to another transaction. Delegation can be used to broaden the visibility of the delegatee, and to tailor the recovery properties of a transaction model. Delegation has been shown to be useful in synthesizing advanced transaction models. With an efficient implementation of delegation it becomes practicable to realize various advanced transaction models whose requirements are specified at a high level language instead of the current expensive practice of building them from scratch. The authors identify the issues in efficiently supporting delegation and hence advanced transaction models, and illustrate this with our solution in ARIES, an industrial-quality system that uses UNDO/REDO recovery. Since delegation is tantamount to rewriting history, a naive implementation can entail frequent, costly log accesses, and can result in complicated recovery protocols. The algorithm achieves the effect of rewriting history without rewriting the log, resulting in an implementation that realizes the semantics of delegation at minimal additional overhead and incurs no overhead when delegation is not used. The work indicates that it is feasible to build efficient and robust, general-purpose machinery for advanced transaction models. It is also a step towards making recovery a first-class concept within advanced transaction models.

#index 462083
#* The WHIPS Prototype for Data Warehouse Creation and Maintenance
#@ Janet L. Wiener;Himanshu Gupta;Wilburt Labio;Yue Zhuge;Hector Garcia-Molina
#t 1997
#c 17
#! Summary form only given. The goal of the Whips project (WareHousing Information Project at Stanford) is to develop algorithms and tools for the creation and maintenance of a data warehouse (J. Wiener et al., 1996). In particular, we have developed an architecture and implemented a prototype for identifying data changes at distributed heterogeneous sources, transforming them and summarizing them in accordance with warehouse specifications, and incrementally integrating them into the warehouse. In effect, the warehouse stores materialized views of the source data. The Whips architecture is designed specifically to fulfil several important and interrelated goals: sources and warehouse views can be added and removed dynamically; it is scalable by adding more internal modules; changes at the sources are detected automatically; the warehouse may be updated continuously as the sources change, without requiring down time; and the warehouse is always kept consistent with the source data by the integration algorithms. The Whips system is composed of many distinct modules that potentially reside on different machines. Each module is implemented as a CORBA object. They communicate with each other using ILU, a COBRA compliant object library developed by Xerox PARC.

#index 462201
#* The CORD Appraoch to Extensible Concurrency Control
#@ George T. Heineman;Gail E. Kaiser
#t 1997
#c 17
#! Database management systems (DBMSs) have been increasingly used for advanced application domains, such as software development environments, workflow management systems, computer-aided design and manufacturing, and managed healthcare. In these domains, the standard correctness model of serializability is often too restrictive. We introduce the notion of a Concurrency Control Language (CCL) that allows a database application designer to specify concurrency control policies to tailor the behavior of a transaction manager. A well-crafted set of policies defines an extended transaction model. The necessary semantic information required by the CCL run-time engine is extracted from a task manager, a (logical) module by definition included in all advanced applications. This module stores task models that encode the semantic information about the transactions submitted to the DBMS. We have designed a rule-based CCL, called CORD, and have implemented a run-time engine that can be hooked to a conventional transaction manager to implement the sophisticated concurrency control required by advanced database applications. We present an architecture for systems based on CORD and describe how we integrated the CORD engine with the Exodus Storage Manager to implement Altruistic Locking.

#index 462202
#* Quantifying Complexity and Performance Gains of Distributed Caching in a Wireless Mobile Computing Environment
#@ Cedric C. F. Fong;John C. S. Lui;Man Hon Wong
#t 1997
#c 17
#! In a mobile computing system, the wireless communication bandwidth is a scarce resource that needs to be managed carefully. In this paper, we investigate the use of distributed caching as an approach to reduce the wireless bandwidth consumption for data access. We find that conventional caching techniques cannot fully utilize the dissemination feature of the wireless channel. We thus propose a novel distributed caching protocol that can minimize the overall system bandwidth consumption at the cost of CPU processing time at the server side. This protocol allows the server to select data items into a broadcast set, based on a performance gain parameter called the bandwidth gain, and then send the broadcast set to all the mobile computers within the server's cell. We show that in general, this selection process is NP-hard, and therefore we propose a heuristic algorithm that can attain a near-optimal performance. We also propose an analytical model for the protocol and derive closed-form performance measures, such as the bandwidth utilization and the expected response time of data access by mobile computers. Experiments show that our distributed caching protocol can greatly reduce the bandwidth consumption so that the wireless network environment can accommodate more users and, at the same time, vastly improve the expected response time for data access by mobile computers.

#index 462203
#* Memory Management for Scalable Web Data Servers
#@ Shivakumar Venkataraman;Miron Livny;Jeffrey F. Naughton
#t 1997
#c 17
#! Popular web sites are already experiencing very heavy loads, and these loads will only increase as the number of users accessing them grows. These loads create both CPU and I/O bottlenecks. One promising solution already being employed to eliminate the CPU bottleneck is to replace a single processor server with a cluster of servers. Our goal in this paper is to develop buffer management algorithms that exploit the aggregate memory capacity of the machines in such a server cluster to attack the I/O bottleneck. The key challenge in designing such buffer management algorithms turns out to be controlling data replication so as to achieve a good balance between intra-cluster network traffic and disk I/O. At one extreme, the straightforward application of client-server memory management techniques to this cluster architecture causes duplication in memory among the servers and this tends to reduce network traffic but increases disk I/O, whereas at the other extreme, eliminating all duplicates tends to increase network traffic while reducing disk I/O. Accordingly, we present a new algorithm, Hybrid, that dynamically controls the amount of duplication. Through a detailed simulation, we show that on workloads characteristic of those experienced by Web servers, the Hybrid algorithm correctly trades off intra-cluster network traffic and disk I/O to minimize average response time.

#index 462204
#* Index Selection for OLAP
#@ Himanshu Gupta;Venky Harinarayan;Anand Rajaraman;Jeffrey D. Ullman
#t 1997
#c 17
#! On-line analytical processing (OLAP) is a recent and important application of database systems. Typically, OLAP data is presented as a multidimensional "data cube." OLAP queries are complex and can take many hours or even days to run, if executed directly on the raw data. The most common method of reducing execution time is to precompute some of the queries into summary tables (subcubes of the data cube) and then to build indexes on these summary tables. In most commercial OLAP systems today, the summary tables that are to be precomputed are picked first, followed by the selection of the appropriate indexes on them. A trial-and-error approach is used to divide the space available between the summary tables and the indexes. This two-step process can perform very poorly. Since both summary tables and indexes consume the same resource-space-their selection should be done together for the most efficient use of space. The authors give algorithms that automate the selection of summary tables and indexes. In particular, they present a family of algorithms of increasing time complexities, and prove strong performance bounds for them. The algorithms with higher complexities have better performance bounds. However, the increase in the performance bound is diminishing, and they show that an algorithm of moderate complexity can perform fairly close to the optimal.

#index 462205
#* General Chair's Message, Program Co-Chairs' Message, Committees, Reviewers, Author Index
#@ 
#t 1998
#c 17

#index 462206
#* An Extended Object-Oriented Database Approach to Networked Multimedia Applications
#@ Hiroshi Ishikawa;Koti Kato;Miyoki Ono;Naomi Yoshizawa;Kazumi Kubota;Akiko Kanaya
#t 1998
#c 17

#index 462207
#* Generalizing "Search'' in Generalized Search Trees (Extended Abstract)
#@ Paul M. Aoki
#t 1998
#c 17

#index 462208
#* Array-Based Evaluation of Multi-Dimensional Queries in Object-Relational Databases Systems
#@ Yihong Zhao;Karthikeyan Ramasamy;Kristin Tufte;Jeffrey F. Naughton
#t 1998
#c 17

#index 462209
#* Junglee: Integrating Data of All Shapes and Sizes
#@ Ashish Gupta
#t 1998
#c 17

#index 462210
#* Future Directions in Database Research (Panel)
#@ Surajit Chaudhuri;Hector Garcia-Molina;Henry F. Korth;Guy M. Lohman;David B. Lomet;David Maier
#t 1998
#c 17

#index 462211
#* Coarse Indices for a Tape-Based Data Warehouse
#@ Theodore Johnson
#t 1998
#c 17

#index 462212
#* Representing and Querying Changes in Semistructured Data
#@ Sudarshan S. Chawathe;Serge Abiteboul;Jennifer Widom
#t 1998
#c 17

#index 462213
#* Graph Structured Views and Their Incremental Maintenance
#@ Yue Zhuge;Hector Garcia-Molina
#t 1998
#c 17

#index 462214
#* Query Folding with Inclusion Dependencies
#@ Jarek Gryz
#t 1998
#c 17

#index 462215
#* SEMCOG: A Hybrid Object-based Image Database System and Its Modeling, Language, and Query Processing
#@ Wen-Syan Li;K. Selçuk Candan
#t 1998
#c 17

#index 462216
#* Content-based Multimedia Information Management
#@ Ramesh Jain
#t 1998
#c 17

#index 462217
#* Encoded Bitmap Indexing for Data Warehouses
#@ Ming-Chuan Wu;Alejandro P. Buchmann
#t 1998
#c 17

#index 462218
#* The Effect of Buffering on the Performance of R-Trees
#@ Scott T. Leutenegger;Mario A. Lopez
#t 1998
#c 17

#index 462219
#* Mining Association Rules: Anti-Skew Algorithms
#@ Jun-Lin Lin;Margaret H. Dunham
#t 1998
#c 17

#index 462220
#* Methodical Restructuring of Complex Workflow Activities
#@ Ling Liu;Calton Pu
#t 1998
#c 17

#index 462221
#* Messaging/Queuing in Oracle8
#@ Dieter Gawlick
#t 1998
#c 17

#index 462222
#* Safeguarding and Charging for Information on the Internet
#@ Hector Garcia-Molina;Steven P. Ketchpel;Narayanan Shivakumar
#t 1998
#c 17

#index 462223
#* Fuzzy Triggers: Incorporating Imprecise Reasoning into Active Databases
#@ Antoni Wolski;Tarik Bouaziz
#t 1998
#c 17

#index 462224
#* The Alps at Your Fingertips: Virtual Reality and Geoinformation Systems
#@ Renato Pajarola;Thomas Ohler;Peter Stucki;Kornel Szabo;Peter Widmayer
#t 1998
#c 17

#index 462225
#* Data Intensive Intra- & Internet Applications - Experiences Using Java and CORBA in the World Wide Web
#@ Jürgen Sellentin;Bernhard Mitschang
#t 1998
#c 17

#index 462226
#* On Query Spreadsheets
#@ Laks V. S. Lakshmanan;Subbu N. Subramanian;Nita Goyal;Ravi Krishnamurthy
#t 1998
#c 17

#index 462227
#* Grouping Techniques for Update Propagation in Intermittently Connected Databases
#@ Sameer Mahajan;Michael J. Donahoo;Shamkant B. Navathe;Mostafa H. Ammar;Sanjoy Malik
#t 1998
#c 17

#index 462228
#* Virtual Database Technology
#@ Ashish Gupta;Venky Harinarayan;Anand Rajaraman
#t 1998
#c 17

#index 462229
#* Parallelizing Loops in Database Programming Languages
#@ Daniel F. Lieuwen
#t 1998
#c 17

#index 462230
#* Point-Versus Interval-Based Temporal Data Models
#@ Michael H. Böhlen;R. Busatto;Christian S. Jensen
#t 1998
#c 17

#index 462231
#* Efficient Retrieval of Similar Time Sequences Under Time Warping
#@ Byoung-Kee Yi;H. V. Jagadish;Christos Faloutsos
#t 1998
#c 17

#index 462232
#* WWW and the Internet - Did We Miss the Boat? (Panel)
#@ Michael Rabinovich;C. Mic Bowman;Hector Garcia-Molina;Alon Y. Levy;Susan Malaika;Alberto O. Mendelzon
#t 1998
#c 17

#index 462233
#* Cyclic Allocation of Two-Dimensional Data
#@ Sunil Prabhakar;Khaled A. S. Abdel-Ghaffar;Divyakant Agrawal;Amr El Abbadi
#t 1998
#c 17

#index 462234
#* Mining Optimized Association Rules with Categorical and Numeric Attributes
#@ Rajeev Rastogi;Kyuseok Shim
#t 1998
#c 17

#index 462235
#* Optimizing Regular Path Expressions Using Graph Schemas
#@ Mary F. Fernandez;Dan Suciu
#t 1998
#c 17

#index 462236
#* High Dimensional Similarity Joins: Algorithms and Performance Evaluation
#@ Nick Koudas;Kenneth C. Sevcik
#t 1998
#c 17

#index 462237
#* Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query
#@ Stacie Hibino;Elke A. Rundensteiner
#t 1998
#c 17

#index 462238
#* Online Generation of Association Rules
#@ Charu C. Aggarwal;Philip S. Yu
#t 1998
#c 17

#index 462239
#* Fast Nearest Neighbor Search in High-Dimensional Space
#@ Stefan Berchtold;Bernhard Ertl;Daniel A. Keim;Hans-Peter Kriegel;Thomas Seidl
#t 1998
#c 17

#index 462240
#* Dynamic Granular Locking Approach to Phantom Protection in R-Trees
#@ Kaushik Chakrabarti;Sharad Mehrotra
#t 1998
#c 17

#index 462241
#* Migrating Legacy Databases and Applications (Panel)
#@ Bhavani M. Thuraisingham;Sandra Heiler;Arnon Rosenthal;Susan Malaika
#t 1998
#c 17

#index 462242
#* Design and Implementation of Display Specification for Multimedia Answers
#@ Chitta Baral;Graciela Gonzalez;Tran Cao Son
#t 1998
#c 17

#index 462243
#* A Distribution-Based Clustering Algorithm for Mining in Large Spatial Databases
#@ Xiaowei Xu;Martin Ester;Hans-Peter Kriegel;Jörg Sander
#t 1998
#c 17

#index 464064
#* A Next Generation Industry Multimedia Database System
#@ Hiroshi Ishikawa;Koki Kato;Miyoki Ono;Naomi Yoshikawa;Kazumi Kubota;Akiko Kondo
#t 1996
#c 17
#! New multimedia applications have emerged on top of information infrastructures, such as on-demand services, digital libraries and museums, online shopping, and document management, which require new databases. That is, next-generation database systems must enable users to efficiently and flexibly develop and execute such advanced multimedia applications. We focus on development of a database system which enables flexible and efficient acquisition, storage, access and retrieval, and distribution and presentation of large amounts of heterogeneous media data. We take an approach based on an object-oriented database, which is more suitable for description of media structures and operations than a traditional relational database. And we extend the object-oriented approach by providing temporal and spatial operators, and control of distributing computing and QOS (quality of service). In this paper, we describe a multimedia data model and its efficient implementation.

#index 464065
#* Prefetching from Broadcast Disks
#@ Swarup Acharya;Michael J. Franklin;Stanley B. Zdonik
#t 1996
#c 17
#! Broadcast Disks have been proposed as a means to efficiently deliver data to clients in ``asymmetric'' environments where the available bandwidth from the server to the clients greatly exceeds the bandwidth in the opposite direction. A previous study investigated the use of cost-based caching to improve performance when clients access the broadcast in a demand-driven manner [. achas 95 .]. Such demand-driven access however, does not fully exploit the dissemination-based nature of the broadcast, which is particularly conducive to client {\em prefetching}. With a Broadcast Disk, pages continually flow past the clients so that, in contrast to traditional environments, prefetching can be performed without placing additional load on shared resources. We argue for the use of a simple prefetch heuristic called \PT{} and show that \PT{} balances the cache residency time of a data item with its bandwidth allocation. Because of this tradeoff, \PT{} is very tolerant of variations in the broadcast program. We describe an implementable approximation for \PT{} and examine its sensitivity to access probability estimation errors. The results show that the technique is effective even when the probability estimation is substantially different from the actual values.

#index 464066
#* Relaxed Index Consistency for a Client-Server Database
#@ Vibby Gottemukkala;Edward Omiecinski;Umakishore Ramachandran
#t 1996
#c 17
#! Client-Server systems cache data in client buffers to deliver good performance. Several efficient protocols have been proposed to maintain the coherence of the cached data. However, none of the protocols distinguish between index pages and data pages. We propose a new coherence protocol, called Relaxed Index Consistency, that exploits the inherent differences in the coherence and concurrency-control (C&CC) requirements for index and data pages. The key idea is to incur a small increase in computation time at the clients to gain a significant reduction in the number of messages exchanged between the clients and the servers. The protocol uses the concurrency control on data pages to maintain coherence of index pages. A performance-conscious implementation of the protocol that makes judicious use of version numbers is proposed. We show, through both qualitative and quantitative analysis, the performance benefits of making the distinction between index pages and data pages for the purposes of C&CC. Our simulation studies show that the Relaxed Index Consistency protocol improves system throughput by as much as 15% to 88%, based on the workload.

#index 464067
#* Query Answering Using Discovered Rules
#@ I-Min A. Chen
#t 1996
#c 17

#index 464068
#* A Uniform Indexing Scheme for Object-Oriented Databases
#@ Ehud Gudes
#t 1996
#c 17
#! The performance of Object-oriented databases (OODB) is a critical factor hindering their current use. Several indexing schemes have been proposed in the literature for enhancing OODB performance and they are briefly reviewed here. In this paper a new and uniform indexing scheme is proposed. This scheme is based on a single B-tree and combines both the hierarchical and nested indexing schemes \cite{Bertino,Kim}. The uniformity of this scheme enables compact and optimized code dealing with a large range of queries on the one hand, and flexibility in adding and removing indexed paths on the other hand. The performance of the scheme is about the same as existing schemes for single-class, exact match or range queries, and much better for multi-class and other complex queries and update.

#index 464069
#* Automating the Assembly of Presentations from Multimedia Databases
#@ Gultekin Özsoyoglu;Veli Hakkoymaz;Joel Kraft
#t 1996
#c 17
#! A multimedia presentation refers to the presentation of multimedia data using output devices such as monitors for text and video, and speakers for audio. Each presentation consists of multimedia segments which are obtained from a multimedia data model. In this paper, we propose to express semantic coherency of a multimedia presentation in terms of presentation inclusion and exclusion constraints that are incorporated into the multimedia data model. Thus, when a user specifies a set of segments for a presentation, the DBMS adds segments into and/or deletes segments from the set in order to satisfy the inclusion and exclusion constraints. To automate the assembly of a presentation with concurrent presentation streams, we also propose presentation organization constraints that are incorporated into the multimedia data model, independent of any presentation. We give two algorithms for automated presentation assembly and discuss their complexity. We discuss the satisfiability of inclusion and exclusion constraints when negation is allowed. And, we briefly describe a prototype system that is being developed for automated presentation assembly.

#index 464070
#* SONET Configuration Management with OpenPM
#@ Weimin Du;Ming-Chien Shan;Chris Whitney
#t 1996
#c 17
#! SONET (Synchronous Optical NETwork) has been proposed as the backbone of future information superhighway infrastructure. SONET network management, however, is a complex process that involves many heterogeneous systems and applications, as well as human interactions. In this paper, we describe a prototype system developed at Hewlett-Packard (HP) that provides a service for configuring large SONET networks. The prototype differs from the existing systems in that it employs the HP OpenPM (Open Process Management) workflow system to define, execute and monitor network management processes. Using OpenPM (a middleware service that enables the automation of activities supporting complex enterprise business processes in a distributed heterogeneous computing environment) as a reliable and efficient workflow execution engine, this prototype supports efficient distributed network management and easy integration of legacy applications. The paper describes how an example network configuration management process is modeled, executed and monitored using OpenPM.

#index 464071
#* The Gold Text Indexing Engine
#@ Daniel Barbará;Sharad Mehrotra;Padmavathi Vallabhaneni
#t 1996
#c 17
#! The proliferation of electronic communication including computer mail, faxes, voice mail, and net news has led to a variety of disjoint applications and usage paradigms that forces users to deal with multiple different user interfaces and access related information arriving over the different communication media separately. To enable users to cope with the overload of information arriving over heterogeneous communication media, we have developed the Gold document handling system that allows users to access all of these forms of communication at once, or to intermix them. The Gold system provides users with an integrated way to send and recieve messages using different media, efficiently store the messages, retrieve the messages based on their contents, and to access a variety of other sources of useful information. At the center of the Gold document handling system is the Gold Text Indexing Engine (GTIE) that provides a full text index over the documents. The paper describes our implementation of GTIE and the concurrency control protocols to ensure consistency of the index in the presence of concurrent operations.

#index 464072
#* What's in a WWW Link? - Panel
#@ Amit P. Sheth;Robert Meersman;Erich J. Neuhold;Calton Pu;V. S. Subrahmanian
#t 1996
#c 17

#index 464073
#* VISUAL: A Graphical Icon-Based Query Language
#@ Nevzat Hurkan Balkir;E. Sukan;Gultekin Özsoyoglu;Z. Meral Özsoyoglu
#t 1996
#c 17
#! VISUAL is a graphical icon-based query language designed for scientific databases where visualization of the relationships are important for the domain scientist to express queries. Graphical objects are not tied to the underlying formalism; instead, they represent the relationships of the application domain. VISUAL supports relational, nested, and object-oriented models naturally and has formal basis. In addition to set and bag constructs for complex objects, sequences are also supported by the data model. Concepts of external and internal queries are developed as modularization tools. A new parallel/distributed query processing paradigm is presented. VISUAL query processing techniques are also discussed.

#index 464074
#* A Toolkit for Constraint Management in Heterogeneous Information Systems
#@ Sudarshan S. Chawathe;Hector Garcia-Molina;Jennifer Widom
#t 1996
#c 17
#! We present a framework and a toolkit to monitor and enforce distributed integrity constraints in loosely coupled heterogeneous information systems. Our framework enables and formalizes weakened notions of consistency, which are essential in such environments. Our framework is used to describe (1) intelfaces provided by a database for the data items involved in inter-site constraints; (2) strategies for monitoring and enforcing such constraints, (3) guarantees regarding the level of consistency the system can provide. Our toolkit uses this framework to provide a set of configurable modules thatare used to monitorand en- force constraints spanning loosely coupled heterogeneous information systems.

#index 464075
#* PICSDesk: A Case Study on Business Process Re-engineering
#@ Manolis M. Tsangaris;Madhur Kohli;Shamim A. Naqvi;Richard Nunziata;Yatin P. Saraiya
#t 1996
#c 17
#! Presents first-hand experiences from an actual re-engineering project. Business re-engineering is a process affecting not only the software system involved, but the underlying business model as well. Indeed, it is the changed business model along with the new technologies that determine the design of the new system. This paper is a walkthrough of the design of PICSDESK, a prototype incorporating some modern technologies to support the old business model of its predecessor and its evolution to a new business model. PICSDESK may be thought of as an example of a new breed of inventory control systems.

#index 464076
#* Mining Knowledge Rules from Databases: A Rough Set Approach
#@ Xiaohua Hu;Nick Cercone
#t 1996
#c 17
#! In this paper, the principle and experimental results of an attribute-oriented rough set approach for knowledge discovery in databases are described. Our method integrates the database operation, rough set theory and machine learning techniques. In our method, we consider the learning procedure consists of two phases: data generalization and data reduction. In data generalization phase, the attribute-oriented induction is performed attribute by attribute using attribute removal and concept ascension, some undesirable attributes to the discovery task are removed and the primitive data is generalized to the desirable level, thus a set of tuples may be generalized to the same generalized tuple, this procedure substantially reduces the computational complexity of the database learning process. Subsequently, in data reduction phase, the rough set method is applied to the generalized relation to find a minimal attribute set relevant to the learning task. The generalized relation is reduced further by removing those attributes which are irrelevant and/or unimportant to the learning task. Finally the tuples in the reduced relation are transformed into different knowledge rules based on different knowledge discovery algorithms. Based upon these principles, a prototype knowledge discovery system DBROUGH has been constructed. In DBROUGH, a variety of knowledge discovery algorithms are incorporated and different kinds of knowledge rules, such as characteristic rules, classification rules, decisions rules, maximal generalized rules can be discovered efficiently and effectively from large databases.

#index 464077
#* Towards Eliminating Random I/O in Hash Joins
#@ Ming-Ling Lo;Chinya V. Ravishankar
#t 1996
#c 17
#! The widening performance gap between CPU and disk is significant for hash join performance. Most current hash join methods try to reduce the volume of data transferred between memory and disk. In this paper, we try to reduce hash-join times by reducing random I/O. We study how current algorithms incur random I/O, and propose a new hash join method, Seq+, that converts much of the random I/O to sequential I/O. Seq+ uses a new organization for hash buckets on disk, and larger input and output buffer sizes. We introduce the technique of batch writes to reduce the bucket-write cost, and the concepts of write- and read-groups of hash buckets to reduce the bucket-read cost. We derive a cost model for our method, and present formulas for choosing various algorithm parameters, including input and output buffer sizes. Our performance study shows that the new hash join method performs many times better than current algorithms under various environments. Since our cost functions under-estimate the cost of current algorithms and over-estimate the cost of Seq+, the actual performance gain of Seq+ is likely to be even greater.

#index 464078
#* Efficient Processing of Outer Joins and Aggregate Functions
#@ Gautam Bhargava;Piyush Goel;Balakrishna R. Iyer
#t 1996
#c 17
#! Removal of redundant outer joins is essential for the reassociation of outer joins with other binary operations. In this paper we present a set of comprehensive algorithms that employ the properties of strong predicates along with the properties of aggregation, intersection, union, and except operations to remove redundant outer joins from a query. For the purpose of query simplification, we generate additional projections by determining the keys. Our algorithm for generating keys is based on a novel concept of weak bindings that is essential for queries containing outer joins. Our algorithm for converting outer joins to joins is based on a novel concept of join-reducibility.

#index 464194
#* Transaction Coordination for the New Millennium: SQL Server Meets OLE Transactions
#@ David Campbell
#t 1996
#c 17

#index 464195
#* Similarity Indexing with the SS-tree
#@ David A. White;Ramesh Jain
#t 1996
#c 17
#! Efficient indexing of high dimensional feature vectors is important to allow visual information systems and a number other applications to scale up to large databases. In this paper, we define this problem as "similarity indexing" and describe the fundamental types of "similarity queries" that we believe should be supported. We also propose a new dynamic structure for similarity indexing called the similarity search tree or SS-tree. In nearly every test we performed on high dimensional data, we found that this structure performed better than the R*-tree. Our tests also show that the SS-tree is much better suited for approximate queries than the R*-tree.

#index 464196
#* HierarchyScan: A Hierarchical Similarity Search Algorithm for Databases of Long Sequences
#@ Chung-Sheng Li;Philip S. Yu;Vittorio Castelli
#t 1996
#c 17
#! We present a hierarchical algorithm, HierarchyScan, that efficiently locates one-dimensional subsequences within a collection of sequences of arbitrary length. The subsequences identified by HierarchyScan match a given template pattern in a scale- and phase-independent fashion. The idea is to perform correlation between the stored sequences and the template in the transformed domain hierarchically. Only those subsequences whose maximum correlation value is higher than a predefined threshold will be selected. The performance of this approach is compared to the sequential scanning and an order-of-magnitude speedup is observed.

#index 464197
#* A Groupware Benchmark Based on Lotus Notes
#@ Kenneth Moore;Michelle Peterson
#t 1996
#c 17
#! In this paper, we propose a new benchmark for groupware systems. It incorporates elements of previous messaging, text retrieval, and database benchmarks. The benchmark is based on groupware functions found in Lotus Notes, but should be adaptable by any groupware system.

#index 464198
#* Reusing (Shrink Wrap) Schemas by Modifying Concept Schemas
#@ Lois M. L. Delcambre;Jimmy Langston
#t 1996
#c 17
#! A shrink wrap schema is a well-crafted, complete, global schema that represents an application. A concept schema is a subset of the shrink wrap schema that addresses one particular point of view in an application. We define schema modification operations to customize each concept schema, to match the designer's perception of the application. We maintain the integrated, customized user schema. We enforce consistency checks to provide feedback to the designer about interactions among the concept schemas. We embody these mechanisms in an interactive system that aids in shrink wrap schema-based design. The shrink wrap schema approach promotes reuse of past design efforts; prior approaches to schema reuse do not attempt to reuse an entire schema nor do they focus on local customization. The focus of this paper is on the definition of concept schemas and their corresponding modification operations.

#index 464199
#* Towards the Reverse Engineering of Denormalized Relational Databases
#@ Jean-Marc Petit;Farouk Toumani;Jean-Francois Boulicaut;Jacques Kouloumdjian
#t 1996
#c 17
#! This paper describes a method to cope with denormalized relational schemas in a database reverse engineering process. We propose two main steps to improve the understanding of data semantics. Firstly we extract inclusion dependencies by analyzing the equi-join queries embedded in application programs and by querying the database extension. Secondly we show how to discover only functional dependencies which influence the way attributes should be restructured. The method is interactive since an expert user has to validate the presumptions on the elicited dependencies. Moreover, a restructuring phase leads to a relational schema in third normal form provided with key dependencies and referential integrity constraints. Finally, we sketch how an Entity-Relationship schema can be derived from such information.

#index 464200
#* DB2 LOBs: The Teenage Years
#@ Tobin J. Lehman;Patrick Gainer
#t 1996
#c 17
#! Previous versions of DB2 Common Server had large objects (LOBs) that were neither large nor functional. Their size was limited to 32,700 bytes and, until recently when support for SUBSTR and CONCAT was added, there was no function available on these objects at all. DB2 LOBs were infants. However, with the latest release of DB2 Common Server, Version 2.1, LOBs have matured considerably, supporting significantly larger sizes and many new language features. To give the reader a feeling for the extent of this new language support, we compare our new SQL LOB language features with that of three other major Relational database competitors: Sybase, Informix and Oracle. Users will find the new DB2 LOBS easy to load and store, easy to search, and easy to integrate into the DB2 user-defined functions (UDFs) and user-defined types (UDTs). In addition, when used in serial mode, the performance of LOB I/O rivals that of file systems and, when used in parallel mode, is a clear winner. DB2 LOBs have now entered the teenage years.

#index 464201
#* Parallel Pointer-Based Join Algorithms in Memory-mapped Environments
#@ Peter A. Buhr;Anil K. Goel;Naomi Nishimura;Prabhakar Ragde
#t 1996
#c 17
#! Three pointer-based parallel join algorithms are presented and analyzed for environments in which secondary storage is made transparent to the programmer through memory mapping. Buhr, Goel, and Wai have shown that data structures such as B-Trees, R-Trees and graph data structures can be implemented as efficiently and effectively in this environment as in a traditional environment using explicit I/O. Here we show how higher-order algorithms, in particular parallel join algorithms, behave in a memory mapped environment. A quantitative analytical model has been developed to conduct performance analysis of the parallel join algorithms. The model has been validated by experiments.

#index 464202
#* A Hybrid Object Clustering Strategy for Large Knowledge-Based Systems
#@ Arun Ramanujapuram;Jim E. Greer
#t 1996
#c 17
#! Object bases underlying knowledge-based applications tend to be complex and require management. This research aims at improving the performance of object bases underlying a class of large knowledge-based systems that utilize object-oriented technology to engineer the knowledge base. In this paper, a hybrid clustering strategy that beneficially combines semantic clustering and iterative graph-paritioning techniques has been developed and evaluated for use in knowledge bases storing information in the form of object graphs. It is demonstrated via experimentation that such a technique is useful and feasible in realistic object bases. A semantic specification mechanism similar to placement trees has been developed for specifying the clustering. The workload and the nature of object graphs in knowledge bases differ significantly from those present in conventional object-oriented databases. Therefore, the evaluation has been performed by building a new benchmark called the Granularity Benchmark. A segmented storage scheme for the knowledge base using large object storage mechanisms of existing storage managers is also examined.

#index 464203
#* Query Folding
#@ Xiaolei Qian
#t 1996
#c 17
#! Query folding refers to the activity of determining if and how a query can be answered using a given set of resources, which might be materialized views, cached results of previous queries, or queries answerable by other databases. We investigate query folding in the context where queries and resources are conjunctive queries. We develop an exponential-time algorithm that finds all complete or partial foldings, and a polynomial-time algorithm for the subclass of acyclic conjunctive queries. Our results can be applied to query optimization in centralized databases, to query processing in distributed databases, and to query answering in federated databases.

#index 464204
#* Maintenance of Discovered Association Rules in Large Databases: An Incremental Updating Technique
#@ David Wai-Lok Cheung;Jiawei Han;Vincent Ng;C. Y. Wong
#t 1996
#c 17
#! An incremental updating technique is developed for maintenance of the association rules discovered by database mining. There have been many studies on efficient discovery of association rules in large databases. However, it is nontrivial to maintain such discovered rules in large databases because a database may allow frequent or occasional updates and such updates may not only invalidate some existing strong association rules but also turn some weak rules into strong ones. In this study, an incremental updating technique is proposed for efficient maintenance of discovered association rules when new transaction data are added to a transaction database.

#index 464205
#* Parallel Processing of Spatial Joins Using R-trees
#@ Thomas Brinkhoff;Hans-Peter Kriegel;Bernhard Seeger
#t 1996
#c 17
#! Fachgebiet Informatik, Universität Marburg, Marburg, GermanyIn this paper, we show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so-called shared virtual memory which is well-suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execution. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that preserves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance comparison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large.

#index 464206
#* A Proposed Method for Creating VCR Functions using MPEG Streams
#@ David B. Andersen
#t 1996
#c 17
#! The development of video-on-demand (VOD) systems for movie delivery requires that the user be able to perform VCR functions over a broadband network system. These functions include Play, Pause, Fast Forward, and Fast Rewind. No standard method exists between content developers, server manufacturers and client applications to provide these functions. This paper proposes a standard method for implementing these functions using MPEG streams and discusses some of the important tradeoffs. The encoding and distribution of content has become one of the most important issues facing video information providers. Today, in the case of movies, every service provider must encode the material for the specific equipment being deployed in the network. Therefore, the ease of use and speed of the algorithms employed to encode the material are extremely important. In the future, the creator of the content may encode the material once and distribute it to the service providers in compressed form, but this is not the case today due to the lack of standards.

#index 464207
#* A Graph-Theoretic Approach to Indexing in Object-Oriented Databases
#@ Boris Shidlovsky;Elisa Bertino
#t 1996
#c 17
#! A graph-theoretic approach to the path indexing problem is proposed. We represent the indexing relationships supported by indices allocated in the classes in the path in the form of a directed graph. All the previous approaches directly fit into the scheme and form a hierarchy of complexity with respect to the time required for selection of the optimal index configuration. Based on the general scheme, we develop a new approach to the path indexing problem exploiting the notion of visibility graph. We introduce a generalized nested-inherited index, give algorithms for retrieval and update operations and compare the behavior of the new structure with previous approaches.

#index 464208
#* Using Partial Differencing for Efficient Monitoring of Deferred Complex Rule Conditions
#@ Martin Sköld;Tore Risch
#t 1996
#c 17
#! Presents a difference calculus for determining changes to rule conditions in an active DBMS. The calculus has been used for implementing an algorithm to efficiently monitor rules with complex conditions. The calculus is based on partial differencing of queries derived from rule conditions. For each rule condition, several partially differentiated queries are generated that each considers changes to a single base relation or view that the condition depends on. The calculus considers both insertions and deletions. The algorithm is optimized for deferred rule condition monitoring in transactions with few updates. The calculus allows us to optimize both space and time. Space optimization is achieved since the calculus and the algorithm does not presuppose materialization of monitored conditions to find its previous state. This is achieved by using a breadth-first, bottom-up propagation algorithm and by calculating previous states by doing a logical rollback. Time optimization is achieved through incremental evaluation techniques. The algorithm has been implemented and a performance study is presented at the end of the paper.

#index 464209
#* OLE DB: A Component DBMS Architecture
#@ José A. Blakeley
#t 1996
#c 17
#! The article describes an effort at Microsoft whose primary goal is to enable applications to have uniform access to data stored in diverse DBMS and non DBMS information containers. Applications continue to take advantage of the benefits of database technology such as declarative queries, transactional access, and security without having to transfer data from its place of origin to a DBMS. Our approach consists of defining an open, extensible collection of interfaces that factor and encapsulate orthogonal, independently reusable portions of DBMS functionality. These interfaces define the boundaries of DBMS components arch as record containers, and query processors that enable uniform, transactional access to data among such components. The proposed interfaces extend Microsoft's OLE Component Object Model (COM) with database functionality, hence these interfaces are collectively referred to as OLE DB. The OLE DB functional areas include data access and updates (rowsets), query processing, catalog information, notifications, transactions, security, and distribution. The article presents an overview of the OLE DB approach and its areas of componentization.

#index 464210
#* Database Research: Lead, Follow, or Get Out of the Way? - Panel Abstract
#@ Surajit Chaudhuri;Ashok K. Chandra;Umeshwar Dayal;Jim Gray;Michael Stonebraker;Gio Wiederhold;Moshe Y. Vardi
#t 1996
#c 17

#index 464211
#* A Transactional Nested Process Management System
#@ Qiming Chen;Umeshwar Dayal
#t 1996
#c 17
#! Providing flexible transaction semantics and incorporating activities, data and agents are the key issues in workflow system development. Unfortunately, most of the commercial workflow systems lack the advanced features of transaction models, and an individual transaction model with specific emphasis lacks sufficient coverage for business process management.This report presents our solutions to the above problems in developing Open Process Management System (OPMS) at HP Labs. OPMS is based on nested activity modeling with the following extensions and constraints: in-process open nesting} for extending closed/open nesting to accommodate applications that require improved process-wide concurrency without sacrificing top-level atomicity; confined open as a constraint on open and in-process open activities for avoiding the semantic inconsistencies in activity triggering and compensation; and two-phase remedy as a generalized hierarchical approach for handling failures.

#index 464212
#* Database Extensions for Complex Domains
#@ Samuel DeFazio;Jagannathan Srinivasan
#t 1996
#c 17
#! Future versions of the Oracle Server will provide an open and extensible framework for supporting complex data domains including, but not limited to, text, image, spatial, video, and OLAP. This framework encompasses features for defining, storing, updating, indexing, and retrieving complex forms of data with full transaction semantics. The underpinning for these features is an extended Oracle Server that is an object-relational database management system (ORDBMS).

#index 464213
#* Representing Retroactive and Proactive Versions in Bi-Temporal Databases
#@ Jongho Won;Ramez Elmasri
#t 1996
#c 17
#! Bi-Temporal databases allow users to record retroactive (past) and proactive (future planned) versions of an entity, and to retrieve the appropriate versiosns for bi-temporal queries that involve both valid-time and transaction-time. Currently used timestamp representations are mainly for either valid-time or transaction-time databases. In this paper, we first categorize the types of problems that can occur in existing models. These are (1) ambiguity, (2) priority specification, and (3) lost information. We then propose a 2TDB model that allows both retroactive and proactive versions, overcomes the identified problems, and permits the correction of recorded facts.

#index 464214
#* Energy-Efficient Caching for Wireless Mobile Computing
#@ Kun-Lung Wu;Philip S. Yu;Ming-Syan Chen
#t 1996
#c 17
#! Caching can reduce the bandwidth requirement in a mobile computing environment. However, due to battery power limitations, a wireless mobile computer may often be forced to operate in a doze or even totally disconnected mode. As a result, the mobile computer may miss some cache invalidation reports broadcasted by a server, forcing it to discard the entire cache contents after waking up. In this paper, we present an energy-efficient cache invalidation method, called GCORE, that allows a mobile computer to operate in a disconnected mode to save battery while still retaining most of the caching benefits after a reconnection. We present an efficient implementation of GCORE and conduct simulations to evaluate its caching effectiveness. The results show that GCORE can substantially improve mobile caching by reducing the communication bandwidth (or energy consumption) for query processing.

#index 464215
#* Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Total
#@ Jim Gray;Adam Bosworth;Andrew Layman;Hamid Pirahesh
#t 1996
#c 17

#index 464216
#* Delta-Sets for Optimized Reactive Adaptive Playout Management in Distributed Multimedia Database Systems
#@ Heiko Thimm;Wolfgang Klas
#t 1996
#c 17
#! A novel database system service called playout management service which performs multimedia presentations was proposed recently. In distributed multimedia database systems without end-to-end guarantees, such a playout management service faces the potential problem that system performance can become insufficient when realizing a stored presentation. This problem can be overcome by making the playout management service reactive such that it balances the data amount to be fetched from a remote multimedia database with the system performance available. For the users this means that, a running presentation is adapted by the playout management service. In this paper, we propose the concept of delta-set to adapt the execution of arbitrary multimedia presentations in an optimized way. We show a heuristic scheme to identify the most adequate delta-set with respect to (1) the actual system state, (2) the user preferences, and (3) the specific properties of multimedia presentations.

#index 464217
#* DSDT: Durable Scripts Containing Database Transactions
#@ Betty Salzberg;Dimitri Tombroff
#t 1996
#c 17
#! DSDT is a proposed method for creating durable scripts which contain short ACID transactions as components. Workflow scripts are an example. The context of the script is made durable by writing a log record whenever an event occurs which cannot be replayed. Log checkpoints are used to minimize recovery time. DSDT can be written in stand-alone mode communicating with DBMSs by transactional remote procedure calls and maintaining its own logging system or it can be made part of a DBMS by modifying the DBMS transaction manager source code. DSDT provides a panic button (signal-exit) and the ability to specify what action should be taken on restart after system failure. The programmer can also specify actions such as "compensation" transactions to be taken after another signal (signal-cancel) arrives. DSDT enables most extended transaction models to be expressed in scripts modulo the guarantees of compensation. Recovery after system failure is shown to be correct.

#index 464218
#* Authorization and Access Control in IRO-DB
#@ W. Eßmayr;Fritz Kastner;Günther Pernul;Stefan Preishuber;A. Min Tjoa
#t 1996
#c 17

#index 464219
#* Client-Based Logging for High Performance Distributed Architectures
#@ Euthimios Panagos;Alexandros Biliris;H. V. Jagadish;Rajeev Rastogi
#t 1996
#c 17
#! In this paper, we propose logging and recovery algorithms for distributed architectures that use local disk space to provide transactional facilities locally. Each node has its own log file where all log records for updates to locally cached pages are written. Transaction rollback and node crash recovery are handled exclusively by each node and log files are not merged at any time. Our algorithms do not require any form of time synchronization between nodes and nodes can take checkpoints independently of each other. Finally, our algorithms make possible a new paradigm for distributed transaction management that has the potential to exploit all available resources and improve scalability and performance.

#index 464220
#* An Executable Graphical Representation of Mediatory Information Systems
#@ Jacques Calmet;Dirk Debertin;Sebastian Jekutsch;Joachim Schü
#t 1996
#c 17
#! In this paper we present an approach towards a unified modeling and query-processing tool for mediatory information systems. Based upon Coloured Petri nets we are able to model the integration of parametric data (external, uncertain and temporal informations) and to visualize the dataflow in mediatory information systems.

#index 464221
#* The Ode Active Database: Trigger Semantics and Implementation
#@ Daniel F. Lieuwen;Narain H. Gehani;Robert M. Arlein
#t 1996
#c 17
#! Triggers are the basic ingredient of active databases. Ode triggers are event-action pairs. An event can be a composite event (i.e., an event composed from other events). Composite events are detected by translating the event specifications into finite state machines. In this paper, we describe the integration and implementation of composite event based triggers into the Ode object database. We focus on implementation details such as the basic trigger events supported, the efficient posting of these events, the handling of transaction-related events, and the integration of triggers into a real database. We also describe the run-time facilities used to support trigger processing and describe some experiences we gained while implementing triggers. We illustrate Ode trigger facilities with a credit card example.

#index 464222
#* MedMaker: A Mediation System Based on Declarative Specifications
#@ Yannis Papakonstantinou;Hector Garcia-Molina;Jeffrey D. Ullman
#t 1996
#c 17
#! Mediators are used for integration of heterogeneous information sources. We present a system for declaratively specifying mediators. It is targeted for integration of sources with unstructured or semi-structured data and/or sources with changing schemas. We illustrate the main features of the Mediator Specification Language (MSL), show how they facilitate integration, and describe the implementation of the system that interprets the MSL specifications.

#index 464223
#* HiTi Graph Model of Topographical Roadmaps in Navigation Systems
#@ Sungwon Jung;Sakti Pramanik
#t 1996
#c 17
#! In navigation systems, a primary task is to compute the minimum cost route from the current location to the destination. One of the major problems for navigation systems is that a significant amount of computation time is required to find a minimum cost path when the topographical road map is large. Since navigation systems are real time systems, it is critical that the path be computed while satisfying a time constraint. In this paper, we propose a new graph model named HiTi (Hierarchical mulTi graph model), for efficiently computing an optimal minimum cost path. Based on HiTi graph model, we propose a new single pair minimum cost path algorithm. We empirically show that our proposed algorithm performs far better than the traditional A* algorithm. Further, we empirically analyze our algorithm by varying both edge cost distribution and hierarchical level number of HiTi graphs.

#index 464224
#* Tioga-2: A Direct Manipulation Database Visualization Environment
#@ Alexander Aiken;Jolly Chen;Michael Stonebraker;Allison Woodruff
#t 1996
#c 17
#! This paper reports on user experience with Tioga, a DBMS-centric visualization tool developed at Berkeley. Based on this experience, we have designed Tioga-2 as a direct manipulation system that is more powerful and much easier to program. A detailed design of the revised system is presented, together with an extensive example of its application.

#index 464225
#* Workflow and Data Management in InConcert
#@ Sunil K. Sarin
#t 1996
#c 17
#! InConcert is an object-oriented client-server workflow management system. An overview is provided of the functionality of InConcert and how it is implemented on an underlying relational database management system. Data management issues in supporting distributed workflow are briefly reviewed.

#index 464226
#* Evaluation and Optimization of the LIVING IN A LATTICE Rule Language
#@ Holger Riedel;Andreas Heuer
#t 1996
#c 17
#! We introduce an evaluation technique for a declarative OODB query language. The query language is rule-based and can be evaluated and optimized using an appropriate object algebra. We introduce a new framework which uses concepts of the object-oriented data model to define adequate accesses to the database. Additionally, the problems according to the evaluation of recursive queries are discussed.

#index 464227
#* Performance Analysis of Several Algorithms for Processing Joins between Textual Attributes
#@ Weiyi Meng;Clement T. Yu;Wei Wang;Naphtali Rishe
#t 1996
#c 17
#! Three algorithms for processing joins on attributes of textual type are presented and analyzed in this paper. Since such joins often involve document collections of very large size, it is very important to find efficient algorithms to process them. The three algorithms differ on whether the documents themselves or the inverted files on the documents are used to process the join. Our analysis and the simulation results indicate that the relative performance of these algorithms depends on the input document collections, system characteristics and the input query. For each algorithm, the type of input document collections with which the algorithm is likely to perform well is identified.

#index 464228
#* Speculative Data Dissemination and Service to Reduce Server Load, Network Traffic and Service Time in Distributed Information Systems
#@ Azer Bestavros
#t 1996
#c 17
#! We present two server-initiated protocols to improve the performance of distributed information systems WWW. Our first protocol is a hierarchical data dissemination mechanism that allows information to propagate from its producers to servers that are closer to its consumers. This dissemination reduces network traffic and balances load amongst servers by exploiting geographic and temporal locality of reference properties exhibited in client access patterns. Our second protocol relies on speculative service, whereby a request for a document is serviced by sending, in addition to the document requested, a number of other documents that the server speculates will be requested in the near future. This speculation reduces service time by exploiting the spatial locality of reference property. We present results of trace-driven simulations that quantify the attainable performance gains for both protocols.

#index 464229
#* Search and Ranking Algorithms for Locating Resources on the World Wide Web
#@ Budi Yuwono;Dik Lun Lee
#t 1996
#c 17
#! Applying information retrieval techniques to the World Wide Web (WWW) environment is a challenge, mostly because of its hypertext/hypermedia nature and the richness of the meta-information it provides. We present four keyword-based search and ranking algorithms for locating relevant WWW pages with respect to user queries. The first algorithm, Boolean Spreading Activation, extends the notion of word occurrence in the Boolean retrieval model by propagating the occurrence of a query word in a page to other pages linked to it. The second algorithm, Most-cited, uses the number of citing hyperlinks between potentially relevant WWW pages to increase the relevance scores of the referenced pages over the referencing pages. The third algorithm, TFxIDF vector space model, is based on word distribution statistics. The last algorithm, Vector Spreading Activation, combines TFxIDF with the spreading activation model. We conducted an experiment to evaluate the retrieval effectiveness of these algorithms. From the results of the experiment, we draw conclusions regarding the nature of the WWW environment with respect to document ranking strategies.

#index 464230
#* Applying a Flexible OODBMS-IRS-Coupling for Structured Document Handling
#@ Marc Volz;Karl Aberer;Klemens Böhm
#t 1996
#c 17
#! In document management systems it is desirable to provide content-based access to documents going beyond regular expression search in addition to access based on structural characteristics or associated attributes. We present a new approach for coupling OODBMSs (Object Oriented Database Management Systems) and IRSs (Information Retrieval Systems) that provides enhanced flexibility and functionality as compared to coupling approaches reported from the literature. Our approach allows to decide freely to which document collections, that are used as retrieval context, document objects belong, which text contents they provide for retrieval and how they derive their associated retrieval values, either directly from the retrieval machine or from the values of related objects. Especially, we show how in this approach different strategies can be applied to hierarchically structured documents, possibly avoiding redundancy and IRS or OODBMS peculiarities. Content-based and structural queries can be freely combined within the OODBMS query language.

#index 464231
#* A Distributed Query Processing Strategy Using Placement Dependency
#@ Chengwen Liu;Hao Chen;Warren Krueger
#t 1996
#c 17
#! We present an algorithm to make use of placement dependency information to process distributed queries. Our algorithm first partitions the referenced relations of a given query into a number of non-exclusive subsets such that the fragmented relations within a subset have placement dependency and the join operation(s) associated with the relations in the subset can be locally processed without data transfer. Each subset is associated with a set of sites and can be used to generate an execution plan for the given query by keeping the fragmented relations in the subset fragmented at the sites where they are situated while replicating the other referenced relations at each of the processing sites. Among the alternatives, our algorithm picks the plan that gives the minimum response time. Our experimental results show that our algorithm improves response time significantly.

#index 464232
#* Transaction Management for a Distributed Object Storage System WAKASHI - Design, Implementation and Performance
#@ Ge Yu;Kunihiko Kaneko;Guangyi Bai;Akifumi Makinouchi
#t 1996
#c 17
#! This paper presents the transaction management in a high performance distributed object storage system WAKASHI. Unlike other systems that use centralized client/server architecture and offer conventional buffer management for distributed persistent object management, WAKASHI is based on symmetric peer-peer architecture and employs memory-mapping and distributed shared virtual memory techniques. Several novel techniques on transaction management for WAKASHI are developed. First, a multi-threaded transaction manager offers ``multi-threaded connection'' so that data control and transaction operations can be performed in parallel manner. Secondly, a concurrency control mechanism supports transparent page-level locks to reduce the complexity of user programs and locking overhead. Thirdly, a ``compact commit'' method is proposed to minimize the communication cost by reducing the amount of data and the number of connections. Fourthly, a redo-only recovery method is implemented by ``shadowed cache'' method to minimize the logging cost, and to allow fast recovery and system restart. Moreover, the system offers ``hierarchical'' control to support nested transactions. A performance evaluation by the OO7 benchmark is presented, as well.

#index 464233
#* Dynamic Optimization of Index Scans Restricted by Booleans
#@ Gennady Antoshenkov
#t 1996
#c 17

#index 464234
#* Using Object-Oriented Principles to Optimize Update Propagation to Materialized Views
#@ Harumi A. Kuno;Elke A. Rundensteiner
#t 1996
#c 17
#! View materialization is known to be a valuable technique for performance optimization in relational databases, and much work has been done addressing the problem of consistently maintaining relational views under update operations. However, little progress has been made thus far regarding the topic of view materialization in object-oriented databases (OODBs). In this paper, we demonstrate that there are several significant differences between the relational and object-oriented paradigms that can be exploited when addressing the object-oriented view materialization problem. We can use the subsumption relationships between classes to identify branches of classes to which we do not need to propagate updates. Similarly, we can use encapsulated interfaces combined with the fact that any unique database property is inherited from a single location to provide a ``registration/notification'' service for optimizing incremental view updates. We have successfully implemented all proposed techniques in the MultiView system, which provides updatable materialized classes and virtual schemata on top of the GemStone OODBMS. We also report results from the experimental studies we have run on the MultiView system measuring the impact of various optimization strategies incorporated into our materialization update algorithms.

#index 464235
#* Synthesizing Distributed Constrained Events from Transactional Workflow
#@ Munindar P. Singh
#t 1996
#c 17
#! Workflows are the semantically appropriate composite activities in heterogeneous computing environments. Such environments typically comprise a great diversity of locally autonomous databases, applications, and interfaces. Much good research has focused on the semantics of workflows, and how to capture them in different extended transaction models. Here we address the complementary issues pertaining to how workflows may be declaratively specified, and how distributed constraints may be derived from those specifications to enable local control, thus obviating a centralized scheduler. Previous approaches to this problem were limited and often lacked a formal semantics.

#index 464814
#* Design and Performance of an Assertional Concurrency Control System
#@ Arthur J. Bernstein;David Scott Gerstl;Wai-Hong Leung;Philip M. Lewis
#t 1998
#c 17

#index 464815
#* Ending the MOLAP/ROLAP Debate: Usage Based Aggregation and Flexible HOLAP (Abstract)
#@ Corey Salka
#t 1998
#c 17

#index 464816
#* Flattening an Object Algebra to Provide Performance
#@ Peter A. Boncz;Annita N. Wilschut;Martin L. Kersten
#t 1998
#c 17

#index 464817
#* A Graphical Editor for the Conceptual Design of Business Rules
#@ Peter Lang;Werner Obermair;W. Kraus;Thomas Thalhammer
#t 1998
#c 17

#index 464818
#* Performance Analysis of Parallel Hash Join Algorithms on a Distributed Shared Memory Machine: Implementation and Evaluation on HP Exemplar SPP 1600
#@ Miyuki Nakano;Hiroomi Imai;Masaru Kitsuregawa
#t 1998
#c 17

#index 464819
#* Global Integration of Visual Databases
#@ Wendy Chang;Deepak Murthy;Aidong Zhang;Tanveer Fathima Syeda-Mahmood
#t 1998
#c 17

#index 464820
#* Data Warehousing Lessons From Experience (Panel)
#@ Patrick E. O'Neil;Richard Winter;Clark D. French;Dan Crowley;William J. McKenna
#t 1998
#c 17

#index 464821
#* Building A Robust Workflow Management System With Persistent Queues and Stored Procedures
#@ Frank Leymann;Dieter Roller
#t 1998
#c 17

#index 464822
#* Mining for Strong Negative Associations in a Large Database of Customer Transactions
#@ Ashoka Savasere;Edward Omiecinski;Shamkant B. Navathe
#t 1998
#c 17

#index 464823
#* Persistent Applications Using Generalized Redo Recovery
#@ David B. Lomet
#t 1998
#c 17

#index 464824
#* A Tightly-Coupled Architecture for Data Mining
#@ Rosa Meo;Giuseppe Psaila;Stefano Ceri
#t 1998
#c 17

#index 464825
#* WebOQL: Restructuring Documents, Databases, and Webs
#@ Gustavo O. Arocena;Alberto O. Mendelzon
#t 1998
#c 17

#index 464826
#* Back to the Future: Dynamic Hierarchical Clustering
#@ Chendong Zou;Betty Salzberg;Rivka Ladin
#t 1998
#c 17

#index 464827
#* Query Processing in a Video Retrieval System
#@ King-Lup Liu;A. Prasad Sistla;Clement T. Yu;Naphtali Rishe
#t 1998
#c 17

#index 464828
#* ROL: A Prototype for Deductive and Object-Oriented Databases (Demo)
#@ Mengchi Liu;Weidong Yu;Min Guo;Riqiang Shan
#t 1998
#c 17

#index 464829
#* Employing Intelligent Agents for Knowledge Discovery (Abstract)
#@ Earl Stahl
#t 1998
#c 17

#index 464830
#* Distributed Video Presentations
#@ Eenjun Hwang;V. S. Subrahmanian;B. Prabhakaran
#t 1998
#c 17

#index 464831
#* Cost Models for Join Queries in Spatial Databases
#@ Yannis Theodoridis;Emmanuel Stefanakis;Timos K. Sellis
#t 1998
#c 17

#index 464832
#* DB-MAN: A Distributed Database System based on Database Migration in ATM Networks
#@ Takahiro Hara;Kaname Harumoto;Masahiko Tsukamoto;Shojiro Nishio
#t 1998
#c 17

#index 464833
#* The Active Hypermedia Delivery System (AHYDS) using the PHASME Application-Oriented DBMS
#@ Frédéric Andrès;Kinji Ono
#t 1998
#c 17

#index 464834
#* Industry Applications of Data Mining: Challenges & Opportunities (Abstract)
#@ Evangelos Simoudis
#t 1998
#c 17

#index 464835
#* The New Database Imperatives
#@ Goetz Graefe
#t 1998
#c 17

#index 464836
#* Failure Handling and Coordinated Execution of Concurrent Workflows
#@ Mohan Kamath;Krithi Ramamritham
#t 1998
#c 17

#index 464837
#* Efficient Discovery of Functional and Approximate Dependencies Using Partitions
#@ Ykä Huhtala;Juha Kärkkäinen;Pasi Porkka;Hannu Toivonen
#t 1998
#c 17

#index 464838
#* Network Latency Optimizations in Distributed Database Systems
#@ Sujata Banerjee;Panos K. Chrysanthis
#t 1998
#c 17

#index 464839
#* Cyclic Association Rules
#@ Banu Özden;Sridhar Ramaswamy;Abraham Silberschatz
#t 1998
#c 17

#index 464840
#* Concurrent Operations in a Distributed and Mobile Collaborative Environment
#@ Maher Suleiman;Michèle Cart;Jean Ferrié
#t 1998
#c 17

#index 464841
#* The LSDh-Tree: An Access Structure for Feature Vectors
#@ Andreas Henrich
#t 1998
#c 17

#index 464842
#* ZEBRA Image Access System
#@ Srilekha Mudumbai;Kshitij Shah;Amit P. Sheth;Krishnan Parasuraman;Clemens Bertram
#t 1998
#c 17

#index 464843
#* Compressing Relations and Indexes
#@ Jonathan Goldstein;Raghu Ramakrishnan;Uri Shaft
#t 1998
#c 17

#index 464844
#* Asynchronous Version Advancement in a Distributed Three-Version Database
#@ H. V. Jagadish;Inderpal Singh Mumick;Michael Rabinovich
#t 1998
#c 17

#index 464845
#* Data Logging: A Method for Efficient Data Updates in Constantly Active RAIDs
#@ Eran Gabber;Henry F. Korth
#t 1998
#c 17

#index 464846
#* Remote Load-Sensitive Caching for Multi-Server Database Systems
#@ Shivakumar Venkataraman;Jeffrey F. Naughton;Miron Livny
#t 1998
#c 17

#index 464847
#* Cost and Imprecision in Modeling the Position of Moving Objects
#@ Ouri Wolfson;Sam Chamberlain;Son Dao;Liqin Jiang;Gisela Mendez
#t 1998
#c 17

#index 464848
#* Cache Management for Mobile Databases: Design and Evaluation
#@ Boris Y. L. Chan;Antonio Si;Hong Va Leong
#t 1998
#c 17

#index 464849
#* Database Managed External File Update
#@ Neeraj Mittal;Hui-I Hsiao
#t 2001
#c 17

#index 464850
#* Selectivity Estimation for Spatial Joins
#@ Ning An;Zhen-Yu Yang;Anand Sivasubramaniam
#t 2001
#c 17

#index 464851
#* Variable Length Queries for Time Series Data
#@ Tamer Kahveci;Ambuj K. Singh
#t 2001
#c 17

#index 464852
#* The Nimble XML Data Integration System
#@ Denise Draper;Alon Y. Halevy;Daniel S. Weld
#t 2001
#c 17

#index 464853
#* Rewriting OLAP Queries Using Materialized Views and Dimension Hierarchies in Data Warehouses
#@ Chang-Sup Park;Myoung-Ho Kim;Yoon-Joon Lee
#t 2001
#c 17

#index 464854
#* A Split Operator for Now-Relative Bitemporal Databases
#@ Mikkel Agesen;Michael H. Böhlen;Lasse Poulsen;Kristian Torp
#t 2001
#c 17

#index 464855
#* High-level Parallelism in a Database Cluster: A Feasibility Study Using Document Services
#@ Torsten Grabs;Klemens Böhm;Hans-Jörg Schek
#t 2001
#c 17

#index 464983
#* SpinCircuit: A Collaboration Portal Powered by E-speak
#@ Rabindra Pathak
#t 2001
#c 17

#index 464984
#* Workflow and Process Synchronization with Interaction Expressions and Graphs
#@ Christian Heinlein
#t 2001
#c 17

#index 464985
#* Prefetching Based on Type-Level Access Pattern in Object-Relational DBMSs
#@ Wook-Shin Han;Yang-Sae Moon;Kyu-Young Whang;Il-Yeol Song
#t 2001
#c 17

#index 464986
#* Mining Partially Periodic Event Patterns with Unknown Periods
#@ Sheng Ma;Joseph L. Hellerstein
#t 2001
#c 17

#index 464987
#* B-Tree Indexes and CPU Caches
#@ Goetz Graefe;Per-Åke Larson
#t 2001
#c 17

#index 464988
#* Bringing the Internet to Your Database: Using SQLServer 2000 and XML to Build Loosely-Coupled Systems
#@ Michael Rys
#t 2001
#c 17

#index 464989
#* Mining Frequent Item Sets with Convertible Constraints
#@ Jian Pei;Jiawei Han;Laks V. S. Lakshmanan
#t 2001
#c 17

#index 464990
#* A Temporal Algebra for an ER-Based Temporal Data Model
#@ Jae Young Lee;Ramez Elmasri
#t 2001
#c 17

#index 464991
#* An Automated Change Detection Algorithm for HTML Documents Based on Semantic Hierarchies
#@ Seung Jin Lim;Yiu-Kai Ng
#t 2001
#c 17

#index 464992
#* Cache-Aware Query Routing in a Cluster of Databases
#@ Uwe Röhm;Klemens Böhm;Hans-Jörg Schek
#t 2001
#c 17

#index 464993
#* Infrasturucture for Web-based Application Integration
#@ Dieter Gawlick
#t 2001
#c 17

#index 464994
#* Duality-Based Subsequence Matching in Time-Series Databases
#@ Yang-Sae Moon;Kyu-Young Whang;Woong-Kee Loh
#t 2001
#c 17

#index 464995
#* TAR: Temporal Association Rules on Evolving Numerical Attributes
#@ Wei Wang;Jiong Yang;Richard R. Muntz
#t 2001
#c 17

#index 464996
#* PrefixSpan: Mining Sequential Patterns by Prefix-Projected Growth
#@ Jian Pei;Jiawei Han;Behzad Mortazavi-Asl;Helen Pinto;Qiming Chen;Umeshwar Dayal;Meichun Hsu
#t 2001
#c 17

#index 464997
#* Database Performance for Next Generation Telecommunications
#@ Munir Cochinwala
#t 2001
#c 17

#index 464998
#* Integrating Data Mining with SQL Databases: OLE DB for Data Mining
#@ Amir Netz;Surajit Chaudhuri;Usama M. Fayyad;Jeff Bernhardt
#t 2001
#c 17

#index 464999
#* SAP Business Information Warehouse - From Data Warehousing to an E-business Platform
#@ Thomas Zurek;Klaus Kreplin
#t 2001
#c 17

#index 465000
#* A Cost Model and Index Architecture for the Similarity Join
#@ Christian Böhm;Hans-Peter Kriegel
#t 2001
#c 17

#index 465001
#* Tuning an SQL-Based PDM System in a Worldwide Client/Server Environment
#@ Erich Müller;Peter Dadam;Jost Enderle;M. Feltes
#t 2001
#c 17

#index 465002
#* Efficient Sequenced Integrity Constraint Checking
#@ Wei Li;Richard Thomas Snodgrass;Shiyan Deng;Vineel Kumar Gattu;Aravindan Kasthurirangan
#t 2001
#c 17

#index 465003
#* MAFIA: A Maximal Frequent Itemset Algorithm for Transactional Databases
#@ Douglas Burdick;Manuel Calimlim;Johannes Gehrke
#t 2001
#c 17

#index 465004
#* Spatial Clustering in the Presence of Obstacles
#@ Anthony K. H. Tung;Jean Hou;Jiawei Han
#t 2001
#c 17

#index 465005
#* A Graph-Based Approach For Extracting Terminological Properties of Elements of XML Documents
#@ Luigi Palopoli;Giorgio Terracina;Domenico Ursino
#t 2001
#c 17

#index 465006
#* Tamino - A DBMS designed for XML
#@ Harald Schöning
#t 2001
#c 17

#index 465007
#* fAST Refresh using Mass Query Optimization
#@ Wolfgang Lehner;Roberta Cochrane;Hamid Pirahesh;Markos Zaharioudakis
#t 2001
#c 17

#index 465008
#* Bundles in Captivity: An Application of Superimposed Information
#@ Lois M. L. Delcambre;David Maier;Shawn Bowers;Mathew Weaver;Longxing Deng;Paul Gorman;Joan Ash;Mary Lavelle;Jason Lyman
#t 2001
#c 17

#index 465009
#* An Index Structure for Efficient Reverse Nearest Neighbor Queries
#@ Congjun Yang;King-Ip Lin
#t 2001
#c 17

#index 465010
#* Incremental Computation and Maintenance of Temporal Aggregates
#@ Jun Yang;Jennifer Widom
#t 2001
#c 17

#index 465011
#* B+ Tree Indexes with Hybrid Row Identifiers in Oracle 8i
#@ Eugene Inseok Chong;Souripriya Das;Chuck Freiwald;Jagannathan Srinivasan;Aravind Yalamanchi;Mahesh Jagannath;Anh-Tuan Tran;Ramkumar Krishnan
#t 2001
#c 17

#index 465012
#* An XML Indexing Structure with Relative Region Coordinate
#@ Dao Dinh Kha;Masatoshi Yoshikawa;Shunsuke Uemura
#t 2001
#c 17

#index 465013
#* Quality-Aware and Load-Sensitive Planning of Image Similarity Queries
#@ Klemens Böhm;Michael Mlivoncic;Roger Weber
#t 2001
#c 17

#index 465014
#* Approximate Nearest Neighbor Searching in Multimedia Databases
#@ Hakan Ferhatosmanoglu;Ertem Tuncel;Divyakant Agrawal;Amr El Abbadi
#t 2001
#c 17

#index 465015
#* High Dimensional Similarity Search With Space Filling Curves
#@ Swanwa Liao;Mario A. Lopez;Scott T. Leutenegger
#t 2001
#c 17

#index 465016
#* Processing Queries with Expensive Functions and Large Objects in Distributed Mediator Systems
#@ Luc Bouganim;Françoise Fabret;Fabio Porto;Patrick Valduriez
#t 2001
#c 17

#index 465017
#* Distinctiveness-Sensitive Nearest Neighbor Search for Efficient Similarity Retrieval of Multimedia Information
#@ Norio Katayama;Shin'ichi Satoh
#t 2001
#c 17

#index 465018
#* Counting Twig Matches in a Tree
#@ Zhiyuan Chen;H. V. Jagadish;Flip Korn;Nick Koudas;S. Muthukrishnan;Raymond T. Ng;Divesh Srivastava
#t 2001
#c 17

#index 465019
#* Differential Logging: A Commutative and Associative Logging Scheme for Highly Parallel Main Memory Databases
#@ Juchang Lee;Kihong Kim;Sang K. Cha
#t 2001
#c 17

#index 465020
#* IBM DB2 Everyplace: A Small Footprint Relational Database System
#@ Jonas S. Karlsson;Amrish Lal;T. Y. Cliff Leung;Thanh Pham
#t 2001
#c 17

#index 465021
#* The Importance of Extensible Database Systems for E-Commerce
#@ Samuel DeFazio;Ramkumar Krishnan;Jagannathan Srinivasan;Saydean Zeldin
#t 2001
#c 17

#index 465022
#* On Dual Mining: From Patterns to Circumstances, and Back
#@ Gösta Grahne;Laks V. S. Lakshmanan;Xiaohong Wang;Ming Hao Xie
#t 2001
#c 17

#index 465023
#* Measuring and Optimizing a System for Persistent Database Sessions
#@ Roger S. Barga;David B. Lomet
#t 2001
#c 17

#index 465024
#* Mobile Data Management: Challenges of Wireless and Offline Data Access
#@ Eric Gignere
#t 2001
#c 17

#index 465149
#* Efficient Bulk Deletes in Relational Databases
#@ Andreas Gärtner;Alfons Kemper;Donald Kossmann;Bernhard Zeller
#t 2001
#c 17

#index 465150
#* Dependable Computing in Virtual Laboratories
#@ Gustavo Alonso;Win Bausch;Cesare Pautasso;Ari Kahn;Michael T. Hallett
#t 2001
#c 17

#index 465151
#* Cache-on-Demand: Recycling with Certainty
#@ Kian-Lee Tan;Shen-Tat Goh;Beng Chin Ooi
#t 2001
#c 17

#index 465152
#* An Efficient Approximation Scheme for Data Mining Tasks
#@ George Kollios;Dimitrios Gunopulos;Nick Koudas;Stefan Berchtold
#t 2001
#c 17

#index 465153
#* Developing Web Service
#@ Adam Bosworth
#t 2001
#c 17

#index 465154
#* Data Management Support of Web Applications
#@ Daniel H. Fishman
#t 2001
#c 17

#index 465155
#* Querying XML Documents Made Easy: Nearest Concept Queries
#@ Albrecht Schmidt;Martin L. Kersten;Menzo Windhouwer
#t 2001
#c 17

#index 465156
#* CORBA Notification Service: Design Challenges and Scalable Solutions
#@ Robert E. Gruber;Balachander Krishnamurthy;Euthimios Panagos
#t 2001
#c 17

#index 465157
#* High-Performance, Space-Efficient, Automated Object Locking
#@ Laurent Daynès;Grzegorz Czajkowski
#t 2001
#c 17

#index 465158
#* XML Data and Object Databases: A Perfect Couple?
#@ Andreas Renner
#t 2001
#c 17

#index 465159
#* Model-Based Mediation with Domain Maps
#@ Bertram Ludäscher;Amarnath Gupta;Maryann E. Martone
#t 2001
#c 17

#index 465160
#* Similarity Search without Tears: The OMNI Family of All-purpose Access Methods
#@ Roberto F. Santos Filho;Agma J. M. Traina;Caetano Traina, Jr.;Christos Faloutsos
#t 2001
#c 17

#index 465161
#* Pseudo Column Level Locking
#@ Nagavamsi Ponnekanti
#t 2001
#c 17

#index 465162
#* Overcoming Limitations of Sampling for Aggregation Queries
#@ Surajit Chaudhuri;Gautam Das;Mayur Datar;Rajeev Motwani;Vivek R. Narasayya
#t 2001
#c 17

#index 465163
#* Discovery and Application of Check Constraints in DB2
#@ Jarek Gryz;K. Bernhard Schiefer;Jian Zheng;Calisto Zuzarte
#t 2001
#c 17

#index 465164
#* E-Business Applications for Supply Chain Automation: Challenges and Solutions
#@ Fabio Casati;Umeshwar Dayal;Ming-Chien Shan
#t 2001
#c 17

#index 465165
#* Using EELs, a Practical Approach to Outerjoin and Antijoin Reordering
#@ Jun Rao;Bruce G. Lindsay;Guy M. Lohman;Hamid Pirahesh;David E. Simmen
#t 2001
#c 17

#index 465166
#* Exactly-once Semantics in a Replicated Messaging System
#@ Yongqiang Huang;Hector Garcia-Molina
#t 2001
#c 17

#index 465167
#* The Skyline Operator
#@ Stephan Börzsönyi;Donald Kossmann;Konrad Stocker
#t 2001
#c 17

#index 465168
#* Microsoft Server Technology for Mobile and Wireless Applications
#@ Praveen Seshadri
#t 2001
#c 17

#index 465169
#* Block Oriented Processing of Relational Database Operations in Modern Computer Architectures
#@ Sriram Padmanabhan;Timothy Malkemus;Ramesh C. Agarwal;Anant Jhingran
#t 2001
#c 17

#index 465170
#* The MD-join: An Operator for Complex OLAP
#@ Damianos Chatziantoniou;Michael O. Akinde;Theodore Johnson;Samuel Kim
#t 2001
#c 17

#index 564237
#* Universal Access versus Universal Storage
#@ William Baker
#t 1997
#c 17

#index 564239
#* Pinwheel Scheduling for Fault-Tolerant Broadcast Disks in Real-time Database Systems
#@ Sanjoy K. Baruah;Azer Bestavros
#t 1997
#c 17
#! The design of programs for broadcast disks which incorporate real-time and fault-tolerance requirements is considered. A generalized model for real-time fault-tolerant broadcast disks is defined. It is shown that designing programs for broadcast disks specified in this model is closely related to the scheduling of pinwheel task systems. Some new results in pinwheel scheduling theory are derived, which facilitate the efficient generation of real-time fault-tolerant broadcast disk programs.

#index 564240
#* ECA Rule Support for Distributed Heterogeneous Environments
#@ Sharma Chakravarthy;Roger Le
#t 1998
#c 17

#index 564253
#* A Directory Service for a Federation of CIM Databases with Migrating Objects
#@ Ajit K. Patankar;Arie Segev;J. George Shanthikumar
#t 1996
#c 17
#! We propose a novel directory scheme for a large federation of databases where object migration is in response to manufacturing events. In our directory scheme, objects report their location to a directory server instead of the traditional method of the directory servers polling sites in the network. The directory is distributed among multiple servers to avoid bottleneck during query processing. A distributed Linear Hashing algorithm is proposed for efficiently determining an appropriate server for an object. Finally, a stochastic dynamic programming model is proposed for minimizing the number of database transactions.

#index 564263
#* An Index-Based Approach for Similarity Search Supporting Time Warping in Large Sequence Databases
#@ Sang-Wook Kim;Sanghyun Park;Wesley W. Chu
#t 2001
#c 17

#index 565183
#* Title, General Chairs' Message, Program Chairs' Message, In Memoriam, Committees, Referees, Author Index
#@ 
#t 1996
#c 17

#index 565464
#* ActionWorkflow in Use: Clark County Department of Business License
#@ Raul Medina-Mora;Kelly W. Cartron
#t 1996
#c 17
#! In this paper we present the basic concepts of ActionWorkflow and a study of a successful implementation in Clark County Department of Business License. The Image/Workflow System reengineers a labyrinthine licensing system into simplistic processes that are more customer oriented, yield superior productivity, establish a work-in-progress tracking mechanism, and archive the resulting licensing processes permanently on an unalterable optical storage system.

#index 565465
#* W3QS - A System for WWW Querying
#@ David Konopnicki;Oded Shmueli
#t 1997
#c 17
#! W3QL is a, SQL-like, high level language for accessing World-Wide Web (WWW) resident data and services. W3QL is declarative. A W3QL query specifies a graph to be matched with portions of the WWW (graph nodes corresponding to WWW pages, edges to hypertext links). A query can specify complex conditions on nodes' contents and their relationships. A W3QL query may use existing search services (e.g. AltaVista). W3QL is extensible as users may use their own data analysis tools (e.g. image analysis). W3QS is a system that manages W3QS queries. W3QS is accessible via the WWW or by using a programming based interface (API). On the WWW, W3QS provides several interfaces: intuitive graphic interfaces, templates of frequently posed queries, and direct programming.

#index 565466
#* Relational Joins for Data on Tertiary Storage
#@ Jussi Myllymaki;Miron Livny
#t 1997
#c 17
#! Despite the steady decrease in secondary storage prices, the data storage requirements of many organizations cannot be met economically using secondary storage alone. Tertiary storage offers a lower-cost alternative but is viewed as a second-class citizen in many systems. For instance, the typical solution in bringing tertiary-resident data under the control of a DBMS is to use operating system facilities to copy the data to secondary storage, and then to perform query optimization and execution as if the data had been in secondary storage all along. This approach fails to recognize the opportunities for saving execution time and storage space if the data were accessed directly on tertiary devices and in parallel with other I/Os. In this paper we explore how to join two DBMS relations stored on magnetic tapes. Both relations are assumed to be larger than available disk space. We show how Grace Hash Join can be modified to handle a range of tape relation sizes. The modified algorithms access data directly on tapes and exploit parallelism between disk and tape I/Os. We also provide performance results of an experimental implementation of the algorithms.

#index 565467
#* ROCK & ROLL: A Deductive Object-Oriented Database with Active and Spatial Extensions
#@ Andrew Dinn;M. Howard Williams;Norman W. Paton
#t 1997
#c 17
#! ROCK & ROLL is a deductive object-oriented database system that supports two languages, one imperative and the other deductive, both derived from the same object-oriented data model. As the languages share a common type system, they can be integrated without manifesting impedance mismatches, and thus programmers can conveniently exploit both deductive and imperative features in a single application. The basic ROCK & ROLL system provides comprehensive modelling and programming facilities, but recent work has extended it with both active rules and spatial data types, thereby demonstrating how the core design is amenable to extensions in its behavioural and structural facilities.

#index 565468
#* SEOF: An Adaptable Object Prefetch Policy for Object-Oriented Database Systems
#@ Jung-Ho Ahn;Hyoung-Joo Kim
#t 1997
#c 17
#! The performance of object access can be drastically improved by efficient object prefetch. In this paper we present a new object prefetch policy, Selective Eager Object Fetch(SEOF) which prefetches objects only from selected candidate pages without using any high level object semantics. Our policy considers both the correlations and the frequencies of fetching objects. Unlike existing prefetch policies, this policy utilizes the memory and the swap space of clients efficiently without resource exhaustion. Furthermore, the proposed policy has good adaptability to both the effectiveness of clustering and database size. We show the performance of the proposed policy through experiments over various multi-client system configurations.

#index 565469
#* Redbrick Vista: Aggregate Computation and Management
#@ Latha S. Colby;Richard L. Cole;Edward Haslam;Nasi Jazayeri;Galt Johnson;William J. McKenna;Lee Schumacher;David Wilhite
#t 1998
#c 17

#index 565470
#* Leveraging Mediator Cost Models with Heterogeneous Data Sources
#@ Hubert Naacke;Georges Gardarin;Anthony Tomasic
#t 1998
#c 17

#index 565471
#* Outstanding Challenges in OLAP
#@ Jeffrey A. Bedell
#t 1998
#c 17

#index 565472
#* Inter-Enterprise Collaborative Business Process Management
#@ Qiming Chen;Meichun Hsu
#t 2001
#c 17

#index 565473
#* Integrating Semi-Join-Reducers into State of the Art Query Processors
#@ Konrad Stocker;Donald Kossmann;Reinhard Braumandl;Alfons Kemper
#t 2001
#c 17

#index 631010
#* Proceedings of the 15th International Conference on Data Engineering
#@ 
#t 1999
#c 17

#index 631011
#* Proceedings of the 16th International Conference on Data Engineering
#@ 
#t 2000
#c 17

#index 631917
#* Data Integration by Describing Sources with Constraint Databases
#@ 
#t 1999
#c 17
#! We develop a data integration approach for the efficient evaluation of queries over autonomous source databases. The approach is based on some novel applications and extensions of constraint databases techniques. We assume the existence of a global database schema. The contents of each data source are described using a set of constraint tuples over the global schema; each such tuple indicates possible contributions from the source. The "source description catalog" (SDC) of a global relation consists of its associated constraint tuples. Such a way of description is advantageous since it is flexible to add new sources and to modify existing ones. In our framework, to evaluate a conjunctive query over the global schema, a plan generator first identifies relevant data sources by "evaluating" the query against the SDCs using techniques of constraint query evaluation; it then formulates an evaluation plan, consisting of some specialized queries over different paths. The evaluation of a query associated with a path is done by a sequence of partial evaluations at data sources along the path, similar to side-ways information passing of Datalog; the partially evaluated queries travel along their associated paths. Our SDC-based query planning is efficient since it avoids the NP-complete query rewriting process. We can achieve further optimization using techniques such as emptiness test.

#index 631918
#* Heterogeneous Query Processing through SQL Table Functions
#@ Ganapathy Krishnamoorthy
#t 1999
#c 17
#! In today's IT infrastructures, data is stored in SQL databases, non-SQL databases, and host databases like ISAM/VSAM files. Non-SQL databases are specialized data stores controlled by applications like spreadsheets, mail, directory and index services. Developing applications accessing a variety of different data sources is challenging for application developers due to different environments, APIs, bindings, etc. 20 years ago, SQL was created to ease the life of database application developers and provide a uniform way for accessing data which is stored in SQL databases. This paper describes an implementation of table functions and its usage for accessing data stored outside SQL databases in diverse external data stores. Table functions are compliant with the relational data model, and therefore fit into the well-established SQL language. The table function architecture is open, and allows the deployment of generic data access infrastructures such as Microsoft's OLE DB or Java's JDBC. This paper describes a prototype implementation of OLE DB table functions with advanced query optimization techniques. The prototype is based on IBM DB2 UDB relational database system.

#index 631919
#* Semantic Brokering over Dynamic Heterogeneous Data Sources in InfoSleuth(tm)
#@ 
#t 1999
#c 17
#! InfoSleuth is an agent-based system for information discovery and retrieval in a dynamic, open environment. This paper discusses InfoSleuth's multibroker design and implementation. InfoSleuth's brokering function combines reasoning over both the syntax and semantics of agents in the domain. The broker must reason over explicitly advertised information about agent capabilities to determine which agent can best provide the requested services. Brokering in InfoSleuth is a match-making process, recommending agents that provide services to agents requesting services. Robustness and scalability issues dictate that brokering must be distributable across collaborating processes. Our multibroker design is a peer-to-peer system that requires brokers to advertise to and receive advertisements from other brokers. Brokers collaborate during matchmaking to give a collective response to requests initiated by non-broker agents. This results in a robust, scalable brokering system.

#index 631920
#* On Similarity-Based Queries for Time Series Data
#@ 
#t 1999
#c 17
#! We study similarity queries for time series data where similarity is defined in terms of a set of linear transformations on the Fourier series representation of a sequence. We have shown in an earlier work that this set of transformations is rich enough to formulate operations such as moving average and time scaling.In this paper, we present a new algorithm for processing queries that define similarity in terms of multiple transformations instead of a single one. The idea is, instead of searching the index multiple times and each time applying a single transformation, to search the index only once and apply a collection of transformations simultaneously to the index. Our experimental results on both synthetic and real data show that the new algorithm for simultaneously processing multiple transformations is much faster than sequential scanning or index traversal using one transformation at a time. We also examine the possibility of composing transformations in a query or of rewriting a query expression such that the resulting query can be efficiently evaluated.

#index 631921
#* Universal Temporal Extensions for Database Languages
#@ 
#t 1999
#c 17
#! Temporal reasoning and temporal query languages present difficult research problems of theoretical interest and practical importance. One problem is the chasm between point-based temporal reasoning and interval- based reasoning. Another problem is the lack of robustness and universality in many proposed solutions, whereby temporal extensions designed for one language cannot be easily applied to other query languages e.g., extensions proposed for SQL cannot be applied to QBE or Datalog. In this paper, we provide a simple solution to both problems by observing that all query languages support (i) single-value based reasoning and (ii) aggregate-based reasoning, and then showing that these two modalities can be naturally extended to support, respectively, point-based and interval-based temporal queries. We follow TSQL2 insofar as practical requirements are concerned, and show that its functionality can be captured by simpler constructs which can be applied uniformly to Datalog, QBE and SQL. Then, we show that an efficient implementation can be achieved by mapping into a different storage representation, and discuss a prototype built along these lines using the LDL++ system with extended aggregates.

#index 631922
#* Parallel Algorithms for Computing Temporal Aggregates
#@ Bruce C. Huang
#t 1999
#c 17
#! The ability to model the temporal dimension is essential to many applications. Furthermore, the rate of increase in database size and response time requirements has out-paced advancements in processor and mass storage technology, leading to the need for parallel temporal database management systems. In this paper, we introduce a variety of parallel temporal aggregation algorithms for a shared-nothing architecture based on the sequential Aggregation Tree algorithm. Via an empirical study, we found that the number of processing nodes, the partitioning of the data, the placement of results, and the degree of data reduction effected by the aggregation impacted the performance of the algorithms. For distributed results placement, we discovered that Time Division Merge was the obvious choice. For centralized results and high data reduction, Pairwise Merge was preferred regardless of the number of processing nodes, but for low data reduction, it only performed well up to 32 nodes. This led us to a centralized variant of Time Division Merge which was best for larger configurations having low data reduction.

#index 631923
#* Efficient Time Series Matching by Wavelets
#@ 
#t 1999
#c 17
#! Time series stored as feature vectors can be indexed by multi-dimensional index trees like R-Tree for fast retrieval. Due to the dimensionality curse problem, transformations are applied to time series to reduce the number of dimensions of the feature vectors. Different transformations like Discrete Fourier Transform (DFT), Discrete Wavelet Transform (DWT), Karhunen-Loeve (K-L) transform or Singular Value Decomposition (SVD) can be applied. While the use of DFT and K-L transform or SVD have been studied in the literature, to our knowledge, there is no in-depth study on the application of DWT. In this paper, we propose to use Haar Wavelet Transform for time series indexing. The major contributions are: (1) we show that Euclidean distance is preserved in the Haar transformed domain and no false dismissal will occur in range query, (2) we show that Haar transform can outperform DFT through experiments, (3) a new similarity model is suggested to accommodate vertical shift of time series, and (4) a two-phase method is proposed for efficient n-nearest neighbor query in time series databases.

#index 631924
#* STING+: An Approach to Active Spatial Data Mining
#@ 
#t 1999
#c 17
#! Spatial data mining presents new challenges due to the large size of spatial data, the complexity of spatial data types, and the special nature of spatial access methods. Most research in this area has focused on efficient query processing of static data. This paper introduces an active spatial data mining approach which extends the current spatial data mining algorithms to efficiently support user-defined triggers on dynamically evolving spatial data. To exploit the locality of the effect of an update and the nature of spatial data, we employ a hierarchical structure with associated statistical information at the various levels of the hierarchy and decompose the user-defined trigger into a set of sub-triggers associated with cells in the hierarchy. Updates are suspended in the hierarchy until their cumulative effect might cause the trigger to fire. It is shown that this approach achieves three orders of magnitude improvement over the naive approach that re-evaluates the condition over the database for each update, while both approaches produce the same result without any delay. Moreover, this scheme can support incremental query processing as well.

#index 631925
#* Multidimensional Data Modeling for Complex Data
#@ 
#t 1999
#c 17

#index 631926
#* Efficient Mining of Partial Periodic Patterns in Time Series Database
#@ 
#t 1999
#c 17
#! Partial periodicity search, i.e., search for partial periodic patterns in time-series databases, is an interesting data mining problem. Previous studies on periodicity search mainly consider finding full periodic patterns, where every point in time contributes (precisely or approximately) to the periodicity. However, partial periodicity is very common in practice since it is more likely that only some of the time episodes may exhibit periodic patterns.We present several algorithms for efficient mining of partial periodic patterns, by exploring some interesting properties related to partial periodicity, such as the Apriori property and the max-subpattern hit set property, and by shared mining of multiple periods. The max-subpattern hit set property is a vital new property which allows us to derive the counts of all frequent patterns from a relatively small subset of patterns existing in the time series. We show that mining partial periodicity needs only two scans over the time series database, even for mining multiple periods. The performance study shows our proposed methods are very efficient in mining long periodic patterns.

#index 631927
#* Query Routing in Large-Scale Digital Library Systems
#@ 
#t 1999
#c 17
#! Modern digital libraries require user-friendly and yet responsive access to the rapidly growing, heterogeneous, and distributed collection of information sources. The increasing volume and diversity of digital information available online have led to a growing problem that conventional data management systems do not have, namely finding which information sources out of many candidate choices are the most relevant to answer a given user query. We refer to this problem as the query routing problem. In this paper we introduce the notation and issues of query routing, and present a practical solution for designing a scalable query routing system based on multi-level progressive pruning strategies. The key idea is to create and maintain user query profiles and source capability profiles independently, and to provide algorithms that can dynamically discover relevant information sources for a given query through the smart use of user query profiles and source capability profiles, including the mechanisms for interleaving query routing with query parallelization and query execution process to continue the pruning at run-time. Comparing with the keyword-based indexing techniques adopted in most of the search engines and software, our approach offers fine-granularity of interest matching, thus it is more powerful and effective for handling queries with complex conditions.

#index 631928
#* Enhancing Semistructured Data Mediators with Document Type Definitions
#@ 
#t 1999
#c 17
#! Mediation is an important application of XML. The MIX mediator uses Document Type Definitions (DTDs) to assist the user in query formulation and query processors in running queries more efficiently.We provide an algorithm for inferring the view DTD from the view definition and the source DTDs. We develop a metric of the quality of the inference algorithm's view DTD by formalizing the notions of soundness and tightness. Intuitively, tightness is similar to precision, i.e., it deteriorates when "many" objects described by the view DTD can never appear as content of the view.In addition we show that DTDs have some inherent deficiencies that prevent the development of tight DTDs. We propose "DTDs with specialization" as a way to resolve this problem.

#index 631929
#* Data Mining from an Al Perspective
#@ 
#t 1999
#c 17
#! Data Mining, or Knowledge Discovery in Databases as it is also called, is claimed as an offspring by three disciplines: databases, statistics, and the machine learning subfield of artificial intelligence. (The term originated in statistics with distinctly pejorative overtones -data mining was characterized as fossicking in data without a guiding model.) Statistics is obviously relevant because that field has always focused on construction of models from data. Databases, too, is clearly central because current applications of data mining can involve very large corpora of information that are not necessarily in flat file form. So what' s left to be claimed by artificial intelligence and, in particular, machine learning? This talk will provide a definitely non-impartial answer to this question from the standpoint of a long-time ML practitioner.

#index 631930
#* Document Warehousing Based on a Multimedia Database System
#@ 
#t 1999
#c 17
#! Nowadays, structured data such as sales and business forms are stored in data warehouses for decision makers to use. Further, unstructured data such as emails, html texts, images, videos, and oftIce documents are increasingly accumulated in personal computer storage due to spread of mailing, Www, and word processing. Such unstructured data, or what we call multimedia documents, are larger in volume than structured data and precious as corporate assets as well. So we need a document warehouse as a software framework where multimedia documents are analyzed and managed for corporate-wide information sharing and reuse like a data warehouse for structured data. We describe a prototype document warehouse system, which supports management of simple and compound documents, keyword-based and content-based retrieval, rule-based classification, SOM-based clustering, and XML data query and view rules.

#index 631931
#* Using XML in Relational Database Applications
#@ 
#t 1999
#c 17
#! This talk will review relational database XML features and will describe their use in database applications. Aspects that will be considered include the creation, validation, transformation, storage and retrieval of XML documents, the inclusion of existing and new relational data in XML documents, and the impact of XML Links.

#index 631932
#* Multiversion Reconciliation for Mobile Databases
#@ 
#t 1999
#c 17
#! As mobile computing devices become more and more popular, mobile databases have started gaining popularity. An important feature of these database is their ability to allow optimistic replication of data by providing disconnected mobile devices the ability to perform local updates. The fundamental problem in this approach is the reconciliation problem, i.e. the problem of serializing potentially conflicting updates by disconnected clients on all copies of the database. In this paper we introduce a new algorithm that combines multiversion concurrency control schemes on a server with reconciliation of updates from disconnected clients. The scheme generalizes to multiversion systems, the single version optimistic method of reconciliation, in which client transactions are allowed to commit on the server iff data items in their read sets are not updated on the server after replication.

#index 631933
#* A Graph Query Language and Its Query Processing
#@ 
#t 1999
#c 17
#! Many new database applications involve querying of graph data. In this paper, we present an object-oriented graph data model, and an OQL like graph query language, GOQL. The data model and the language are illustrated in the application domain of multimedia presentation graphs. We then discuss the query processing techniques for GOQL, more specifically, the translation of GOQL into an operator-based language, called O-Algebra, extended with operators to deal with paths and sequences. We also discuss different approaches for efficient implementation of algebra operators for paths and sequences.

#index 631934
#* Processing Operations with Restrictions in RDBMS without External Sorting: The Tetris Algorithm
#@ 
#t 1999
#c 17
#! Most operations of the relational algebra or SQL require a sorted stream of tuples for efficient processing. Therefore, processing complex relational queries relies on efficient access to a table in some sort order. In principle, indexes could be used, but they are superior to a full table scan only, if the result set is sufficiently restricted in the index attribute. In this paper we present the Tetris algorithm, which utilizes restrictions to process a table in sort order of any attribute without the need of external sorting. The algorithm relies on the space partitioning of a multidimensional access method. A sweep line technique is used to read data in sort order of any attribute, while accessing each disk page of a table only once. Results are produced earlier than with traditional sorting techniques, allowing better response times for interactive applications and pipelined processing of the result set. We describe a prototype implementation of the Tetris algorithm using UB-Trees on top of Oracle 8, define a cost model and present performance measurements for some queries of the TPC-D benchmark.

#index 631935
#* Fast Approximate Query Answering Using Precomputed Statistics
#@ 
#t 1999
#c 17
#! The last few years have witnessed a significant increase in the use of databases for complex data analysis (OLAP) applications. These applications often require very quick responses from the DBMS. However, they also involve complex queries on large volumes of data. Despite significant improvement in database support for OLAP over the last few years, most DBMSs still fall short of providing quick enough responses. In this paper we present a novel solution to this problem: we use small amounts of precomputed summary statistics of the data to answer the queries quickly, albeit approximately. Our hypothesis is that many OLAP applications can tolerate approximations in query results in return for huge response time reductions. This work is part of our efforts to build an efficient data analysis system called AQUA. Next, we describe some of the technical problems we addressed in this effort.

#index 631936
#* Hash in Place with Memory Shifting: Datacube Computation Revisited
#@ 
#t 1999
#c 17
#! A datacube on n attributes requires the computation of an aggregation function over all groups generated by 2**n interrelated GROUP-BYs. Even n is not very large, the computation could be very expensive if the database involved is large. Although a number of algorithms with various optimization techniques have been proposed, accurate estimation of memory requirement and efficient use of the available memory remain difficult issues. In this paper, we present a new approach for datacube computation. The approach uses the available memory to maintain a minimum number of hash tables for computing related cuboids and manages memory dynamically by shifting memory pages among hash tables. Therefore, no memory requirement estimation is necessary and all memory available can be fully utilized.

#index 631937
#* The Bulk Index Join: A Generic Approach to Processing Non-Equijoins
#@ 
#t 1999
#c 17
#! Efficient join algorithms have been developed for processing different types of non-equijoins like spatial join, band join, temporal join or similarity join. Each of these algorithms is tailor-cut for a specific type of join, and a generalization of these algorithms to other join types is not obvious. We present an efficient algorithm called bulk index join that can be easily applied to a broad class of non-equijoins. Similar to the well-known index nested-loops join algorithm, the bulk index join probes the records of the outer relation against the inner relation by using a preexisting tree-based index structure. In order to support the index lookups efficiently, the nodes of the tree are visited in a top-down fashion. For each node, all of its assigned queries are distributed among its qualifying child nodes in bulk. The implementation of our algorithm only requires a small set of routines generally available in tree-based index structures. In our experiments, the so-called band join serves as an example. It is shown that probing in bulk reduces the I/O cost of the index nested-loops join up to two orders of magnitude.

#index 631938
#* Speeding up Heterogeneous Data Access by Converting and Pushing Down String Comparisons
#@ 
#t 1999
#c 17
#! Heterogeneous data access is becoming an important feature in database systems because it makes it possible to integrate information from a variety of data sources. The performance of queries involving external data sources is greatly affected by the cost of transporting data over the network. Pushing down subqueries, particularly selections and projections, to external data sources can reduce this cost. However, if the external data source uses a different collating sequence, the semantics of predicates involving string comparisons differ at the two site so such predicates cannot be pushed down unchanged. Our work is focused on how to convert predicates containing a comparison of a column with a string constant so that they can still be pushed down to the external data source.

#index 631939
#* Data Warehouse Evolution: Trade-offs between Quality and Cost of Query Rewritings
#@ 
#t 1999
#c 17
#! The problem of rewriting queries has been heavily explored in recent years, including in work on query processing and optimization, semantic query refinement in decentralized environments, the rewriting of queries using views, and view maintenance. Previous work has made the restricting assumption that the rewritten query must be {\em equivalent} to the initially given query. We now propose to relax this assumption to allow for query rewriting in situations where {\em equivalent} rewritings may not exist -- yet alternate {\em not necessarily equivalent} query rewritings may still be preferable to users over not receiving any answers at all. In this paper, we apply this concept of non-equivalent query rewriting to the problem of maintaining view definitions (data warehouses), where it now allows us to handle a much larger class of changes of the underlying information sources, namely not only data but also schema changes. This relaxed query notion allows for more diversity in acceptable query answers, but also raises a new issue of the evaluation of such queries. For this purpose, we introduce an analytical model of query rewritings that incorporates measures of information preservation (quality) of a query in addition to the commonly studied measures of view maintenance performance (query cost). Quality is modeled as a function of the divergence from the intended view extent, both in terms of the preservation of the information amount and the information type. Both quality and cost are integrated into one uniform model, called the Quality-Cost-Model or QC-Model, to allow for a trade-off among these two measures. This model can be used to compare two alternate (even if not equivalent) rewritings, and thus to automatically select a good view maintenance solution from among numerous non-equivalent rewritings. To our best knowledge, this is the first work that deals with this novel issue, the {\it non-equivalent query rewriting problem}.

#index 631940
#* Multiple Index Structures for Efficient Retrieval of 2D Objects
#@ Hezhi Ai
#t 1999
#c 17
#! For efficient retrieval of 2D objects by shape, we propose three index structures on features that are extracted from the objects' minimum bounding circles (MBC). Besides traditional applications (e.g., CAD/CAM and Trademark registry), new multimedia applications such as structured video, animation, and MPEG-4 standard require the storage and management of well-defined objects. A sample query type in these applications is to find objects that are similar to or exactly match a given object. Due to large size of these multimedia databases and the complexity of the 2D object comparison algorithms, appropriate indexing techniques in order to invoke the comparison algorithms for only a subset of the stored objects is crucial.In this paper, by using MBC of an object, we identify six common features for that object such as the center coordinate and the radius of its MBC. The other four features are based on the vertices of the object that are on its MBC, termed touch points. A major observation is that those features are unique per object and can be utilized to filter out non-similar candidates. By utilizing these features in three alternative index structures, we guarantee no false drops for our similarity searches while reducing the number of false hits. Finally, we propose variations to known comparison algorithms, again based on our identified features, in order to eliminate false hits. To evaluate our techniques, we conducted a simulation study on a database of 2D objects. The results show the superiority of our techniques as compared to a naive indexing which was based on the number of vertices of the objects (at least 40% improvement in I/O cost). We also identify one of the indexing structures as the superior one, independent of the size of the database and the number of vertices of the objects.

#index 631941
#* Data Warehouse Evolution: Trade-offs between Quality and Cost of Query Rewritings
#@ 
#t 1999
#c 17
#! Query rewriting has been used as a query optimization technique for several decades to reduce the computational cost of a query. It has generally been assumed that any rewritten query will generate the identical query result as the original query, in terms of both the query interface and the query extent. Hence, this is called "equivalent query rewriting".Recently, query rewriting with relaxed semantics has been proposed as a means of retaining the validity of a data warehouse (i.e., materialized queries) in a changing envi-ronment [2, 3, 4]. Attributes in the query interface can be classified as essential or dispensable (if it cannot be retained) according to the query definer's preferences. Simi-larly, preferences for query extent can be specified, for example, to indicate whether a subset of the original result is acceptable or not. A query rewriting is said to be acceptable if it preserves the essential information of the original query and satisfies the constraint on the view extent. Since each rewriting may preserve the original query to a different degree, a potentially large number of acceptable yet non-equivalent query rewritings may be found. Therefore, we need to systematically select the most promising rewriting out of all possible ones. Research issues that must be an-swered for solving this problem are outlined below.We have found that the two most important factors influencing the desirability of a query rewriting are: the information preserved by the rewriting w.r.t. the original query result (quality) and the cost of acquiring the query results (cost). A rewriting is more desired than others if it is "semantically close" to the original one and could be acquired economically. We have designed the first analytic model, Quality-Cost Model (QC Model), to assess rewritings on both factors [1].A rewriting has a better quality in terms of the query interface if it retains more dispensable attributes than others. A rewriting is superior in extent to others if it preserves more of the original extent without introducing surplus tuples.The percentage of the tuples preserved by a rewriting is computed by the overlapping query extents between the rewriting and the original query. Estimation of the over-lapping extents before rewritings are actually computed is a research issue [1].Since data content changes are more frequent than schema changes at the ISs, we propose to use the long-term (incremental) view maintenance cost as our indicator for costs. The view maintenance cost is composed of three factors, namely the number of exchanged messages between the information space and the data warehouse, the number of bytes of data transferred between these two sites, and the number of I/Os performed by the external ISs in order to process incremental view maintenance. We have run experimental studies to assess trade-offs between these three factors in a real environment [1].The quality factor is then combined with the cost factor to decide the overall ranking of a rewriting. To our best knowledge, this is the first work that deals with this novel issue, the non-equivalent query rewriting problem.References [1] A. J. Lee, A. Koeller, A. Nica, and E. A. Rundensteiner. Data Warehouse Evolution: Trade-offs between Quality and Cost. Technical Report WPI-CS-TR-98-2, WPI, 1998. [2] A. J. Lee, A. Nica, and E. A. Rundensteiner. Keeping Virtual Information Resources Up and Running. In Proceedings of IBM Centre for Advanced Studies Conference CASCON'97, Best Paper Award, pp 1-14, November 1997. [3] A. Nica, A. J. Lee, and E. A. Rundensteiner. The CVS Al-gorithm for View Synchronization in Evolvable Large-Scale Information Systems. EDBT'98, pp 359-373. [4] E. A. Rundensteiner, A. J. Lee, and A. Nica. On Preserving Views in Evolving Environments. KRDB'97, pp 13.1-13.11.

#index 631942
#* Declarative and Procedural Object-Oriented Views
#@ 
#t 1999
#c 17
#! One major approach to realise database integration is to adapt and merge the database schemas by defining views. When integrating object-oriented databases, views need to adequately support the two main concepts of object-oriented data models: object identity and methods. View objects need to be identified on the basis of the objects they have been derived from. Methods require an efficient language binding and need to be substituted by declarative query mappings where possible. In this paper we present a view system that supports both declarative and procedural integration of object-oriented databases. We have extended the object definition language ODL of ODMG-93 and use simple OQL queries for instantiating the extents of derived classes and for performing simple attribute derivations. In addition, methods can be attached to view objects in order to provide more complex view semantics. A pre-processor is employed for consistently generating the declarative and the procedural parts from a common source, and an object manager has been designed that interacts with both the query processor and the view implementation and guarantees consistent object identification and method dispatching at run-time. The presented view concept provides flexible integration semantics for object-oriented databases without sacrificing the optimisation potential.

#index 631943
#* Fast Approximate Search Algorithm for Nearest Neighbor Queries in High Dimensions
#@ 
#t 1999
#c 17
#! In this paper, we present a fast approximate nearest neighbor (NN) search index structure called the AB-tree, which uses heuristics to decide whether or not to access a node in the index tree based on the intersecting angle and the weight of the node. The goal of the NN search algorithm presented in this paper is to decrease unnecessary node accesses in the search due to overlap among bounding regions in existing index structures. We have observed the following three properties of bounding hyperspheres that motivated the creation of the proposed heuristics for NN search.

#index 631944
#* Exploiting Data Lineage for Parallel Optimization in Extensible DBMSs
#@ 
#t 1999
#c 17
#! Accommodation of complex user-defined data types and operations, and high query performance are among the most important requirements of advanced information systems. They together strongly suggest the need for extensibility and non-relational query support in parallel geoscientific query optimization, which have been recognized as open research problems. Parallelizing a query execution plan involves determining how input data streams to evaluators implementing logical operations can be divided to be processed by clones of the same evaluator in parallel. The fine-grained data lineage characteristics of user-defined evaluators dictates how they can be parallelized, and hence have to be systematically captured and made available to the parallel query optimizer. In this paper, we introduce a model for characterizing the data lineage properties of user-defined evaluators in an extensible DBMS, and describe a parallel query processing system that exploits such data lineage characteristics for parallel query optimization. We have demonstrated and validated our system with applications such as geoscientific data mining.

#index 631945
#* Algorithms for Index-Assisted Selectivity Estimation
#@ 
#t 1999
#c 17
#! The standard mechanisms for query selectivity estimation used in relational database systems rely on properties specific to the attribute types. The query optimizer in an extensible database system will, in general, be unable to exploit these mechanisms for user-defined types, forcing the database extender to invent new estimation mechanisms. In this work, we discuss extensions to the generalized search tree, or GiST, that simplify the creation of user-defined selectivity estimation methods. An experimental comparison of such methods with multidimensional estimators from the literature has demonstrated very competitive results.

#index 631946
#* Maintaining Data Cubes under Dimension Updates
#@ 
#t 1999
#c 17
#! OLAP systems support data analysis through a multidimensional data model, according to which data facts are viewed as points in a space of application-related "dimensions", organized into levels which conform a hierarchy. The usual assumption is that the data points reflect the dynamic aspect of the data warehouse, while dimensions are relatively static. However, in practice, dimension updates are often necessary to adapt the multidimensional database to changing requirements. Structural updates can also take place, like addition of categories or modification of the hierarchical structure. When these updates are performed, the materialized aggregate views that are typically stored in OLAP systems must be efficiently maintained. These updates are poorly supported (or not supported at all) in current commercial systems, and have received little attention in the research literature. We present a formal model of dimension updates in a multidimensional model, a collection of primitive operators to perform them, and a study of the effect of these updates on a class of materialized views, giving an algorithm to efficiently maintain them.

#index 631947
#* Relative Prefix Sums: An Efficient Approach for Querying Dynamic OLAP Data Cubes
#@ 
#t 1999
#c 17
#! Range sum queries on data cubes are a powerful tool for analysis. A range sum query applies an aggregation operation (e.g., SUM) over all selected cells in a data cube, where the selection is specified by providing ranges of values for numeric dimensions. Many application domains require that information provided by analysis tools be current or "near-current." Existing techniques for range sum queries on data cubes, however, can incur update costs on the order of the size of the data cube. Since the size of a data cube is exponential in the number of its dimensions, rebuilding the entire data cube can be very costly. We present an approach that achieves constant time range sum queries while constraining update costs. Our method reduces the overall complexity of the range sum problem.

#index 631948
#* Optimizer and Parallel Engine Extensions for Handling Expensive Methods Based on Large Objects
#@ Felipe Carinoy;Glenn Linderman
#t 1999
#c 17
#! Object-Relational database systems allow users to define new user-defined types and functions. This typing extension allows structural semantics to be applied to new data types (non-SQL92 data types). However, defining new large complex objects within a tuple is not always practical. Practicality alone requires that large object data structures be used. This implies that predicate evaluation can be performed on tuple columns that are physically stored outside of a tuple's physical record. This out-of-line storage mechanism is commonly used today for column values that are physically large (implemented with a large object descriptor in the tuple pointing to a large object data structure). This type of predicate-based evaluation presents new optimizer and run-time challenges to the database system on clustered and MPP shared-nothing architectures.

#index 631949
#* Developing a DataBlade for a New Index
#@ G. Slivinskas
#t 1999
#c 17

#index 631950
#* Index Merging
#@ 
#t 1999
#c 17
#! Indexes play a vital role in decision support systems by reducing the cost of answering complex queries. A popular methodology for choosing indexes that is adopted by database administrators as well as automatic tools is: (a) Consider poorly performing queries in the workload. (b) For each query, propose a set of candidate indexes that potentially benefits the query. (c) Choose a subset from the candidate indexes in (b). Unfortunately, such a strategy can result in significant storage and index maintenance cost. In this paper, we present a novel technique called index merging to address the above shortcoming. Index merging can take an existing set of indexes (perhaps optimized for individual queries in the workload), and produce a new set of indexes with significantly lower storage and maintenance overhead, while retaining almost all the querying benefits of the initial set of indexes. We present an efficient algorithm for index merging, and demonstrate significant savings in index storage and maintenance by virtue of index merging, through experiments on Microsoft SQL Server 7.0.

#index 631951
#* Improving RAID Performance Using a Multibuffer Technique
#@ 
#t 1999
#c 17
#! RAID (redundant array of inexpensive disks) offers high performance for read accesses and large writes to many consecutive blocks. On small writes, however, it entails large penalties. Two approaches have been proposed to address this problem:1. The first approach records the update information on a separate log disk, and only brings the affected parity blocks to the consistent state when the system is idle. This strategy increases the chance of disk failure due to the additional log disks. Furthermore, heavy system loads for an extended period of time can overflow the log disks and cause sudden disastrous performance.2. The second approach avoids the above problems by grouping the updated blocks into new stripes and writing them as large writes. Unfortunately, this strategy improves write performance on the expense of read operations. After many updates, a set of logically consecutive data blocks can migrate to only a few disks making fetching them more expensive.In this paper, we improve on the second approach by eliminating its negative side effects. Our simulation results indicate that the existing scheme sometime performs worse than the standard RAID5 design. Our method is consistently better than either of these techniques.

#index 631952
#* Managing Distributed Memory to Meet Multiclass Workload Response Time Goals
#@ Arnd Christian König
#t 1999
#c 17
#! In this paper we present an online method for managing a goal oriented buffer partitioning in the distributed memory of a network of workstations. Our algorithm implements a feedback mechanism which dynamically changes the sizes of dedicated buffer areas and thereby the buffer hit rate for the different classes in such a way that user specified response time goals will be satisfied. The aggregated size of the buffer memory across all network nodes remains constant and only the partitioning is changed. The algorithm is based on efficiently approximating the trajectory of the per-class response time curves as a function of the available buffer. Changes in the workload that would lead to the violation of response time goals are counteracted by accordingly adjusting the buffer allocation. For local replacement decisions, we integrate a cost based buffer replacement algorithm to fit into our goal oriented approach. We have implemented out algorithm in a detailled simulation prototype and we present some first results with this prototype.

#index 631953
#* Cooperative Caching in Append-Only Databases with Hot Spots
#@ Aman Sinha
#t 1999
#c 17
#! We measure the performance of several cooperative caching policies for a database with hot spots. The workload consists of queries and append-only update transactions, and is modeled after a financial database of stock (historical) trading information.We show that cooperative caching is effective for this application. We show that selecting the correct set of peer servers when servicing a cache miss is crucial to achieving high performance, and we demonstrate a greedy algorithm that performs close to optimal for this workload. We also evaluate several cache replacement policies and show that a 2$^{nd}$-chance algorithm performs best. In a 2$^{nd}$-chance algorithm, replaced pages are transferred to a peer server rather than being discarded. When a page is selected for replacement a 2$^{nd}$ time, the page is discarded.Our results can be applied in the design of ``proxy'' servers for databases or web servers where a layer of proxy servers are used to scale the system performance.

#index 631954
#* Design and Evaluation of Disk Scheduling Policies for High-Demand Multimedia Servers
#@ 
#t 1999
#c 17
#! We propose and evaluate techniques to manage disk storage systems for multimedia document servers with constrained I/O resources. We also evaluate the requirements for the main memory to maintain desired levels of quality of presentation (QoP) for multiple users. This is achieved by attempting to minimize the number of tardy frames in en-to-end delivery of multimedia data. Inter-media and intra-media temporal synchronization at the user-interaction level are essential concerns for multimedia servers. Scheduling of multimedia frames has direct impact on both intra-stream and inter-stream multimedia synchronization. In order to effect such synchronization, we propose several O(n log n+mn) heuristics to schedule multimedia frames between the disks and destination memories of the multimedia systems, where m is the number of logical I/O channels and n is the number of frames of multimedia data. We study the performance of these heuristics via simulations and show the tradeoffs between the system resources and QoP parameters.

#index 631955
#* Concentric Hyperspaces and Disk Allocation for Fast Parallel Range Searching
#@ H. Ferhatosmanoglu
#t 1999
#c 17
#! Data partitioning and declustering have been extensively used in the past to parallelize I/O for range queries. Numerous declustering and disk allocation techniques have been proposed in the literature. However, most of these techniques were primarily designed for two-dimensional data and for balanced partitioning of the data space. As databases increasingly integrate multimedia information in the form of image, video, and audio data, it is necessary to extend the declustering techniques for multidimensional data. In this paper, we first establish that traditional declustering techniques do not scale for high-dimensional data. We then propose several new partitioning schemes based on concentric hyperspaces. We then develop disk allocation methods for each of the proposed schemes. We conclude with an evaluation of range queries based on these schemes and show that partitioning based on concentric hyperspaces has a significant advantage over balanced partitioning approach for parallel I/O.

#index 631956
#* Raster-Spatial Data Declustering Revisited: an Interactive Navigation Perspective
#@ 
#t 1999
#c 17
#! Various declustering techniques have been proposed in the literature for raster-geospatial data. Their primary focus is on reducing response time for static range queries. In this paper we focus on interactive navigation queries, which exhibit a new class of data access patterns. We analyze and compare the performance of three well-known declustering schemes: Disk Modulo, Exclusive-OR, and Hilbert Curve Access Method. The results show that Disk Modulo is close to optimal and, contrary to the case of range queries, is significantly better than Hilbert Curve Access Method. In addition, we propose a new scheme that further improves Disk Modulo for a realistic situation when the navigation window is bounded by a maximum size, a common constraint due to either monitor resolution or limited memory size. Performance properties of the scheme are also analyzed.

#index 631957
#* Mobile Agents for WWW Distributed Database Access
#@ 
#t 1999
#c 17
#! The popularity of the web as a universal access mechanism for network information has created the need for developing web-based DBMS client/server applications. However, the current commercial applet-based methodologies for accessing database systems offer limited flexibility, scalability and robustness. In this paper, we propose a new framework for Web- based distributed access to database systems based on Java-based mobile agents. The framework supports light-weight, portable and autonomous clients as well as operation on slow or expensive networks. The implementation of the framework shows that its performance is comparable to, and in some case outperforms, the current approach. In fact, in a wireless and dial-up environments and for average size transactions, a client/agent/server adaptation of the framework provided a performance improvement of approximately a factor often. For the fixed network the gains were about 40% and 30% respectively.

#index 631958
#* Working Together in Harmony - An Implementation of the CORBA Object Query Service and its Evaluation
#@ 
#t 1999
#c 17

#index 631959
#* Using Java and CORBA for Implementing Internet Databases
#@ 
#t 1999
#c 17
#! We describe an architecture called WebFINDIT that allows dynamic couplings of Web accessible databases based on their content and interest. We propose an implementation using WWW, Java, JDBC, and CORBA's ORBS that communicate via the CORBA's HOP protocol. The combination of these technologies offers a compelling middleware infrastructure to implement wide-area enterprise applications. In addition to a discussion of WebFINDIT's core concepts and implementalion architecture, we also discuss an experience of using WebFINDIT in a healthcare application.

#index 631960
#* Integrating Light-Weight Workflow Management Systems within Existing Business Environments
#@ 
#t 1999
#c 17
#! Workflow management systems support the eficient, largely automated execution of business processes. However; using a workflow management system typically requires implementing the application's control flow exclusively by the workflow management system. This approach is powerful if the control flow is specified and implemented from scratch, but it has severe drawbacks if a workflow management system is to be integrated within environments with existing solutions for implementing control flow. Usually, the existing solutions are too complex to be substituted by the workflow management system at once. Hence, the workflow management system must support an incremental integration, i.e. the reuse of existing implementations of control flow as well as their incremental substitution.Extending the workflow management system's functionality according to future application needs, e.g. by worklist and history management, must also be possible. In particular at the beginning of an incremental integration process, only a limited amount of a workflow management system's functionality is actually exploited by the workflow application. Later on, as the integration proceeds, more advanced requirements arise and demand the customization of the workfow management system to the evolving application needs.In this paper; we present the architecture and implementation of a light-weight workflow management system, coined Mentor-lite, which aims to overcome the above mentioned shortcomings of conventional workflow management systems. Mentor-lite supports an easy integration of workflow functionality into an existing environment, and can be tailored to specific workflow application needs.

#index 631961
#* Using Codewords to Protect Database Data from a Class of Software Errors
#@ 
#t 1999
#c 17
#! Increasingly, for extensibility and performance, special-purpose application code is being integrated with database system code. Such application code has direct access to database system buffers, and as a result, the danger of data being corrupted due to inadvertent application writes is increased. Previously proposed hardware techniques to protect from corruption require system calls, and their performance depends on details of the hardware architecture.We investigate an alternative approach which uses codewords associated with regions of data to detect corruption and to prevent corrupted data from being used by subsequent transactions. We develop several such techniques which vary in the level of protection, space overhead, performance, and impact on concurrency. These techniques are implemented in the Dali main-memory storage manager, and the performance impact of each on normal processing is evaluated. Novel techniques are developed to recover when a transaction had read corrupted data caused by a bad write, and gone on to write other data in the database. These techniques use limited and relatively low-cost logging of transaction reads to trace the corruption, and may also prove useful when resolving problems caused by incorrect data entry and other logical errors.

#index 631962
#* Scalable Trigger Processing
#@ Chris Carnes;J. B. Park;Albert Vernon
#t 1999
#c 17
#! Current database trigger systems have extremely limited scalability. This paper proposes a way to develop a truly scalable trigger system. Scalability to large numbers of triggers is achieved with a trigger cache to use main memory effectively, and a memory-conserving selection predicate index based on the use of unique expression formats called expression signatures. A key observation is that if a very large number of triggers are created, many will have the same structure, except for the appearance of different constant values. When a trigger is created, tuples are added to special relations created for expression signatures to hold the trigger's constants. These tables can be augmented with a database index or main-memory index structure to serve as a predicate index. The design presented also uses a number of types of concurrency to achieve scalability, including token (tuple)-level, condition-level, rule action-level, and data-level concurrency.

#index 631963
#* The Hybrid Tree: An Index Structure for High Dimensional Feature Spaces
#@ 
#t 1999
#c 17
#! Feature based similarity search is emerging as an important search paradigm in database systems. The technique used is to map the data items as points into a high dimensional feature space which is indexed using a multidimensional data structure. Similarity search then corresponds to a range search over the data structure. Although several data structures have been proposed for feature indexing, none of them is known to scale beyond 10-15 dimensional spaces. This paper introduces the hybrid tree -- a multidimensional data structure for indexing high dimensional feature spaces. Unlike other multidimensional data structures, the hybrid tree cannot be classified as either a pure data partitioning (DP) index structure (e.g., R-tree, SS-tree, SR-tree) or a pure space partitioning (SP) one (e.g., KDB-tree, hB-tree); rather, it ``combines'' positive aspects of the two types of index structures a single data structure to achieve search performance more scalable to high dimensionalities than either of the above techniques (hence, the name ``hybrid''). Furthermore, unlike many data structures (e.g., distance based index structures like SS-tree, SR-tree), the hybrid tree can support queries based on arbitrary dist ance functions. Our experiments on ``real'' high dimensional large size feature databases demonstrate that the hybrid tree scal es well to high dimensionality and large database sizes. It significantly outperforms both purely DP-based and SP-based index mechanisms as well as linear scan at all dimensionalities for large sized databases.

#index 631964
#* Real-Time Data Access Control on B-Tree Index Structures
#@ 
#t 1999
#c 17
#! This paper proposes methodologies to control the access of B-tree-indexed data in a batch and real-time fashion. Algorithms are proposed to insert, query, delete, and rebalance B-tree-indexed data based on non-real-time algorithms in [12] and the idea of priority inheritance [23]. We propose methodologies to reduce the number of disk I/O's to improve the system performance without introducing more priority inversion. The performance of our methodologies was evaluated by a series of experiments, for which we have some encouraging results.

#index 631965
#* Fat-Btree: An Update-Conscious Parallel Directory Structure
#@ 
#t 1999
#c 17
#! We propose a parallel directory structure, Fat Btree,to improve high speed access for parallel database systems in shared nothing environments. The Fat Btree has a three fold aim: to provide an indexing mechanism for fast retrieval in each processor, to balance the amount of data among distributed disks, and to reduce synchronization costs between processors during update operations. We use a probability based model to compare the throughput and response time of the Fat Btree with two ordinary parallel Btree structures, with copies of a whole Btree in each processor, and storing index nodes in a processor. The comparison results indicate that the Fat Btree is suitable for actual parallel database systems that accept update operations.

#index 631966
#* Scalable Classification over SQL Databases
#@ 
#t 1999
#c 17
#! We identify data-intensive operations that are common to classifiers and develop a middleware that decomposes and schedules these operations efficiently using a backend SQL database. Our approach has the added advantage of not requiring any specialized physical data organization. We demonstrate the scalability characteristics of our enhanced client with experiments on Microsoft SQL Server 7.0 by varying data size, number of attributes and characteristics of decision trees.

#index 631967
#* Storage of Multidimensional Arrays Based on Arbitrary Tiling
#@ 
#t 1999
#c 17
#! Storage management of multidimensional arrays aims at supporting the array model needed by applications and insuring fast execution of access operations. Current approaches to store multidimensional arrays rely on partitioning data into chunks (equally sized subarrays). Regular partitioning, however, does not adapt to access patterns, leading to suboptimal access performance. In this paper, we propose a storage approach for multidimensional discrete data (MDD) based on multidimensional arbitrary tiling. Tiling is arbitrary in that any partitioning into disjoint multidimensional intervals as well as incomplete coverage of n-D space and gradual growth of MDDs are supported. The proposed approach allows the storage structure to be configured according to user access patterns through tunable tiling strategies. We describe four strategies and respective tiling algorithms and present performance measurements which show their effectiveness in reducing disk access and post-processing times for range queries.

#index 631968
#* Complements for Data Warehouses
#@ J. Lechtenbörger
#t 1999
#c 17
#! Views over databases have recently regained attention in the context of data warehouses, which are seen as materialized views. In this setting, efficient view maintenance is an important issue, for which the notion of self-maintainability has been identified as desirable. In this paper, we extend self-maintainability to (query and update) independence, and we establish an intuitively appealing connection between warehouse independence and view complements. Moreover, we study minimal complements and show how to compute them in the presence of key constraints and inclusion dependencies in the underlying databases. Taking advantage of these complements, an algorithm is outlined for the specification of independent warehouses.

#index 631969
#* Parallel Classification for Data Mining on Shared-Memory Multiprocessors
#@ 
#t 1999
#c 17
#! We present parallel algorithms for building decision-tree classifiers on shared-memory multiprocessor (SMP) systems. The proposed algorithms span the gamut of data and task parallelism. The data parallelism is based on attribute scheduling among processors. This basic scheme is extended with task pipelining and dynamic load balancing to yield faster implementations. The task parallel approach uses dynamic subtree partitioning among processors. Our performance evaluation shows that the construction of a decision-tree classifier can be effectively parallelized on an SMP machine with good speedup.

#index 631970
#* Constraint-Based Rule Mining in Large, Dense Databases
#@ 
#t 1999
#c 17
#! Constraint-based rule miners find all rules in a given data-set meeting user-specified constraints such as minimum support and confidence. We describe a new algorithm that directly exploits all user-specified constraints including minimum support, minimum confidence, and a new constraint that ensures every mined rule offers a predictive advantage over any of its simplifications. Our algorithm maintains efficiency even at low supports on data that is dense (e.g. relational data). Previous approaches such as Apriori and its variants exploit only the minimum support constraint, and as a result are ineffective on dense data due to a combinatorial explosion of "frequent itemsets".

#index 631971
#* Mining Optimized Support Rules for Numeric Attributes
#@ 
#t 1999
#c 17
#! In this paper, we generalize the optimized support association rule problem by permitting rules to contain disjunctions over uninstantiated numeric attributes. For rules containing a single numeric attribute, we present a dynamic programming algorithm for computing optimized association rules. Furthermore, we propose a bucketing technique for reducing the input size, and a divide and conquer strategy that improves the performance significantly without sacrificing optimality. Our experimental results for a single numeric attribute indicate that our bucketing and divide and conquer enhancements are very effective in reducing the execution times and memory requirements of our dynamic programming algorithm. Furthermore, they show that our algorithms scale up almost linearly with the attribute's domain size as well as the number of disjunctions.

#index 631972
#* Formal Semantics of Composite Events for Distributed Environments
#@ 
#t 1999
#c 17
#! Languages for event specification in centralized systems and their semantics have received considerable attention in the literature. In contrast, very little work exists on extending the semantics of event specification languages to distributed environments.This paper provides a well-defined notion of distributed composite time stamps and their least restricted strict ordering are defined. The ordering is carefully chosen based on mathematical reasoning to ensure the best semantics. The concurrence and weaker-less-than-or-equal temporal relations are also introduced for the expressiveness of ECA rules. Furthermore, a Max operator is introduced for propagating the composite event time stamps. Based on this partial ordering and the Max operator on the time stamps, the semantics of Sentinel composite events is described for distributed event detection.

#index 631973
#* An Agent-Based Approach to Extending the Native Active Capability of Relational Database Systems
#@ 
#t 1999
#c 17
#! Event-condition-action (or ECA) rules are used to capture active capability. While a number of research protorypes of active database systems have been built, ECA rule capability in Relational DBA4Ss is still vey limited. In this paper, we address the problem of turning a traditional database management system into a full-fledged active database system without changing the underlying system. The advantages of this approach are: transparency; ability to add active capability without changing the client programs,. retain relational DBMS's underlying functionality; and persistence of ECA rules using the native database functionality. In this paper we describe how complete active database semantics can be supported on an existing SQL Server (Sybuse, in our case) by adding a mediator, termed ECA Agent, between the SQL Server and the clients. ECA rules are fully supported through the ECA Agent without changing applications or the SQL Server. Composite events are detected in the ECA Agent and actions are invoked in the SQL Server. Events are persisted in the native database system. ECA Agent is designed to connect to SQL Server by using Sybase connectivity products. The architecture, design, and implementation details are presented in this paper.

#index 631974
#* On the Semantics of Complex Events in Active Database Management Systems
#@ D. Zimmer
#t 1999
#c 17
#! Active database management systems have been developed for applications needing an automatic reaction in response to certain events. Events can be simple in nature or complex. Complex events rely on simpler ones and are usually specified with the help of operators of an event algebra. There are quite a few papers dealing with extensions of existing event algebras. However, a systematic and comprehensive analysis of the semantics of complex events is still lacking. As a consequence most proposals suffer from different kinds of peculiarities. Independent aspects are not treated independently leading to shady mixtures of aspects in operators. Moreover, aspects are not always treated uniformly. Operators may have other semantics than expected. Operators of different algebras which, at first glance, look the same may have different semantics.This paper addresses these problems by an extensive and in-depth analysis of the foundations of complex events. As a result of this analysis, a (formal) meta-model for event algebras will be introduced that subdivides the semantics of complex events into elementary, independent dimensions. Each of these dimensions will be discussed in detail. The resulting language specification fulfils the criteria for a good language design (like orthogonality, symmetry, homogeneity, lean set of language constructs) to a large extend.

#index 631975
#* Confirmation: A Solution for Non-Compensatability in Workflow Applications
#@ 
#t 1999
#c 17
#! The notion of a compensation is widely used in advanced transaction models as means of recovery from a failure. Similar concepts are adopted for providing ``transaction-like'' behaviour for long business processes supported by workflows technology. Generally, designing a compensating task in the context of a workflow process is a non-trivial job. In fact, not every task is compensatable since the forcibility of ``reverse'' operations of the task is not always guaranteed by the application semantics. In addition, the isolation requirement on data resources may make a task difficult to compensate. In this paper, we introduce a new concept called confirmation. By using confirmation, we can modify some originally non-compensatable tasks so that they become compensatable. Upon success of a workflow instance, the confirmation tasks of all executed tasks, if defined, are required for execution. An object-oriented framework which incorporates the confirmation concept is presented in this paper as well as its implementation issues.

#index 631976
#* Atomic Commitment in Database Systems over Active Networks
#@ 
#t 1999
#c 17
#! This paper proposes a novel approach to atomic commitment of transactions in distributed database systems. In our approach, active nodes participate in the process of atomic commitment of distributed transactions by maintaining passive "blackboards" which record voting status of participant subtransactions.

#index 631977
#* Scalable Web Server Design for Distributed Data Management
#@ 
#t 1999
#c 17
#! Traditional techniques for a distributed web server design rely on manipulation of central resources, such as routers or DNS services, to distribute requests designated for a single IP address to multiple web servers. The goal of the Distributed Scalable Web Server development is to explore application-level techniques for distributing web content. We achieve this by dynamically manipulating the hyperlinks stored within the web documents themselves. The distributed scalable web server effectively eliminates the bottleneck of centralized resources, while balancing the load among distributed web servers. The web servers may be located in different networks, or even different continents and still balance load effectively. The distributed scalable web server design is fully compatible with existing HTTP protocol semantics and existing web client software products.

#index 631978
#* TP-Monitor-Based Workflow Management System Architecture
#@ 
#t 1999
#c 17
#! Workflow Management System (WFMS) implementations traditionally follow a client/server architecture with monolithic servers (workflow engines). This poster presents a WFMS architecture based on a TP-Monitor environment which partitions the workflow engine into several resource managers (RMs) individually managed by a TP-Monitor environment. The RMs together form the workflow engine serving user requests.

#index 631979
#* Policies in a Resource Manager of Work ow Systems: Modeling, Enforcement and Management
#@ 
#t 1999
#c 17
#! We are interested in Work ow Management Systems (WFMS) [3, 4], and particularly, in Resource Management (RM) [1, 2] of WFMS. A WFMS consists of coordinating executions of multiple activities, instructing who (resource) do what (activity) and when. The "when" part is taken care of by the workflow engine which orders the executions of activities based on a process definition. The "who" part is handled by the resource manager that aims at finding suitable resources at the run-time for the accomplishment of an activity as the engine steps through the process definition. Resources of different kinds (human and material, for example) constitute the information system of our interest, their management consists of resource modeling and effective allocation upon users' requests. Since resource allocation needs to follow certain general guidelines (authority, security, for example) - no matter who or what application issues requests: so those general guidelines are better considered as part of the resources' semantics. That is the reason why we are interested in resource policy management in RM. Resource policies are general guidelines every individual resource allocation must observe. They differ from process specific policies which are only applied to a particular process. The policy manager is a module within the resource manager, responsible for efficiently managing a (potentially large) set of policies and enforcing them in resource allocation. We propose to enforce policies by query rewriting. A resource query is sent to the policy manager where relevant policies are first retrieved, then either additional selection criteria are appended to the initial query (in the case of requirement policies) or a new query is returned (in the case of substitution policies). Therefore, the policy manager can be seen as both a regulator and a facilitator where a resource query is either "polished" or given alternatives in a controlled way before submitted for actual resource retrieval. By doing so, returned resources can always be guaranteed to fully comply with the resource usage guidelines. We studied several issues related to resource policies in Work ow Systems. A policy language was proposed allowing users to specify policies of different types. To enforce the policy, a resource query is first rewritten based on relevant policies, before submitted to the resource manager for actual retrieval. The originality of the present work is on the resource policy model, the policy enforcement mechanism and policy management techniques including relational representation of, and efficient access to, a large policy set. It seems that the interval-based representation proposed in the paper provides a general framework for effective storage and efficient retrieval of large Boolean expression sets. A prototype was implemented in Java on NT 4.0, with experimental policies managed in an Oracle database. An alternative implementation would load policies into the main memory (periodically or at start-up time), an in-memory query optimizer ought to be devised in this case. Comparisons of pros/cons of these two implementations are worth further investigating.References [1] W. Du, G. Eddy and M.-C. Shan, "Distributed Resource Management in Work ow Environments", Proc. of Database Systems for Advanced Applications (DAS-FAA), Melbourne, Australia, April, 1997. [2] Y. -N. Huang and M.-C. Shan, "Policies in a Resource Manager of Work ow Systems: Modeling, Enforcement and Management", HP Tech. Report, HPL-98-156, Palo Alto, CA. [3] M. -C. Shan, J. Davis and W. Du, "HP Workflow Research: Past, Present and Future", Proc. of NATO ASI on Workflow Systems and Interoperability, Springer-Verlag, Turkey, August 1997. [4] WFMC, "The Work ow Reference Model", http://www.aiim.org/wfmc/DOCS/refmodel/rmv1-16.html.

#index 631980
#* A Hypertext Database for Advanced Sharing of Distributed Web Pages
#@ 
#t 1999
#c 17
#! There are many learning systems using their web pages. These systems provide not only contents of the web pages but also the embedded links on them for learners. In these systems, since the learners can use all the hyperlinks embedded on the web page, they can visit unnecessary/invalid web pages.In order to use web pages as instructional materials, authors of instructional materials need to be able to select appropriate web pages. Moreover, it is necessary to provide only the hyperlinks among these selected pages.A hyperlink view means the network of these selected web pages and hyperlinks. We regard this style of reusing web pages based on the hyperlink view as the advanced sharing of them. For the purpose of constructing a hypertext database that supports above advanced sharing of web pages, we developed an object- oriented framework of hypertext database. This framework is an extension of the Dexter model by adding link-context layer, context selecting interface, and more detailed presentation specifications to it. Material Authors can create their own hyperlink views and store each view in the hypertext database as a new link context individually. As the context selecting interface accomplishes composition of some link contexts, they can extend their link contexts. In this framework, a unit for sharing of web pages is a link context.It is necessary to hide hotspots about invalid hyperlinks on a screen dynamically. We add hiding mechanism of the invalid hotspots to the Dexter model. It is possible to treat hyperlink views through an ordinary web browsers. We developed a prototypical instructional material database, that is an example of the applications of our framework. Through this experience, we confirmed that the advanced sharing of web pages and their hyperlinks reduced the cost of material developments.

#index 631981
#* Integrating Heterogeneous OO Schemas
#@ 
#t 1999
#c 17
#! In this paper, we would like to introduce a new assertion, the so-called derivationassertion, to accommodate more heterogeneities, which can not be treated by the existing methodologies. As an example, consider two local object-oriented schemas: S 1 and S 2 and assume that S 1 contains two classes: parent and brother, and S 2 contains a class: uncle. A derivation assertion of the form: S 1 (parent, brother) (r) S 2 (uncle) can specify their corresponding semantic relationship clearly, which can not be established otherwise. We claim that this kind of assertions is necessary for the following reason. Imagine a query concerning uncle, submitted to the integrated schema from S 1 and S 2 . If the above assertion is not specified, the query evaluation will not take schema S 1 into account and thus the answers to the query can not be correctly computed in the sense of cooperations. Some more complicated examples will be given in a full paper to show that derivation assertions can always be used to handle intricate semantic relationships.

#index 631982
#* Similarity Searching in Text Databases with Multiple Field Types
#@ 
#t 1999
#c 17

#index 631983
#* A Transparent Replication of HTTP Service
#@ 
#t 1999
#c 17
#! We describe the architecture of Web++, a prototype for user-transparent replication of Web service. Web++ is built on top of the standard HTTP protocol and does not require any changes to existing Web browsers or the instalation of any software on the client side. We report on live Internet experiments that show that Web++ improves the client response time on the average by 36%, when compared to current Web performance.

#index 631984
#* Clustering Large Datasets in Arbitrary Metric Spaces
#@ Venkatesh Ganti;Raghu Ramakrishnan;Johannes Gehrke;Allison Powell
#t 1999
#c 17
#! Clustering partitions a collection of objects into groups called clusters, such that similar objects fall into the same group. Similarity between objects is defined by a distance function satisfying the triangle inequality; this distance function along with the collection of objects describes a distance space. In a distance space, the only operation possible on data objects is the computation of distance between them. All scalable algorithms in the literature assume a special type of distance space, namely a k-dimensional vector space, which allows vector operations on objects. We present two scalable algorithms designed for clustering very large datasets in distance spaces. Our first algorithm BUBBLE is, to our knowledge, the first scalable clustering algorithm for data in a distance space. Our second algorithm BUBBLE-FM improves upon BUBBLE by reducing the number of calls to the distance function, which may be computationally very expensive. Both algorithms make only a single scan over the database while producing high clustering quality. In a detailed experimental evaluation, we study both algorithms in terms of scalability and quality of clustering. We also show results of applying the algorithms to a real-life dataset.

#index 631985
#* ROCK: A Robust Clustering Algorithm for Categorical Attributes
#@ 
#t 1999
#c 17
#! We study clustering algorithms for data with boolean and categorical attributes. We show that traditional clustering algorithms that use distances between points for clustering are not appropriate for boolean and categorical attributes. Instead, we propose a novel concept of links to measure the similarity/proximity between a pair of data points. We develop a robust hierarchical clustering algorithm ROCK that employs links and not distances when merging clusters. Our methods naturally extend to non-metric similarity measures that are relevant in situations where a domain expert/similarity table is the only source of knowledge. In addition to presenting detailed complexity results for ROCK, we also conduct an experimental study with real-life as well as synthetic data sets. Our study shows that ROCK not only generates better quality clusters than traditional algorithms, but also exhibits good scalability properties.

#index 631986
#* Data Organization and Access for Efficient Data Mining
#@ 
#t 1999
#c 17
#! Efficient mining of data presents a significant challenge due to problems of combinatorial explosion in the space and time often required for such processing. While previous work has focused on improving the efficiency of the mining algorithms, we consider how the representation, organization, and access of the data may significantly affect performance, especially when I/O costs are also considered. By a simple analysis and comparison of the counting stage for the Apriori association rules algorithm, we show that a `column-wise' approach to data access is often more efficient than the standard row-wise approach. We also provide the results of empirical simulations to validate our analysis. The key idea in our approach is that counting in the Apriori algorithm with data accessed in a column-wise manner significantly reduces the number of disk accesses required to identify itemsets with a minimum support in the database -- primarily by reducing the degree to which data and counters need to be repeatedly brought into memory.

#index 631987
#* Program Committee Chairs
#@ 
#t 1999
#c 17

#index 631988
#* Query Processing Issues in Image(Multimedia) Databases
#@ 
#t 1999
#c 17
#! Multimedia databases have attracted academic and industrial interest, and systems such as QBIC (Content Based Image Retrieval system from IBM) have been released. Such systems are essential to effectively and efficiently use the existing large collections of image data in the modern computing environment. The aim of such systems is to enable retrieval of images based on their contents. This problem has brought together the (decades old) database and image processing communities.As part of our research in this area, we are building a prototype CBIR system called CHITRA. This uses a four level data model, and we have defined a Fuzzy Object Query Language(FOQL) for this system. This system enables retrieval based on high level concepts, such as "retrieve images of MOUNTAINS", "retrieve images of MOUNTAINS and SUNSET".A problem faced in this system is processing of complex queries such as "retrieve all images that have similar color histogram and similar texture to the given example image". Such problems have attracted research attention in recent times, notably by Fagin, Chaudhury and Gravano. Fagin has given an algorithm for processing such queries and provided a probabilistic upper bound for the complexity of the algorithm (which has been implemented in IBM's Garlic project). In this paper we provide theoretical (probabilistic) analysis of the expected cost of this algorithm. We propose a new multi-step query processing algorithm and prove that it performs better than Fagin's algorithm in all cases. Our algorithm requires fewer database accesses. We have evaluated both algorithms against an image database of 1000 images on our CHITRA system. We have used color histogram and Gabor texture features. Our analysis presented and the reported experimental results validate our algorithm (which has significant performance improvement).

#index 631989
#* Efficient Theme and Non-Trivial Repeating Pattern Discovering in Music Databases
#@ 
#t 1999
#c 17
#! In this paper, we propose an approach for fast discovering all non-trivial repeating patterns in music objects. A repeating pattern is a sequence of notes which appears more than once in a music object. The longest repeating patterns in music objects are typically their themes. The themes and other non-trivial repeating patterns are important music features which can be used for both content-based retrieval of music data and music data analysis. We present a data structure called RT-tree and its associated algorithms for fast extracting all non-trivial repeating patterns in a music object. Experiments are performed to compare with the related approaches. The results are further analyzed to show the efficiency and the effectiveness of our approach.

#index 631990
#* A Database Approach for Modeling and Querying Video Data
#@ 
#t 1999
#c 17
#! Indexing video data is essential for providing content based access. In this paper, we consider how database technology can offer an integrated framework for modeling and querying video data. As many concerns in video (e.g., modeling and querying) are also found in databases, databases provide an interesting angle to attack many of the problems. From a video applications perspective, database systems provide a nice basis for future video systems. More generally, database research will provide solutions to many video issues even if these are partial or fragmented. From a database perspective, video applications provide beautiful challenges. Next generation database systems will need to provide support for multimedia data (e.g., image, video, audio). These data types require new techniques for their management (i.e., storing, modeling, querying, etc.). Hence new solutions are significant.This paper develops a data model and a rule-based query language for video content based indexing and retrieval. The data model is designed around the object and constraint paradigms. A video sequence is split into a set of fragments. Each fragment can be analyzed to extract the information (i.e., symbolic descriptions) of interest that can be put into a database. This database can then be searched to find information of interest. Two types of information are considered: (1) the entities (i.e., objects) of interest in the domain of a video sequence, (2) video frames which contain these entities. To represent these information, our data model allows facts as well as objects and constraints. We present a declarative, rule-based, constraint query language that can be used to infer relationships about information represented in the model. The language has a clear declarative and operational semantics.

#index 631991
#* Indexing Constraint Databases by Using a Dual Representation
#@ B. Chidlovskii
#t 1999
#c 17
#! Linear constraint databases are a powerful framework to model spatial and temporal data. The use of constraint databases should be supported by access data structures that make effective use of secondary storage and reduce query processing time. Such structures should be able to store both finite and infinite objects and perform both containment (ALL) and intersection (EXIST) queries. As standard indexing techniques have certain limitations in satisfying such requirements, we employ the concept of geometric duality for designing new indexing techniques. In [5], we have used the dual transformation for polyhedra to develop a dynamic optimal indexing solution based on B+-trees, to detect all objects contained in or intersecting a given half-plane, when the angular coefficient belongs to a predefined set. In this paper, we extend the previous solution to allow angular coefficients to take any value. We present two approximation techniques for the dual representation of spatial objects, based on B+-trees. The techniques handle both finite and infinite objects and process both ALL and EXIST selections in a uniform way. We show the practical applicability of the proposed techniques by an experimental comparison with respect to R+-trees.

#index 631992
#* An Index Structure for Spatial Joins in Linear Constraint Databases
#@ 
#t 1999
#c 17
#! Constraint databases integrate database technology with constraint solving to deal with new applications such as spatial or geographical applications and those requiring arithmetic computations. Although the conceptual framework is elegant, issues related to efficient query evaluation and optimization techniques have not been sufficiently addressed. In this paper, we study efficient evaluation of spatial join in linear constraint databases in terms of the I/O complexity. We develop an extension of the classical B+ trees, called "interval B+ trees", and show that they can be used to effciently evaluate the spatial join of relations with dense-order and linear constraints. Specifically, we develop a general algorithm for joining two sets of rectangles using interval B+ trees. We show that the algorithm has the worst case I/O complexity of O(bNlog_b(N/b) +k), where N is the number of input rectangles and k the number of intersections. We show that the algorithm can be used in performing joins of relations with linear constraints. For relations with dense-order constraints where the spatial objects are not strictly rectangles, we extend the algorithm so that it can process the natural join of two N-tuple relations within O(bNlog_b(N/b) +k) I/Os, where k is the number of intersections. It remains open if one can achieve the same upper bound in the linear case.

#index 631993
#* I/O Complexity for Range Queries on Region Data Stored Using an R-tree
#@ 
#t 1999
#c 17

#index 631994
#* Tape-Disk Join Strategies under Disk Contention
#@ 
#t 1999
#c 17
#! Large-scale data warehousing, data mining, and scientific applications require the analysis of terabytes of facts data accumulated over long periods of time. Tape libraries are suitable devices for storing such mass data. The online analytical processing (OLAP) of this data typically leads to long-running aggregation queries joining the tape-resident facts relation with disk-resident dimension relations.Typically, during the execution of the join, the disks storing the dimension relations are not dedicated to the join. They are subject of reads and writes invoked by concurrently running applications. In many cases, it is desirable that the performance of these concurrent applications must not be degraded too much by the processing of the join. In this paper, we present an accurate model for analyzing the performance of three different tape-disk join strategies in multi-query systems like database or OLAP servers. The major contributions of this paper are (a) a detailed cost model considering tape and disk bandwidth, tape and disk latencies, available buffer sizes, CPU costs, and the selectivity of filters on tape data, (b) the consideration of disk queueing effects due to concurrent reads and writes at the disk, and (c) the consideration of two different disk scheduling strategies. Based on the analytical model, we show the superiority of a disk scheduling strategy giving preference to the service of the concurrent disk load. Furthermore, we present a strategy for dynamically selecting the most beneficial join algorithm and its parameters at runtime. We have implemented the tape-disk join strategies in a prototype system based on detailed simulations of secondary and tertiary storage devices. Our experimental evaluations confirm that the analytical model is indeed very accurate and a suitable basis for run-time strategy decisions.

#index 631995
#* Improving the Access Time Performance of Serpentine Tape Drives
#@ 
#t 1999
#c 17
#! This paper presents a general model for estimating access times of serpentine tape drives. The model is used to schedule I/O requests in order to minimize the total access time. We propose a new scheduling algorithm, Multi-Pass Scan Star (MPScan*), which makes good utilization of the streaming capability of the tape drive and avoids the pitfalls of naive multi-pass scan algorithms and greedy algorithms like Shortest Locate Time First. The performance of several scheduling algorithms have been simulated for problem sizes up to 2048 concurrent I/O requests. For scheduling of two to 1000 I/O requests, MPScan* gives equal or better results than any other algorithm, and provides up to 85 percent reduction of the total access time. All results have been validated by extensive experiments on Tandberg MLR1 and Quantum DLT 2000 drives.

#index 631996
#* Scheduling and Data Replication to Improve Tape Jukebox Performance
#@ 
#t 1999
#c 17
#! An increasing number of database applications require on-line access to massive amounts of data. Since large-scale storage systems implemented entirely on magnetic disk can be impractical or too costly for many applications, tape jukeboxes can provide an attractive solution. This paper shows how the performance of tape jukeboxes can be improved across a broad parameter space via a new scheduling algorithm and schemes for the placement and replication of hot data. We substantiate our claim by an extensive simulation study that quantifies the improvements obtained over a wide variety of workload characteristics. Our experiments suggest that system throughput increases when replicas of hot data are placed at the tape ends (not in the middle or at the beginning). As a result, the proposed replication techniques can be used to fill existing spare capacity in a tape jukebox, thus improving the performance of the jukebox ``for free''.

#index 631997
#* Estimating the Usefulness of Search Engines
#@ 
#t 1999
#c 17
#! In this paper, we present a statistical method to estimate the usefulness of a search engine for any given query. The estimates can be used by a metasearch engine to choose local search engines to invoke. For a given query, the usefulness of a search engine in this paper is defined to be a combination of the number of documents in the search engine that are sufficiently similar to the query and the average similarity of these documents. Experimental results indicate that the proposed estimation method is quite accurate.

#index 631998
#* Que Sera, Sera: The Coincidental Confluence of Economics, Business, and Collaborative Computing
#@ 
#t 1999
#c 17
#! The World Wide Web (WWW) changes everything, but how? Amazon.com is the current exemplar of e-business but does not begin to suggest what is possible. The Web frequently raises technology-based ideas such as WWW sites, Intranets, extra-nets, global access to information systems, and WWW-based interoperation. However, technical ideas serve to realize more significant changes such as the way business is conducted (e.g., e-business, business-to-business interactions, globalization, elimination of geographic boundaries, consolidation, dis-intermediation, stores without products, and worldwide price competition). More radical and more fundamental changes are those related to new economic models that underlie, predict, and enable new business models and which define technology requirements. The potential offered by the WWW is so great and so radical that it will take at least a decade to understand and possibly another decade to realize, since it involves fundamental change not only in computing models and practice but also in business and most significantly in economics.This is a time of radical change in what appears to be the parallel worlds of technology, business, and economics. They are not parallel. The intimate relationship of these domains has resulted in collateral homeostasis due in part to their interdependence. Recent radical change in each domain is now opening the door to collateral change. This presentation focuses on the confluence of these changes. Technology change includes not only the WWW but also other major factors. Gizmos, such as the Palm III, will become the dominant computing platform in sheer numbers. The trend toward packaged applications, e.g., SAP R/3, will extend far beyond the current strength in Enterprise Resource Planning to become the dominant method for software development and delivery. Business change involves not only organizations racing to capitalize on e-business opportunities, but also constant re-organization as seen in consolidations, mergers, and acquisitions that pose serious challenges to conventional technology (e.g., integration of generations old technology). Economic change is less obvious and more radical. It involves the move from cost-based accounting to economic-chain accounting.Current technology is designed to support cost-based accounting, which involves the management of data/information/knowledge within the boundaries of an organization. Current technology is inadequate to support economic-chain accounting, which involves the acquisition and management of data/information/knowledge beyond the boundaries of an organization.This presentation is an exploration of the next generation computing based on the confluence of radical and coincidental changes in economics, business, and technology. Whereas technology is a key enabler of change, it is and is the servant, not the master. Without a depth of understanding of this enabling role and the content in which technology serves, technology can be misguided and its developers can lose perspective. This presentation outlines a proposal made to the US President' s Office of Science and Technology for technology research for the next decade, which calls for new computational models, operating systems, data models, and infrastructure, amongst others, to support the next generation of computing, collaborative.

#index 631999
#* The ECHO Method: Concurrency Control Method for a Large-Scale Distributed Database
#@ 
#t 1999
#c 17
#! When constructing a large-scale database system, data replication is useful to give users efficient access to that data. However, for databases with a very large number of replica sites, using traditional networking concurrency control protocols makes communication too costly for practical use. In this paper, we propose a new protocol called ECHO for concurrency control of replica data distributed on a large scale. Using broadcasting, such as satellite broadcasting, terrestrial broadcasting, and cable television, the ECHO method can keep communication costs constant, regardless of the number of sites. The ECHO method also has a backup mechanism for failed broadcast reception using the communication network.

#index 632000
#* Database Extensions for Complex Forms of Data
#@ 
#t 1999
#c 17
#! To adequately support text, image, spatial, message and other complex forms of data, modern database management systems must provide an extensive set of data integration features. The emerging object-relational database systems provide features for defining, storing, updating, indexing, and retrieving complex data types with full transaction semantics. This talk will describe these features in the context of OracleSi database technology with examples from existing e-commerce and emerging XML data.

#index 632001
#* On Getting Some Answers Quickly, and Perhaps More Later
#@ 
#t 1999
#c 17
#! Traditionally, the answer to a database query is construed to be the set of all tuples that meet the criteria stated. Strict adherence to this notion in query evaluation is however increasingly unsatisfactory because decision makers are more prone to adopting an exploratory strategy for information search which we call ``getting some answers quickly, and perhaps more later.'' In this paper, we propose a progressive query processing strategy that exploits this behavior to conserve system resources and to minimize query response time. This is accomplished by the heuristic decomposition of user queries into subqueries that can be evaluated on demand. We also describe the architecture of a prototype system that provides a non-intrusive implementation of our approach. Finally, we present experimental results that demonstrate the benefits of the progressive query processing strategy.

#index 632002
#* Capability-Sensitive Query Processing on Internet Sources
#@ 
#t 1999
#c 17
#! On the Internet, the limited query-processing capabilities of sources make answering even the simplest queries challenging. In this paper, we present a scheme called GenCompact for generating capability-sensitive plans for queries on Internet sources. The query plans generated by GenCompact have the following advantages over those generated by existing query processing systems: (1) the sources are guaranteed to support the query plans; (2) the plans take advantage of the source capabilities; and (3) the plans are more efficient since a larger space of plans is examined.

#index 632003
#* Systematic Multiresolution and its Application to the World Wide Web
#@ 
#t 1999
#c 17
#! Many emerging environments are increasingly facing the problem where the requirements of applications easily outstrip the system resources. This is particularly acute in the World Wide Web (WWW) and many data-intensive applications like OLAP and multimedia databases. In this paper, we address this problem in the Web context via ``systematic multiresolution'', i.e., a framework for providing responses at different qualities (resolutions) and costs.We validate our conceptual contributions by implementing NetBlitz, a multiresolution-based proxy server on the WWW. NetBlitz addresses two key problems facing the Web: high latencies and heterogeneity of client resources and requirements. It solves these problems by dynamically generating the ``required version'' of a web object based on client preferences and capabilities. We also propose novel multiresolution-aware caching techniques that further improve performance. Finally, we experimentally demonstrate the utility of multiresolution and the caching enhancements proposed.

#index 632004
#* Business Objects and Application Integration
#@ 
#t 1999
#c 17
#! Application integration addresses the need for diverse applications to be able to communicate with each other across application boundaries and across platform boundaries. Business objects extend the type system of relational databases to deal with the needs of application integration.The requirements of business application integration include event-based information workflow to manage the exchange of data, well-formed business objects to reduce the number of application interfaces and to keep the semantic integrity of the information exchange intact, seamless transformation to integrate the validation and construction of interfacing business objects, and heterogeneous transport adapters to move business objects from one place to another.The objective of this paper is to examine the requirements for business objects and their role in application integration and to show examples of how the Mercator product addresses these requirements.

#index 632005
#* Database as an Application Integration Platform
#@ 
#t 1999
#c 17
#! This paper describes critical features that should be provided by any platform for application integration. These features include message queuing, message filtering and data transformation. The integration of these features with a database makes for a powerful platform for application integration that is scalable, reliable, available and manageable.

#index 632006
#* A Messaging-Based Architecture for Enterprise Application Integration
#@ 
#t 1999
#c 17
#! This talk discusses a messaging-based architecture for Enterprise Application Integration (EAI). The first generation of Enterprise Application Integration focused on the problem of making data held by various enterprise applications available from a common location and in a common format. The current generation of EAI goes beyond this by also propagating and controlling the flow of business events from application to application as they occur. A flexible, robust and scaleable messaging infrastructure is essential to this form of "active integration."

#index 632007
#* Ad Hoc OLAP: Expression and Evaluation
#@ 
#t 1999
#c 17
#! Users frequently formulate complex data analysis queries in order to identify interesting trends, make unusual patterns stand out, or verify hypotheses. Being able to express these data mining queries concisely is of major importance not only from the user's, but also from the system's point of view. Recent research in OLAP has focused on datacubes and their applications; however, expression and processing of ad hoc decision support queries has been given very little attention. In this paper we present an appropriate framework for these queries and introduce a syntactic construct to support it. This SQL extension allows most OLAP queries, such as pivoting, complex intra- and inter-group comparisons, trends and hierarchical comparisons, to be expressed in a compact, intuitive and simple manner. This succinct representation of a complex OLAP query translates immediately to a novel, simple and efficient evaluation algorithm. We show how to optimize, analyze and parallelize this algorithm and discuss issues such as multiple query analysis and scaling. We present several experimental results of real-life queries that show orders of magnitude of performance improvement. We argue that this tight coupling between representation and algorithm is essential to efficient processing of ad hoc OLAP queries.

#index 632008
#* Query Plans for Conventional and Temporal Queries Involving Duplicates and Ordering
#@ 
#t 2000
#c 17

#index 632009
#* Efficiently Supporting Multiple Similarity Queries for Mining in Metric Databases
#@ 
#t 2000
#c 17
#! Metric databases are databases where a metric distance function is defined for pairs of database objects. In such databases, similarity queries in the form of range queries or k-nearest neighbor queries are the most important queries. In traditional query processing, single queries are issued independently by different users. In many data mining applications, however, the database is typically explored by iteratively asking similarity queries for answers of previous similarity queries.In this paper, we introduce a generic scheme for such data mining algorithms and we investigate two orthogonal approaches, reducing I/O cost as well as CPU cost, to speed-up the processing of multiple similarity queries. The proposed techniques apply to any type of similarity query and to an implementation based on an index or using a sequential scan. Parallelization yields an additional impressive speed-up. An extensive performance evaluation confirms the efficiency of our approach.

#index 632010
#* Image Database Retrieval with Multiple-Instance Learning Techniques
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632011
#* PAC Nearest Neighbor Queries: Approximate and Controlled Search in High-Dimensional and Metric Spaces
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632012
#* The Collaboration Management Infrastructure
#@ 
#t 2000
#c 17
#! The Collaboration Management Infrastructure (CMI) project at MCC is developing comprehensive process- and service-oriented solutions to promote, support, and manage collaboration. CMI research solutions address key requirements of applications that cannot be fully supported by existing workflow and groupware technology. In this demonstration we show how a crisis mitigation application can be supported by CMI technology.A crisis is a situation that occurs unexpectedly and its exact course is unknown and unpredictable. While responding to an unfolding crisis may involve unpredictability and improvisation, organizations that respond to a large number of similar crises have developed crisis mitigation processes that provide enough structure to prevent chaotic response and increase mitigation effectiveness. However, such mitigation processes must also allow coordination flexibility and dynamic change that empower process participants to make and carry out decisions that deal with the current situation. To accomplish this, CMI provides novel process model capabilities, such as process templates, repeated optional dependencies, and customized awareness provisioning.

#index 632013
#* Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments
#@ 
#t 2000
#c 17
#! A prototype semiorder database used for activity recognition in multi-sensory monitoring environments is described. Activities are spatio-temporal compositions of events, which are a type of atomic semantic units for such compositions. The focus is on the temporal composition of activities from events in the presence of bounded duration of temporal uncertainty in an event occurrence. Such temporal uncertainty forces the concurrency between event occurrences to be intransitive. Under certain assumptions, a subclass of partial orders, known as semiorders, models such intransitive concurrency appropriately.The semiorder database stores events and their semiorder temporal order of occurrences. A semiorder data model and the corresponding query language that embeds a semiorder pattern language are the main constituents of the semiorder database. We demonstrate this database and queries for activity recognition in a real time environment. The demonstration also includes a transducer subsystem for detection of events.

#index 632014
#* The Mentor-Lite Prototype: A Light-Weight Workflow Management System
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632015
#* Location Prediction and Queries for Tracking Moving Objects
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632016
#* Lineage Tracing in a Data Warehousing System
#@ 
#t 2000
#c 17
#! A data warehousing system collects data from multiple distributed sources and stores the integrated information as materialized views in a local data warehouse. Users then perform data analysis and mining on the warehouse views. In many cases, the warehouse view contents alone are not sufficient for in-depth analysis.It is often useful to be able to "drill through" from interesting (or potentially erroneous) view data to the original source data that derived the view data. For a given view data item, identifying the exact set of base data items that produced the view data item is termed the view data lineage problem.Motivation for and applications of lineage tracing in a warehousing environment are provided in [2, 3]. In the context of the WHIPS data warehousing project at Stanford [4], we have developed a system that performs efficient and consistent lineage tracing. Some commercial data warehousing systems support schema-level lineage tracing, or provide specialized drill-down and/or drill-through facilities for multi-dimensional warehouse views.Our lineage tracing system supports more fine-grained instance-level lineage tracing for arbitrarily complex relational views, including aggregation. At view definition time, our system automatically generates lineage tracing procedures and supporting auxiliary views. At lineage tracing time, the system applies the tracing procedures to the source tables and/or auxiliary views to obtain the lineage results and to illustrate the specific view data derivation process.

#index 632017
#* Assisting the Integration of Taxonomic Data: The LITCHI Toolkit
#@ 
#t 2000
#c 17
#! We demonstrate a prototype toolkit that uses constraints and constraint violation repair techniques to enable the automated detection and, where possible, the automated resolution of conflicts in taxonomic databases.

#index 632018
#* TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications
#@ 
#t 2000
#c 17
#! Although there has been much written about various information integration technologies, little has been said regarding how to combine these technologies together to build an entire application. We demonstrate TheaterLoc, an information integration application that allows users to retrieve information about theaters and restaurants for various U.S. cities, including an interactive map depicting their relative locations. The data retrieved by TheaterLoc comes from five distinct heterogeneous and distributed sources. The enabling technology used to achieve the integration includes: the Ariadne information mediator, a web site wrapper learning tool, the Theseus execution system, and a mechanism for distributed spatial query planning. Our system is novel because it demonstrates how "virtual applications" can be rapidly built from a set of integration tools and existing online data sources.

#index 632019
#* READY: A High Performance Event Notification Service
#@ 
#t 2000
#c 17
#! READY is an event notification service that provides efficient, decoupled, and asynchronous event notifications. READY supports consumer specifications that match over single and compound event patterns, communication sessions that manage quality of service for event delivery, grouping constructs for sessions and specifications, event zones and boundary routers that bound the scope of event distribution and control the mapping of events across zones.

#index 632020
#* DISIMA: An Object-Oriented Approach to Developing an Image Database System
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632021
#* The IDEAL Approach to Internet-Based Negotiation for E-Business
#@ C. B. Huang;C. Pluempitiwiriyawej
#t 2000
#c 17
#! With the emergence of e-business as the next killer application for the Web, automating bargaining-type negotiations between clients (i.e., buyers and sellers) has become increasingly important. With IDEAL (Internet-based Dealmaker for e-business), we have developed an architecture and framework, including a negotiation protocol, for automated negotiations among multiple IDEAL servers.The main components of IDEAL are a constraint satisfaction processor (CSP) to evaluate a negotiation proposal, an Event-Trigger-Rule (ETR) server for managing and triggering the execution of rules which make up the negotiation strategy (rules can be updated at run-time to deal with the dynamic nature of negotiations), and a cost-benefit analysis to help in the selection of alternative strategies. We have implemented a fully functional prototype system of IDEAL to demonstrate automated negotiations among buyers and suppliers participating in a supply chain.

#index 632022
#* ReQueSS: Relational Querying of Semi-Structured Data
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632023
#* A Multimedia Information Server with Mixed Workload Scheduling
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632024
#* Web Query Optimizer
#@ 
#t 2000
#c 17
#! We demonstrate a Web Query Optimizer (WQO) within an architecture of mediators and wrappers, for WebSources of limited capability in a wide area environment. The WQO has several innovative features including a CBR (capability based rewriting) Tool, an enhanced randomized relational optimizer extended to a Web environment, and a WebWrapper cost model that can provide relevant metrics for accessing WebSources. The prototype has been tested against a number of WebSources.

#index 632025
#* Multi-Level Multi-Channel Air Cache Designs for Broadcasting in a Mobile Environment
#@ 
#t 2000
#c 17
#! In this paper, we investigate efficient ways of broadcasting data to mobile users over multiple physical channels, which cannot be coalesced into a lesser number of high-bandwidth channels. We propose the use of MLMC (Multi-Level Multi-Channel ) Air-Cache which can provide mobile users with data based on their popularity factor. We provide a wide range of design considerations for the server which broadcasts over the MLMC cache. We also investigate some novel techniques for the mobile user to access data from the MLMC cache and show the advantages of designing the broadcast strategy in tandem with the access behavior of the mobile users . Finally, we provide experimental results to compare the techniques we introduce.

#index 632026
#* An Algebraic Compression Framework for Query Results
#@ 
#t 2000
#c 17
#! Decision-support applications in emerging environments require that SQL query results or intermediate results be shipped to clients for further analysis and presentation. These clients may use low bandwidth connections or have severe storage restrictions. Consequently, there is a need to compress the results of a query for efficient transfer and client-side access.This paper explores a variety of techniques that address this issue. Instead of using a fixed method, we choose a combination of compression methods that use statistical and semantic information of the query results to enhance the effect of compression. To represent such a combination, we present a framework of "compression plans" formed by composing primitive compression operators.We also present optimization algorithms that enumerate valid compression plans and choose an optimal plan. Our experiments show that our techniques achieve significant performance improvement over standard compression tools like WinZip.

#index 632027
#* Power Conservative Multi-Attribute Queries on Data Broadcast
#@ 
#t 2000
#c 17
#! In this paper, we study power conservation techniques for multi-attribute queries on wireless data broadcast channels. Indexing data on broadcast channels can improve client filtering capability, while clustering and scheduling can reduce both access time and tune-in time. Thus, indexing techniques should be coupled with clustering and scheduling methods to reduce the battery power consumption of mobile computers. In this study, three indexing schemes for multi-attribute queries, namely, index tree, signature, and hybrid index, are discussed. We develop cost models for these three indexing schemes and evaluate their performance based on multi-attribute queries on wireless data broadcast channels.

#index 632028
#* Efficient Mining of Constrained Correlated Sets
#@ 
#t 2000
#c 17
#! In this paper, we study the problem of efficiently computing correlated itemsets satisfying given constraints. We call them valid correlated itemsets. It turns out constraints can have subtle interactions with correlated itemsets, depending on their underlying properties. We show that in general the set of minimal valid correlated itemsets does not coincide with that of minimal correlated itemsets that are valid, and characterize classes of constraints for which these sets coincide. We delineate the meaning of these two spaces and give algorithms for computing them. We also give an analytical evaluation of their performance and validate our analysis with a detailed experimental evaluation.

#index 632029
#* Finding Interesting Associations without Support Pruning
#@ 
#t 2000
#c 17

#index 632030
#* Dynamic Miss-Counting Algorithms: Finding Implication and Similarity Rules with Confidence Pruning
#@ 
#t 2000
#c 17

#index 632031
#* Join Enumeration in a Memory-Constrained Environment
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632032
#* In-Memory Data Management in the Application Tier
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632033
#* SQLServer for Windows CE " A Database Engine for Mobile and Embedded Platforms
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632034
#* Rules of Thumb in Data Engineering
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for system design need only slight revision after 35 years-the major change being the increased use of RAM. An analysis also indicates storage should be used to cache both database and web data to save disk bandwidth, network bandwidth, and people's time. Surprisingly, the 5-minute rule for disk caching becomes a cache-everything rule for web caching.

#index 632035
#* Independent Quantization: An Index Compression Technique for High-Dimensional Data Spaces
#@ 
#t 2000
#c 17
#! Two major approaches have been proposed to efficiently process queries in databases: Speeding up the search by using index structures, and speeding up the search by operating on a compressed database, such as a signature file. Both approaches have their limitations: Indexing techniques are inefficient in extreme configurations, such as high-dimensional spaces, where even a simple scan may be cheaper than an index-based search. Compression techniques are not very efficient in all other situations. We propose to combine both techniques to search for nearest neighbors in a high-dimensional space.For this purpose, we develop a compressed index, called the IQ-tree, with a three-level structure: The first level is a regular (flat) directory consisting of minimum bounding boxes, the second level contains data points in a compressed representation, and the third level contains the actual data.We overcome several engineering challenges in constructing an effective index structure of this type. The most significant of these is to decide how much to compress at the second level. Too much compression will lead to many needless expensive accesses to the third level. Too little compression will increase both the storage and the access cost for the first two levels.We develop a cost model and an optimization algorithm based on this cost model that permits an independent determination of the degree of compression for each second level page to minimize expected query cost. In an experimental evaluation, we demonstrate that the IQ-tree shows a performance that is the "best of both worlds" for a wide range of data distributions and dimensionalities.

#index 632036
#* DEMON: Mining and Monitoring Evolving Data
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632037
#* Mining Recurrent Items in Multimedia with Progressive Resolution Refinement
#@ 
#t 2000
#c 17
#! Despite the overwhelming amounts of multimedia data recently generated and the significance of such data, very few people have systematically investigated multimedia data mining. With our previous studies on content-based retrieval of visual artifacts, we study in this paper the methods for mining content-based associations with recurrent items and with spatial relationships from large visual data repositories.A progressive resolution refinement approach is proposed in which frequent item-sets at rough resolution levels are mined, and progressively, finer resolutions are mined only on the candidate frequent item-sets derived from mining rough resolution levels. Such a multi-resolution mining strategy substantially reduces the overall data mining cost without loss of the quality and completeness of the results.

#index 632038
#* CMP: A Fast Decision Tree Classifier Using Multivariate Predictions
#@ 
#t 2000
#c 17
#! Most decision tree classifiers are designed to keep class histograms for single attributes, and to select a particular attribute for the next split using said histograms. In this paper, we propose a technique where, by keeping histograms on attribute pairs, we achieve (i) a significant speed-up over traditional classifiers based on single attribute splitting, and (ii) the ability of building classifiers that use linear combinations of values from non-categorical attribute pairs as split criterion. Indeed, by keeping two-dimensional histograms, CMP can often predict the best successive split, in addition to computing the current one; therefore, CMP is normally able to grow more than one level of a decision tree for each data scan.CMP's performance improvements are also due to techniques whereby non-categorical attributes are discretized without loss in classification accuracy; in fact, we introduce simple techniques, whereby classification errors caused by discretization at one step can then be corrected in the following step. In summary, CMP represents a unified algorithm that extends the functionality of existing classifiers and improves their performance.

#index 632039
#* Answering Regular Path Queries Using Views
#@ 
#t 2000
#c 17
#! Query answering using views amounts to computing the answer to a query having information only on the extension of a set of views. This problem is relevant in several fields, such as information integration, data warehousing, query optimization, mobile computing, and maintaining physical data independence.We address query answering using views in a context where queries and views are regular path queries, i.e., regular expressions that denote the pairs of objects in the database connected by a matching path. Regular path queries are the basic query mechanism when the database is conceived as a graph, such as in semistructured data and data on the web.We study algorithms for answering regular path queries using views under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We characterize data, expression, and combined complexity of the problem, showing that the proposed algorithms are essentially optimal. Our results are the first to exhibit decidability in cases where the language for expressing the query and the views allows for recursion.

#index 632040
#* Practical Lineage Tracing in Data Warehouses
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632041
#* On-line Schema Update for a Telecom Database
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632042
#* Similarity Search for Multidimensional Data Sequences
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632043
#* Deflating the Dimensionality Curse Using Multiple Fractal Dimensions
#@ 
#t 2000
#c 17
#! Nearest neighbor queries are important in many settings, including spatial databases (Find the k closest cities) and multimedia databases (Find the k most similar images). Previous analyses have concluded that nearest neighbor search is hopeless in high dimensions, due to the notorious "curse of dimensionality". However, their precise analysis over real data sets is still an open problem.The typical and often implicit assumption in previous studies is that the data is uniformly distributed, with independence between attributes. However, real data sets overwhelmingly disobey these assumptions; rather, they typically are skewed and exhibit intrinsic ("fractal") dimensionalities that are much lower than their embedding dimension, e.g., due to subtle dependencies between attributes.In this paper, we show how the Hausdorff and Correlation fractal dimensions of a data set can yield extremely accurate formulas that can predict I/O performance to within one standard deviation. The practical contributions of this work are our accurate formulas which can be used for query optimization in spatial and multimedia databases. The theoretical contribution is the 'deflation' of the dimensionality curse.Our theoretical and empirical results show that previous worst-case analyses of nearest neighbor search in high dimensions are over-pessimistic, to the point of being unrealistic. The performance depends critically on the intrinsic ("fractal") dimensionality as opposed to the embedding dimension that the uniformity assumption incorrectly implies.

#index 632044
#* Program Vice-Chairs and Award Committee Members
#@ 
#t 2000
#c 17

#index 632045
#* Creating a Customized Access Method for Blobworld
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632046
#* Efficient Query Subscription Processing in a Multicast Environment
#@ 
#t 2000
#c 17
#! This paper introduces techniques for reducing data dissemination costs of query subscriptions. The reduction is achieved by merging queries with overlapping, but not necessarily equal, answers. The paper formalizes the query-merging problem and introduces a general cost model for it. We prove that the problem is NP-hard and propose exhaustive algorithms and three heuristic algorithms: the Pair Merging Algorithm, the Directed Search Algorithm and the Clustering Algorithm. We develop a simulator for evaluating the different heuristics and show that the performance of our heuristics is close to optimal.

#index 632047
#* The DC-Tree: A Fully Dynamic Index Structure for Data Warehouses
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632048
#* Automating Statistics Management for Query Optimizers
#@ 
#t 2000
#c 17
#! Statistics play a key role in influencing the quality of plans chosen by a database query optimizer. In this paper, we identify the statistics that are essential for an optimizer. We introduce novel techniques that help significantly reduce the set of statistics that need to be created without sacrificing the quality of query plans generated. We discuss how these techniques can be leveraged to automate statistics management in databases. We have implemented and experimentally evaluated our approach on Microsoft SQL Server 7.0.

#index 632049
#* A Novel Deadline Driven Disk Scheduling Algorithm for Multi-Priority Multimedia Objects
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632050
#* Self-Adaptive User Profiles for Large-Scale Data Delivery
#@ 
#t 2000
#c 17
#! Push-based data delivery requires knowledge of user interests for making scheduling, bandwidth allocation, and routing decisions. Such information is maintained as user profiles. We propose a new incremental algorithm for constructing user profiles based on monitoring and user feedback. In contrast to earlier approaches, which typically represent profiles as a single weighted interest vector, we represent user profiles as multiple interest vectors, whose number, size, and elements change adaptively based on user access behavior. This flexible approach allows the profile to more accurately represent complex user interests. Although there has been significant research on user profiles, our approach is unique in that it can be tuned to trade off profile complexity and quality. This feature, together with its incremental nature, makes our method suitable for use in large-scale information filtering applications such as push-based WWW page dissemination. We evaluate the method by experimentally investigating its ability to categorize WWW pages taken from Yahoo! categories. Our results show that the method can provide high filtering effectiveness with modest profile sizes and can effectively adapt to changes in users' interests.

#index 632051
#* XWRAP: An XML-Enabled Wrapper Construction System for Web Information Sources
#@ 
#t 2000
#c 17
#! This paper describes the methodology and the software development of XWRAP, an XML-enabled wrapper construction system for semi-automatic generation of wrapper programs. By XML-enabled we mean that the metadata about information content that are implicit in the original web pages will be extracted and encoded explicitly as XML tags in the wrapped documents. In addition, the query-based content filtering process is performed against the XML documents.The XWRAP wrapper generation framework has three distinct features. First, it explicitly separates tasks of building wrappers that are specific to a Web source from the tasks that are repetitive for any source, and uses a component library to provide basic building blocks for wrapper programs. Second, it provides a user-friendly interface program to allow wrapper developers to generate their wrapper code with a few mouse clicks. Third and most importantly, we introduce and develop a two-phase code generation framework.The first phase utilizes an interactive interface facility to encode the source-specific metadata knowledge identified by individual wrapper developers as declarative information extraction rules. The second phase combines the information extraction rules generated at the first phase with the XWRAP component library to construct an executable wrapper program for the given web source. We report the initial experiments on performance of the XWRAP code generation system and the wrapper programs generated by XWRAP.

#index 632052
#* Distributed Query Processing on the Web
#@ 
#t 2000
#c 17
#! Current Web querying systems are based on a "data shipping" mode wherein data is downloaded from remote sites to the user-site, queries are processed locally against these documents, and then further data is downloaded from the network based on these results. A data shipping approach suffers from several disadvantages, including the transfer of large amounts of unnecessary data resulting in network congestion and poor bandwidth utilization, the client-site becoming a processing bottleneck, and extended user response times due to sequential processing.In this paper, we present an alternative "query shipping" approach wherein queries emanating from the user-site are forwarded from one site to another on the Web, the query is processed at each recipient site, and the associated results are returned to the user. Our design does not require co-ordination from any "master site", making it a truly distributed scheme. It has been implemented as part of DIASPORA (DIstributed Answering System for Processing of Remote Agents), a new Java-based Web database system that is currently operational and is undergoing field trials on our campus network.

#index 632053
#* Probabilistic Data Consistency for Wide-Area Applications
#@ 
#t 2000
#c 17
#! A growing and diverse range of wide-area applications rely on distributed data management services which often employ data replication to meet availability and response time constraints imposed by the target domains. Since many of these applications can tolerate some staleness in the data they access, the constraints may be better met by exploiting relaxed data consistency among copies in the system. However, in the current Internet architecture, it is nearly impossible to provide deterministic guarantees for the bounds on data inconsistency among copies.We present an approach for providing probabilistic data access guarantees while maintaining availability and timing predictability. Our proposed service, which can be superimposed on traditional replication schemes, guarantees with a certain probability that the value returned to satisfy a client request will be temporally consistent with the newest copy of the same object in the system. We also include a discussion on a Solaris-based implementation of the proposed probabilistic data access service, and present a performance evaluation of its implementation on a prototype replication layer.

#index 632054
#* Metadata Propagation in Large, Multi-Layer Database Systems
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632055
#* Squeezing the Most Out of Relational Database Systems
#@ 
#t 2000
#c 17
#! We present compelling experimental evidence of the suitability of FOR compression for many database applications. While there has been some previous and concurrent work on compressing relations, no alternative solution combines the high compression ratios, low over-head, ease of incorporation into an RDBMS, and selective decompression that FOR compression achieves. Overall, we believe that this is a very attractive approach to implementing compression in an RDBMS.

#index 632056
#* Dynamic Histograms: Capturing Evolving Data Sets
#@ 
#t 2000
#c 17
#! Conventional histograms are `static' since they cannot be updated but only recalculated. In this paper, we introduce a `dynamic' version of V-optimal histograms, which is constructed and maintained incrementally. Our experimental results indicate that a variation of Dynamic V-optimal histograms has comparable precision to recalculation methods but is much cheaper to maintain.

#index 632057
#* Device Database Systems
#@ 
#t 2000
#c 17
#! In the next decade, networks of devices will be widely deployed for measurement, detection and surveillance applications. Millions of sensors and small-scale mobile devices will integrate processors, memory and communication capabilities. The Cornell COUGAR project studies how database technology can be adapted to meet the challenges of this new computing environment. In our novel concept of a device database system, physical devices are modeled as database objects. This permits large collections of devices to be controlled through declarative queries.

#index 632058
#* Efficient Storage of XML Data
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632059
#* A Semi-Structured Data Cartridge for Relational Databases
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632060
#* Efficient Query Refinement in Multimedia Databases
#@ 
#t 2000
#c 17
#! The proposed approaches are independent of the refinement model used (e.g., QPM or QEX) and hence work for all models. Our first contribution is to generalize the notion of similarity queries and allow multiple query points in a query (referred to as multipoint queries). This generalization is necessary since refined queries cannot be always expressed as single point queries.We develop a k-NN algorithm that can handle multipoint queries and show that it performs significantly better than the naive approach (i.e. execute several single point queries using the 'single-point' k-NN algorithm and merge results). The second and the main problem we address is how to evaluate refined queries efficiently.

#index 632061
#* Interactive-Time Similarity Search for Large Image Collections Using Parallel VA-Files
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632062
#* Distance Exponent: A New Concept for Selectivity Estimation in Metric Trees
#@ Caetano Traina Jr.
#t 2000
#c 17
#! This paper discusses the problem of selectivity estimation for range queries in metric datasets, which include vector, or dimensional, datasets as a special case. The main contribution of this paper is that, surprisingly, many different real datasets follow a "power law". From this observation we derive an analysis for the distance distribution of metric datasets. This is the first analysis of distance distributions for real metric datasets.We called the exponent of our power law as "distance exponent". We show that it plays a relevant role for the analysis of real, metric datasets. Specifically, we show (a) how to exploit the distance exponent to derive formulas for selectivity estimation of range queries and (b) how to compute it quickly from a metric index tree.We performed several experiments on many real datasets (road intersections of U.S. counties, vectors characteristics extracted from face matching systems, sets of words, distance matrixes) and synthetic datasets (Sierpinsky triangle, a 2-dimensional uniform distribution and a 2-dimensional line). Our selectivity estimation formulas are accurate, within relative error from 4% to 17%, and always within one standard deviation from the analytical results. Moreover, we present also a quick algorithm to estimate the "distance exponent", which gives good accuracy and saves orders of magnitude in computation time.

#index 632063
#* ACQ: An Automatic Clustering and Querying Approach for Large Image Databases
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632064
#* XML and DB2
#@ 
#t 2000
#c 17
#! The eXtensible Markup Language (XML) is a key technology that facilitates both information exchange and e-business transactions. Starting with DB2 UDB Net. Data V1, an application can generate XML documents from SQL queries against DB2 or any ODBC compliant databases.Today DB2 UDB XML Extender not only serves as a repository for both XML documents and their Document Type Definitions (DTDs), but also provides data management functionalities such as data integrity, security, recoverability and manageability. User has the option to store the entire document as an XML user-defined column or to decompose the document into multiple tables and columns.Fast search via indices is provided for both XML elements and attributes. Section search can be done against the content of the document. Query syntax adheres to W3C standards such as Extensive Stylesheet Language Transformation (XSLT) and XML Path Language (XPath) specifications. User can retrieve the entire document or extract XML elements and attributes dynamically in an SQL query. In addition, XML Extender provides stored procedure to generate XML documents from existing data. Together with Net.Data, one can browse the content of the XML documents via the Internet.

#index 632065
#* The MARIFlow Workflow Management System
#@ 
#t 2000
#c 17
#! MARIFlow System provides for automating and monitoring the flow of control and data over the Internet among different organizations, thereby creating a platform necessary to describe higher order processes involving several organizations and companies.The architecture is general enough to be applied to any business practice where data flow among different industries and co operations and the invocation of activities follow a pattern that can be described through a process definition. The example application provided within the scope of this project is on maritime industry.A MARIFlow process is executed through cooperating agents, called MARCAs (MARIFlow Cooperating Agents) that are automatically initialized at each site that the process executes. MARCAs handle the activities at their site, provide for coordination with other MARCAs in the system by routing the documents in electronic form according to the process description, keeping track of process information, and providing for the security and authentication of documents as well as comprehensive monitoring facilities.More specifically, the functionality provided by the system is as follows: A declarative means to specify the control of document flow over the Internet where it is possible to define the source of data, its control flow and the activities that make use of this data. Fully distributed execution architecture achieved through cooperating agents over the Internet. The agents know about other agents that they need to communicate with and preserve their state during communication. They also manage local information for monitoring purposes and for recovering from failures. Communicating with inside firewall applications. A MARCA can activate in-house activities automatically. However it should be noted that most organizations maybe reluctant to grant access inside the corporate firewall. In such cases, the MARCA passes the documents to an in-house system by properly acknowledging the in-house system on further processing that may be necessary on the documents. A MARCA is also responsible for getting the documents from the in-house system and forwarding them to the related agents as specified in the process definition. There is a coordinating MARCA in the system through which it is possible to define processes graphically from a Web interface. The coordinating MARCA is also responsible for initializing the MARCAs at each site for a new process definition and acting as a facilitator among MARCAs in the sense that for a new workflow definition it decides which new MARCAs are necessary. Note that only one MARCA exists at each site and handles all the activities of all workflow definitions related with that site. Therefore a new MARCA is generated only for a site participating to a workflow definition for the first time. Coordinating MARCA also acts as a data warehouse for monitoring purposes. Authentication and authorization of documents and the process related information. A monitoring mechanism for keeping track of the documents and for providing detailed account of the current status of a process instance within the system. Ability to recover the system from various types of failures.

#index 632066
#* Oracle8i"The XML Enabled Data Management System
#@ 
#t 2000
#c 17
#! XML is here as the internet standard for information exchange among e-businesses and applications. With its dramatic adoption and its ability to model structured, unstructured and semi-structured data, XML has the potential of becoming the data model for internet data. In the recent years, Oracle has evolved its DBMS to support complex, structured, and un-structured data. Oracle has now extended that technology to enable the storage and querying of XML data by evolving its DBMS to an XML enabled DBMS - Oracle8i.In this paper, we will present Oracle's XML-enabling database technology. In particular, we will discuss how XML data can be stored, managed, and queried in the Oracle8i database.

#index 632067
#* Optimal Index and Data Allocation in Multiple Broadcast Channels
#@ 
#t 2000
#c 17
#! The issue of data broadcast has received much attention in mobile computing. A periodic broadcast of frequently requested data can reduce the workload of the up-link channel and facilitate data access for the mobile user. Since the mobile units usually have limited battery capacity, the minimization of the access latency for the broadcast data is an important problem. The indexing and scheduling techniques on the broadcast data should be considered.In this paper we propose a solution to find the optimal index and data allocation, which minimizes the access latency for any number of broadcast channels. We represent all the possible allocations as a tree in which the optimal one is searched, and propose a pruning strategy based on some properties to greatly reduce the search space. Experiments are performed to show the effectiveness of the pruning strategy. Moreover, we pro-pose two heuristics to solve the same problem when the size of the broadcast data is large.

#index 632068
#* Optimization Techniques for Data-Intensive Decision Flows
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632069
#* Declustering Using Golden Ratio Sequences
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632070
#* User Defined Aggregates in Object-Relational Systems
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632071
#* Scalable Algorithms for Large Temporal Aggregation
#@ Ines Fernando Vega Lopez
#t 2000
#c 17
#! The ability to model time-varying natures is essential to many database applications such as data warehousing and mining. However, the temporal aspects provide many unique characteristics and challenges for query processing and optimization. Among the challenges is computing temporal aggregates, which is complicated by having to compute temporal grouping.In this paper, we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work. First, for small-scale aggregations, both the worst-case and average-case processing time have been improved significantly. Second, for large-scale aggregations, the proposed algorithms can deal with a database that is substantially larger than the size of available memory.

#index 632072
#* Accurate Estimation of the Cost of Spatial Selections
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632073
#* Clustering Categorical Data
#@ Chun Hing Cai
#t 2000
#c 17
#! In this paper we propose two methods to study the problem of clustering categorical data. The first method is based on dynamical system approach. The second method is based on the graph partitioning approach.

#index 632074
#* Association-Based Multiple Imputation in Multivariate Datasets: A Summary
#@ 
#t 2000
#c 17
#! Missing data are ubiquitous and inevitable in real databases and datasets. They can lead to worrisome problems, making a given dataset incomplete and undependable as well as causing various complications in applications. How to handle missing data is an important issue that needs to be addressed properly.

#index 632075
#* Optimization of Hypothetical Queries in an OLAP Environment
#@ 
#t 2000
#c 17
#! Analysts and decision-makers use what-if analysis to assess the effects of hypothetical scenarios on historical data. Current On-Line Analytical Processing (OLAP) systems support what-if analysis only by physically replicating the data warehouse and modifying it according to the scenario. This process may take hours, hence limiting the applicability of OLAP.To eliminate this inefficiency, we built and OLAP toolkit, called Sesame, that exploits the following two opportunities. First, typically a small part of the modified data is needed to answer the hypothetical query. For example, the analyst, might be interested in results of the new strategy only during certain periods of time, and maybe 90% of the modifications were not needed. Second, data warehouses tend to have pre-computed materialized views, to help answer popular queries.

#index 632076
#* Discovering Temporal Association Rules: Algorithms, Language and System
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632077
#* Mining Bases for Association Rules Using Closed Sets
#@ 
#t 2000
#c 17
#! We address the problem of the usefulness and the relevance of the set of discovered association rules. Using the frequent closed item set groundwork, we propose to generate bases for association rules, that are non-redundant generating sets for all association rules.

#index 632078
#* Approximate Query Answering with Frequent Sets and Maximum Entropy
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632079
#* An Extensible Framework for Data Cleaning
#@ Daniela Florescuand
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632080
#* Extracting Delta for Incremental Data Warehouse Maintenance
#@ 
#t 2000
#c 17
#! This paper seeks to highlight an area important to commercial data warehouse deployments that has received limited research attention, namely, the extraction of changes to the data at the source systems. We refer to these changes as deltas. Extracting deltas from source systems is the first step in the incremental maintenance of data warehouses. A common assumption among current incremental maintenance methods is that deltas are somehow made available - normally in the form of differential files. Extraction of deltas from source systems is often not a straight forward process nor an efficient one.In this paper, we analyze how deltas can be extracted from large systems. We analyze delta extraction methods that are currently available, namely, time stamps, differential snapshots, triggers, and archive logs. We point out the strengths and weaknesses of each method through analysis and when appropriate through experimentation. We have been investigating the method called Op-Delta at Boeing that better suits delta extraction from large integrated systems. We discuss the benefits of Op-Delta, discuss how it could be implemented, and present comparative results from our experimentation.

#index 632081
#* Speeding Up View Maintenance Using Cheap Filters at the Warehouse
#@ 
#t 2000
#c 17
#! We consider the problem of speeding up the incremental maintenance of materialized views defined by conjunctive queries (CQ) over external base relations, when querying these base relations is expensive. Our approach consists of detecting, without using the base relations, situations where a view either is not affected by a base update (VDU) or can be maintained using only the views in the warehouse (VSM). We are doing runtime optimization of view maintenance, since the tests take the current state of the warehouse database into account. Testing VSM for CQ views in general is known to be co-NP-complete in the size of the views, and testing VDU is conjectured to be as hard.In this paper, we identify important subclasses of CQ views for which VDU and VSM can be tested completely and efficiently, using only a small constant number of view lookups. This result is significant because, by maintaining indexes on selected view attributes, we can speed up view maintenance practically without incurring any substantial overhead. For more general CQ views, we show sufficient tests for VDU and VSM that have comparable efficiency. To demonstrate the performance of our method, we implement a view manager in Oracle PL-SQL which maintains a simple view under various synthetic update streams and under various delays in querying the base relations. Our results clearly show situations where tremendous speedup can be achieved. While further performance studies remain to be done, we believe our approach has a great potential to significantly speed up incremental data warehouse maintenance.

#index 632082
#* MetaComm: A Meta-Directory for Telecommunications
#@ H. Urroz;G. Michael;J. Orbach
#t 2000
#c 17
#! A great deal of corporate data is buried in network devices --- such as PBX messaging/email platforms, and data networking equipment --- where it is difficult to access and modify. Typically, the data is only available to the device itself for its internal purposes and it must be administered using either a proprietary interface or a standard protocol against a proprietary schema. This leads to many problems, most notably: the need for data replication and difficult interoperation with other devices and applications. MetaComm addresses these problems by providing a framework to integrate data from multiple devices into a meta-directory. The system allows user information to be modified through a directory using the LDAP protocol as well as directly through two legacy devices: a Definity (R) PBX and a voice messaging system. In order to prevent data inconsistencies, updates to any system must be reflected appropriately in all systems. We also discuss implementation details and experiences.

#index 632083
#* A Data-Warehouse/OLAP Framework for Scalable Telecommunication Tandem Traffic Analysis
#@ 
#t 2000
#c 17
#! In a telecommunication network, hundreds of millions of call detail records (CDRs) are generated daily. Applications such as tandem traffic analysis require the collection and mining of CDRs on a continuous basis. The data volumes and data flow rates pose serious scalability and performance challenges. This has motivated us to develop a scalable data-warehouse/OLAP framework, and based on this framework, tackle the issue of scaling the whole operation chain, including data cleansing, loading, maintenance, access and analysis.We introduce the notion of dynamic data warehousing for managing information at different aggregation levels with different life spans. We use OLAP servers, together with the associated multidimensional databases, as a computation platform for data caching, reduction and aggregation, in addition to data analysis. The framework supports parallel computation for scaling up data mining, and supports incremental OLAP for providing continuous data mining. A tandem traffic analysis engine is implemented on the proposed framework.In addition to the parallel and incremental computation architecture, we provide a set of application-specific optimization mechanisms for scaling performance. These mechanisms fit well into the above framework. Our experience demonstrates the practical value of the above framework in supporting an important class of telecommunication business intelligence applications.

#index 632084
#* The Changing Art of Computer Research
#@ 
#t 2000
#c 17
#! Computer researchers should be ecstatic, but they are not. Their area is à la mode. The daily news are filled with stories about computers, communication, and the media. The economy as a whole is not only affected but is spearheaded by new technology. It is not a revolt, it is a revolution. On the other hand, the visions are prompted not by scientists but by modern entrepreneurs. The new ideas are coming from the small companies and not from the research groups. The bright students do not dream of scientific valor but of commercial success. Something is changing profoundly. Thirty years ago we were fighting for acceptance and independence as scientists. Now the area is becoming so big and so dynamic that we are fighting for position and attention inside our own field.

#index 632085
#* Developing Cost Models with Qualitative Variables for Dynamic Multidatabase Environments
#@ 
#t 2000
#c 17
#! A major challenge for global query optimization in a multidatabase system (MDBS) is lack of local cost information at the global level due to local autonomy. A number of methods to derive local cost models have been suggested in the literature recently. However, these methods are only suitable for a static multidatabase environment.In this paper, we propose a new multi-states query sampling method to develop local cost models for a dynamic environment. The system contention level at a dynamic local site is divided into a number of discrete contention states based on the costs of a probing query.To determine an appropriate set of contention states for a dynamic environment, two algorithms based on iterative uniform partition and data clustering, respectively, are introduced. A qualitative variable is used to indicate the contention states for the dynamic environment.The techniques from our previous (static) query sampling method, including query sampling, automatic variable selection, regression analysis, and model validation, are extended so as to develop a cost model incorporating the qualitative variable for a dynamic environment. Experimental results demonstrate that this new multi-states query sampling method is quite promising in developing useful cost models for a dynamic multidatabase environment.

#index 632086
#* Query Planning with Limited Source Capabilities
#@ 
#t 2000
#c 17
#! In information-integration systems, sources may have diverse and limited query capabilities. In this paper we show that because sources have restrictions on retrieving their information, sources not mentioned in a query can contribute to the query result by providing useful bindings. In some cases we can access sources repeatedly to retrieve bindings to answer a query, and query planning thus becomes considerably more challenging.We find all the obtainable answers to a query by translating the query and source descriptions to a simple recursive Data-log program, and evaluating the program on the source relations. This program often accesses sources that are not in the query. Some of these accesses are essential, as they provide bindings that let us query sources, which we could not do otherwise. However, some of these accesses can be proven not to add anything to the query's answer.We show in which cases these off-query accesses are useless, and prove that in these cases we can compute the complete answer to the query by using only the sources in the query. In the cases where off-query accesses are necessary, we propose an algorithm for finding all the useful sources for a query. We thus solve the optimization problem of eliminating the unnecessary source accesses, and optimize the program to answer the query.

#index 632087
#* Dynamic Query Scheduling in Data Integration Systems
#@ Françoise Fabret
#t 2000
#c 17
#! Execution plans produced by traditional query optimizers for data integration queries may yield poor performance for several reasons. The cost estimates may be inaccurate, the memory available at run-time may be insufficient, or data delivery rate can be unpredictable. In this paper, we address the problem of unpredictable data arrival rate. We propose to dynamically schedule queries in order to deal with irregular data delivery rate and gracefully adapt to the available memory. Our approach performs careful step-by-step scheduling of several query fragments and processes these fragments based on data arrivals. We describe a performance evaluation that shows important performance gains in several configurations.

#index 632088
#* Efficient Searches for Similar Subsequences of Different Lengths in Sequence Databases
#@ Chihcheng Hsu
#t 2000
#c 17
#! We propose an indexing technique for fast retrieval of similar subsequences using time warping distances. A time warping distance is a more suitable similarity measure than the Euclidean distance in many applications, where sequences may be of different lengths or different sampling rates. Our indexing technique uses a disk-based suffix tree as an index structure and employs lower-bound distance functions to filter out dissimilar subsequences without false dismissals. To make the index structure compact and thus accelerate the query processing, we convert sequences of continuous values to sequences of discrete values via a categorization method and store only a subset of suffixes whose first values are different from their preceding values. The experimental results reveal that our proposed technique can be a few orders of magnitude faster than sequential scanning.

#index 632089
#* Landmarks: A New Model for Similarity-Based Pattern Querying in Time Series Databases
#@ 
#t 2000
#c 17
#! In this paper we present the Landmark Model, a model for time series that yields new techniques for similarity-based time series pattern querying. The Landmark Model does not follow traditional similarity models that rely on point-wise Euclidean distance. Instead, it leads to Landmark Similarity, a general model of similarity that is consistent with human intuition and episodic memory.By tracking different specific subsets of features of landmarks, we can efficiently compute different Landmark Similarity measures that are invariant under corresponding subsets of six transformations; namely, Shifting, Uniform Amplitude Scaling, Uniform Time Scaling, Uniform Bi-scaling, Time Warping and Non-uniform Amplitude Scaling.A method of identifying features that are invariant under these transformations is proposed. We also discuss a generalized approach for removing noise from raw time series without smoothing out the peaks and bottoms. Beside these new capabilities, our experiments show that Landmark Indexing is considerably fast.

#index 632090
#* Online Data Mining for Co-Evolving Time Sequences
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632091
#* Generalized Isolation Level Definitions
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632092
#* Semantic Conditions for Correctness at Different Isolation Levels
#@ 
#t 2000
#c 17
#! Many transaction processing applications execute at isolation levels lower than SERIALIZABLE in order to increase throughput and reduce response time. The problem is that non-serializable schedules are not guaranteed to be correct for all applications. The semantics of a particular application determines whether that application will run correctly at a lower isolation level, and in practice it appears that many applications do. Unfortunately, we know of no analysis technique that has been developed to test an application for its correctness at a particular level. Apparently decisions of this nature are made on an informal basis. In this paper we describe such a technique in a formal way.We use a new definition of correctness, semantic correctness, which is weaker than serializability, to investigate the correctness of such executions. For each isolation level, we prove a condition under which transactions that execute at that level will be semantically correct. In addition to the ANSI/ISO isolation levels of READ UNCOMMITTED, READ COMMITTED, and REPEATABLE READ, we also prove a condition for correct execution at the READ COMMITTED with first-committer-wins (a variation of READ COMMITTED) and at the SNAPSHOT isolation level. We assume that different transactions can be executing at different isolation levels, but that each transaction is executing at least at the READ UNCOMMITTED level.

#index 632093
#* Managing Escalation of Collaboration Processes in Crisis Mitigation Situations
#@ 
#t 2000
#c 17
#! Processes for crisis mitigation must permit coordination flexibility and dynamic change to empower crisis mitigation coordinators and experts to deal with the unexpected situations. However, such mitigation processes must also provide enough structure to prevent chaotic response and increase mitigation effectiveness. Such combination of structure and flexibility cannot be effectively supported by existing workflow or groupware technologies.In this paper, we introduce the Collaboration Management Infrastructure (CMI) and describe its capabilities for supporting crisis mitigation processes. CMI provides a comprehensive Collaboration Management Model (CMM) and a corresponding federated system. CMM supports process templates that provide the initial activities, control and data flow structure, and resources needed to start mitigating a variety of crisis situations.In the event of a crisis, the appropriate process template is selected and instantiated. Crisis mitigation is achieved by escalating the instantiated process template. Escalation involves selecting and adding new process templates, creating new activities, roles, and task forces as needed to deal with the current demands in the crisis, and delegating responsibilities to process participants and task forces. CMM provides advanced composable primitives that empower crisis mitigation coordinators and experts to escalate the process. We provide an overview of the implementation of a federated CMI system and discuss our initial experience with various applications in the area of crisis management.

#index 632094
#* Tutorial 3: Data Mining with Decision Trees
#@ 
#t 2000
#c 17
#! In this tutorial, we survey recent developments in learning tree-based models for classification and regression called predictor trees. The tutorial has three parts: (1) A general overview of tree-based classification and regression. (2) A survey of methods to construct predictor trees. (3) An overview of scalable data access methods to construct predictor trees from very large training databases.In the first part, we motivate predictor trees and their use in a data mining environment. We show results from real-life studies that illustrate how predictor trees give understandable models where traditional models are hard or counter-intuitive to interpret, and compare related methods for classification and regression.In the second part of the tutorial, we discuss choices involved in tree construction, including different split selection methods and tree pruning. Although we survey the most popular methods, including work from all KDD sub-communities, we emphasize recent work from the statistics literature. Wherever possible, we interleave results on real datasets. The methods presented in this part assume that the complete training database fits into main memory.The third part of the tutorial covers scalable methods for predictor tree construction. We first motivate the concept of scalability and then survey recent work in the database literature on scalable data access methods for constructing predictor trees from very large training databases.

#index 632095
#* Tutorial 4: Directories: Managing Data for Networked Applications
#@ 
#t 2000
#c 17
#! Directories have recently emerged as an essential component of the network infrastructure, and are being used to store a wide variety of information to support network applications. These include address books for messaging applications, user preferences for configuration management, access control lists and certificates for security applications, and profiles and policies in the DEN (directory enabled networks) initiative. Network directory data is typically heterogeneous, highly distributed and replicated, requiring autonomy across multiple directory servers while allowing for conceptual unity, in a way that is not well supported by conventional relational or object-oriented databases. Efficient management of such data creates new challenges and exciting opportunities for the database community.In late 1997, LDAPv3 (Lightweight Directory Access Protocol, version 3) was approved as a proposed IETF standard for modeling and querying directory information, as well as accessing directory services over the Internet. A large number of LDAP directory products are now available from vendors such as Oracle, Novell, AOL (Netscape), IBM, and Innosoft, and a variety of non-LDAP directories providing LDAP interfaces. This tutorial describes key features of LDAP directories, and presents recent research inspired by LDAP directories. Its intended audience includes researchers in databases and/or network systems, software and application developers, and the LDAP community.

#index 632096
#* Tutorial 5: Indexing High-Dimensional Spaces: Database Support for Next Decade's Applications
#@ 
#t 2000
#c 17
#! During recent years, a variety of new database applications has been developed which substantially differ from conventional database applications. For example, new database applications such as data warehousing produce very large relations which require a multidimensional view on the data, and in areas such as multimedia and CAD a content-based search is essential which is often implemented using some kind of feature vectors. All the new applications have in common that the underlying database system has to support the processing of queries on large amounts of high-dimensional data. Now, we may ask what the difference is between processing low- and high-dimensional data. A result of recent research activities is that basically none of the querying and indexing techniques, which provide good results on low-dimensional data, also performs sufficiently well on higher-dimensional data. The problem of dealing with high-dimensional spaces has therefore been addressed in a variety of recent database research projects. The goal of the tutorial is to spread the knowledge about high-dimensional spaces and the proposed techniques to a large community of both, researchers and practitioners 3/4 researchers who are interested in querying and indexing techniques for high-dimensional data, and practitioners who are interested in the state-of-the art of database support for their applications. Also, the tutorial will be very interesting for non-database computer scientists because the problem of dealing with high-dimensional spaces has a large number of other applications such as robot motion planning, optimization problems, and visualization techniques. Therefore, a large part of the tutorial is dedicated to convey the understanding of the effects occurring in these spaces.The tutorial is structured as follows: In the first section, we describe two examples of new database applications, which demonstrate the need for efficient query processing techniques in high-dimensional spaces. In the second section, we discuss the effects occurring in high-dimensional spaces - first from a pure mathematical point of view and then from a database perspective. Next, we describe the different approaches for modeling the costs of processing queries on high-dimensional data. The description of the different approaches demonstrates nicely what happens if we ignore the special properties of high-dimensional spaces. In the fourth section, we then provide a structured overview of the proposed querying and indexing techniques, discussing their advantages and drawbacks. In this section, we also cover a number of additional techniques dealing with optimization and parallelization. In concluding the tutorial, we try to stir further research activities by presenting a number of interesting research problems.

#index 632097
#* Tutorial 1: Web Information Retrieval
#@ 
#t 2000
#c 17
#! The Web explosion offers a bonanza of algorithmic problems. In particular, information retrieval in the Web context requires methods and ideas that have not been addressed in the classic IR literature. This tutorial will survey emerging techniques for IR in the Web context and discuss some of the pertinent open problems. The list of topics includes search engine technology, ranking and classification methods, Web measurements (usage, size, connectivity) and new graph and data structure problems arising in the Web IR context.

#index 632098
#* Tutorial 2: Mobile and Wireless Database Access for Pervasive Computing
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632099
#* Extensible Indexing: a Framework for Integrating Domain-Specific Indexing Schemes into Oracle8i
#@ 
#t 2000
#c 17
#! Extensible Indexing is a SQL-based framework that allows users to define domain-specific indexing schemes, and integrate them into the Oracle8i server. Users register a new indexing scheme, the set of related operators, and additional properties through SQL data definition language extensions. The implementation for an indexing scheme is provided as a set of Oracle Data Cartridge Interface (ODCIIndex) routines for index-definition, index-maintenance, and index-scan operations. An index created using the new indexing scheme, referred to as domain index, behaves and performs analogous to those built natively by the database system. Oracle8i server implicitly invokes user-supplied index implementation code when domain index operations are performed, and executes user-supplied index scan routines for efficient evaluation of domain-specific operators.This paper provides an overview of the framework and describes the steps needed to implement an indexing scheme. The paper also presents a case study of Oracle Cartridges (InterMedia Text, Spatial, and Visual Information Retrieval), and Daylight (Chemical compound searching) Cartridge, which have implemented new indexing schemes using this framework and discusses the benefits and limitations.

#index 632100
#* DB2 Advisor: An Optimizer Smart Enough to Recommend its own Indexes
#@ Alan Skelley
#t 2000
#c 17
#! This paper introduces the concept of letting an RDBMS Optimizer optimize its own environment. In our project, we have used the DB2 Optimizer to tackle the index selection problem, a variation of the knapack problem. This paper will discuss our implementation of index recommendation, the user interface, and provide measurements on the quality of the recommended indexes.

#index 632101
#* Taming the Downtime: High Availability in Sybase ASE 12
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632102
#* Database Technology for Internet Applications
#@ 
#t 2000
#c 17

#index 632103
#* Pure Java Databases for Deployed Applications
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 632104
#* Analyzing Range Queries on Spatial Data
#@ 
#t 2000
#c 17
#! Analysis of range queries on spatial (multidimensional) data is both important and challenging. Most previous analysis attempts have made certain simplifying assumptions about the datasets and/or queries to keep the analysis tractable. As a result, they may not be universally applicable.This paper proposes a set of five analysis techniques to estimate the selectivity and number of index nodes accessed in serving a range query. The underlying philosophy behind these techniques is to maintain an auxiliary data structure called a density file, whose creation is a one-time cost, which can be quickly consulted when the query is given. The schemes differ in what information is kept in the density file, how it is maintained, and how this information is looked up. It is shown that one of the proposed schemes, called Cumulative Density (CD), gives very accurate results (usually less than 5% error) using a diverse suite of point and rectangular datasets, that are uniform or skewed, and a wide range of query window parameters. The estimation takes a constant amount of time, which is typically lower than 1% of the time that it would take to execute the query, regardless of dataset or query window parameters.

#index 632105
#* Data Redundancy and Duplicate Detection in Spatial Join Processing
#@ 
#t 2000
#c 17
#! This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for ...

#index 636126
#* In Memoriam-James Clifford
#@ 
#t 1996
#c 17

#index 636127
#* In Memoriam-Gennady Antoshenkov
#@ 
#t 1996
#c 17

#index 636128
#* Dynamic Optimization of Index Scans Restricted by Booleans
#@ 
#t 1996
#c 17
#! When index retrieval is restricted to a range or singleton, an index scan is not done in its entirety because key portions before and after the range are skipped. Likewise, in some production databases, gaps between multiple ranges are skipped. However, ranges on the second attribute of a composite key are considered unproductive for key skip because they do not constitute a key range. This is not so. Restriction age=40 for index [sex,age] can be viewed as ORed singletons "female"

#index 636129
#* Program Chairs Message
#@ 
#t 1997
#c 17

#index 636130
#* Program Area Co-Chairs
#@ 
#t 1997
#c 17

#index 636131
#* Program Committee Vice Chairs
#@ 
#t 1998
#c 17

#index 657920
#* Proceedings of the 18th International Conference on Data Engineering
#@ 
#t 2002
#c 17

#index 657926
#* Proceedings of the 17th International Conference on Data Engineering
#@ 
#t 2001
#c 17

#index 659917
#* Data Reduction by Partial Preaggregation
#@ 
#t 2002
#c 17
#! Partial preaggregation is a simple data reduction operator that can be applied to aggregation queries. Whenever we group and aggregate on a column set G, we can preaggregate on any column set that functionally determines G. Preaggregation can be used, for example, to reduce the input size to a join. Regular aggregation reduces the input to one record per group. Partial preaggregation exploit the fact that preaggregation need not be complete -if multiple record happen to be output for a group, they will be combined into the same group by the final aggregation. This paper describes a straightforward hash-based algorithm for partial preaggregation, discusses where it can be applied, and derives a mathematical model for estimating the output size. The effectiveness of the technique and the accuracy of the model are shown on both artificial and real data. It is also shown how to reduce memory requirement by combining partial preaggregation with the input phase of a subsequent join or sort operator. Partial preaggregation has been implemented, in part, in Microsoft SQL Server.

#index 659918
#* Decoupled Query Optimization for Federated Database Systems
#@ 
#t 2002
#c 17
#! We study the problem of query optimization in federated relational database systems. The nature of federated databases explicitly decouples many aspects of the optimization process, often making it imperative for the optimizer to consult underlying data sources while doing cost-based optimization. This not only increases the cost of optimization, but also changes the trade-offs involved in the optimization process significantly. The dominant cost in the decoupled optimization process is the "cost of costing" that traditionally has been considered insignificant. The optimizer can only afford a few rounds of messages to the underlying data sources and hence the optimization techniques in this environment must be geared toward gathering all the required cost information with minimal communication.In this paper, we explore the design space for a query optimizer in this environment and demonstrate the need for decoupling various aspects of the optimization process. We present minimum-communication decoupled variants of various query optimization techniques, and discuss trade-offs in their performance in this scenario. We have implemented these techniques in the Cohera federated database system and our experimental results, somewhat surprisingly, indicate that a simple two-phase optimization scheme performs fairly well as long as the physical database design is known to the optimizer, though more aggressive algorithms are required otherwise.

#index 659919
#* A Non-Blocking Parallel Spatial Join Algorithm
#@ 
#t 2002
#c 17
#! Interest in incremental and adaptive query processing has led to the investigation of equijoin evaluation algorithms that are non-blocking. This investigation has yielded a number of algorithms, including the symmetric hash join, the XJoin, the Ripple Join, and their variants. However, to our knowledge no one has proposed a non-blocking spatial join algorithm. In this paper, we propose a parallel non-blocking spatial join algorithm that uses duplicate avoidance rather than duplicate elimination. Results from a prototype implementation in a commercial parallel object-relational DBMS show that it generates answer tuples steadily even in the presence of memory overflow, and that its rate of producing answer tuples scales with the number of processors. Also, when allowed to run to completion, its performance is comparable with the state-of-the-art blocking parallel spatial join algorithm.

#index 659920
#* Improving Range Query Estimation on Histograms
#@ 
#t 2002
#c 17
#! Histograms are used to summarize the contents of relations for the estimation of query result sizes into a number of buckets. Several techniques (e.g., MaxDiff and V-Optimal) have been proposed in the past for determining bucket boundaries which provide better estimations. This paper proposes to use a 32-bit information (4-level tree index) for each bucket for storing approximated cumulative frequencies at 7 internal intervals of a bucket. Both theoretical analysis and experimental results show that the 4-level tree index provides the best frequency estimation inside a bucket. The index is later added to two well-known techniques for constructing histograms, MaxDiff and V-Optimal, thus obtaining high improvements in the frequency estimation over inter-bucket ranges w.r.t. the original methods.

#index 659921
#* A Sampling-Based Estimator for Top-k Query
#@ 
#t 2002
#c 17
#! Top-k queries arise naturally in many database applications that require searching for records whose attribute values are close to those specified in a query. In this paper, we study the problem of processing a top-k query by translating it into an approximate range query that can be efficiently processed by traditional relational DBMSs. We propose a sampling-based approach, along with various query mapping strategies, to determine a range query that yields high recall with low access cost.Our experiments on real-world datasets show that, given the same memory budgets, our sampling-based estimator outperforms a previous histogram-based method in terms of access cost, while achieving the same level of recall. Furthermore, unlike the histogram-based approach, our sampling-based query mapping scheme scales well for high-dimensional data and is easy to implement with low maintenance cost.

#index 659922
#* Query Estimation by Adaptive Sampling
#@ 
#t 2002
#c 17
#! The ability to provide accurate and efficient result estimations of user queries is very important for the query optimizer in database systems. In this paper, we show that the traditional estimation techniques with data reduction points of view do not produce satisfiable estimation results if the query patterns are dynamically changing. We further show that to reduce query estimation error, instead of accurately capturing the data distribution, it is more effective to capture the user query patterns. In this paper, we propose query estimation techniques that can adapt to user query patterns for more accurate estimates of the size of selection or range queries over databases.

#index 659923
#* Detecting Changes in XML Documents
#@ Amelie Marian
#t 2002
#c 17
#! We present a diff algorithm for XML data. This work is motivated by the support for change control in the context of the Xyleme project that is investigating dynamic warehouses capable of storing massive volume of XML data. Because of the context, our algorithm has to be very efficient in terms of speed and memory space even at the cost of some loss of ``quality''. Also, it considers, besides insertions, deletions and updates (standard in diffs), a move operation on subtrees that is essential in the context of XML. Intuitively, our diff algorithm uses signatures to match (large) subtrees that were left unchanged between the old and new versions. Such exact matchings are then possibly propagated to ancestors and descendants to obtain more matchings. It also uses XML specific information such as ID attributes. We provide a performance analysis of the algorithm. We show that it runs in average in linear time vs. quadratic time for previous algorithms. We present experiments on synthetic data that confirm the analysis. Since this problem is NP-hard, the linear time is obtained by trading some quality. We present experiments (again on synthetic data) that show that the output of our algorithm is reasonably close to the ``optimal'' in terms of quality. Finally we present experiments on a small sample of XML pages found on the Web.

#index 659924
#* From XML Schema to Relations: A Cost-Based Approach to XML Storage
#@ 
#t 2002
#c 17
#! As Web applications manipulate an increasing amount of XML, there is a growing interest in storing XML data in relational databases. Due to the mismatch between the complexity of XML's tree structure and the simplicity of flat relational tables, there are many ways to store the same document in and RDBMS, and a number of heuristic techniques have been proposed. These techniques typically define fixed mappings and do not take application characteristics into account. However a fixed mapping is unlikely to work well for all possible applications. In contrast, LegoDB is a cost-based XML storage mapping engine that explores and space of possible XML-to-relational mappings and selects the best mapping for a given application. LegoDB leverages current XML and relational technologies: 1) is models the target application with an XML Schema, XML data statistics, and an Xquery workload; 2) the space of configurations is generated through XML-Schema rewritings; and 3) the best among the derived configurations is selected using cost estimates obtained through a standard relational optimizer. In this paper, we describe the LegoDB storage engine and provide experimental results that demonstrate the effectiveness of this approach.

#index 659925
#* Reverse Engineering for Web Data: From Visual to Semantic Structures
#@ 
#t 2002
#c 17
#! Despite the advancement of XML, the majority of documents on the Web is still marked up with HTML for visual rendering purposes only, thus building a huge amount of "legacy" data. In order to facilitate querying Web based data in a way more efficient and effective than just keyword based retrieval, enriching such Web documents with both structure and semantics is necessary.This paper describes a novel approach to the integration of topic specific HTML documents into a repository of XML documents. In particular, we describe how topic specific HTML documents are transformed into XML documents. The proposed document transformation and semantic element tagging process utilizes document restructuring rules and minimum information about the topic in form of concepts. For the resulting XML documents, a majority schema is derived that describes common structures among the documents in the form of a DTD.We explore and discuss different techniques and rules for document conversion and majority schema discovery. We finally demonstrate the feasibility and effectiveness of our approach by applying it to a set of resume HTML documents gathered by a Web crawler.

#index 659926
#* Content-Based Video Indexing for the Support of Digital Library Search
#@ 
#t 2002
#c 17

#index 659927
#* OntoWebber: A Novel Approach for Managing Data on the Web
#@ 
#t 2002
#c 17

#index 659928
#* Mapping XML and Relational Schemas with Clio
#@ , M. A.  Herná L. Pop
#t 2002
#c 17

#index 659929
#* Advanced Process-Based Component Integration in Telcordia's Cable OSS
#@ 
#t 2002
#c 17

#index 659930
#* Managing Complex and Varied Data with the IndexFabric(tm)
#@ B. Cooper;G. Hjaltason
#t 2002
#c 17

#index 659931
#* A Distributed Database Server for Continuous Media
#@ A. Rezgui;E. Terzi
#t 2002
#c 17

#index 659932
#* StreamCorder: Fast Trial-and-Error Analysis in Scientific Databases
#@ E. Stolte
#t 2002
#c 17

#index 659933
#* NAPA: Nearest Available Parking Lot Application
#@ 
#t 2002
#c 17

#index 659934
#* HP-Inventing the Future of Storage
#@ 
#t 2002
#c 17

#index 659935
#* Cost Models for Overlapping and Multi-Version B-Trees
#@ 
#t 2002
#c 17
#! Overlapping and multi-version techniques are two popular frameworks that transform an ephemeral index into a multiple logical-tree structure in order to support versioning databases. Although both frameworks have produced numerous efficient indexing methods, their performance analysis is rather limited; as a result there is no clear understanding about the behavior of the alternative structures and the choice of the best one, given the data and query characteristics. Furthermore, query optimization based on these methods is currently impossible. These are serious problems due to the incorporation of overlapping and multi-version techniques in several traditional (e.g., banking) and emerging (e.g., spatio-temporal) applications. In this paper, we propose frameworks for reducing performance analysis of overlapping and multi-version structures to that of the corresponding ephemeral structures, thus simplifying the problem significantly. The frameworks lead to accurate cost models that predict the sizes of the trees, the node accesses and query selectivity. Although we focus on B-tree-based structures, the proposed models can be employed with a variety of indexes.

#index 659936
#* Similarity Search Over Time-Series Data Using Wavelets
#@ 
#t 2002
#c 17
#! We consider the use of wavelet transformations as a dimensionality reduction technique to permit efficient similarity search over high-dimensional time-series data. While numerous transformations have been proposed and studied, the only wavelet that has been shown to be effective for this application is the Haar wavelet. In this work, we observe that a large class of wavelet transformations (not only orthonormal wavelets but also bi-orthonormal wavelets) can be used to support similarity search. This class includes the most popular and most effective wavelets being used in image compression. We present a detailed performance study of the effects of using different wavelets on the performance of similarity search for time-series data. We include several wavelets that outperform both the Haar wavelet and the best known non-wavelet transformations for this application. To ensure our results are usable by an application engineer, we also show how to configure an indexing strategy for the best performing transformations. Finally, we identify classes of data that can be indexed efficiently using these wavelet transformations.

#index 659937
#* GADT: A Probability Space ADT for Representing and Querying the Physical World
#@ Anton Faradjian
#t 2002
#c 17
#! Large sensor networks are being widely deployed for measurement, detection, and monitoring applications. Many of these applications involve database systems to store and process data from the physical world. This data has inherent measurement uncertainties that are properly represented by continuous probability distribution functions (pdf's). We introduce a new object-relational data type, the Gaussian ADT GADT, that models physical data as gaussian pdf's, and we show that existing index structures can be used as fast access methods for GADT data. We also present a measure-theoretic model of probabilistic data and evaluate GADT in its light.

#index 659938
#* Efficient Algorithm for Projected Clustering
#@ E. Ng Ka Ka;A. W. Fu
#t 2002
#c 17

#index 659939
#* Multivariate Time Series Prediction via Temporal Classification
#@ 
#t 2002
#c 17

#index 659940
#* NeT & CoT: Inferring XML Schemas from Relational World
#@ F. Chiu
#t 2002
#c 17

#index 659941
#* Attribute Classification Using Feature Analysis
#@ 
#t 2002
#c 17

#index 659942
#* FAST: A New Sampling-Based Algorithm for Discovering Association Rules
#@ 
#t 2002
#c 17

#index 659943
#* An Intuitive Framework for Understanding Changes in Evolving Data Streams
#@ 
#t 2002
#c 17

#index 659944
#* An Efficient Index Structure for Shift and Scale Invariant Search of Multi-Attribute Time Sequences
#@ , A.  Singh, T. Kahvec;A. Gü
#t 2002
#c 17

#index 659945
#* Exploiting Punctuation Semantics in Data Streams
#@ 
#t 2002
#c 17

#index 659946
#* Using Smodels (Declarative Logic Programming) to Verify Correctness of Certain Active Rules
#@ 
#t 2002
#c 17

#index 659947
#* Multiple Query Optimization by Cache-Aware Middleware Using Query Teamwork
#@ 
#t 2002
#c 17

#index 659948
#* Runtime Data Declustering over SAN-Connected PC Cluster System
#@ 
#t 2002
#c 17

#index 659949
#* Extensible and Similarity-Based Grouping for Data Integration
#@ E. Schallehn
#t 2002
#c 17

#index 659950
#* Efficient OLAP Query Processing in Distributed Data Warehouses
#@ , M.  Bö M. Akind
#t 2002
#c 17

#index 659951
#* A Graphical XML Query Language
#@ 
#t 2002
#c 17

#index 659952
#* Data Cleaning and XML: The DBLP Experience
#@ 
#t 2002
#c 17

#index 659953
#* The ATLaS System and Its Powerful Database Language Based on Simple Extensions of SQL
#@ 
#t 2002
#c 17

#index 659954
#* Out From Under the Trees
#@ C. Jermaine
#t 2002
#c 17

#index 659955
#* A Framework Towards Efficient and Effective Sequence Clustering
#@ 
#t 2002
#c 17

#index 659956
#* Specification-Based Data Reduction in Dimensional Data Warehouses
#@ 
#t 2002
#c 17

#index 659957
#* BestPeer: A Self-Configurable Peer-to-Peer System
#@ 
#t 2002
#c 17

#index 659958
#* How Good Are Association-Rule Mining Algorithms?
#@ 
#t 2002
#c 17

#index 659959
#* Efficient Indexing Structures for Mining Frequent Patterns
#@ B. Lan
#t 2002
#c 17

#index 659960
#* SCADDAR: An Efficient Randomized Technique to Reorganize Continuous Media Blocks
#@ 
#t 2002
#c 17
#! Scalable storage architectures allow for the addition of disks to increase storage capacity and/or bandwidth. In its general form, disk scaling also refers to disk removals when either capacity needs to be conserved or old disk drives are retired. Assuming random placement of blocks on multiple nodes of a continuous media server, our optimization objective is to redistribute a minimum number of media blocks after disk scaling. This objective should be met under two restrictions. First, uniform distribution and hence a balanced load should be ensured after redistribution. Second, the redistributed blocks should be retrieved at the normal mode of operation in one disk access and through low complexity computation. We propose a technique that meets the objective, while we prove that it also satisfies both restrictions. The SCADDAR approach is based on using a series of Remap functions which can derive the location of a new block using only its original location as a basis.

#index 659961
#* Indexing of Moving Objects for Location-Based Services
#@ 
#t 2002
#c 17
#! Visionaries predict that the Internet will soon extend to billions of wireless devices, or objects, a substantial fraction of which will offer their changing positions to location-based services. This paper assumes an Internet-service scenario where objects that have not reported their position within a specified duration of time are expected to no longer be interested in, or of interest to, the service. Due to the possibility of many "expiring" objects, a highly dynamic database results.The paper presents an R-tree based technique for the indexing of the current positions of such objects. Different types of bounding regions are studied, and new algorithms are provided for maintaining the tree structure. Performance experiments indicate that, when compared to the approach where the objects are not assumed to expire, the new indexing technique can improve search performance by a factor of two or more without sacrificing update performance.

#index 659962
#* A Publish & Subscribe Architecture for Distributed Metadata Management
#@ 
#t 2002
#c 17
#! The emergence of electronic marketplaces and other electronic services and applications on the Internet is creating a growing demand for effective management of resources. Due to the nature of the Internet such information changes rapidly. Furthermore, such information must be available for a large number of users and applications, and copies of pieces of information should be stored near the users that need this particular information. In this paper, we present the architecture of MDV, a distributed metadata management system. MDV has a 3-tier architecture and supports caching and replication in the middle-tier so that queries can be evaluated locally. Users and applications specify the information they need and that is replicated using a specialized subscription language. In order to keep replicas up-to-date and initiate the replication of new and relevant information, MDV implements a novel, scalable publish & subscribe algorithm. We describe this algorithm in detail, show how it can be implemented using a standard relational database system, and present the results of performance experiments conducted using our prototype implementation.

#index 659963
#* Integrating Workflow Management Systems with Business-to-Business Interaction Standards
#@ 
#t 2002
#c 17
#! Business-to-Business (B2B) E-commerce is emerging as a new market with tremendous potential. Organizations are trying to link services across organizational boundaries in order to electronically trade goods and services. Standards such as RosettaNet, CBL, EDI, OBI, and cXML, describe how electronic B2B interactions should be carried on so that dynamic trade partnerships can be established and transactions can be executed across organizations. While the development of standards is a fundamental step towards enabling e-business, the problem of linking B2B interactions with internal business processes is still a challenge. In addition, as the industry standards evolve continuously based on changing needs, organizations have to adopt new standards quickly. In this paper we describe how workflow technology can be extended in order to support B2B interactions and to link them with the internal workflows. The proposed framework can be used to speed up both the development of new business processes that support B2B interaction standards and the enhancement of the existing business processes by the addition of B2B interaction capability. We demonstrate the benefits of our framework through an example in which we describe how RosettaNet Partner Interface Processes (PIPs) can be interfaced with HP Process Manager (HPPM), HP's business process management product that was formerly known as Changengine. An analogous solution can be developed for other workflow management systems and B2B interaction standards.

#index 659964
#* Declarative Composition and Peer-to-Peer Provisioning of Dynamic Web Services
#@ 
#t 2002
#c 17
#! The development of new services through the integration of existing ones has gained a considerable momentum as a means to create and streamline business-to-business collaborations. Unfortunately, as Web services are often autonomous and heterogeneous entities, connecting and coordinating them in order to build integrated services is a delicate and time-consuming task. In this paper, we describe the design and implementation of a system through which existing Web services can be declaratively composed, and the resulting composite services can be executed following a peer-to-peer paradigm, within a dynamic environment. This system provides tools for specifying composite services through statecharts, data conversion rules, and provider selection policies. These specifications are then translated into XML documents that can be interpreted by peer-to-peer inter-connected software components, in order to provision the composite service without requiring a central authority.

#index 659965
#* Data Mining Meets Performance Evaluation: Fast Algorithms for Modeling Bursty Traffic
#@ 
#t 2002
#c 17
#! Network, web, and disk I/O traffic are usually bursty, self-similar and therefore can not be modeled adequately with Poisson arrivals. However, we do want to model these types of traffic and to generate realistic traces, because of obvious applications for disk scheduling, network management, web server design.Previous models (like fractional Brownian motion, FARIMA etc) tried to capture the `burstiness'. However, the proposed models either require too many parameters to fit and/or require prohibitively large (quadratic) time to generate large traces. We propose a simple, parsimonious method, the b-model, which solves both problems: It requires just one parameter, and it can easily generate large traces. In addition, it has many more attractive properties: (a) With our proposed estimation algorithm, it requires just a single pass over the actual trace to estimate b. For example, a one-day-long disk trace in milliseconds contains about 86Mb data points and requires about 3 minutes for model fitting and 5 minutes for generation. (b) The resulting synthetic traces are very realistic: our experiments on real disk and web traces show that our synthetic traces match the real ones very well in terms of queuing behavior.

#index 659966
#* Efficient Evaluation of Queries with Mining Predicates
#@ 
#t 2002
#c 17
#! Modern relational database systems are beginning to support ad hoc queries on mining models. In this paper, we explore novel techniques for optimizing queries that apply mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for some popular discrete mining models:decision trees, naive Bayes, and clustering.Our experiments on Microsoft SQL Server 2000 demonstrate that these derived predicates can signi?cantly reduce the cost of evaluating such queries.

#index 659967
#* d-Clusters: Capturing Subspace Correlation in a Large Data Set
#@ 
#t 2002
#c 17
#! Clustering has been an active research area of great practical importance for recent years. Most previous clustering models have focused on grouping objects with similar values on a (sub)set of dimensions (e.g., subspace cluster) and assumed that every object has an associated value on every dimension (e.g., bicluster). These existing cluster models may not always be adequate in capturing coherence exhibited among objects. Strong coherence may still exist among a set of objects (on a subset of attributes) even if they take quite different values on each attribute and the attribute values are not fully specified.This is very common in many applications including bio-informatics analysis as well as collaborative filtering analysis, where the data may be incomplete and subject to biases. In bio-informatics, a bicluster model has recently been proposed to capture coherence among a subset of the attributes. Here, we introduce a more general model, referred to as the delta-cluster model, to capture coherence exhibited by a subset of objects on a subset of attributes, while allowing absent attribute values. A move-based algorithm (FLOC) is devised to efficiently produce a near-optimal clustering results.The delta-cluster model takes the bicluster model as a special case,where the FLOC algorithm performs far superior to the bicluster algorithm. We demonstrate the correctness and efficiency of the delta-cluster model and the FLOC algorithm on a number of real and synthetic data sets.

#index 659968
#* Efficiently Ordering Query Plans for Data Integration
#@ 
#t 2002
#c 17
#! The goal of a data integration system is to provide a uniform interface to a multitude of data sources. Given a user query formulated in this interface, the system translates it into a set of query plans. Each plan is a query formulated over the data sources, and specifies a way to access sources and combine data to answer the user query.In practice, when the number of sources is large, a data-integration system must generate and execute many query plans with significantly varying utilities. Hence, it is crucial that the system finds the best plans efficiently and executes them first, to guarantee acceptable time to and the quality of the first answers. We describe efficient solutions to this problem. First, we formally define the problem of ordering query plans. Second, we identify several interesting structural properties of the problem and describe three ordering algorithms that exploit these properties. Finally, we describe experimental results that suggest guidance on which algorithms perform best under which conditions.

#index 659969
#* Exploring Aggregate Effect with Weighted Transcoding Graphs for Efficient Cache Replacement in Transcoding Proxies
#@ 
#t 2002
#c 17
#! This paper explores the a re ate effect when caching multiple versions of the same Web object in the transcoding proxy. Explicitly, the aggregate pro?t from caching multiple versions of an object is not simply the sum of the pro?ts from caching individual versions, but rather, depends on the transcoding relationships among them. Hence, to evaluate the pro?t from caching each version of an object ef?ciently, we devise the notion of a weighted transcoding graph and formulate a generalized pro?t function which explicitly considers the aggregate effect and several new emerging factors in the transcoding proxy. Based on the weighted transcoding graph and the generalized pro?t function, an innovative cache replacement algorithm for transcoding proxies is proposed in this paper. Experimental results show that the algorithm proposed consistently outperforms companion schemes in terms of the delay saving ratios and cache hit ratios.

#index 659970
#* Active XQuery
#@ 
#t 2002
#c 17
#! Besides being adopted as the new interchange format for the Internet, XML is finding increasing acceptance as a native data repository language. In order to make XML repositories fully equipped with data management capabilities, suitable query and update languages are being developed. However, once the user is allowed to perform updates, it is perceivably necessary to guarantee the correctness of his/her updates, especially if document validity or semantic constraints are violated. We address this problem by exploiting the well-grounded concept of active rules.In this paper, we propose Active XQuery, an active language for XML repositories that is based on a previously defined XQuery update model. In particular, we present the syntax and semantics of our language, aiming at emulating the trigger definition and execution model of SQL3. An active extension of XQuery arises nontrivial problems, related to the need of interleaving updates and triggers. These problems have led us to define an algorithm for update reformulation and to devise a compact semantics. In conclusion, the paper presents an architecture for rapid prototyping, and hints optimization and research issues.

#index 659971
#* Discovering Similar Multidimensional Trajectories
#@ Michail Vlachos;Dimitrios Gunopoulos;George Kollios
#t 2002
#c 17
#! We investigate techniques for analysis and retrieval of object trajectories in a two or three dimensional space. Examples include features extracted from video clips, animal mobility experiments, sign language recognition, mobile phone usage and so on. Such data usually contain a great amount of noise, that degrades the performance of previously used metrics. Therefore, here we formalize non-metric similarity functions based on the Longest Common Subsequence (LCSS), which are very robust to noise and furthermore provide an intuitive notion of similarity between trajectories by giving more weight to the similar portions of the sequences. Stretching of sequences in time is allowed, as well as global translating of the sequences in space. Efficient approximate algorithms that compute these similarity measures are also provided. We compare these new methods to the widely used Euclidean and Time Warping distance functions (for real and synthetic data) and show the superiority of our approach, especially under the strong presence of noise. We prove a weaker version of the triangle inequality and employ it in an indexing structure to answer nearest neighbor queries. Finally, we present experimental results that validate the accuracy and efficiency of our approach.

#index 659972
#* Streaming-Data Algorithms for High-Quality Clustering
#@ 
#t 2002
#c 17
#! Streaming data analysis has recently attracted attention in numerous applications including telephone records, web documents and clickstreams. For such analysis, single-pass algorithms that consume a small amount of memory are critical. We describe such a streaming algorithm that effectively clusters large data streams. We also provide empirical evidence of the algorithm's performance on synthetic and real data streams.

#index 659973
#* Lossy Reduction for Very High Dimensional Data
#@ 
#t 2002
#c 17
#! We consider the use of data reduction techniques for the problem of approximate query answering. We focus on applications for which accurate answers to selective queries are required, and for which the data are very high dimensional (having hundreds of attributes). We present a new data reduction method for this type of application, called the RS Kernel. We demonstrate the effectiveness of this method for answering difficult, highly selective queries over high dimensional data using several real datasets.

#index 659974
#* Sequenced Subset Operators: Definition and Implementation
#@ Anne Descour
#t 2002
#c 17
#! Difference, intersection, semi-join and anti-semi-join may be considered binary subset operators, in that they all return a subset of their left-hand argument. These operators are useful for implementing SQL's EXCEPT, INTERSECT, NOT IN and NOT EXISTS, distributed queries and referential integrity. Difference-all and intersection-all operate on multi-sets and track the number of duplicates in both argument relations; they are used to implement SQL's EXCEPT ALL and INTERSECT ALL. Their temporally sequenced analogues, which effectively apply the subset operator at each point in time, are needed for implementing these constructs in temporal databases. These SQL expressions are complex; most necessitate at least a three-way join, with nested NOT EXISTS clauses. We consider how to implement these operators directly in a DBMS. These operators are interesting in that they can fragment the left-hand validity periods (sequenced difference-all also fragments the right-hand periods) and thus introduce memory complications found neither in their non-temporal counterparts nor in temporal joins and semi-joins. This paper introduces novel algorithms for implementing these operators by ordering the computation so that fragments need not be retained in main memory. We evaluate these algorithms and demonstrate that they are no more expensive than a single conventional join.

#index 659975
#* Efficient Temporal Join Processing Using Indices
#@ 
#t 2002
#c 17
#! We examine the problem of processing temporal joins in the presence of indexing schemes. Previous work on temporal joins has concentrated on non-indexed relations which were fully scanned. Given the large data volumes created by the ever increasing time dimension, sequential scanning is prohibitive. This is especially true when the temporal join involves only parts of the joining relations (e.g., a given time interval instead of the whole timeline). Utilizing an index becomes then beneficial as it directs the join to the data of interest. We consider temporal join algorithms for three representative indexing schemes, namely a B+-tree, an R*-tree and a temporal index, the Multiversion B+-tree (MVBT). Both the B+-tree and R*-tree result in simple but not efficient join algorithms because neither index achieves good temporal data clustering. Better clustering is maintained by the MVBT through record copying. Nevertheless, copies can greatly affect the correctness and effectiveness of the join algorithms. We identify these problems and propose efficient solutions and optimizations. An extensive comparison of all index based temporal joins, using a variety of datasets and query characteristics shows that the MVBT based join algorithms are consistently faster. In particular the link-based algorithm has the most robust behavior. In our experiments it showed a ten-fold improvement over the R*-tree joins while it was between six and thirty times faster than the B+-tree joins.

#index 659976
#* Exploring Spatial Datasets with Histograms
#@ 
#t 2002
#c 17
#! As online spatial datasets grow both in number and sophistication, it becomes increasingly difficult for users to decide whether a dataset is suitable for their tasks, especially when they do not have prior knowledge of the dataset. The GeoBrowsing service developed for the ADL project provides users an effective and efficient way to explore the content of a spatial dataset. In this paper, we identify a set of spatial relations that need to be supported in browsing applications, namely, the contains, contained and the overlap relations. We prove a storage lower bound to answer queries about the contains relation accurately at a given grid resolution. We then present three storage-efficient approximation algorithms which we believe to be the first to estimate query selectivities about these spatial relations. Experimental results show that these algorithms provide highly accurate estimates in real time for a wide range of datasets with various characteristics.

#index 659977
#* OSSM: A Segmentation Approach to Optimize Frequency Counting
#@ 
#t 2002
#c 17
#! Computing the frequency of a pattern is one of the key operations in data mining algorithms. We describe a simple yet powerful way of speeding up any form of frequency counting satisfying the monotonicity condition. Our method, the optimized segment support map (OSSM), is a light-weight structure which partitions the collection of transactions into m segments, so as to reduce the number of candidate patterns that require frequency counting. We study the following problems: (1) What is the optimal number of segments to be used; and (2) Given a user-determined m, what is the best segmentation/composition of the m segments? For Problem 1, we provide a thorough analysis and a theorem establishing the minimum value of m for which there is no accuracy lost in using the OSSM. For Problem 2, we develop various algorithms and heuristics, which efficiently generate OSSMs that are compact and effective, to help facilitate segmentation.

#index 659978
#* Towards Meaningful High-Dimensional Nearest Neighbor Search by Human-Computer Interaction
#@ 
#t 2002
#c 17
#! Nearest Neighbor search is an important and widely used problem in a number of important application domains. In many of these domains, the dimensionality of the data representation is often very high. Recent theoretical results have shown that the concept of proximity or nearest neighbors may not be very meaningful for the high dimensional case. Therefore, it is often a complex problem to find good quality nearest neighbors in such data sets. Furthermore, it is also difficult to judge the value and relevance of the returned results. In fact, it is hard for any fully automated system to satisfy a user about the quality of the nearest neighbors found unless he is directly involved in the process. This is especially the case for high dimensional data in which the meaningfulness of the nearest neighbors found is questionable. In this paper, we address the complex problem of high dimensional nearest neighbor search from the user perspective by designing a system which uses effective cooperation between the human and the computer. The system provides the user with visual representations of carefully chosen subspaces of the data in order to repeatedly elicit his preferences about the data patterns which are most closely related to the query point. These preferences are used in order to determine and quantify the meaningfulness of the nearest neighbors. Our system is not only able to find and quantify the meaningfulness of the nearest neighbors, but is also able to diagnose situations in which the nearest neighbors found are truly not meaningful.

#index 659979
#* Fast Mining of Massive Tabular Data via Approximate Distance Computations
#@ 
#t 2002
#c 17
#! Tabular data abound in many data stores: traditional relational databases store tables, and new applications also generate massive tabular datasets. For example, consider the geographic distribution of cell phone traffic at different base stations across the country or the evolution of traffic at Internet routers over time.Detecting similarity patterns in such data sets (e.g., which geographic regions have similar cell phone usage distribution, which IP subnet traffic distributions over time intervals are similar, etc) is of great importance. Identification of such patterns poses many conceptual challenges (what is a suitable similarity distance function for two ``regions'') as well as technical challenges (how to perform similarity computations efficiently as massive tables get accumulated over time) that we address.We present methods for determining similar regions in massive tabular data. Our methods are for computing the ``distance'' between any two subregions of a tabular data: they are approximate, but highly accurate as we prove mathematically, and they are fast, running in time nearly linear in the table size. Our methods are general since these distance computations can be applied to any mining or similarity algorithms that use Lp norms. A novelty of our distance computation procedures is that they work for any Lp norms --- not only the traditional p=2 or p=1, but for all p

#index 659980
#* The BINGO! Focused Crawler: From Bookmarks to Archetypes
#@ 
#t 2002
#c 17

#index 659981
#* Demonstration: Active Asynchronous Transaction Management in High-Autonomy Federated Environment Using Data Agents: Global Change Master Directory v8.0
#@ 
#t 2002
#c 17

#index 659982
#* An Authorization System for Temporal Data
#@ 
#t 2002
#c 17

#index 659983
#* SG-WRAP: A Schema-Guided Wrapper Generator
#@ 
#t 2002
#c 17

#index 659984
#* XParent: An Efficient RDBMS-Based XML Database System
#@ 
#t 2002
#c 17

#index 659985
#* Predator-Miner: Ad hoc Mining of Associations Rules within a Database Management System
#@ 
#t 2002
#c 17

#index 659986
#* Using Unity to Semi-Automatically Integrate Relational Schema
#@ 
#t 2002
#c 17

#index 659987
#* YFilter: Efficient and Scalable Filtering of XML Documents
#@ 
#t 2002
#c 17

#index 659988
#* Peer-to-Peer Data Management
#@ 
#t 2002
#c 17

#index 659989
#* Database Replication for the Mobile Era
#@ 
#t 2002
#c 17

#index 659990
#* DBXplorer: A System for Keyword-Based Search over Relational Databases
#@ 
#t 2002
#c 17
#! Keyword-based search has been popularized by Internet search engines. While traditional database management systems offer powerful query languages, they do not allow keyword-based search. In this paper, we discuss DBXplorer, a system that enables keyword-based search in relational databases. DBXplorer has been implemented using a commercial relational database and web server and allows users to interact via a browser front-end. We outline the challenges and discuss the implementation of our system including the results of extensive experimental evaluation.

#index 659991
#* TAILOR: A Record Linkage Tool Box
#@ 
#t 2002
#c 17
#! Data cleaning is a vital process that ensures the quality of data stored in real-world databases. Data cleaning prob-lems are frequently encountered in many research areas, such as knowledge discovery in databases, data ware-housing, system integration and e-services. The process of identifying the record pairs that represent the same entity (duplicate records), commonly known as record linkage, is one of the essential elements of data cleaning. In this paper, we address the record linkage problem by adopt-ing a machine learning approach. Three models are pro-posed and are analyzed empirically. Since no existing model, including those proposed in this paper, has been proved to be superior, we have developed an interactive Record Linkage Toolbox named TAILOR. Users of TAI-LOR can build their own record linkage models by tuning system parameters and by plugging in in-house developed and public domain tools. The proposed toolbox serves as a framework for the record linkage process, and is de-signed in an extensible way to interface with existing and future record linkage models. We have conducted an ex-tensive experimental study to evaluate our proposed mod-els using not only synthetic but also real data. Results show that the proposed machine learning record linkage models outperform the existing ones both in accuracy and in performance.

#index 659992
#* Providing Database as a Service
#@ 
#t 2002
#c 17
#! In this paper, we explore a new paradigm for data management in which a third party service provider hosts "database as a service" providing its customers seamless mechanisms to create, store, and access their databases at the host site. Such a model alleviates the need for organizations to purchase expensive hardware and software, deal with software upgrades, and hire professionals for administrative and maintenance tasks which are taken over by the service provider. We have developed and deployed a database service on the Internet, called NetDB2, which is in constant use. In a sense, data management model supported by NetDB2 provides an effective mechanism for organizations to purchase data management as a service, thereby freeing them to concentrate on their core businesses. Among the primary challenges introduced by "database as a service" are additional overhead of remote access to data, an infrastructure to guarantee data privacy, and user interface design for such a service. These issues are investigated in the study. We identify data privacy as a particularly vital problem and propose alternative solutions based on data encryption. This paper is meant as a challenges paper for the database community to explore a rich set of research issues that arise in developing such a service.

#index 659993
#* Evaluating Top-k Queries over Web-Accessible Databases
#@ Amelie Marian
#t 2002
#c 17
#! A query to a web search engine usually consists of a list of keywords, to which the search engine responds with the best or ``top" k pages for the query. This top-k query model is prevalent over multimedia collections in general, but also over plain relational data for certain applications. For example, consider a relation with information on available restaurants, including their location, price range for one diner, and overall food rating. A user who queries such a relation might simply specify the user's location and target price range, and expect in return the best 10 restaurants in terms of some combination of proximity to the user, closeness of match to the target price range, and overall food rating. Processing such top-k queries efficiently is challenging for a number of reasons. One critical such reason is that, in many web applications, the relation attributes might not be available other than through external web-accessible form interfaces, which we will have to query repeatedly for a potentially large set of candidate objects. In this paper, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present several algorithms for processing such queries, and evaluate them thoroughly using both synthetic and real web-accessible data.

#index 659994
#* Design and Implementation of a High-Performance Distributed Web Crawler
#@ 
#t 2002
#c 17
#! Broad web search engines as well as many more specialized search tools rely on web crawlers to acquire large collections of pages for indexing and analysis. Such a web crawler may interact with millions of hosts over a period of weeks or months, and thus issues of robustness, flexibility, and manageability are of major importance. In addition, I/O performance, network resources, and OS limits must be taken into account in order to achieve high performance at a reasonable cost.In this paper, we describe the design and implementation of a distributed web crawler that runs on a network of workstations. The crawler scales to (at least) several hundred pages per second, is resilient against system crashes and other events, and can be adapted to various crawling applications. We present the software architecture of the system, discuss the performance bottlenecks, and describe efficient techniques for achieving high performance. We also report preliminary experimental results based on a crawl of $120$ million pages on $5$ million hosts.

#index 659995
#* Efficient Filtering of XML Documents with XPath Expressions
#@ 
#t 2002
#c 17
#! We propose a novel index structure, termed XTrie, that supports the efficient filtering of XML documents based on XPath expressions. Our XTrie index structure offers several novel features that make it especially attractive for large-scale publish/subscribe systems. First, XTrie is designed to support effective filtering based on complex XPath expressions (as opposed to simple, single-path specifications). Second, our XTrie structure and algorithms are designed to support both ordered and unordered matching of XML data. Third, by indexing on sequences of element names organized in a trie structure and using a sophisticated matching algorithm, XTrie is able to both reduce the number of unnecessary index probes as well as avoid redundant matchings, thereby providing extremely efficient filtering. Our experimental results over a wide range of XML document and XPath expression workloads demonstrate that our XTrie index structure outperforms earlier approaches by wide margins.

#index 659996
#* Design and Evaluation of Alternative Selection Placement Strategies in Optimizing Continuous Queries
#@ 
#t 2002
#c 17
#! In this paper, we design and evaluate alternative selection placement strategies for optimizing a very large number of continuous queries in an Internet environment. Two grouping strategies, PushDown and PullUp, in which selections are either pushed below, or pulled above, joins are proposed and investigated. While our earlier research has demonstrated that the incremental group optimization can significantly outperform an ungrouped approach, the results from this paper show that different incremental group optimization strategies can have significantly different performance characteristics. Surprisingly, in our studies, PullUp, in which selections are pulled above joins, is often better and achieves an average 10-fold performance improvement over PushDown (occasionally 100 times faster). Furthermore, a revised algorithm of PullUp, termed filtered PullUp is proposed that is able to further reduce the cost of PullUp by 75% when the union of the selection predicates is selective. Detailed cost models, which consider several special parameters, including (1) characteristics of queries to be grouped, and (2) characteristics of data changes, are presented in this paper. Preliminary experiments using an implementation of both strategies show that our models are fairly accurate in predicting the results obtained from the implementation of these techniques in the Niagara system. This work can serve as the basis for building a cost-based incremental group query optimizer to choose a better grouping strategy.

#index 659997
#* XGRIND: A Query-Friendly XML Compressor
#@ 
#t 2002
#c 17
#! XML documents are extremely verbose since the "schema" is repeated for every "record" in the document. While a variety of compressors are available to address this problem, they are not designed to support direct querying of the compressed document, a useful feature from a database perspective. In this paper, we propose a new compression tool called XGrind, that directly supports queries in the compressed domain. A special feature of XGrind is that the compressed document retains the structure of the original document, permitting reuse of the standard XML techniques for processing the compressed document. Performance evaluation over a variety of XML documents and user queries indicates that XGrind simultaneously delivers improved query processing times and reasonable compression ratios.

#index 659998
#* Mixing Querying and Navigation in MIX
#@ 
#t 2002
#c 17
#! Web-based information systems provide to their users the ability to interleave querying and browsing during their information discovery efforts. The MIX system provides an API called QDOM (Querible Document Object Model) that supports the interleaved querying and browsing of virtual XML views, specified in an XQuery-like language.QDOM is based on the DOM standard. It allows the client applications to navigate into the view using standard DOM navigation commands. Then the application can use any visited node as the root for a query that creates a new view.The query/navigation processing algorithms of MIX perform decontextualization, i.e., they translate a query that has been issued from within the context of other queries and navigations into efficient queries that are understood by the source outside of the context of previous operations. In addition, MIX provides a navigation-driven query evaluation model, where source data are retrieved only as needed by the subsequent navigations.

#index 659999
#* Structural Joins: A Primitive for Efficient XML Query Pattern Matching
#@ 
#t 2002
#c 17
#! XML queries typically specify patterns of selection predicates on multiple elements that have some specified tree structured relationships. The primitive tree structured relationships are parent-child and ancestor-descendant, and finding all occurrences of these relationships in an XML database is a core operation for XML query processing.In this paper, we develop two families of structural join algorithms for this task: tree-merge and stack-tree. The tree-merge algorithms are a natural extension of traditional merge joins and the recently proposed multi-predicate merge joins, while the stack-tree algorithms have no counterpart in traditional relational join processing. We present experimental results on a range of data and queries using the TIMBER native XML query engine built on top of SHORE. We show that while, in some cases, tree-merge algorithms can have performance comparable to stack-tree algorithms, in many cases they are considerably worse. This behavior is explained by analytical results that demonstrate that, on sorted inputs, the stack-tree algorithms have worst-case I/O and CPU complexities linear in the sum of the sizes of inputs and output, while the tree-merge algorithms do not have the same guarantee.

#index 660000
#* Exploiting Local Similarity for Indexing Paths in Graph-Structured Data
#@ 
#t 2002
#c 17
#! XML and other semi-structured data may have partially specified or missing schema information, motivating the use of a structural summary which can be automatically computed from the data. These summaries also serve as indices for evaluating the complex path expressions common to XML and semi-structured query languages. However, to answer all path queries accurately, summaries must encode information about long, seldom-queried paths, leading to increased size and complexity with little added value. We introduce the A(k)-indices, a family of approximate structural summaries. They are based on the concept of k-bisimilarity, in which nodes are grouped based on local structure, i.e., the incoming paths of length up to k. The parameter k thus smoothly varies the level of detail (and accuracy) of the A(k)-index. For small values of k, the size of the index is substantially reduced. While smaller, the A(k) index is approximate, and we describe techniques for efficiently extracting exact answers to regular path queries. Our experiments show that, for moderate values of k, path evaluation using the A(k)-index ranges from being very efficient for simple queries to competitive for most complex queries, while using significantly less space than comparable structures.

#index 660001
#* Similarity Flooding: A Versatile Graph Matching Algorithm and Its Application to Schema Matching
#@ Sergey Melnik;Hector Garcia-Molina;Erhard Rahm
#t 2002
#c 17
#! Matching elements of two data schemas or two data instances plays a key role in data warehousing, e-business, or even biochemical applications. In this paper we present a matching algorithm based on a fixpoint computation that is usable across different scenarios. The algorithm takes two graphs (schemas, catalogs, or other data structures) as input, and produces as output a mapping between corresponding nodes of the graphs. Depending on the matching goal, a subset of the mapping is chosen using filters. After our algorithm runs, we expect a human to check and if necessary adjust the results. As a matter of fact, we evaluate the `accuracy' of the algorithm by counting the number of needed adjustments. We conducted a user study, in which our accuracy metric was used to estimate the labor savings that the users could obtain by utilizing our algorithm to obtain an initial matching. Finally, we illustrate how our matching algorithm is deployed as one of several high-level operators in an implemented testbed for managing information models and mappings.

#index 660002
#* Recovery Guarantees for General Multi-Tier Applications
#@ 
#t 2002
#c 17
#! Database recovery does not mask failures to applications and users. Recovery is needed that considers data, messages, and application components. Special cases have been studied, but clear principles for recovery guarantees in general multi-tier applications such as web-based e-services are missing. We develop a framework for recovery guarantees that masks almost all failures. The main concept is an interaction contract between two components, a pledge as to message and state persistence, and contract release. Contracts are composed into system-wide agreements so that a set of components is provably recoverable with exactly-once message delivery and execution, except perhaps for crash interrupted user input or output. Our implementation techniques reduce logging cost, allow effective log truncation, and provide independent recovery for critical server components. Interaction contracts form the basis for our Phoenix/COM project on persistent components. Our framework's utility is demonstrated with a case study of a web-based e-service.

#index 660003
#* Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation
#@ 
#t 2002
#c 17
#! Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner.In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements.In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal.

#index 660004
#* Fjording the Stream: An Architecture for Queries Over Streaming Sensor Data
#@ 
#t 2002
#c 17
#! If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.

#index 660005
#* Bioinformatics Databases 1
#@ 
#t 2002
#c 17

#index 660006
#* Condensed Cube: An Efficient Approach to Reducing Data Cube Size
#@ 
#t 2002
#c 17
#! Pre-computed data cube facilitates OLAP (On-Line Analytical Processing). It is a well-known fact that data cube computation is an expensive operation, which attracts a lot of attention. While most proposed algorithms devoted themselves to optimizing memory management and reducing computation costs, less work addresses one of the fundamental issues: the size of a data cube is huge when a large base relation with a large number of attributes is involved. In this paper, we propose a new concept, called a condensed data cube. The condensed cube is of much smaller size of a complete non-condensed cube. More importantly, it is a fully pre-computed cube without compression, and, hence, it requires neither decompression nor further aggregation when answering queries. Several algorithms for computing condensed cube are proposed. Results of experiments on the effectiveness of condensed data cube are presented, using both synthetic and real-world data . The results indicate that the proposed condensed cube can reduce both the cube size and therefore its computation time.

#index 660007
#* Indexing Spatio-Temporal Data Warehouses
#@ 
#t 2002
#c 17
#! Spatio-temporal databases store information about the positions of individual objects over time. In many applications however, such as traffic supervision or mobile communication systems, only summarized data, like the average number of cars in an area for a specific period, or phones serviced by a cell each day, is required. Although this information can be obtained from operational databases, its computation is expensive, rendering online processing inapplicable. A vital solution is the construction of a spatiotemporal data warehouse. In this paper, we describe a framework for supporting OLAP operations over spatiotemporal data. We argue that the spatial and temporal dimensions should be modeled as a combined dimension on the data cube and present data structures, which integrate spatiotemporal indexing with pre-aggregation. While the well-known materialization techniques require a-priori knowledge of the grouping hierarchy, we develop methods that utilize the proposed structures for efficient execution of ad-hoc group-bys. Our techniques can be used for both static and dynamic dimensions.

#index 660008
#* Processing Reporting Function Views in a Data Warehouse Environment
#@ 
#t 2002
#c 17
#! Reporting functions reflect a novel technique to formulate sequence-oriented queries in SQL. They extend the classical way of grouping and applying aggregation functions by additionally providing a column-based ordering, partitioning, and windowing mechanism. The application area of reporting functions ranges from simple ranking queries (TOP(n)-analyses) over cumulative (Year-To-Date- analyses) to sliding window queries. In this paper we discuss the problem of deriving reporting function queries from materialized reporting function views, which is one of the most important issues in efficiently processing queries in a data warehouse environment. Two different derivation algorithms including their relational mappings are introduced and compared in a test scenario.

#index 660009
#* A Fast Regular Expression Indexing Engine
#@ 
#t 2002
#c 17
#! In this paper, we describe the design, architecture, and the lessons learned from the implementation of a fast regular expression indexing engine FREE. FREE uses a pre-built index to identify the text data units which may contain a matching string and only examines these further. In this way, FREE shows orders of magnitude performance improvement in certain cases over standard regular expression matching systems, such as lex, awk and grep.

#index 660010
#* Geometric-Similarity Retrieval in Large Image Bases
#@ 
#t 2002
#c 17
#! We propose a novel approach to shape-based image retrieval that builds upon a similarity criterion which is based on the average point set distance. Compared to traditional techniques, such as dimensionality reduction, our method exhibits better behavior in that it maintains the average topology of shapes independently of the number of points used to represent them and is more resilient to noise. An efficient algorithm is presented based on an incremental ``fattening'' of the query shape until the best match is discovered. The algorithm uses simplex range search techniques and fractional cascading to provide an average poly-logarithmic time complexity on the total number of shape vertices. The algorithm is extended to perform additional fast approximate matching, when there is no image sufficiently similar to the query image. We present techniques for the efficient external storage of the shape base and of the auxiliary geometric data structures used by the algorithm. Finally, we show how our approach can be used for processing queries, containing pairwise relations of object boundaries such as contain, tangent, and overlap. Such queries are either extracted from some user drafted sketch or defined explicitly by the user. Alternative methods are presented for forming query execution plans.

#index 660011
#* Keyword Searching and Browsing in Databases using BANKS
#@ Arvind Hulgeri;Charuta Nakhe
#t 2002
#c 17
#! With the growth of the Web, there has been a rapid increase in the number of users who need to access online databases without having a detailed knowledge of the schema or of query languages; even relatively simple query languages designed for non-experts are too complicated for them. We describe BANKS, a system which enables keyword-based search on relational databases, together with data and schema browsing. BANKS enables users to extract information in a simple manner without any knowledge of the schema or any need for writing complex queries. A user can get information by typing a few keywords, following hyperlinks, and interacting with controls on the displayed results.BANKS models tuples as nodes in a graph, connected by links induced by foreign key and other relationships. Answers to a query are modeled as rooted trees connecting tuples that match individual keywords in the query. Answers are ranked using a notion of proximity coupled with a notion of prestige of nodes based on inlinks, similar to techniques developed for Web search. We present an efficient heuristic algorithm for finding and ranking query results.

#index 660012
#* The Evolution of eBusiness Integration-from Data to Process
#@ 
#t 2002
#c 17

#index 660013
#* P2P Information Systems
#@ 
#t 2002
#c 17

#index 660014
#* Techniques for Storing XML
#@ 
#t 2002
#c 17

#index 660597
#* A Graph-Based Approach for Extracting Terminological Properties of Elements of XML Documents
#@ 
#t 2001
#c 17
#! Abstract: XML is rapidly becoming a standard for information exchange over the Web. Web providers and applications using XML for representing and exchanging their data make their information available in such a way that interoperability can be easily reached. However, in order to guarantee both the exchange of XML documents and the interoperability between information providers, it is often needed to single out semantic similarity properties relating concepts of different XML documents. This paper gives a contribution in this framework by proposing a technique for extracting synonymies and homonymies. The derivation technique is based on a rich conceptual model (called SDR-Network) which is used to represent concepts expressed in XML documents as well as the semantic relationships holding among them.

#index 660598
#* B+-Tree Indexes with Hybrid Row Identifiers in Oracle8i
#@ 
#t 2001
#c 17
#! Abstract: Most commercial database systems support B+-tree indexes using either 1) physical row identifiers, for example, DB2 or 2) logical row identifiers, for example, NonStop SQL. Physical row identifiers provide fast access to data. However, unlike logical row identifiers, they need to be updated whenever the row moves. This paper describes an alternate approach where hybrid row identifiers are used. A hybrid row identifier consists of two components: 1) a logical component, namely, the primary key of the base table row, and 2) a physical component, namely, the Database Block Address (DBA) of the row. By treating the DBA as a guess regarding where the row may be found, performance comparable to physical B+-tree indexes is attained for valid guess-DBAs. This scheme retains the logical index advantage of avoiding an immediate index update when the base table row moves. Instead, an online utility can be used to lazily fix the invalid guess-DBAs. This scheme has been used to implement B+-tree indexes for Oracle8i index-organized tables (primary B+-tree like structure) which encounter both row movement and table reorganization. It can also be applied to conventional tables that undergo some form of row movement such as replication or transportation of data. This paper discusses the following: 1) key concepts for supporting B+-tree indexes with hybrid row identifiers, 2) performance study comparing such indexes with physical B+-tree indexes, and 3) applicability of B+-tree indexes with hybrid row identifiers to speed up table replication (full-refresh), to increase index availability, and to support online table operations.

#index 660599
#* B-Tree Indexes and CPU Caches
#@ 
#t 2001
#c 17
#! Abstract: Since many existing techniques for exploiting CPU caches in the implementation of B-tree indexes have not been discussed in the literature, most of them are surveyed here. Rather than providing a detailed performance evaluation for one or two of them on some specific contemporary hardware, the purpose here is to survey and to make widely available this heretofore-folkloric knowledge in order to enable, structure, and hopefully stimulate future research.

#index 660600
#* Spatial Clustering in the Presence of Obstacles
#@ 
#t 2001
#c 17
#! Abstract: Clustering in spatial data mining is to group similar objects based on their distance, connectivity, or their relative density in space. In the real world, there exist many physical obstacles such as rivers, lakes and high-ways, and their presence may affect the result of clustering substantially. In this paper, we study the problem of clustering in the presence of obstacles and define it as a COD (Clustering with Obstructed Distance) problem. As a solution to this problem, we propose a scalable clustering algorithm, called COD-CLARANS . We discuss various forms of pre-processed information that could enhance the efficiency of COD-CLARANS . In thestrictest sense, the COD problem can be treated as a change in distance function and thus could be handled by current clustering algorithms by changing the distance function. However, we show that by pushing the task of handling obstacles into COD-CLARANS instead of abstracting it at the distance function level, more optimization can be done in the form of a pruning function E'. We conduct various performance studies to show that COD-CLARANS is both efficient and effective.

#index 660601
#* Selectivity Estimation for Spatial Joins
#@ 
#t 2001
#c 17
#! Abstract: Spatial Joins are important and time consuming operations in spatial database management systems. It is crucial to be able to accurately estimate the performance of these operations so that one can derive efficient query execution plans, and even develop/refine data structures to improve their performance. While estimation techniques for analyzing the performance of other operations, such as range queries, on spatial data has come under scrutiny, the problem of estimating selectivity for spatial joins has been little explored. The limited forays into this area have used parametric techniques, which are largely restrictive on the data sets that they can be used for since they tend to make simplifying assumptions about the nature of the datasets to be joined. Sampling and histogram based techniques, on the other hand, are much less restrictive. However, there has been no prior attempt at understanding the accuracy of sampling techniques, or developing histogram based techniques to estimate the selectivity of spatial joins. Apart from extensively evaluating the accuracy of sampling techniques for the very first time, this paper presents two novel histogram based solutions for spatial join estimation. Using a wide spectrum of both real and synthetic datasets, it is shown that one of our proposed schemes, called Geometric Histograms (GH), can accurately quantify the selectivity of spatial joins.

#index 660602
#* Integrating Data Mining with SQL Databases: OLE DB for Data Mining
#@ 
#t 2001
#c 17
#! Abstract: The integration of data mining with traditional database systems is key to making it convenient, easy to deploy in real applications, and to growing its user base. In this paper we describe the new API for data mining proposed by Microsoft as extensions to OLE DB standard. We illustrate the basic notions that motivated the API's design and describe the key components of an OLE DB for Data Mining provider. We also include examples of the usage and treat the problems of data representation and integration with the SQL framework. We believe this new API will go a long way in enabling deployment of data mining in enterprise data-warehouses. A reference implementation of a provider is available with the recent release of Microsoft SQL Server 2000 database system.

#index 660603
#* SAP Business Information Warehouse - From Data Warehousing to an E-Business Platform
#@ 
#t 2001
#c 17
#! Abstract: E-commerce applications have imposed a huge set of new paradigms and challenges to the entire soft- and hardware community. In this paper, we focus on the changes and challenges that data warehouses already face in this context. SAP provides with its Business Information Warehouse (BW) a basic infrastructure element for its e-commerce platform mySAP.com. We will summarise some of the experience that we have made when adjusting and extending BW to fit the requirements of mySAP.com.

#index 660604
#* fAST Refresh using Mass Query Optimization
#@ Bobbie Cochrane
#t 2001
#c 17
#! Abstract: Automatic Summary Tables (ASTs), more commonly known as materialized views, are widely used to enhance query performance, particularly for aggregate queries. Such queries access a huge number of rows to retrieve aggregated summary data while performing multiple joins in the context of a typical data warehouse star schema. To keep ASTs consistent with their underlying base data, the ASTs are either immediately synchronized or fully recomputed. This paper proposes an optimization strategy for simultaneously refreshing multiple ASTs, thus avoiding multiple scans of a large fact table (one pass for AST computation). A query stacking strategy detects common sub-expressions using the available query matching technology of DB2. Since exact common sub-expressions are rare, the novel query sharing approach systematically generates common sub-expressions for a given set of "related" queries, considering different predicates, grouping expressions, and sets of base tables. The theoretical framework, a prototype implementation of both strategies in the IBM DB2 UDB/UWO database system, and performance evaluations based on the TPC/R data schema are presented in this paper.

#index 660605
#* Quality-Aware and Load-Sensitive Planning of Image Similarity Queries
#@ 
#t 2001
#c 17
#! Abstract: Evaluating similarity queries over image collections effectively and efficiently is an important but difficult issue. In many settings, a system does not deal with individual queries1 in isolation, there rather is a stream of queries. Re-searchers have proposed a number of query-evaluation alternatives and generalizations, in particular parallel methods over several components, and methods that yield approximate results. Choosing a plan for a given query is subject to more criteria than in conventional settings, notably result quality next to response time and resource consumption. We have designed and implemented a query planner that incorporates these concepts. We describe our space of possible plans and how we search this space. The usefulness of such a planner depends on a number of criteria, e.g., increase of throughput, adaptivity to different workloads, query planning overhead, or influence of the scoring function in quantitative terms. This article describes respective evaluations and shows that the benefit of our particular approach is significant.

#index 660606
#* A Cost Model and Index Architecture for the Similarity Join
#@ 
#t 2001
#c 17
#! Abstract: The similarity join is an important database primitive which has been successfully applied to speed up data mining algorithms. In the similarity join, two point sets of a multidimensional vector space are combined such that the result contains all point pairs where the distance does not exceed a parameter e . Due to its high practical relevance, many similarity join algorithms have been devised. In this paper, we propose an analytical cost model for the similarity join operation based on indexes. Our problem analysis reveals a serious optimization conflict between CPU time and I/O time: Fine-grained index structures are beneficial for the CPU efficiency, but deteriorate the I/O performance. As a consequence of this observation, we propose a new index architecture and join algorithm which allows a separate optimization of CPU time and I/O time. Our solution utilizes large pages which are optimized for I/O processing. The pages accommodate a search structure which minimizes the computational effort. In the experimental evaluation, a substantial improvement over competitive techniques is shown.

#index 660607
#* The Skyline Operator
#@ 
#t 2001
#c 17
#! Abstract: We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SQL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations, e.g., join.

#index 660608
#* Mining Frequent Itemsets with Convertible Constraints
#@ 
#t 2001
#c 17
#! Abstract: Recent work has highlighted the importance of the constraint-based mining paradigm in the context of frequent itemsets, associations, correlations, sequential patterns, and many other interesting patterns in large databases. In this paper, we study constraints which cannot be handled with existing theory and techniques. For example, avg(S)\theta v, median(S)\theta v, sum(S)\theta v(S can contain items of arbitrary values) (\theta\in\{\geq,\leq\}), are customarily regarded as "tough" constraints in that they cannot be pushed inside an algorithm such as a priori. We develop notion of convertible constraints and systematically analyze, classify, and characterize this class. We also develop techniques which enable them to be readily pushed deep inside the recently developed FP-growth algorithm for frequent itemset mining. Results from our detailed experiments show the effectiveness of the techniques developed.

#index 660609
#* MAFIA: A Maximal Frequent Itemset Algorithm for Transactional Databases
#@ Doug Burdick
#t 2001
#c 17
#! Abstract: We present a new algorithm for mining maximal frequent itemsets from a transactional database. Our algorithm is especially efficient when the itemsets in the database are very long. The search strategy of our algorithm integrates a depth-first traversal of the itemset lattice with effective pruning mechanisms. Our implementation of the search strategy combines a vertical bitmap representation of the database with an efficient relative bitmap compression schema. In thorough experimental analysis of our algorithm on real data, we isolate the effect of the individual components of the algorithm. Our performance numbers show that our algorithm outperforms previous work by a factor of three to five.

#index 660610
#* An Efficient Approximation Scheme for Data Mining Tasks
#@ 
#t 2001
#c 17
#! Abstract: We investigate the use of biased sampling according to the density of the dataset, to speed up the operation of general data mining tasks, such as clustering and outlier detection in large multidimensional datasets. In density-biased sampling, the probability that a given point will be included in the sample depends on the local density of the dataset. We propose a general technique for density-biased sampling that can factor in user requirements to sample for properties of interest, and can be tuned for specific data mining tasks. This allows great flexibility, and improved accuracy of the results over simple random sampling. We describe our approach in detail, we analytically evaluate it, and show how it can be optimized for approximate clustering and outlier detection. Finally we present a thorough experimental evaluation of the proposed method, applying density-biased sampling on real and synthetic data sets, and employing clustering and outlier detection algorithms, thus highlighting the utility of our approach.

#index 660611
#* Bringing the Internet to Your Database: Using SQL Server 2000 and XML to Build Loosely-Coupled Systems
#@ 
#t 2001
#c 17
#! Abstract: Loosely-coupled, distributed system architectures need to be flexible enough to allow individual components to join or leave the heterogeneous conglomerate of services and components and to change their internal design and data models without jeopardizing the whole architecture. A well-established approach is to use XML as the lingua franca for the integration layer that hides the heterogeneity among the components and provides the glue that allows the individual components to take part in the loosely integrated system. This presentation focuses on how to provide the basic technology to enable a relational database to become a component in such loosely-coupled systems and it will provide an overview over the features that are needed to provide access via HTTP and XML.

#index 660612
#* Infrastructure for Web-based Application Integration
#@ 
#t 2001
#c 17
#! Abstract: Over the last couple of years application integration has taken a central position in the business world. Application integration deals with integration of computing environments within and between companies and depends on connectivity provided by the Intranet and Internet respectively. Application Integration is typically referred to as EAI (e-Business Application Integration). This article sketches first the evolution of business computing and EAI. The major elements of a modem EAI technology are the focus of the discussion, with special attention to Web-based application integration. Finally, the article points to some interesting research topics.

#index 660613
#* Developing Web Services
#@ 
#t 2001
#c 17
#! Abstract: Web-based application developers are attempting to move towards Web Services as a mechanism for developing component-based Web applications. Unfortunately traditional tools and development models are inadequately architected to meet the rapidly evolving needs for the future of scalable Web Services. Today's Web Services development model is mired with complexity as traditional tools and technologies, focused on client-server application development, are ill suited. In this paper we examine the challenges of today's Web Service development model.

#index 660614
#* An Index Structure for Efficient Reverse Nearest Neighbor Queries
#@ 
#t 2001
#c 17
#! Abstract: The Reverse Nearest Neighbor (RNN) problem is to find all points in a given data set whose nearest neighbor is a given query point. Just like the Nearest Neighbor (NN) queries, the RNN queries appear in many practical situations such as marketing and resource management. Thus efficient methods for the RNN queries in databases are required. This paper introduces a new index structure, the Rdnn-tree, that answers both RNN and NN queries efficiently. A single index structure is employed for a dynamic database, in contrast to the use of multiple indexes in previous work. This leads to significant savings in dynamically maintaining the index structure. The Rdnn-tree outperforms existing methods in various aspects. Experiments on both synthetic and real world data show that our index structure outperforms previous method by a significant margin (more than 90% in terms of number of leaf nodes accessed) in RNN queries. It also shows improvement in NN queries over standard techniques. Furthermore, performance in insertion and deletion is significantly enhanced by the ability to combine multiple queries (NN and RNN) in one traversal of the tree. These facts make our index structure extremely preferable in both static and dynamic cases.

#index 660615
#* Distinctiveness-Sensitive Nearest-Neighbor Search for Efficient Similarity Retrieval of Multimedia Information
#@ 
#t 2001
#c 17
#! Abstract: Nearest neighbor (NN) search in high dimensional feature space is widely used for similarity retrieval of multi-media information. However, recent research results in the database literature reveal that a curious problem happens in high dimensional space. Since high dimensional space has high degree of freedom, points could be so scattered that every distance between them might yield no significant difference. In this case, we can say that the NN is indistinctive because many points exist at the similar distance. To make matters worse, indistinctive NNs require more search cost because search completes only after choosing the NN from plenty of strong candidates. In order to circumvent the harmful effect of indistinctive NNs, this paper presents a new NN search algorithm which determines the distinctiveness of the NN during search operation. This enables us not only to cut down search cost but also to distinguish distinctive NNs from indistinctive ones. These advantages are especially beneficial to interactive retrieval systems.

#index 660616
#* Approximate Nearest Neighbor Searching in Multimedia Databases
#@ 
#t 2001
#c 17
#! Abstract: In this paper, we develop a general framework for approximate nearest neighbor queries. We categorize the current approaches for nearest neighbor query processing based on either their ability to reduce the data set that needs to be examined, or their ability to reduce the representation size of each data object. We first propose modifications to well-known techniques to support the progressive processing of approximate nearest neighbor queries. A user may therefore stop the retrieval process once enough information has been returned. We then develop a new technique based on clustering that merges the benefits of the two general classes of approaches. Our cluster-based approach allows a user to progressively explore the approximate results with increasing accuracy. We propose a new metric for evaluation of approximate nearest neighbor searching techniques. Using both the proposed and the traditional metrics, we analyze and compare several techniques with a detailed performance evaluation. We demonstrate the feasibility and efficiency of approximate nearest neighbor searching. We perform experiments on several real data sets and establish the superiority of the proposed cluster-based technique over the existing techniques for approximate nearest neighbor searching.

#index 660617
#* Rewriting OLAP Queries Using Materialized Views and Dimension Hierarchies in Data Warehouses
#@ 
#t 2001
#c 17
#! Abstract: OLAP queries involve a lot of aggregations on a large amount of data in data warehouses. To process expensive OLAP queries efficiently, we propose a new method for rewriting a given OLAP query using various kinds of materialized aggregate views which already exist in data ware-houses. We first define the normal forms of OLAP queries and materialized views based on the lattice of dimension hierarchies, the semantic information in data warehouses. Conditions for usability of a materialized view in rewriting a given query are specified by relationships between the components of their normal forms. We present a rewriting algorithm for OLAP queries that effectively utilizes existing materialized views. The proposed algorithm can make use of materialized views having different selection granularities, selection regions, and aggregation granularities together to generate an efficient rewritten query.

#index 660618
#* The MD-Join: An Operator for Complex OLAP
#@ 
#t 2001
#c 17
#! Abstract: OLAP queries (i.e. group-by or cube-by queries with aggregation) have proven to be valuable for data analysis and exploration. Many decision support applications need very complex OLAP queries, requiring a fine degree of control over both the group definition and the aggregates that are computed. For example, suppose that the user has access to a data cube whose measure attribute is Sum(Sales). Then the user might wish to compute the sum of sales in New York and the sum of sales in California for those data cube entries in which Sum(Sales) $1,000,000. This type of complex OLAP query is often difficult to express and difficult to optimize using standard relational operators (including standard aggregation operators). In this paper we propose the MD-join operator for complex OLAP queries. The MD-join provides a clean separation between group definition and aggregate computation, allowing great flexibility in the expression of OLAP queries. In addition, the MD-join has a simple and easily optimizable implementation, while the equivalent relational algebra expression is often complex and difficult to optimize. We present several algebraic transformations that allow relational algebra queries that include MD-joins to be optimized.

#index 660619
#* Overcoming Limitations of Sampling for Aggregation Queries
#@ 
#t 2001
#c 17
#! Abstract: We study the problem of approximately answering aggregation queries using sampling. We observe that uniform sampling performs poorly when the distribution of the aggregated attribute is skewed. To address this issue, we introduce a technique called outlier-indexing. Uniform sampling is also ineffective for queries with low selectivity. We rely on weighted sampling based on workload information to overcome this shortcoming. We demonstrate that a combination of outlier-indexing with weighted sampling can be used to answer aggregation queries with significantly reduced approximation error compared to either uniform sampling or weighted sampling alone. We discuss the implementation of these techniques on Microsoft's SQL Server, and present experimental results that demonstrate the merits of our techniques.

#index 660620
#* Pseudo Column Level Locking
#@ 
#t 2001
#c 17
#! Abstract: In this paper we present an optimization that can significantly enhance the concurrency provided by row level locking in some RDBMSs. With this optimization, the degree of concurrency obtained is comparable to that in column level locking in some cases that are common at customer sites. Besides, this optimization does not increase the overhead of concurrency control significantly. We use the term pseudo column level locking (PCLL) for row level locking enhanced with this optimization. While some versioning based concurrency control schemes may offer better concurrency than PCLL, they are usually hard to incorporate in an RDBMS that relies on locking for concurrency control. Our technique is simple to implement, and it has been implemented in Sybase Adaptive Server Enterprise (ASE).

#index 660621
#* Discovery and Application of Check Constraints in DB2
#@ 
#t 2001
#c 17
#! Abstract: The traditional role of integrity constraints is to protect the integrity of data. But integrity constraints can and do play other roles in databases; for example, they can be used for query optimization. In this role, they do not need to model the domain; it is sufficient that they describe regularities that are true about the data currently stored in a database. In this paper we describe two algorithms for finding such regularities (in the syntactic form of check constraints) and discuss some of their applications in DB2. In particular, we show their use in query optimization.

#index 660622
#* Database Managed External File Update
#@ 
#t 2001
#c 17
#! Abstract: RDBMS's have evolved to an extent that they are used to manage almost all of traditional business data in a robust fashion. Nevertheless, a large fraction of unstructured and semi-structured data continues to be managed by file systems. As companies increasingly depend on non-traditional data such as web pages and images for their daily business operations, it becomes more and more important to provide higher degree of integrity, security, and reliability to the data stored in file systems. DataLinks technology developed at IBM Almaden Research center achieves this by providing a vital integration between RDBMS and file system. It enables DBMS to manage files residing in file systems as though they are logically within the database. Current DataLinks technology supports only read access to external files that are being managed by DBMS. This severely restricts the applicability of DataLinks technology in transaction oriented and/or e-business applications. Traditional database systems enforce ACID properties for database update. Extending these properties to cover both external files (such as web pages) stored outside of a DBMS and metadata stored in the DBMS is a hard problem. This is because files are updated through standard file system API while metadata, which reference the files, are updated through database API. This paper describes our experiences in the design and prototyping of an advanced DataLinks technology that supports database managed external file update. This enhanced capability makes DataLinks technology an even more attractive solution for managing world's data.

#index 660623
#* Block Oriented Processing of Relational Database Operations in Modern Computer Architectures
#@ 
#t 2001
#c 17
#! Abstract: Several recent papers have pointed out that database systems are not well tuned to take advantage of modern superscalar processor architectures. In particular, the Clocks-Per-Instruction (CPI) for rather simple database queries are quite poor compared to scientific kernels or SPEC benchmarks. The lack of performance of database systems has been attributed to poor utilization of caches and processor function units as well as higher branching penalties. In this paper, we argue that a block oriented processing strategy for database operations can lead to better utilization of the processors and caches generating significantly higher performance. We have implemented the block oriented processing technique for aggregation, expression evaluation, and sorting operations as a feature in the DB2 Universal Database system. We present results from representative queries on a 30 GB TPC-H database to show the value of this technique.

#index 660624
#* Integrating Semi-Join-Reducers into State-of-the-Art Query Processors
#@ 
#t 2001
#c 17
#! Abstract: Semi-join reducers were introduced in the late seventies as a means to reduce the communication costs of distributed database systems. Subsequent work in the eighties showed, however, that semi-join reducers are rarely beneficial for the distributed systems of that time. This work shows that semi-join reducers can indeed be beneficial in modern client-server or middleware systems--either to reduce communication costs or to better exploit all the resources of a system. Furthermore, we present and evaluate alternative ways to extend state-of-the-art (dynamic programming) query optimizers in order to generate good query plans with semi-join reducers. We present two variants, called Access Root and Join Root, which differ in their implementation complexity, their running times, and the quality of plans they produce. We present the results of performance experiments that compare both variants with a traditional query optimizer.

#index 660625
#* Using EELs, a Practical Approach to Outerjoin and Antijoin Reordering
#@ 
#t 2001
#c 17
#! Abstract: Outerjoins and antijoins are two important classes of joins in database systems. Reordering outerjoins and antijoins with inner joins is challenging because not all the join orders preserve the semantics of the original query. Previous work didn't consider antijoins and was restricted to a limited class of queries. We consider using a conventional bottom-up optmizer to reorder different types of joins. We propose extending each join predicate's eligibility list, which contains all the tables referenced in the predicate. An extended eligibility list (EEL) includes all the tables needed by a predicate to preserve the semantics of the original query. We describe an algorithm that can set up the EELs properly in a bottom-up traversal of the original operator tree. A conventional join optimizer is then modified to check the EELs when generating subplans. Our approach handles antijoin and can resolve many practical issues. It is now being implemented in an upcoming release of IBM's Universal Database Server for Unix, Windows and OS/2.

#index 660626
#* Counting Twig Matches in a Tree
#@ 
#t 2001
#c 17
#! Abstract: We describe efficient algorithms for accurately estimating the number of matches of a small node-labeled tree, i.e., a twig, in a large node-labeled tree, using a summary data structure. This problem is of interest for queries on XML and other hierarchical data, to provide query feedback and for cost-based query optimization. Our summary data structure scalably represents approximate frequency information about twiglets (i.e., small twigs) in the data tree. Given a twig query, the number of matches is estimated by creating a set of query twiglets, and combining two complementary approaches: Set Hashing, used to estimate the number of matches of each query twiglet, and Maximal Overlap, used to combine the query twiglet estimates into an estimate for the twig query. We propose several estimation algorithms that apply these approaches on query twiglets formed using variations on different twiglet decomposition techniques. We present an extensive experimental evaluation using several real XML data sets, with a variety of twig queries. Our results demonstrate that accurate and robust estimates can be achieved, even with limited space.

#index 660627
#* An Index-Based Approach for Similarity Search Supporting Time Warping in Large Sequence Databases
#@ 
#t 2001
#c 17
#! Abstract: This paper proposes a new novel method for similarity search that supports time warping in large sequence databases. Time warping enables finding sequences with similar patterns even when they are of different lengths. Previous methods for processing similarity search that supports time warping fail to employ multi-dimensional indexes without false dismissal since the time warping distance does not satisfy the triangular inequality. Our primary goal is to innovate on search performance without permitting any false dismissal. To attain this goal, we devise a new distance function D_{tw-lb} that consistently underestimates the time warping distance and also satisfies the triangular inequality. D_{tw-lb} uses a 4-tuple feature vector that is extracted from each sequence and is invariant to time warping. For efficient processing of similarity search, we employ a multi-dimensional index that uses the 4-tuple feature vector as indexing attributes and D_{tw-lb} as distance function. The extensive experimental results reveal that our method achieves significant speedup up to 43 times with real-world S&P 500 stock data and up to 720 times with very large synthetic data.

#index 660628
#* High Dimensional Similarity Search With Space Filling Curves
#@ 
#t 2001
#c 17
#! Abstract: We present a new approach for approximate nearest neighbor queries for sets of high dimensional points under any Lt-metric, t = 1; : : : ;1. The proposed algorithm is efficient and simple to implement. The algorithm uses multiple shifted copies of the data points and stores them in up to (d +1) B-trees where d is the dimensionality of the data, sorted according to their position along a space filling curve. This is done in a way that allows us to guarantee that a neighbor within an O(d 1+1=t ) factor of the exact nearest, can be returned with at most (d + 1)log p n page accesses, where p is the branching factor of the B-trees. In practice, for real data sets, our approximate technique finds the exact nearest neighbor between 87% and 99% of the time and a point no farther than the third nearest neighbor between 98% and 100% of the time. Our solution is dynamic, allowing insertion or deletion of points in O(d log p n) page accesses and generalizes easily to find approximate k-nearest neighbors.

#index 660629
#* Similarity Search without Tears: The OMNI-Family of All-Purpose Access Methods
#@ 
#t 2001
#c 17
#! Abstract: Designing a new access method inside a commercial DBMS is cumbersome and expensive. We propose a family of metric access methods that are fast and easy to implement on top of existing access methods, such as sequential scan, R-trees and Slim-trees. The idea is to elect a set of objects as foci, and gauge all other object with their distances from this set. We show how to define the foci set cardinality, how to choose appropriate foci, and how to perform range and nearest-neighbor queries using them, without false dismissals. The foci increase the pruning of distance calculations during the query processing. Furthermore we index the distances from each object to the foci to reduce even triangular inequality comparisons. Experiments on real and synthetic datasets show that our methods match or outperform existing methods. They are up to 10 times faster, and perform up to 10 times fewer distance calculations and disk accesses. In addition, it scale up well, exhibiting sub-linear performance with growing database size.

#index 660630
#* Tutorials Program
#@ 
#t 2001
#c 17

#index 660631
#* Demos Program
#@ 
#t 2001
#c 17

#index 660632
#* Cache-On-Demand: Recycling with Certainty
#@ 
#t 2001
#c 17
#! Abstract: Queries posed to a database usually access some common relations, or share some common sub-expressions. In this paper, we examine the issue of caching using a novel framework, called cache-on-demand (CoD). CoD views intermediate/ final answers of existing running queries as virtual caches that an incoming query can exploit. Those caches that are beneficial may then be materialized for the incoming query. Such an approach is essentially non-speculative: the exact cost of investment and the return on investment are known, and the cache is certain to be reused! We addressed several issues for CoD to be realized. We also propose two optimizing strategies, Conform-CoD and Scramble-CoD, and evaluated their performance. Our results show that CoD-based schemes can provide substantial performance improvement.

#index 660633
#* Cache-Aware Query Routing in a Cluster of Databases
#@ 
#t 2001
#c 17
#! Abstract: We investigate query routing techniques in a cluster of databases for a query-dominant environment. The objective is to decrease query response time. Each component of the cluster runs an off-the-shelf DBMS and holds a copy of the whole database. The cluster has a coordinator that routes each query to an appropriate component. Considering queries of realistic complexity, e.g., TPC-R, this article addresses the following questions: Can routing benefit from caching effects due to previous queries? Since our components are black-boxes, how can we approximate their cache content? How to route a query, given such cache approximations? To answer these questions, we have developed a cache-aware query router that is based on signature approximations of queries. We report on experimental evaluations with the TPC-R benchmark using our PowerDBdatabase cluster prototype. Our main result is that our approach of cache approximation routing is better than state-of-the-art strategies by a factor of two with regard to mean response time.

#index 660634
#* Prefetching Based on the Type-Level Access Pattern in Object-Relational DBMSs
#@ 
#t 2001
#c 17
#! Abstract: Prefetching is an effective method for minimizing the number of round-trips between the client and the server in database management systems. In this paper, we propose new notions of the type-level access locality and the type-level access pattern. We also formally define the notions of capturing and prefetching to help understand the underlying mechanisms. We then develop an efficient prefetching policy based on these notions and the framework. The type-level access locality is a pheonomenon that repetitive pat-terns exist in the attributes referenced. The type-level access pat-tern is a pattern of attributes that are referenced in accessing the objects. Existing prefetching methods are based on object-level or page-level access patterns, which consist of object-ids or page-ids of the objects accessed. However, the drawback of these methods is that they work only when exactly the same objects or pages are accessed repeatedly. In contrast, even though the same objects are not accessed repeatedly, our technique effectively prefetches objects if the same attributes are referenced repeatedly, i.e., if there is type-level access locality. Many navigational applications in Object-Relational Database Management Systems(ORDBMSs) have type-level access locality. Therefore, our technique can be employed in ORDBMSs to effectively reduce the number of round-trips, thereby significantly enhancing the performance. We have conducted extensive experiments in a prototype ORDBMS to show the effectiveness of our algorithm. Experimental results using the OO7 benchmark and a real GIS application show that our technique provides orders of magnitude improvements in round-trips and several factors of improvements in overall performance over on-demand fetching and context-based prefetching, which is a state-of-the-art prefetching method. These results indicate that our approach provides a new paradigm in prefetching that improves performance of navigational applications significantly and is a practical method that can be implemented in commercial OR-DBMSs.

#index 660635
#* SpinCircuit: A Collaborative Portal Powered By E-Speak
#@ 
#t 2001
#c 17
#! Abstract: SpinCircuit is collaborative portal serving semiconductor industry. SpinCircuit provides a web-based environment facilitating B-2-B collaboration to bring together component manufacturers component suppliers, contract manufacturers and design community in semiconductor space. It is based on E-speak technology from Hewlett-Packard. E-speak provides secure E-services infrastructure for creation, composition and discovery of E-services distributed across internet.

#index 660636
#* Exactly-Once Semantics in a Replicated Messaging System
#@ 
#t 2001
#c 17
#! Abstract: A wide-area distributed message delivery system can use replication to improve performance and availability. However, without safeguards, replicated messages may be delivered to a mobile device more than once, making the device's user repeat actions (e.g., making unnecessary phone calls, firing weapons repeatedly). Alternatively, they may not be delivered at all, making the user miss important messages. In this paper we address the problem of exactly-once delivery to mobile clients when messages are replicated globally. We define exactly-once semantics and propose algorithms to guarantee it. We also propose and define a relaxed version of exactly-once semantics which is appropriate for limited capability mobile devices. We study the relative performance of our algorithms compared to weaker at-least-once semantics, and find that the performance overhead of exactly-once can be minimized in most cases by careful design of the system.

#index 660637
#* CORBA Notification Service: Design Challenges and Scalable Solutions
#@ 
#t 2001
#c 17
#! Abstract: In this paper, we present READY, a multi-threaded implementation of the CORBA Notification Service. The main contribution of our work is the design and development of scalable solutions for the implementation of the CORBA Notification Service. In particular, we present the overall architecture of READY, discuss the key design challenges and choices we made with respect to filter evaluation and event dispatching, and present the current implementation status. Finally, we present preliminary experimental results from our current implementation.

#index 660638
#* Measuring and Optimizing a System for Persistent Database Sessions
#@ 
#t 2001
#c 17
#! Abstract: High availability for both data and applications is rapidly becoming a business requirement. While database systems support recovery, providing high database availability, applications may still lose work because of server outages. When a server crashes, volatile state associated with the application's database session is lost and the application may require operator-assisted restart. This exposes server failures end-users and always degrades application availability. Our Phoenix/ODBC system supports persistent database sessions that can survive a database crash without the application being aware of the outage, except for possible timing considerations. This improves application availability and eliminates application programming needed to cope with database crashes. Phoenix/ODBC requires no changes to database system, data access routines, or applications. Hence, it can be deployed any application that uses ODBC to access a database. Further, our generic approach can be exploited for a variety of data access protocols. In this paper, we describe the design Phoenix/ODBC and introduce an extension to optimize response time and reduce overhead for OLTP workloads. We present performance evaluation using TPC-C and TPC-H benchmarks that demonstrate Phoenix/ODBC's extra overhead is modest.

#index 660639
#* A Temporal Algebra for an ER-Based Temporal Data Model
#@ 
#t 2001
#c 17
#! Abstract: There were many temporal data models proposed in the literature. Most of them are based on relational models. Despite their popularity as design and analysis tools for information systems, ER-based temporal data models do not draw as much attention as those based on relational models did. One reason is that most ER-based temporal data models lack underlying formalism and algebra. If a conceptual model, along with an algebra, is formally defined, we can design a query language that operates on the conceptual model, not on the implementation model. Also, it can provide a basis for a user-friendly, visual query language. In this paper, we present a temporal algebra for an ER-based temporal data model called ITDM (Integrated Temporal Data Model). We define ten algebraic operations and ten temporal aggregate operations. We also define time-series specific operations.

#index 660640
#* A Split Operator for Now-Relative Bitemporal Databases
#@ 
#t 2001
#c 17
#! Abstract: The timestamps of now-relative bitemporal databases are modeled as growing, shrinking, or rectangular regions. The shape of these regions makes it a challenge to design bitemporal operators that a) are consistent with the point-based interpretation of a temporal database, b) preserve identity of the argument timestamps, c) ensure locality, and d) perform efficiently. We identify the bitemporal split operator as the basic primitive to implement a wide range advanced now-relative bitemporal operations. The bitemporal split operator splits each tuple of a bitemporal argument relation, such that equality and standard nontemporal algorithms can be used to implement the bitemporal counterparts with the aforementioned properties. Both a native database algorithm and an SQL implementation are provided. Our performance results show that the bitemporal split operator outperforms related approaches by orders magnitude and scales well.

#index 660641
#* Incremental Computation and Maintenance of Temporal Aggregates
#@ 
#t 2001
#c 17
#! Abstract: We consider the problems of computing aggregation queries in temporal databases, and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-trees and B-trees. SB-trees support fast lookup of aggregate results based on time, and can be maintained efficiently when the data changes. We also extend the basic SB-tree index to handle cumulative (also called moving-window)aggregates. For materialized aggregate views in a temporal database or warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.

#index 660642
#* The Importance of Extensible Database Systems for e-Commerce
#@ 
#t 2001
#c 17
#! Abstract: Over the last decade, database system products have been extended to provide support for defining, storing, updating, indexing, and retrieving complex data with full transaction semantics. Oracle, IBM, Informix and others have used extensibility technology to build database system extensions for text, image, spatial, audio/video, chemical, genetic, and other types of complex data. Currently, we find database systems being deployed in support of e-Commerce. In many cases these e-Commerce database applications use only simple SQL data types to represent items such as office supplies, computers, books, and CDs. There is also a large and important set of e-Commerce applications that employ complex data formats such as EDI, SWIFT, and HL7. The database extensibility features initially developed to support text, spatial and similar forms of complex data are now being used to build e-Commerce applications. Thus, database extensibility technology is evolving into an important mechanism to enable development of e-Commerce systems.

#index 660643
#* E-Business Applications for Supply Chain Management: Challenges and Solutions
#@ 
#t 2001
#c 17
#! Abstract: Supply chain management is a crucial activity in every company. Surprisingly, today most of the supply chain activities are carried out manually, and IT support often limited to having a set of (disconnected) data repositories. In addition, B2B communications are performed via phone, fax, or email. Increasing operational efficiency of the supply chain results in huge savings, and is the key towards remaining competitive even gaining a competitive advantage. Furthermore, more efficient supply chain also enables revenue growth, which is often impossible to sustain with the current manual operations. In this paper we discuss the requirements and challenges for e-business applications that support supply chain management. Then, we propose an architecture that meets the requirements and enables solutions that deliver results quickly and evolve with the business and environment. Both the requirements and the architecture are the results of several different types of supply chain automation projects in which we have been involved.

#index 660644
#* Model-Based Mediation with Domain Maps
#@ 
#t 2001
#c 17
#! Abstract: We propose an extension to current view-based mediator systems called model-based mediation, in which views are defined and executed at the level of conceptual models (CMs) rather than at the structural level. Structural integration and lifting of data to the conceptual level is "pushed down" from the mediator to wrappers which in our system export classes, associations, constraints, and query capabilities of a source. Another novel feature of our architecture is the use of domain maps, semantic nets of concepts and relationships that are used to mediate across sources from multiple worlds (i.e., whose data are related in indirect and often complex ways). As part of registering a source's CM with the mediator, the wrapper creates a "semantic index" of its data into the domain map. We show that these indexes not only semantically correlate the multiple worlds data and thereby support the definition of the integrated CM, but that they are also useful during query processing, for example, to select relevant sources. A first prototype of the system has been implemented for a complex Neuroscience mediation problem.

#index 660645
#* Processing Queries with Expensive Functions and Large Objects in Distributed Mediator Systems
#@ 
#t 2001
#c 17
#! Abstract: LeSelect is a mediator system which allows scientists to publish their resources (data and programs) so they can be transparently accessed. The scientists can typically issue queries which access distributed published data and involve the execution of expensive functions (corresponding to programs). Furthermore, the queries can involve large objects such as images (e.g., archived meteorological satellite data). In this context, the costs of transmitting large objects and invoking expensive functions are the dominant factors of execution time. In this paper, we first propose three query execution techniques which minimize these costs by taking full advantage of the distributed architecture of mediator systems like LeSelect. Then, we devise parallel processing strategies for queries including expensive functions. Based on experimentation, we show that it is hard to predict the optimal execution order when dealing with several functions. We propose a new hybrid parallel technique to solve this problem and give some experimental results.

#index 660646
#* Tuning an SQL-Based PDM System in a Worldwide Client/Server Environment
#@ E. Müller
#t 2001
#c 17
#! Abstract: The management of product-related data in a uniform and consistent way is a big challenge for many manufacturing enterprises, especially the large ones like DaimlerChrysler. So-called Product Data Management systems (PDMS) are a promising way to achieve this goal. For various reasons PDMS often sit on-top of a relational DBMS using it (more or less) as a simple record manager. User interactions with the PDMS are translated into series of SQL queries. This does not cause too much harm when DBMS and PDMS are located in the same local-area network with high bandwidth and little latency times. The picture may change dramatically, however, if the users are working in geographically distributed environments. Response times may rise by orders of magnitude, e. g. from 1-2 minutes in the local context to 30 minutes and even more in the "intercontinental" context. The paper shows how a more sophisticated utilization of the (advanced) SQL features coming along with SQL:1999 can help to cut down response times significantly.

#index 660647
#* Bundles in Captivity: An Application of Superimposed Information
#@ 
#t 2001
#c 17
#! Abstract: What do you do to make sense of a mass of information on a given topic? Paradoxically, you likely add yet more information to the pile: annotations, underlining, bookmarks, cross-references. We want to build digital information systems for managing such added or super-imposed information and support applications that create and manipulate it. We find that requirements for a superimposed information system can be quite different from those for a traditional database management system: a lightweight implementation, multi-model information structures, "schema-later" data entry, interacting with data that is "outside the box" (controlled by other applications), and support, rather than removal, of redundancy. We report here on SLIMPad, a superimposed application, which was inspired by the "bundling" of information elements from disparate sources we observed in a medical setting. We propose an architecture for superimposed applications and information management. Our prototype components to implement the architecture give flexibility in structuring superimposed information, and also encapsulate addressing, at a sub-document granularity, into a variety of base information sources.

#index 660648
#* High-Level Parallelisation in a Database Cluster: A Feasibility Study Using Document Services
#@ 
#t 2001
#c 17
#! Abstract: Our concern is the design of a scalable infrastructure for complex application services. We want to find out if a cluster of commodity database systems is well-suited as such an infrastructure. To this end, we have carried out a feasibility study based on document services, e.g., document insertion and retrieval. We decompose a service request into short parallel database transactions. Our system, implemented as an extension of a transaction processing monitor, routes the short transactions to the appropriate database systems in the cluster. Routing depends on the data distribution that we have chosen. To avoid bottlenecks, we distribute document functionality such as term extraction over the cluster. Extensive experiments show the following: (1) A relatively small number of components--for example 8 components--already suffices to cope with high workloads of more than 100 concurrently active clients. (2) Speedup and throughput increase linearly for insertion operations when increasing the cluster size. These observations also hold when bundling service invocations into transactions at the semantic layer. A specialized coordinator component then implements semantic serializability and atomicity. Our experiments show that such a coordinator has minimal impact on CPU resource consumption and on response times.

#index 660649
#* Efficient Sequenced Temporal Integrity Checking
#@ Aravindan Kasthurirangany
#t 2001
#c 17
#! Abstract: Primary key and referential integrity are the most widely used integrity constraints in relational databases. Each has a sequenced analogue in temporal databases, in which the constraint must apply independently at every point in time. In this paper, we assume a stratum approach, which expresses the checking in conventional SQL, as triggers on period-stamped relations. We evaluate several novel approaches that exploit B+-tree indexes to enable efficient checking of sequenced primary key (SPK) and referential integrity (SRI) constraints. We start out with a brute force SPK algorithm, then adapt the Relational Interval-tree overlap algorithm. After that, we propose a new method, the Straight Traversal algorithm, which utilizes the B+-tree more directly to identify when multiple key values are present. Our evaluation, on two platforms, shows that Straight Traversal algorithm approaches the performance of built-in nontemporal primary key and referential integrity checking, with constant time per tuple.

#index 660650
#* XML Data and Object Databases: The Perfect Couple?
#@ 
#t 2001
#c 17
#! Abstract: XML is increasingly gaining acceptance as a medium for exchanging data between applications. Given its text-based structure, XML can easily be distributed across any type of communication channel, including the Internet. This article provides an overview of an efficient way to store XML data inside an object-oriented database management system (OODBMS). It first discusses the difference between XML data and XML documents, and then introduces an approach to integrate XML data into the Java .programming language and programming model. This integration is combined with the transparent persistence of Java objects defined by the ODMG.

#index 660651
#* Tamino - A DBMS Designed for XML
#@ Dr. Harald Schöning
#t 2001
#c 17
#! Abstract: Tamino is Software AG's XML database management system. In contrast to solutions of other DBMS vendors, Tamino is not just another layer on top of a database system designed to support the relational or an object-oriented data model. Rather, Tamino has been completely designed for XML. This paper gives a short overview of Tamino's architecture and then addresses some of the design considerations for Tamino. In particular, areas are presented where database design for XML was nontrivial, and where some issues are still open.

#index 660652
#* The Nimble XML Data Integration System
#@ Alon Y. HaLevy
#t 2001
#c 17
#! Abstract: For better or for worse, XML has emerged as a de facto standard for data interchange. This consensus is likely to lead to increased demand for technology that allows users to integrate data from a variety of applications, repositories, and partners which are located across the corporate intranet or on the Internet. Nimble Technology has spent two years developing a product to service this market. Originally conceived after decades of person-years of research on data integration, the product is now being deployed at several Fortune-500 beta-customer sites. This abstract reports on the key challenges we faced in the design of our product and highlights some issues we think require more attention from the research community. In particular, we address architectural issues arising from designing a product to support XML as its core representation, choices in the design of the underlying algebra, on-the-fly data cleaning and caching and materialization policies.

#index 660653
#* High-Performance, Space-Efficient, Automated Object Locking
#@ 
#t 2001
#c 17
#! Abstract: The paper studies the impact of several lock manager designs on the overhead imposed to a persistent programming language by automated object locking. Our study reveals that a lock management method based on lock state sharing outperforms more traditional lock management designs. Lock state sharing is a novel lock management method that represents all lock data structures with equal values with a single shared data structure. Sharing the value of locks has numerous benefits: (i) it makes the space consumed by the lock manager small and independent of the number of locks acquired by transactions, (ii) it eliminates the need for expensive book-keeping of locks by transactions, and (iii) it enables the use of memoization techniques for whole locking operations. These advantages add up to make the release of locks practically free, and the processing of over 99% of lock requests between 8 to 14 RISC instructions.

#index 660654
#* Differential Logging: A Commutative and Associative Logging Scheme for Highly Parallel Main Memory Database
#@ 
#t 2001
#c 17
#! Abstract: With a gigabyte of memory priced at less than $2,000, the main-memory DBMS (MMDBMS) is emerging as an economically viable alternative to the disk-resident DBMS (DRDBMS) in many problem domains. The MMDBMS can show significantly higher performance than the DRDBMS by reducing disk accesses to the sequential form of log writing and the occasional checkpointing. Upon the system crash, the recovery process begins by accessing the disk-resident log and checkpoint data to restore a consistent state. With the increasing CPU speed, however, such disk access is still the dominant bottleneck in the MMDBMS. To overcome this bottleneck, this paper explores alternatives of parallel logging and recovery. The major contribution of this paper is the so-called differential logging scheme that permits unrestricted parallelism in logging and recovery. Using the bit-wise XOR operation both to compute the differential log between the before and after images and to recover the consistent database state, this scheme offers the room for significant performance improvement in the MMDBMS. First, with logging done on the difference, the log volume is reduced to almost half compared with the conventional physical logging. Second, the commutativity and associativity of XOR enables processing of log records in an arbitrary order. This means that we can freely distribute log records to multiple disks to improve the logging performance. During the recovery time, we can do parallel restart independently for each log disk. This paper shows the superior performance of the differential logging comparatively with the physical logging in the shared-memory multiprocessor environment.

#index 660655
#* Efficient Bulk Deletes in Relational Databases
#@ A. Gartner;B. Zeller
#t 2001
#c 17
#! Abstract: Many applications require that large amounts of data are deleted from the database - typically, such bulk deletes are carried out periodically and involve old or out-of-date data. If the data is not partitioned in such a way that bulk deletes can be carried out by simply deleting whole partitions, then most current database products execute such bulk delete operations very poorly. The reason is that every record is deleted from each index individually. This paper proposes and evaluates a new class of techniques to support bulk delete operations more efficiently. These techniques outperform the "record-at-a-time" approach implemented in many database products by about one order of magnitude.

#index 660656
#* On Dual Mining: From Patterns to Circumstances, and Back
#@ 
#t 2001
#c 17
#! Abstract: Previous work on frequent itemset mining has focused on finding all itemsets that are frequent in a specified part of a database. In this paper, we motivate the dual question of finding under what circumstances a given itemset satisfies a pattern of interest (e.g., frequency) in a database. Circumstances form a lattice that generalizes the instance lattice associated with datacube. Exploiting this, we adapt known cube algorithms and propose our own, minCirc, for mining the strongest (e.g., minimal) circumstances under which an itemset satisfies a pattern. Our experiments show minCirc is competitive with the adapted algorithms. We motivate mining queries involving migration between itemset and circumstance lattices and propose the notion of Armstrong Basis as a structure that provides efficient support for such migration queries, as well as a simple algorithm for computing it.

#index 660657
#* Mining Partially Periodic Event Patterns with Unknown Periods
#@ 
#t 2001
#c 17
#! Abstract: Periodic behavior is common in real-world applications. However; in many cases, periodicities are partial in that they are present only intermittently. Herein, we study such intermittent patterns, which we refer to as p-pattems. Our formulation of p-patterns takes into account imprecise time information ( e.g., due to unsynchronized clocks in distributed environments), noisy data (e.g., due to extraneous events), and shifts in phase and/or periods. We structure mining for p-patterns as two sub-tasks: (1) finding the periods of p-patterns and (2) mining temporal associations. For (2), a level-wise algorithm is used. For (1), we develop a novel approach based on a chi-squared test, and study its performance in the presence of noise. Further; we develop two algorithms for mining p-patterns based on the order in which the aforementioned sub-tasks are performed: the period-first algorithm and the association-first algorithm. Our resuits show that the association-first algorithm has a higher tolerance to noise; the period-first algorithm is more computationally efficient and provides flexibility as to the specification of support levels. In addition, we apply the period-first algorithm to mining data collected from two production computer networks, a process that led to several actionable insights.

#index 660658
#* PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth
#@ 
#t 2001
#c 17
#! Abstract: Sequential pattern mining is an important data mining problem with broad applications. It is challenging since one may need to examine a combinatorially explosive number of possible subsequence patterns. Most of the previously developed sequential pattern mining methods follow the methodology of Apriori which may substantially reduce the number of combinations to be examined. However, Apriori still encounters problems when a sequence database is large and/or when sequential patterns to be mined are numerous and/or long. In this paper, we propose a novel sequential pattern mining method, called PrefixSpan (i.e., Prefix-projected Sequential pattern mining), which explores prefix-projection in sequential pattern mining. PrefixSpan mines the complete set of patterns but greatly reduces the efforts of candidate subsequence generation. Moreover, prefix-projection substantially reduces the size of projected databases and leads to efficient processing. Our performance study shows that PrefixSpan outperforms both the Apriori-based GSP algorithm and another recently proposed method, FreeSpan, in mining large sequence databases.PrefixSpan

#index 660659
#* Mobile Data Management: Challenges of Wireless and Offline Data Access
#@ Eric Giguère
#t 2001
#c 17
#! Abstract: Applications require access to database servers for many purposes. Mobile users, those who use their computing devices away from a traditional local area network, require access to data even when central database servers are unavailable. iAnywhere Solutions provides a number of solutions that address the challenges of offline and wireless data access. In this talk, we discuss those challenges and our solutions.

#index 660660
#* Microsoft Server Technology for Mobile and Wireless Applications
#@ 
#t 2001
#c 17
#! Abstract: Microsoft is building a number of server technologies that are targeted at mobile and wireless applications. These technologies cover a wide range of customer scenarios and application requirements. This presentation discusses some of these technologies in detail.

#index 660661
#* IBM DB2 Everyplace: A Small Footprint Relational Database System
#@ Cliff Leung
#t 2001
#c 17
#! Abstract: Handheld and embedded devices are becoming increasingly popular and their uses are more versatile. Applications on these devices often need storing, retrieving and synchronizing data. IBM DB2 Everyplace is a high performance small footprint DBMS that is targeted in this market segment. In this paper, we will describe the overall architecture, features and several design consideration.

#index 660662
#* Dependable Computing in Virtual Laboratories
#@ C. Pautasso
#t 2001
#c 17
#! Abstract: Many scientific disciplines are shifting from in vitro to in silico research as more physical processes and natural phenomena are examined in a computer (in silico) instead of being observed (in vitro). In many of these virtual laboratories, the computations involved are very complex and long lived. Currently, users are required to manually handle almost all aspects of such computations, including their dependability. Not surprisingly, this is a major bottleneck and a significant source of inefficiencies. To address this issue, we have developed BioOpera, an extensible process support management system for virtual laboratories. In this paper, we briefly discuss the architecture and functionality of Bio-Opera and show how it can be used to efficiently manage long lived computations.

#index 660663
#* Workflow and Process Synchronization with Interaction Expressions and Graphs
#@ 
#t 2001
#c 17
#! Abstract: Current workflow management technology does not provide adequate means for inter-workflow coordination as concurrently executing workflows are considered completely independent. While this simplified view might suffice for one application domain or the other, there are many real-world application scenarios where workflows--though independently modeled in order to remain comprehensible and manageable--are semantically interrelated. As pragmatical approaches, like merging interdependent workflows or inter-workflow message passing, do not satisfactorily solve the inter-workflow coordination problem, interaction expressions and graphs are proposed as a simple yet powerful formalism for the specification and implementation of synchronization conditions in general and inter-workflow dependencies in particular. In addition to a graph-based semi-formal interpretation of the formalism, a precise formal semantics, an equivalent operational semantics, an efficient implementation of the latter, and detailed complexity analyses have been developed allowing the formalism to be actually applied to solve real-world problems like inter-workflow coordination.

#index 660664
#* Inter-Enterprise Collaborative Business Process Management
#@ 
#t 2001
#c 17
#! Abstract: Conventional workflow systems are primarily designed for intra-enterprise process management, and they are hardly used to handle processes with tasks and data separated by enterprise boundaries, for reasons such as security, privacy, sharability, firewalls, etc. Further the cooperation of multiple enterprises is often based on peer-to-peer interactions rather than centralized coordination. As a result, the conventional centralized process management architecture does not fit into the picture of inter-enterprise business-to-business E-Commerce. We have developed a Collaborative Process Manager (CPM) to support decentralized, peer-to-peer process management for inter-enterprise collaboration at the business process level. A collaborative process is not handled by a centralized workflow engine, but by multiple CPMs, each represents a player in the business process. Each CPM is used to schedule, dispatch and control the tasks of the process that the player is responsible for, and the CPMs interoperate through an inter-CPM messaging protocol. We have implemented CPM and embedded it into a dynamic software agent architecture, E-Carry, that we developed at HP Labs, to elevate multi-agent cooperation from the conversation level to the process level for mediating E-Commerce applications.

#index 660665
#* Duality-Based Subsequence Matching in Time-Series Databases
#@ 
#t 2001
#c 17
#! Abstract: In this paper, we propose a new subsequence matching method, Dual Match, which exploits duality in constructing windows and significantly improves performance. Dual Match divides data sequences into disjoint windows and the query sequence into sliding windows, and thus, is a dual approach of the one by Faloutsos et al. (FRM in short), which divides data sequences into sliding windows and the query sequence into disjoint windows. We formally prove that our dual approach is correct, i.e., it incurs no false dismissal. We also prove that, given the minimum query length, there is a maximum bound of the window size to guarantee correctness of Dual Match and discuss the effect of the window size on performance. FRM causes a lot of false alarms (i.e., candidates that do not qualify) by storing minimum bounding rectangles rather than individual points representing windows to avoid excessive storage space required for the index. Dual Match solves this problem by directly storing points, but without incurring excessive storage overhead. Experimental results show that, in most cases, Dual Match provides large improvement in both false alarms and performance over FRM, given the same amount of storage space. In particular, for low selectivities (less than 10^{-4}), Dual Match significantly improves performance up to 430-fold. On the other hand, for high selectivities (more than 10^{-2}), it shows a very minor degradation (less than 29%). For selectivities in between (10^{-4} \approx 10^{-2}), Dual Match shows performance slightly better than that of FRM. Dual Match is also 4.10 \approx 25.6 times faster than FRM in building indexes of approximately the same size. Overall, these results indicate that our approach provides a new paradigm in subsequence matching that improves performance significantly in large database applications.

#index 660666
#* Variable Length Queries for Time Series Data
#@ 
#t 2001
#c 17
#! Abstract: Finding similar patterns in a time sequence is a well-studied problem. Most of the current techniques work well for queries of a prespecified length, but not for variable length queries. We propose a new indexing technique that works well for variable length queries. The central idea is to store index structures at different resolutions for a given dataset. The resolutions are based on wavelets. For a given query, a number of subqueries at different resolutions are generated. The ranges of the subqueries are progressively refined based on results from previous subqueries. Our experiments show that the total cost for our method is 4 to 20 times less than the current techniques including Linear Scan. Because of the need to store information at multiple resolution levels, the storage requirement of our method could potentially be large. In the second part of the paper, we show how the index information can be compressed with minimal information loss. According to our experimental results, even after compressing the size of the index to one fifth, the total cost of our method is 3 to 15 times less than the current techniques.

#index 660667
#* TAR: Temporal Association Rules on Evolving Numerical Attributes
#@ 
#t 2001
#c 17
#! Abstract: Data mining has been an area of increasing interests during recent years. The association rule discovery problem in particular has been widely studied. However, there are still some unresolved problems. For example, research on mining patterns in the evolution of numerical attributes is still lacking. This is both a challenging problem and one with significant practical application in business, science, and medicine. In this paper, we present a temporal association rule model for evolving numerical attributes. Metrics for qualifying a temporal association rule include the familiar measures of support and strength used in the traditional association rule mining and a new metric called density. The density metric not only gives us a way to extract the rules that best represent the data, but also provides an effective mechanism to prune the search space. An efficient algorithm is devised for mining temporal association rules, which utilizes all three thresholds (especially the strength) to prune the search space drastically. Moreover, the resulting rules are represented in a concise manner via rule sets to reduce the output size. Experimental results on real and synthetic data sets demonstrate the efficiency of our algorithm.

#index 660668
#* Database Performance for Next Generation Telecommunications
#@ 
#t 2001
#c 17
#! Abstract: Three important trends in telecommunications have received media attention for their potentially wide-ranging impacts. The first is increasing competition among telephone service providers, spurred in the United States by the Telecommunications Act of 1996. The second is the technological convergence between telephony and the Internet. The third is the ever-increasing number of mobile devices and users. In this paper we explore the effects of these trends on telecommunication network databases.

#index 660669
#* Data Management Support of Web Applications
#@ 
#t 2001
#c 17
#! Abstract: Automating the interactions between trusted business partners is a major goal of businesses today. This is often called "supply chain integration." The intent is to make the businesses more responsive to customer needs, and more efficient in their business or manufacturing processes. This talk describes an infrastructure that facilitates the collaboration of trusted business partners to achieve common business goals.

#index 660670
#* An XML Indexing Structure with Relative Region Coordinate
#@ 
#t 2001
#c 17
#! Abstract: For most index structures for XML data proposed so far, update is a problem because XML element's coordinates are expressed by absolute values. Due to the structural relationship among elements in XML documents, we have to re-compute these absolute values if the content of source data is updated. The reconstruction requires update of large portion of index files, which causes a serious problem especially when XML data content is frequently updated. In this paper, we propose an indexing structure scheme based on the Relative Region Coordinate that can effectively deal with the update problem. The main idea is that we express the coordinate of an XML element based on the region of its parent element. We present an algorithm to construct a tree-structured index in which related coordinates are stored together. In consequence, our indexing scheme requires update of only a small portion of index file in case of updating.

#index 660671
#* Querying XML Documents Made Easy: Nearest Concept Queries
#@ 
#t 2001
#c 17
#! Abstract: Due to the ubiquity and popularity of XML, users of-ten are in the following situation: they want to query XML documents which contain potentially interesting information but they are unaware of the mark-up structure that is used. For example, it is easy to guess the contents of an XML bibliography file whereas the mark-up depends on the methodological, cultural and personal background of the author(s). Nonetheless, it is this hierarchical structure that forms the basis of XML query languages. In this paper we exploit the tree structure of XML documents to equip users with a powerful tool, the meet operator, that lets them query databases with whose content they are familiar, but without requiring knowledge of tags and hierarchies. Our approach is based on computing the lowest common ancestor of nodes in the XML syntax tree: e.g., given two strings, we are looking for nodes whose offspring contains these two strings. The novelty of this approach is that the result type is unknown at query formulation time and dependent on the database instance. If the two strings are an author's name and a year, mainly publications of the author in this year are returned. If the two strings are numbers the result mostly consists of publications that have the numbers as year or page numbers. Because the result type of a query is not specified by the user we refer to the lowest common ancestor as nearest concept. We also present a running example taken from the bibliography domain, and demonstrate that the operator can be implemented efficiently.

#index 660672
#* An Automated Change-Detection Algorithm for HTML Documents Based on Semantic Hierarchies
#@ 
#t 2001
#c 17
#! Abstract: Data at many Web sites are changing rapidly, and a significant amount of these data are presented in HTML documents that consist of markups and data contents. Although XML is getting more popular in data exchange, the presentation of data contained in XML documents is given by and large in the HTML format using XSL(T). Since HTML was designed to "display" data from the human perspective, it is not trivial for a machine to detect (hierarchical) changes of data in an HTML document. In this paper, we propose a heuristic algorithm, called SCD, to detect semantic changes of hierarchical data contents in any two HTML documents automatically. Semantic changes differ from syntactic changes since the latter refer to changes of data contents with respect to markup structures according to the HTML grammar. SCD does not require preprocessing nor any knowledge of the internal structure of the source documents beforehand. The time complexity of SCD is O((\mid X \mid \times \mid Y\mid) log(\mid X\mid \times \mid Y\mid)), where \mid X \mid and \mid Y \mid are the number of unique branches in the syntactic hierarchies of any two given HTML documents, respectively.

#index 678914
#* Algorithmic Redistribution Methods for Block CyclicDecompositions
#@ Antoine Petitet
#t 1997
#c 17
#! This research aims at creating and providing a frame- work to describe algorithmic redistribution methods for various block cyclic decompositions. To do so properties of this data distribution scheme are formally exhibited. The examination of a number of basic dense linear algebra operations illustrates the application of those properties. This study analyzes the extent to which the general two-dimensional block cyclic data distribution allows for the expression of efficient as well as flexible matrix operations. This study also quantifies theoretically and practically how much of the efficiency of optimal block cyclic data layouts can be maintained. The general block cyclic decomposition scheme is shown to allow for the expression of flexible basic matrix operations with little impact on the performance and efficiency delivered by optimal and restricted kernels available today. Second, block cyclic data layouts, such as the purely scattered distribution, which seem less promising as far as performance is concerned, are shown to be able to achieve optimal performance and efficiency for a given set of matrix operations. Conse- quently, this research not only demonstrates that the restrictions imposed by the optimal block cyclic data layouts can be alleviated, but also that efficiency and flexibility are not antagonistic features of the block cyclic mappings. These results are particularly relevant to the design of dense linear algebra software libraries as well as to data parallel compiler technology.

#index 744821
#* Proceedings of the 20th International Conference on Data Engineering
#@ 
#t 2004
#c 17

#index 745419
#* GODIVA: Lightweight Data Management for Scientific Visualization Applications
#@ Xiaosong Ma;Marianne Winslett;John Norris;Xiangmin Jiao;Robert Fiedler
#t 2004
#c 17
#% 116035
#% 202140
#% 308500
#% 319327
#% 329370
#% 376071
#% 397422
#% 436686
#% 471494
#% 481102
#% 587741
#% 614591
#% 619522
#% 623537
#% 746428
#! Scientific visualization applications are very data-intensive,with high demands for I/O and data management.Developers of many visualization tools hesitate to use traditionalDBMSs, due to the lack of support for these DBMSson parallel platforms and the risk of reducing the portabilityof their tools and the user data. In this paper, we proposethe GODIVA framework, which provides simple database-likeinterfaces to help visualization tool developers managetheir in-memory data, and I/O optimizations such asprefetching and caching to improve input performance atrun time. We implemented the GODIVA interfaces in astand-alone, portable user library, which can be used by alltypes of visualization codes: interactive and batch-mode,sequential and parallel. Performance results from runninga visualization tool using the GODIVA library on multipleplatforms show that the GODIVA framework is easy to use,alleviates developers' data management burden, and canbring substantial I/O performance improvement.

#index 745420
#* Outrageous Ideas and/or Thoughts While Shaving
#@ Mike Stonebraker
#t 2004
#c 17

#index 745421
#* Database Research for the Current Millennium
#@ Daniela Florescu
#t 2004
#c 17

#index 745422
#* Database Kernel Research: What, if anything, is left to do?
#@ David Lomet
#t 2004
#c 17

#index 745423
#* Minimization and Group-By Detection for Nested XQueries
#@ Alin Deutsch;Yannis Papakonstantinou;Yu Xu
#t 2004
#c 17

#index 745424
#* Efficient Execution of Computation Modules in a Model with Massive Data
#@ Gary Kratkiewicz;Renu Kurien Bostwick;Geoffrey S. Knauth
#t 2004
#c 17

#index 745425
#* Multi-Scale Histograms for Answering Queries over Time Series Data
#@ Lei Chen;M. Tamer Özsu
#t 2004
#c 17

#index 745426
#* Storing XML (with XSD) in SQL Databases: Interplay of Logical and Physical Designs
#@ Surajit Chaudhuri;Zhiyuan Chen;Kyuseok Shim;Yuqing Wu
#t 2004
#c 17
#% 570875

#index 745427
#* Routing XML Queries
#@ Nick Koudas;Michael Rabinovich;Divesh Srivastava;Ting Yu
#t 2004
#c 17

#index 745428
#* Peering and Querying e-Catalog Communities
#@ Boualem Benatallah;Mohand-Said Hacid;Hye-young Paik;Christophe Rey;Farouk Toumani
#t 2004
#c 17
#% 413389
#% 443375

#index 745429
#* Web Service Composition Through Declarative Queries: The Case of Conjunctive Queries with Union and Negation
#@ Bertram Ludäscher;Alan Nash
#t 2004
#c 17
#% 726626
#% 801698

#index 745430
#* LexEQUAL: Supporting Multilexical Queries in SQL
#@ A. Kumaran;Jayant R. Haritsa
#t 2004
#c 17
#% 219033
#% 480654
#% 1015263

#index 745431
#* A Type-Safe Object-Oriented Solution for the Dynamic Construction of Queries
#@ Peter Rosenthal
#t 2004
#c 17

#index 745432
#* Dynamic Extensible Query Processing in Super-Peer Based P2P Systems
#@ Christian Wiesner;Alfons Kemper;Stefan Brandl
#t 2004
#c 17
#% 572300

#index 745433
#* VirGIS: Mediation for Geographical Information Systems
#@ Omar Boucelma;Mehdi Essid;Zoé Lacroix;Julien Vinel;Jean-Yves Garinet;Abdelkader Bétari
#t 2004
#c 17
#% 413786

#index 745434
#* Nile: A Query Processing Engine for Data Streams
#@ M. A. Hammad;M. F. Mokbel;M. H. Ali;W. G. Aref;A. C. Catlin;A. K. Elmagarmid;M. Eltabakh;M. G. Elfeky;T. M. Ghanem;R. Gwadera;I. F. Ilyas;M. Marzouk;X. Xiong
#t 2004
#c 17
#% 245996
#% 853011
#% 1015279

#index 745435
#* Using Stream Semantics for Continuous Queries in Media Stream Processors
#@ Amarnath Gupta;Bin Liu;Pilho Kim;Ramesh Jan
#t 2004
#c 17

#index 745436
#* RACCOON: A Peer-Based System for Data Integration and Sharing
#@ Chen Li;Jia Li;Qi Zhong
#t 2004
#c 17
#% 587725

#index 745437
#* Function Proxy: Template-Based Proxy Caching for Table-Valued Functions
#@ Qiong Luo;Wenwei Xue
#t 2004
#c 17
#% 480495

#index 745438
#* Bitmap-Tree Indexing for Set Operations on Free Text
#@ Ilias Nitsos;Georgios Evangelidis;Dimitris Dervos
#t 2004
#c 17
#% 79221
#% 249989
#! In the present study we report on our implementation ofa hybrid-indexing scheme (Bitmap-Tree) that combines theadvantages of bitmap indexing and file inversion. The resultswe obtained are compared to those of the compressedinverted file index. Both storage overhead and query processingefficiency are taken into consideration. The proposednew method is shown to excel in handling queriesinvolving set operations. For general-purpose user queries,the Bitmap-Tree is shown to perform as good as the compressedinverted file index.

#index 745439
#* OntoBuilder: Fully Automatic Extraction and Consolidation of Ontologies from Web Sources
#@ Avigdor Gal;Giovanni Modica;Hasan Jamil
#t 2004
#c 17
#% 509547
#% 800005
#% 830524

#index 745440
#* FLYINGDOC: An Architecture for Distributed, User-friendly, and Personalized Information Systems
#@ Ilvio Bruder;Andre Zeitz;Holger Meyer;Birger Hänsel;Andreas Heuer
#t 2004
#c 17

#index 745441
#* SQLCM: A Continuous Monitoring Framework for Relational Database Engines
#@ Surajit Chaudhuri;Arnd Christian König;Vivek Narasayya
#t 2004
#c 17
#% 333965
#% 397353
#% 397354
#% 993933
#% 993949
#% 994004
#% 1469345
#! The ability to monitor a database server is crucial foreffective database administration. Today's commercialdatabase systems support two basic mechanisms formonitoring: (a) obtaining a snapshot of counters tocapture current state, and (b) logging events in the serverto a table/file to capture history. In this paper we showthat for a large class of important databaseadministration tasks the above mechanisms areinadequate in functionality or performance. We presentan infrastructure called SQLCM that enables continuousmonitoring inside the database server and that has theability to automatically take actions based on monitoring.We describe the implementation of SQLCM in MicrosoftSQL Server and show how several common and importantmonitoring tasks can be easily specified in SQLCM. Ourexperimental evaluation indicates that SQLCM imposeslow overhead on normal server execution end enablesmonitoring tasks on a production server that would be tooexpensive using today's monitoring mechanisms.

#index 745442
#* Approximate Aggregation Techniques for Sensor Databases
#@ Jeffrey Considine;Feifei Li;George Kollios;John Byers
#t 2004
#c 17
#% 2833
#% 36280
#% 69264
#% 278835
#% 309433
#% 317317
#% 336610
#% 346977
#% 420053
#% 427022
#% 519953
#% 577219
#% 654463
#% 654482
#% 805466
#% 889758
#% 993959
#! In the emerging area of sensor-based systems, a significantchallenge is to develop scalable, fault-tolerantmethods to extract useful information from the data thesensors collect.An approach to this data managementproblem is the use of sensor database systems, exemplifiedby TinyDB and Cougar, which allow users to performaggregation queries such as MIN, COUNT andAVG on a sensor network.Due to power and range constraints,centralized approaches are generally impractical,so most systems use in-network aggregation to reducenetwork traffic.However, these aggregation strategiesbecome bandwidth-intensive when combined with thefault-tolerant, multi-path routing methods often used inthese environments.For example, duplicate-sensitive aggregatessuch as SUM cannot be computed exactly usingsubstantially less bandwidth than explicit enumeration.To avoid this expense, we investigate the use of approximatein-network aggregation using small sketches.Our contributions are as follows: 1) we generalize wellknown duplicate-insensitive sketches for approximatingCOUNT to handle SUM, 2) we present and analyze methodsfor using sketches to produce accurate results withlow communication and computation overhead, and 3)we present an extensive experimental validation of ourmethods.

#index 745443
#* An Efficient Framework for Order Optimization
#@ Thomas Neumann;Guido Moerkotte
#t 2004
#c 17
#% 43162
#% 83154
#% 210169
#% 411554
#% 565457
#! Since the introduction of cost-based query optimization,the performance-critical role of interesting orders has beenrecognized. Some algebraic operators change interestingorders (e.g. sort and select), while others exploit interesting orders (e.g. merge join). The two operations performed by any query optimizer during plan generation are 1) computing the resulting order given an input order and an algebraic operator and 2) determining the compatibility between a given input order and the required order a given algebraic operator can beneficially exploit. Since these twooperations are called millions of times during plan generation, they are highly performance-critical. The third crucial parameter is the space requirement for annotating every plan node with its output order.Lately, a powerful framework for reasoning about ordershas been developed, which is based on functional dependencies. Within this framework, the current state-of-the-art algorithms for implementing the above operations both havea lower bound time requirement of 驴(n), where n is thenumber of functional dependencies involved. Further, thelower bound for the space requirement for every plan nodeis 驴(n).We improve these bounds by new algorithms with uppertime bounds O(1). That is, our algorithms for both operations work in constant time during plan generation, after a one-time preparation step. Further, the upper bound for thespace requirement for plan nodes is O(1) for our approach.Besides, our algorithm reduces the search space by detecting and ignoring irrelevant orderings. Experimental results with a full fledged query optimizer show that our approachsignificantly reduces the total time needed for plan generation. As a corollary of our experiments, it follows that thetime spent for order processing is a non-negligible part ofplan generation.

#index 745444
#* Improving Logging and Recovery Performance in Phoenix/App
#@ Roger Barga;Shimin Chen;David Lomet
#t 2004
#c 17
#% 50718
#% 86930
#% 114582
#% 248823
#% 399766
#% 403195
#% 602679
#% 660923
#! Phoenix/App supports software components whosestates are made persistent across a system crash via redorecovery, replaying logged interactions. Our initialprototype force logged all request/reply events resultingfrom inter-component method calls and returns. Thispaper describes an enhanced prototype that implements:(i) log optimizations to improve normal executionperformance; and (ii) checkpointing to improve recoveryperformance. Logging is reduced in two ways: (1) weonly log information required to remove non-determinism,and we only force the log when an event"commits" the state of the component to other parts of thesystem; (2) we introduce new component types thatprovide our enhanced system with more information,enabling further reduction in logging. To improverecovery performance, we save the values of the fields ofa component to the log in an application "checkpoint".We describe the system elements that we exploit for theseoptimizations, and characterize the performance gainsthat result.

#index 745445
#* NEXSORT: Sorting XML in External Memory
#@ Adam Silberstein;Jun Yang
#t 2004
#c 17
#% 41684
#% 77972
#% 236416
#% 397349
#% 460839
#% 479806
#% 504580
#% 572305
#! XML plays an important role in delivering data over theInternet, and the need to store and manipulate XML in itsnative format has become increasingly relevant. This growingneed necessitates work on developing native XML operators,especially for one as fundamental as sort. In this paperwe present NEXSORT, an algorithm that leverages thehierarchical nature of XML to efficiently sort an XML documentin external memory. In a fully sorted XML document,children of every non-leaf element are ordered accordingto a given sorting criterion. Among NEXSORT's uses is incombination with structural merge as the XML version ofsort-merge join, which allows us to merge large XML documentsusing only a single pass once they are sorted.The hierarchical structure of an XML document limitsthe number of possible legal orderings among itselements, which means that sorting XML is fundamentally"easier" than sorting a flat file. We prove thatthe I/O lower bound for sorting XML in external memoryis 驴(max {n, n logm(k/B)}), where is the numberof blocks in the input XML document, m is the numberof main memory blocks available for sorting, B is the numberof elements that can fit in one block, and k is the maximumfan-out of the input document tree. We show thatNEXSORT performs within a constant factor of this theoreticallower bound. In practice we demonstrate, evenwith a naive implementation, NEXSORT significantly outperformsa regular external merge sort of all elements bytheir key paths, unless the XML document is nearly flat,in which case NEXSORT degenerates essentially to externalmerge sort.

#index 745446
#* Privacy Preservation for Data Cubes
#@ Sam Sung;Yao Liu;Peter Ng
#t 2004
#c 17

#index 745447
#* SQUIRE: Sequential Pattern Mining with Quantities
#@ Chulyun Kim;Jong-Hwa Lim;Raymond Ng;Kyuseok Shim
#t 2004
#c 17
#% 463903
#% 660658
#! In this paper, we consider the problem of mining sequentialpatterns with quantities. Naive extensions to existingalgorithms for sequential patterns are inefficient, as theymay enumerate the search space blindly. To alleviate thesituation, we propose hash filtering and quantity samplingtechniques that significantly improve the performance of thenaive extensions.

#index 745448
#* Spectral Analysis of Text Collection for Similarity-based Clustering
#@ Wenyuan Li;Wee-Keong Ng;Ee-Peng Lim
#t 2004
#c 17

#index 745449
#* Scaling Clustering Algorithms for Massive Data Sets using Data Streams
#@ Silvia Nittel;Kelvin T. Leung;Amy Braverman
#t 2004
#c 17

#index 745450
#* On the Integration of Structure Indexes and Inverted Lists
#@ Raghav Kaushik;Rajasekar Krishnamurthy;Jeffrey F. Naughton;Raghu Ramakrishnan
#t 2004
#c 17
#% 333854

#index 745451
#* Mining the Web for Generating Thematic Metadata from Textual Data
#@ Chien-Chung Huang;Shui-Lung Chuang;Lee-Feng Chien
#t 2004
#c 17

#index 745452
#* XJoin Index: Indexing XML Data for Efficient Handling of Branching Path Expressions
#@ Elisa Bertino;Barbara Catania;Wen Qiang Wang
#t 2004
#c 17
#% 18614

#index 745453
#* Efficient Similarity Search in Large Databases of Tree Structured Objects
#@ Karin Kailing;Hans-Peter Kriegel;Stefan Schönauer;Thomas Seidl
#t 2004
#c 17
#% 248797

#index 745454
#* Using vTree Indices for Queries over Objects with Complex Motions
#@ Sandeep Gupta;Chinya Ravishankar
#t 2004
#c 17

#index 745455
#* On Local Pruning of Association Rules Using Directed Hypergraphs
#@ Sanjay Chawla;Joseph Davis;Gaurav Pandey
#t 2004
#c 17
#! In this paper we propose an adaptive local pruningmethod for association rules. Our method exploits the exactmapping between a certain class of association rules,namely those whose consequents are singletons and backwarddirected hypergraphs (B-Graphs). The hypergraphwhich represents the association rules is called an AssociationRules Network(ARN). Here we present a simple exampleof an ARN. In the full paper we prove several propertiesof the ARN and apply the results of our approach totwo popular data sets.

#index 745456
#* "My Personal Web": A Seminar on Personalization and Privacy for Web and Converged Services
#@ Irini Fundulaki;Richard Hull;Bharat Kumar;Daniel Liewen;Arnaud Sahuguet
#t 2004
#c 17

#index 745457
#* Data Mining for Intrusion Detection: Techniques, Applications and Systems
#@ Jian Pei;Shambhu J. Upadhyaya;Faisal Farooq;Venugopal Govindaraju
#t 2004
#c 17

#index 745458
#* Querying about the Past, the Present, and the Future in Spatio-Temporal Databases
#@ Jimeng Sun;Dimitris Papadias;Yufei Tao;Bin Liu
#t 2004
#c 17
#% 86950
#% 248812
#% 248822
#% 273706
#% 273887
#% 273901
#% 273903
#% 300174
#% 300193
#% 333947
#% 378398
#% 397385
#% 397386
#% 411355
#% 421124
#% 458857
#% 480465
#% 480473
#% 480817
#% 482092
#% 630969
#% 853023
#% 871097
#% 1015285
#% 1015320
#! Moving objects (e.g., vehicles in road networks)continuously generate large amounts of spatio-temporalinformation in the form of data streams. Efficientmanagement of such streams is a challenging goal due tothe highly dynamic nature of the data and the need forfast, on-line computations. In this paper we present anovel approach for approximate query processing aboutthe present, past, or the future in spatio-temporaldatabases. In particular, we first propose an incrementallyupdateable, multi-dimensional histogram for present-timequeries. Second, we develop a general architecture formaintaining and querying historical data. Third, weimplement a stochastic approach for predicting the resultsof queries that refer to the future. Finally, weexperimentally prove the effectiveness and efficiency ofour techniques using a realistic simulation.

#index 745459
#* Direct Mesh: a Multiresolution Approach to Terrain Visualization
#@ Kai Xu;Xiaofang Zhou;Xuemin Lin
#t 2004
#c 17
#% 86950
#% 137887
#% 153260
#% 164360
#% 213525
#% 213975
#% 232955
#% 259608
#% 318051
#% 426958
#% 480830
#% 482510
#% 527172
#% 527196
#% 607812
#% 631993
#% 632104
#! Terrain can be approximated by a triangular mesh consistingmillions of 3D points. Multiresolution triangularmesh (MTM) structures are designed to support applicationsthat use terrain data at variable levels of detail (LOD).Typically, an MTM adopts a tree structure where a parentnode represents a lower-resolution approximation of its descendants.Given a region of interest (ROI) and a LOD,the process of retrieving the required terrain data from thedatabase is to traverse the MTM tree from the root to reachall the nodes satisfying the ROI and LOD conditions. Thisprocess, while being commonly used for multiresolution terrainvisualization, is inefficient as either a large numberof sequential I/O operations or fetching a large amount ofextraneous data is incurred. Various spatial indexes havebeen proposed in the past to address this problem, howeverlevel-by-level tree traversal remains a common practice inorder to obtain topological information among the retrievedterrain data. In this paper, a new MTM data structure calleddirect mesh is proposed. We demonstrate that with directmesh the amount of data retrieval can be substantially reduced.Comparing with existing MTM indexing methods,a significant performance improvement has been observedfor real-life terrain data.

#index 745460
#* Content-based Three-dimensional Engineering Shape Search
#@ K. Lou;S. Prabhakar;K. Ramani
#t 2004
#c 17
#% 102772
#% 172949
#% 201876
#% 237204
#% 241094
#% 248797
#% 261906
#% 263093
#% 335107
#% 342101
#% 427199
#% 527186
#% 579852
#% 661030
#% 664407
#% 664408
#% 956243
#% 1167472
#% 1632973
#! In this paper, we discuss the design andimplementation of a prototype 3D Engineering ShapeSearch system. The system incorporates multiplefeature vectors, relevance feedback, and query byexample and browsing, flexible definition of shapesimilarity, and efficient execution through multi-dimensionalindexing and clustering. In order to offermore information for a user to determine similarity of3D engineering shape, a 3D interface that allows usersto manipulate shapes is proposed and implemented topresent the search results. The system allows users tospecify which feature vectors should be used toperform the search.The system is used to conduct extensiveexperimentation real data to test the effectiveness ofvarious feature vectors for shape - the first suchcomparison of this type. The test results show that thedescending order of the average precision of featurevectors is: principal moments, moment invariants,geometric parameters, and eigenvalues. In addition, amulti-step similarity search strategy is proposed andtested in this paper to improve the effectiveness of 3Dengineering shape search. It is shown that the multi-stepapproach is more effective than the one-shotsearch approach, when a fixed number of shapes areretrieved.

#index 745461
#* PRIX: Indexing And Querying XML Using Prüfer Sequences
#@ Praveen Rao;Bongki Moon
#t 2004
#c 17
#% 333981
#% 340144
#% 397358
#% 397360
#% 397375
#% 462062
#% 464883
#% 479465
#% 480489
#% 481599
#% 598374
#% 654450
#% 659999
#% 660000
#% 993953
#! We propose a new way of indexing XML documents and processingtwig patterns in an XML database. Every XML documentin the database can be transformed into a sequence oflabels by Prüfer's method that constructs a one-to-one correspondencebetween trees and sequences. During query processing,a twig pattern is also transformed into its Prüfer sequence.By performing subsequence matching on the set ofsequences in the database, and performing a series of refinementphases that we have developed, we can find all the occurrencesof a twig pattern in the database. Our approachallows holistic processing of a twig pattern without breakingthe twig into root-to-leaf paths and processing these paths individually.Furthermore, we show in the paper that all correctanswers are found without any false dismissals or false alarms.Experimental results demonstrate the performance benefits ofour proposed techniques.

#index 745462
#* A Machine Learning Approach to Rapid Development of XML Mapping Queries
#@ Atsuyuki Morishima;Hiroyuki Kitagawa;Akira Matsumoto
#t 2004
#c 17
#% 31215
#% 82145
#% 82158
#% 115521
#% 115523
#% 276573
#% 333857
#% 333990
#% 428147
#% 478629
#% 480641
#% 543895
#% 993981
#! This paper presents XLearner, a novel tool that helpsthe rapid development of XML mapping queries writtenin XQuery. XLearner is novel in that it learns XQueryqueries consistent with given examples (fragments) of intendedquery results. XLearner combines known learningtechniques, incorporates mechanisms to cope with issuesspecific to the XQuery learning context, and provides a systematicway for the semi-automatic development of queries.This paper describes the XLearner system. It presents algorithmsfor learning various classes of XQuery, shows thata minor extension gives the system a practical expressivepower, and reports experimental results to demonstrate howXLearner outputs reasonably complicated queries with onlya small number of interactions with the user.

#index 745463
#* Selectivity Estimation for XML Twigs
#@ Neoklis Polyzotis;Minos Garofalakis;Yannis Ioannidis
#t 2004
#c 17
#% 145196
#% 210190
#% 273902
#% 333946
#% 333986
#% 397364
#% 397379
#% 458836
#% 465018
#% 480488
#% 482092
#% 745463
#% 993968
#% 993970
#! Twig queries represent the building blocks of declarativequery languages over XML data. A twig query describesa complex traversal of the document graph and generatesa set of element tuples based on the intertwined evaluation(i.e., join) of multiple path expressions. Estimatingthe result cardinality of twig queries or, equivalently, thenumber of tuples in such a structural (path-based) join, isa fundamental problem that arises in the optimization ofdeclarative queries over XML. It is crucial, therefore, to developconcise synopsis structures that summarize the documentgraph and enable such selectivity estimates within thetime and space constraints of the optimizer. In this paper,we propose novel summarization and estimation techniquesfor estimating the selectivity of twig queries with complexXPath expressions over tree-structured data. Our approachis based on the XSKETCH model, augmented with new typesof distribution information for capturing complex correlationpatterns across structural joins. Briefly, the key ideais to represent joins as points in a multidimensional spaceof path counts that capture aggregate information on thecontents of the resulting element tuples. We develop a systematicframework that combines distribution informationwith appropriate statistical assumptions in order to provideselectivity estimates for twig queries over concise XS-KETCHsynopses and we describe an efficient algorithm forconstructing an accurate summary for a given space budget.Implementation results with both synthetic and real-lifedata sets verify the effectiveness of our approach anddemonstrate its benefits over earlier techniques.

#index 745464
#* Group Nearest Neighbor Queries
#@ Dimitris Papadias;Qiongmao Shen;Yufei Tao;Kyriakos Mouratidis
#t 2004
#c 17
#% 86950
#% 201876
#% 237970
#% 248804
#% 264161
#% 287466
#% 296738
#% 300162
#% 300163
#% 318703
#% 333854
#% 333929
#% 397377
#% 397608
#% 399762
#% 413797
#% 427199
#% 464859
#% 464888
#% 479649
#% 480133
#% 480632
#% 492627
#% 495433
#% 527187
#% 527328
#% 564630
#% 579313
#% 993955
#% 993999
#% 1015321
#! Given two sets of points P and Q, a group nearest neighbor(GNN) query retrieves the point(s) of P with the smallestsum of distances to all points in Q. Consider, for instance,three users at locations q1, q2 and q3 that want to find a meeting point (e.g., a restaurant); the corresponding queryreturns the data point p that minimizes the sum of Euclideandistances |pqi| for 1驴i 驴3. Assuming that Q fits in memoryand P is indexed by an R-tree, we propose severalalgorithms for finding the group nearest neighborsefficiently. As a second step, we extend our techniques forsituations where Q cannot fit in memory, covering bothindexed and non-indexed query points. An experimentalevaluation identifies the best alternative based on the dataand query properties.

#index 745465
#* SPINE: Putting Backbone into String Indexing
#@ Naresh Neelapala;Romil Mittal;Jayant R. Haritsa
#t 2004
#c 17
#% 143306
#% 235941
#% 300312
#% 480482
#% 480484
#% 528694
#% 544193
#% 546291
#! The indexing technique commonly used for long strings,such as genomes, is the suffix tree, which is based on a vertical(intra-path) compaction of the underlying trie structure.In this paper, we investigate an alternative approach to indexbuilding, based on horizontal (inter-path) compactionof the trie. In particular, we present SPINE, a carefully engineeredhorizontally-compacted trie index. SPINE consistsof a backbone formed by a linear chain of nodes representingthe underlying string, with the nodes connected by arich set of edges for facilitating fast forward and backwardtraversals over the backbone during index construction andquery search. A special feature of SPINE is that it collapsesthe trie into a linear structure, representing the logical extremeof horizontal compaction.We describe algorithms for SPINE construction and forsearching this index to find the occurrences of query patterns.Our experimental results on a variety of real genomicand proteomic strings show that SPINE requires significantlyless space than standard implementations of suffixtrees. Further, SPINE takes lesser time for both constructionand search as compared to suffix trees, especially whenthe index is disk-resident. Finally, the linearity of its structuremakes it more amenable for integration with databaseengines.

#index 745466
#* Making the Pyramid Technique Robust to Query Types and Workloads
#@ Rui Zhang;Beng Chin Ooi;Kian-Lee Tan
#t 2004
#c 17
#% 86950
#% 102772
#% 169940
#% 227939
#% 248796
#% 299978
#% 321455
#% 397376
#% 427199
#% 463597
#% 464195
#% 479462
#% 479649
#% 480133
#% 480632
#% 481455
#% 481956
#! The effectiveness of many existing high-dimensional indexingstructures is limited to specific types of queries andworkloads. For example, while the Pyramid technique andthe iMinMax are efficient for window queries, the iDistanceis superior for kNN queries. In this paper, we present anew structure, called the P+-tree, that supports both windowqueries and kNN queries under different workloads efficiently.In the P+-tree, a B+-tree is employed to indexthe data points as follows. The data space is partitionedinto subspaces based on clustering, and points in each subspaceare mapped onto a single dimensional space using thePyramid technique, and stored in the B+-tree. The crux ofthe scheme lies in the transformation of the data which hastwo crucial properties. First, it maps each subspace intoa hypercube so that the Pyramid technique can be applied.Second, it shifts the cluster center to the top of the pyramid,which is the case that the Pyramid technique worksvery efficiently. We present window and kNN query processingalgorithms for the P+-tree. Through an extensiveperformance study, we show that the P+-tree has considerablespeedup over the Pyramid technique and the iMinMaxfor window queries and outperforms the iDistance for kNN queries.

#index 745467
#* Efficient Incremental Validation of XML Documents
#@ Denilson Barbosa;Alberto O. Mendelzon;Leonid Libkin;Laurent Mignet;Marcelo Arenas
#t 2004
#c 17
#% 152835
#% 164381
#% 175440
#% 241136
#% 262724
#% 279164
#% 299944
#% 308452
#% 333979
#% 355864
#% 378392
#% 390964
#% 465059
#% 482083
#% 576107
#% 577353
#% 659924
#% 994015
#! We discuss incremental validation of XML documentswith respect to DTDs and XML Schema definitions. We considerinsertions and deletions of subtrees, as opposed to leafnodes only, and we also consider the validation of ID andIDREF attributes. For arbitrary schemas, we give a worst-casen log n time and linear space algorithm, and showthat it often is far superior to revalidation from scratch. Wepresent two classes of schemas, which capture most real-lifeDTDs, and show that they admit a logarithmic timeincremental validation algorithm that, in many cases, requiresonly constant auxiliary space. We then discuss animplementation of these algorithms that is independent of,and can be customized for different storage mechanismsfor XML. Finally, we present extensive experimental resultsshowing that our approach is highly efficient and scalable.

#index 745468
#* Multiresolution Indexing of XML for Frequent Queries
#@ Hao He;Jun Yang
#t 2004
#c 17
#% 31484
#% 236416
#% 333981
#% 397359
#% 397360
#% 464720
#% 464883
#% 479465
#% 480489
#% 480656
#% 650962
#% 654452
#% 660000
#! XML and other types of semi-structured data are typicallyrepresented by a labeled directed graph. To speedup path expression queries over the graph, a variety ofstructural indexes have been proposed. They usually workby partitioning nodes in the data graph into equivalenceclasses and storing equivalence classes as index nodes.A(k)-index introduces the concept of local bisimilarity forpartitioning, allowing the trade-off between index size andquery answering power. However, all index nodes in A(k)-indexhave the same local similarity k, which cannot takeadvantage of the fact that a workload may contain path expressionsof different lengths, or that different parts of thedata graph may have different local similarity requirements.To overcome these limitations, we propose M(k)- andM*(k)-indexes. The basic M(k)-index is workload-aware:Like the previously proposed D(k)-index, it allows differentindex nodes to have different local similarity requirements,providing finer partitioning only for parts of the datagraph targeted by longer path expressions. Unlike D(k)-index,M(k)-index is never over-refined for irrelevant indexor data nodes. However, the workload-aware featurestill incurs overrefinement due to over-qualified parent indexnodes. Moreover, fine partitions penalize the performanceof short path expressions. To solve these problems,we further propose the M*(k)-index. An M*(k)-index consistsof a collection of indexes whose nodes are organizedin a partition hierarchy, allowing successively coarser partitioninginformation to co-exist with the finest partitioninginformation required. Experiments show that our indexesare superior to previously proposed indexes in terms of indexsize and query performance.

#index 745469
#* Modeling Uncertainties in Publish/Subscribe Systems
#@ Haifeng Liu;Hans-Arno Jacobsen
#t 2004
#c 17
#% 25443
#% 215225
#% 248010
#% 333938
#% 383684
#% 397918
#% 462223
#% 463734
#% 480649
#% 567463
#% 661478
#% 994039
#% 1015360
#! In the publish/subscribe paradigm, informationproviders disseminate publications to all consumers whohave expressed interest by registering subscriptions. Thisparadigm has found wide-spread applications, rangingfrom selective information dissemination to network management.However, all existing publish/subscribe systemscannot capture uncertainty inherent to the information ineither subscriptions or publications. In many situations,exact knowledge of either specific subscriptions or publicationsis not available. Moreover, especially in selectiveinformation dissemination applications, it is often moreappropriate for a user to formulate her search requestsor information offers in less precise terms, rather thandefining a sharp limit. To address these problems, thispaper proposes a new publish/subscribe model based onpossibility theory and fuzzy set theory to process uncertaintiesfor both subscriptions and publications. Furthermore,an approximate publish/subscribe matching problem isdefined and algorithms for solving it are developed andevaluated.

#index 745470
#* Scalable Multimedia Disk Scheduling
#@ Mohamed F. Mokbel;Walid G. Aref;Khaled Elbassioni;Ibrahim Kamel
#t 2004
#c 17
#% 77990
#% 149274
#% 232806
#% 235758
#% 250179
#% 288821
#% 342721
#% 435110
#% 443457
#% 495420
#% 631954
#% 632049
#% 657736
#% 1136878
#% 1180241
#! A new multimedia disk scheduling algorithm, termedCascaded-SFC, is presented. The Cascaded-SFC multimediadisk scheduler is applicable in environments where multimediadata requests arrive with different quality of service(QoS) requirements such as real-time deadline and user priority.Previous work on disk scheduling has focused on optimizingthe seek times and/or meeting the real-time deadlines.The Cascaded-SFC disk scheduler provides a unifiedframework for multimedia disk scheduling that scaleswith the number of scheduling parameters. The generalidea is based on modeling the multimedia disk requestsas points in multiple multi-dimensional sub-spaces, whereeach of the dimensions represents one of the parameters(e.g., one dimension represents the request deadline, anotherrepresents the disk cylinder number, and a third dimensionrepresents the priority of the request, etc.). Eachmulti-dimensional sub-space represents a subset of the QoSparameters that share some common scheduling characteristics.Then the multimedia disk scheduling problem reducesto the problem of finding a linear order to traversethe multi-dimensional points in each sub-space. Multiplespace-filling curves are selected to fit the scheduling needsof the QoS parameters in each sub-space. The orders ineach sub-space are integrated in a cascaded way to providea total order for the whole space. Comprehensive experimentsdemonstrate the efficiency and scalability of theCascaded-SFC disk scheduling algorithm over other diskschedulers.

#index 745471
#* Priority Mechanisms for OLTP and Transactional Web Applications
#@ David T. McWherter;Bianca Schroeder;Anastassia Ailamaki;Mor Harchol-Balter
#t 2004
#c 17
#% 77988
#% 77990
#% 117903
#% 287230
#% 438098
#% 442012
#% 480266
#% 481127
#% 661413
#! Transactional workloads are a hallmark of modernOLTP and Web applications, ranging from electronic commerceand banking to online shopping. Often, the databaseat the core of these applications is the performance bottleneck.Given the limited resources available to the database,transaction execution times can vary wildly as they competeand wait for critical resources. As the competitor is "only aclick away," valuable (high-priority) users must be ensuredconsistently good performance via QoS and transaction prioritization.This paper analyzes and proposes prioritization fortransactional workloads in traditional database systems(DBMS). This work first performs a detailed bottleneckanalysis of resource usage by transactional workloads oncommercial and noncommercial DBMS (IBM DB2, PostgreSQL,Shore) under a range of configurations. Second,this work implements and evaluates the performance of severalpreemptive and non-preemptive DBMS prioritizationpolicies in PostgreSQL and Shore. The primary contributionsof this work include (i) understanding the bottleneckresources in transactional DBMS workloads and (ii) ademonstration that prioritization in traditional DBMS canprovide 2x-5x improvement for high-priority transactionsusing simple scheduling policies, without expense to low-prioritytransactions.

#index 745472
#* A Probabilistic Approach to Metasearching with Adaptive Probing
#@ Zhenyu Liu;Chang Luo;Junghoo Cho;Wesley W. Chu
#t 2004
#c 17
#% 172898
#% 194246
#% 210172
#% 213981
#% 229827
#% 231524
#% 262063
#% 274483
#% 280854
#% 287463
#% 301225
#% 333945
#% 397378
#% 404719
#% 406493
#% 479642
#% 479816
#% 481748
#% 567255
#% 572311
#% 993964
#! An ever-increasing amount of valuable information isstored in Web databases, "hidden" behind search interfaces.To save the user's effort in manually exploring eachdatabase, metasearchers automatically select the most relevantdatabases to a user's query. In thispaper, we focus on one of the technical challenges in metasearching,namely database selection. Past research uses a pre-collectedsummary of each database to estimate its "relevancy" to thequery, and in many cases make incorrect database selection.In this paper, we propose two techniques: probabilisticrelevancy modelling and adaptive probing. First, we modelthe relevancy of each database to a given query as a probabilisticdistribution, derived by sampling that database. Usingthe probabilistic model, the user can explicitly specify a desiredlevel of certainty for database selection. The adaptiveprobing technique decides which and how many databases to contactin order to satisfy the user's requirement. Our experimentson real Hidden-Web databases indicate that our approach significantlyimproves the accuracy of database selection at the cost ofa small number of database probing.

#index 745473
#* A Web-Services Architecture for Efficient XML Data Exchange
#@ Sihem Amer-Yahia;Yannis Kotidis
#t 2004
#c 17
#% 83933
#% 86949
#% 333935
#% 386455
#% 480317
#% 578855
#% 654485
#% 993941
#% 994001
#! Business applications often exchange large amounts ofenterprise data stored in legacy systems. The advent of XMLas a standard specification format has improved applicationsinteroperability. However, optimizing the performanceof XML data exchange, in particular, when data volumesare large, is still in its infancy. Quite often, the target systemhas to undo some of the work the source did to assembledocuments in order to map XML elements into its owndata structures. This publish&map process is both resourceand time consuming.In this paper, we develop a middle-tier Web services architectureto optimize the exchange of large XML data volumes.The key idea is to allow systems to negotiate thedata exchange process using an extension to WSDL. Thesource (target) can specify document fragments that it iswilling to produce (consume). Given these fragmentations,the middle-ware instruments the data exchange process betweenthe two systems to minimize the number of necessaryoperations and optimize the distributed processing betweenthe source and the target systems. We show that ournew exchange paradigm outperforms publish&map and enablesmore flexible scenarios without necessitating substantialmodifications to the underlying systems.

#index 745474
#* From Sipping on a Straw to Drinking from a Fire Hose: Data Integration in a Public Genome Database
#@ J. E. Richardson;J. A. Kadin;J. A. Blake;C. J. Bult;J. T. Eppig;M. Ringwald
#t 2004
#c 17
#! Biology is a vast domain. The Mouse GenomeInformatics (MGI) system, which focuses on thebiology of the laboratory mouse, covers only a small,carefully chosen slice. Nevertheless, we deal with dataof immense variety, deep complexity, and exponentiallygrowing volume. Our role as an integration nexus is toadd value by combining data sets of diverse types andorigins, eliminating redundancy and resolvingconflicts. In this paper, we briefly describe some of theissues we face and approaches we have adopted to theintegration problem.

#index 745475
#* Stream Query Processing for Healthcare Bio-sensor Applications
#@ Chung-Min Chen;Hira Agrawal;Munir Cochinwala;David Rosenbluth
#t 2004
#c 17
#% 578560
#% 979303
#! The need of a data stream management system(DSMS), with the capability of querying continuous datastreams, has been well understood by the databaseresearch community and witnessed by a proliferation ofrelated publications in this area (see, e.g., for a partialsurvey). Examples of applications abound in manydomains: from environmental and military applicationsconsuming streams of sensor data, to telecommunicationsand data network assurance systems analyzing real-timenetwork traffic data.This article provides an overview on a DSMSprototype called T2. T2 inherits some of the concepts ofan early prototype, Tribeca, developed also atTelcordia, but with complete new design andimplementation in Java with an SQL-like query language.

#index 745476
#* Adapting a Generic Match Algorithm to Align Ontologies of Human Anatomy
#@ Peter Mork;Philip A. Bernstein
#t 2004
#c 17
#% 174161
#% 480645
#% 572314
#% 660001
#% 766116
#% 1015326
#! The difficulty inherent in schema matching has ledto the development of several generic match algorithms.This paper describes how we adapted generalapproaches to the specific task of aligning two ontologiesof human anatomy, the Foundational Model ofAnatomy and the GALEN Common Reference Model.Our approach consists of three phases: lexical, structuraland hierarchical, which leverage different aspectsof the ontologies as they are represented in ageneric meta-model. Lexical matching identifies conceptswith similar names. Structural matching identifiesconcepts whose neighbors are similar. Finally,hierarchical matching identifies concepts with similardescendants. We conclude by reporting on the lessonswe learned.

#index 745477
#* A Succinct Physical Storage Scheme for Efficient Evaluation of Path Queries in XML
#@ Ning Zhang;Varun Kacholia;M. Tamer Özsu
#t 2004
#c 17
#% 157167
#% 234905
#% 289335
#% 333981
#% 378391
#% 378412
#% 397366
#% 397375
#% 458776
#% 562318
#% 570876
#% 654442
#% 654450
#% 654493
#% 659999
#% 745518
#% 770338
#% 993939
#% 1015275
#% 1015277
#! Path expressions are ubiquitous in XML processing languages.Existing approaches evaluate a path expression byselecting nodes that satisfies the tag-name and value constraintsconstraints. In this paper, we propose a novel approach,and then joining them according to the structuralnext-of-kin (NoK) pattern matching, to speed up the node-selectionstep, and to reduce the join size significantly in thesecond step. To efficiently perform NoK pattern matching,we also propose a succinct XML physical storage schemethat is adaptive to updates and streaming XML as well. Ourperformance results demonstrate that the proposed storagescheme and path evaluation algorithm is highly efficient andoutperforms the other tested systems in most cases.

#index 745478
#* Recursive XML Schemas, Recursive XML Queries, and Relational Storage: XML-to-SQL Query Translation
#@ Rajasekar Krishnamurthy;Venkatesan T. Chakaravarthy;Raghav Kaushik;Jeffrey F. Naughton
#t 2004
#c 17
#% 70370
#% 273922
#% 309851
#% 333935
#% 333989
#% 340144
#% 345742
#% 348183
#% 397366
#% 397374
#% 428146
#% 462235
#% 464724
#% 479956
#% 480152
#% 480657
#% 480822
#% 489174
#% 504574
#% 654484
#% 654493
#% 993941
#% 994001
#% 1015271
#% 1373479
#% 1393700
#! We consider the problem of translating XML queries intoSQL when XML documents have been stored in an RDBMSusing a schema-based relational decomposition. Surprisingly,there is no published XML-to-SQL query translationalgorithm for this scenario that handles recursive XMLschemas. We present a generic algorithm to translate pathexpression queries into SQL in the presence of recursionin the schema and queries. This algorithm handles a generalclass of XML-to-Relational mappings, which includesall techniques proposed in literature. Some of the salientfeatures of this algorithm are: (i) It translates a path expressionquery into a single SQL query, irrespective of howcomplex the XML schema is, (ii) It uses the "with" clause inSQL99 to handle recursive queries even over non-recursiveschemas, (iii) It reconstructs recursive XML subtrees witha single SQL query and (iv) It shows that the support forlinear recursion in SQL99 is sufficient for handling pathexpression queries over arbitrarily complex recursive XMLschema.

#index 745479
#* A Prime Number Labeling Scheme for Dynamic Ordered XML Trees
#@ Xiaodong Wu;Mong Li Lee;Wynne Hsu
#t 2004
#c 17
#% 236416
#% 325384
#% 340144
#% 378412
#% 379484
#% 397366
#% 479465
#% 480489
#% 480656
#% 577358
#! Efficient evaluation of XML queries requires thedetermination of whether a relationship exists betweentwo elements. A number of labeling schemes have beendesigned to label the element nodes such that therelationships between nodes can be easily determinedby comparing their labels. With the increasedpopularity of XML on the web, finding a labelingscheme that is able to support order-sensitive queriesin the presence of dynamic updates becomes urgent. Inthis paper, we propose a new labeling scheme thattakes advantage of the unique property of primenumbers to meet this need. The global order of thenodes can be captured by generating simultaneouscongruence values from the prime number node labels.Theoretical analysis of the label size requirements forthe various labeling schemes is given. Experimentresults indicate that the prime number labeling schemeis compact compared to existing dynamic labelingschemes, and provides efficient support to order-sensitivequeries and updates.

#index 745480
#* Similarity Search in Multimedia Databases
#@ Daniel A. Keim;Benjamin Bustos
#t 2004
#c 17

#index 745481
#* Data Management in Location-Dependent Information Services
#@ Baihua Zheng;Jianliang Xu;Wang-Chien Lee
#t 2004
#c 17

#index 745482
#* Implementation and Research Issues in Query Processing for Wireless Sensor Networks
#@ Wei Hong;Samuel Madden
#t 2004
#c 17

#index 745483
#* Meta Data Management
#@ Philip A. Bernstein;Sergey Melnik
#t 2004
#c 17

#index 745484
#* XML Query Processing
#@ Daniela Florescu;Donald Kossmann
#t 2004
#c 17
#% 1015338

#index 745485
#* Approximate Temporal Aggregation
#@ Yufei Tao;Dimitris Papadias;Christos Faloutsos
#t 2004
#c 17
#% 164360
#% 172902
#% 227883
#% 287070
#% 318703
#% 333874
#% 378398
#% 397385
#% 411356
#% 443130
#% 443396
#% 453192
#% 458858
#% 465010
#% 465060
#% 465162
#% 527189
#% 527328
#% 571296
#! Temporal aggregate queries retrieve summarizedinformation about records with time-evolving attributes.Existing approaches have at least one of the followingshortcomings: (i) they incur large space requirements, (ii)they have high processing cost and (iii) they are based oncomplex structures, which are not available in commercialsystems. In this paper we solve these problems byapproximation techniques with bounded error. Wepropose two methods: the first one is based on multi-versionB-trees and has logarithmic worst-case query cost,while the second technique uses off-the-shelf B- and R-trees,and achieves the same performance in the expectedcase. We experimentally demonstrate that the proposedmethods consume an order of magnitude less space thantheir competitors and are significantly faster, even forcases that the permissible error bound is very small.

#index 745486
#* Spatio-Temporal Aggregation Using Sketches
#@ Yufei Tao;George Kollios;Jeffrey Considine;Feifei Li;Dimitris Papadias
#t 2004
#c 17
#% 2833
#% 86950
#% 172902
#% 227883
#% 300174
#% 333874
#% 333977
#% 378398
#% 397385
#% 427199
#% 458858
#% 465060
#% 465162
#% 480093
#% 527189
#% 577219
#% 617845
#% 654463
#% 731404
#% 745442
#% 1015320
#! Several spatio-temporal applications require the retrievalof summarized information about moving objects that liein a query region during a query interval (e.g., the numberof mobile users covered by a cell, traffic volume in adistrict, etc.). Existing solutions have the distinct countingproblem: if an object remains in the query region forseveral timestamps during the query interval, it will becounted multiple times in the result. The paper solves thisproblem by integrating spatio-temporal indexes withsketches, traditionally used for approximate queryprocessing. The proposed techniques can also be appliedto reduce the space requirements of conventional spatio-temporaldata and to mine spatio-temporal association rules.

#index 745487
#* Incorporating Updates in Domain Indexes: Experiences with Oracle Spatial R-trees
#@ Ravi Kanth V. Kothuri;Siva Ravada;Ning An
#t 2004
#c 17
#% 25152
#% 68091
#% 86950
#% 88056
#% 169805
#% 172922
#% 194253
#% 227864
#% 273888
#% 273941
#% 397396
#% 421118
#% 427199
#% 435141
#% 462059
#% 480482
#% 481759
#% 481956
#% 527193
#% 654479
#% 659961
#% 993386
#% 1015305
#% 1015325
#% 1015333
#% 1015340
#! Much research has been devoted to scalable storage andretrieval techniques for domain databases such as spatial,text, xml and gene sequence data. Many efficient indexingtechniques have been developed in this context. Given theimprovement in the underlying technology, database applicationsare increasingly using domain data in transactionalsemantics. In this paper, we examine the issue of when duringthe lifetime of a transaction is it better to incorporateupdates in domain indexes. We present our experiences withR-tree indexes in Oracle.We examine two approaches for incorporating updatesin spatial R-tree indexes: the first at update time, and thesecond at commit time. The first approach immediatelyincorporates changes in the index right away using systemtransactions and at commit time makes them visibleto other transactions. The second approach, referred toas the deferred-incorporate approach, defers the updatesin a secondary table and incorporates the changes in theindex only at commit time. In experiments on real datasets, we compare the performance of the two approaches.For most transactions with reasonable number of updateoperations, we observe that the deferred approach outperformsthe immediate-incorporate approach significantlyfor update operations and with appropriate optimizationsachieves comparable query performance.

#index 745488
#* Hash-Merge Join: A Non-blocking Join Algorithm for Producing Fast and Early Join Results
#@ Mohamed F. Mokbel;Ming Lu;Walid G. Aref
#t 2004
#c 17
#% 3771
#% 58352
#% 77960
#% 114577
#% 115661
#% 136740
#% 214602
#% 227883
#% 227894
#% 263479
#% 273910
#% 273911
#% 300167
#% 340305
#% 340635
#% 397370
#% 452838
#% 576104
#% 659919
#% 993956
#! This paper introduces the hash-merge join algorithm(HMJ, for short); a new non-blocking join algorithm thatdeals with data items from remote sources via unpredictable,slow, or bursty network traffic. The HMJ algorithmis designed with two goals in mind: (1) Minimize thetime to produce the first few results, and (2) Produce joinresults even if the two sources of the join operator occasionallyget blocked. The HMJ algorithm has two phases: Thehashing phase and the merging phase. The hashing phaseemploys an in-memory hash-based join algorithm that producesjoin results as quickly as data arrives. The mergingphase is responsible for producing join results if the twosources are blocked. Both phases of the HMJ algorithmare connected via a flushing policy that flushes in-memoryparts into disk storage once the memory is exhausted. Experimentalresults show that HMJ combines the advantagesof two state-of-the-art non-blocking join algorithms (XJoinand Progressive Merge Join) while avoiding their short-comings.

#index 745489
#* Selectivity Estimation for String Predicates: Overcoming the Underestimation Problem
#@ Surajit Chaudhuri;Venkatesh Ganti;Luis Gravano
#t 2004
#c 17
#% 201921
#% 209021
#% 210189
#% 273901
#% 299984
#% 326293
#% 480488
#% 571046
#% 993968
#! Queries with (equality or LIKE) selection predicatesover string attributes are widely used in relationaldatabases. However, state-of-the-art techniques forestimating selectivities of string predicates are often biasedtowards severely underestimating selectivities. In thispaper, we develop accurate selectivity estimators for stringpredicates that adapt to data and query characteristics,and which can exploit and build on a variety of existingestimators. A thorough experimental evaluation over realdata sets demonstrates the resilience of our estimators tovariations in both data and query characteristics.

#index 745490
#* A Frequency-based Approach for Mining Coverage Statistics in Data Integration
#@ Zaiging Nie;Subbarao Kambhampati
#t 2004
#c 17
#% 210176
#% 316709
#% 340306
#% 342684
#% 413643
#% 479813
#% 480149
#% 481290
#% 481923
#% 482108
#% 496091
#% 571037
#% 659968
#% 665561
#% 993964
#% 1015359
#! Query optimization in data integration requires source coverageand overlap statistics.Gathering and storing the requiredstatistics presents many challenges, not the least of which is controllingthe amount of statistics learned.In this paper we introduceStatMiner, a novel statistics mining approach which automaticallygenerates attribute value hierarchies, efficiently discoversfrequently accesses query classes based on the learned attributevalue hierarchies, and learns statistics only with respect to theseclasses.We describe the details of our method, and present experimentalresults demonstrating the efficiency and effectiveness of ourapproach.Our experiments are done in the context of BibFinder,a publicly fielded bibliography mediator.

#index 745491
#* CrossMine: Efficient Classification Across Multiple Database Relations
#@ Xiaoxin Yin;Jiawei Han;Jiong Yang;Philip S. Yu
#t 2004
#c 17
#% 99396
#% 136350
#% 376266
#% 393907
#% 396021
#% 398844
#% 420077
#% 458257
#% 464304
#% 466073
#% 479787
#% 629708
#% 1289267
#! Most of today's structured data is stored in relationaldatabases. Such a database consists of multiplerelations which are linked together conceptually viaentity-relationship links in the design of relational databaseschemas. Multi-relational classification can be widelyused in many disciplines, such as financial decision making,medical research, and geographical applications.However, most classification approaches only work on single"flat" data relations. It is usually difficult to convertmultiple relations into a single flat relation without eitherintroducing huge, undesirable "universal relation" orlosing essential information. Previous works using InductiveLogic Programming approaches (recently also knownas Relational Mining) have proven effective with high accuracyin multi-relational classification. Unfortunately,they suffer from poor scalability w.r.t. the number of relationsand the number of attributes in databases.In this paper we propose CrossMine, an efficientand scalable approach for multi-relational classification.Several novel methods are developed in CrossMine,including (1) tuple ID propagation, which performssemantics-preserving virtual join to achieve high efficiencyon databases with complex schemas, and (2) a selectivesampling method, which makes it highly scalablew.r.t. the number of tuples in the databases. Both theoreticalbackgrounds and implementation techniques ofCrossMine are introduced. Our comprehensive experimentson both real and synthetic databases demonstratethe high scalability and accuracy of CrossMine.

#index 745492
#* An Efficient Algorithm for Mining Frequent Sequences by a New Strategy without Support Counting
#@ Ding-Ying Chiu;Yi-Hung Wu;Arbee L.  P. Chen
#t 2004
#c 17
#% 223891
#% 280488
#% 310559
#% 329537
#% 342666
#% 397383
#% 413550
#% 424330
#% 443502
#% 459006
#% 463903
#% 464996
#% 466500
#% 577256
#% 1775124
#! Mining sequential patterns in large databases is animportant research topic. The main challenge of miningsequential patterns is the high processing cost due to thelarge amount of data. In this paper, we propose a newstrategy called DIrect Sequence Comparison (abbreviatedas DISC), which can find frequent sequences without havingto compute the support counts of non-frequent sequences.The main difference between the DISC strategy and theprevious works is the way to prune non-frequent sequences.The previous works are based on the anti-monotoneproperty, which prune the non-frequent sequencesaccording to the frequent sequences with shorter lengths.On the contrary, the DISC strategy prunes the non-frequentsequences according to the other sequences with the samelength. Moreover, we summarize three strategies used in theprevious works and design an efficient algorithm calledDISC-all to take advantages of all the four strategies. Theexperimental results show that the DISC-all algorithmoutperforms the PrefixSpan algorithm on mining frequentsequences in large databases. In addition, we analyze thesestrategies to design the dynamic version of our algorithm,which achieves a much better performance.

#index 745493
#* Substructure Clustering on Sequential 3d Object Datasets
#@ Zhenqiang Tan;Anthony K.  H. Tung
#t 2004
#c 17
#% 397382
#% 397383
#% 629607
#% 636971
#% 729938
#% 1015336
#! In this paper, we will look at substructure clustering ofsequentail 3d objects.A sequential 3d object is a set ofpoints located in a three dimensional space that are linkedup to form a sequence.Given a set of sequential 3d objects,our aim is to find significantly large substructures whichare present in many of the sequential 3d objects.Unliketraditional subspace clustering methods in which objectsare compared based on values in the same dimension, thematching dimensions between two 3d sequential objects areaffected by both the translation and rotation of the objectsand are thus not well defined.Instead, similarity betweenthe objects are judge by computing a structural distancemeasurement call rmsd(Root Mean Square Distance)which require proper alignment (including translation androtation) of the objects.As the computation of rmsd isexpensive, we proposed a new measure call ald(AngelLength Distance) which is shown experimentally to approximatermsd.Based on ald, we define a new clusteringmodel called sCluster and devise an algorithm for discoveringall maximum sCluster in a 3d sequentail dataset.Experiments are conducted to illustrate the efficiency andeffectiveness of our algorithm.

#index 745494
#* ItCompress: An Iterative Semantic Compression Algorithm
#@ H. V. Jagadish;Raymond T. Ng;Beng Chin Ooi;Anthony K.  H. Tung
#t 2004
#c 17
#% 11805
#% 36672
#% 80995
#% 201893
#% 210173
#% 227924
#% 248790
#% 333954
#% 408396
#% 420082
#% 443122
#% 464890
#% 465004
#% 480124
#% 1012144
#! Real datasets are often large enough to necessitate datacompression. Traditional 'syntactic' data compression methodstreat the table as a large byte string and operate at thebyte level. The tradeoff in such cases is usually between theease of retrieval (the ease with which one can retrieve a singletuple or attribute value without decompressing a much largerunit) and the effectiveness of the compression. In this regard,the use of semantic compression has generated considerableinterest and motivated certain recent works.In this paper, we propose a semantic compression algorithmcalled ItCompress ITerative Compression, whichachieves good compression while permitting access even atattribute level without requiring the decompression of a largerunit. ItCompress iteratively improves the compression ratioof the compressed output during each scan of the table. Theamount of compression can be tuned based on the number ofiterations. Moreover, the initial iterations provide significantcompression, thereby making it a cost-effective compressiontechnique. Extensive experiments were conducted and the resultsindicate the superiority of ItCompress with respect topreviously known tehniques, such as 'SPARTAN' and 'fascicles'.

#index 745495
#* Bulk Operations for Space-Partitioning Trees
#@ Thanaa M. Ghanem;Rahul Shah;Mohamed F. Mokbel;Walid G. Aref;Jeffrey S. Vitter
#t 2004
#c 17
#% 12181
#% 68091
#% 100832
#% 227868
#% 241127
#% 252304
#% 254749
#% 264161
#% 282483
#% 286237
#% 287381
#% 288578
#% 317313
#% 321455
#% 326878
#% 341100
#% 415957
#% 421118
#% 427199
#% 430750
#% 458741
#% 462059
#% 479462
#% 479473
#% 480651
#% 481455
#% 481599
#% 481956
#% 482090
#% 487509
#% 492596
#% 527174
#% 548299
#% 567865
#% 605182
#% 664822
#% 1015333
#! The emergence of extensible index structures, e.g.,GiST (Generalized Search Tree) and SP-GiST (Space-PartitioningGeneralized Search Tree), calls for a set ofextensible algorithms to support different operations (e.g.,insertion, deletion, and search). Extensible bulk operations(e.g., bulk loading and bulk insertion) are of the same importanceand need to be supported in these index engines.In this paper, we propose two extensible buffer-based algorithmsfor bulk operations in the class of space-partitioningtrees; a class of hierarchical data structures that recursivelydecompose the space into disjoint partitions. Themain idea of these algorithms is to build an in-memory treeof the target space-partitioning index. Then, data itemsare recursively partitioned into disk-based buffers usingthe in-memory tree. Although the second algorithm is designedfor bulk insertion, it can be used in bulk loading aswell. The proposed extensible algorithms are implementedinside SP-GiST; a framework for supporting the class ofspace-partitioning trees. Both algorithms have I/O boundO(NH/B), whereN is the number of data items to be bulkloaded/inserted, B is the number of tree nodes that can fitin one disk page, H is the tree height in terms of pages afterapplying a clustering algorithm. Experimental results areprovided to show the scalability and applicability of the proposedalgorithms for the class of space-partitioning trees.A comparison of the two proposed algorithms shows thatthe first algorithm performs better in case of bulk loading.However the second algorithm is more general and can beused for efficient bulk insertion.

#index 745496
#* LDC: Enabling Search By Partial Distance In A Hyper-Dimensional Space
#@ Nick Koudas;Beng Chin Ooi;Heng Tao Shen;Anthony K.  H. Tung
#t 2004
#c 17
#% 210173
#% 248790
#% 248796
#% 271237
#% 342828
#% 397376
#% 464888
#% 479649
#% 480133
#% 480307
#% 480632
#% 632035
#! Recent advances in research fields like multimediaand bioinformatics have brought about a new generation of hyper-dimensional databases which can contain hundreds or even thousands of dimensions. Such hyper-dimensional databases pose significant problems to existinghigh-dimensional indexing techniques which have been developed for indexing databases with (commonly) lessthan a hundred dimensions. To support efficient querying and retrieval on hyper-dimensional databases, we propose a methodology called Local Digital Coding (LDC)which can support k-nearest neighbors (KNN) queries onhyper-dimensional databases and yet co-exist with ubiquitous indices, such as B+-trees. LDC extracts a simple bitmap representation called Digital Code(DC) for each point in the database.Pruning during KNN search is performed by dynamically selecting only a subset of the bits from the DC based on which subsequent comparisons are performed. In doing so, expensive operations involved in computing L-norm distance functions between hyper-dimensional data can be avoided. Extensive experiments are conducted to show that our methodology offers significant performance advantages over other existing indexing methods on both real life and synthetic hyper-dimensional datasets.

#index 745497
#* Simple, Robust and Highly Concurrent B-trees with Node Deletion
#@ David Lomet
#t 2004
#c 17
#% 6716
#% 36118
#% 64430
#% 102808
#% 114582
#% 116063
#% 116085
#% 116087
#% 135557
#% 227864
#% 268786
#% 286929
#% 336201
#% 403195
#% 427199
#% 571081
#% 571093
#% 604297
#! Why might B-tree concurrency control still beinteresting? For two reasons: (i) currentlyexploited "real world" approaches arecomplicated; (ii) simpler proposals are not usedbecause they are not sufficiently robust. In the"real world", systems need to deal robustly withnode deletion, and this is an important reasonwhy the currently exploited techniques arecomplicated. In our effort to simplify the worldof robust and highly concurrent B-tree methods,we focus on exactly where b-tree concurrencycontrol needs information about node deletes,and describe mechanisms that provide thatinformation. We exploit the Blink-tree property ofbeing "well-formed" even when index termposting for a node split has not been completedto greatly simplify our algorithms. Our goal is todescribe a very simple but nonetheless robustmethod.

#index 745498
#* A Peer-to-peer Framework for Caching Range Queries
#@ O. D. Sahin;A. Gupta;D. Agrawal;A. El Abbadi
#t 2004
#c 17
#% 198465
#% 340175
#% 340176
#% 349973
#% 496151
#% 496291
#% 505869
#% 612643
#% 636009
#% 654468
#% 674136
#! Peer-to-peer systems are mainly used for object sharingalthough they can provide the infrastructure for manyother applications. In this paper, we extend the idea of objectsharing to data sharing on a peer-to-peer system. Wepropose a method, which is based on the multidimensionalCAN system, for efficiently evaluating range queries. Theanswers of the range queries are cached at the peers andare used to answer future range queries. The scalabilityand efficiency of our design is shown through simulation.

#index 745499
#* Improved File Synchronization Techniques for Maintaining Large Replicated Collections over Slow Networks
#@ Torsten Suel;Patrick Noel;Dimitre Trendafilov
#t 2004
#c 17
#% 92669
#% 146377
#% 238182
#% 251442
#% 259646
#% 282618
#% 300139
#% 302728
#% 309747
#% 310770
#% 330604
#% 342373
#% 343021
#% 398147
#% 401087
#% 438251
#% 444141
#% 444351
#% 480136
#% 521993
#% 571888
#% 577365
#% 768815
#% 779311
#% 805476
#% 960121
#% 978374
#% 993974
#% 1012313
#% 1863306
#! We study the problem of maintaining large replicated collectionsof files or documents in a distributed environment withlimited bandwidth. This problem arises in a number of importantapplications, such as synchronization of data betweenaccounts or devices, content distibution and web caching networks,web site mirroring, storage networks, and large scaleweb search and mining. At the core of the problem lies thefollowing challenge, called the file synchronization problem:given two versions of a file on different machines, say an outdatedand a current one, how can we update the outdatedversion with minimum communication cost, by exploiting thesignificant similarity between the versions? While a popularopen source tool for this problem called rsync is used in hundredsof thousands of installations, there have been only veryfew attempts to improve upon this tool in practice.In this paper, we propose a framework for remote file synchronizationand describe several new techniques that resultin significant bandwidth savings. Our focus is on applicationswhere very large collections have to be maintainedover slow connections. We show that a prototype implementationof our framework and techniques achieves significantimprovements over rsync. As an example application, we focuson the efficient synchronization of very large web pagecollections for the purpose of search, mining, and contentdistribution.

#index 745500
#* ContextMetricsTM: Semantic and Syntactic Interoperability in Cross-Border Trading Systems
#@ Chito Jovellanos
#t 2004
#c 17
#% 333988
#% 345758
#% 572314
#% 577524
#! This paper describes a method and system forquantifying the variances in the semantics and syntaxof electronic transactions exchanged betweenbusiness counterparties. ContextMetricsTM enables (a)dynamic transformations of outbound and inboundtransactions needed to effect 'straight-through-processing'(STP); (b) unbiased assessments ofcounterparty systems' capabilities to support STP;and (c) modeling of operational risks and financialexposures stemming from an enterprise'stransactional systems.

#index 745501
#* Improving Hash Join Performance through Prefetching
#@ Shimin Chen;Anastassia Ailamaki;Phillip B. Gibbons;Todd C. Mowry
#t 2004
#c 17
#% 3771
#% 83132
#% 97771
#% 111995
#% 128287
#% 136740
#% 172911
#% 201344
#% 213501
#% 251473
#% 251474
#% 267990
#% 270917
#% 300194
#% 304001
#% 304012
#% 333942
#% 333949
#% 397362
#% 479821
#% 480119
#% 480272
#% 480464
#% 566122
#% 693831
#! Hash join algorithms suffer from extensive CPU cachestalls. This paper shows that the standard hash join algorithm for disk-oriented databases (i.e. GRACE) spends over73% of its user time stalled on CPU cache misses, and explores the use of prefetching to improve its cache performance. Applying prefetching to hash joins is complicatedby the data dependencies, multiple code paths, and inherent randomness of hashing. We present two techniques, group prefetching and software-pipelined prefetching, thatovercome these complications.These schemes achieve 2.0- 2.9X speedups for the join phase and 1.4-2.6X speedups forthe partition phase over GRACE and simple prefetching approaches. Compared with previous cache-aware approaches(i.e. cache partitioning), the schemes are at least 50% fasteron large relations and do not require exclusive use of theCPU cache to be effective.

#index 745502
#* Go Green: Recycle and Reuse Frequent Patterns
#@ Gao Cong;Beng Chin Ooi;Kian-Lee Tan;Anthony K.  H. Tung
#t 2004
#c 17
#% 248785
#% 248791
#% 287242
#% 300120
#% 300124
#% 316709
#% 329598
#% 464204
#% 464989
#% 466490
#% 481290
#% 577234
#% 629655
#! In constrained data mining, users can specify constraintsto prune the search space to avoid mining uninterestingknowledge.This is typically done by specifyingsome initial values of the constraints that aresubsequently refined iteratively until satisfactory resultsare obtained.Existing mining schemes treat each iterationas a distinct mining process, and fail to exploit theinformation generated between iterations.In this paper,we propose to salvage knowledge that is discoveredfrom an earlier iteration of mining to enhance subsequentrounds of mining.In particular, we look at howfrequent patterns can be recycled.Our proposed strategyoperates in two phases.In the first phase, frequentpatterns obtained from an early iteration are used tocompress a database.In the second phase, subsequentmining processes operate on the compressed database.We propose two compression strategies and adapt threeexisting frequent pattern mining techniques to exploitthe compressed database.Results from our extensiveexperimental study show that our proposed recycling algorithmsoutperform their non-recycling counterpart byan order of magnitude.

#index 745503
#* Approximate Selection Queries over Imprecise Data
#@ Iosif Lazaridis;Sharad Mehrotra
#t 2004
#c 17
#% 68140
#% 71597
#% 84656
#% 201876
#% 227883
#% 273902
#% 273909
#% 325683
#% 333863
#% 333977
#% 397389
#% 479984
#% 480306
#% 480332
#% 654444
#% 654487
#! We examine the problem of evaluating selection queriesover imprecisely represented objects. Such objects are usedeither because they are much smaller in size than the preciseones (e.g., compressed versions of time series), or asimprecise replicas of fast-changing objects across the network(e.g., interval approximations for time-varying sensorreadings). It may be impossible to determine whether an impreciseobject meets the selection predicate. Additionally,the objects appearing in the output are also imprecise. Retrievingthe precise objects themselves (at additional cost)can be used to increase the quality of the reported answer.In our paper we allow queries to specify their own answerquality requirements. We show how the query evaluationsystem may do the minimal amount of work to meetthese requirements. Our work presents two important contributions:first, by considering queries with set-based answers,rather than the approximate aggregate queries overnumerical data examined in the literature; second, by aimingto minimize the combined cost of both data processingand probe operations in a single framework. Thus, we establishthat the answer accuracy/performance tradeoff canbe realized in a more general setting than previously seen.

#index 745504
#* Integrating XML Data in the TARGITOLAP System
#@ Dennis Pedersen;Jesper Pedersen;Torben Bach Pedersen
#t 2004
#c 17
#% 504154
#% 1084756
#! This paper presents work on logical integration of OLAPand XML data sources, carried out in cooperation betweenTARGIT, a Danish OLAP client vendor, and AalborgUniversity. A prototype has been developed that allowsXML data on the WWW to be used as dimensions and measuresin the OLAP system in the same way as ordinary dimensionsand measures, providing a powerful and flexibleway to handle unexpected or short-term data requirementsas well as rapidly changing data. Compared to earlier work,this paper presents several major extensions that resultedfrom TARGIT's requirements. These include the ability touse XML data as measures, as well as a novel multigranulardata model and query language that formalizes and extendsthe TARGIT data model and query language.

#index 745505
#* Benchmarking SAP R/3 Archiving Scenarios
#@ Bernhard Zeller;Alfons Kemper
#t 2004
#c 17
#% 227872
#% 411759
#% 465149
#% 572305
#! According to a survey of the University of Berkeley,about 5 Exabytes of new information has been created in2002. This information explosion affects also the databasevolumes of enterprise resource planning (ERP) systems likeSAP R/3, the market leader for ERP systems. Just like theoverall information explosion, the database volumes of ERPsystems are growing at a tremendous rate and some of themhave reached a size of several Terabytes. OLTP (OnlineTransaction Processing) databases of this size are hard tomaintain and tend to perform poorly. One way to limit thesize of a database is data staging, i.e., to make use of anSAP technique called archiving. That is, data which arenot needed for every-day operations are demoted from thedatabase (disks) to tertiary storage (tapes). In cooperationwith our research group, SAP is adapting their archivingtechniques to accelerate the archiving process by integratingnew technologies like XML and advanced database features.However, so far no benchmark existed to evaluate differentarchiving scenarios and to measure the impact of a changein the archiving technique. We therefore designed and implementeda generic benchmark which is applicable to manydifferent system layouts and allows the users to evaluate variousarchiving scenarios.

#index 745506
#* Range CUBE: Efficient Cube Computation by Exploiting Data Correlation
#@ Ying Feng;Divyakant Agrawal;Amr El Abbadi;Ahmed Metwally
#t 2004
#c 17
#% 227880
#% 236410
#% 273916
#% 280448
#% 333925
#% 397388
#% 464215
#% 479450
#% 479795
#% 479829
#% 481290
#% 654446
#% 660006
#% 993996
#% 1015294
#! Data cube computation and representation are prohibitivelyexpensive in terms of time and space. Prior workhas focused on either reducing the computation time or condensingthe representation of a data cube. In this paper,we introduce Range Cubing as an efficient way to computeand compress the data cube without any loss of precision.A new data structure, range trie, is used to compress andidentify correlation in attribute values, and compress theinput dataset to effectively reduce the computational cost.The range cubing algorithm generates a compressed cube,called range cube, which partitions all cells into disjointranges. Each range represents a subset of cells with thesame aggregation value, as a tuple which has the same numberof dimensions as the input data tuples. The range cubepreserves the roll-up/drill-down semantics of a data cube.Compared to H-Cubing, experiments on real dataset showa running time of less than one thirtieth, still generating arange cube of less than one ninth of the space of the fullcube, when both algorithms run in their preferred dimensionorders. On synthetic data, range cubing demonstratesmuch better scalability, as well as higher adaptiveness toboth data sparsity and skew.

#index 745507
#* Can A Semantic Web for Life Sciences Improve Drug Discovery?
#@ Eric K. Neumann
#t 2004
#c 17

#index 745508
#* Enabling Communities of Knowledge Workers
#@ David Lehman
#t 2004
#c 17
#! The MITRE corporation is a geographicallydistributed company of knowledge workers workingon projects either related by the same customer or bythe problems and technologies being addressed.MITRE early on recognized the power of informationtechnology to enhance the effectiveness of each of itsstaff with knowledge sharing tools. Early deploymentof an Intranet with novel functions has promotedinformation sharing among the staff, enabling eachstaff member to effectively leverage the knowledge ofthe entire company. Specific examples, storagestructures, information spaces, search mechanisms andpolicy issues that have promoted sharing have revealedmany lessons for enabling the knowledge worker.

#index 745509
#* Driving Forces in Database Technology
#@ Steven Hagan
#t 2004
#c 17
#! Several forces, with impacts so fundamental thatthey are akin to tectonic plate movements, are drivingthe commercial database marketplace. First ishardware commoditization: arrays of low pricedcomputers with high speed interconnects which yieldthe new cluster based computing capabilities referredto as 'Grid,' 'Utility,' and 'on-demand' computing, atprice points radically lower than standard Moore's lawprojections. The dramatic reductions in online storagehardware costs now makes it cost effective forcompanies to keep previously unimagined amounts ofcomplex data online. This will enable V/ULDBprojects with petabyte databases such as online imageapplications and data-driven supply chain managementapproaches (e.g. RFID) that store huge volumes ofhighly granular detail information in data warehouses(with significant history of temporal and spatialinterest).

#index 745510
#* Engineering a Fast Online Persistent Suffix Tree Construction
#@ Srikanta J. Bedathur;Jayant R. Haritsa
#t 2004
#c 17
#% 735
#% 152943
#% 198770
#% 200784
#% 235941
#% 289010
#% 300312
#% 352402
#% 480100
#% 480484
#% 481450
#% 591632
#% 593764
#% 593861
#% 993385
#% 1081012
#% 1116726
#! Online persistent suffix tree construction has been consideredimpractical due to its excessive I/O costs. However,these prior studies have not taken into account the effects ofthe buffer management policy and the internal node structureof the suffix tree on I/O behavior of construction andsubsequent retrievals over the tree. In this paper, we studythese two issues in detail in the context of large genomicDNA and Protein sequences. In particular, we make the followingcontributions: (i) a novel, low-overhead bufferingpolicy called TOP-Q which improves the on-disk behaviorof suffix tree construction and subsequent retrievals, and (ii)empirical evidence that the space efficient linked-list representationof suffix tree nodes provides significantly inferiorperformance when compared to the array representation.These results demonstrate that a careful choice ofimplementation strategies can make online persistent suffixtree construction considerably more scalable - in termsof length of sequences indexed with a fixed memory budget,than currently perceived.

#index 745511
#* Unordered Tree Mining with Applications to Phylogeny
#@ Dennis Shasha;Jason T.  L. Wang;Sen Zhang
#t 2004
#c 17
#% 186
#% 184048
#% 187659
#% 232136
#% 262071
#% 282470
#% 300033
#% 378391
#% 443349
#% 443514
#% 458757
#% 465018
#% 466644
#% 478274
#% 498538
#% 576105
#% 577218
#% 577219
#% 629617
#% 660001
#% 665390
#% 729938
#% 853020
#% 1268739
#! Frequent structure mining (FSM) aims to discover andextract patterns frequently occuring in structural data,such as trees and graphs.FSM finds many applications inbioinformatics, XML processing, Web log analysis, and soon.In this paper we present a new FSM technique for findingpatterns in rooted unordered labeled trees.The patternsof interest are cousin pairs in these trees.A cousin pair isa pair of nodes sharing the same parent, the same grand-parent,or the same great-grandparent, etc.Given a treeT, our algorithm finds all interesting cousin pairs of T inO(|T|2) time when |T| is the number of nodes in T.Experimentalresults on synthetic data and phylogenies showthe scalability and effectiveness of the proposed technique.To demonstrate the usefulness of our approach, we discussits applications to locating co-occurring patterns in multipleevolutionary trees, evaluating the consensus of equallyparsimonious trees, and finding kernel trees of groups ofphylogenies.We also describe extensions of our algorithmsfor undirected acyclic graphs (or free trees).

#index 745512
#* Querying the Past, the Present, and the Future
#@ Dieter Gawlick
#t 2004
#c 17

#index 745513
#* Online Amnesic Approximation of Streaming Time Series
#@ Themistoklis Palpanas;Michail Vlachos;Eamonn Keogh;Dimitrios Gunopulos;Wagner Truppel
#t 2004
#c 17
#% 69316
#% 172949
#% 309473
#% 326303
#% 399763
#% 466506
#% 477482
#% 480146
#% 480628
#% 485777
#% 549273
#% 576112
#% 577221
#% 631920
#% 631923
#% 646215
#% 659936
#% 660003
#% 709882
#% 993958
#% 993961
#% 1378172
#! The past decade has seen a wealth of research on time series representations, because the manipulation, storage, andindexing of large volumes of raw time series data is impractical. The vast majority of research has concentrated on representations that are calculated in batch mode and representeach value with approximately equal fidelity. However, the increasing deployment of mobile devices and real time sensorshas brought home the need for representations that can beincrementally updated, and can approximate the data with fidelity proportional to its age. The latter property allows us toanswer queries about the recent past with greater precision,since in many domains recent information is more useful thanolder information. We call such representations amnesic.While there has been previous work on amnesic representations, the class of amnesic functions possible was dictatedby the representation itself. In this work, we introduce anovel representation of time series that can represent arbitrary, user-specified amnesic functions. For example, a meteorologist may decide that data that is twice as old can toleratetwice as much error, and thus, specify a linear amnesic function. In contrast, an econometrist might opt for an exponentialamnesic function. We propose online algorithms for our representation, and discuss their properties. Finally, we performan extensive empirical evaluation on 40 datasets, and showthat our approach can efficiently maintain a high quality amnesicapproximation.

#index 745514
#* Mining Frequent Labeled and Partially Labeled Graph Patterns
#@ N. Vanetik;E. Gudes
#t 2004
#c 17
#% 262071
#% 273922
#% 443194
#% 466644
#% 478274
#% 479465
#% 481290
#% 528124
#% 571763
#% 619154
#% 629646
#% 629708
#% 1289345
#! Whereas data mining in structured data focuses on frequentdata values, in semi-structured and graph data theemphasis is on frequent labels and common topologies.Here, the structure of the data is just as important as itscontent.When data contains large amount of differentlabels, both fully labeled and partially data maybe useful.More informative patterns can be found in thedatabase if some of the pattern nodes can be regarded as'unlabeled'.We study the problem of discovering typicalfully and partially labeled patterns of graph data.Discovered patterns are useful in many applications, including:compact representation of source informationand a road-map for browsing and querying informationsources.

#index 745515
#* BIDE: Efficient Mining of Frequent Closed Sequences
#@ Jianyong Wang;Jiawei Han
#t 2004
#c 17
#% 310559
#% 329537
#% 338609
#% 397383
#% 413550
#% 459006
#% 463903
#% 464839
#% 464873
#% 477791
#% 479971
#% 481290
#% 577256
#% 629623
#% 629644
#% 631926
#% 660658
#% 729933
#% 729938
#! Previous studies have presented convincing argumentsthat a frequent pattern mining algorithm should not mineall frequent patterns but only the closed ones because thelatter leads to not only more compact yet complete resultset but also better efficiency. However, most of the previouslydeveloped closed pattern mining algorithms work underthe candidate maintenance-and-test paradigm which isinherently costly in both runtime and space usage when thesupport threshold is low or the patterns become long.In this paper, we present, BIDE, an efficient algorithmfor mining frequent closed sequences without candidatemaintenance. It adopts a novel sequence closure checkingscheme called BI-Directional Extension, and prunes thesearch space more deeply compared to the previous algorithmsby using the BackScan pruning method and the Scan-Skipoptimization technique. A thorough performance studywith both sparse and dense real-life data sets has demonstratedthat BIDE significantly outperforms the previous algorithms:it consumes order(s) of magnitude less memoryand can be more than an order of magnitude faster. It isalso linearly scalable in terms of database size.

#index 745516
#* Lazy Database Replication with Ordering Guarantees
#@ Khuzaima Daudjee;Kenneth Salem
#t 2004
#c 17
#% 9241
#% 91620
#% 124019
#% 210179
#% 237196
#% 237197
#% 240016
#% 256717
#% 273894
#% 314924
#% 335454
#% 340608
#% 435104
#% 452805
#% 463101
#% 480791
#% 570890
#% 1015337
#! Lazy replication is a popular technique for improvingthe performance and availability of database systems. Althoughthere are concurrency control techniques whichguarantee serializability in lazy replication systems, thesetechniques may result in undesirable transaction orderings.Since transactions may see stale data, they may be serializedin an order different from the one in which they weresubmitted. Strong serializability avoids such problems, butit is very costly to implement. In this paper, we propose ageneralized form of strong serializability that is suitable foruse with lazy replication. In addition to having many of theadvantages of strong serializability, it can be implementedmore efficiently. We show how generalized strong serializabilitycan be implemented in a lazy replication system, andwe present the results of a simulation study that quantifiesthe strengths and limitations of the approach.

#index 745517
#* Algebraic Signatures for Scalable Distributed Data Structures
#@ Witold Litwin;Thomas Schwarz
#t 2004
#c 17
#% 36360
#% 147139
#% 193286
#% 199547
#% 213080
#% 230435
#% 263364
#% 268473
#% 300165
#% 345753
#% 398751
#% 438284
#% 444141
#% 481296
#% 612167
#% 768815
#% 1012313
#% 1088933
#! Signatures detect changes to data objects.Numerous schemes are in use, especially thecryptographically secure standards SHA-1. Wepropose a novel signature scheme which we callalgebraic signatures. The scheme uses the Galois Fieldcalculations. Its major property is the sure detection ofany changes up to a parameterized size. Moreprecisely, we detect for sure any changes that do notexceed n-symbols for an n-symbol algebraic signature.This property is new for any known signature scheme.For larger changes, the collision probability istypically negligible, as for the other known schemes.We apply the algebraic signatures to the ScalableDistributed Data Structures (SDDS). We filter at theSDDS client node the updates that do not actuallychange the records. We also manage the concurrentupdates to data stored in the SDDS RAM buckets at theserver nodes. We further use the scheme for the fastdisk backup of these buckets. We sign our objects with4-byte signatures, instead of 20-byte standard SHA-1signatures. Our algebraic calculus is then also abouttwice as fast.

#index 745518
#* XBench Benchmark and Performance Testing of XML DBMSs
#@ Benjamin Bin Yao;M. Tamer Özsu;Nitin Khandelwal
#t 2004
#c 17
#% 152904
#% 246333
#% 397407
#% 458776
#% 541480
#% 562318
#% 650962
#% 654493
#! XML support is being added to existing database managementsystems (DBMSs) and native XML systems are beingdeveloped both in industry and in academia. The individualperformance characteristics of these approachesas well as the relative performance of various systems isan ongoing concern. In this paper we discuss the XBenchXML benchmark and report on the relative performance ofvarious DBMSs. XBench is a family of XML benchmarkswhich recognizes that the XML data that DBMSs manageare quite varied and no one database schema and workloadcan properly capture this variety. Thus, the members of thisbenchmark family have been defined for capturing diverseapplication domains.

#index 745519
#* Personalization of Queries in Database Systems
#@ Georgia Koutrika;Yannis Ioannidis
#t 2004
#c 17
#% 300170
#% 300179
#% 309726
#% 333951
#% 345045
#% 399057
#% 443298
#% 458873
#% 479816
#% 509541
#% 564130
#% 659990
#% 660011
#% 993987
#% 994017
#! As information becomes available in increasingamounts to a wide spectrum of users, the need fora shift towards a more user-centered informationaccess paradigm arises. We develop a personalizationframework for database systems based onuser profiles and identify the basic architecturalmodules required to support it. We define a preferencemodel that assigns to each atomic querycondition a personal degree of interest and providea mechanism to compute the degree of interestin any complex query condition based on thedegrees of interest in the constituent atomic ones.Preferences are stored in profiles. At query time,personalization proceeds in two steps: (a) preferenceselection and (b) preference integration intothe original user query. We formulate the mainpersonalization step, i.e. preference selection, asa graph computation problem and provide an efficientalgorithm for it. We also discuss results ofexperimentation with a prototype query personalization system.

#index 745520
#* Applications for Expression Data in Relational Database Systems
#@ Dieter Gawlick;Dmitry Lenkov;Aravind Yalamanchi;Lucy Chernobrod
#t 2004
#c 17
#% 210352
#% 271199
#% 427214
#% 435054
#% 488710
#% 562058
#% 610668
#% 736392
#! The support for the expression data type in arelational database system allows storing of conditionalexpressions as data in database tables and evaluatingthem using SQL queries. In the context of this newcapability, expressions can be interpreted asdescriptions, queries, and filters, and this significantlybroadens the use of a relational database system tosupport new types of applications. The paper presentsan overview of the expression data type, relatesexpressions to descriptions, queries, and filters,considers applications pertaining to informationdistribution, demand analysis, and task assignment, andshows how these applications can be easily supportedwith improved functionality.

#index 745521
#* BOSS: Browsing OPTICS-Plots for Similarity Search
#@ Stefan Brecheisen;Hans-Peter Kriegel;Peer Kröger;Martin Pfeifle;Maximilian Viermetz;Marco Pötke
#t 2004
#c 17
#% 273890
#% 587733

#index 745522
#* DBA Companion: A Tool for Logical Database Tuning
#@ Stéphane Lopes;Fabien De Marchi;Jean-Marc Petit
#t 2004
#c 17
#% 275367
#% 451552

#index 745523
#* wmdb.*: Rights Protection for Numeric Relational Data
#@ Radu Sion;Mikhail Atallah;Sunil Prabhakar
#t 2004
#c 17
#% 654449

#index 745524
#* A Flexible Infrastructure for Gathering XML Statistics and Estimating Query Cardinality
#@ Juliana Freire;Maya Ramanath;Lingzhi Zhang
#t 2004
#c 17
#% 397364
#% 659924

#index 745525
#* Superimposed Applications using SPARCE
#@ Sudarshan Murthy;David Maier;Lois Delcambre;Shawn Bowers
#t 2004
#c 17
#% 533911

#index 745526
#* "-Synopses: A System for Run-Time Management of Remote Synopses
#@ Yossi Matias;Leon Portman
#t 2004
#c 17

#index 745527
#* GenExplore: Interactive Exploration of Gene Interactions from Microarray Data
#@ Yong Ye;Xintao Wu;Kalpathi R. Subramanian;Liying Zhang
#t 2004
#c 17
#! DNA Microarray provides a powerful basis for analysisof gene expression. Data mining methods such as clusteringhave been widely applied to microarray data to link genesthat show similar expression patterns. However, this approachusually fails to unveil gene-gene interactions in thesame cluster. In this project, we propose to combine graphicalmodel based interaction analysis with other data miningtechniques (e.g., association rule, hierarchical clustering)for this purpose. For interaction analysis, we propose theuse of Graphical Gaussian Modelto discover pairwise geneinteractions and loglinear model to discover multi-gene interactions.We have constructed a prototype system that permitsrapid interactive exploration of gene relationships.

#index 745528
#* ToMAS: A System for Adapting Mappings while Schemas Evolve
#@ Yannis Velegrakis;Renée J. Miller;Lucian Popa;John Mylopoulos
#t 2004
#c 17
#% 328429
#% 378409
#% 993981
#% 994035
#% 1015303

#index 745529
#* Nested Queries and Quantifiers in an Ordered Context
#@ Norman May;Sven Helmer;Guido Moerkotte
#t 2004
#c 17
#% 32878
#% 86947
#% 116043
#% 116090
#% 169846
#% 201927
#% 220425
#% 287005
#% 289370
#% 321631
#% 335725
#% 413563
#% 424320
#% 458539
#% 458550
#% 461897
#% 480091
#% 481273
#% 482103
#% 482653
#% 487267
#% 562135
#% 562456
#% 564426
#% 570876
#% 993939
#% 1015283
#! We present algebraic equivalences that allow to unnestnested algebraic expressions for order-preserving algebraicoperators. We illustrate how these equivalences canbe applied successfully to unnest nested queries given inthe XQuery language. Measurements illustrate the performancegains possible by unnesting.

#index 745530
#* Hiding Data Accesses in Steganographic File System
#@ Xuan Zhou;HweeHwa Pang;Kian-Lee Tan
#t 2004
#c 17
#% 150212
#% 210387
#% 232721
#% 264163
#% 330615
#% 337046
#% 524838
#! To support ubiquitous computing, the underlying datahave to be persistent and available anywhere-anytime. Thedata thus have to migrate from devices local to individualcomputers, to shared storage volumes that are accessibleover open network. This potentially exposes the datato heightened security risks. We propose two mechanisms,in the context of a steganographic file system, to mitigatethe risk of attacks initiated through analyzing data accessesfrom user applications. The first mechanism is intended tocounter attempts to locate data through updates in betweensnapshots - in short, update analysis. The second mechanismprevents traffic analysis - identifying data from I/Otraffic patterns. We have implemented the first mechanismon Linux and conducted experiments to demonstrate its effectivenessand practicality. Simulation results on the secondmechanism also show its potential for real world applications.

#index 745531
#* Proving Ownership over Categorical Data
#@ Radu Sion
#t 2004
#c 17
#% 191721
#% 263455
#% 389077
#% 539761
#% 576109
#% 583804
#% 654449
#% 660348
#% 993944
#% 1395188
#! This paper introduces a novel method of rightsprotection for categorical data through watermarking.We discover new watermark embedding channelsfor relational data with categorical types. Wedesign novel watermark encoding algorithms andanalyze important theoretical bounds including markvulnerability. While fully preserving data qualityrequirements, our solution survives important attacks,such as subset selection and random alterations. Markdetection is fully "blind" in that it doesn't require theoriginal data, an important characteristic especiallyin the case of massive data. We propose variousimprovements and alternative encoding methods. Weperform validation experiments by watermarking theoutsourced Wal-Mart sales data available at ourinstitute. We prove (experimentally and by analysis)our solution to be extremely resilient to both alterationand data loss attacks, for example tolerating up to 80%data loss with a watermark alteration of only 25%.

#index 745532
#* Authenticating Query Results in Edge Computing
#@ HweeHwa Pang;Kian-Lee Tan
#t 2004
#c 17
#% 64430
#% 104987
#% 317987
#% 321521
#% 342345
#% 528442
#% 558598
#% 657774
#% 730195
#% 978647
#! Edge computing pushes application logic and the underlyingdata to the edge of the network, with the aim of improvingavailability and scalability. As the edge servers arenot necessarily secure, there must be provisions for validatingtheir outputs. This paper proposes a mechanism thatcreates a verification object (VO) for checking the integrityof each query result produced by an edge server - that valuesin the result tuples are not tampered with, and that nospurious tuples are introduced. The primary advantages ofour proposed mechanism are that the VO is independent ofthe database size, and that relational operations can stillbe fulfilled by the edge servers. These advantages reducetransmission load and processing at the clients. We alsoshow how insert and delete transactions can be supported.

#index 745533
#* Continuously Maintaining Quantile Summaries of the Most Recent N Elements over a Data Stream
#@ Xuemin Lin;Hongjun Lu;Jian Xu;Jeffrey Xu Yu
#t 2004
#c 17
#% 248820
#% 273907
#% 333926
#% 333931
#% 338425
#% 379445
#% 393844
#% 397354
#% 482104
#% 576105
#% 594012
#% 654444
#% 654476
#% 654488
#% 993958
#% 993960
#% 993961
#% 993969
#! Statistics over the most recently observed data elementsare often required in applications involving data streams,such as intrusion detection in network monitoring, stockprice prediction in financial markets, web log mining foraccess prediction, and user click stream mining for personalization.Among various statistics, computing quantilesummary is probably most challenging because of its complexity.In this paper, we study the problem of continuouslymaintaining quantile summary of the most recentlyobserved N elements over a stream so that quantile queriescan be answered with a guaranteed precision of 驴N.Wedeveloped a space efficient algorithm for pre-defined Nthat requires only one scan of the input data stream andO({{\log ( \in ^2 N)} \over\in } + {1 \over { \in ^2 }}) space in the worst cases.We alsodeveloped an algorithm that maintains quantile summaries formost recent N elements so that quantile queries on any mostrecent n elements (n 驴 N) can be answered with a guaranteedprecision of 驴n.The worst case space requirement forthis algorithm is only O({{\log ^2 ( \in N)} \over { \in ^2 }}).Our performance studyindicated that not only the actual quantile estimation erroris far below the guaranteed precision but the space requirementis also much less than the given theoretical bound.

#index 745534
#* Load Shedding for Aggregation Queries over Data Streams
#@ Brian Babcock;Mayur Datar;Rajeev Motwani
#t 2004
#c 17
#% 160390
#% 188026
#% 227883
#% 273908
#% 273909
#% 273910
#% 300179
#% 333955
#% 378388
#% 397353
#% 577220
#% 654444
#% 654462
#% 745534
#% 993949
#% 1015280
#! Systems for processing continuous monitoring queriesover data streams must be adaptive because data streamsare often bursty and data characteristics may vary overtime. In this paper, we focus on one particular type ofadaptivity: the ability to gracefully degrade performancevia "load shedding" (dropping unprocessed tuples to reducesystem load) when the demands placed on the systemcannot be met in full given available resources. Focusingon aggregation queries, we present algorithms that determineat what points in a query plan should load sheddingbe performed and what amount of load should be shed ateach point in order to minimize the degree of inaccuracyintroduced into query answers. We report the results of experimentsthat validate our analytical conclusions.

#index 745535
#* Probe, Cluster, and Discover: Focused Extraction of QA-Pagelets from the Deep Web
#@ James Caverlee;Ling Liu;David Buttler
#t 2004
#c 17
#% 248221
#% 249110
#% 255137
#% 262045
#% 262061
#% 267454
#% 273926
#% 281209
#% 281214
#% 283050
#% 290830
#% 296738
#% 310567
#% 321635
#% 348180
#% 447946
#% 480824
#% 591588
#% 654469
#% 729437
#% 842709
#% 993964
#! In this paper, we introduce the concept of a QA-Pageletto refer to the content region in a dynamic page that containsquery matches. We present THOR, a scalable andefficient mining system for discovering and extracting QA-Pageletsfrom the Deep Web. A unique feature of THOR isits two-phase extraction framework. In the first phase, pagesfrom a deep web site are grouped into distinct clusters ofstructurally-similar pages. In the second phase, pages fromeach page cluster are examined through a subtree filteringalgorithm that exploits the structural and content similarityat subtree level to identify the QA-Pagelets.

#index 745536
#* MTCache: Transparent Mid-Tier Database Caching in SQL Server
#@ Per-Åke Larson;Jonathan Goldstein;Jingren Zhou
#t 2004
#c 17
#% 210176
#% 281184
#% 300177
#% 333965
#% 333995
#% 397400
#% 397401
#% 397402
#% 397403
#% 480293
#% 480495
#% 480818
#% 481916
#% 566137
#% 571216
#% 654503
#% 654504
#% 1015314
#! Many applications today run in a multi-tier environmentwith browser-based clients, mid-tier (application)servers and a backend database server.Mid-tier databasecaching attempts to improve system throughput and scalabilityby offloading part of the database workload to intermediatedatabase servers that partially replicate datafrom the backend server.The fact that some queries areoffloaded to an intermediate server should be completelytransparent to applications - one of the key distinctionsbetween caching and replication.MTCache is a prototypemid-tier database caching solution for SQL Server thatachieves this transparency.It builds on SQL Server's supportfor materialized views, distributed queries and replication.This paper describes MTCache and reportsexperimental results on the TPC-W benchmark.The experimentsshow that a significant part of the query workloadcan be offloaded to cache servers, resulting in greatlyimproved scale-out on the read-dominated workloads ofthe benchmark.Replication overhead was small with anaverage replication delay of less than two seconds.

#index 745537
#* Detection and Correction of Conflicting Source Updates for View Maintenance
#@ Songting Chen;Jun Chen;Xin Zhang;Elke A. Rundensteiner
#t 2004
#c 17
#% 9241
#% 201898
#% 201928
#% 201929
#% 201930
#% 210210
#% 227947
#% 300141
#% 334043
#% 443527
#% 480134
#% 480645
#% 487819
#% 487993
#% 577359
#% 654525
#% 1015303
#! Data integration over multiple heterogeneous datasources has become increasingly important for modern applications. The integrated data is usually stored in materialized views for high availability and better performance. Such views must be maintained after the datasources change. In a loosely-coupled and dynamic environment, such as the Data Grid, the sources may autonomously change not only their data but also their schema, query capabilities or semantics, which may consequently cause theon-going view maintenance fail. In this paper, first, we analyze the maintenance errors and classify them into different classes of dependencies. We then propose severaldependency detection and correction algorithms to handle these new classes of concurrency. Our techniques arenot tied to specific maintenance algorithms nor to a particular data model. To our knowledge, this is the first completesolution to the view maintenance concurrency problems for both data and schema changes. We have implemented the proposed solutions and experimentally evaluated the impact of anomalies on maintenance performanceand trade-offs between different dependency detection algorithms.

#index 745538
#* EShopMonitor: A Web Content Monitoring Tool
#@ Neeraj Agrawal;Rema Ananthanarayanan;Rahul Gupta;Sachindra Joshi;Raghu Krishnapuram;Sumit Negi
#t 2004
#c 17
#% 333997
#% 480648
#% 632051
#% 729974
#! Data presented on commerce sites runs into thousandsof pages, and is typically delivered from multiple back-endsources. This makes it difficult to identify incorrect, anomalous,or interesting data such as $9.99 air fares, missinglinks, drastic changes in prices and addition of new productsor promotions. In this paper, we describe a systemthat monitors Websites automatically and generates varioustypes of reports so that the content of the site can be monitoredand the quality maintained. The solution designedand implemented by us consists of a site crawler that crawlsdynamic pages, an information miner that learns to extractuseful information from the pages based on examples providedby the user, and a reporter that can be configured bythe user to answer specific queries. The tool can also beused for identifying price trends and new products or promotionsat competitor sites. A pilot run of the tool has beensuccessfully completed at the ibm.com site.

#index 745539
#* Extending XML Database to Support Open XML
#@ Jinyu Wang;Kongyi Zhou;K. Karun;Mark Scardina
#t 2004
#c 17
#! XML is a widely accepted standard for exchangingbusiness data. To optimize the management of XMLand help companies build up their business partnernetworks over the Internet, database servers haveintroduced new XML storage and query features.However, each enterprise defines its own dataelements in XML and modifies the XML documents tohandle the evolving business needs. This makes XMLdata conform to heterogeneous schemas or schemasthat evolve over time, which is not suitable for XMLdatabase storage. This paper provides an overview ofthe current XML database strategies and presents astreaming metadata-processing approach, enablingdatabases to handle multiple XML formats seamlessly.

#index 745540
#* Publish/Subscribe in NonStop SQL: Transactional Streams in a Relational Context
#@ Mike Hanlon;Johannes Klein;Robbert Van der Linden;Hansjörg Zeller
#t 2004
#c 17
#% 340325
#% 654510
#! Relational queries on continuous streams of data arethe subject of many recent database research projects. In1998 a small group of people started a similar projectwith the goal to transform our product, NonStop SQL/MX,into an active RDBMS. This project tried to integratefunctionality of transactional queuing systems with relationaltables and with SQL, using simple extensions to theSQL syntax and guaranteeing clearly defined query andtransactional semantics. The result is the first commerciallyavailable RDBMS that incorporates streams. Alldata flowing through the system is contained in relationaltables and is protected by ACID transactions. Insert andupdate operations on any NonStop SQL table can be consideredpublishing of data and can therefore be transparentto the (legacy) applications performing them. Unliketriggers, the publish operation does not increase the pathlength of the application and it allows the subscriber toexecute in a separate transaction. Subscribers, using anextended SQL syntax, see a continuous stream of data,consisting of all rows originally in the table plus all rowsthat are inserted or updated thereafter. The system scalesby using partitioned tables and therefore partitionedstreams.

#index 745541
#* BEA Liquid Data for WebLogic: XML-Based Enterprise Information Integration
#@ Michael J. Carey
#t 2004
#c 17
#! This presentation provides a technical overview ofBEA Liquid Data for WebLogic, a relatively newproduct from BEA Systems that provides enterpriseinformation integration capabilities to enterpriseapplications that are built and deployed using the BEAWebLogic Platform.Liquid Data takes an XML-centricapproach to tackling the long-standingproblem of integrating data from disparate datasources and making that information easily accessibleto applications.In particular, Liquid Data uses theforthcoming XQuery language standard as the basisfor defining integrated views of enterprise data andquerying over those views.We provide a briefoverview of the Liquid Data product architecture andthen discuss some of the query processing technologythat lies at the heart of the product.

#index 745542
#* Information Lifecycle Management: The EMC Perspective
#@ David Reiner;Gil Press;Mike Lenaghan;David Barta;Rich Urmston
#t 2004
#c 17
#! Information is a strategic component of modernbusiness, and its effective management has become acritical business challenge. Electronic information hasnot only been growing in volume at unprecedentedrates, its value to business has never been greater.Around-the-clock operations, electronic commerce,corporate governance rules and legally-mandatedretention laws have all added to the pressure for betterinformation management. Information LifecycleManagement (ILM) is a business-centric strategy forproactive management of information throughout itslife, from its creation and use to its ultimate disposal.EMC's ILM initiative is enhancing our customers'information management capabilities through dataclassification, centralized management, automation,product integration, and policy-based management.Research issues related to ILM include informationclassification and optimization of policy-basedinformation management.

#index 800209
#* Proceedings of the 21st International Conference on Data Engineering
#@ 
#t 2005
#c 17

#index 800490
#* Demo Program Committee
#@ 
#t 2005
#c 17

#index 800491
#* "One Size Fits All": An Idea Whose Time Has Come and Gone
#@ Michael Stonebraker;Ugur Cetintemel
#t 2005
#c 17
#% 77980
#% 228299
#% 287647
#% 287664
#% 287751
#% 297915
#% 339217
#% 411560
#% 654482
#% 654508
#% 723279
#% 726621
#% 755211
#% 755212
#% 800583
#% 993949
#% 1016169
#! The last 25 years of commercial DBMS development can be summed up in a single phrase: "One size fits all". This phrase refers to the fact that the traditional DBMS architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the data-warehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into products.

#index 800492
#* Top Five Data Challenges for the Next Decade
#@ Pat Selinger
#t 2005
#c 17

#index 800493
#* IC Tag Based Traceability: System and Solutions
#@ Yoji Taniguchi;Nobutoshi Sagawa
#t 2005
#c 17
#! An increasing number of companies want to improve product traceability for several reasons: to meet stricter government regulations about food and medical safety, to cope with ever-stronger consumer demands to know exactly what they are buying, and to improve and protect the company's brand value through more transparent business operations. Two aspects of traceability are technically important: (1) techniques for tracing the events associated with the goods a company handles at all necessary points of the business operation, possibly through the use of IC tags and tag readers; and (2) ways to store, manage, and use the collected logs of events either to cope with problems or to improve business processes. In this paper, we first review currently available traceability systems by considering examples from real-world situations. After that, we discuss the likely directions and possibilities of next-generation traceability systems.

#index 800494
#* Effective Computation of Biased Quantiles over Data Streams
#@ Graham Cormode;Flip Korn;S. Muthukrishnan;Divesh Srivastava
#t 2005
#c 17
#% 152585
#% 248820
#% 273907
#% 333931
#% 378388
#% 397426
#% 453493
#% 453512
#% 654497
#% 745533
#% 765404
#% 801696
#% 816392
#% 993969
#% 1015256
#! Skew is prevalent in many data sources such as IP traffic streams. To continually summarize the distribution of such data, a high-biased set of quantiles (e.g., 50th, 90th and 99th percentiles) with finer error guarantees at higher ranks (e.g., errors of 5, 1 and 0.1 percent, respectively) is more useful than uniformly distributed quantiles (e.g., 25th, 50th and 75th percentiles) with uniform error guarantees. In this paper, we address the following two problems. First, can we compute quantiles with finer error guarantees for the higher ranks of the data distribution effectively, using less space and computation time than computing all quantiles uniformly at the finest error? Second, if specific quantiles and their error bounds are requested a priori, can the necessary space usage and computation time be reduced? We answer both questions in the affirmative by formalizing them as the "high-biased" and the "targeted" quantiles problems, respectively, and presenting algorithms with provable guarantees, that perform significantly better than previously known solutions for these problems. We implemented our algorithms in the Gigascope data stream management system, and evaluated alternate approaches for maintaining the relevant summary structures. Our experimental results on real and synthetic IP data streams complement our theoretical analyses, and highlight the importance of lightweight, non-blocking implementations when maintaining summary structures over high-speed data streams.

#index 800495
#* Range-Efficient Computation of F" over Massive Data Streams
#@ A. Pavan;Srikanta Tirthapura
#t 2005
#c 17
#% 2833
#% 278835
#% 336610
#% 347224
#% 378388
#% 379443
#% 414993
#% 419377
#% 480805
#% 481749
#% 519953
#% 723898
#% 745442
#% 749449
#% 783741
#% 813797
#% 993959
#! Efficient one-pass computation of F驴, the number of distinct elements in a data stream, is a fundamental problem arising in various contexts in databases and networking. We consider the problem of efficiently estimating F驴 of a data stream where each element of the stream is an interval of integers. We present a randomized algorithm which gives an (\varepsilon ,\delta ) approximation of F驴, with the following time complexity (n is the size of the universe of the items): (1) The amortized processing time per interval is 0(\log \frac{1}{\delta }Log\frac{n}{\varepsilon }). (2) The time to answer a query for F驴 is 0(\log {1 \mathord{\left/ {\vphantom {1 {\delta )}}} \right. \kern-\nulldelimiterspace} {\delta )}}. The workspace used is 0(\frac{1}{{\varepsilon ^2 }}\log \frac{1}{\delta }\log n) bits. Our algorithm improves upon a previous algorithm by Bar-Yossef, Kumar and Sivakumar [5], which requires 0(\frac{1}{{\varepsilon ^5 }}\log \frac{1}{\delta }\log ^5 n) processing time per item. Our algorithm can be used to compute the max-dominance norm of a stream of multiple signals, and significantly improves upon the current best bounds due to Cormode and Muthukrishnan [11]. This also provides efficient and novel solutions for data aggregation problems in sensor networks studied by Nath and Gibbons [22] and Considine et. al. [8].

#index 800496
#* A Unified Framework for Monitoring Data Streams in Real Time
#@ Ahmet Bulut;Ambuj K. Singh
#t 2005
#c 17
#% 86950
#% 172949
#% 201876
#% 317468
#% 333941
#% 397381
#% 464851
#% 632090
#% 729943
#% 745513
#% 765403
#% 993949
#% 993961
#% 1015301
#% 1015305
#% 1016157
#! Online monitoring of data streams poses a challenge in many data-centric applications, such as telecommunications networks, traffic management, trend-related analysis, web-click streams, intrusion detection, and sensor networks. Mining techniques employed in these applications have to beefficient in terms of space usage and per-item processing time while providing a high quality of answers to (1) aggregate monitoring queries, such as finding surprising levels of a data stream, detecting bursts, and to (2) similarity queries, such as detecting correlations and finding interesting patterns. The most important aspect of these tasks is their need for flexible query lengths, i.e., it is difficult to set the appropriate lengths a priori. For example, bursts of events can occur at variable temporal modalities from hours to days to weeks. Correlated trends can occur at various temporal scales. The system has to discover "interesting" behavior online and monitor over flexible window sizes. In this paper, we propose a multi-resolution indexing scheme, which handles variable length queries efficiently. We demonstrate the effectiveness of our framework over existing techniques through an extensive set of experiments.

#index 800497
#* Corpus-Based Schema Matching
#@ Jayant Madhavan;Philip A. Bernstein;AnHai Doan;Alon Halevy
#t 2005
#c 17
#% 85086
#% 116303
#% 246831
#% 333990
#% 348187
#% 378409
#% 480134
#% 488766
#% 529190
#% 572314
#% 576214
#% 587740
#% 654458
#% 654459
#% 660001
#% 762653
#% 765409
#% 765433
#% 840583
#% 993982
#% 1016128
#% 1016160
#% 1016163
#% 1272397
#% 1279488
#! Schema Matching is the problem of identifying corresponding elements in different schemas. Discovering these correspondences or matches is inherently difficult to automate. Past solutions have proposed a principled combination of multiple algorithms. However, these solutions sometimes perform rather poorly due to the lack ofsufficient evidence in the schemas being matched. In this paper we show how a corpus of schemas and mappings can be used to augment the evidence about the schemas being matched, so they can be matched better. Such a corpus typically contains multiple schemas that model similar concepts and hence enables us to learn variations in the elements and their properties. We exploit such a corpus in two ways. First, we increase the evidence about each element being matched by including evidence from similar elements in the corpus. Second, we learn statistics about elements and their relationships and use them to infer constraints that we use to prune candidate mappings. We also describe how to use known mappings to learn the importance of domain and generic constraints. We present experimental results that demonstrate corpus-based matching outperforms direct matching (without the benefit of a corpus) in multiple domains.

#index 800498
#* Schema Matching Using Duplicates
#@ Alexander Bilke;Felix Naumann
#t 2005
#c 17
#% 3938
#% 25998
#% 235411
#% 248801
#% 333990
#% 350103
#% 420072
#% 463445
#% 480645
#% 551850
#% 572314
#% 577309
#% 659941
#% 659991
#% 660001
#% 726627
#% 729913
#% 765433
#% 993980
#% 993981
#% 993982
#% 1275347
#! Most data integration applications require a matching between the schemas of the respective data sets. We show how the existence of duplicates within these data sets can be exploited to automatically identify matching attributes. We describe an algorithm that first discovers duplicates among data sets with unaligned schemas and then uses these duplicates to perform schema matching between schemas with opaque column names. Discovering duplicates among data sets with unaligned schemas is more difficult than in the usual setting, because it is not clear which fields in one object should be compared with which fields in the other. We have developed a new algorithm that efficiently finds the most likely duplicates in such a setting. Now, our schema matching algorithm is able to identify corresponding attributes by comparing data values within those duplicate records. An experimental study on real-world data shows the effectiveness of this approach.

#index 800499
#* Representing and Querying Data Transformations
#@ Yannis Velegrakis;Renee J. Miller;John Mylopoulos
#t 2005
#c 17
#% 198465
#% 227981
#% 283052
#% 328429
#% 330770
#% 378401
#% 378409
#% 462212
#% 464891
#% 465057
#% 480134
#% 481269
#% 481944
#% 533911
#% 566114
#% 632040
#% 654457
#% 765432
#% 765446
#% 801676
#% 993981
#% 1015302
#% 1015303
#% 1016204
#! Modern information systems often store data that has been transformed and integrated from a variety of sources. This integration may obscure the original source semantics of data items. For many tasks, it is important to be able to determine not only where data items originated, but also why they appear in the integration as they do and through what transformation they were derived. This problem is known as data provenance. In this work, we consider data provenance at the schema and mapping level. In particular, we consider how to answer questions such as "what schema elements in the source(s) contributed to this value", or "through what transformations or mappings was this value derived?" Towards this end, we elevate schemas and mappings to first-class citizens that are stored in a repository and are associated with the actual data values. An extended query language, called MXQL, is also developed that allows meta-data to be queried as regular data and we describe its implementation. scenario.

#index 800500
#* Bypass Caching: Making Scientific Databases Good Network Citizens
#@ Tanu Malik;Randal Burns;Amitabh Chaudhary
#t 2005
#c 17
#% 6798
#% 29590
#% 113704
#% 144932
#% 152943
#% 172930
#% 232777
#% 255027
#% 261358
#% 282923
#% 348037
#% 378384
#% 397403
#% 397726
#% 404765
#% 413569
#% 463728
#% 467363
#% 480149
#% 480260
#% 480495
#% 481916
#% 498256
#% 566126
#% 599549
#% 610277
#% 635920
#% 830700
#% 978378
#% 1834790
#% 1848711
#! Scientific database federations are geographically distributed and network bound. Thus, they could benefit from proxy caching. However, existing caching techniques are not suitable for their workloads, which compare and join large data sets. Existing techniques reduce parallelism by conducting distributed queries in a single cache and lose the data reduction benefits of performing selections at each database. We develop the bypass-yield formulation of caching, which reduces network traffic in wide-area database federations, while preserving parallelism and data reduction. Bypass-yield caching is altruistic; caches minimize the overall network traffic generated by the federation, rather than focusing on local performance. We present an adaptive, workload-driven algorithm for managing a bypass-yield cache. We also develop on-line algorithms that make no assumptions about workload: a k-competitive deterministic algorithm and a randomized algorithm with minimal space complexity. We verify the efficacy of bypass-yield caching by running workload traces collected from the Sloan Digital Sky Survey through a prototype implementation.

#index 800501
#* Asymmetric Batch Incremental View Maintenance
#@ Hao He;Junyi Xie;Jun Yang;Hai Yu
#t 2005
#c 17
#% 59350
#% 201928
#% 201929
#% 210210
#% 227869
#% 227945
#% 227947
#% 279164
#% 300141
#% 300179
#% 333982
#% 340301
#% 397355
#% 413556
#% 443298
#% 479621
#% 480141
#% 480623
#% 482098
#% 576214
#% 993998
#% 1016159
#! Incremental view maintenance has found a growing number of applications recently, including data warehousing, continuous query processing, publish/subscribe systems, etc. Batch processing of base table modifications, when applicable, can be much more efficient than processing individual modifications one at a time. In this paper, we tackle the problem of finding the most efficient batch incremental maintenance strategy under a refresh response time constraint; that is, at any point in time, the system, upon request, must be able to bring the view up to date within a specified amount of time. The traditional approach is to process all batched modifications relevant to the view whenever the constraint is violated. However, we observe that there often exists natural asymmetry among different components of the maintenance cost; for example,modifications on one base table might be cheaper to process than those on another base table because of some index. We exploit such asymmetries using an unconventional strategy that selectively processes modifications on some base tables while keeping batching others. We present a series of analytical results leading to the development of practical algorithms that approximate an "oracle algorithm" with perfect knowledge of the future. With experiments on a TPC-R database, we demonstrate that our strategy offers substantial performance gains over traditional deferred view maintenance techniques.

#index 800502
#* Adaptive Caching for Continuous Queries
#@ Shivnath Babu;Kamesh Munagala;Jennifer Widom;Rajeev Motwani
#t 2005
#c 17
#% 98469
#% 210206
#% 210208
#% 217812
#% 248793
#% 248795
#% 286991
#% 300167
#% 322884
#% 340301
#% 397371
#% 480141
#% 480803
#% 715955
#% 765435
#% 765437
#% 765456
#% 765500
#% 993949
#% 1015278
#% 1015296
#% 1016208
#! We address the problem of executing continuous multiway join queries in unpredictable and volatile environments. Our query class captures windowed join queries in data stream systems as well as conventional maintenance of materialized join views. Our adaptive approach handles streams of updates whose rates and data characteristics may change over time, as well as changes in system conditions such as memory availability. In this paper we focus specifically on the problem of adaptive placement and removal of caches to optimize join performance. Our approach automatically considers conventional tree-shaped join plans with materialized subresults at every intermediate node, subresult-free MJoins, and the entire spectrum between them. We provide algorithms for selecting caches, monitoring their cost and benefits in current conditions, allocating memory to caches, and adapting as conditions change. All of our algorithms are implemented in the STREAM prototype Data Stream Management System and a thorough experimental evaluation is included.

#index 800503
#* Snapshot Queries: Towards Data-Centric Sensor Networks
#@ Yannis Kotidis
#t 2005
#c 17
#% 281556
#% 309430
#% 333863
#% 427022
#% 480332
#% 608160
#% 654482
#% 654488
#% 723903
#% 745442
#% 765445
#% 788219
#% 805466
#% 1016178
#! In this paper we introduce the idea of snapshot queries for energy efficient data acquisition in sensor networks. Network nodes generate models of their surrounding environment that are used for electing, using a localized algorithm, a small set of representative nodes in the network. These representative nodes constitute a network snapshot and can be used to provide quick approximate answers to user queries while reducing substantially the energy consumption in the network. We present a detailed experimental study of our framework and algorithms, varying multiple parameters like the available memory of the sensor nodes, their transmission range, the network message loss etc. Depending on the configuration, snapshot queries provide a reduction of up to 90% in the number of nodes that need to participate in a user query.

#index 800504
#* Data Triage: An Adaptive Architecture for Load Shedding in TelegraphCQ
#@ Frederick Reiss;Joseph M. Hellerstein
#t 2005
#c 17
#% 149237
#% 188026
#! Many of the data sources used in stream query processing are known to exhibit bursty behavior. Data in a burst often has different characteristics than steady-state data, and therefore may be of particular interest. In this paper, we describe the Data Triage architecture that we are adding to TelegraphCQ to provide low latency results with good accuracy under such bursts.

#index 800505
#* Exploiting Correlated Attributes in Acquisitional Query Processing
#@ Amol Deshpande;Carlos Guestrin;Wei Hong;Samuel Madden
#t 2005
#c 17
#% 58375
#% 172900
#% 249985
#% 287461
#% 297915
#% 333953
#% 336865
#% 388024
#% 479636
#% 479786
#% 479938
#% 480955
#% 578764
#% 654482
#% 765435
#% 1016178
#% 1700123
#! Sensor networks and other distributed information systems (such as the Web) must frequently access data that has a high per-attribute acquisition cost, in terms of energy, latency, or computational resources. When executing queries that contain several predicates over such expensive attributes, we observe that it can be beneficial to use correlations to automatically introduce low-cost attributes whose observation will allow the query processor to better estimate the selectivity of these expensive predicates. In particular, we show how to build conditional plans that branch into one or more sub-plans, each with a different ordering for the expensive query predicates, based on the runtime observation of low-cost attributes. We frame the problem of constructing the optimal conditional plan for a given user query and set of candidate low-cost attributes as an optimization problem. We describe an exponential time algorithm for finding such optimal plans, and describe a polynomial-time heuristic for identifying conditional plans that perform well in practice. We also show how to compactly model conditional probability distributions needed to identify correlations and build these plans. We evaluate our algorithms against several real-world sensor-network data sets, showing several-times performance increases for a variety of queries versus traditional optimization techniques.

#index 800506
#* Energy-Efficient Data Organization and Query Processing in Sensor Networks
#@ Ramakrishna Gummadi;Xin Li;Ramesh Govindan;Cyrus Shahabi;Wei Hong
#t 2005
#c 17
#% 554726
#% 576977
#% 654482
#% 731091
#! Recent sensor networks research has produced a class of data storage and query processing techniques called Data-Centric Storage that leverages locality-preserving distributed indexes to efficiently answer multi-dimensional range and range-aggregate queries. These distributed indexes offer a rich design space of a) logical decompositions of sensor relation schema into indexes, as well as b) physical mappings of these indexes onto sensors. In this paper, we explore this space for energy-efficient data organizations (logical and physical mappings of tuples and attributes to sensor nodes) and devise purely local query optimization techniques for processing queries that span such decomposed relations.

#index 800507
#* AutoLag: Automatic Discovery of Lag Correlations in Stream Data
#@ Yasushi Sakurai;Spiros Papadimitriou;Christos Faloutsos
#t 2005
#c 17
#% 578388
#% 993961
#% 1015301

#index 800508
#* Adaptive Processing of Top-k Queries in XML
#@ Amelie Marian;Sihem Amer-Yahia;Nick Koudas;Divesh Srivastava
#t 2005
#c 17
#% 213981
#% 227894
#% 248795
#% 300167
#% 333845
#% 333854
#% 333951
#% 397378
#% 399762
#% 406493
#% 458828
#% 458829
#% 458854
#% 458861
#% 480819
#% 726621
#% 742047
#% 763882
#% 765408
#% 765466
#% 1015317
#! The ability to compute top-k matches to XML queries is gaining importance due to the increasing number of large XML repositories. The efficiency of top-k query evaluation relies on using scores to prune irrelevant answers as early as possible in the evaluation process. In this context, evaluating the same query plan for all answers might be too rigid because, at any time in the evaluation, answers have gone through the same number and sequence of operations, which limits the speed at which scores grow. Therefore, adaptive query processing that permits different plans for different partial matches and maximizes the best scores is more appropriate. In this paper, we propose an architecture and adaptive algorithms for efficiently computing top-k matches to XML queries. Our techniques can be used to evaluate both exact and approximate matches where approximation is defined by relaxing XPath axes. In order to compute the scores of query answers, we extend the traditional tf*idf measure to account for document structure. We conduct extensive experiments on a variety of benchmark data and queries, and demonstrate the usefulness of the adaptive approach for computing top-k queries in XML.

#index 800509
#* Progressive Distributed Top-k Retrieval in Peer-to-Peer Networks
#@ Wolf-Tilo Balke;Wolfgang Nejdl;Wolf Siberski;Uwe Thaden
#t 2005
#c 17
#% 333854
#% 348182
#% 458873
#% 480330
#% 509871
#% 577320
#% 577357
#% 577359
#% 610851
#% 659993
#% 723447
#% 1386265
#% 1394450
#! Query processing in traditional information management systems has moved from an exact match model to more flexible paradigms allowing cooperative retrieval by aggregating the database objectsý degree of match for each different query predicate and returning the best matching objects only. In peer-to-peer systems such strategies are even more important, given the potentially large number of peers, which may contribute to the results. Yet current peer-to-peer research has barely started to investigate such approaches. In this paper we will discuss the benefits of best match/top-k queries in the context of distributed peer-to-peer information infrastructures and show how to extend the limited query processing in current peer-to-peer networks by allowing the distributed processing of top-k queries, while maintaining a minimum of data traffic. Relying on a super-peer backbone organized in the HyperCuP topology we will show how to use local indexes for optimizing the necessary query routing and how to process intermediate results in inner network nodes at the earliest possible point in time cutting down the necessary data traffic within the network. Our algorithm is based on dynamically collected query statistics only, no continuous index update processes are necessary, allowing it to scale easily to large numbers of peers, as well as dynamic additions/deletions of peers. We will show our approach to always deliver correct result sets and to be optimal in terms of necessary object accesses and data traffic. Finally, we present simulation results for both static and dynamic network environments.

#index 800510
#* Reverse Nearest Neighbors in Large Graphs
#@ Man Lung Yiu;Dimitris Papadias;Nikos Mamoulis;Yufei Tao
#t 2005
#c 17
#% 300163
#% 480661
#% 1016191
#! A reverse nearest neighbor query returns the data objects that have a query point as their nearest neighbor. Although such queries have been studied quite extensively in Euclidean spaces, there is no previous work in the context of large graphs. In this paper, we propose algorithms and optimization techniques for RNN queries by utilizing some characteristics of networks.

#index 800511
#* Optimizing Access Cost for Top-k Queries over Web Sources: A Unified Cost-Based Approach
#@ Seung-won Hwang;Kevin Chen-Chuan Chang
#t 2005
#c 17
#% 213981
#% 333854
#% 397378
#% 411554
#% 480330
#% 509871
#% 591565
#% 659255

#index 800512
#* Efficient Processing of Skyline Queries with Partially-Ordered Domains
#@ Chee-Yong Chan;Pin-Kwang Eng;Kian-Lee Tan
#t 2005
#c 17
#% 465167
#% 480671
#% 654480
#% 993954

#index 800513
#* A Framework for High-Accuracy Privacy-Preserving Mining
#@ Shipra Agrawal;Jayant R. Haritsa
#t 2005
#c 17
#% 152934
#% 300184
#% 333876
#% 481290
#% 576111
#% 577233
#% 727904
#% 993988
#! To preserve client privacy in the data mining process, a variety of techniques based on random perturbation of individual data records have been proposed recently. In this paper, we present FRAPP, a generalized matrix-theoretic framework of random perturbation, which facilitates a systematic approach to the design of perturbation mechanisms for privacy-preserving mining. Specifically, FRAPP is used to demonstrate that (a) the prior techniques differ only in their choices for the perturbation matrix elements, and (b) a symmetric perturbation matrix with minimal condition number can be identified, maximizing the accuracy even under strict privacy guarantees. We also propose a novel perturbation mechanism wherein the matrix elements are themselves characterized as random variables, and demonstrate that this feature provides significant improvements in privacy at only a marginal cost in accuracy. The quantitative utility of FRAPP, which applies to random-perturbation-based privacy-preserving mining in general, is evaluated specifically with regard to frequent-itemset mining on a variety of real datasets. Our experimental results indicate that, for a given privacy requirement, substantially lower errors are incurred, with respect to both itemset identity and itemset support, as compared to the prior techniques.

#index 800514
#* Top-Down Specialization for Information and Privacy Preservation
#@ Benjamin C.  M. Fung;Ke Wang;Philip S. Yu
#t 2005
#c 17
#% 136350
#% 300184
#% 443463
#% 488324
#% 577239
#% 785363
#! Releasing person-specific data in its most specific state poses a threat to individual privacy. This paper presents a practical and efficient algorithm for determining a generalized version of data that masks sensitive information and remains useful for modelling classification. The generalization of data is implemented by specializing or detailing the level of information in a top-down manner until a minimum privacy requirement is violated. This top-down specialization is natural and efficient for handling both categorical and continuous attributes. Our approach exploits the fact that data usually contains redundant structures forclassification. While generalization may eliminate some structures, other structures emerge to help. Our results show that quality of classification can be preserved even for highly restrictive privacy requirements. This work has great applicability to both public and private sectors that share information for mutual benefits and productivity.

#index 800515
#* Data Privacy through Optimal k-Anonymization
#@ Roberto J. Bayardo;Rakesh Agrawal
#t 2005
#c 17
#% 248030
#% 300184
#% 443463
#% 488324
#% 576761
#% 576762
#% 577239
#% 631970
#% 801690
#% 1272179
#! Data de-identification reconciles the demand for release of data for research purposes and the demand for privacy from individuals. This paper proposes and evaluates an optimization algorithm for the powerful de-identification procedure known as k-anonymization. A k-anonymized dataset has the property that each record is indistinguishable from at least k - 1 others. Even simple restrictions of optimized k-anonymity are NP-hard, leading to significant computational challenges. We present a new approach to exploring the space of possible anonymizations that tames the combinatorics of the problem, and develop data-management strategies to reduce reliance on expensive operations such as sorting. Through experiments on real census data, we show the resulting algorithm can find optimalk-anonymizations under two representative cost measures and a wide range of k. We also show that the algorithm can produce good anonymizations in circumstances where the input data or input parameters preclude finding an optimal solution in reasonable time. Finally, we use the algorithm to explore the effects of different coding approaches and problem variations on anonymization quality and performance. To our knowledge, this is the first result demonstrating optimal k-anonymization of a nontrivial dataset under a general model of the problem.

#index 800516
#* A Comparative Evaluation of Transparent Scaling Techniques for Dynamic Content Servers
#@ C. Amza;A. L. Cox;W. Zwaenepoel
#t 2005
#c 17
#% 9241
#% 210179
#% 248825
#% 262135
#% 271217
#% 300214
#% 323980
#% 346714
#% 353187
#% 397402
#% 480310
#% 484369
#% 577348
#% 617445
#% 635832
#% 657633
#% 754085
#% 793894
#% 793895
#% 960202
#% 963655
#% 963862
#% 963869
#% 963871
#% 979488
#% 993994
#% 1180884
#! We study several transparent techniques for scaling dynamic content web sites, and we evaluate their relative impact when used in combination. Full transparency implies strong data consistency as perceived by the user, nomodifications to existing dynamic content site tiers and no additional programming effort from the user or site administrator upon deployment. We study strategies for scheduling and load balancing queries on a cluster of replicated database back-ends. We also investigate transparent query caching as a means of enhancing database replication. Our work shows that, on an experimental platform with up to 8 database replicas, the various techniques work in synergy to improve overall scaling for the e-commerce TPCW benchmark. We rank the techniques necessary for high performance in order of impact as follows. Key among the strategies are scheduling strategies, such as conflict-aware scheduling, that minimize consistency maintainance over-heads. The choice of load balancing strategy is less important. Transparent query result caching increases performance significantly at any given cluster size for a mostly-read workload. Its benefits are limited for write-intensive workloads, where content-aware scheduling is the only scaling option.

#index 800517
#* SemCast: Semantic Multicast for Content-Based Data Dissemination
#@ Olga Papaemmanouil;Ugur Cetintemel
#t 2005
#c 17
#% 215190
#% 290142
#% 302816
#% 336297
#% 338354
#% 342372
#% 465051
#% 480296
#% 570879
#% 612477
#% 646220
#% 654508
#% 661478
#% 662367
#% 723296
#% 723297
#% 772024
#% 993949
#% 1015276
#% 1016180
#% 1502090
#% 1849768
#! We address the problem of content-based dissemination of highly-distributed, high-volume data streams for stream-based monitoring applications and large-scale data delivery. Existing content-based dissemination approaches commonly rely on distributed filtering trees that require filtering at all brokers on the tree. We present a new semantic multicast approach that eliminates the need for content-based filtering at interior brokers and facilitates fine-grained control over the construction of efficient dissemination trees. The central idea is to split the incoming data streams (based on their contents, rates, and destinations) and then spread the pieces across multiple channels, each of which is implemented as an independent dissemination tree. We present the basic design and evaluation of SemCast, an overlay-network based system that implements this semantic multicast approach. Through a detailed simulation study and realistic network topologies, we demonstrate that SemCast significantly improves the efficiency of dissemination compared to traditional approaches.

#index 800518
#* A Distributed Quadtree Index for Peer-to-Peer Settings
#@ Egemen Tanin;Aaron Harwood;Hanan Samet
#t 2005
#c 17
#% 340175
#% 772021
#% 1712588

#index 800519
#* Adlib: A Self-Tuning Index for Dynamic Peer-to-Peer Systems
#@ Prasanna Ganesan;Qixiang Sun;Hector Garcia-Molina
#t 2005
#c 17
#% 340175
#% 480647
#% 745389
#% 1016166
#! Peer-to-peer (P2P) systems enable queries over a large database horizontally partitioned across a dynamic set of nodes. We devise a self-tuning index for such systems that can trade off index maintenance cost against queryefficiency, in order to optimize the overall system cost. The index, Adlib, dynamically adapts itself to operate at the optimal trade-off point, even as the optimal configuration changes with nodes joining and leaving the system. We use experiments on realistic workloads to demonstrate that Adlib can reduce the overall system cost by a factor of four.

#index 800520
#* DUP: Dynamic-Tree Based Update Propagation in Peer-to-Peer Networks
#@ Liangzhong Yin;Guohong Cao
#t 2005
#c 17
#% 340175
#! In peer-to-peer networks, indices are used to map data id to nodes that host the data. The performance of data access can be improved by actively pushing indices to interested nodes. This paper proposes the Dynamic-tree based Update Propagation (DUP) scheme, which builds the update propagation tree to facilitate the propagation of indices. Because the update propagation tree only involves nodes that are essential for update propagation, the overhead of DUP is very small and the query latency is significantly reduced.

#index 800521
#* Vectorizing and Querying Large XML Repositories
#@ Peter Buneman;Byron Choi;Wenfei Fan;Robert Hutchison;Robert Mann;Stratis D. Viglas
#t 2005
#c 17
#% 36683
#% 172939
#% 286258
#% 287349
#% 300153
#% 384978
#% 397366
#% 458776
#% 464816
#% 479956
#% 480657
#% 480821
#% 504574
#% 654514
#% 765443
#% 994015
#% 1015266
#! Vertical partitioning is a well-known technique for optimizing query performance in relational databases. An extreme form of this technique, which we call vectorization, is to store each column separately. We use a generalization of vectorization as the basis for a native XML store. The idea is to decompose an XML document into a set of vectors that contain the data values and a compressed skeleton that describes the structure. In order to query this representation and produce results in the same vectorized format, we consider a practical fragment of XQuery and introduce the notion of query graphs and a novel graph reduction algorithm that allows us to leverage relational optimization techniques as well as to reduce the unnecessary loading of data vectors and decompression of skeletons. A preliminary experimental study based on some scientific and synthetic XML data repositories in the order of gigabytes supports the claim that these techniques are scalable and have the potential to provide performance comparable with established relational database technology.

#index 800522
#* IMAX: Incremental Maintenance of Schema-Based XML Statistics
#@ Maya Ramanath;Lingzhi Zhang;Juliana Freire;Jayant R. Haritsa
#t 2005
#c 17
#% 43163
#% 333979
#% 378412
#% 397364
#% 397379
#% 411355
#% 427219
#% 458836
#% 465018
#% 465059
#% 479806
#% 480488
#% 632056
#% 659924
#% 730031
#% 745467
#% 745479
#% 993968
#% 993970
#% 1016149
#! Current approaches for estimating the cardinality of XML queries are applicable to a static scenario wherein the underlying XML data does not change subsequent to the collection of statistics on the repository. However, in practice, many XML-based applications are dynamic and involve frequent updates to the data. In this paper, we investigate efficient strategies for incrementally maintaining statistical summaries as and when updates are applied to the data. Specifically, we propose algorithms that handle both the addition of new documents as well as random insertions in the existing document trees. We also show, through a detailed performance evaluation, that our incremental techniques are significantly faster than the naive recomputation approach; and that estimation accuracy can be maintained even with a fixed memory budget.

#index 800523
#* BOXes: Efficient Maintenance of Order-Based Labeling for Dynamic XML Data
#@ Adam Silberstein;Hao He;Ke Yi;Jun Yang
#t 2005
#c 17
#% 23651
#% 333981
#% 378412
#% 397358
#% 397366
#% 397375
#% 480489
#% 548489
#% 548643
#% 570875
#% 598374
#% 656697
#% 730054
#% 745479
#% 765488
#% 1015273
#% 1016224
#% 1712557
#! Order-based element labeling for tree-structured XML data is an important technique in XML processing. It lies at the core of many fundamental XML operations such as containment join and twig matching. While labeling for static XML documents is well understood, less is known about how to maintain accurate labeling for dynamic XML documents, when elements and subtrees are inserted and deleted. Most existing approaches do not work well for arbitrary update patterns; they either produce unacceptably long labels or incur enormous relabeling costs. We present two novel I/O-efficient data structures, W-BOX and B-BOX, thatefficiently maintain labeling for large, dynamic XML documents. We show analytically and experimentally that both, despite consuming minimal amounts of storage, gracefully handle arbitrary update patterns without sacrificing lookupefficiency. The two structures together provide a nice tradeoff between update and lookup costs: W-BOX has logarithmic amortized update cost and constant worst-case lookup cost, while B-BOX has constant amortized update cost and logarithmic worst-case lookup cost. We further propose techniques to eliminate the lookup cost for read-heavy workloads.

#index 800524
#* Efficient Inverted Lists and Query Algorithms for Structured Value Ranking in Update-Intensive Relational Databases
#@ Lin Guo;Jayavel Shanmugasundaram;Kevin Beyer;Eugene Shekita
#t 2005
#c 17
#% 67565
#% 115462
#% 172922
#% 212665
#% 268079
#% 333854
#% 333951
#% 340886
#% 399762
#% 428409
#% 480819
#% 577220
#% 577310
#% 660011
#% 765418
#% 993987
#% 1015265
#% 1015325
#% 1016176
#% 1016183
#% 1016203
#! We propose a new ranking paradigm for relational databases called Structured Value Ranking (SVR). SVR uses structured data values to score (rank) the results of keyword search queries over text columns. Our main contribution is a new family of inverted list indices and associated query algorithms that can support SVR efficiently in update-intensive databases, where the structured data values (and hence the scores of documents) change frequently. Our experimental results on real and synthetic data sets using BerkeleyDB show that we can support SVR efficiently in relational databases.

#index 800525
#* Dynamic Load Management for Distributed Continuous Query Systems
#@ Yongluan Zhou;Beng Chin Ooi;Kian-Lee Tan
#t 2005
#c 17

#index 800526
#* Compressing Bitmap Indices by Data Reorganization
#@ Ali Pinar;Tao Tao;Hakan Ferhatosmanoglu
#t 2005
#c 17
#% 5683
#% 68091
#% 88056
#% 237187
#% 248814
#% 252304
#% 273904
#% 296792
#% 342735
#% 342828
#% 390741
#% 435141
#% 466953
#% 479649
#% 479808
#% 480329
#% 481956
#% 504155
#% 564096
#% 617842
#% 631963
#% 664831
#% 1016130
#% 1376414
#! Many scientific applications generate massive volumes of data through observations or computer simulations, bringing up the need for effective indexing methods for efficient storage and retrieval of scientific data. Unlike conventional databases, scientific data is mostly read-only and its volume can reach to the order of petabytes, making a compact index structure vital. Bitmap indexing has been successfully applied to scientific databases by exploiting the fact that scientific data are enumerated or numerical. Bitmap indices can be compressed with variants of run length encoding for a compact index structure. However even this may not be enough for the enormous data generated in some applications such as high energy physics. In this paper, we study how to reorganize bitmap tables for improved compression rates. Our algorithms are used just as a preprocessing step, thus there is no need to revise the current indexing techniques and the query processing algorithms. We introduce the tuple reordering problem, which aims to reorganize database tuples for optimal compression rates. We propose Gray code ordering algorithm for this NP-Complete problem, which is an inplace algorithm, and runs in linear time in the order of the size of the database. We also discuss how the tuple reordering problem can be reduced to the traveling salesperson problem. Our experimental results on real data sets show that the compression ratio can be improved by a factor of 2 to 10.

#index 800527
#* Practical Data Management Techniques for Vehicle Tracking Data
#@ Sotiris Brakatsoulas;Dieter Pfoser;Nectaria Tryfona
#t 2005
#c 17
#% 729853
#% 775840
#% 1015340
#! A novel data source for assessing traffic conditions is floating car data (FCD) in the form of vehicle tracking data, or, in database terms, trajectory data. This work proposes practical data management techniques including data pre-processing, data modeling and indexing to support the analysis and the data mining of vehicle tracking data

#index 800528
#* Filter Based Directory Replication and Caching
#@ Apurva Kumar
#t 2005
#c 17
#% 458761
#! This paper describes a novel filter based replication model for Lightweight Directory Access Protocol (LDAP) directories. Instead of replicating entire subtrees from the Directory Information Tree (DIT), only entries matching a filter specification are replicated. Advantages of the filter based replication framework over existing subtree based mechanisms have been demonstrated for a real enterprise directory using real workloads.

#index 800529
#* On Discovery of Extremely Low-Dimensional Clusters Using Semi-Supervised Projected Clustering
#@ Kevin Y. Yip;David W. Cheung;Michael K. Ng
#t 2005
#c 17
#% 248792
#% 273891
#% 300131
#% 376266
#% 397382
#% 397384
#% 464291
#% 464608
#% 464631
#% 466890
#% 469422
#% 481281
#% 549419
#% 778729
#! Recent studies suggest that projected clusters with extremely low dimensionality exist in many real datasets. A number of projected clustering algorithms have been proposed in the past several years, but few can identify clusters with dimensionality lower than 10% of the total number of dimensions, which are commonly found in some real datasets such as gene expression profiles. In this paper we propose a new algorithm that can accurately identify projected clusters with relevant dimensions as few as 5% of the total number of dimensions. It makes use of a robust objective function that combines object clustering and dimension selection into a single optimization problem. The algorithm can also utilize domain knowledge in the form of labeled objects and labeled dimensions to improve its clustering accuracy. We believe this is the first semi-supervised projected clustering algorithm. Both theoretical analysis and experimental results show that by using a small amount of input knowledge, possibly covering only a portion of the underlying classes, the new algorithm can be further improved to accurately detect clusters with only 1% of the dimensions being relevant. The algorithm is also useful in getting a target set of clusters when there are multiple possible groupings of the objects.

#index 800530
#* Clustering Aggregation
#@ Aristides Gionis;Heikki Mannila;Panayiotis Tsaparas
#t 2005
#c 17
#% 314054
#% 330769
#% 424809
#% 453464
#% 460812
#% 722902
#% 723892
#% 728025
#% 749492
#% 799743
#! We consider the following problem: given a set of clusterings, find a clustering that agrees as much as possible with the given clusterings. This problem, clustering aggregation, appears naturally in various contexts. For example, clustering categorical data is an instance of the problem: each categorical variable can be viewed as a clustering of the input rows. Moreover, clustering aggregation can be used as a meta-clustering method to improve the robustness of clusterings. The problem formulation does not require a-priori information about the number of clusters, and it gives a naturalway for handlingmissing values. We give a formal statement of the clustering-aggregation problem, we discuss related work, and we suggest a number of algorithms. For several of the methods we provide theoretical guarantees on the quality of the solutions. We also show how sampling can be used to scale the algorithms for large data sets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions.

#index 800531
#* Mining Cross-Graph Quasi-Cliques in Gene Expression and Protein Interaction Data
#@ Jian Pei;Daxin Jiang;Aidong Zhang
#t 2005
#c 17
#% 397382
#% 469422
#% 731609

#index 800532
#* CLICKS: Mining Subspace Clusters in Categorical Data via K-Partite Maximal Cliques
#@ Mohammed J. Zaki;Markus Peters
#t 2005
#c 17
#% 280419
#% 479659
#! We present a novel algorithm called CLICKS, that finds clusters in categorical datasets based on a search for k-partite maximal cliques. Unlike previous methods, CLICKS mines subspace clusters. It uses a selective vertical method to guarantee complete search. CLICKS outperforms previous approaches by over an order of magnitude and scales better than any of the existing method for high-dimensional datasets. We demonstrate this improvement in an excerpt from our comprehensive performance studies.

#index 800533
#* Mining Closed Relational Graphs with Connectivity Constraints
#@ Xifeng Yan;X. Jasmine Zhou;Jiawei Han
#t 2005
#c 17
#% 313959
#% 443723
#% 729938
#% 731608

#index 800534
#* Efficient Creation and Incremental Maintenance of the HOPI Index for Complex XML Document Collections
#@ Ralf Schenkel;Anja Theobald;Gerhard Weikum
#t 2005
#c 17
#% 79312
#% 340914
#% 378412
#% 379482
#% 379484
#% 397358
#% 397359
#% 397360
#% 397406
#% 458829
#% 458861
#% 479465
#% 654441
#% 654442
#% 654452
#% 659923
#% 745468
#% 745479
#% 765408
#% 765442
#% 765488
#% 1015258
#% 1712557
#% 1712559
#% 1712560
#! The HOPI index, a connection index for XML documents based on the concept of a 2-hop cover, provides space- and time-efficient reachability tests along the ancestor, descendant, and link axes to support path expressions with wildcards in XML search engines. This paper presents enhanced algorithms for building HOPI, shows how to augment the index with distance information, and discusses incremental index maintenance. Our experiments show substantial improvements over the existing divide-and-conquer algorithm for index creation, low space overhead for including distance information in the index, and efficient updates.

#index 800535
#* On the Sequencing of Tree Structures for XML Indexing
#@ Haixun Wang;Xiaofeng Meng
#t 2005
#c 17
#% 289010
#% 291299
#% 325384
#% 379483
#% 379484
#% 397359
#% 397360
#% 464883
#% 468476
#% 479465
#% 480489
#% 480656
#% 654450
#% 765505
#! Sequence-based XML indexing aims at avoiding expensive join operations in query processing. It transforms structured XML data into sequences so that a structured query can be answered holistically through subsequence matching. In this paper, we address the problem of query equivalence with respect to this transformation, and we introduce a performance-oriented principle for sequencing tree structures. With query equivalence, XML queries can be performed through subsequence matching without join operations, post-processing, or other special handling for problems such as false alarms. We identify a class of sequencing methods for this purpose, and we present a novel subsequence matching algorithm that observe query equivalence. Still, query equivalence is just a prerequisite for sequence-based XML indexing. Our goal is to find the best sequencing strategy with regard to the time and space complexity in indexing and querying XML data. To this end, we introduce a performance-oriented principle to guide the sequencing of tree structures. For any given XML dataset, the principle finds an optimal sequencing strategy according to its schema and its data distribution. We present a novel method that realizes this principle. In our experiments, we show the advantages of sequence-based indexing over traditional XML indexing methods, and we compare several sequencing strategies and demonstrate the benefit of the performance-oriented sequencing principle.

#index 800536
#* Efficient Algorithms for Pattern Matching on Directed Acyclic Graphs
#@ Li Chen;Amarnath Gupta;M. Erdem Kurul
#t 2005
#c 17
#% 58365
#% 397375
#! Recently graph data models have become increasingly popular in many scientific fields. Efficient query processing over such data is critical. Existing works often rely on index structures that store pre-computed transitive relations to achieve efficient graph matching. In this paper, we present a family of stack-based algorithms to handle path and twig pattern queries for directed acyclic graphs (DAGs) in particular. With the worst-case space cost linearly bounded by the number of edges in the graph, our algorithms achieve a quadratic runtime complexity in the average size of the query variable bindings. This is optimal among the navigation-based graph matching algorithms.

#index 800537
#* VLEI Code: An Efficient Labeling Method for Handling XML Documents in an RDB
#@ Kazuhito Kobayashi;Wenxin Wenxin;Dai Kobayashi;Akitsugu Watanabe;Haruo Yokota
#t 2005
#c 17
#% 397366
#% 598374
#% 765488
#! A number of XML labeling methods have been proposed to store XML documents in relational databases. However, they have a vulnerable point, in insertion operations. We propose the Variable Length Endless Insertable (VLEI) code and apply it to XML labeling to reduce the cost of insertion operations. Results of our experiments indicate that a combination of the VLEI code and Dewey order is effective for handling skewed insertions.

#index 800538
#* BlossomTree: Evaluating XPaths in FLWOR Expressions
#@ Ning Zhang;Shishir K. Agrawal;M. Tamer Ozsu
#t 2005
#c 17
#! Efficient evaluation of path expressions has been studied extensively. However, evaluating more complex FLWOR expressions that contain multiple path expressions has not been well studied. In this paper, we propose a novel pattern matching approach, called BlossomTree, to evaluate a FLWOR expression that contains correlated path expressions. BlossomTree is a formalism to capture the semantics of the path expressions and their correlations. We propose a general algebraic framework (abstract data types and logical operators) to evaluate BlossomTreepattern matching that facilitates efficient evaluation and experimentation. We design efficient data structures and algorithms to implement the abstract data types and logical operators. Our experimental studies demonstrate that the BlossomTreeapproach can generate highly efficient query plans in different environments.

#index 800539
#* Change Tolerant Indexing for Constantly Evolving Data
#@ Reynold Cheng;Yuni Xia;Sunil Prabhakar;Rahul Shah
#t 2005
#c 17
#% 273706
#% 273714
#% 282343
#% 282588
#% 299979
#% 300174
#% 390132
#% 427199
#% 481956
#% 554884
#% 1015320
#! Index structures are designed to optimize search performance, while at the same time supporting efficient data updates. Although not explicit, existing index structures are typically based upon the assumption that the rate of updates will be small compared to the rate of querying. This assumption is not valid in streaming data environments such as sensor and moving object databases, where updates are received incessantly. In fact, for many applications, the rate of updates may well exceed the rate of querying. In such environments, index structures suffer from poor performance due to the large overhead of keeping the index updated with the latest data. Recent efforts at indexing moving object data assume objects move in a restrictive manner (e.g. in straight lines with constant velocity). In this paper, we propose an index structure explicitly designed to perform well for both querying and updating. We assume a more relaxed model of object movement. In particular, we observe that objects often stay in a region (e.g., building) for an extended amount of time, and exploit this phenomenon to optimize an index for both updates and queries. The paper is developed with the example of R-trees, but the ideas can be extended to other index structures as well. We present the design of the Change Tolerant R-tree, and an experimental evaluation.

#index 800540
#* Proactive Caching for Spatial Queries in Mobile Environments
#@ Haibo Hu;Jianliang Xu;Wing Sing Wong;Baihua Zheng;Dik Lun Lee;Wang-Chien Lee
#t 2005
#c 17
#% 86950
#% 152937
#% 172875
#% 259642
#% 287466
#% 309461
#% 399548
#% 413181
#% 427199
#% 443263
#% 452851
#% 452871
#% 464848
#% 479453
#% 480597
#% 480927
#% 481916
#% 527191
#% 654478
#! Semantic caching enables mobile clients to answer spatial queries locally by storing the query descriptions together with the results. However, it supports only a limited number of query types, and sharing results among these types is difficult. To address these issues, we propose a proactive caching model which caches the result objects as well as the index that supports these objects as the results. The cached index enables the objects to be reused for all common types of queries. We also propose an adaptive scheme to cache such an index, which further optimizes the query response time for the best user experience. Simulation results show that proactive caching achieves a significant performance gain over page caching and semantic caching in mobile environments where wireless bandwidth and battery are precious resources.

#index 800541
#* Improving Data Accessibility for Mobile Clients through Cooperative Hoarding
#@ Kwong Yuen Lai;Zahir Tari;Peter Bertok
#t 2005
#c 17
#% 720827
#% 730028
#% 742046
#! In this paper, we introduce the concept of cooperative hoarding to reduce the risks of cache misses for mobile clients. Cooperative hoarding takes advantage of group mobility behaviour, combined with peer cooperation in ad-hoc mode, to improve hoard performance. Two cooperative hoarding approaches that take into account clientsý access frequencies, connection probabilities and cache size when performing hoarding are proposed. Test results show that the proposed methods significantly improve cache hit ratio and reduce query costs compared to existing approaches.

#index 800542
#* A Fully Distributed Spatial Index for Wireless Data Broadcast
#@ Wang-Chien Lee;Baihua Zheng
#t 2005
#c 17
#% 427199
#% 443127
#! To support location-based services in wireless data broadcast systems, a distributed spatial index (called DSI) is proposed in this paper. DSI is highly efficient because it has a linear yet fully distributed structure that naturally facilitates multiple replications of the index by sharing links in different search trees. Search algorithms for point queries, window queries, and kNN queries, based on DSI are presented. Empirical evaluation of DSI are conducted. Result shows that DSI significantly out-performs R-tree and Hilbert Curve Index, two state-of-the-art spatial indexing techniques for wireless data broadcast.

#index 800543
#* Efficient Data Management on Lightweight Computing Devices
#@ Rajkumar Sen;Krithi Ramamritham
#t 2005
#c 17
#% 566138
#! Lightweight computing devices are becoming ubiquitous and an increasing number of applications are being developed for these devices. Many of these applications deal with significant amounts of data and involve complex joins and aggregate operations which necessitate a local database management system on the device. This is a challenge as these devices are constrained by limited stable storage and main memory. Hence new storage models that reduce storage costs are needed and a storage scheme should be selected based on data characteristics, nature of queries, and updates. Also, query execution plan should be chosen depending on the amount of available memory and the underlying storage scheme; memory should be optimally allocated among the database operators involved in the query. To achieve these goals, we utilize a novel storage model, ID based Storage, which reduces storage costs considerably. We present an exact algorithm for allocating memory among the database operators. Because of its high complexity, we also propose a heuristic solution based on the benefit of an operator per unit memory allocation.

#index 800544
#* Postgres-R(SI): Combining Replica Control with Concurrency Control Based on Snapshot Isolation
#@ Shuqing Wu;Bettina Kemme
#t 2005
#c 17
#% 9241
#% 201869
#% 210179
#% 248825
#% 273894
#% 323980
#% 335454
#% 342964
#% 480310
#% 490341
#% 508197
#% 570890
#% 594328
#% 717164
#% 721142
#% 745516
#% 793894
#% 793895
#% 800516
#% 960202
#% 993994
#% 1180884
#! Replicating data over a cluster of workstations is a powerful tool to increase performance, and provide fault-tolerance for demanding database applications. The big challenge in such systems is to combine replica control (keeping the copies consistent) with concurrency control. Most of the research so far has focused on providing the traditional correctness criteria serializability. However, more and more database systems, e.g., Oracle and PostgreSQL, use multi-version concurrency control providing the isolation level snapshot isolation. In this paper, we present Postgres-R(SI), an extension of PostgreSQL offering transparent replication. Our replication tool is designed to work smoothly with PostgreSQLýs concurrency control providing snapshot isolation for the entire replicated system. We present a detailed description of the replica control algorithm, and how it is combined with PostgreSQLýs concurrency control component. Furthermore, we discuss some challenges we encountered when implementing the protocol. Our performance analysis based on the TPC-W benchmark shows that this approach exhibits excellent performance for real-life applications even if they are update intensive.

#index 800545
#* SNAP: Efficient Snapshots for Back-in-Time Execution
#@ Liuba Shrira;Hao Xu
#t 2005
#c 17
#% 107692
#% 152904
#% 152926
#% 271908
#% 286472
#% 403195
#% 442967
#% 480096
#% 488227
#% 702279
#% 830695
#% 978174
#% 978723
#% 993493
#! SNAP is a novel high-performance snapshot system for object storage systems. The goal is to provide a snapshot service that is efficient enough to permit "back-in-time" read-only activities to run against application-specified snapshots. Such activities are often impossible to run against rapidly evolving current state because of interference or because the required activity is determined in retrospect. A key innovation in SNAP is that it provides snapshots that are transactionally consistent, yet non-disruptive. Unlike earlier systems, we use novel in-memory data structures to ensure that frequent snapshots do not block applications from accessing the storage system, and do not cause unnecessary disk operations. SNAP takes a novel approach to dealing with snapshot meta-data using a new technique that supports both incremental meta-data creation and efficient meta-data reconstruction. We have implemented a SNAP prototype and analyzed its performance. Preliminary results show that providing snapshots for back-in-time activities has low impact on system performance even when snapshots are frequent.

#index 800546
#* Improving Preemptive Prioritization via Statistical Characterization of OLTP Locking
#@ David T. McWherter;Bianca Schroeder;Anastassia Ailamaki;Mor Harchol-Balter
#t 2005
#c 17
#% 1326
#% 114710
#% 116927
#% 118658
#% 124815
#% 124816
#% 172939
#% 287470
#% 442012
#% 443132
#% 451767
#% 463092
#% 480266
#% 745471
#! OLTP and transactional workloads are increasingly common in computer systems, ranging from e-commerce to warehousing to inventory management. It is valuable to provide priority scheduling in these systems, to reduce the response time for the most important clients, e.g. the "big spenders". Two-phase locking, commonly used in DBMS, makes prioritization difficult, as transactions wait for locks held by others regardless of priority. Common lock scheduling solutions, including non-preemptive priority inheritance and preemptive abort, have performance drawbacks for TPC-C type workloads. The contributions of this paper are two-fold: (i) We provide a detailed statistical analysis of locking in TPC-C workloads with priorities under several common preemptive and non-preemptive lock prioritization policies. We determine why non-preemptive policies fail tosufficiently help high-priority transactions, and why pre-emptive policies excessively hurt low-priority transactions. (ii) We propose and implement a policy, POW, that provides all the benefits of preemptive prioritization without its penalties.

#index 800547
#* A Probabilistic XML Approach to Data Integration
#@ Maurice van Keulen;Ander de Keijzer;Wouter Alink
#t 2005
#c 17
#% 77650
#% 209725
#% 215225
#% 235023
#% 333990
#% 417773
#% 482108
#% 773977
#% 993985
#! In mobile and ambient environments, devices need to become autonomous, managing and resolving problems without interference from a user. The database of a (mobile) device can be seen as its knowledge about objects in the ýreal worldý. Data exchange between small and/or large computing devices can be used to supplement and update this knowledge whenever a connection gets established. In many situations, however, data from different data sources referring to the same real world objects, may conflict. It is the task of the data management system of the device to resolve such conflicts without interference from a user. In this paper, we take a first step in the development of a probabilistic XML DBMS. The main idea is to drop the assumption that data in the database should be certain: subtrees in XML documents may denote possible views on the real world. We formally define the notion of probabilistic XML tree and several operations thereon. We also present an approach for determining a logical semantics for queries on probabilistic XML data. Finally, we introduce an approach for XML data integration where conflicts are resolved by the introduction of possibilities in the database.

#index 800548
#* A Relationally Complete Visual Query Language for Heterogeneous Data Sources and Pervasive Querying
#@ Stavros Polyviou;George Samaras;Paraskevas Evripidou
#t 2005
#c 17
#% 32904
#% 68197
#% 68199
#% 82353
#% 144223
#% 172811
#% 172825
#% 232476
#% 402992
#% 479465
#% 482087
#% 527869
#% 725502
#% 1305906
#! In this paper we introduce and formally define Query by Browsing (QBB), a scalable, relationally complete visual query language based on the desktop user interface paradigm and tuple relational calculus that allows the formulation of complex queries over relational, entity-relationship, object-oriented and XML data sources on a variety of handheld and desktop platforms. It is to our knowledge the first visual query language to combine the important characteristics of usability, scalability, expressive power and flexibility. We support these claims by demonstrating the similarity of the QBB paradigm to the popular desktop user interface paradigm, by relating it to relational calculus and relational algebra and by describing Chiromancer II, a web-based implementation of the QBB paradigm for handheld devices. We also discuss ways in which non-relational sources can be represented and queried and compare QBB to related work in the area of visual query languages for a variety of data models. We finally offer conclusions and thoughts for future work.

#index 800549
#* Triggers over XML Views of Relational Data
#@ Feng Shao;Antal Novak;Jayavel Shanmugasundaram
#t 2005
#c 17
#% 309851
#% 480435
#% 480623
#% 480657

#index 800550
#* THALIA: Test Harness for the Assessment of Legacy Information Integration Approaches
#@ Joachim Hammer;Mike Stonebraker;Oguzhan Topsakal
#t 2005
#c 17
#% 85086
#% 654522
#% 1017856

#index 800551
#* Integrating Data from Disparate Sources: A Mass Collaboration Approach
#@ Robert McCann;Alexander Kramnik;Warren Shen;Vanitha Varadarajan;Olu Sobulo;AnHai Doan
#t 2005
#c 17
#% 480824
#% 1271981

#index 800552
#* On the Optimal Ordering of Maps and Selections under Factorization
#@ Thomas Neumann;Sven Helmer;Guido Moerkotte
#t 2005
#c 17
#% 554
#% 152940
#% 152942
#% 172930
#% 172931
#% 210206
#% 287461
#% 443341
#% 479938
#% 480443
#% 480944
#% 481621
#% 481915
#% 510676
#% 654471
#! The query optimizer of a database system is confronted with two aspects when handling user-defined functions (UDFs) in query predicates: the vast differences in evaluation costs between UDFs (and other functions) and multiple calls of the same (expensive) UDF. The former is dealt with by ordering the evaluation of the predicates optimally, the latter by identifying common subexpressions and thereby avoiding costly recomputation. Current approaches order n predicates optimally (neglecting factorization) in O(n log n). Their result may deviate significantly from the optimal solution under factorization. We formalize the problem of finding optimal orderings under factorization and prove that it is NP-hard. Furthermore, we show how to improve on the run time of the brute-force algorithm (which computes all possible orderings) by presenting different enhanced algorithms. Although in the worst case these algorithms obviously still behave exponentially, our experiments demonstrate that for real-life examples their performance is much better.

#index 800553
#* RankFP: A Framework for Supporting Rank Formulation and Processing
#@ Hwanjo Yu;Seung-won Hwang;Kevin Chen-Chuan Chang
#t 2005
#c 17
#% 55490
#% 227894
#% 333854
#% 397378
#% 479816
#% 577224

#index 800554
#* Evaluation of Spatio-temporal Predicates on Moving Objects
#@ Markus Schneider
#t 2005
#c 17
#% 300173
#% 315005
#% 443521
#! Moving objects databases managing spatial objects with continuously changing position and extent over time have recently found large interest in the database community. Queries about moving objects become particularly interesting when they ask for temporal changes in the topological relationships between evolving spatial objects. A concept of spatio-temporal predicates has been proposed to describe these relationships. The goal of this paper is to designefficient algorithms for them so that they can be used in spatio-temporal joins and selections. This paper proposes not to design an algorithm for each new predicate individually but to employ a generic algorithmic scheme which is able to cover present and future predicate definitions.

#index 800555
#* Stabbing the Sky: Efficient Skyline Computation over Sliding Windows
#@ Xuemin Lin;Yidong Yuan;Wei Wang;Hongjun Lu
#t 2005
#c 17
#% 672
#% 2115
#% 86950
#% 201876
#% 252304
#% 265396
#% 287466
#% 288976
#% 289148
#% 317729
#% 333951
#% 338425
#% 378388
#% 393844
#% 410276
#% 427199
#% 443327
#% 465167
#% 480671
#% 654480
#% 745533
#% 993954
#% 1016196
#! We consider the problem of efficiently computing the skyline against the most recent N elements in a data stream seen so far. Specifically, we study the n-of-N skyline queries; that is, computing the skyline for the most recent n (驴 驴 N) elements. Firstly, we developed an effective pruning technique to minimize the number of elements to be kept. It can be shown that on average storing only O(log^d N) elements from the most recent N elements is sufficient to support the precise computation of all n-of-N skyline queries in a d-dimension space if the data distribution on each dimension is independent. Then, a novel encoding scheme is proposed, together with efficient update techniques, for the stored elements, so that computing an n-of-N skyline query in a d-dimension space takes O(log N + s) time that is reduced to O(d log log N + s) if the data distribution is independent, where s is the number of skyline points. Thirdly, a novel trigger based technique is provided to process continuous n-of-N skyline queries with O(驴) time to update the current result per new data element and O(log s) time to update the trigger list per result change, where 驴 is the number of element changes from the current result to the new result. Finally, we extend our techniques to computing the skyline against an arbitrary window in the most recent N elements. Besides theoretical performance guarantees, our extensive experiments demonstrated that the new techniques can support on-line skyline query computation over very rapid data streams.

#index 800556
#* Towards Exploring Interactive Relationship between Clusters and Outliers in Multi-Dimensional Data Analysis
#@ Yong Shi;Aidong Zhang
#t 2005
#c 17
#% 465031
#% 480132
#% 721143
#! Nowadays many data mining algorithms focus on clustering methods. There are also a lot of approaches designed for outlier detection. We observe that, in many situations, clusters and outliers are concepts whose meanings are inseparable to each other, especially for those data sets with noise. Thus, it is necessary to treat clusters and outliers as concepts of the same importance in data analysis. In this paper, we present a cluster-outlier iterative detection algorithm, tending to detect the clusters and outliers in another perspective for noisy data sets. In this algorithm, clusters are detected and adjusted according to the intra-relationship within clusters and the inter-relationship between clusters and outliers, and vice versa. The adjustment and modification of the clusters and outliers are performed iteratively until a certain termination condition is reached. This data processing algorithm can be applied in many fields such as pattern recognition, data clustering and signal processing. Experimental results demonstrate the advantages of our approach.

#index 800557
#* Privacy and Ownership Preserving of Outsourced Medical Data
#@ Elisa Bertino;Beng Chin Ooi;Yanjiang Yang;Robert H. Deng
#t 2005
#c 17
#% 300184
#% 397367
#% 443463
#% 488324
#% 576109
#% 577239
#% 654447
#% 654448
#% 654449
#% 724776
#% 745531
#% 993942
#% 993944
#% 1015329
#% 1015331
#% 1395188
#! The demand for the secondary use of medical data is increasing steadily to allow for the provision of better quality health care. Two important issues pertaining to this sharing of data have to be addressed: one is the privacy protection for individuals referred to in the data; the other is copyright protection over the data. In this paper, we present a unified framework that seamlessly combines techniques of binning and digital watermarking to attain the dual goals of privacy and copyright protection. Our binning method is built upon an earlier approach of generalization and suppression by allowing a broader concept of generalization. To ensure data usefulness, we propose constraining Binning by usage metrics that define maximal allowable information loss, and the metrics can be enforced off-line. Our watermarking algorithm watermarks the binned data in a hierarchical manner by leveraging on the very nature of the data. The method is resilient to the generalization attack that is specific to the binned data, as well as other attacks intended to destroy the inserted mark. We prove that watermarking could not adversely interfere with binning, and implemented the framework. Experiments were conducted, and the results show the robustness of the proposed framework.

#index 800558
#* Configurable Security Protocols for Multi-party Data Analysis with Malicious Participants
#@ Bradley Malin;Edoardo Airoldi;Samuel Edoho-Eket;Yiheng Li
#t 2005
#c 17
#% 23638
#% 169589
#% 214257
#% 319994
#% 340291
#% 347237
#% 495694
#% 583829
#% 616944
#% 729930
#% 772829
#% 1068712
#% 1385044
#% 1549196
#! Standard multi-party computation models assume semi-honest behavior, where the majority of participants implement protocols according to specification, an assumption not always plausible. In this paper we introduce a multi-party protocol for collaborative data analysis when participants are malicious and fail to follow specification. The protocol incorporates a semi-trusted third party, which analyzes encrypted data and provides honest responses that only intended recipients can successfully decrypt. The protocol incorporates data confidentiality by enabling participants to receive encrypted responses tailored to their own encrypted data submissions without revealing plaintext to other participants, including the third party. As opposed to previous models, trust need only be placed on a single participant with no data at stake. Additionally, the proposed protocol is configurable in a way that security features are controlled by independent subprotocols. Various combinations of subprotocols allow for a flexible security system, appropriate for a number of distributed data applications, such as secure list comparison.

#index 800559
#* Privacy-Preserving Top-k Queries
#@ Jaideep Vaidya;Chris Clifton
#t 2005
#c 17
#% 278831
#% 479816
#% 772829
#% 954159
#% 1068712

#index 800560
#* Secure Third Party Distribution of XML Data
#@ B. Carminati;E. Ferrari;E. Bertino
#t 2005
#c 17
#% 397367
#% 772846
#% 1015329

#index 800561
#* Improving Performance of Cluster-Based Secure Application Servers with User-Level Communication
#@ Jin-Ha Kim;Gyu Sang Choi;Chita R. Das
#t 2005
#c 17
#% 209895
#% 330759
#% 728669

#index 800562
#* GPIVOT: Efficient Incremental Maintenance of Complex ROLAP Views
#@ Songting Chen;Elke A. Rundensteiner
#t 2005
#c 17
#% 53706
#% 152928
#% 201929
#% 210210
#% 223781
#% 227869
#% 300141
#% 300199
#% 464215
#% 479792
#% 479968
#% 480629
#% 564202
#% 770333
#% 1016212
#! Data warehousing and on-line analytical processing (OLAP) are essential for decision support applications. Common OLAP operations include for example drill down, roll up, pivot and unpivot. Typically, such queries are fairly complex and are often executed over huge volumes of data. The solution in practice is to use materialized views to reduce the query cost. Utilizing materialized views that incorporate not just traditional simple SELECT-PROJECT-JOIN operators but also complex OLAP operators such as pivot and unpivot is crucial to improve the OLAP query performance but as of now unexplored topic. In this work, we demonstrate that the efficient maintenance of views with pivot and unpivot operators requires the definition of more generalized operators, which we call GPIVOT and GUNPIVOT. We propose rewriting rules, combination rules and propagation rules for such operators. We also design a novel view maintenance framework for applying these rules to obtain an efficient maintenance plan. Our query transformation rules are thus dual purpose serving both view maintenance and query optimization. This paves the way for the inclusion of the GPIVOT and GUNPIVOT into any DBMS engine.

#index 800563
#* Optimizing ETL Processes in Data Warehouses
#@ Alkis Simitsis;Panos Vassiliadis;Timos Sellis
#t 2005
#c 17
#% 83933
#% 136740
#% 300127
#% 301169
#% 318049
#% 428155
#% 480499
#% 533937
#% 577523
#% 726621
#% 1388088
#! Extraction-Transformation-Loading (ETL) tools are pieces of software responsible for the extraction of data from several sources, their cleansing, customization and insertion into a data warehouse. Usually, these processes must be completed in a certain time window; thus, it is necessary to optimize their execution time. In this paper, we delve into the logical optimization of ETL processes, modeling it as a state-space search problem. We consider each ETL workflow as a state and fabricate the state space through a set of correct state transitions. Moreover, we provide algorithms towards the minimization of the execution cost of an ETL workflow.

#index 800564
#* PnP: Parallel and External Memory Iceberg Cube Computation
#@ Ying Chen;Frank Dehne;Todd Eavis;Andrew Rau-Chaplin
#t 2005
#c 17
#% 273916
#% 333927
#% 1015294

#index 800565
#* Cost-Driven General Join View Maintenance over Distributed Data Sources
#@ Bin Liu;Elke A. Rundensteiner
#t 2005
#c 17
#% 201928
#% 210210
#% 227947
#% 273918
#% 300141
#% 330305
#% 413556
#% 791180
#! Maintainingmaterialized views that have join conditions between arbitrary pairs of data sources possibly with cycles is critical for many applications. In this work, we model view maintenance as the process of answering a set of inter-related distributed multi-join queries. We illustrate two strategies for maintaining as well as optimizing such general join views. We propose a cost-driven view maintenance framework which generates optimized maintenance plans tuned to a given environmental settings. This framework can significantly improve view maintenance performance especially in a distributed environment.

#index 800566
#* Mining Evolving Customer-Product Relationships in Multi-dimensional Space
#@ Xiaolei Li;Jiawei Han;Xiaoxin Yin;Dong Xin
#t 2005
#c 17
#% 342600
#% 420053
#% 463903
#% 481290
#% 481609
#% 993958
#! Previous work on mining transactional database has focused primarily on mining frequent itemsets, association rules, and sequential patterns. However, interesting relationships between customers and items, especially their evolution with time, have not been studied thoroughly. In this paper, we propose a Gaussian transformation-based regression model that captures time-variant relationships between customers and products. Moreover, since it is interesting to discover such relationships in a multi-dimensional space, an efficient method has been developed to compute multi-dimensional aggregates of such curves in a data cube environment. Our experimental results have demonstrated the promise of the approach.

#index 800567
#* Bootstrapping Semantic Annotation for Content-Rich HTML Documents
#@ Saikat Mukherjee;I. V. Ramakrishnan;Amarjeet Singh
#t 2005
#c 17
#% 25998
#% 219052
#% 227987
#% 244103
#% 259991
#% 273925
#% 280817
#% 309737
#% 330770
#% 344447
#% 348146
#% 348168
#% 350859
#% 397605
#% 465754
#% 577301
#% 577318
#% 577323
#% 577346
#% 632051
#% 658747
#% 659925
#% 729939
#% 754102
#! Enormous amount of semantic data is still being encoded in HTML documents. Identifying and annotating the semantic concepts implicit in such documents makes them directly amenable for Semantic Web processing. In this paper we describe a highly automated technique for annotating HTML documents, especially template-based content-rich documents, containing many different semantic concepts per document. Starting with a (small) seed of hand-labeled instances of semantic concepts in a set of HTML documents we bootstrap an annotation process that automatically identifies unlabeled concept instances present in other documents. The bootstrapping technique exploits the observation that semantically related items in content-rich documents exhibit consistency in presentation style and spatial locality to learn a statistical model for accurately identifying different semantic concepts in HTML documents drawn from a variety ofWeb sources. We also present experimental results on the effectiveness of the technique.

#index 800568
#* Text Classification without Labeled Negative Documents
#@ Gabriel Pui Cheong Fung;Jeffrey Xu Yu;Hongjun Lu;Philip S. Yu
#t 2005
#c 17
#% 46803
#% 118771
#% 194283
#% 280404
#% 280817
#% 304876
#% 311027
#% 340904
#% 344447
#% 458379
#% 464604
#% 464641
#% 464777
#% 466083
#% 577235
#% 629610
#% 727883
#% 1279298
#! This paper presents a new solution for the problem of building a text classifier with a small set of labeled positive documents (P) and a large set of unlabeled documents (U). Here, the unlabeled documents are mixed with both of the positive and negative documents. In other words, no document is labeled as negative. This makes the task of building a reliable text classifier challenging. In general, the existing approaches for solving this kind of problem use a two-step approach: i) extract the negative documents (N) from U; and ii) build a classifier based on P and N. However, none of the reported studies tries to further extract any positive documents (P驴) from U. Intuitively, extracting P驴 from U will increase the reliability of the classifier. However, extracting P驴 from U is difficult. A document in U that possesses some of the features exhibited in P does not necessarily mean that it is a positive document, and vice versa. It is very sensitive to extract positive documents, because those extracted positive samples may become noises. The very large size of U and the very high diversity exhibited there also contribute to the difficulty of extracting any positive documents. In this paper, we propose a partitionbased heuristic which aims at extracting both of the positive and negative documents in U. Extensive experiments based on three benchmarks are conducted. The favorable results indicated that our proposed heuristic outperforms all of the existing approaches significantly, especially in the case where the size of P is extremely small.

#index 800569
#* Modeling and Managing Content Changes in Text Databases
#@ Panagiotis G. Ipeirotis;Alexandros Ntoulas;Junghoo Cho;Luis Gravano
#t 2005
#c 17
#% 227891
#% 252472
#% 281154
#% 287463
#% 300139
#% 309746
#% 330604
#% 340146
#% 397355
#% 438251
#% 447946
#% 480136
#% 482615
#% 577370
#% 640706
#% 643012
#% 754058
#% 765465
#% 978374
#% 993964
#% 993974
#! Large amounts of (often valuable) information are stored in web-accessible text databases. "Metasearchers" provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the database contents to select the best databases for each query. So far, database selection research has largely assumed that databases are static, so the associated statistical summaries do not need to change over time. However, databases are rarely static and the statistical summaries that describe their contents need to be updated periodically to reflect content changes. In this paper, we first report the results of a study showing how the content summaries of 152 real web databases evolved over a period of 52 weeks. Then, we show how to use "survival analysis" techniques in general, and Coxýs proportional hazards regression in particular, to model database changes over time and predict when we should update each content summary. Finally, we exploit our change model to devise update schedules that keep the summaries up to date by contacting databases only when needed, and then we evaluate the quality of our schedules experimentally over real web databases.

#index 800570
#* Fast Approximate Similarity Search in Extremely High-Dimensional Data Sets
#@ Michael E. Houle;Jun Sakuma
#t 2005
#c 17
#% 159853
#% 172922
#% 201935
#% 219847
#% 227939
#% 249321
#% 252304
#% 264161
#% 294634
#% 299978
#% 306893
#% 318051
#% 321455
#% 342827
#% 427199
#% 443517
#% 464888
#% 465014
#% 479462
#% 479649
#% 479973
#% 480133
#% 480632
#% 481460
#% 481620
#% 481956
#% 571079
#% 615785
#% 617175
#% 632043
#% 654466
#% 659921
#% 729969
#% 730010
#% 840583
#! This paper introduces a practical index for approximate similarity queries of large multi-dimensional data sets: the spatial approximation sample hierarchy (SASH). A SASH is a multi-level structure of random samples, recursively constructed by building a SASH on a large randomly selected sample of data objects, and then connecting each remaining object to several of their approximate nearest neighbors from within the sample. Queries are processed by first locating approximate neighbors within the sample, and then using the pre-established connections to discover neighbors within the remainder of the data set. The SASH index relies on a pairwise distance measure, but otherwise makes no assumptions regarding the representation of the data. Experimental results are provided for query-by-example operations on protein sequence, image, and text data sets, including one consisting of more than 1 million vectors spanning more than 1.1 million terms 驴 far in excess of what spatial search indices can handle efficiently. For sets of this size, the SASH can return a large proportion of the true neighbors roughly 2 orders of magnitude faster than sequential search.

#index 800571
#* Monitoring k-Nearest Neighbor Queries over Moving Objects
#@ Xiaohui Yu;Ken Q. Pu;Nick Koudas
#t 2005
#c 17
#% 201876
#% 248797
#% 252304
#% 300174
#% 397377
#% 427199
#% 479816
#% 481947
#% 495433
#% 510675
#% 564630
#% 574283
#% 736290
#% 743565
#% 745458
#% 745464
#% 765452
#% 1015297
#% 1015305
#% 1015320
#% 1016191
#! Many location-based applications require constant monitoring of k-nearest neighbor (k-NN) queries over moving objects within a geographic area. Existing approaches to this problem have focused on predictive queries, and relied on the assumption that the trajectories of the objects are fully predictable at query processing time. We relax this assumption, and propose two efficient and scalable algorithms using grid indices. One is based on indexing objects, and the other on queries. For each approach, a cost model is developed, and a detailed analysis along with the respective applicability are presented. The Object-Indexing approach is further extended to multi-levels to handle skewed data. We show by experiments that our grid-based algorithms significantly outperform R-tree-based solutions. Extensive experiments are also carried out to study the properties and evaluate the performance of the proposed approaches under a variety of settings.

#index 800572
#* SEA-CNN: Scalable Processing of Continuous K-Nearest Neighbor Queries in Spatio-temporal Databases
#@ Xiaopeng Xiong;Mohamed F. Mokbel;Walid G. Aref
#t 2005
#c 17
#% 201876
#% 210187
#% 227939
#% 287466
#% 300174
#% 300179
#% 421124
#% 427199
#% 442615
#% 458853
#% 461923
#% 464859
#% 495433
#% 527187
#% 554884
#% 726622
#% 729864
#% 765166
#% 765453
#% 783643
#% 993948
#% 993955
#% 1015297
#% 1015305
#% 1015320
#% 1016192
#% 1016193
#% 1016198
#! Location-aware environments are characterized by a large number of objects and a large number of continuous queries. Both the objects and continuous queries may change their locations over time. In this paper, we focus on continuous k-nearest neighbor queries (CKNN, for short). We present a new algorithm, termed SEA-CNN, for answering continuously a collection of concurrent CKNN queries. SEA-CNN has two important features: incremental evaluation and shared execution. SEA-CNN achieves bothefficiency and scalability in the presence of a set of concurrent queries. Furthermore, SEA-CNN does not make any assumptions about the movement of objects, e.g., the objects velocities and shapes of trajectories, or about the mutability of the objects and/or the queries, i.e., moving or stationary queries issued on moving or stationary objects. We provide theoretical analysis of SEA-CNN with respect to the execution costs, memory requirements and effects of tunable parameters. Comprehensive experimentation shows that SEA-CNN is highly scalable and is more efficient in terms of both I/O and CPU costs in comparison to other R-tree-based CKNN techniques.

#index 800573
#* Architecture and Performance of Application Networking in Pervasive Content Delivery
#@ Mu Su;Chi-Hung Chi
#t 2005
#c 17
#% 70370
#% 316564
#% 330765
#% 482513
#% 577323
#% 622633
#% 664061
#% 723558
#% 1303525
#! This paper proposes Application Networking (App.Net) architecture that enables Web server to deploy intermediate response with associated service logic to edge proxies in form of a workflow. The recipient proxy is allowed to instantiate the workflow using local services or by downloading mobile applications from remote sites. The final response is the output from the workflow execution fed with the intermediate result. In this paper, we defined a workflow modulation method to represent service logic and manipulate it in uniform operations. An App.Net caching method is designed to cache intermediate results as well as final response presentations. According to cost model measuring the bandwidth usage, we designed workflow placement algorithms to deploy intermediate response objects to the proxy in optimal or greedy ways. Finally, we developed an App.Net prototype and implemented a wide range of applications existed in the Web. Our simulation results show that using workflow and dynamic application deployment, App.Net can achieve better performance than conventional methods.

#index 800574
#* A Multiresolution Symbolic Representation of Time Series
#@ Vasileios Megalooikonomou;Qiang Wang;Guo Li;Christos Faloutsos
#t 2005
#c 17
#% 114667
#% 172949
#% 235941
#% 310580
#% 316560
#% 333941
#% 345089
#% 460862
#% 466507
#% 478455
#% 480146
#% 480156
#% 481609
#% 501658
#% 534183
#% 616530
#% 617188
#% 631920
#% 632088
#% 662750
#% 745425
#% 765445
#% 783492
#! Efficiently and accurately searching for similarities among time series and discovering interesting patterns is an important and non-trivial problem. In this paper, we introduce a new representation of time series, the Multiresolution Vector Quantized (MVQ) approximation, along with a new distance function. The novelty of MVQ is that it keeps both local and global information about the original time series in a hierarchical mechanism, processing the original time series at multiple resolutions. Moreover, the proposed representation is symbolic employing key subsequences and potentially allows the application of text-based retrieval techniques into the similarity analysis of time series. The proposed method is fast and scales linearly with the size of database and the dimensionality. Contrary to the vast majority in the literature that uses the Euclidean distance, MVQ uses a multi-resolution/hierarchical distance function. We performed experiments with real and synthetic data. The proposed distance function consistently outperforms all the major competitors (Euclidean, Dynamic Time Warping, Piecewise Aggregate Approximation) achieving up to 20% better precision/recall and clustering accuracy on the tested datasets.

#index 800575
#* Venn Sampling: A Novel Prediction Technique for Moving Objects
#@ Yufei Tao;Dimitris Papadias;Jian Zhai;Qing Li
#t 2005
#c 17
#% 1331
#% 82346
#% 248812
#% 299988
#% 300174
#% 300195
#% 333955
#% 333983
#% 397386
#% 465162
#% 480471
#% 482123
#% 576115
#% 654444
#% 654460
#% 731404
#% 853023
#% 1015320
#! Given a region q_R and a future timestamp q_T, a "range aggregate" query estimates the number of objects expected to appear in q_R at time q_T. Currently the only methods for processing such queries are based on spatio-temporal histograms, which have several serious problems. First, they consume considerable space in order to provide accurate estimation. Second, they incur high evaluation cost. Third, their efficiency continuously deteriorates with time. Fourth, their maintenance requires significant update overhead. Motivated by this, we develop Venn sampling (VS), a novel estimation method optimized for a set of "pivot queries" that reflect the distribution of actual ones. In particular, given m pivot queries, VS achieves perfect estimation with only O(m) samples, as opposed to O(2^m) required by the current state of the art in workload-aware sampling. Compared with histograms, our technique is much more accurate (given the same space), produces estimates with negligible cost, and does not deteriorate with time. Furthermore, it permits the development of a novel "query-driven" update policy, which reduces the update cost of conventional policies significantly.

#index 800576
#* XML Views as Integrity Constraints and Their Use in Query Translation
#@ Rajasekar Krishnamurthy;Raghav Kaushik;Jeffrey F. Naughton
#t 2005
#c 17
#% 333935
#% 333989
#% 348183
#% 378393
#% 397374
#% 465051
#% 480657
#% 745478
#% 1015267
#% 1015271
#% 1016141
#! The SQL queries produced in XML-to-SQL query translation are often unnecessarily complex, even for simple input XML queries. In this paper we argue that relational systems can do a better job of XML-to-SQL query translation with the addition of a simple new constraint, which we term the "lossless from XML" constraint. Intuitively, this constraint states that a given relational data set resulted from the shredding of an XML document that conformed to a given schema. We illustrate the power of this approach by giving an algorithm that exploits the "lossless from XML" constraint to translate path expression queries into efficient SQL, even in the presence of recursive XML schemas. We argue that this approach is likely to be simpler and more effective than the current state of the art in optimizingXML-to-SQL query translation, which involves identifying and declaring multiple complex relational constraints and then reasoning about relational query containment in the presence of these constraints.

#index 800577
#* Full-Fledged Algebraic XPath Processing in Natix
#@ Matthias Brantner;Sven Helmer;Carl-Christian Kanne;Guido Moerkotte
#t 2005
#c 17
#% 136740
#% 210206
#% 397358
#% 397375
#% 482653
#% 487257
#% 495421
#% 562456
#% 570876
#% 584866
#% 584879
#% 654514
#% 659999
#% 993939
#% 1015354
#% 1016150
#! We present the first complete translation of XPath into an algebra, paving the way for a comprehensive, state-of-the-art XPath (and later on, XQuery) compiler based on algebraic optimization techniques. Our translation includes all XPath features such as nested expressions, position-based predicates and node-set functions. The translated algebraic expressions can be executed using the proven, scalable, iterator-based approach, as we demonstrate in form of a corresponding physical algebra in our native XML DBMS Natix. A first glance at performance results shows that even without further optimization of the expressions, we provide a competitive evaluation technique for XPath queries.

#index 800578
#* RDF Aggregate Queries and Views
#@ Edward Hung;Yu Deng;V. S. Subrahmanian
#t 2005
#c 17
#% 152928
#% 279164
#% 462213
#% 479629
#% 487267
#% 655352
#% 738955
#% 993998
#! Resource Description Framework (RDF) is a rapidly expanding web standard. RDF databases attempt to track the massive amounts of web data and services available. In this paper, we study the problem of aggregate queries. We develop an algorithm to compute answers to aggregate queries over RDF databases and algorithms to maintain views involving those aggregates. Though RDF data can be stored in a standard relational DBMS (and hence we can execute standard relational aggregate queries and view maintenance methods on them), we show experimentally that our algorithms that operate directly on the RDF representation exhibit significantly superior performance.

#index 800579
#* Maintaining Implicated Statistics in Constrained Environments
#@ Yannis Sismanis;Nick Roussopoulos
#t 2005
#c 17
#% 2833
#% 69273
#% 189872
#% 210190
#% 248821
#% 278835
#% 299989
#% 333946
#% 348152
#% 378388
#% 378408
#% 379445
#% 428400
#% 453192
#% 480805
#% 481620
#% 481749
#% 496116
#% 519953
#% 646218
#% 654448
#% 654463
#% 765455
#% 993960
#% 1015293
#! Aggregated information regarding implicated entities is critical for online applications like network management, traffic characterization or identifying patters of resource consumption. Recently there has been a flurry of research for online aggregation on streams (like quantiles, hot items, hierarchical heavy hitters) but surprizingly the problem of summarizing implicated information in stream data has received no attention. As an example, consider an IP-network and the implication source 驴 destination. Flash crowds, 驴 such as those that follow recent sport events (like the olympics) or seek information regarding catastrophic events 驴 or denial of service attacks direct a large volume of traffic from a huge number of sources to a very small number of destinations. In this paper we present novel randomized algorithms for monitoring such implications with constraints in both memory and processing power for environments like network routers. Our experiments demonstrate several factors of improvements over straightforward approaches.

#index 800580
#* On the Signature Trees and Balanced Signature Trees
#@ Yangjun Chen
#t 2005
#c 17
#% 1921
#% 6806
#% 77938
#% 115465
#% 115466
#% 152938
#% 179696
#% 209691
#% 227783
#% 249989
#% 259477
#% 287715
#% 288578
#% 328462
#% 408638
#% 427870
#% 463740
#! Advanced database application areas, such as computer aided design, office automation, digital libraries, data-mining as well as hypertext and multimedia systems need to handle complex data structures with set-valued attributes, which can be represented as bit strings, called signatures. A set of signatures can be stored in a file, called a signature file. In this paper, we propose a new method to organize a signature file into a tree structure, called a signature tree, to speed up the signature file scanning and query evaluation.

#index 800581
#* Index Support for Frequent Itemset Mining in a Relational DBMS
#@ Elena Baralis;Tania Cerquitelli;Silvia Chiusano
#t 2005
#c 17
#% 248813
#% 300120
#% 443350
#% 452821
#% 464824
#% 481290
#% 481954
#% 487994
#% 500882
#% 659959
#% 729920
#% 729979
#! Many efforts have been devoted to couple data mining activities with relational DBMSs, but a true integration into the relational DBMS kernel has been rarely achieved. This paper presents a novel indexing technique, which represents transactions in a succinct form, appropriate for tightly integrating frequent itemset mining in a relational DBMS. The data representation is complete, i.e., no support threshold is enforced, in order to allow reusing the index for mining itemsets with any support threshold. Furthermore, an appropriate structure of the stored information has been devised, in order to allow a selective access of the index blocks necessary for the current extraction phase. The index has been implemented into the PostgreSQL open source DBMS and exploits its physical level access methods. Experiments have been run for various datasets, characterized by different data distributions. The execution time of the frequent itemset extraction task exploiting the index is always comparable with and sometime faster than a C++ implementation of the FP-growth algorithm accessing data stored on a flat file.

#index 800582
#* Finding (Recently) Frequent Items in Distributed Data Streams
#@ Amit Manjhi;Vladislav Shkapenyuk;Kedar Dhamdhere;Christopher Olston
#t 2005
#c 17
#% 248812
#% 333925
#% 336610
#% 446438
#% 479795
#% 481290
#% 492912
#% 548479
#% 569754
#% 576112
#% 576119
#% 654443
#% 801695
#% 801696
#% 963799
#% 993960
#! We consider the problem of maintaining frequency counts for items occurring frequently in the union of multiple distributed data streams. Na篓ýve methods of combining approximate frequency counts from multiple nodes tend to result in excessively large data structures that are costly to transfer among nodes. To minimize communication requirements, the degree of precision maintained by each node while counting item frequencies must be managed carefully. We introduce the concept of a precision gradient for managing precision when nodes are arranged in a hierarchical communication structure. We then study the optimization problem of how to set the precision gradient so as to minimize communication, and provide optimal solutions that minimize worst-case communication load over all possible inputs. We then introduce a variant designed to perform well in practice, with input data that does not conform to worst-case characteristics. We verify the effectiveness of our approach empirically using real-world data, and show that our methods incur substantially less communication than na篓ýve approaches while providing the same error guarantees on answers.

#index 800583
#* High-Availability Algorithms for Distributed Stream Processing
#@ Jeong-Hyon Hwang;Magdalena Balazinska;Alexander Rasin;Ugur Cetintemel;Michael Stonebraker;Stan Zdonik
#t 2005
#c 17
#% 300127
#% 378388
#% 399766
#% 459005
#% 660004
#% 674194
#% 726621
#% 765470
#% 993949
#% 1086936
#! Stream-processing systems are designed to support an emerging class of applications that require sophisticated and timely processing of high-volume data streams, often originating in distributed environments. Unlike traditional data-processing applications that require precise recovery for correctness, many stream-processing applications can tolerate and benefit from weaker recovery guarantees. In this paper, we study various recovery guarantees and pertinent recovery techniques that can meet the correctness and performance requirements of stream-processing applications. We discuss the design and algorithmic challenges associated with the proposed recovery techniques and describe how each can provide different guarantees with proper combinations of redundant processing, checkpointing, and remote logging. Using analysis and simulations, we quantify the cost of our recovery guarantees and examine the performance and applicability of the recovery techniques. We also analyze how the knowledge of query network properties can help decrease the cost of high availability.

#index 800584
#* Dynamic Load Distribution in the Borealis Stream Processor
#@ Ying Xing;Stan Zdonik;Jeong-Hyon Hwang
#t 2005
#c 17
#% 220801
#% 408396
#% 412350
#% 726621
#% 884962
#% 963595
#% 979734
#% 1830105
#! Distributed and parallel computing environments are becoming cheap and commonplace. The availability of large numbers of CPUýs makes it possible to process more data at higher speeds. Stream-processing systems are also becoming more important, as broad classes of applications require results in real-time. Since load can vary in unpredictable ways, exploiting the abundant processor cycles requires effective dynamic load distribution techniques. Although load distribution has been extensively studied for the traditional pull-based systems, it has not yet been fully studied in the context of push-based continuous query processing. In this paper, we present a correlation based load distribution algorithm that aims at avoiding overload and minimizing end-to-end latency by minimizing load variance and maximizing load correlation. While finding the optimal solution for such a problem is NP-hard, our greedy algorithm can find reasonable solutions in polynomial time. We present both a global algorithm for initial load distribution and a pair-wise algorithm for dynamic load migration.

#index 800585
#* Deep Store: An Archival Storage System Architecture
#@ Lawrence L. You;Kristal T. Pollack;Darrell D.  E. Long
#t 2005
#c 17
#% 255137
#% 300153
#% 311808
#% 342373
#% 397349
#% 398751
#% 459936
#% 459945
#% 632034
#% 723279
#% 960181
#% 960182
#! We present the Deep Store archival storage architecture, a large-scale storage system that stores immutable dataefficiently and reliably for long periods of time. Archived data is stored across a cluster of nodes and recorded to hard disk. The design differentiates itself from traditional file systems by eliminating redundancy within and across files, distributing content for scalability, associating rich metadata with content, and using variable levels of replication based on the importance or degree of dependency of each piece of stored data. We evaluate the foundations of our design, including PRESIDIO, a virtual content-addressable storage framework with multiple methods for inter-file and intra-file compression that effectively addresses the data-dependent variability of data compression. We measure content and metadata storage efficiency, demonstrate the need for a variable-degree replication model, and provide preliminary results for storage performance.

#index 800586
#* QoSMig: Adaptive Rate-Controlled Migration of Bulk Data in Storage Systems
#@ Koustuv Dasgupta;Sugata Ghosal;Rohit Jain;Upendra Sharma;Akshat Verma
#t 2005
#c 17
#% 70370
#% 250208
#% 263479
#% 300129
#% 300158
#% 300164
#% 320903
#% 330760
#% 338433
#% 341937
#% 443234
#% 459937
#% 459939
#% 463722
#% 528869
#% 577313
#% 663027
#% 770366
#% 830701
#% 835864
#! Logical reorganization of data and requirements of differentiated QoS in information systems necessitate bulk data migration by the underlying storage layer. Such data migration needs to ensure that regular client I/Os are not impacted significantly while migration is in progress. We formalize the data migration problem in a unified admission control framework that captures both the performance requirements of client I/Os and the constraints associated with migration. We propose an adaptive rate-control based data migration methodology, QoSMig, that achieves the optimal client performance in a differentiated QoS setting, while ensuring that the specified migration constraints are met. QoSMig uses both long term averages and short term forecasts of client traffic to compute a migration schedule. We present an architecture based on Service Level Enforcement Discipline for Storage (SLEDS) that supports QoSMig. Our trace-driven experimental study demonstrates that QoSMig provides significantly better I/O performance as compared to existing migration methodologies.

#index 800587
#* Adaptive Overlapped Declustering: A Highly Available Data-Placement Method Balancing Access Load and Space Utilization
#@ Akitsugu Watanabe;Haruo Yokota
#t 2005
#c 17
#% 43172
#% 83130
#% 115661
#% 203481
#% 210175
#% 210180
#% 300164
#% 438292
#% 460873
#% 462779
#% 479920
#% 511819
#% 563321
#% 587699
#% 631965
#% 710798
#! This paper proposes a new data-placement method named Adaptive Overlapped Declustering, which can be applied to a parallel storage system using a value range partitioning-based distributed directory and primary-backup data replication, to improve the space utilization by balancing their access loads. The proposed method reduces data skews generated by data migration for balancing access load. While some data-placement methods capable of balancing access load or reducing data skew have been proposed, both requirements satisfied simultaneously. The proposed method also improves the reliability and availability of the system because it reduces recovery time for damaged backups after a disk failure. The method achieves this acceleration by reducing a large amount of network communications and disk I/O. Mathematical analysis shows the efficiency of space utilization under skewed access workloads. Queuing simulations demonstrated that the proposed method halves backup restoration time, compared with the traditional chained declustering method.

#index 800588
#* Personalized Queries under a Generalized Preference Model
#@ Georgia Koutrika;Yannis Ioannidis
#t 2005
#c 17
#% 59542
#% 278287
#% 300170
#% 342687
#% 345047
#% 399057
#% 399762
#% 413615
#% 465167
#% 509541
#% 654480
#% 731407
#% 745519
#% 765418
#% 993957
#% 994017
#% 1477356
#! Query Personalization is the process of dynamically enhancing a query with related user preferences stored in a user profile with the aim of providing personalized answers. The underlying idea is that different users may find different things relevant to a search due to different preferences. Essential ingredients of query personalization are: (a) a model for representing and storing preferences in user profiles, and (b) algorithms for the generation of personalized answers using stored preferences. Modeling the plethora of preference types is a challenge. In this paper, we present a preference model that combines expressivity and concision. In addition, we provide efficient algorithms for the selection of preferences related to a query, and an algorithm for the progressive generation of personalized results, which are ranked based on user interest. Several classes of ranking functions are provided for this purpose. We present results of experiments both synthetic and with real users (a) demonstrating the efficiency of our algorithms, (b) showing the benefits of query personalization, and (c) providing insight as to the appropriateness of the proposed ranking functions.

#index 800589
#* Increasing the Accuracy and Coverage of SQL Progress Indicators
#@ Gang Luo;Jeffrey F. Naughton;Curt J. Ellmann;Michael W. Watzke
#t 2005
#c 17
#% 152996
#% 172900
#% 248793
#% 252608
#% 393844
#% 411554
#% 463444
#% 480803
#% 571088
#% 617870
#% 765456
#% 765464
#% 765467
#% 765468
#! Recently, progress indicators have been proposed for long-running SQL queries in RDBMSs. Although the proposed techniques work well for a subset of SQL queries, they are preliminary in the sense that (1) they cannot provide non-trivial estimates for some SQL queries, and (2) the provided estimates can be rather imprecise in certain cases. In this paper, we consider the problem of supporting non-trivial progress indicators for a wider class of SQL queries with more precise estimates. We present a set of techniques in achieving this goal. We report an initial implementation of these techniques in PostgreSQL.

#index 800590
#* Robust Identification of Fuzzy Duplicates
#@ Surajit Chaudhuri;Venkatesh Ganti;Rajeev Motwani
#t 2005
#c 17
#% 248801
#% 287222
#% 300136
#% 420072
#% 443394
#% 480654
#% 572265
#% 577238
#% 577247
#% 577263
#% 654467
#% 729913
#% 749509
#% 993980
#! Detecting and eliminating fuzzy duplicates is a critical data cleaning task that is required by many applications. Fuzzy duplicates are multiple seemingly distinct tuples which represent the same real-world entity. We propose two novel criteria that enable characterization of fuzzy duplicates more accurately than is possible with existing techniques. Using these criteria, we propose a novel framework for the fuzzy duplicate elimination problem. We show that solutions within the new framework result in better accuracy than earlier approaches. We present an efficient algorithm for solving instantiations within the framework. We evaluate it on real datasets to demonstrate the accuracy and scalability of our algorithm.

#index 800591
#* Cache-Conscious Automata for XML Filtering
#@ Bingsheng He;Qiong Luo;Byron Choi
#t 2005
#c 17
#% 67164
#% 274595
#% 300194
#% 377480
#% 397362
#% 465061
#% 479819
#% 480296
#% 480821
#% 526938
#% 529630
#% 566122
#% 570879
#% 731408
#% 745501
#% 993947
#% 1015288
#% 1015289
#! Hardware cache behavior is an important factor in the performance of memory-resident, data-intensive systems such as XML filtering engines. A key data structure in several recent XML filters is the automaton, which is used to represent the long-running XML queries in the main memory. In this paper, we study the cache performance of automaton-based XML filtering through analytical modeling and system measurement. Furthermore, we propose a cache-conscious automaton organization technique, called the hot buffer, to improve the locality of automaton state transitions. Our results show that (1) our cache performance model for XML filtering automata is highly accurate and (2) the hot buffer improves the cache performance as well as the overall performance of automaton-based XML filtering.

#index 800592
#* Bloom Filter-Based XML Packets Filtering for Millions of Path Queries
#@ Xueqing Gong;Weining Qian;Ying Yan;Aoying Zhou
#t 2005
#c 17
#% 69791
#% 199575
#% 256883
#% 322884
#% 342372
#% 465061
#% 480296
#% 576098
#% 654476
#% 654477
#% 659987
#% 659995
#% 731408
#! The filtering of XML data is the basis of many complex applications. Lots of algorithms have been proposed to solve this problem [2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 18]. One important challenge is that the number of path queries is huge. It is necessary to take an efficient data structure representing path queries. Another challenge is that these path queries usually vary with time. The maintenance of path queries determines the flexibility and capacity of a filtering system. In this paper, we introduce a novel approximate method for XML data filtering, which uses Bloom filters representing path queries. In this method, millions of path queries can be stored efficiently. At the same time, it is easy to deal with the change of these path queries. To improve the filtering performance, we introduce a new data structure, Prefix Filters, to decrease the number of candidate paths. Experiments show that our Bloom filter-based method takes less time to build routing table than automaton-based method. And our method has a good performance with acceptable false positive when filtering XML packets of relatively small depth with millions of path queries.

#index 800593
#* Batched Processing for Information Filters
#@ Peter M. Fischer;Donald Kossmann
#t 2005
#c 17
#% 64791
#% 86950
#% 206915
#% 271199
#% 333938
#% 427199
#% 461923
#% 464720
#% 465149
#% 480651
#% 527174
#% 731408
#% 993948
#% 1015288
#! This paper describes batching, a novel technique in order to improve the throughput of an information filter (e.g. message broker or publish & subscribe system). Rather than processing each message individually, incoming messages are reordered, grouped and a whole group of similar messages is processed. This paper presents alternative strategies to do batching. Extensive performance experiments are conducted on those strategies in order to compare their tradeoffs.

#index 800594
#* Uncovering Database Access Optimizations in the Middle Tier with TORPEDO
#@ Bruce E. Martin
#t 2005
#c 17
#% 380441
#% 385828
#% 404611
#! A popular architecture for enterprise applications is one of a stateless object-based server accessing persistent data through Object-Relational mapping software. The reported benefits of usingObject-Relational mapping software are increased developer productivity, greater database portability and improved runtime performance over hand-written SQL due to caching. In spite of these supposed benefits, many software architects are suspicious of the "black box" nature of O-R mapping software. Discerning how O-R mapping software actually accesses a database is difficult. The Testbed of Object Relational Products for Enterprise Distributed Objects (TORPEDO) is designed to reveal the sophistication of O-R mapping software in accessing databases in single server and clustered environments. TORPEDO defines a set of realistic application level operations that detect significant set of database access optimizations. TORPEDO supports two standard Java APIs for O-R mapping, namely, Container Managed Persistence (CMP 2.0) and Java Data Objects (JDO). TORPEDO also supports the TopLink and Hibernate APIs. There are dozens of commercial and open-source O-R mapping products supporting these APIs. Results from running TORPEDO on different O-R mapping systems are comparable. We provide sample results from running TORPEDO on popular O-R mapping solutions. We describe why the optimizations TORPEDO reveals are important and how the application level operations detect the optimizations

#index 800595
#* Load and Network Aware Query Routing for Information Integration
#@ Wen-Syan Li;Vishal S. Batra;Vijayshankar Raman;Wei Han;K. Selcuk Candan;Inderpal Narang
#t 2005
#c 17
#% 36117
#% 201976
#% 238413
#% 300138
#% 300166
#% 300167
#% 397393
#% 427030
#% 443235
#% 462025
#% 463728
#% 464056
#% 479449
#% 480788
#% 480803
#% 564419
#% 571294
#% 614579
#% 765469
#% 1015282
#! Current federated systems deploy cost-based query optimization mechanisms; i.e., the optimizer selects a global query plan with the lowest cost to execute. Thus, cost functions influence what remote sources (i.e. equivalent data sources) to access and how federated queries are processed. In most federated systems, the underlying cost model is based on database statistics and query statements; however, the system load of remote sources and the dynamic nature of the network latency in wide area networks are not considered. As a result, federated query processing solutions can not adapt to runtime environment changes, such as network congestion or heavy workloads at remote sources. We present a novel system architecture that deploys a Query Cost Calibrator to calibrate the cost function based on system load and network latency at the remote sources and consequently indirectly "influences" query routing and load distribution in federated information systems.

#index 800596
#* Predicate Derivation and Monotonicity Detection in DB2 UDB
#@ T. Malkemus;S. Padmanabhan;B. Bhattacharjee;L. Cranston
#t 2005
#c 17
#% 442706
#% 654495
#! DB2 Universal Database allows database schema designers to specify generated columns. These generated columns are useful for maintaining rollup hierarchy variables in warehouses (e.g., date, month, quarter). In order for the generated columns to be useful for query processing, queries must automatically make use of such columns when applicable. In particular, query predicates on the original columns should be rewritten to make use of the generated columns. In this paper, we describe two main aspects of this predicate rewriting technique that allows usage of the generated columns for a variety of query predicate types. The first aspect, monotonicity detection, allows for rewrites in the case of range predicates. The second aspect, predicate derivation, is the technique for using generating expressions for query processing. We show the value of this technique for providing significant performance improvement when combined with indexing or multidimensional clustering in DB2.

#index 800597
#* TRMeister: A DBMS with High-Performance Full-Text Search Functions
#@ Tetsuya Ikeda;Hiroko Mano;Hideo Itoh;Hiroshi Takegawa;Takuya Hiraoka;Shiroh Horibe;Yasushi Ogawa
#t 2005
#c 17
#% 92696
#% 172922
#% 232645
#% 262144
#% 464071
#% 481439
#! TRMeister is a DBMS with high-performance full-text search functions. With TRMeister, high-speed full-text search, including high-precision ranking search in addition to Boolean search, is possible. Further, in addition to search, high-speed insert and delete are possible, allowing full-text search to be used in the same way as other types of database search in which data can be searched right after data is inserted. This makes it easy to combine normal attribute search with full-text search and thus easily create text search applications.

#index 800598
#* Acceleration Technique of Snake-Shaped Regions Retrieval Method for Telematics Navigation Service System
#@ Masaaki Tanizaki;Kishiko Maruyama;Shigeru Shimada
#t 2005
#c 17
#% 86950
#% 227939
#% 427199
#! Telematics services, which provide traffic information such as route guidance, congestion warnings, etc. via a wireless communication network, have spread recently. The demand is growing for graphical guide information to be provided in addition to the conventional service that provides text only guidance. To improve graphical service, we propose a new retrieval method. This method enables fast extraction of map objects within a Snake-Shaped Region (SSR) along a driving route from a geo-spatial database that stores map data without rectangular mesh boundaries. For this retrieval method, we have considered three techniques. The first is based on simplification of the Snake-Shaped route Region through point elimination, and the second is based on reduction of the processing load of the geometrical intersection detection processes. This second technique is accomplished by dividing the Snake-Shaped Region into multiple cells, and the third is multiple distributions of the SSR retrieval result to terminals for quick start of navigation processing. We have developed a prototype to evaluate the performance of the proposed methods. The prototype provides route guidance information for an actual terminal, and uses information taken from United States road maps. Even in an urban area, we managed to provide an approximately 200-mile route of guide information within 10 seconds. We are convinced that the proposed method can be applied to actual Telematics services.

#index 800599
#* Distributed XML Stream Filtering System with High Scalability
#@ Hiroyuki Uchiyama;Makoto Onizuka;Takashi Honishi
#t 2005
#c 17
#% 291299
#% 342372
#% 378393
#% 465061
#% 480296
#% 480649
#% 564264
#% 578560
#% 612477
#% 640616
#% 654476
#% 659995
#% 730053
#% 1015276
#% 1016180
#! We propose a distributed XML stream filtering system that uses a large number of subscribersý profiles, written in XPath expressions, to filter XML streams and then publish the filtered data in real-time. To realize the proposed system, we define XPath expression features on XML data and utilize them to forecast the serversý loads. Our method is realized by combining methods to share the total transfer loads of each filtering server and to equalize the sum of overlap size between filtering servers. Experiments show that the rate at which the publishing time increases with the number of XPath expressions is three times smaller in the proposed system than in the round-robin method. Furthermore, the overhead of the proposed method is quite low.

#index 800600
#* Network-Based Problem Detection for Distributed Systems
#@ Hisashi Kashima;Tadashi Tsumura;Tsuyoshi Ide;Takahide Nogayama;Ryo Hirade;Hiroaki Etoh;Takeshi Fukuda
#t 2005
#c 17
#% 79312
#% 152934
#% 236359
#% 310552
#% 449074
#% 463903
#% 546061
#% 577295
#% 769920
#% 857390
#% 1756868
#% 1829875
#! We introduce a network-based problem detection framework for distributed systems, which includes a data-mining method for discovering dynamic dependencies among distributed services from transaction data collected from network, and a novel problem detection method based on the discovered dependencies. From observed containments of transaction execution time periods, we estimate the probabilities of accidental and non-accidental containments, and build a competitive model for discovering direct dependencies by using a model estimation method based on the online EM algorithm. Utilizing the discovered dependency information, we also propose a hierarchical problem detection framework, where microscopic dependency information is incorporated with a macroscopic anomaly metric that monitors the behavior of the system as a whole. This feature is made possible by employing a network-based design which provides overall information of the system without any impact on the performance.

#index 800601
#* Towards an Industrial Strength SQL/XML Infrastructure
#@ Muralidhar Krishnaprasad;Zhen Hua Liu;Anand Manikutty;James W. Warner;Vikas Arora
#t 2005
#c 17
#% 333935
#% 380546
#% 411759
#% 480152
#% 480657
#% 1015339
#% 1016223
#! XML has become an attractive data processing model for applications. SQL/XML is a SQL standard that integrates XML with SQL. It introduces the XML datatype as a native SQL datatype and defines XML generation functions in the SQL/XML 2003 standard. The goal for the next version of SQL/XML is integrating XQuery with SQL by supporting XQuery embedded inside SQL functions such as the XMLQuery and XMLTable functions. Starting with the 9i database release, Oracle has supported the XML datatype and various operations on XML instances. In this paper, we present the design and implementation strategies of the SQL/XML standard in Oracle XMLDB. We explore the various critical infrastructures needed in the SQL database kernel to support an efficient native XML datatype implementation and the design approaches for efficient generation, query and update of the XML instances. Furthermore, we also illustrate extensions to SQL/XML that makes Oracle XMLDB a truly industrial strength platform for XML processing.

#index 800602
#* Distributed/Heterogeneous Query Processing in Microsoft SQL Server
#@ Jose A. Blakeley;Conor Cunningham;Nigel Ellis;Balaji Rathakrishnan;Ming-Chuan Wu
#t 2005
#c 17
#% 1909
#% 210178
#% 248832
#% 286916
#% 316872
#% 384349
#% 433305
#% 464209
#% 479452
#% 614579
#% 765472
#! This paper presents an architecture overview of the distributed, heterogeneous query processor (DHQP) in the Microsoft SQL Server database system to enable queries over a large collection of diverse data sources. The paper highlights three salient aspects of the architecture. First, the system introduces well-defined abstractions such as connections, commands, and rowsets that enable sources to plug into the system. These abstractions are formalized by the OLE DB data access interfaces. The generality of OLE DB and its broad industry adoption enables our system to reach a very large collection of diverse data sources ranging from personal productivity tools, to database management systems, to file system data. Second, the DHQP is built-in to the relational optimizer and execution engine of the system. This enables DH queries and updates to benefit from the cost-based algebraic transformations and execution strategies available in the system. Finally, the architecture is inherently extensible to support new data sources as they emerge as well as serves as a key extensibility point for the relational engine to add new features such as full-text search and distributed partitioned views.

#index 800603
#* Extending Relational Database Systems to Automatically Enforce Privacy Policies
#@ Rakesh Agrawal;Paul Bird;Tyrone Grandison;Jerry Kiernan;Scott Logan;Walid Rjaibi
#t 2005
#c 17
#% 252481
#% 606353
#% 993943
#% 1016138
#! Databases are at the core of successful businesses. Due to the voluminous stores of personal data being held by companies today, preserving privacy has become a crucial requirement for operating a business. This paper proposes how current relational database management systems can be transformed into their privacy-preserving equivalents. Specifically, we present language constructs and implementation design for fine-grained access control to achieve this goal.

#index 800604
#* Design, Implementation, and Evaluation of a Repairable Database Management System
#@ Tzi-cker Chiueh;Dhruv Pilania
#t 2005
#c 17
#% 114582
#% 286472
#% 300458
#% 438292
#% 443542
#% 459945
#% 646043
#% 674157
#% 963646
#% 963831
#! Although conventional database management systems are designed to tolerate hardware and to a lesser extent even software errors, they cannot protect themselves against syntactically correct and semantically damaging transactions, which could arise because of malicious attacks or honest mistakes. The lack of fast post-intrusion or post-error damage repair in modern DBMSs results in a longer Mean Time to Repair (MTTR) and sometimes permanent data loss that could have been saved by more intelligent repair mechanisms. In this paper, we describe the design and implementation of Phoenix - a system that significantly improves the efficiency and precision of a database damage repair process after an intrusion or operator error and thus, increases the overall database system availability. The two key ideas underlying Phoenix are (1) maintaining persistent inter-transaction dependency information at run time to allow selective undo of database transactions that are considered "infected" by the intrusion or error in question and (2) exploiting information present in standard database logs for fast selective undo. Performance measurements on a fully operational Phoenix prototype, which is based on the PostgreSQL DBMS, demonstrate that Phoenix incurs a response time and a throughput penalty of less than 5% and 8%, respectively, under the TPC-C benchmark, but it can speed up the post-intrusion database repair process by at least an order of magnitude when compared with a manual repair process.

#index 800605
#* NFMi: An Inter-domain Network Fault Management System
#@ Qingchun Jiang;Raman Adaikkalavan;Sharma Chakravarthy
#t 2005
#c 17
#% 177755
#% 273945
#% 300179
#% 304708
#% 481448
#% 611024
#% 660004
#% 726621
#% 730044
#% 737352
#% 1016169
#% 1830476
#! Network fault management has been an active research area for a long period of time because of its complexity, and the returns it generates for service providers. However, most fault management systems are currently custom-developed for a particular domain. As communication service providers continuously add greater capabilities and sophistication to their systems in order to meet demands of a growing user population, these systems have to manage a multi-layered network along with its built-in legacy logical processing procedure. Stream processing has been receiving a lot of attention to deal with applications that generate large amounts of data in real-time at varying input rates and to compute functions over multiple streams, such as network fault management. In this paper, we propose an integrated inter-domain network fault management system for such a multi-layered network based on data stream and event processing techniques. We discuss various components in our system and how data stream processing techniques are used to build a flexible system for a sophisticated real-world application. We further identify a number of important issues related to data stream processing during the course of the discussion of our proposed system, which will further extend the boundaries of data stream processing.

#index 800606
#* SVL: Storage Virtualization Engine Leveraging DBMS Technology
#@ Lin Qiao;Balakrishna R. Iyer;Divyakant Agrawal;Amr El Abbadi
#t 2005
#c 17
#% 112182
#% 128177
#% 159275
#% 201956
#% 340700
#% 397391
#% 459945
#% 464843
#% 479480
#% 480310
#% 481424
#% 481926
#% 523876
#% 612173
#% 659992
#% 771301
#% 824543
#% 830701
#% 830703
#% 1015332
#% 1088888
#! The demands on storage systems are increasingly requiring expressiveness, fault-tolerance, security, distribution, etc. Such functionalities have been traditionally provided by DBMS. We propose a storage management system, SVL, that leverages DBMS technology. The primary problem in block storage management is block virtualization which is essentially an abstraction layer that separates the user view of storage from the implementation of storage. Storage virtualization standardizes storage management in a heterogeneous storage and/or host environment, and plays a crucial role in enhancing storage functionality and utilization. Currently specialized hardware or microcode based solutions are popular for implementing block storage management systems, commonly referred to as disk controllers. We demonstrate how to take a general purpose commercial RDBMS, rather than a specialized solution, to support block storage management. We exploit the simple semantics of storage management systems to streamline database performance and thus attain acceptability from a storage point of view. This work promises to pave the way for diverse and innovative industrial applications of database management systems.

#index 800607
#* Knowledge Discovery from Transportation Network Data
#@ Wei Jiang;Jaideep Vaidya;Zahir Balaporia;Chris Clifton;Brett Banich
#t 2005
#c 17
#% 124679
#% 274612
#% 290482
#% 408396
#% 420073
#% 420075
#% 466644
#% 478274
#% 480635
#% 481290
#% 577214
#% 577218
#% 629630
#% 629646
#% 629656
#% 629708
#% 1268739
#! Transportation and Logistics are a major sector of the economy, however data analysis in this domain has remained largely in the province of optimization. The potential of data mining and knowledge discovery techniques is largely untapped. Transportation networks are naturally represented as graphs. This paper explores the problems in mining of transportation network graphs: We hope to find how current techniques both succeed and fail on this problem, and from the failures, we hope to present new challenges for data mining. Experimental results from applying both existing graph mining and conventional data mining techniques to real transportation network data are provided, including new approaches to making these techniques applicable to the problems. Reasons why these techniques are not appropriate are discussed. We also suggest several challenging problems to precipitate research and galvanize future work in this area.

#index 800608
#* Sentiment Mining in WebFountain
#@ Jeonghee Yi;Wayne Niblack
#t 2005
#c 17
#% 118040
#% 180254
#% 220707
#% 279755
#% 330677
#% 330701
#% 342650
#% 348180
#% 529193
#% 577246
#% 577337
#% 577355
#% 727877
#% 730042
#% 740900
#% 740916
#% 746885
#% 770307
#% 815915
#% 854646
#% 938686
#% 938687
#% 1788189
#! WebFountain is a platform for very large-scale text analytics applications that allows uniform access to a wide variety of sources. It enables the deployment of a variety of document-level and corpus-level miners in a scalable manner, and feeds information that drives end-user applications through a set of hosted Web services. Sentiment (or opinion) mining is one of the most useful analyses for various end-user applications, such as reputation management. Instead of classifying the sentiment of an entire document about a subject, our sentiment miner determines sentiment of each subject reference using natural language processing techniques. In this paper, we describe the fully functional system environment and the algorithms, and report the performance of the sentiment miner. The performance of the algorithms was verified on online product review articles, and more general documents including Web pages and news articles.

#index 800609
#* iBOM: A Platform for Intelligent Business Operation Management
#@ Malu Castellanos;Fabio Casati;Ming-Chien Shan;Umesh Dayal
#t 2005
#c 17
#% 136350
#% 316709
#% 466410
#! As IT systems become more and more complex and as business operations become increasingly automated, there is a growing need from business managers to have better control on business operations and on how these are aligned with business goals. This paper describes iBOM, a platform for business operation management developed by HP that allows users to i) analyze operations from a business perspective and manage them based on business goals; ii) define business metrics, perform intelligent analysis on them to understand causes of undesired metric values, and predict future values; iii) optimize operations to improve business metrics. A key aspect is that all this functionality is readily available almost at the click of the mouse. The description of the work proceeds from some specific requirements to the solution developed to address them. We also show that the platform is indeed general, as demonstrated by subsequent deployment domains other than finance.

#index 800610
#* Towards Building a MetaQuerier: Extracting and Matching Web Query Interfaces
#@ Bin He;Zhen Zhang;Kevin Chen-Chuan Chang
#t 2005
#c 17
#% 654459
#% 765410
#% 769890
#% 783472
#% 783791
#% 1015284

#index 800611
#* DCbot: Exploring the Web as Value-Added Service for Location-Based Applications
#@ Mihaly Jakob;Matthias Grossmann;Nicola Honle;Daniela Nicklas
#t 2005
#c 17
#% 480467
#% 640097

#index 800612
#* RelaxImage: A Cross-Media Meta-Search Engine for Searching Images from Web Based on Query Relaxation
#@ Akihiro Kuwabara;Katsumi Tanaka
#t 2005
#c 17
#! We introduce a cross-media meta-search engine RelaxImage for searching images from Web. Notable features of the RelaxImage are as follows: (1) Each userýs keyword query is "relaxed", that is, by gradually relaxing the search terms used for image search, we can solve the problem of conventional image search engine such as Google. (2) For searching images, our RelaxImage sends a different keyword-query to each search engine of different media-type. We show several examples of how the relaxation approach works as well as ways that it can be applied. That is, our RelaxImage shows a great improvement for increasing recall ratio without decreasing of precision ratio.

#index 800613
#* Odysseus: A High-Performance ORDBMS Tightly-Coupled with IR Features
#@ Kyu-Young Whang;Min-Jae Lee;Jae-Gil Lee;Min-Soo Kim;Wook-Shin Han
#t 2005
#c 17
#% 1921
#% 116073
#% 654442
#% 1015273
#! We propose the notion of tight-coupling [8] to add new data types into the DBMS engine. In this paper, we introduce the Odysseus ORDBMS and present its tightly-coupled IR features (U.S. patented). We demonstrate a web search engine capable of managing 20 million web pages in a non-parallel configuration using Odysseus.

#index 800614
#* Querying and Visualizing Gridded Datasets for e-Science
#@ Bill Howe;David Maier
#t 2005
#c 17
#% 1016206

#index 800615
#* Scrutinizing Frequent Pattern Discovery Performance
#@ Osmar R. Zaiane;Mohammad El-Hajj;Yi Li;Stella Luk
#t 2005
#c 17
#% 248791
#% 274146
#% 300120
#% 310558
#% 342643
#% 465003
#% 466664
#% 481290
#% 577215
#% 729920
#% 729933
#! Benchmarking technical solutions is as important as the solutions themselves. Yet many fields still lack any type of rigorous evaluation. Performance benchmarking has always been an important issue in databases and has played a significant role in the development, deployment and adoption of technologies. To help assessing the myriad algorithms for frequent itemset mining, we built an open framework and testbed to analytically study the performance of different algorithms and their implementations, and contrast their achievements given different data characteristics, different conditions, and different types of patterns to discover and their constraints. This facilitates reporting consistent and reproducible performance results using known conditions.

#index 800616
#* ModelGen: Model Independent Schema Translation
#@ Paolo Atzeni;Paolo Cappellari;Philip A. Bernstein
#t 2005
#c 17
#% 38696
#% 227985
#% 458995
#% 480429
#! A customizable and extensible tool is proposed to implement ModelGen, the model management operator that translates a schema from one model to another. A wide family of models is handled, by using a metamodel in which models can be succinctly and precisely described. The approach is novel because the tool exposes the dictionary that stores models, schemas, and the rules used to implement translations. In this way, the transformations can be customized and the tool can be easily extended.

#index 800617
#* Adaptive Process Management with ADEPT2
#@ Manfred Reichert;Stefanie Rinderle;Ulrich Kreher;Peter Dadam
#t 2005
#c 17
#% 261267
#% 753124
#% 787270

#index 800618
#* SECONDO: An Extensible DBMS Platform for Research Prototyping and Teaching
#@ Ralf Hartmut Guting;Victor Almeida;Dirk Ansorge;Thomas Behr;Zhiming Ding;Thomas Hose;Frank Hoffmann;Markus Spiekermann;Ulrich Telle
#t 2005
#c 17
#% 495249

#index 800619
#* ViteX: A Streaming XPath Processing System
#@ Yi Chen;Susan B. Davidson;Yifeng Zheng
#t 2005
#c 17
#! We present ViteX, an XPath processing system on XML streams with polynomial time complexity. ViteX uses a polynomial-space data structure to encode an exponential number of pattern matches (in the query size) which are required to process queries correctly during a single sequential scan of XML. Then ViteX computes query solutions by probing the data structure in a lazy fashion without enumerating pattern matches.

#index 800620
#* The XML Stream Query Processor SPEX
#@ Francois Bry;Fatih Coskun;Serap Durmaz;Tim Furche;Dan Olteanu;Markus Spannagel
#t 2005
#c 17
#% 487257
#% 1015373

#index 800621
#* Constructing and Querying Peer-to-Peer Warehouses of XML Resources
#@ Serge Abiteboul;Ioana Manolescu;Nicoleta Preda
#t 2005
#c 17
#% 1388056
#! We present KADOP, a distributed infrastructure for warehousing XML resources in a peer-to-peer framework. KADOP allows users to build a shared, distributed repository of resources such as XML documents, semantic information about such documents, Web services, and collections of such items. KADOP leverages several existing technologies and models: it uses distributed hash tables as a peer communication layer, and ActiveXML as a model for constructing and querying the resources in the peer network.

#index 800622
#* XGuard: A System for Publishing XML Documents without Information Leakage in the Presence of Data Inference
#@ Xiaochun Yang;Chen Li;Ge Yu;Lei Shi
#t 2005
#c 17
#% 344639
#% 745436
#% 765449
#% 1016137

#index 800623
#* Online Latent Variable Detection in Sensor Networks
#@ Jimeng Sun;Spiros Papadimitriou;Christos Faloutsos
#t 2005
#c 17
#! Sensor networks attract increasing interest, for a broad range of applications. Given a sensor network, one key issue becomes how to utilize it efficiently and effectively. In particular, how can we detect the underlying correlations (latent variables) among many co-evolving sensor measurements? Can we do it incrementally? We present a system that can (1) collect the measurements from the real wireless sensors; (2) process them in real-time; and (3) determine the correlations (latent variables) among the sensor streams on the fly.

#index 800624
#* Spatiotemporal Annotation Graph (STAG): A Data Model for Composite Digital Objects
#@ Smriti Yamini;Amarnath Gupta
#t 2005
#c 17
#! In this demonstration, we present a database over complex documents, which, in addition to a structured text content, also has update information, annotations, and embedded objects. We propose a new data model called Spatio-temporal Annotation Graphs (STAG) for a database of composite digital objects and present a system that shows a query language to efficiently and effectively query such database. The particular application to be demonstrated is a database over annotated MS Word and PowerPoint presentations with embedded multimedia objects.

#index 800625
#* MoDB: Database System for Synthesizing Human Motion
#@ Timothy Edmunds;S. Muthukrishnan;Subarna Sadhukhan;Shinjiro Sueda
#t 2005
#c 17
#% 398426
#% 398427
#% 398428
#% 662807
#! Enacting and capturing real motion for all potential scenarios is prohibitively expensive; hence, there is a great demand to synthetically generate realistic human motion. However, it is a central challenge in character animation to synthetically generate a large sequence of smooth human motion. We present a novel, database-centric solution to address this challenge.We demonstrate a method of generating long sequences of motion by performing various similarity-based "joins" on a database of captured motion sequences. This demo illustrates our system (MoDB) and show-cases the process of encoding captured motion into relational data and generating realistic motion by concatenating sub-sequences of the captured data according to feasibility metrics. The demo features an interactive character that moves towards user-specified targets; the characterýs motion is generated by relying on the real time performance of the database for indexing and selection of feasible sub-sequences.

#index 800626
#* An Enhanced Query Model for Soccer Video Retrieval Using Temporal Relationships
#@ Shu-Ching Chen;Mei-Ling Shyu;Na Zhao
#t 2005
#c 17
#% 319244
#% 480808
#% 733375
#% 1775377

#index 800627
#* Spatio-Temporal Databases in Practice: Directly Supporting Previously Developed Land Data Using Tripod
#@ Tony Griffiths;Alvaro A.  A. Fernandes;Norman W. Paton;Seung-Hyun Jeong;Nassima Djafri;Keith T. Mason
#t 2005
#c 17
#% 755897

#index 800628
#* Fuzzy Spatial Objects: An Algebra Implementation in SECONDO
#@ Thomas Behr;Ralf Hartmut Guting
#t 2005
#c 17
#% 152941
#% 495249
#! This paper describes a data model for fuzzy spatial objects implemented as an algebra module in SECONDO. Furthermore, the graphical representation of such objects is discussed.

#index 800629
#* Paradigm Shift to New DBMS Architectures: Research Issues and Market Needs
#@ Anastassia Ailamaki;Yoshinori Hara;Vishal Sikka;Sang K. Cha
#t 2005
#c 17

#index 800630
#* Panel on Business Process Intelligence
#@ Malu Castellanos;Fabio Casati
#t 2005
#c 17

#index 800632
#* Rank-Aware Query Processing and Optimization
#@ Ihab F. Ilyas;Walid G. Aref
#t 2005
#c 17

#index 800633
#* Data Stream Query Processing
#@ Nick Koudas;Divesh Srivastava
#t 2005
#c 17
#! This tutorial provides a comprehensive and cohesive overview of the key research results in the area of data stream query processing, both for SQL-like and XML query languages.

#index 800634
#* Online Mining of Data Streams: Applications, Techniques and Progress
#@ Haixun Wang;Jian Pei;Philip S. Yu
#t 2005
#c 17

#index 800635
#* Web Services and Service-Oriented Architectures
#@ Gustavo Alonso;Fabio Casati
#t 2005
#c 17
#% 1561977

#index 800636
#* Database Architectures for New Hardware
#@ Anastassia Ailamaki
#t 2005
#c 17
#! Thirty years ago, DBMS stored data on disks and cached recently used data in main memory buffer pools, while designers worried about improving I/O performance and maximizing main memory utilization. Today, however, databases live in multi-level memory hierarchies that include disks, main memories, and several levels of processor caches. Recent research shows that database performance is directly influenced by all levels of the underlying computer hardware and devices. This tutorial aims at (a) explaining why database performance depends on modern processor and memory microarchitectures, (b) surveying and contrasting research on the topic over the past decade, and (c) discussing future research challenges.

#index 800637
#* Data Mining Techniques for Microarray Datasets
#@ Lei Liu;Jiong Yang;Anthony K.  H. Tung
#t 2005
#c 17

#index 864292
#* Proceedings of the 22nd International Conference on Data Engineering
#@ 
#t 2006
#c 17

#index 864388
#* MiniCount: Efficient Rewriting of COUNT-Queries Using Views
#@ Vaclav Lin;Vasilis Vassalos;Prodromos Malakasiotis
#t 2006
#c 17
#! We present MiniCount, the first efficient sound and complete algorithm for finding maximally contained rewritings of conjunctive queries with count, using conjunctive views with count and conjunctive views without aggregation. An efficient and scalable solution to this problem yields significant benefits for data warehousing and decision support systems, as well as for powerful data integration systems.We first present a naive rewriting algorithm implicit in the recent theoretical results by Cohen et al. [5] and identify three independent sources of exponential complexity in the naive algorithm, including an expensive containment check. Then we present and discuss MiniCount and prove it sound and complete. We also present an experimental study that shows Mini- Count to be orders of magnitude faster than the naive algorithm, and to be able to scale to large numbers of views

#index 864389
#* Updates Through Views: A New Hope
#@ Yannis Kotidis;Divesh Srivastava;Yannis Velegrakis
#t 2006
#c 17
#! Database views are extensively used to represent unmaterialized tables. Applications rarely distinguish between a materialized base table and a virtual view, thus, they may issue update requests on the views. Since views are virtual, update requests on them need to be translated to updates on the base tables. Existing literature has shown the difficulty of translating view updates in a side-effect free manner. To address this problem, we propose a novel approach for separating the data instance into a logical and a physical level. This separation allows us to achieve side-effect free translations of any kind of update on the view. Furthermore, deletes on a view can be translated without affecting the base tables. We describe the implementation of the framework and present our experimental results

#index 864390
#* Learning from Aggregate Views
#@ Bee-Chung Chen;Lei Chen;Raghu Ramakrishnan;David R. Musicant
#t 2006
#c 17
#! In this paper, we introduce a new class of data mining problems called learning from aggregate views. In contrast to the traditional problem of learning from a single table of training examples, the new goal is to learn from multiple aggregate views of the underlying data, without access to the un-aggregated data. We motivate this new problem, present a general problem framework, develop learning methods for RFA (Restriction-Free Aggregate) views defined using COUNT, SUM, AVG and STDEV, and offer theoretical and experimental results that characterize the proposed methods.

#index 864391
#* C-Cubing: Efficient Computation of Closed Cubes by Aggregation-Based Checking
#@ Dong Xin;Zheng Shao;Jiawei Han;Hongyan Liu
#t 2006
#c 17
#! It is well recognized that data cubing often produces huge outputs. Two popular efforts devoted to this problem are (1) iceberg cube, where only significant cells are kept, and (2) closed cube, where a group of cells which preserve roll-up/drill-down semantics are losslessly compressed to one cell. Due to its usability and importance, efficient computation of closed cubes still warrants a thorough study. In this paper, we propose a new measure, called closedness, for efficient closed data cubing. We show that closedness is an algebraic measure and can be computed efficiently and incrementally. Based on closedness measure, we develop an an aggregation-based approach, called C-Cubing (i.e., Closed-Cubing), and integrate it into two successful iceberg cubing algorithms: MM-Cubing and Star-Cubing. Our performance study shows that C-Cubing runs almost one order of magnitude faster than the previous approaches. We further study how the performance of the alternative algorithms of C-Cubing varies w.r.t the properties of the data sets.

#index 864392
#* A Primitive Operator for Similarity Joins in Data Cleaning
#@ Surajit Chaudhuri;Venkatesh Ganti;Raghav Kaushik
#t 2006
#c 17
#! Data cleaning based on similarities involves identification of "close" tuples, where closeness is evaluated using a variety of similarity functions chosen to suit the domain and application. Current approaches for efficiently implementing such similarity joins are tightly tied to the chosen similarity function. In this paper, we propose a new primitive operator which can be used as a foundation to implement similarity joins according to a variety of popular string similarity functions, and notions of similarity which go beyond textual similarity. We then propose efficient implementations for this operator. In an experimental evaluation using real datasets, we show that the implementation of similarity joins using our operator is comparable to, and often substantially better than, previous customized implementations for particular similarity functions.

#index 864393
#* Techniques for Warehousing of Sample Data
#@ Paul G. Brown;Peter J. Haas
#t 2006
#c 17
#! We consider the problem of maintaining a warehouse of sampled data that "shadows" a full-scale data warehouse, in order to support quick approximate analytics and metadata discovery. The full-scale warehouse comprises many "data sets," where a data set is a bag of values; the data sets can vary enormously in size. The values constituting a data set can arrive in batch or stream form. We provide and compare several new algorithms for independent and parallel uniform random sampling of data-set partitions, where the partitions are created by dividing the batch or splitting the stream. We also provide novel methods for merging samples to create a uniform sample from an arbitrary union of data-set partitions. Our sampling/merge methods are the first to simultaneously support statistical uniformity, a priori bounds on the sample footprint, and concise sample storage. As partitions are rolled in and out of the warehouse, the corresponding samples are rolled in and out of the sample warehouse. In this manner our sampling methods approximate the behavior of more sophisticated stream-sampling methods, while also supporting parallel processing. Experiments indicate that our methods are efficient and scalable, and provide guidance for their application.

#index 864394
#* Working Models for Uncertain Data
#@ Anish Das Sarma;Omar Benjelloun;Alon Halevy;Jennifer Widom
#t 2006
#c 17
#! We present MiniCount, the first efficient sound and complete algorithm for finding maximally contained rewritings of conjunctive queries with count, using conjunctive views with count and conjunctive views without aggregation. An efficient and scalable ...

#index 864395
#* Reasoning About Approximate Match Query Results
#@ Sudipto Guha;Nick Koudas;Divesh Srivastava;Xiaohui Yu
#t 2006
#c 17
#! Join techniques deploying approximate match predicates are fundamental data cleaning operations. A variety of predicates have been utilized to quantify approximate match in such operations and some have been embedded in a declarative data cleaning framework. These techniques return pairs of tuples from both relations, tagged with a score, signifying the degree of similarity between the tuples in the pair according to the specific approximate match predicate. In this paper, we consider the problem of estimating various parameters on the output of declarative approximate join algorithms for planning purposes. Such algorithms are highly time consuming, so precise knowledge of the result size as well as its score distribution is a pressing concern. This knowledge aids decisions as to which operations are more promising for identifying highly similar tuples, which is a key operation for data cleaning. We propose solution strategies that fully comply with a declarative framework and analytically reason about the quality of the estimates we obtain as well as the performance of our strategies. We present the results of a detailed performance evaluation of all strategies proposed. Our experimental results validate our analytical expectations and shed additional light on the quality and performance of our estimation framework. Our study offers a set of simple, fully declarative techniques for this problem, which can be readily deployed in data cleaning systems.

#index 864396
#* The Gauss-Tree: Efficient Object Identification in Databases of Probabilistic Feature Vectors
#@ Christian Bohm;Alexey Pryakhin;Matthias Schubert
#t 2006
#c 17
#! In applications of biometric databases the typical task is to identify individuals according to features which are not exactly known. Reasons for this inexactness are varying measuring techniques or environmental circumstances. Since these circumstances are not necessarily the same when determining the features for different individuals, the exactness might strongly vary between the individuals as well as between the features. To identify individuals, similarity search on feature vectors is applicable, but even the use of adaptable distance measures is not capable to handle objects having an individual level of exactness. Therefore, we develop a comprehensive probabilistic theory in which uncertain observations are modeled by probabilistic feature vectors (pfv), i.e. feature vectors where the conventional feature values are replaced by Gaussian probability distribution functions. Each feature value of each object is complemented by a variance value indicating its uncertainty. We define two types of identification queries, k-mostlikely identification and threshold identification. For efficient query processing, we propose a novel index structure, the Gauss-tree. Our experimental evaluation demonstrates that pfv stored in a Gauss-tree significantly improve the result quality compared to traditional feature vectors. Additionally, we show that the Gauss-tree significantly speeds up query times compared to competitive methods.

#index 864397
#* Finding Fastest Paths on A Road Network with Speed Patterns
#@ Evangelos Kanoulas;Yang Du;Tian Xia;Donghui Zhang
#t 2006
#c 17
#! This paper proposes and solves the Time-Interval All Fastest Path (allFP) query. Given a user-defined leaving or arrival time interval I, a source node s and an end node e, allFP asks for a set of all fastest paths from s to e, one for each sub-interval of I. Note that the query algorithm should find a partitioning of I into sub-intervals. Existing methods can only be used to solve a very special case of the problem, when the leaving time is a single time instant. A straightforward solution to the allFP query is to run existing methods many times, once for every time instant in I. This paper proposes a solution based on novel extensions to the A* algorithm. Instead of expanding the network many times, we expand once. The travel time on a path is kept as a function of leaving time. Methods to combine travel-time functions are provided to expand a path. A novel lower-bound estimator for travel time is proposed. Performance results reveal that our method is more efficient and more accurate than the discrete-time approach.

#index 864398
#* Approximation Techniques for Indexing the Earth Mover's Distance in Multimedia Databases
#@ Ira Assent;Andrea Wenning;Thomas Seidl
#t 2006
#c 17
#! Todays abundance of storage coupled with digital technologies in virtually any scientific or commercial application such as medical and biological imaging or music archives deal with tremendous quantities of images, videos or audio files stored in large multimedia databases. For content-based data mining and retrieval purposes suitable similarity models are crucial. The Earth Mover's Distance was introduced in Computer Vision to better approach human perceptual similarities. Its computation, however, is too complex for usage in interactive multimedia database scenarios. In order to enable efficient query processing in large databases, we propose an index-supported multistep algorithm. We therefore develop new lower bounding approximation techniques for the Earth Mover's Distance which satisfy high quality criteria including completeness (no false drops), index-suitability and fast computation. We demonstrate the efficiency of our approach in extensive experiments on large image databases

#index 864399
#* Indexing for Dynamic Abstract Regions
#@ Joxan Jaffar;Roland H. C. Yap;Kenny Q. Zhu
#t 2006
#c 17
#! We propose a new main memory index structure for abstract regions (objects) which may heavily overlap, the RCtree. These objects are "dynamic" and may have short life spans. The novelty is that rather than representing an object by its minimum bounding rectangle (MBR), possibly with pre-processed segmentation into many small MBRs, we use the actual shape of the object to maintain the index. This saves significant space for objects with large spatial extents since pre-segmentation is not needed. We show that the query performance of RC-tree is much better than many indexing schemes on synthetic overlapping data sets. The performance is also competitive on real-life GIS nonoverlapping data sets.

#index 864400
#* Efficient Processing of Updates in Dynamic XML Data
#@ Changqing Li;Tok Wang Ling;Min Hu
#t 2006
#c 17
#! It is important to process the updates when nodes are inserted into or deleted from the XML tree. All the existing labeling schemes have high update cost, thus in this paper we propose a novel Compact Dynamic Binary String (CDBS) encoding to efficiently process the updates. CDBS has two important properties which form the foundations of this paper: (1) CDBS supports that codes can be inserted between any two consecutive CDBS codes with the orders kept and without re-encoding the existing codes; (2) CDBS is orthogonal to specific labeling schemes, thus it can be applied broadly to different labeling schemes or other applications to efficiently process the updates. We report our experimental results to show that our CDBS is superior to previous approaches to process updates in terms of the number of nodes to re-label and the time for updating.

#index 864401
#* A Complete and Efficient Algebraic Compiler for XQuery
#@ Christopher Re;Jerome Simeon;Mary Fernandez
#t 2006
#c 17
#! As XQuery nears standardization, more sophisticated XQuery applications are emerging, which often exploit the entire language and are applied to non-trivial XML sources. We propose an algebra and optimization techniques that are suitable for building an XQuery compiler that is complete, correct, and efficient. We describe the compilation rules for the complete language into that algebra and present novel optimization techniques that address the needs of complex queries. These techniques include new query unnesting rewritings and specialized join algorithms that account for XQuery's complex predicate semantics. The algebra and optimizations are implemented in the Galax XQuery engine, and yield execution plans that are up to three orders of magnitude faster than earlier versions of Galax.

#index 864402
#* Making Designer Schemas with Colors
#@ Nuwee Wiwatwattana;H. V. Jagadish;Laks V.  S. Lakshmanan;Divesh Srivastava
#t 2006
#c 17
#! XML schema design has two opposing goals: elimination of update anomalies requires that the schema be as normalized as possible; yet higher query performance and simpler query expression are often obtained through the use of schemas that permit redundancy. In this paper, we show that the recently proposed MCT data model, which extends XML by adding colors, can be used to address this dichotomy effectively. Specifically, we formalize the intuition of anomaly avoidance in MCT using notions of node normal and edge normal forms, and the goal of efficient query processing using notions of association recoverability and direct recoverability. We develop algorithms for transforming design specifications given as ER diagrams into MCT schemas that are in a node or edge normal form and satisfy association or direct recoverability. Experimental results using a wide variety of ER diagrams validate the benefits of our design methodology.

#index 864403
#* Mining Actionable Patterns by Role Models
#@ Ke Wang;Yuelong Jiang;Alexander Tuzhilin
#t 2006
#c 17
#! Data mining promises to discover valid and potentially useful patterns in data. Often, discovered patterns are not useful to the user."Actionability" addresses this problem in that a pattern is deemed actionable if the user can act upon it in her favor. We introduce the notion of "action" as a domain-independent way to model the domain knowledge. Given a data set about actionable features and an utility measure, a pattern is actionable if it summarizes a population that can be acted upon towards a more promising population observed with a higher utility. We present several pruning strategies taking into account the actionability requirement to reduce the search space, and algorithms for mining all actionable patterns as well as mining the top k actionable patterns. We evaluate the usefulness of patterns and the focus of search on a real-world application domain.

#index 864404
#* Systematic Approach for Optimizing Complex Mining Tasks on Multiple Databases
#@ Ruoming Jin;Gagan Agrawal
#t 2006
#c 17
#! Many real world applications involve not just a single dataset, but a view of multiple datasets. These datasets may be collected from different sources and/or at different time instances. In such scenarios, comparing patterns or features from different datasets and understanding their relationships can be an extremely important part of the KDD process. This paper considers the problem of optimizing a mining task over multiple datasets, when it has been expressed using a highlevel interface. Specifically, we make the following contributions: 1) We present an SQL-based mechanism for querying frequent patterns across multiple datasets, and establish an algebra for these queries. 2) We develop a systematic method for enumerating query plans and present several algorithms for finding optimized query plan which reduce execution costs. 3) We evaluate our algorithms on real and synthetic datasets, and show up to an order of magnitude performance improvement

#index 864405
#* New Sampling-Based Estimators for OLAP Queries
#@ Ruoming Jin;Leo Glimcher;Chris Jermaine;Gagan Agrawal
#t 2006
#c 17
#! One important way in which sampling for approximate query processing in a database environment differs from traditional applications of sampling is that in a database, it is feasible to collect accurate summary statistics from the data in addition to the sample. This paper describes a set of sampling-based estimators for approximate query processing that make use of simple summary statistics to to greatly increase the accuracy of sampling-based estimators. Our estimators are able to give tight probabilistic guarantees on estimation accuracy. They are suitable for low or high dimensional data, and work with categorical or numerical attributes. Furthermore, the information used by our estimators can easily be gathered in a single pass, making them suitable for use in a streaming environment.

#index 864406
#* Mondrian Multidimensional K-Anonymity
#@ Kristen LeFevre;David J. DeWitt;Raghu Ramakrishnan
#t 2006
#c 17
#! K-Anonymity has been proposed as a mechanism for protecting privacy in microdata publishing, and numerous recoding "models" have been considered for achieving anonymity. This paper proposes a new multidimensional model, which provides an additional degree of flexibility not seen in previous (single-dimensional) approaches. Often this flexibility leads to higher-quality anonymizations, as measured both by general-purpose metrics and more specific notions of query answerability. Optimal multidimensional anonymization is NP-hard (like previous optimal anonymity problems). However, we introduce a simple greedy approximation algorithm, and experimental results show that this greedy algorithm frequently leads to more desirable anonymizations than exhaustive optimal algorithms for two single-dimensional models.

#index 864407
#* Efficiently Evaluating Order Preserving Similarity Queries over Historical Market-Basket Data
#@ Reza Sherkat;Davood Rafiei
#t 2006
#c 17
#! We introduce a new domain-independent framework for formulating and efficiently evaluating similarity queries over historical data, where given a history as a sequence of timestamped observations and the pair-wise similarity of observations, we want to find similar histories. For instance, given a database of customer transactions and a time period, we can find customers with similar purchasing behaviors over this period. Our work is different from the work on retrieving similar time series; it addresses the general problem in which a history cannot be modeled as a time series, hence the relevant conventional approaches are not applicable. We derive a similarity measure for histories, based on an aggregation of the similarities between the observations of the two histories, and propose efficient algorithms for finding an optimal alignment between two histories. Given the non-metric nature of our measure, we develop some upper bounds and an algorithm that makes use of those bounds to prune histories that are guaranteed not to be in the answer set. Our experimental results on real and synthetic data confirm the effectiveness and efficiency of our approach. For instance, when the minimum length of a match is provided, our algorithm achieves up to an order of magnitude speed-up over alternative methods.

#index 864408
#* End-biased Samples for Join Cardinality Estimation
#@ Cristian Estan;Jeffrey F. Naughton
#t 2006
#c 17
#! We present a new technique for using samples to estimate join cardinalities. This technique, which we term "end-biased samples," is inspired by recent work in network traffic measurement. It improves on random samples by using coordinated pseudo-random samples and retaining the sampled values in proportion to their frequency. We show that end-biased samples always provide more accurate estimates than random samples with the same sample size. The comparison with histograms is more interesting ― while end-biased histograms are somewhat better than end-biased samples for uncorrelated data sets, end-biased samples dominate by a large margin when the data is correlated. Finally, we compare end-biased samples to the recently proposed "skimmed sketches" and show that neither dominates the other, that each has different and compelling strengths and weaknesses. These results suggest that endbiased samples may be a useful addition to the repertoire of techniques used for data summarization.

#index 864409
#* Laws for Rewriting Queries Containing Division Operators
#@ Ralf Rantzau;Christoph Mangold
#t 2006
#c 17
#! Relational division, also known as small divide, is derived operator of the relational algebra that realizes many-to-one set containment test, where a set is represented as a group of tuples: Small divide discovers which sets in a dividend relation contain all elements of the set stored in a divisor relation. The great divide operator extends small divide by realizing many-to-many set containment tests. It is also similar to the set containment join operator for schemas that are not in first normal form. Neither small nor great divide has been implemented in commercial relational database systems although the operators solve important problems and many efficient algorithms for them exist. We present algebraic laws that allow rewriting expressions containing small or great divide, illustrate their importance for query optimization, and discuss the use of great divide for frequent itemset discovery, an important data mining primitive. A recent theoretic result shows that small divide must be implemented by special purpose algorithms and not be simulated by pure relational algebra expressions to achieve efficiency. Consequently, an efficient implementation requires that the optimizer treats small divide as a first-class operator and possesses powerful algebraic laws for query rewriting.

#index 864410
#* R-trees with Update Memos
#@ Xiaopeng Xiong;Walid G. Aref
#t 2006
#c 17
#! The problem of frequently updating multi-dimensional indexes arises in many location-dependent applications. While the R-tree and its variants are one of the dominant choices for indexing multi-dimensional objects, the R-tree exhibits inferior performance in the presence of frequent updates. In this paper, we present an R-tree variant, termed the RUM-tree (stands for R-tree with Update Memo) that minimizes the cost of object updates. The RUM-tree processes updates in a memo-based approach that avoids disk accesses for purging old entries during an update process. Therefore, the cost of an update operation in the RUM-tree reduces to the cost of only an insert operation. The removal of old object entries is carried out by a garbage cleaner inside the RUM-tree. In this paper, we present the details of the RUM-tree and study its properties. Theoretical analysis and experimental evaluation demonstrate that the RUMtree outperforms other R-tree variants by up to a factor of eight in scenarios with frequent updates.

#index 864411
#* Compiled Query Execution Engine using JVM
#@ Jun Rao;Hamid Pirahesh;C. Mohan;Guy Lohman
#t 2006
#c 17
#! A conventional query execution engine in a database system essentially uses a SQL virtual machine (SVM) to interpret a dataflow tree in which each node is associated with a relational operator. During query evaluation, a single tuple at a time is processed and passed among the operators. Such a model is popular because of its efficiency for pipelined processing. However, since each operator is implemented statically, it has to be very generic in order to deal with all possible queries. Such generality tends to introduce significant runtime inefficiency, especially in the context of memory-resident systems, because the granularity of data commercial system, using SVM. processing (a tuple) is too small compared with the associated overhead. Another disadvantage in such an engine is that each operator code is compiled statically, so query-specific optimization cannot be applied. To improve runtime efficiency, we propose a compiled execution engine, which, for a given query, generates new query-specific code on the fly, and then dynamically compiles and executes the code. The Java platform makes our approach particularly interesting for several reasons: (1) modern Java Virtual Machines (JVM) have Just- In-Time (JIT) compilers that optimize code at runtime based on the execution pattern, a key feature that SVMs lack; (2) because of Java's continued popularity, JVMs keep improving at a faster pace than SVMs, allowing us to exploit new advances in the Java runtime in the future; (3) Java is a dynamic language, which makes it convenient to load a piece of new code on the fly. In this paper, we develop both an interpreted and a compiled query execution engine in a relational, Java-based, in-memory database prototype, and perform an experimental study. Our experimental results on the TPC-H data set show that, despite both engines benefiting from JIT, the compiled engine runs on average about twice as fast as the interpreted one, and significantly faster than an in-memory

#index 864412
#* \ell -Diversity: Privacy Beyond \kappa -Anonymity
#@ Ashwin Machanavajjhala;Johannes Gehrke;Daniel Kifer;Muthuramakrishnan Venkitasubramaniam
#t 2006
#c 17
#! We present MiniCount, the first efficient sound and complete algorithm for finding maximally contained rewritings of conjunctive queries with count, using conjunctive views with count and conjunctive views without aggregation. An efficient and scalable ...

#index 864413
#* Sovereign Joins
#@ Rakesh Agrawal;Dmitri Asonov;Murat Kantarcioglu;Yaping Li
#t 2006
#c 17
#! We present a secure network service for sovereign information sharing whose only trusted component is an off-theshelf secure coprocessor. The participating data providers send encrypted relations to the service that sends the encrypted results to the recipients. The technical challenge in implementing such a service arises from the limited capability of the secure coprocessors: they have small memory, no attached disk, and no facility for communicating directly with other machines in the network. The internal state of an ongoing computation within the secure coprocessor cannot be seen from outside, but its interactions with the server can be exploited by an adversary. We formulate the problem of computing join in this setting where the goal is to prevent information leakage through patterns in I/O while maximizing performance. We specify criteria for proving the security of a join algorithm and provide provably safe algorithms. These algorithms can be used to compute general joins involving arbitrary predicates and multiple sovereign databases. We thus enable a new class of applications requiring query processing across sovereign entities such that nothing apart from the result is revealed to the recipients.

#index 864414
#* Privacy Preserving Query Processing Using Third Parties
#@ Fatih Emekci;Divyakant Agrawal;Amr El Abbadi;Aziz Gulbeden
#t 2006
#c 17
#! Data integration from multiple autonomous data sources has emerged as an important practical problem. The key requirement for such data integration is that owners of such data need to cooperate in a competitive landscape in most of the cases. The research challenge in developing a query processing solution is that the answers to the queries need to be provided while preserving the privacy of the data sources. In general, allowing unrestricted read access to the whole data may give rise to potential vulnerabilities as well as may have legal implications. Therefore, there is a need for privacy preserving database operations for querying data residing at different parties. In this paper, we propose a new query processing technique using third parties in a peer-to-peer system. We propose and evaluate two different protocols for various database operations. Our scheme is able to answer queries without revealing any useful information to the data sources or to the third parties. Analytical comparison of the proposed approach with other recent proposals for privacy-preserving data integration establishes the superiority of the proposed approach in terms of query response time

#index 864415
#* Efficient Batch Top-k Search for Dictionary-based Entity Recognition
#@ Amit Chandel;P. C. Nagesh;Sunita Sarawagi
#t 2006
#c 17
#! We consider the problem of speeding up Entity Recognition systems that exploit existing large databases of structured entities to improve extraction accuracy. These systems require the computation of the maximum similarity scores of several overlapping segments of the input text with the entity database. We formulate a Batch-Top-K problem with the goal of sharing computations across overlapping segments. Our proposed algorithm performs a factor of three faster than independent Top-K queries and only a factor of two slower than an unachievable lower bound on total cost. We then propose a novel modification of the popular Viterbi algorithm for recognizing entities so as to work with easily computable bounds on match scores, thereby reducing the total inference time by a factor of eight compared to stateof- the-art methods.

#index 864416
#* Integrating Unstructured Data into Relational Databases
#@ Imran R. Mansuri;Sunita Sarawagi
#t 2006
#c 17
#! In this paper we present a system for automatically integrating unstructured text into a multi-relational database using state-of-the-art statistical models for structure extraction and matching. We show how to extend current highperforming models, Conditional Random Fields and their semi-markov counterparts, to effectively exploit a variety of recognition clues available in a database of entities, thereby significantly reducing the dependence on manually labeled training data. Our system is designed to load unstructured records into columns spread across multiple tables in the database while resolving the relationship of the extracted text with existing column values, and preserving the cardinality and link constraints of the database. We show how to combine the inference algorithms of statistical models with the database imposed constraints for optimal data integration.

#index 864417
#* Clean Answers over Dirty Databases: A Probabilistic Approach
#@ Periklis Andritsos;Ariel Fuxman;Renee J. Miller
#t 2006
#c 17
#! The detection of duplicate tuples, corresponding to the same real-world entity, is an important task in data integration and cleaning. While many techniques exist to identify such tuples, the merging or elimination of duplicates can be a difficult task that relies on ad-hoc and often manual solutions. We propose a complementary approach that permits declarative query answering over duplicated data, where each duplicate is associated with a probability of being in the clean database. We rewrite queries over a database containing duplicates to return each answer with the probability that the answer is in the clean database. Our rewritten queries are sensitive to the semantics of duplication and help a user understand which query answers are most likely to be present in the clean database. The semantics that we adopt is independent of the way the probabilities are produced, but is able to effectively exploit them during query answering. In the absence of external knowledge that associates each database tuple with a probability, we offer a technique, based on tuple summaries, that automates this task. We experimentally study the performance of our rewritten queries. Our studies show that the rewriting does not introduce a significant overhead in query execution time. This work is done in the context of the ConQuer project at the University of Toronto, which focuses on the efficient management of inconsistent and dirty databases.

#index 864418
#* Syntactic Rule Based Approach toWeb Service Composition
#@ Ken Pu;Vagelis Hristidis;Nick Koudas
#t 2006
#c 17
#! This paper studies a problem of web service composition from a syntactic approach. In contrast with other approaches on enriched semantic description such as statetransition description of web services, our focus is in the case when only the input-output type information from the WSDL specifications is available. The web service composition problem is formally formulated as deriving a given desired type from a collection of available types and web services using a prescribed set of rules with costs. We show that solving the minimal cost composition is NP-complete in general, and present a practical solution based on dynamic programming. Experiements using a mixture of synthetic and real data sets show that our approach is viable and produces good results.

#index 864419
#* Hilda: A High-Level Language for Data-DrivenWeb Applications
#@ Fan Yang;Jayavel Shanmugasundaram;Mirek Riedewald;Johannes Gehrke
#t 2006
#c 17
#! We propose Hilda, a high-level language for developing data-driven web applications. The primary benefits of Hilda over existing development platforms are: (a) it uses a unified data model for all layers of the application, (b) it is declarative, (c) it models both application queries and updates, (d) it supports structured programming for web sites, and (e) it enables conflict detection for concurrent updates. We also describe the implementation of a simple proof-ofconcept Hilda compiler, which translates a Hilda application program into Java Servlet code.

#index 864420
#* UNIT: User-centric Transaction Management in Web-Database Systems
#@ Huiming Qu;Alexandros Labrinidis;Daniel Mosse
#t 2006
#c 17
#! Web-database systems are nowadays an integral part of everybody's life, with applications ranging from monitoring/ trading stock portfolios, to personalized blog aggregation and news services, to personalized weather tracking services. For most of these services to be successful (and their users to be kept satisfied), two criteria need to be met: user requests must be answered in a timely fashion and using fresh data. This paper presents a framework to balance both requirements from the users' perspective. Toward this, we propose a user satisfaction metric to measure the overall effectiveness of the Web-database system. We also provide a set of algorithms to dynamically optimize this metric, through query admission control and update frequency modulation. Finally, we present extensive experimental results which compare our proposed algorithms to the current state of the art and show that we outperform competitors under various workloads (generated based on real traces) and user requirements.

#index 864421
#* VBI-Tree: A Peer-to-Peer Framework for Supporting Multi-Dimensional Indexing Schemes
#@ H. V. Jagadish;Beng Chin Ooi;Quang Hieu Vu;Rong Zhang;Aoying Zhou
#t 2006
#c 17
#! Multi-dimensional data indexing has received much attention in a centralized database. However, not so much work has been done on this topic in the context of Peerto- Peer systems. In this paper, we propose a new Peer-to- Peer framework based on a balanced tree structure overlay, which can support extensible centralized mapping methods and query processing based on a variety of multidimensional tree structures, including R-Tree, X-Tree, SSTree, and M-Tree. Specifically, in a network with N nodes, our framework guarantees that point queries and range queries can be answered within O(logN) hops. We also provide an effective load balancing strategy to allow nodes to balance their work load efficiently. An experimental assessment validates the practicality of our proposal.

#index 864422
#* Transaction Time Support Inside a Database Engine
#@ David Lomet;Roger Barga;Mohamed F. Mokbel;German Shegalov
#t 2006
#c 17
#! Transaction time databases retain and provide access to prior states of a database. An update "inserts" a new record while preserving the old version. Immortal DB builds transaction time database support into a database engine, not in middleware. It supports as of queries returning records current at the specified time. It also supports snapshot isolation concurrency control. Versions are stamped with the "clock times" of their updating transactions. The timestamp order agrees with transaction serialization order. Lazy timestamping propagates timestamps to transaction updates after commit. Versions are kept in an integrated storage structure, with historical versions initially stored with current data. Time-splits of pages permit large histories to be maintained, and enable time based indexing, which is essential for high performance historical queries. Experiments show that Immortal DB introduces little overhead for accessing recent database states while providing access to past states.

#index 864423
#* Expiration Times for Data Management
#@ Albrecht Schmidt;Christian S. Jensen;Simonas Saltenis
#t 2006
#c 17
#! This paper describes an approach to incorporating the notion of expiration time into data management based on the relational model. Expiration times indicate when tuples cease to be current in a database. The paper presents a formal data model and a query algebra that handle expiration times transparently and declaratively. In particular, expiration times are exposed to users only on insertion and update, and when triggers fire due to the expiration of a tuple; for queries, they are handled behind the scenes and do not concern the user. Notably, tuples are removed automatically from (materialised) query results as they expire in the (base) relations. For application developers, the benefits of using expiration times are leaner application code, lower transaction volume, smaller databases, and higher consistency for replicated data with lower overhead. Expiration times turn out to be especially useful in open architectures and loosely-coupled systems, which abound on the World Wide Web as well as in mobile networks, be it as Web Services or as ad hoc and intermittent networks of mobile devices.

#index 864424
#* Scalable Exploration of Physical Database Design
#@ Arnd Christian Konig;Shubha U. Nabar
#t 2006
#c 17
#! Physical database design is critical to the performance of a large-scale DBMS. The corresponding automated design tuning tools need to select the best physical design from a large set of candidate designs quickly. However, for large workloads, evaluating the cost of each query in the workload for every candidate does not scale. To overcome this, we present a novel comparison primitive that only evaluates a fraction of the workload and provides an accurate estimate of the likelihood of selecting correctly. We show how to use this primitive to construct accurate and scalable selection procedures. Furthermore, we address the issue of ensuring that the estimates are conservative, even for highly skewed cost distributions. The proposed techniques are evaluated through a prototype implementation inside a commercial physical design tool.

#index 864425
#* Closure-Tree: An Index Structure for Graph Queries
#@ Huahai He;Ambuj K. Singh
#t 2006
#c 17
#! Graphs have become popular for modeling structured data. As a result, graph queries are becoming common and graph indexing has come to play an essential role in query processing. We introduce the concept of a graph closure, a generalized graph that represents a number of graphs. Our indexing technique, called Closure-tree, organizes graphs hierarchically where each node summarizes its descendants by a graph closure. Closure-tree can efficiently support both subgraph queries and similarity queries. Subgraph queries find graphs that contain a specific subgraph, whereas similarity queries find graphs that are similar to a query graph. For subgraph queries, we propose a technique called pseudo subgraph isomorphism which approximates subgraph isomorphism with high accuracy. For similarity queries, we measure graph similarity through edit distance using heuristic graph mapping methods. We implement two kinds of similarity queries: K-NN query and range query. Our experiments on chemical compounds and synthetic graphs show that for subgraph queries, Closuretree outperforms existing techniques by up to two orders of magnitude in terms of candidate answer set size and index size. For similarity queries, our experiments validate the quality and efficiency of the presented algorithms.

#index 864426
#* ISOMER: Consistent Histogram Construction Using Query Feedback
#@ U. Srivastava;P. J. Haas;V. Markl;M. Kutsch;T. M. Tran
#t 2006
#c 17
#! Database columns are often correlated, so that cardinality estimates computed by assuming independence often lead to a poor choice of query plan by the optimizer. Multidimensional histograms can help solve this problem, but the traditional approach of building such histograms using a data scan often scales poorly and does not always yield the best histogram for a given workload. An attractive alternative is to gather feedback from the query execution engine about the observed cardinality of predicates and use this feedback as the basis for a histogram. In this paper we describe ISOMER, a new feedback-based algorithm for collecting optimizer statistics by constructing and maintaining multidimensional histograms. ISOMER uses the maximumentropy principle to approximate the true data distribution by a histogram distribution that is as "simple"as possible while being consistent with the observed predicate cardinalities. ISOMER adapts readily to changes in the underlying data, automatically detecting and eliminating inconsistent feedback information in an efficient manner. The algorithm controls the size of the histogram by retaining only the most "important" feedback. Our experiments indicate that, unlike previous methods for feedback-driven histogram maintenance, ISOMER imposes little overhead, is extremely scalable, and yields highly accurate cardinality estimates while using only a modest amount of storage.

#index 864427
#* Counting at Large: Efficient Cardinality Estimation in Internet-Scale Data Networks
#@ Nikos Ntarmos;Peter Triantafillou;Gerhard Weikum
#t 2006
#c 17
#! Counting in general, and estimating the cardinality of (multi-) sets in particular, is highly desirable for a large variety of applications, representing a foundational block for the efficient deployment and access of emerging internetscale information systems. Examples of such applications range from optimizing query access plans in internet-scale databases, to evaluating the significance (rank/score) of various data items in information retrieval applications. The key constraints that any acceptable solution must satisfy are: (i) efficiency: the number of nodes that need be contacted for counting purposes must be small in order to enjoy small latency and bandwidth requirements; (ii) scalability, seemingly contradicting the efficiency goal: arbitrarily large numbers of nodes nay need to add elements to a (multi-) set, which dictates the need for a highly distributed solution, avoiding server-based scalability, bottleneck, and availability problems; (iii) access and storage load balancing: counting and related overhead chores should be distributed fairly to the nodes of the network; (iv) accuracy: tunable, robust (in the presence of dynamics and failures) and highly accurate cardinality estimation; (v) simplicity and ease of integration: special, solution-specific indexing structures should be avoided. In this paper, first we contribute a highly-distributed, scalable, efficient, and accurate (multi-) set cardinality estimator. Subsequently, we show how to use our solution to build and maintain histograms, which have been a basic building block for query optimization for centralized databases, facilitating their porting into the realm of internet-scale data networks.

#index 864428
#* Probabilistic Message Passing in Peer Data Management Systems
#@ Philippe Cudre-Mauroux;Karl Aberer;Andras Feher
#t 2006
#c 17
#! Until recently, most data integration techniques involved central components, e.g., global schemas, to enable transparent access to heterogeneous databases. Today, however, with the democratization of tools facilitating knowledge elicitation in machine-processable formats, one cannot rely on global, centralized schemas anymore as knowledge creation and consumption are getting more and more dynamic and decentralized. Peer Data Management Systems (PDMS) provide an answer to this problem by eliminating the central semantic component and considering instead compositions of local, pair-wise mappings to propagate queries from one database to the others. PDMS approaches proposed so far make the implicit assumption that all mappings used in this way are correct. This obviously cannot be taken as granted in typical PDMS settings where mappings can be created (semi) automatically by independent parties. In this work, we propose a totally decentralized, efficient message passing scheme to automatically detect erroneous mappings in PDMS. Our scheme is based on a probabilistic model where we take advantage of transitive closures of mapping operations to confront local belief on the correctness of a mapping against evidences gathered around the network. We show that our scheme can be efficiently embedded in any PDMS and provide a preliminary evaluation of our techniques on sets of both automatically-generated and real-world schemas.

#index 864429
#* Approximating Aggregation Queries in Peer-to-Peer Networks
#@ Benjamin Arai;Gautam Das;Dimitrios Gunopulos;Vana Kalogeraki
#t 2006
#c 17
#! Peer-to-peer databases are becoming prevalent on the Internet for distribution and sharing of documents, applications, and other digital media. The problem of answering large scale, ad-hoc analysis queries ― e.g., aggregation queries ― on these databases poses unique challenges. Exact solutions can be time consuming and difficult to implement given the distributed and dynamic nature of peer-to-peer databases. In this paper we present novel sampling-based techniques for approximate answering of ad-hoc aggregation queries in such databases. Computing a high-quality random sample of the database efficiently in the P2P environment is complicated due to several factors ― the data is distributed (usually in uneven quantities) across many peers, within each peer the data is often highly correlated, and moreover, even collecting a random sample of the peers is difficult to accomplish. To counter these problems, we have developed an adaptive two-phase sampling approach, based on random walks of the P2P graph as well as block-level sampling techniques. We present extensive experimental evaluations to demonstrate the feasibility of our proposed solutio

#index 864430
#* Distributed Evaluation of Continuous Equi-join Queries over Large Structured Overlay Networks
#@ Stratos Idreos;Christos Tryfonopoulos;Manolis Koubarakis
#t 2006
#c 17
#! We study the problem of continuous relational query processing in Internet-scale overlay networks realized by distributed hash tables. We concentrate on the case of continuous two-way equi-join queries. Joins are hard to evaluate in a distributed continuous query environment because data from more than one relations is needed, and this data is inserted in the network asynchronously. Each time a new tuple is inserted, the network nodes have to cooperate to check if this tuple can contribute to the satisfaction of a query when combined with previously inserted tuples. We propose a series of algorithms that initially index queries at network nodes using hashing. Then, they exploit the values of join attributes in incoming tuples to rewrite the given queries into simpler ones, and reindex them in the network where they might be satisfied by existing or future tuples. We present a detailed experimental evaluation in a simulated environment and we show that our algorithms are scalable, balance the storage and query processing load and keep the network traffic low.

#index 864431
#* WebIQ: Learning from the Web to Match Deep-Web Query Interfaces
#@ Wensheng Wu;AnHai Doan;Clement Yu
#t 2006
#c 17
#! Integrating Deep Web sources requires highly accurate semantic matches between the attributes of the source query interfaces. These matches are usually established by comparing the similarities of the attributes' labels and instances. However, attributes on query interfaces often have no or very few data instances. The pervasive lack of instances seriously reduces the accuracy of current matching techniques. To address this problem, we describe WebIQ, a solution that learns from both the Surface Web and the Deep Web to automatically discover instances for interface attributes. WebIQ extends question answering techniques commonly used in the AI community for this purpose. We describe how to incorporate WebIQ into current interface matching systems. Extensive experiments over five realworld domains show the utility ofWebIQ. In particular, the results show that acquired instances help improve matching accuracy from 89.5% F-1 to 97.5%, at only a modest runtime overhead.

#index 864432
#* Answering Imprecise Queries over Autonomous Web Databases
#@ Ullas Nambiar;Subbarao Kambhampati
#t 2006
#c 17
#! Current approaches for answering queries with imprecise constraints require user-specific distance metrics and importance measures for attributes of interest - metrics that are hard to elicit from lay users. We present AIMQ, a domain and user independent approach for answering imprecise queries over autonomous Web databases. We developed methods for query relaxation that use approximate functional dependencies. We also present an approach to automatically estimate the similarity between values of categorical attributes. Experimental results demonstrating the robustness, efficiency and effectiveness of AIMQ are presented. Results of a preliminary user study demonstrating the high precision of the AIMQ system is also provided.

#index 864433
#* Merging Source Query Interfaces onWeb Databases
#@ Eduard Dragut;Wensheng Wu;Prasad Sistla;Clement Yu;Weiyi Meng
#t 2006
#c 17
#! Recently, there are many e-commerce search engines that return information from Web databases. Unlike text search engines, these e-commerce search engines have more complicated user interfaces. Our aim is to construct automatically a natural query user interface that integrates a set of interfaces over a given domain of interest. For example, each airline company has a query interface for ticket reservation and our system can construct an integrated interface for all these companies. This will permit users to access information uniformly from multiple sources. Each query interface from an e-commerce search engine is designed so as to facilitate users to provide necessary information. Specifically, (1) related pieces of information such as first name and last name are grouped together and (2) certain hierarchical relationships are maintained. In this paper, we provide an algorithm to compute an integrated interface from query interfaces of the same domain. The integrated query interface can be proved to preserve the above two types of relationships. Experiments on five domains verify our theoretical study.

#index 864434
#* Query Selection Techniques for Efficient Crawling of Structured Web Sources
#@ Ping Wu;Ji-Rong Wen;Huan Liu;Wei-Ying Ma
#t 2006
#c 17
#! The high quality, structured data from Web structured sources is invaluable for many applications. Hidden Web databases are not directly crawlable by Web search engines and are only accessible through Web query forms or via Web service interfaces. Recent research efforts have been focusing on understanding these Web query forms. A critical but still largely unresolved question is: how to efficiently acquire the structured information inside Web databases through iteratively issuing meaningful queries? In this paper we focus on the central issue of enabling efficient Web database crawling through query selection, i.e. how to select good queries to rapidly harvest data records from Web databases. We model each structured Web database as a distinct attribute-value graph. Under this theoretical framework, the database crawling problem is transformed into a graph traversal one that follows "relational" links. We show that finding an optimal query selection plan is equivalent to finding a Minimum Weighted Dominating Set of the corresponding database graph, a well-known NP-Complete problem. We propose a suite of query selection techniques aiming at optimizing the query harvest rate. Extensive experimental evaluations over real Web sources and simulations over controlled database servers validate the effectiveness of our techniques and provide insights for future efforts in this

#index 864435
#* Approximate Data Collection in Sensor Networks using Probabilistic Models
#@ David Chu;Amol Deshpande;Joseph M. Hellerstein;Wei Hong
#t 2006
#c 17
#! Wireless sensor networks are proving to be useful in a variety of settings. A core challenge in these networks is to minimize energy consumption. Prior database research has proposed to achieve this by pushing data-reducing operators like aggregation and selection down into the network. This approach has proven unpopular with early adopters of sensor network technology, who typically want to extract complete "dumps" of the sensor readings, i.e., to run "SELECT *" queries. Unfortunately, because these queries do no data reduction, they consume significant energy in current sensornet query processors. In this paper we attack the "SELECT " problem for sensor networks. We propose a robust approximate technique called Ken that uses replicated dynamic probabilistic models to minimize communication from sensor nodes to the network's PC base station. In addition to data collection, we show that Ken is well suited to anomaly- and event-detection applications. A key challenge in this work is to intelligently exploit spatial correlations across sensor nodes without imposing undue sensor-to-sensor communication burdens to maintain the models. Using traces from two real-world sensor network deployments, we demonstrate that relatively simple models can provide significant communication (and hence energy) savings without undue sacrifice in result quality or frequency. Choosing optimally among even our simple models is NPhard, but our experiments show that a greedy heuristic performs nearly as well as an exhaustive algorithm.

#index 864436
#* Network-Aware Operator Placement for Stream-Processing Systems
#@ Peter Pietzuch;Jonathan Ledlie;Jeffrey Shneidman;Mema Roussopoulos;Matt Welsh;Margo Seltzer
#t 2006
#c 17
#! To use their pool of resources efficiently, distributed stream-processing systems push query operators to nodes within the network. Currently, these operators, ranging from simple filters to custom business logic, are placed manually at intermediate nodes along the transmission path to meet application-specific performance goals. Determining placement locations is challenging because network and node conditions change over time and because streams may interact with each other, opening venues for reuse and repositioning of operators. This paper describes a stream-based overlay network (SBON), a layer between a stream-processing system and the physical network that manages operator placement for stream-processing systems. Our design is based on a cost space, an abstract representation of the network and on-going streams, which permits decentralized, large-scale multi-query optimization decisions. We present an evaluation of the SBON approach through simulation, experiments on PlanetLab, and an integration with Borealis, an existing stream-processing engine. Our results show that an SBON consistently improves network utilization, provides low stream latency, and enables dynamic optimization at low engineering cost.

#index 864437
#* Unifying the Processing of XML Streams and Relational Data Streams
#@ Xin Zhou;Hetal Thakkar;Carlo Zaniolo
#t 2006
#c 17
#! Relational data streams and XML streams have previously provided two separate research foci, but their unified support by a single Data Stream Management System (DSMS) is very desirable from an application viewpoint. In this paper, we propose a simple approach to extend relational DSMSs to support both kinds of streams efficiently. In our Stream Mill system, XML streams expressed as SAX events, can be easily transformed into relational streams, and vice versa. This enables a close cooperation of their query languages, resulting in great power and flexibility. For instance, XQuery can call functions defined in our SQLbased Expressive Stream Language (ESL) using the logical/ physical windows that have proved so useful on relational data streams. Many benefits are also gained at the system level, since relational DSMS techniques for load shedding, memory management, query scheduling, approximate query answering, and synopsis maintenance can now be applied to XML streams. Moreover, the many FSA-based optimization techniques developed for XPath and XQuery can be easily and efficiently incorporated in our system. Indeed, we show that YFilter, which is capable of efficiently processing multiple complex XML queries, can be easily integrated in Stream Mill via ESL user-defined and systemdefined aggregates. This approach produces a powerful and flexible system where relational and XML streams are unified and processed efficiently.

#index 864438
#* Space-efficient Relative Error Order Sketch over Data Streams
#@ Ying Zhang;Xuemin Lin;Jian Xu;Flip Korn;Wei Wang
#t 2006
#c 17
#! We consider the problem of continuously maintaining order sketches over data streams with a relative rank error guarantee ∊. Novel space-efficient and one-scan randomised techniques are developed. Our first randomised algorithm can guarantee such a relative error precision ∊ with confidence 1 - \deltausing O( 1\_ \in \frac{1} {2}2 log 1d log ∊^2N) space, where N is the number of data elements seen so far in a data stream. Then, a new one-scan space compression technique is developed. Combined with the first randomised algorithm, the one-scan space compression technique yields another one-scan randomised algorithm that guarantees the space requirement is O( 1\frac{1} { \in } log(1\frac{1}{ \in } log 1\begin{gathered} \frac{1}{\delta } \hfill \\ \hfill \\ \end{gathered})\frac{{\log ^{2 + \alpha }\in N}} {{1 - 1/2^\alpha}}(for\alpha \gt 0) on average while the worst case space remains O( \frac{1}{{ \in ^2 }}\log \frac{1} {\delta }\log\in ^2 N). These results are immediately applicable to approximately computing quantiles over data streams with a relative error guarantee\inand significantly improve the previous best space bound O( \frac{1} {{ \in ^3 }}\log \frac{1}{\delta }\log N). Our extensive experiment results demonstrate that both techniques can support an on-line computation against high speed data streams.

#index 864439
#* Designing and Evaluating an XPath Dialect for Linguistic Queries
#@ Steven Bird;Yi Chen;Susan B. Davidson;Haejoong Lee;Yifeng Zheng
#t 2006
#c 17
#! Linguistic research and natural language processing employ large repositories of ordered trees. XML, a standard ordered tree model, and XPath, its associated language, are natural choices for linguistic data and queries. However, several important expressive features required for linguistic queries are missing or hard to express in XPath. In this paper, we motivate and illustrate these features with a variety of linguistic queries. Then we propose extensions to XPath to support linguistic queries, and design an efficient query engine based on a novel labeling scheme. Experiments demonstrate that our language is not only sufficiently expressive for linguistic trees but also efficient for practical usage.

#index 864440
#* Predicate-based Filtering of XPath Expressions
#@ Shuang Hou;H.-Arno Jacobsen
#t 2006
#c 17
#! The XML/XPath filtering problem has found wide-spread interest. In this paper, we propose a novel algorithm for solving it. Our approach encodes XPath expressions (XPEs) as ordered sets of predicates and translates XML documents into sets of tuples, which are evaluated over these predicates. Predicates representing overlapping portions of XPEs are stored and processed once, thus fully exploiting potential overlap in XPEs. We experimentally evaluate the performance of our algorithm, demonstrating its scalability to millions of XPEs, with matching performance in the millisecond range. We show interesting trade-offs to alternative approaches.

#index 864441
#* An Estimation System for XPath Expressions
#@ Hanyu Li;Mong Li Lee;Wynne Hsu;Gao Cong
#t 2006
#c 17
#! Estimating the result sizes of XML queries is important in query optimization and is useful in providing a quick feedback about the queries. Existing works have focused on the selectivity estimation of XML queries without order-based axes. In this work, we develop a framework to estimate the result sizes of XPath expressions with order-based axes. We describe how the path and order information of XML elements can be captured and summarized in compact data structures. We also describe methods to estimate the selectivity of XPath queries. The results of extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness and accuracy of the proposed approach.

#index 864442
#* Adaptive Reorganization of Coherency-Preserving Dissemination Tree for Streaming Data
#@ Yongluan Zhou;Beng Chin Ooi;Kian-Lee Tan;Feng Yu
#t 2006
#c 17
#! We present MiniCount, the first efficient sound and complete algorithm for finding maximally contained rewritings of conjunctive queries with count, using conjunctive views with count and conjunctive views without aggregation. An efficient and scalable ...

#index 864443
#* Declarative Network Monitoring with an Underprovisioned Query Processor
#@ Frederick Reiss;Joseph M. Hellerstein
#t 2006
#c 17
#! Many of the data sources used in stream query processing are known to exhibit bursty behavior. We focus here on passive network monitoring, an application in which the data rates typically exhibit a large peak-to-average ratio. Provisioning a stream query processor to handle peak rates in such a setting can be prohibitively expensive. In this paper, we propose to solve this problem by provisioning the query processor for typical data rates instead of much higher peak data rates. To enable this strategy, we present mechanisms and policies for managing the tradeoffs between the latency and accuracy of query results when bursts exceed the steady-state capacity of the query processor. We describe the current status of our implementation and present experimental results on a testbed network monitoring application to demonstrate the utility of our approach

#index 864444
#* What's Different: Distributed, Continuous Monitoring of Duplicate-Resilient Aggregates on Data Streams
#@ Graham Cormode;S. Muthukrishnan;Wei Zhuang
#t 2006
#c 17
#! Emerging applications in sensor systems and network-wide IP traffic analysis present many technical challenges. They need distributed monitoring and continuous tracking of events. They have severe resource constraints not only at each site in terms of per-update processing time and archival space for highspeed streams of observations, but also crucially, communication constraints for collaborating on the monitoring task. These elements have been addressed in a series of recent works. A fundamental issue that arises is that one cannot make the "uniqueness" assumption on observed events which is present in previous works, since widescale monitoring invariably encounters the same events at different points. For example, within the network of an Internet Service Provider packets of the same flow will be observed in different routers; similarly, the same individual will be observed by multiple mobile sensors in monitoring wild animals. Aggregates of interest on such distributed environments must be resilient to duplicate observations. We study such duplicate-resilient aggregates that measure the extent of the duplication―how many unique observations are there, how many observations are unique―as well as standard holistic aggregates such as quantiles and heavy hitters over the unique items. We present accuracy guaranteed, highly communication-efficient algorithms for these aggregates that work within the time and space constraints of high speed streams. We also present results of a detailed experimental study on both real-life and synthetic data.

#index 864445
#* Extending RDBMSs To Support Sparse Datasets Using An Interpreted Attribute Storage Format
#@ Jennifer L. Beckmann;Alan Halverson;Rajasekar Krishnamurthy;Jeffrey F. Naughton
#t 2006
#c 17
#! "Sparse" data, in which relations have many attributes that are null for most tuples, presents a challenge for relational database management systems. If one uses the normal "horizontal" schema to store such data sets in any of the three leading commercial RDBMS, the result is tables that occupy vast amounts of storage, most of which is devoted to nulls. If one attempts to avoid this storage blowup by using a "vertical" schema, the storage utilization is indeed better, but query performance is orders of magnitude slower for certain classes of queries. In this paper, we argue that the proper way to handle sparse data is not to use a vertical schema, but rather to extend the RDBMS tuple storage format to allow the representation of sparse attributes as interpreted fields. The addition of interpreted storage allows for efficient and transparent querying of sparse data, uniform access to all attributes, and schema scalability. We show, through an implementation in PostgreSQL, that the interpreted storage approach dominates in query efficiency and ease-of-use over the current horizontal storage and vertical schema approaches over a wide range of queries and sparse data sets.

#index 864446
#* Super-Scalar RAM-CPU Cache Compression
#@ Marcin Zukowski;Sandor Heman;Niels Nes;Peter Boncz
#t 2006
#c 17
#! High-performance data-intensive query processing tasks like OLAP, data mining or scientific data analysis can be severely I/O bound, even when high-end RAID storage systems are used. Compression can alleviate this bottleneck only if encoding and decoding speeds significantly exceed RAID I/O bandwidth. For this purpose, we propose three new versatile compression schemes (PDICT, PFOR, and PFOR-DELTA) that are specifically designed to extract maximum IPC from modern CPUs. We compare these algorithms with compression techniques used in (commercial) database and information retrieval systems. Our experiments on the MonetDB/X100 database system, using both DSM and PAX disk storage, show that these techniques strongly accelerate TPC-H performance to the point that the I/O bottleneck is eliminated.

#index 864447
#* How to Determine a Good Multi-Programming Level for External Scheduling
#@ Bianca Schroeder;Mor Harchol-Balter;Arun Iyengar;Erich Nahum;Adam Wierman
#t 2006
#c 17
#! Scheduling/prioritization of DBMS transactions is important for many applications that rely on database backends. A convenient way to achieve scheduling is to limit the number of transactions within the database, maintaining most of the transactions in an external queue, which can be ordered as desired by the application. While external scheduling has many advantages in that it doesn't require changes to internal resources, it is also difficult to get right in that its performance depends critically on the particular multiprogramming limit used (the MPL), i.e. the number of transactions allowed into the database. If the MPL is too low, throughput will suffer, since not all DBMS resources will be utilized. On the other hand, if the MPL is too high, there is insufficient control on scheduling. The question of how to adjust theMPL to achieve both goals simultaneously is an open problem, not just for databases but in system design in general. Herein we study this problem in the context of transactional workloads, both via extensive experimentation and queueing theoretic analysis. We find that the two most critical factors in adjusting the MPL are the number of resources that the workload utilizes and the variability of the transactions' service demands. We develop a feedback based controller, augmented by queueing theoretic models for automatically adjusting the MPL. Finally, we apply our methods to the specific problem of external prioritization of transactions. We find that external prioritization can be nearly as effective as internal prioritization, without any negative consequences, when the MPL is set appropriately.

#index 864448
#* XSEED: Accurate and Fast Cardinality Estimation for XPath Queries
#@ Ning Zhang;M. Tamer Ozsu;Ashraf Aboulnaga;Ihab F. Ilyas
#t 2006
#c 17
#! We propose XSEED, a synopsis of path queries for cardinality estimation that is accurate, robust, efficient, and adaptive to memory budgets. XSEED starts from a very small kernel, and then incrementally updates information of the synopsis. With such an incremental construction, a synopsis structure can be dynamically configured to accommodate different memory budgets. Cardinality estimation based on XSEED can be performed very efficiently and accurately. Extensive experiments on both synthetic and real data sets show that even with less memory, XSEED could achieve accuracy that is an order of magnitude better than that of other synopsis structures. The cardinality estimation time is under 2% of the actual querying time for a wide range of queries in all test cases.

#index 864449
#* Estimating XML Structural Join Size Quickly and Economically
#@ Cheng Luo;Zhewei Jiang;Wen-Chi Hou;Feng Yan;Chih-Fang Wang
#t 2006
#c 17
#! XML structural joins, which evaluate the containment (ancestor-descendant) relationships between XML elements, are important operations of XML query processing. Estimating structural join size accurately and quickly is thus crucial to the success of XML query plan selection and the query optimization. XML structural joins are essentially complex unequal joins, which render well-known estimation techniques, such as cosine transform, wavelet transform, and sketch, not directly applicable. In this paper, we propose a relation model to capture the structural information of XML data such that the original complex unequal joins are converted to equal joins and those well-known estimation techniques become directly applicable to structural join size estimation. Theoretical analyses and extensive experiments have been performed on these estimation methods. It is shown that the cosine transform requires the least memory and yields the best estimates.

#index 864450
#* XCluster Synopses for Structured XML Content
#@ Neoklis Polyzotis;Minos Garofalakis
#t 2006
#c 17
#! We tackle the difficult problem of summarizing the path/branching structure and value content of an XML database that comprises both numeric and textual values. We introduce a novel XML-summarization model, termed XCLUSTERs, that enables accurate selectivity estimates for the class of twig queries with numeric-range, substring, and textual IR predicates over the content of XML elements. In a nutshell, an XCLUSTER synopsis represents an effective clustering of XML elements based on both their structural and value-based characteristics. By leveraging techniques for summarizing XML-document structure as well as numeric and textual data distributions, our XCLUSTER model provides the first known unified framework for handling path/branching structure and different types of element values. We detail the XCLUSTER model, and develop a systematic framework for the construction of effective XCLUSTER summaries within a specified storage budget. Experimental results on synthetic and real-life data verify the effectiveness of our XCLUSTER synopses, clearly demonstrating their ability to accurately summarize XML databases with mixed-value content. To the best of our knowledge, ours is the first work to address the summarization problem for structured XML content in its full generality.

#index 864451
#* Robust Cardinality and Cost Estimation for Skyline Operator
#@ Surajit Chaudhuri;Nilesh Dalvi;Raghav Kaushik
#t 2006
#c 17
#! Incorporating the skyline operator inside the relational engine requires solving the cardinality estimation and the cost estimation problem, hitherto unaddressed. We propose robust techniques to estimate the cardinality and the computational cost of Skyline, and through an empirical comparison, show that our technique is substantially more effective than traditional approaches. Finally, we show through an implementation in Microsoft SQL Server that skyline queries can substantially benefit from our techniques.

#index 864452
#* SUBSKY: Efficient Computation of Skylines in Subspaces
#@ Yufei Tao;Xiaokui Xiao;Jian Pei
#t 2006
#c 17
#! Given a set of multi-dimensional points, the skyline contains the best points according to any preference function that is monotone on all axes. In practice, applications that require skyline analysis usually provide numerous candidate attributes, and various users depending on their interests may issue queries regarding different (small) subsets of the dimensions. Formally, given a relation with a large number (e.g.,ge 10) of attributes, a query aims at finding the skyline in an arbitrary subspace with a low dimensionality (e.g., 2). The existing algorithms do not support subspace skyline retrieval efficiently because they (i) require scanning the entire database at least once, or (ii) are optimized for one particular subspace but incur significant overhead for other subspaces. In this paper, we propose a technique SUBSKY which settles the problem using a single B-tree, and can be implemented in any relational database. The core of SUBSKY is a transformation that converts multi-dimensional data to 1D values, and enables several effective pruning heuristics. Extensive experiments with real data confirm that SUBSKY outperforms alternative approaches significantly in both efficiency and scalability.

#index 864453
#* Skyline Queries Against Mobile Lightweight Devices in MANETs
#@ Zhiyong Huang;Christian S. Jensen;Hua Lu;Beng Chin Ooi
#t 2006
#c 17
#! Skyline queries are well suited when retrieving data according to multiple criteria. While most previous work has assumed a centralized setting this paper considers skyline querying in a mobile and distributed setting, where each mobile device is capable of holding only a portion of the whole dataset; where devices communicate through mobile ad hoc networks; and where a query issued by a mobile user is interested only in the user's local area, although a query generally involves data stored on many mobile devices due to the storage limitations. We present techniques that aim to reduce the costs of communication among mobile devices and reduce the execution time on each single mobile device. For the former, skyline query requests are forwarded among mobile devices in a deliberate way, such that the amount of data to be transferred is reduced. For the latter, specific optimization measures are proposed for resource-constrained mobile devices. We conduct extensive experiments to show that our proposal performs efficiently in real mobile devices and simulated wireless ad hoc networks.

#index 864454
#* Approximately Processing Multi-granularity Aggregate Queries over Data Streams
#@ Shouke Qin;Weining Qian;Aoying Zhou
#t 2006
#c 17
#! Aggregate monitoring over data streams is attracting more and more attention in research community due to its broad potential applications. Existing methods suffer two problems, 1) The aggregate functions which could be monitored are restricted to be first-order statistic or monotonic with respect to the window size. 2) Only a limited number of granularity and time scales could be monitored over a stream, thus some interesting patterns might be neglected, and users might be misled by the incomplete changing profile about current data streams. These two impede the development of online mining techniques over data streams, and some kind of breakthrough is urged. In this paper, we employed the powerful tool of fractal analysis to enable the monitoring of both monotonic and non-monotonic aggregates on time-changing data streams. The monotony property of aggregate monitoring is revealed and monotonic search space is built to decrease the time overhead for accessing the synopsis from O(m) to O(logm), where m is the number of windows to be monitored. With the help of a novel inverted histogram, the statistical summary is compressed to be fit in limited main memory, so that high aggregates on windows of any length can be detected accurately and efficiently on-line. Theoretical analysis show the space and time complexity bound of this method are relatively low, while experimental results prove the applicability and efficiency of the proposed algorithm in different application settings.

#index 864455
#* A Sampling-Based Approach to Optimizing Top-k Queries in Sensor Networks
#@ Adam Silberstein Silberstein;Rebecca Braynard;Carla Ellis;Kamesh Munagala;Jun Yang
#t 2006
#c 17
#! Wireless sensor networks generate a vast amount of data. This data, however, must be sparingly extracted to conserve energy, usually the most precious resource in battery-powered sensors. When approximation is acceptable, a model-driven approach to query processing is effective in saving energy by avoiding contacting nodes whose values can be predicted or are unlikely to be in the result set. To optimize queries such as top-k, however, reasoning directly with models of joint probability distributions can be prohibitively expensive. Instead of using models explicitly, we propose to use samples of past sensor readings. Not only are such samples simple to maintain, but they are also computationally efficient to use in query optimization. With these samples, we can formulate the problem of optimizing approximate top-k queries under an energy constraint as a linear program. We demonstrate the power and flexibility of our sampling-based approach by developing a series of topk query planning algorithms with linear programming, which are capable of efficiently producing plans with better performance and novel features. We show that our approach is both theoretically sound and practically effective on simulated and real-world datasets.

#index 864456
#* Précis: The Essence of a Query Answer
#@ Georgia Koutrika;Alkis Simitsis;Yannis Ioannidis
#t 2006
#c 17
#! Wide spread use of database systems in modern society has brought the need to provide inexperienced users with the ability to easily search a database with no specific knowledge of a query language. Several recent research efforts have focused on supporting keyword-based searches over relational databases. This paper presents an alternative proposal and introduces the idea of précis queries. These are free-form queries whose answer (a précis) is a synthesis of results, containing not only information directly related to the query selections but also information implicitly related to them in various ways. Our approach to précis queries includes two additional novelties: (a) queries do not generate individual relations but entire multi-relation databases; and (b) query results are personalized to user-specific and/or domain requirements. We develop a framework and system architecture for supporting such queries in the context of a relational database system and describe algorithms that implement the required functionality. Finally, we present a set of experimental results that evaluate the proposed algorithms and show the potential of this work.

#index 864457
#* ProcessingWindow Queries in Wireless Sensor Networks
#@ Yingqi Xu;Wang-Chien Lee;Jianliang Xu;Gail Mitchell
#t 2006
#c 17
#! The existing query processing techniques for sensor networks rely on a network infrastructure for query propagation and data collection. However, such an infrastructure is very susceptible to network topology transients that widely exist in sensor networks. In this paper, we propose an infrastructure-free window query processing technique for sensor networks, called itinerary-based window query execution (IWQE), in which query propagation and data collection are combined into one single stage and executed along a well-designed itinerary inside a query window. We study the parameters for setting up an itinerary (e.g., width and route) and incorporate into IWQE three data collection schemes based on different performance trade-offs. Finally we demonstrate, by extensive simulations, the superior energy-time efficiency, robustness, and accuracy of IWQE over the current state-of-the-art techniques in supporting window queries under various network conditions.

#index 864458
#* Effective Density Queries on ContinuouslyMoving Objects
#@ Christian S. Jensen;Dan Lin;Beng Chin Ooi;Rui Zhang
#t 2006
#c 17
#! This paper assumes a setting where a population of objects move continuously in the Euclidean plane. The position of each object, modeled as a linear function from time to points, is assumed known. In this setting, the paper studies the querying for dense regions. In particular, the paper defines a particular type of density query with desirable properties and then proceeds to propose an algorithm for the efficient computation of density queries. While the algorithm may exploit any existing index for the current and near-future positions of moving objects, the Bx-tree is used. The paper reports on an extensive empirical study, which elicits the performance properties of the algorithm.

#index 864459
#* Efficient Aggregation of Ranked Inputs
#@ Nikos Mamoulis;Kit Hung Cheng;Man Lung Yiu;David W. Cheung
#t 2006
#c 17
#! A top-k query combines different rankings of the same set of objects and returns the k objects with the highest combined score according to an aggregate function. We bring to light some key observations, which impose two phases that any top-k algorithm, based on sorted accesses, should go through. Based on them, we propose a new algorithm, which is designed to minimize the number of object accesses, the computational cost, and the memory requirements of top-k search. Adaptations of our algorithm for search variants (exact scores, on-line and incremental search, top-k joins, other aggregate functions, etc.) are also provided. Extensive experiments with synthetic and real data show that, compared to previous techniques, our method accesses fewer objects, while being orders of magnitude faster.

#index 864460
#* CLAN: An Algorithm for Mining Closed Cliques from Large Dense Graph Databases
#@ Jianyong Wang;Zhiping Zeng;Lizhu Zhou
#t 2006
#c 17
#! Most previously proposed frequent graph mining algorithms are intended to find the complete set of all frequent, closed subgraphs. However, in many cases only a subset of the frequent subgraphs with a certain topology is of special interest. Thus, the method of mining the complete set of all frequent subgraphs is not suitable for mining these frequent subgraphs of special interest as it wastes considerable computing power and space on uninteresting subgraphs. In this paper we develop a new algorithm, CLAN, to mine the frequent closed cliques, the most coherent structures in the graph setting. By exploring some properties of the clique pattern, we can simplify the canonical label design and the corresponding clique (or subclique) isomorphism testing. Several effective pruning methods are proposed to prune the search space, while the clique closure checking scheme is used to remove the non-closed clique patterns. Our empirical results show that CLAN is very efficient for large dense graph databases with which the traditional graph mining algorithms fail. The novelty of our method is further demonstrated by the application of CLAN in mining highly correlated stocks from large stock market data.

#index 864461
#* A Partition-Based Approach to Graph Mining
#@ Junmei Wang;Wynne Hsu;Mong Li Lee;Chang Sheng
#t 2006
#c 17
#! Existing graph mining algorithms typically assume that databases are relatively static and can fit into the main memory. Mining of subgraphs in a dynamic environment is currently beyond the scope of these algorithms. To bridge this gap, we first introduce a partition-based approach called PartMiner for mining graphs. The PartMiner algorithm finds the frequent subgraphs by dividing the database into smaller and more manageable units, mining frequent subgraphs on these smaller units and finally combining the results of these units to losslessly recover the complete set of subgraphs in the database. Next, we extend PartMiner to handle updates in the dynamic environment. Experimental results indicate that PartMiner is effective and scalable in finding frequent subgraphs, and outperforms existing algorithms in the presence of updates.

#index 864462
#* Dual Labeling: Answering Graph Reachability Queries in Constant Time
#@ Haixun Wang;Hao He2;Jun Yang;Philip S. Yu;Jeffrey Xu Yu
#t 2006
#c 17
#! Graph reachability is fundamental to a wide range of applications, including XML indexing, geographic navigation, Internet routing, ontology queries based on RDF/OWL, etc. Many applications involve huge graphs and require fast answering of reachability queries. Several reachability labeling methods have been proposed for this purpose. They assign labels to the vertices, such that the reachability between any two vertices may be decided using their labels only. For sparse graphs, 2-hop based reachability labeling schemes answer reachability queries efficiently using relatively small label space. However, the labeling process itself is often too time consuming to be practical for large graphs. In this paper, we propose a novel labeling scheme for sparse graphs. Our scheme ensures that graph reachability queries can be answered in constant time. Furthermore, for sparse graphs, the complexity of the labeling process is almost linear, which makes our algorithm applicable to massive datasets. Analytical and experimental results show that our approach is much more efficient than stateof- the-art approaches. Furthermore, our labeling method also provides an alternative scheme to tradeoff query time for label space, which further benefits applications that use tree-like graphs.

#index 864463
#* Reverse Nearest Neighbors Search in Ad-hoc Subspaces
#@ Man Lung Yiu;Nikos Mamoulis
#t 2006
#c 17
#! Given an object q, modeled by a multidimensional point, a reverse nearest neighbors (RNN) query returns the set of objects in the database that have q as their nearest neighbor. In this paper, we study an interesting generalization of the RNN query, where not all dimensions are considered, but only an ad-hoc subset thereof. The rationale is that (i) the dimensionality might be too high for the result of a regular RNN query to be useful, (ii) missing values may implicitly define a meaningful subspace for RNN retrieval, and (iii) analysts may be interested in the query results only for a set of (ad-hoc) problem dimensions (i.e., object attributes). We consider a suitable storage scheme and develop appropriate algorithms for projected RNN queries, without relying on multidimensional indexes. Our methods are experimentally evaluated with real and synthetic data.

#index 864464
#* Continuous Reverse Nearest Neighbor Monitoring
#@ Tian Xia;Donghui Zhang
#t 2006
#c 17
#! Continuous spatio-temporal queries have recently received increasing attention due to the abundance of location-aware applications. This paper addresses the Continuous Reverse Nearest Neighbor (CRNN) Query. Given a set of objects O and a query set Q, the CRNN query monitors the exact reverse nearest neighbors of each query point, under the model that both the objects and the query points may move unpredictably. Existing methods for the reverse nearest neighbor (RNN) query either are static or assume a priori knowledge of the trajectory information, and thus do not apply. Related recent work on continuous range query and continuous nearest neighbor query relies on the fact that a simple monitoring region exists. Due to the unique features of the RNN problem, it is non-trivial to even define a monitoring region for the CRNN query. This paper defines the monitoring region for the CRNN query, discusses how to perform initial computation, and then focuses on incremental CRNN monitoring upon updates. The monitoring region according to one query point consists of two types of regions. We argue that the two types should be handled separately. In continuous monitoring, two optimization techniques are proposed. Experimental results prove that our proposed approach is both efficient and scalable.

#index 864465
#* An Efficient XPath Query Processor for XML Streams
#@ Yi Chen;Susan B. Davidson;Yifeng Zheng
#t 2006
#c 17
#! Streaming XPath evaluation algorithms must record a potentially exponential number of pattern matches when both predicates and descendant axes are present in queries, and the XML data is recursive. In this paper, we use a compact data structure to encode these pattern matches rather than storing them explicitly. We then propose a polynomial time streaming algorithm to evaluate XPath queries by probing the data structure in a lazy fashion. Extensive experiments show that our approach not only has a good theoretical complexity bound but is also efficient in practice.

#index 864466
#* Surface k-NN Query Processing
#@ Ke Deng;Heng Tao Shen;Kai Xu;Xuemin Lin
#t 2006
#c 17
#! A k-NN query finds the k nearest-neighbors of a given point from a point database. When it is sufficient to measure object distance using the Euclidian distance, the key to efficient k-NN query processing is to fetch and check the distances of a minimum number of points from the database. For many applications, such as vehicle movement along road networks or rover and animal movement along terrain surfaces, the distance is only meaningful when it is along a valid movement path. For this type of k-NN queries, the focus of efficient query processing is to minimize the cost of computing distances using the environment data (such as the road network data and the terrain data), which can be several orders of magnitude larger than that of the point data. Efficient processing of k-NN queries based on the Euclidian distance or the road network distance has been investigated extensively in the past. In this paper, we investigate the problem of surface k-NN query processing, where the distance is calculated from the shortest path along a terrain surface. This problem is very challenging, as the terrain data can be very large and the computational cost of finding shortest paths is very high. We propose an efficient solution based on multiresolution terrain models. Our approach eliminates the need of costly process of finding shortest paths by ranking objects using estimated lower and upper bounds of distance on multiresolution terrain models.

#index 864467
#* SketchTree: Approximate Tree Pattern Counts over Streaming Labeled Trees
#@ Praveen Rao;Bongki Moon
#t 2006
#c 17
#! In recent years, there has been a rising interest in developing online approximation algorithms for data streams. Some of the key challenges are posed by the fact that streaming data can be read only once in a fixed order of arrival and only a limited amount of memory is available for storage. In this paper, we address the problem of approximately counting tree patterns over a stream of labeled trees (e.g., XML documents). We propose a new approximation algorithm called SketchTree that computes a synopsis of the stream in a single pass by processing each tree only once. Using a limited amount of memory, SketchTree provides approximate answers for both ordered and unordered tree pattern counts. Furthermore, we discuss a class of count queries that can be handled by SketchTree and their utility. We provide theoretical analyses to show that our algorithm has provably strong guarantees on the error bounds. Experiments on real datasets demonstrate that SketchTree can indeed estimate tree pattern counts within 10-15% relative error with high confidence under various situations.

#index 864468
#* Characterizing and Exploiting Reference Locality in Data Stream Applications
#@ Feifei Li;Ching Chang;George Kollios;Azer Bestavros
#t 2006
#c 17
#! In this paper, we investigate a new approach to process queries in data stream applications. We show that reference locality characteristics of data streams could be exploited in the design of superior and flexible data stream query processing techniques. We identify two different causes of reference locality: popularity over long time scales and temporal correlations over shorter time scales. An elegant mathematical model is shown to precisely quantify the degree of those sources of locality. Furthermore, we analyze the impact of locality-awareness on achievable performance gains over traditional algorithms on applications such asMAX-subset approximate sliding window join and approximate count estimation. In a comprehensive experimental study, we compare several existing algorithms against our locality-aware algorithms over a number of real datasets. The results validate the usefulness and efficiency of our approach.

#index 864469
#* MONDRIAN: Annotating and Querying Databases through Colors and Blocks
#@ Floris Geerts;Anastasios Kementsietsidis;Diego Milano
#t 2006
#c 17
#! Annotations play a central role in the curation of scientific databases. Despite their importance, data formats and schemas are not designed to manage the increasing variety of annotations. Moreover, DBMS's often lack support for storing and querying annotations. Furthermore, annotations and data are only loosely coupled. This paper introduces an annotation-oriented data model for the manipulation and querying of both data and annotations. In particular, the model allows for the specification of annotations on sets of values and for effectively querying the information on their association. We use the concept of block to represent an annotated set of values. Different colors applied to the blocks represent different annotations. We introduce a color query language for our model and prove it to be both complete (it can express all possible queries over the class of annotated databases), and minimal (all the algebra operators are primitive). We present MONDRIAN, a prototype implementation of our annotation mechanism, and we conduct experiments that investigate the set of parameters which influence the evaluation cost for color queries.

#index 864470
#* Warehousing and Analyzing Massive RFID Data Sets
#@ Hector Gonzalez;Jiawei Han;Xiaolei Li;Diego Klabjan
#t 2006
#c 17
#! Radio Frequency Identification (RFID) applications are set to play an essential role in object tracking and supply chain management systems. In the near future, it is expected that every major retailer will use RFID systems to track the movement of products from suppliers to warehouses, store backrooms and eventually to points of sale. The volume of information generated by such systems can be enormous as each individual item (a pallet, a case, or an SKU) will leave a trail of data as it moves through different locations. As a departure from the traditional data cube, we propose a new warehousing model that preserves object transitions while providing significant compression and path-dependent aggregates, based on the following observations: (1) items usually move together in large groups through early stages in the system (e.g., distribution centers) and only in later stages (e.g., stores) do they move in smaller groups, and (2) although RFID data is registered at the primitive level, data analysis usually takes place at a higher abstraction level. Techniques for summarizing and indexing data, and methods for processing a variety of queries based on this framework are developed in this study. Our experiments demonstrate the utility and feasibility of our design, data structure, and algorithms.

#index 864471
#* Query Decomposition: A Multiple Neighborhood Approach to Relevance Feedback Processing in Content-based Image Retrieval
#@ Kien A. Hua;Ning Yu;Danzhou Liu
#t 2006
#c 17
#! Today's Content-Based Image Retrieval (CBIR) techniques are based on the "k-nearest neighbors" (k- NN) model. They retrieve images from a single neighborhood using low-level visual features. In this model, semantically similar images are assumed to be clustered in the high-dimensional feature space. Unfortunately, no visual-based feature vector is sufficient to facilitate perfect semantic clustering; and semantically similar images with different appearances are always clustered into distinct neighborhoods in the feature space. Confinement of the search results to a single neighborhood is an inherent limitation of the k-NN techniques. In this paper we consider a new image retrieval paradigm — the Query Decomposition model - that facilitates retrieval of semantically similar images from multiple neighborhoods in the feature space. The retrieval results are the k most similar images from different relevant clusters. We introduce a prototype, and present experimental results to illustrate the effectiveness and efficiency of this new approach to content-based image retrieval.

#index 864472
#* Nearest Surrounder Queries
#@ Ken C. K. Lee;Wang-Chien Lee;Hong Va Leong
#t 2006
#c 17
#! In this paper, we study a new type of spatial query, Nearest Surrounder (NS), which searches the nearest surrounding spatial objects around a query point. NS query can be more useful than conventional nearest neighbor (NN) query as NS query takes the object orientation into consideration. To address this new type of query, we identify angle-based bounding properties and distance-bound properties of Rtree index. The former has not been explored for conventional spatial queries. With these identified properties, we propose two algorithms, namely, Sweep and Ripple. Sweep searches surrounders according to their orientation, while Ripple searches surrounders ordered by their distances to the query point. Both algorithms can deliver result incrementally with a single dataset lookup. We also consider the multiple-tier NS (mNS) query that searches multiple layers of NSs. We evaluate the algorithms and report their performance on both synthetic and real datasets.

#index 864473
#* Closest-Point-of-Approach Join for Moving Object Histories
#@ Subramanian Arumugam;Christopher Jermaine
#t 2006
#c 17
#! In applications that produce a large amount of data describing the paths of moving objects, there is a need to ask questions about the interaction of objects over a long recorded history. In this paper, we consider the problem of computing joins over massive moving object histories. The particular join that we study is the "Closest-Point-Of- Approach" join, which asks: Given a massive moving object history, which objects approached within a distance 'd' of one another? We carefully consider several relatively obvious strategies for computing the answer to such a join, and then propose a novel, adaptive join algorithm which naturally alters the way in which it computes the join in response to the characteristics of the underlying data.

#index 864474
#* Declarative Querying for Biological Sequences
#@ Sandeep Tata;James S. Friedman;Anand Swaroop
#t 2006
#c 17
#! The ongoing revolution in life sciences research is producing vast amounts of genetic and proteomic sequence data. Scientists want to pose increasingly complex queries on this data, but current methods for querying biological sequences are primitive and largely procedural. This limits the ease with which complex queries can be posed, and often results in very inefficient query plans. There is a growing and urgent need for declarative and efficient methods for querying biological sequence data. In this paper, we introduce a system called Periscope/SQ which addresses this need. Queries in our system are based on a well-defined extension of relational algebra. We introduce new physical operators and support for novel indexes in the database. As part of the optimization framework, we describe a new technique for selectivity estimation of string pattern matching predicates that is more accurate than previous methods. We also describe a simple, yet highly effective algorithm to optimize sequence queries. Finally, using a real-world application in eye genetics, we show how Periscope/SQ can be used to achieve a speedup of two orders of magnitude over existing procedural methods!

#index 864475
#* Searching Substructures with Superimposed Distance
#@ Xifeng Yan;Feida Zhu;Jiawei Han;Philip S. Yu
#t 2006
#c 17
#! Efficient indexing techniques have been developed for the exact and approximate substructure search in large scale graph databases. Unfortunately, the retrieval problem of structures with categorical or geometric distance constraints is not solved yet. In this paper, we develop a method called PIS (Partition-based Graph Index and Search) to support similarity search on substructures with superimposed distance constraints. PIS selects discriminative fragments in a query graph and uses an index to prune the graphs that violate the distance constraints. We identify a criterion to distinguish the selectivity of fragments in multiple graphs and develop a partition method to obtain a set of highly selective fragments, which is able to improve the pruning performance. Experimental results show that PIS is effective in processing real graph queries.

#index 864476
#* Mining Shifting-and-Scaling Co-Regulation Patterns on Gene Expression Profiles
#@ Xin Xu;Ying Lu;Anthony K.  H. Tung;Wei Wang
#t 2006
#c 17
#! In this paper, we propose a new model for coherent clustering of gene expression data called reg-cluster. The proposed model allows (1) the expression profiles of genes in a cluster to follow any shifting-and-scaling patterns in subspace, where the scaling can be either positive or negative, and (2) the expression value changes across any two conditions of the cluster to be significant. No previous work measures up to the task that we have set: the density-based subspace clustering algorithms require genes to have similar expression levels to each other in subspace; the pattern-based biclustering algorithms only allow pure shifting or pure scaling patterns; and the tendency-based biclustering algorithms have no coherence guarantees. We also develop a novel patternbased biclustering algorithm for identifying shifting-andscaling co-regulation patterns, satisfying both coherence constraint and regulation constraint. Our experimental results show that the reg-cluster algorithm is able to detect a significant amount of clusters missed by previous models, and these clusters are potentially of high biological significance.

#index 864477
#* AutoGlobe: An Automatic Administration Concept for Service-Oriented Database Applications
#@ Stefan Seltzsam;Daniel Gmach;Stefan Krompass;Alfons Kemper
#t 2006
#c 17
#! Future database application systems will be designed as Service Oriented Architectures (SOAs) like SAP's NetWeaver instead of monolithic software systems such as SAP's R/3. The decomposition in finer-grained services allows the usage of hardware clusters and a flexible serviceto- server allocation but also increases the complexity of administration. Thus, new administration techniques like our self-organizing infrastructure that we developed in cooperation with the SAP Adaptive Computing Infrastructure (ACI) group are necessary. For our purpose the available hardware is virtualized, pooled, and monitored. A fuzzy logic based controller module supervises all services running on the hardware platform and remedies exceptional situations automatically. With this self-organizing infrastructure we reduce the necessary hardware and administration overhead and, thus, lower the total cost of ownership (TCO). We used our prototype implementation, called Auto- Globe, for SAP-internal tests and we performed comprehensive simulation studies to demonstrate the effectiveness of our proposed concept.

#index 864478
#* Collaborative Business Process Support in IHE XDS through ebXML Business Processes
#@ Asuman Dogac;Veli Bicer;Alper Okcan
#t 2006
#c 17
#! Currently, clinical information is stored in all kinds of proprietary formats through a multitude of medical information systems available on the market. This results in a severe interoperability problem in sharing electronic healthcare records. To address this problem, an industry initiative, called "Integrating Healthcare Enterprise (IHE)" has specified the "Cross Enterprise Document Sharing (XDS)" Profile to store healthcare documents in an ebXML registry/ repository to facilitate their sharing. Through a separate effort, IHE has also defined interdepartmental Workflow Profiles to identify the transactions required to integrate information flow among several information systems. Although the clinical documents stored in XDS registries are obtained as a result of executing these workflows, IHE has not yet specified collaborative healthcare processes for the XDS. Hence, there is no way to track the workflows in XDS and the clinical documents produced through the workflows are manually inserted into the registry/ repository. Given that IHE XDS is using the ebXML architecture, the most natural way to integrate IHE Workflow Profiles to IHE XDS is using ebXML Business Processes (ebBP). In this paper, we describe the implementation of an enhanced IHE architecture demonstrating how ebXML Business Processes, IHE Workflow Profiles and the IHE XDS architecture can all be integrated to provide collaborative business process support in the healthcare domain.

#index 864479
#* Taming Compliance with Sarbanes-Oxley Internal Controls Using Database Technology
#@ Rakesh Agrawal;Christopher Johnson;Jerry Kiernan;Frank Leymann
#t 2006
#c 17
#! The Sarbanes-Oxley Act instituted a series of corporate reforms to improve the accuracy and reliability of financial reporting. Sections 302 and 404 of the Act require SEC-reporting companies to implement internal controls over financial reporting, periodically assess the effectiveness of these internal controls, and certify the accuracy of their financial statements. We suggest that database technology can play an important role in assisting compliance with the internal control provisions of the Act. The core components of our solution include: (i) modeling of required workflows, (ii) active enforcement of control activities, (iii) auditing of actual workflows to verify compliance with internal controls, and (iv) discovery-driven OLAP to identify irregularities in financial data. We illustrate how the features of our solution fulfill Sarbanes-Oxley requirements using several real-life scenarios. In the process, we identify opportunities for new database research.

#index 864480
#* RDF Object Type and Reification in the Database
#@ Nicole Alexander;Siva Ravada
#t 2006
#c 17
#! The Resource Description Framework (RDF) is a standard for representing information that can be identified using a Uniform Resource Identifier. In particular, it is intended for representing metadata about Web resources. RDF is being used in numerous application areas, including Life Sciences, Digital Libraries, and Intelligence. The RDF data structure is a directed graph or network. Current solutions to managing RDF data utilize flat relational tables for database storage. This paper presents an alternative approach to managing RDF data in the database. We introduce a new object type for storing RDF data. The object type is built on top of the Oracle Spatial Network Data Model (NDM), which is Oracle's network solution in the database. This exposes the NDM functionality to RDF data, allowing RDF data to be managed as objects and analyzed as networks. Reification, a means of providing metadata for the RDF data, puts a strain on storage. We present a streamlined approach to representing reified RDF data for faster retrievals. An RDF object type and reification in the database provide the basic infrastructure for effective metadata management.

#index 864481
#* RDF/RDFS-based Relational Database Integration
#@ Huajun Chen;Zhaohui Wu;Heng Wang;Yuxin Mao
#t 2006
#c 17
#! We study the problem of answering queries through a RDF/RDFS ontology, given a set of view-based mappings between one or more relational schemas and this target ontology. Particularly, we consider a set of RDFS semantic constraints such as rdfs:subClassof, rdfs:subPropertyof, rdfs:domain, and rdfs:range, which are present in RDF model but neither XML nor relational models. We formally define the query semantics in such an integration scenario, and design a novel query rewriting algorithm to implement the semantics. On our approach, we highlight the important role played by RDF Blank Node in representing incomplete semantics of relational data. A set of semantic tools supporting relational data integration by RDF are also introduced. The approach have been used to integrate 70 relational databases at China Academy of Traditional Chinese Medicine.

#index 864482
#* Supporting Keyword Columns with Ontology-based Referential Constraints in DBMS
#@ Eugene Inseok Chong;Souripriya Das;George Eadon;Jagannathan Srinivasan
#t 2006
#c 17
#! Keywords are typically used to qualify rows in a table. However, the fact that a keyword denotes a concept, which belongs to a specific knowledge domain, is not semantically enforced in current database systems. This paper proposes defining ontology based referential constraint for such keyword columns. A query on ontology, specified as part of the referential constraint, is used to identify the domain for the keyword column. Furthermore, since ontology may evolve causing change to the domain of the keyword column, the paper proposes use of ontology based transformation functions to either automatically evolve or to recommend refinements for the values in the keyword column. Also, queries on a keyword column can perform semantic match, that is, match a keyword to related terms based on the associated ontology. Thus, the proposed approach of semantically connecting keyword columns to ontologies 1) enhances semantic data integrity, 2) facilitates evolution of keyword columns with the referenced ontology, and 3) enables semantic match queries on keyword columns.

#index 864483
#* Experiment Management with Metadata-based Integration for Collaborative Scientific Research
#@ Fusheng Wang;Peiya Liu;John Pearson;Fred Azar;Gerald Madlmayr
#t 2006
#c 17
#! Scientific research in many fields is increasingly a collaborative effort across multiple institutions and disciplines. Scientific researchers need not only an effective system to manage their data, results, and the experiments that generate the results, but also a platform to integrate, share and search these across multiple institutions. Therefore, researchers are able to reuse experiments, pool expertise and validate approaches. In this paper, we present Sci- Port, a system of experiment management and integration for collaborative scientific research. SciPort's architecture uses i) a general transformation-based data model to represent and link experiment processes; ii) hierarchical data classification across multiple institutions according to research programs' goals and organization; iii) metadatacentric representation that concisely captures the context of experiments; and iv) virtual data integration through centralized metadata integration. The system is built for open source, and the metadata-based representation and integration provides a unified framework and tool set to manage and share experiments for scientific research communities.

#index 864484
#* SQL to XQuery Translation in the AquaLogic Data Services Platform
#@ Sunil Jigyasu;Sujeet Banerjee;Vinayak Borkar;Michael Carey;Kanad Dixit;Anil Malkani;Sachin Thatte
#t 2006
#c 17
#! SQL has long been the standard language for retrieving and manipulating data in relational database systems. XML has become the standard format for data exchange, and XQuery is on its way to becoming the standard language for querying XML data. The BEA AquaLogic Data Services Platform provides a service-oriented, XML-based view of heterogeneous enterprise data sources and allows this view to be queried using XQuery. AquaLogic DSP includes a JDBC driver that connects the old (SQL) world with the new (XML) world via a SQL-to-XQuery translator. This paper outlines the issues related to creating such a driver and details the approach used to translate SQL queries into XQuery expressions. The paper also touches on performance considerations related to handling XML query results in a context where JDBC result sets are the desired output format.

#index 864485
#* On Pushing Multilingual Query Operators into Relational Engines
#@ A. Kumaran;Pavan K. Chowdary;Jayant R. Haritsa
#t 2006
#c 17
#! To effectively support today's global economy, database systems need to manage data in multiple languages simultaneously. While current database systems do support the storage and management of multilingual data, they are not capable of querying across different natural languages. To address this lacuna, we have recently proposed two cross-lingual functionalities, LexEQUAL[13] and SemEQUAL[14], for matching multilingual names and concepts, respectively. In this paper, we investigate the native implementation of these multilingual functionalities as first-class operators on relational engines. Specifically, we propose a new multilingual storage datatype, and an associated algebra of the multilingual operators on this datatype. These components have been successfully implemented in the PostgreSQL database system, including integration of the algebra with the query optimizer and inclusion of a metric index in the access layer. Our experiments demonstrate that the performance of the native implementation is up to two orders-of-magnitude faster than the corresponding outsidethe- server implementation. Further, these multilingual additions do not adversely impact the existing functionality and performance. To the best of our knowledge, our prototype represents the first practical implementation of a crosslingual database query engine.

#index 864486
#* SIREN: A Memory-Conserving, Snapshot-Consistent Checkpoint Algorithm for in-Memory Databases
#@ Antti-Pekka Liedes;Antoni Wolski
#t 2006
#c 17
#! Checkpoint of an in-memory database is the main source of a persistent database image surviving a software crash, or a power outage, and is, together with transactions logs, a foundation for transaction durability. Since checkpoints are created simultaneously with transaction processing, they tend to decrease database throughput and increase its memory footprint. Of the current methods, most efficient are the fuzzy checkpoint algorithms that write dirty pages to disk and require transaction logs for reconstructing a consistent state. Known consistency-preserving methods suffer from excessive memory usage or a transaction-blocking behavior. In this paper, we present a consistency-preserving and memory-efficient checkpoint method. It is based on tuple shadowing as opposed to known page shadowing methods, and rearranging of tuples between pages for minimal memory usage overhead. The method's algorithms are introduced and both analytical and experimental analysis of the proposed algorithms show significant reduction in the memory usage overhead, and up to 30% higher transaction throughput compared with a fuzzy checkpoint method with undo/redo log.

#index 864487
#* Space-Partitioning Trees in PostgreSQL: Realization and Performance
#@ Mohamed Y. Eltabakh;Ramy Eltarras;Walid G. Aref
#t 2006
#c 17
#! Many evolving database applications warrant the use of non-traditional indexing mechanisms beyond B+-trees and hash tables. SP-GiST is an extensible indexing framework that broadens the class of supported indexes to include disk-based versions of a wide variety of space-partitioning trees, e.g., disk-based trie variants, quadtree variants, and kd-trees. This paper presents a serious attempt at implementing and realizing SP-GiST-based indexes inside PostgreSQL. Several index types are realized inside PostgreSQL facilitated by rapid SP-GiST instantiations. Challenges, experiences, and performance issues are addressed in the paper. Performance comparisons are conducted from within PostgreSQL to compare update and search performances of SP-GiST-based indexes against the B+-tree and the R-tree for string, point, and line segment data sets. Interesting results that highlight the potential performance gains of SPGiST- based indexes are presented in the paper.

#index 864488
#* Automatic Sales Lead Generation from Web Data
#@ Ganesh Ramakrishnan;Sachindra Joshi;Sumit Negi;Raghu Krishnapuram;Sreeram Balakrishnan
#t 2006
#c 17
#! Speed to market is critical to companies that are driven by sales in a competitive market. The earlier a potential customer can be approached in the decision making process of a purchase, the higher are the chances of converting that prospect into a customer. Traditional methods to identify sales leads such as company surveys and direct marketing are manual, expensive and not scalable. Over the past decade the World Wide Web has grown into an information-mesh, with most important facts being reported through Web sites. Several news papers, press releases, trade journals, business magazines and other related sources are on-line. These sources could be used to identify prospective buyers automatically. In this paper, we present a system called ETAP (Electronic Trigger Alert Program) that extracts trigger events from Web data that help in identifying prospective buyers. Trigger events are events of corporate relevance and indicative of the propensity of companies to purchase new products associated with these events. Examples of trigger events are change in management, revenue growth and mergers & acquisitions. The unstructured nature of information makes the extraction task of trigger events difficult. We pose the problem of trigger events extraction as a classification problem and develop methods for learning trigger event classifiers using existing classification methods. We present methods to automatically generate the training data required to learn the classifiers. We also propose a method of feature abstraction that uses named entity recognition to solve the problem of data sparsity. We score and rank the trigger events extracted from ETAP for easy browsing. Our experiments show the effectiveness of the method and thus establish the feasibility of automatic sales lead generation using the Web data.

#index 864489
#* Load Balancing for Multi-tiered Database Systems through Autonomic Placement of Materialized Views
#@ Wen-Syan Li;Daniel C. Zilio;Vishal S. Batra;Mahadevan Subramanian;Calisto Zuzarte;Inderpal Narang
#t 2006
#c 17
#! A materialized view or Materialized Query Table (MQT) is an auxiliary table with precomputed data that can be used to significantly improve the performance of a database query. AMaterialized Query Table Advisor (MQTA) is often used to recommend and create MQTs. The state-of-the-art MQTA works in a standalone database server where MQTs are placed on the same server as that in which the base tables are located. The MQTA does not apply to a federated or scaleout scenario in which MQTs need to be placed on other servers close to applications (i.e. a frontend database server) for offloading the workload on the backend database server. In this paper, we propose a Data Placement Advisor (DPA) and load balancing strategies for multi-tiered database systems. Built on top of the MQTA, DPA recommends MQTs and advises placement strategies for minimizing the response time for a query workload. To demonstrate the benefit of the data placement advising, we implemented a prototype of DPA that works with theMQTA in the IBM® DB2® Universal Database^TM (DB2 UDB) and the IBM WebSphere® Information Integrator (WebSphere II). The evaluation results showed substantial improvements of workload response times when MQTs are intelligently recommended and placed at a frontend database server subject to space and load characteristics for TPC-H and OLAP type workloads.

#index 864490
#* Schema and Data Translation
#@ Paolo Atzeni
#t 2006
#c 17
#! The need to transform, integrate and exchange data is common to many application contexts. In databases, we often use different systems to handle data, with different models, and we therefore need to translate data and their description from one to another. The problem has been considered for decades, but definitive solutions are not yet available. The problem is relevant at the schema level, during the specification or design phase, and at the data level, when we have databases, and we want to translate them into some other system, which may be similar (for example, relational to relational) or completely different (for example, XML to relational or viceversa). In current practice, translation problems are often tackled by means of ad-hoc solutions, for example by writing code for each specific application, but this is clearly very heavy and hard to maintain.

#index 864491
#* Foundations of Automated Database Tuning
#@ Surajit Chaudhuri;Gerhard Weikum
#t 2006
#c 17
#! Our society is more dependent on information systems than ever before. However, managing the information systems infrastructure in a cost-effective manner is a growing challenge. The total cost of ownership (TCO) of information technology is increasingly dominated by people costs. In fact, mistakes in operations and administration of information systems are the single most reasons for system outage and unacceptable performance. For information systems to provide value to their customers, we must reduce the complexity associated with their deployment and usage.

#index 864492
#* Models and Methods for Privacy-Preserving Data Analysis and Publishing
#@ Johannes Gehrke
#t 2006
#c 17
#! The digitization of our daily lives has led to an explosion in the collection of data by governments, corporations, and individuals. Protection of confidentiality of this data is of utmost importance. However, knowledge of statistical properties of this private data can have significant societal benefit, for example, in decisions about the allocation of public funds based on Census data, or in the analysis of medical data from different hospitals to understand the interaction of drugs. This tutorial will survey recent research that builds bridges between the two seemingly conflicting goals of sharing data while preserving data privacy and confidentiality. The tutorial will cover definitions of privacy and disclosure, and associated methods how to enforce them.

#index 864493
#* Mining, Indexing, and Similarity Search in Graphs and Complex Structures
#@ Jiawei Han;Xifeng Yan;Philip S. Yu
#t 2006
#c 17
#! Scalable methods for mining, indexing, and similarity search in graphs and other complex structures, such as trees, lattices, and networks, have become increasingly important in data mining and database management. This is because a large set of emerging applications need to handle new kinds of objects with complex structures, such as trees (e.g., XML data), graphs (e.g., Web, chemical structures and biological graphs) and networks (e.g., social and biological networks). Such complicated data structures pose many new challenging research problems related to data mining, data management, and similarity search that do not exist in the traditional database and data mining studies.

#index 864494
#* Query Co-Processing on Commodity Hardware
#@ Anastassia Ailamaki;Naga K. Govindaraju;Dinesh Manocha
#t 2006
#c 17
#! The rapid increase in the data volumes for the past few decades has intensified the need for high processing power for database and data mining applications. Researchers have actively sought to design and develop new architectures for improving the performance. Recent research shows that the performance can be significantly improved using either (a) effective utilization of architectural features and memory hierarchies used by the conventional processors, or (b) the high computational power and memory bandwidth in commodity hardware such as network processing units (NPUs), and graphics processing units (GPUs). This seminar will survey the micro-architectural and architectural differences across these processors with data management in mind, and will present previous work and future opportunities for expanding query processing algorithms to other hardware than general-purpose processors. In addition to the database community, we intend to increase awareness in the computer architecture scene about opportunities to construct heterogeneous chips (chip multiprocessors with different architectures in them).

#index 864495
#* Efficient Continuous Skyline Computation
#@ Michael Morse;Jignesh M. Patel;William I. Grosky
#t 2006
#c 17
#! In a number of emerging streaming applications, the data values that are produced have an associated time interval for which they are valid. A useful computation over such streaming data sets is to produce a continuous and valid skyline summary. To the best of our knowledge, this problem has not been addressed before. In this paper we introduce an operator called the continuous time-interval skyline operator for evaluating this computation. We also present a new algorithm called LookOut for evaluating the continuous time-interval skyline efficiently, and empirically demonstrate the scalability of this algorithm.

#index 864496
#* Detecting Duplicates in Complex XML Data
#@ Melanie Weis;Felix Naumann
#t 2006
#c 17
#! Recent work both in the relational and the XML world have shown that the efficacy and efficiency of duplicate detection is enhanced by regarding relationships between entities. However, most approaches for XML data rely on 1:n parent/child relationships, and do not apply to XML data that represents m:n relationships. We present a novel comparison strategy, which performs duplicate detection effectively for all kinds of parent/child relationships, given dependencies between different XML elements. Due to cyclic dependencies, it is possible that a pairwise classification is performed more than once, which compromises efficiency. We propose an order that reduces the number of such reclassifications and apply it to two algorithms. The first algorithm performs reclassifications, and efficiency is increased by using the order reducing the number of reclassifications. The second algorithm does not perform a comparison more than once, and the order is used to miss few reclassifications and hence few potential duplicates.

#index 864497
#* SaveRF: Towards Efficient Relevance Feedback Search
#@ Heng Tao Shen;Beng Chin Ooi;Kian-Lee Tan
#t 2006
#c 17
#! In multimedia retrieval, a query is typically interactively refined towards the 'optimal' answers by exploiting user feedback. However, in existing work, in each iteration, the refined query is re-evaluated. This is not only inefficient but fails to exploit the answers that may be common between iterations. In this paper, we introduce a new approach called SaveRF (Save random accesses in Relevance Feedback) for iterative relevance feedback search. SaveRF predicts the potential candidates for the next iteration and maintains this small set for efficient sequential scan. By doing so, repeated candidate accesses can be saved, hence reducing the number of random accesses. In addition, efficient scan on the overlap before the search starts also tightens the search space with smaller pruning radius. We implemented SaveRF and our experimental study on real life data sets show that it can reduce the I/O cost significantly.

#index 864498
#* On the Inverse Classification Problem and its Applications
#@ Charu C. Aggarwal;Chen Chen;Jiawei Han
#t 2006
#c 17
#! In this paper, we discuss the inverse classification problem, in which we desire to define the features of an incomplete record in such a way that will result in a desired class label. Such an approach is useful in applications in which it is an objective to determine a set of actions to be taken in order to guide the data mining application towards a desired solution. This system can be used for a variety of decision support applications which have pre-determined task criteria.

#index 864499
#* MIC Framework: An Information-Theoretic Approach to Quantitative Association Rule Mining
#@ Yiping Ke;James Cheng;Wilfred Ng
#t 2006
#c 17
#! We propose a framework, called MIC, which adopts an information-theoretic approach to address the problem of quantitative association rule mining. In our MIC framework, we first discretize the quantitative attributes. Then, we compute the normalized mutual information between the attributes to construct a graph that indicates the strong informative-relationship between the attributes. We utilize the cliques in the graph to prune the unpromising attribute sets and hence the joined intervals between these attributes. Our experimental results show that the MIC framework significantly improves the mining speed. Importantly, we are able to obtain most of the high-confidence rules and the missing rules are shown to be less interesting.

#index 864500
#* Efficient Discovery of Emerging Frequent Patterns in ArbitraryWindows on Data Streams
#@ Xiaoming Jin;Xinqiang Zuo;Kwok-Yan Lam;Jianmin Wang;Jiaguang Sun
#t 2006
#c 17
#! This paper proposes an effective data mining technique for finding useful patterns in streaming sequences. At present, typical approaches to this problem are to search for patterns in a fixed-size window sliding through the stream of data being collected. The practical values of such approaches are limited in that, in typical application scenarios, the patterns are emerging and it is difficult, if not impossible, to determine a priori a suitable window size within which useful patterns may exist. It is therefore desirable to devise techniques that can identify useful patterns with arbitrary window sizes. Attempts to this problem are challenging, however, because it requires a highly efficient searching in a substantially bigger solution space. This paper presents a new method which includes firstly a pruning strategy to reduce the search space and secondly a mining strategy that adopts a dynamic index structure to allow efficient discovery of emerging patterns in a streaming sequence. Experimental results on real data and synthetic data show that the proposed method outperforms other existing schemes both in computational efficiency and effectiveness in finding useful patterns.

#index 864501
#* Top-Down Mining of Interesting Patterns from Very High Dimensional Data
#@ Hongyan Liu;Jiawei Han;Dong Xin;Zheng Shao
#t 2006
#c 17
#! Many real world applications deal with transactional data, characterized by a huge number of transactions (tuples) with a small number of dimensions (attributes). However, there are some other applications that involve rather high dimensional data with a small number of tuples. Examples of such applications include bioinformatics, survey-based statistical analysis, text processing, and so on. High dimensional data pose great challenges to most existing data mining algorithms. Although there are numerous algorithms dealing with transactional data sets, there are few algorithms oriented to very high dimensional data sets with a relatively small number of tuples.

#index 864502
#* Mining Dense Periodic Patterns in Time Series Data
#@ Chang Sheng;Wynne Hsu;Mong Li Lee
#t 2006
#c 17
#! Existing techniques to mine periodic patterns in time series data are focused on discovering full-cycle periodic patterns from an entire time series. However, many useful partial periodic patterns are hidden in long and complex time series data. In this paper, we aim to discover the partial periodicity in local segments of the time series data. We introduce the notion of character density to partition the time series into variable-length fragments and to determine the lower bound of each character's period. We propose a novel algorithm, called DPMiner, to find the dense periodic patterns in time series data. Experimental results on both synthetic and real-life datasets demonstrate that the proposed algorithm is effective and efficient to reveal interesting dense periodic patterns.

#index 864503
#* Private Updates to Anonymous Databases
#@ Alberto Trombetta;Elisa Bertino
#t 2006
#c 17
#! Suppose that Alice, owner of a k-anonymous database, needs to determine whether her database, when adjoined with a tuple owned by Bob, is still k-anonymous. Suppose moreover that access to the database is strictly controlled, because for example data are used for experiments that need to be maintained confidential. Clearly, allowing Alice to directly read the contents of the tuple breaks the privacy of Bob; on the other hand, the confidentiality of the database managed by Alice is violated once Bob has access to the contents of the database. Thus the problem is to check whether the database adjoined with the tuple is still k-anonymous, without letting Alice and Bob know the contents of, respectively, the tuple and the database. In this paper, we propose two protocols solving this problem.

#index 864504
#* Provable Security for Outsourcing Database Operations
#@ Sergei Evdokimov;Matthias Fischmann;Oliver Gunther
#t 2006
#c 17
#! Database outsourcing, whilst becoming more popular in recent years, is creating substantial security and privacy risks. In this paper, we assess cryptographic solutions to the problem that some client party (Alex) wants to outsource database operations on sensitive data sets to a service provider (Eve) without having to trust her. Contracts are an option, but for various reasons their effectiveness is limited [2]. Alex would rather like to use privacy homomorphisms [6], i.e., encryption schemes that transform relational data sets and queries into ciphertext such that (i) the data is securely hidden from Eve; and (ii) Eve computes hidden results from hidden queries that Alex can efficiently decrypt. Unfortunately, all privacy homomorphisms we know of lack a rigorous security analysis. Before they can be used in practice, we need formal definitions that are both sound and practical to assess their effectiveness.

#index 864505
#* Composition and Disclosure of Unlinkable Distributed Databases
#@ Bradley Malin;Latanya Sweeney
#t 2006
#c 17
#! An individual's location-visit pattern, or trail, can be leveraged to link sensitive data back to identity. We propose a secure multiparty computation protocol that enables locations to provably prevent such linkages. The protocol incorporates a controllable parameter specifying the minimum number of identities a sensitive piece of data must be linkable to via its trail.

#index 864506
#* Technique for Optimal Adaptation of Time-Dependent Workflows with Security Constraints
#@ Basit Shafiq;Arjmand Samuel;Elisa Bertino;Arif Ghafoor
#t 2006
#c 17
#! Distributed workflow based systems are widely used in various application domains including e-commerce, digital government, healthcare, manufacturing and many others. Workflows in these application domains are not restricted to the administrative boundaries of a single organization [1]. The tasks in a workflow need to be performed in a certain order and often times are subject to temporal constraints and dependencies [1, 2]. A key requirement for such workflow applications is to provide the right data to the right person at the right time. This requirement motivates for dynamic adaptations of workflows for dealing with changing environmental conditions and exceptions.

#index 864507
#* Segmentation of Publication Records of Authors from the Web
#@ Wei Zhang;Clement Yu;Neil Smalheiser;Vetle Torvik
#t 2006
#c 17
#! Publication records are often found in the authors' personal home pages. If such a record is partitioned into a list of semantic fields of authors, title, date, etc., the unstructured texts can be converted into structured data, which can be used in other applications. In this paper, we present PEPURS, a publication record segmentation system. It adopts a novel "Split and Merge" strategy. A publication record is split into segments; multiple statistical classifiers compute their likelihoods of belonging to different fields; finally adjacent segments are merged if they belong to the same field. PEPURS introduces the punctuation marks and their neighboring texts as a new feature to distinguish different roles of the marks. PEPURS yields high accuracy scores in experiments.

#index 864508
#* Text Classification Improved through Automatically Extracted Sequences
#@ Dou Shen;Jian-Tao Sun;Qiang Yang;Hui Zhao;Zheng Chen
#t 2006
#c 17
#! We propose to use the n-multigram model to help the automatic text classification task. This model could automatically discover the latent semantic sequences contained in the document set of each category. Based on the n-multigram model and the n-gram language model, we put forward two text classification algorithms. The experiments on RCV1 show that our proposed algorithm based on n-multigram model can achieve the similar classification performance compared with the one based on n-gram model. However, the model size of our algorithm is only 4.21% of the latter one. Another proposed algorithm based on the combination of nmultigram model and n-gram model improves the micro- F1 and macro-F1 values by 3.5% and 4.5% respectively which support the validity of our approach.

#index 864509
#* Holistic Query Interface Matching using Parallel Schema Matching
#@ Weifeng Su;Jiying Wang;Frederick Lochovsky
#t 2006
#c 17
#! Using query interfaces of different Web databases, we propose a new complex schema matching approach, Parallel Schema Matching (PSM). A parallel schema is formed by comparing two individual schemas and deleting common attributes. The attribute matching can be discovered from the attribute-occurrence patterns if many parallel schemas are available. A count-based greedy algorithm identifies which attributes are more likely to be matched. Experiments show that PSM can identify both simple matching and complex matching accurately and efficiently.

#index 864510
#* Extracting Objects from the Web
#@ Zaiqing Nie;Fei Wu2;Ji-Rong Wen;Wei-Ying Ma
#t 2006
#c 17
#! Extracting and integrating object information from the Web is of great significance for Web data management. The existing Web information extraction techniques cannot provide satisfactory solution to the Web object extraction task since objects of the same type are distributed in diverse Web sources, whose structures are highly heterogeneous. In this paper, we propose a novel approach called Object-Level Information Extraction (OLIE) to extract Web objects. This approach extends a classic information extraction algorithm, Conditional Random Fields (CRF), by adding Web-specific information. The experimental results show OLIE can significantly improve the Web object extraction accuracy.

#index 864511
#* On the Controlled Evolution of Process Choreographies
#@ Stefanie Rinderle;Andreas Wombacher;Manfred Reichert
#t 2006
#c 17
#! Process-aware information systems have to be frequently adapted due to business process changes. One important challenge not adequately addressed so far concerns the evolution of process choreographies. If respective modifications are applied in an uncontrolled manner, inconsistencies or errors might occur in the sequel. In particular, modifications of private processes performed by a single party may affect the implementation of the private processes of partners as well. In this paper we sketch a framework that allows process engineers to detect how changes of private processes may affect related public views and - if so - how they can be propagated to the public and private processes of partners. Our approach exploits the semantics of the applied changes in order to automatically determine the adaptations necessary for the partner processes.

#index 864512
#* Automating the Design and Construction of Query Forms
#@ Magesh Jayapandian;H. V. Jagadish
#t 2006
#c 17
#! One of the simplest ways to query a database is through a form, where a user can fill in relevant information and obtain desired results by submitting the form. Designing good static forms is a non-trivial manual task, and the designer needs a sound understanding of both the data organization and the querying needs. Furthermore, form design has two conflicting goals: forms should be simple to understand, and at the same time must provide the broadest possible querying capability to the user. In this paper, we present a framework for generating forms in an automatic and principled way, given the database schema and a sample query workload. We design a tunable clustering algorithm for establishing form structure based on multiple "similar"queries, which includes a mechanism for extending form structure to support other "similar" queries the system may see in the future. The algorithm is adaptive and can incrementally adjust the form structure to reflect the addition or removal of queries in the workload. We have implemented our form generation system on a real database and evaluated it on a comprehensive set of query loads and database schemas. We observe that our system can significantly reduce the numbers of forms needed for various query loads by exploiting similarities across queries, even after placing a strict bound on form complexity.

#index 864513
#* U-Filter: A Lightweight XML View Update Checker
#@ Ling Wang;Elke A. Rundensteiner;Murali Mani
#t 2006
#c 17
#! Both XML-relational systems and native XML systems support creating XML wrapper views and querying against them. However, update operations against such virtual XML views in most cases are not supported yet.

#index 864514
#* XCut: Indexing XML Data for Efficient Twig Evaluation
#@ Simon Sheu;Nigel Wu
#t 2006
#c 17
#! We present MiniCount, the first efficient sound and complete algorithm for finding maximally contained rewritings of conjunctive queries with count, using conjunctive views with count and conjunctive views without aggregation. An efficient and scalable ...

#index 864515
#* Algebraic Optimization of Nested XPath Expressions
#@ Matthias Brantner;Carl-Christian Kanne;Guido Moerkotte;Sven Helmer
#t 2006
#c 17
#! The XPath language incorporates powerful primitives for formulating queries containing nested subexpressions which are existentially or universally quantified. However, even the best published approaches for evaluating XPath have unsatisfactory performance when applied to nested queries. We examine optimization techniques that unnest complex XPath queries. For this purpose, we classify XPath expressions particularly with regard to properties that are relevant for unnesting. We present algebraic equivalences that transform nested expressions into unnested expressions. In our experiments we compare the evaluation times with existing XPath evaluators and the naive evaluation.

#index 864516
#* Incremental Maintenance of Materialized XQuery Views
#@ Maged El-Sayed;Elke A. Rundensteiner;Murali Mani
#t 2006
#c 17
#! Materializing the contents of views has important applications including providing fast access to derived database repositories, optimizing query processing based on cached results, and increasing availability. Maintaining the consistency between materialized views and their base data in the presence of source updates is important to ensure that the materialized views are up-to-date. The straightforward solution for this problem is to recompute the view from scratch over the updated sources.

#index 864517
#* Don't be a Pessimist: Use Snapshot based Concurrency Control for XML
#@ Zeeshan Sardar;Bettina Kemme
#t 2006
#c 17
#! As native XML database systems (e.g., [3, 7, 8]) get increasingly popular, fine-granularity concurrency control becomes imperative in order to allow different clients to concurrently access the same documents. Existing concurrency control approaches for XML are mainly based on locking [2, 3, 4, 6, 5]. However, the experiments of [5] have shown that the locking overhead, especially for read operations, can be tremendous. In this paper, we present two snapshot based concurrency control mechanisms that avoid locking. Instead, transactions access a committed snapshot of the data.

#index 864518
#* Using XML to Build Efficient Transaction-Time Temporal Database Systems on Relational Databases
#@ Fusheng Wang;Xin Zhou;Carlo Zaniolo
#t 2006
#c 17
#! In this paper, we present the ArchIS system that achieves full-functionality transaction-time databases without requiring temporal extensions in XML or database standards. ArchIS' architecture uses (a) XML to support temporally grouped (virtual) representations of the database history, (b) XQuery to express powerful temporal queries on such views, (c) temporal clustering and indexing techniques for managing the actual historical data in a relational database, and (d) SQL/XML for executing the queries on the XML views as equivalent queries on the relational database. The performance studies presented in the paper show that ArchIS is quite effective at storing and retrieving under complex query conditions the transaction-time history of relational databases.

#index 864519
#* Partial Selection Query in Peer-to-Peer Databases
#@ Farnoush Banaei-Kashani;Cyrus Shahabi
#t 2006
#c 17
#! In this paper, we propose DBSampler, a query execution mechanism to answer "partial selection" queries in peerto- peer databases. A partial selection query is an arbitrary selection query that is satisfied with a fraction\inof the results; a universal operation with applications in database tuning, query optimization and approximate query processing in peer-to-peer databases. DBSampler is based on an epidemic dissemination algorithm. We model the epidemic dissemination as a percolation problem and by rigorous percolation analysis tune DBSampler per-query and on-thefly to answer partial queries correctly and efficiently. We verify the efficiency of DBSampler in terms of query cost and query time via extensive simulation.

#index 864520
#* DREAM: A Data Replication Technique for Real-Time Mobile Ad-hoc Network Databases
#@ Prasanna Pabmanabhan;Le Gruenwald
#t 2006
#c 17
#! In a Mobile Ad-hoc Network (MANET), due to the mobility and energy limitations of nodes, disconnection and network partitioning occur frequently. In addition, transactions in many MANET database applications have time constraints. In this paper, a Data REplication technique for real-time Ad-hoc Mobile databases (DREAM) that addresses all these issues is proposed. DREAM is prototyped on laptops and PDAs and compared with two existing replication techniques using a military database application.

#index 864521
#* HiWaRPP ― Hierarchical Wavelet-based Retrieval on Peer-to-Peer Network
#@ Mihai Lupu;Bei Yu
#t 2006
#c 17
#! This paper introduces the use of wavelets for information retrieval in a peer-to-peer environment. In order to achieve our purposes, we use a new combination between broadcasting and a hierarchical overlay. Compared to previous approaches, we do not store complete information about the children of a super-peer, nor do we broadcast the queries blindly. We approximate the feature vectors using the multiresolution analysis and the discrete wavelet transform. Each peer is represented by a high-dimensional feature vector and the height of the hierarchy is logarithmic in the dimensionality of this feature vector. Leaf nodes represent real peers, while internal nodes are virtual peers used for routing. Our retrieval method has been tested with both real and synthetic data and shown to be efficient in retrieving relevant information, resulting in good precision and recall on four standard test collections.

#index 864522
#* An Air Index for Data Access over Multiple Wireless Broadcast Channels
#@ Damdinsuren Amarmend;Masayoshi Aritsugi;Yoshinari Kanamori
#t 2006
#c 17
#! In this paper, we propose an index allocation method for data access over multiple wireless channels. Our method first derives external index information from the scheduled data, and then allocates it over upper channels. Moreover, local exponential indexes with different parameters are built within each channel for local data search. Experiments are performed to compare the effectiveness of our approach with an existing approach. The results show that our method outperforms the existing method.

#index 864523
#* Searching Local Information in Mobile Databases
#@ Ouri Wolfson;Bo Xu;Huabei Yin;Hu Cao
#t 2006
#c 17
#! A mobile ad-hoc network (MANET) is a set of moving objects that communicate with each other via unregulated, short-range wireless technologies such as IEEE 802.11, Bluetooth, or Ultra Wide Band (UWB). No fixed infrastructure is assumed or relied upon. An important application domain of MANET's is local resource discovery. In a local resource discovery application, a user finds local resources that satisfy specified criteria. For example, a driver finds an available parking slot in a region by receiving information generated by the parking meter, or gets the traffic conditions on a highway segment a mile ahead; a cab driver finds a near-by customer, or a participant at a convention finds another participant with a matching profile.

#index 864524
#* An Approach to Adaptive Memory Management in Data Stream Systems
#@ Michael Cammert;Jurgen Kramer;Bernhard Seeger;Sonny Vaupel
#t 2006
#c 17
#! Adaptivity is a challenging open issue in data stream management. In this paper, we tackle the problem of memory adaptivity inside a system executing temporal sliding window queries over continuous data streams. Two different techniques to control the memory usage at runtime are proposed which refer to changes in window sizes and time granularities. Both techniques differ from standard load shedding approaches based on sampling as they ensure precise query answers for user-defined Quality of Service (QoS) specifications, even under query re-optimization.

#index 864525
#* Cluster Hull: A Technique for Summarizing Spatial Data Streams
#@ John Hershberger;Nisheeth Shrivastava;Subhash Suri
#t 2006
#c 17
#! Recently there has been a growing interest in detecting patterns and analyzing trends in data that are generated continuously, often delivered in some fixed order and at a rapid rate, in the form of a data stream [5, 6]. When the stream consists of spatial data, its geometric "shape" can convey important qualitative aspects of the data set more effectively than many numerical statistics. In a stream setting, where the data must be constantly discarded and compressed, special care must be taken to ensure that the compressed summary faithfully captures the overall shape of the point distribution. We propose a novel scheme, ClusterHulls, to represent the shape of a stream of two-dimensional points. Our scheme is particularly useful when the input contains clusters with widely varying shapes and sizes, and the boundary shape, orientation, or volume of those clusters may be important in the analysis.

#index 864526
#* SlidingWindow based Multi-Join Algorithms over Distributed Data Streams
#@ Dongdong Zhang;Jianzhong Li;Kimutai Kimeli;Weiping Wang
#t 2006
#c 17
#! This paper focuses on multi-way sliding window join (SWJoin) processing over distributed data streams. A novel Join algorithm is proposed based on two distributed data stream transfer models. To reduce the communication cost and lighten the workload on the central processor node, the algorithm filters out tuples that can't contribute to multiway SWJoin results by transforming the join conditions of SWJoin into filtering conditions during data stream transfer. Furthermore, the algorithm guarantees that all necessary data for generating exact multi-way SWJoin results can be transmitted to the central processor node.

#index 864527
#* A Pipelined Framework for Online Cleaning of Sensor Data Streams
#@ Shawn R. Jeffery;Gustavo Alonso;Michael J. Franklin;Wei Hong;Jennifer Widom
#t 2006
#c 17
#! Data captured from the physical world through sensor devices tends to be noisy and unreliable. The data cleaning process for such data is not easily handled by standard data warehouse-oriented techniques, which do not take into account the strong temporal and spatial components of receptor data. We present Extensible receptor Stream Processing (ESP), a declarative query-based framework designed to clean the data streams produced by sensor devices.

#index 864528
#* Revision Processing in a Stream Processing Engine: A High-Level Design
#@ Esther Ryvkina;Anurag S. Maskey;Mitch Cherniack;Stan Zdonik
#t 2006
#c 17
#! Data stream processing systems have become ubiquitous in academic [1, 2, 5, 6] and commercial [11] sectors, with application areas that include financial services, network traffic analysis, battlefield monitoring and traffic control [3]. The append-only model of streams implies that input data is immutable and therefore always correct. But in practice, streaming data sources often contend with noise (e.g., embedded sensors) or data entry errors (e.g., financial data feeds) resulting in erroneous inputs and therefore, erroneous query results. Many data stream sources (e.g., commercial ticker feeds) issue "revision tuples" (revisions) that amend previously issued tuples (e.g. erroneous share prices). Ideally, any stream processing engine should process revision inputs by generating revision outputs that correct previous query results. We know of no stream processing system that presently has this capability.

#index 864529
#* Approximating StreamingWindow Joins Under CPU Limitations
#@ Ahmed Ayad;Jeffrey Naughton;Stephen Wright;Utkarsh Srivastava
#t 2006
#c 17
#! Data streaming systems face the possibility of having to shed load in the case of CPU or memory resource limitations. We study the CPU limited scenario in detail. First, we propose a new model for the CPU cost. Then we formally state the problem of shedding load for the goal of obtaining the maximum possible subset of the complete answer, and propose an online strategy for semantic load shedding. Moving on to random load shedding, we discuss random load shedding strategies that decouple the window maintenance and tuple production operations of the symmetric hash join, and prove that one of them — Probe-No-Insert — always dominates the previously proposed coin flipping strategy.

#index 864530
#* Monitoring Top-k Query inWireless Sensor Networks
#@ Minji Wu;Jianliang Xu;Xueyan Tang;Wang-Chien Lee
#t 2006
#c 17
#! Top-k monitoring is important to many wireless sensor applications. This paper exploits the semantics of top-k query and proposes a novel energy-efficient monitoring approach, called FILA. The basic idea is to install a filter at each sensor node to suppress unnecessary sensor updates. The correctness of the top-k result is ensured if all sensor nodes perform updates according to their filters. We show via simulation that FILA outperforms the existing TAGbased approach by an order of magnitude.

#index 864531
#* LB-Index: A Multi-Resolution Index Structure for Images
#@ Vebjorn Ljosa;Arnab Bhattacharya;Ambuj K. Singh
#t 2006
#c 17
#! In many domains, the similarity between two images depends on the spatial locations of their features. The earth mover's distance (EMD), first proposed by Werman et al. [8], measures such similarity. It yields higher-quality image retrieval results than the Lp-norm, quadratic-form distance, and Jeffrey divergence [6], and has also been used for similarity search on contours [3], melodies [7], and graphs [2].

#index 864532
#* Energy-Efficient Continuous Isoline Queries in Sensor Networks
#@ Adam Silberstein;Rebecca Braynard;Jun Yang
#t 2006
#c 17
#! Environmental monitoring is a promising application for sensor networks. Many scenarios produce geographically correlated readings, making them visually interesting and good targets for the isoline query. This query depicts boundaries showing how values change in the network. Temporal and spatial suppression provide opportunities for reducing the cost of maintaining the query result. We combine both techniques for maximal benefit by monitoring node and edge constraints. A monitored node triggers a report if its value changes. A monitored edge triggers a report if the difference between its nodes' values changes. The root collects reports and derives all node values, from which the query result is generated. We fully exploit this strategy in our algorithm, CONCH, which maintains the set of node and edge constraints that minimizes message traffic.

#index 864533
#* Better Burst Detection
#@ Xin Zhang;Dennis Shasha
#t 2006
#c 17
#! A burst is a large number of events occurring within a certain time window. Many data stream applications require the detection of bursts across a variety of window sizes. For example, stock traders may be interested in bursts having to do with institutional purchases or sales that are spread out over minutes or hours. In this paper, we present a new algorithmic framework for elastic burst detection [1]: a family of data structures that generalizes the Shifted Binary Tree, and a heuristic search algorithm to find an efficient structure given the input. We study how different inputs affect the desired structures and the probability to trigger a detailed search. Experiments on both synthetic and real world data show a factor of up to 35 times improvement compared with the Shifted Binary Tree over a wide variety of inputs, depending on the inputs.

#index 864534
#* Operators for Expensive Functions in Continuous Queries
#@ Matthew Denny;Michael J. Franklin
#t 2006
#c 17
#! Many analysis and monitoring applications require the repeated execution of expensive functions over streams of rapidly changing data. These applications appear in fields as varied as finance, supply chain management, and power utility monitoring.While many of these applications can be expressed declaratively, current continuous query processors are not designed to optimize queries with expensive user-defined functions. Such optimizations are hindered by "black box" function interfaces, where the operator has no control over the processing inside each invocation. We are currently developing VAOs (Variable Accuracy Operators), a new class of operators that allow the query processor to speed up individual function calls. VAOs use a new function interface that exposes the trade-off between work and accuracy inherent inmany functions. Using this new interface, VAOs can eliminate unneeded work by running each function call to only the accuracy needed by the query. VAOs play a key role in our larger research agenda of optimizing queries with expensive functions, and we briefly describe this larger agenda as well.

#index 864535
#* Faster In-Network Evaluation of Spatial Aggregationin Sensor Networks
#@ Dina Goldin
#t 2006
#c 17
#! Spatial aggregation is an important class of queries for geoaware spatial sensor database applications. Given a set of spatial regions, it involves the aggregation of dynamic sensor readings over each of these regions simultaneously. Nested spatial aggregation involves one more level of aggregation, combining these aggregates into a single aggregate value. We show that spatial aggregate values can often be computed in-network, rather than waiting until the partial aggregate records reach the root as is now the case. This decreases the amount of communication involved in query evaluation, thereby reducing the network's power consumption. We describe an algorithm that allows us to determine when an aggregate record for any spatial region is ready to be evaluated in-network, based on decorating the routing tree with region leader lists. We also identify several important scenarios, such as nested spatial aggregation and filtering predicates, when the savings from our approach are expected to be particularly great.

#index 864536
#* Threshold Similarity Queries in Large Time Series Databases
#@ Johannes Assfalg;Hans-Peter Kriegel;Peer Kroger;Peter Kunath;Alexey Pryakhin;Matthias Renz
#t 2006
#c 17
#! Similarity search in time series data is an active area of research. In this paper, we introduce the novel concept of threshold-similarity queries in time series databases which report those time series exceeding a user-defined query threshold at similar time frames compared to the query time series. In addition, we present a new data structure to support threshold similarity queries efficiently. The performance of our solution is demonstrated by an extensive experimental evaluation.

#index 864537
#* Automated Storage Management with QoS Guarantees
#@ Lin Qiao;Balakrishna R. Iyer;Divyakant Agrawal;Amr El Abbadi
#t 2006
#c 17
#! Automated storage management is critical for most dataintensive applications running on DBMSs. In large-scale storage subsystems, the workload is expected to vary with time. In order to ensure both QoS and efficient usage of storage resources, variation in the actual physical disks is allowed to support a single virtual disk. Such data migration generates extra IOs and consumes storage resources. Not only does data migration need to be scheduled ahead but it must also be scheduled in such a way that QoS violations do not occur because of the extra migration IOs. In this paper, we present a novel analytic framework, PULSTORE, for autonomically managing the storage to provide performance guarantee during migration.

#index 864538
#* Materialized Sample Views for Database Approximation
#@ Shantanu Joshi;Christopher Jermaine
#t 2006
#c 17
#! We consider the problem of creating a sample view of a database table. A sample view is an indexed, materialized view that permits efficient sampling from an arbitrary range query over the view. Our core technical contribution is a new file organization called the ACE Tree that is suitable for organizing and indexing a sample view.

#index 864539
#* Every Click You Make, IWill Be Fetching It: Efficient XML Query Processing in RDMS Using GUI-driven Prefetching
#@ Sourav S. Bhowmick;Sandeep Prakash
#t 2006
#c 17
#! formulation and efficient processing of the formulated query. However, due to the nature of XML data, formulating an XML query using an XML query language such as XQuery requires considerable effort. A user must be completely familiar with the syntax of the query language, and must be able to express his/her needs accurately in a syntactically correct form. In many real life applications it is not realistic to assume that users are proficient in expressing such textual queries. Hence, there is a need for a user-friendly visual querying schemes to replace data retrieval aspects of XQuery. In this paper, we address the problem of efficient processing of XQueries in the relational environment where the queries are formulated using a user-friendly GUI. We take a novel and non-traditional approach to improving query performance by prefetching data during the formulation of a query in a single-user environment. The latency offered by the GUI-based query formulation is utilized to prefetch portions of the query results. The basic idea we employ for prefetching is that we prefetch constituent path expressions, store the intermediary results, reuse them when connective is added or "Run" is pressed.

#index 864540
#* Achieving Class-Based QoS for Transactional Workloads
#@ Bianca Schroeder;Mor Harchol-Balter;Arun Iyengar;Erich Nahum
#t 2006
#c 17
#! Transaction processing systems lie at the core of modern e-commerce applications such as on-line retail stores, banks and airline reservation systems. The economic success of these applications depends on the ability to achieve high user satisfaction, since a single mouse-click is all that it takes a frustrated user to switch to a competitor. Given that system resources are limited and demands are varying, it is difficult to provide optimal performance to all users at all times. However, often transactions can be divided into different classes based on how important they are to the online retailer. For example, transactions initiated by a "big spending" client are more important than transactions from a client that only browses the site. A natural goal then is to ensure short delays for the class of important transactions, while for the less important transactions longer delays are acceptable.

#index 864541
#* Inferring a Serialization Order for Distributed Transactions
#@ Khuzaima Daudjee;Kenneth Salem
#t 2006
#c 17
#! Data partitioning is often used to scale-up a database system. In a centralized database system, the serialization order of commited update transactions can be inferred from the database log. To achieve this in a shared-nothing distributed database, the serialization order of update transactions must be inferred from multiple database logs. We describe a technique to generate a single stream of updates from logs of multiple database systems. This single stream represents a valid serialization order of update transactions at the sites over which the database is partitioned.

#index 864542
#* Fractal Modeling of IP Network Traffic at Streaming Speeds
#@ Flip Korn;S. Muthukrishnan;Yihua Wu
#t 2006
#c 17
#! This paper describes how to fit fractal models, online, on IP traffic data streams. Our approach relies on maintaining a sketch of the data stream and fitting straight lines: it yields algorithms that are fast, space-efficient, and accurate. We implemented our methods in AT&T's Gigascope data stream management system, to demonstrate their practicality at streaming line speeds.

#index 864543
#* Message from Demo Chairs
#@ Govi Govindarajan;Leo Mark;Arjen P. de Vries
#t 2006
#c 17

#index 864544
#* UNIDOOR: a Deductive Object-Oriented Database Management System
#@ Mohammed K. Jaber;Andrei Voronkov
#t 2006
#c 17
#! In this paper, we present UNIDOOR, a deductive objectoriented database system (DOOD). We demonstrate the distinctive features of UNIDOOR data model and its query language. We then show how essential object-oriented and database management features, that were missing from other DOOD implementations, are successfully supported in UNIDOOR. These features include a scalable persistent store with crash recovery, database integrity and transaction control facilities in a multi-user environment.

#index 864545
#* DaWaII: a Tool for the Integration of Autonomous Data Marts
#@ Luca Cabibbo;Ivan Panella;Riccardo Torlone
#t 2006
#c 17
#! DaWaII (Data Warehouse IntegratIon) is a tool supporting the various activities related to the integration of multidimensional data. This problem arises in common scenarios where there is the need to combine independently developed data warehouses. Actually, a today common practice for building a data warehouse is to develop a collection of integrated data marts, each of which provide a dimensional view of a single business process. These data marts should be based on shared dimensions but very often, even within the same company, designers develop their data marts independently and it turns out that their integration is a difficult task. Indeed, the problem arises in other common cases. For instance, when different companies get involved in a federated project or when there is the need to combine a proprietary data warehouse with external information, for instance, with multidimensional data wrapped from the Web.

#index 864546
#* MAPLE: A Mobile Scalable P2P Nearest Neighbor Query System for Location-based Services
#@ Wei-Shinn Ku;Roger Zimmermann;Chi-Ngai Wan;Haojun Wang
#t 2006
#c 17
#! In this demonstration we present MAPLE, a scalable peer-to-peer nearest neighbor (NN) query system for mobile environments. MAPLE is designed for the efficient sharing of query results cached in the local storage of mobile peers. The MAPLE system is innovative in its ability to either fully or partially compute location-dependent nearest neighbor objects on each host. The demonstration illustrates how cooperative data sharing and distributed processing among mobile peers results in a considerable reduction of the load on remote spatial databases.

#index 864547
#* SIPPER: Selecting Informative Peers in Structured P2P Environment for Content-Based Retrieval
#@ Shuigeng Zhou;Zheng Zhang;Weining Qian;Aoying Zhou
#t 2006
#c 17
#! In this demonstration, we present a prototype system called SIPPER, which is the abbreviation for Selecting Informative Peers in Structured P2P Environment for Content-based Retrieval. SIPPER distinguishes itself from the existing P2P-IR systems by the following two features: First, to improve retrieval efficiency, SIPPER employs a novel peer selection method to direct the query to a small fraction of relevant peers in the network for searching globally relevant documents. Second, to reduce the bandwidth cost of meta data publishing, SIPPER uses a new publishing mechanism, the term-node publishing mechanism, which is different from the traditional term-document model [2].

#index 864548
#* Simultaneous Pipelining in QPipe: Exploiting Work Sharing Opportunities Across Queries
#@ Kun Gao;Stavros Harizopoulos;Ippokratis Pandis;Vladislav Shkapenyuk;Anastassia Ailamaki
#t 2006
#c 17
#! Data warehousing and scientific database applications operate on massive datasets and are characterized by complex queries accessing large portions of the database. Concurrent queries often exhibit high data and computation overlap, e.g., they access the same relations on disk, compute similar aggregates, or share intermediate results. Unfortunately, run-time sharing in modern database engines is limited by the paradigm of invoking an independent set of operator instances per query, potentially missing sharing opportunities if the buffer pool evicts data early.

#index 864549
#* Enabling Query Processing on Spatial Networks
#@ Jagan Sankaranarayanan;Houman Alborzi;Hanan Samet
#t 2006
#c 17
#! _cf_loadingtexthtml="";_cf_contextpath="";_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";_cf_jsonprefix='//';_cf_clientid='354EA52E1DE460FCC989BC2CD2C6FF36';Enabling Query Processing on Spatial Networks function settab() { var mytabs = ColdFusion.Layout.getTabLayout('citationdetails'); mytabs.on('tabchange', function(tabpanel,activetab) { document.cookie = 'picked=' + '1130022' + ',' + activetab.id; }) }function letemknow(){ ColdFusion.Window.show('letemknow');}function testthis(){alert('test');}function loadalert(){ alert("I am in the load alert"); }function loadalert2(){ alert("I am in the load alert2"); } google.load('visualization', '1', {packages:['orgchart']}); google.setOnLoadCallback(drawChart); function drawChart() { var data = new google.visualization.DataTable(); data.addColumn('string', 'Name'); data.addColumn('string', 'Manager'); data.addColumn('string', 'ToolTip'); data.addRows([ [{v:'0', f:'CCS for this Article

#index 864550
#* cgmOLAP: Efficient Parallel Generation and Querying of Terabyte Size ROLAP Data Cubes
#@ Y. Chen;A. Rau-Chaplin;F. Dehne;T. Eavis;D. Green;E. Sithirasenan
#t 2006
#c 17
#! We present the cgmOLAP server, the first fully functional parallel OLAP system able to build data cubes at a rate of more than 1 Terabyte per hour. cgmOLAP incorporates a variety of novel approaches for the parallel computation of full cubes, partial cubes, and iceberg cubes as well as new parallel cube indexing schemes. The cgmOLAP system consists of an application interface, a parallel query engine, a parallel cube materialization engine, meta data and cost model repositories, and shared server components that provide uniform management of I/O, memory, communications, and disk resources.

#index 864551
#* Practical Adaptation to Changing Resources in Grid Query Processing
#@ Anastasios Gounaris;Norman W. Paton;Rizos Sakellariou;Alvaro A. A. Fernandes;Jim Smith;Paul Watson
#t 2006
#c 17
#! Grid computational resources, as well as being heterogeneous, may also exhibit unpredictable, volatile behaviour. Therefore, query processing on the Grid needs to be adaptive in order to cope with evolving resource characteristics, such as machine load and availability. To address this challenge in a Grid environment, the non-adaptive OGSA-DQP1 system described in [1] has been enhanced with adaptive capabilities.

#index 864552
#* ViEWNet: Visual Exploration of Region-Wide Traffic Networks
#@ Hans-Peter Kriegel;Peter Kunath;Martin Pfeifle;Matthias Renz
#t 2006
#c 17
#! Location-based services and data mining algorithms analyzing objects moving on a complex traffic network are becoming increasingly important. In this paper, we introduce a new approach which effectively and efficiently detects dense areas in spatial networks. In an offline phase, we generate a hierarchical partitioning of the traffic network. Thereby, static entities like roads and buildings are likely to be in the same partitioning if they are close to each other according to their network distance. In the online phase, our prototype ViEWNet allows the effective and efficient monitoring of objects moving on a spatial network. Based on a clear visualization of the traffic intensity in each network cell, the user can easily detect hierarchies of dense areas by our powerful prototype ViEWNet.

#index 864553
#* AITVS: Advanced Interactive Traffic Visualization System
#@ Chang-Tien Lu;Arnold P. Boedihardjo;Jinping Zheng
#t 2006
#c 17
#! Transportation and the highway network form the backbone of the total public infrastructure system. As such, planning and monitoring for an effective transportation system is crucial in the building and maintenance of a region's economy and safety. However, demand for road travel continues to expand as population increases (particularly in the metropolitan areas) while new constructions have not kept pace. According to the Federal Highway Administration, it is forecasted that the volume of freight movement alone is to nearly double by 2020 [1]. Congestion and looming gridlock crises seriously threaten any region's mobility, safety and economic vitality. A crucial component in addressing these concerns is the development of specific technologies to monitor, model, and optimize traffic flow.

#index 864554
#* Stream Processing in Production-to-Business Software
#@ Michael Cammert;Christoph Heinz;Jurgen Kramer;Tobias Riemenschneider;Maxim Schwarzkopf;Bernhard Seeger;Alexander Zeiss
#t 2006
#c 17
#! In order to support continuous queries over data streams, a plethora of suitable techniques as well as prototypes have been developed and evaluated in recent years. In particular, it is of utmost importance to confirm their necessity and feasibility in real-world applications. For that reason, we have successfully coupled our infrastructure for data stream processing (PIPES) with an industrial Production-to-Business software (i-Plant) dedicated to highly automated manufacturing processes.

#index 864555
#* HSI: A Novel Framework for Efficient Automated Singer Identification in Large Music Database
#@ Jialie Shen;John Shepherd;Bin Cui;Kian-Lee Tan
#t 2006
#c 17
#! The singer's information is essential in organising, browsing and exploring music data. As an important component of music database systems, the automated artist identification is gaining considerable momentum due to numerous potential applications including music indexing and retrieval, copy right management and music recommendation systems. Unfortunately, the most currently employed approaches are still in their infancy and the performance is by far less satisfactory. Indeed, they suffer from low effectiveness, less robustness and poor scalability to accommodate large scale of data. In this demo, we presents a novel system, called Hybrid Singer Identifier (HSI), for efficient and effective automated singer identification in large music databases.

#index 864556
#* XPlainer: An XPath Debugging Framework
#@ Mariano P. Consens;John W. S. Liu;Bill O'Farrell
#t 2006
#c 17
#! We present MiniCount, the first efficient sound and complete algorithm for finding maximally contained rewritings of conjunctive queries with count, using conjunctive views with count and conjunctive views without aggregation. An efficient and scalable ...

#index 864557
#* ACXESS - Access Control for XML with Enhanced Security Specifications
#@ Sriram Mohan;Jonathan Klinginsmith;Arijit Sengupta;Yuqing Wu
#t 2006
#c 17
#! We present ACXESS (Access Control for XML with Enhanced Security Specifications), a system for specifying and enforcing enhanced security constraints on XML via virtual "security views" and query rewrites. ACXESS is the first system that bears the capability to specify and enforce complicated security policies on both subtrees and structural relationships.

#index 864558
#* PETRANET: a Power Efficient Transaction Management Technique for Real-Time Mobile Ad-hoc Network Databases
#@ Le Gruenwald;Percy Bernedo;Prasanna Padmanabhan
#t 2006
#c 17
#! A Mobile Ad-Hoc Network (MANET) is a collection of wireless autonomous mobile nodes with no fixed infrastructure. Since no fixed infrastructure is required, MANET fits well in military operations, emergency disaster rescue, and mobile ad-hoc voting. There are many issues that have to be addressed while designing a technique for managing real-time database transactions in MANET: 1) energy limitations; 2) client and server mobility; 3) real-time constraints imposed on transactions; and 4) frequent disconnection and network partitioning. We have designed PETRANET1: a Power-Efficient Transaction management technique for Real-time mobile Ad-hoc NETwork databases that addresses the above specified issues. In this paper, we present a system prototype that we have developed to implement PETRANET for a military database application.

#index 864559
#* ConQueSt: a Constraint-based Querying System for Exploratory Pattern Discovery
#@ Francesco Bonchi;Fosca Giannotti;Claudio Lucchese;Salvatore Orlando;Raffaele Perego;Roberto Trasarti
#t 2006
#c 17
#! ConQueSt is a constraint-based querying system devised with the aim of supporting the intrinsically exploratory nature of pattern discovery. It provides users with an expressive constraint-based query language which allows the discovery process to be effectively driven toward potentially interesting patterns. Constraints are also exploited to reduce the cost of pattern mining. The system is built around an efficient constraint-based mining engine which entails several data and search space reduction techniques, and allows new user-defined constraints to be easily added.

#index 1045340
#* On the algebraicde rham complex
#@ Madhav Nori;Ben Lee
#t 2007
#c 17
#! In this thesis we reinterpret algebraic de Rham cohomology for a possibly singular complex variety X as sheaf cohomology in the site of smooth schemes over X with Voevodsky's h-topology. Our results extend to the algebraic de Rham complex as well, reinterpreting results of Du Bois. Our main technique is a generalization of a result of Verdier on &Ccaron;ech cohomology of hypercovers to arbitrary local acyclic fibrations of simplicial presheaves.

#index 1206373
#* Proceedings of the 2008 IEEE 24th International Conference on Data Engineering
#@ 
#t 2008
#c 17

#index 1206374
#* Proceedings of the 2009 IEEE International Conference on Data Engineering
#@ 
#t 2009
#c 17

#index 1206570
#* Simultaneous Equation Systems for Query Processing on Continuous-Time Data Streams
#@ Yanif Ahmad;Olga Papaemmanouil;Ugur Cetintemel;Jennie Rogers
#t 2008
#c 17
#! We introduce Pulse, a framework for processing continuous queries over models of continuous-time data, which can compactly and accurately represent many real-world activities and processes. Pulse implements several query operators, including filters, aggregates and joins, that work by solving simultaneous equation systems, which in many cases is significantly cheaper than processing a stream of tuples. As such, Pulse translates regular queries to work on continuous-time inputs, to reduce computational overhead and latency while meeting user-specified error bounds on query results. For error bound checking, Pulse uses an approximate query inversion technique that ensures the solver executes infrequently and only in the presence of errors, or no previously known results. We first discuss the high-level design of Pulse, which we fully implemented in a stream processing system. We then characterise Pulse's behavior through experiments with real data, including financial data from the New York Stock Exchange, and spatial data from the Automatic Identification System for tracking naval vessels. Our results verify that Pulse is practical and demonstrates significant performance gains for a variety of workload and query types.

#index 1206571
#* Runtime Semantic Query Optimization for Event Stream Processing
#@ Luping Ding;Songting Chen;Elke A. Rundensteiner;Junichi Tatemura;Wang-Pin Hsiung;K. Selcuk Candan
#t 2008
#c 17
#! Detecting complex patterns in event streams, i.e., complex event processing (CEP), has become increasingly important for modern enterprises to react quickly to critical situations. In many practical cases business events are generated based on pre-defined business logics. Hence constraints, such as occurrence and order constraints, often hold among events. Reasoning using these known constraints enables us to predict the non-occurrences of certain future events, thereby helping us to identify and then terminate the long running query processes that are guaranteed to not lead to successful matches. In this work, we focus on exploiting event constraints to optimize CEP over large volumes of business transaction streams. Since the optimization opportunities arise at runtime, we develop a runtime query unsatisfiability (RunSAT) checking technique that detects optimal points for terminating query evaluation. To assure efficiency of RunSAT checking, we propose mechanisms to precompute the query failure conditions to be checked at runtime. This guarantees a constant-time RunSAT reasoning cost, making our technique highly scalable. We realize our optimal query termination strategies by augmenting the query with Event-Condition-Action rules encoding the pre-computed failure conditions. This results in an event processing solution compatible with state-of-the-art CEP architectures. Extensive experimental results demonstrate that significant performance gains are achieved, while the optimization overhead is small.

#index 1206572
#* Efficient Online Subsequence Searching in Data Streams under Dynamic Time Warping Distance
#@ Mi Zhou;Man Hon Wong
#t 2008
#c 17
#! Data streams of real numbers are generated naturally in many applications. The technology of online subsequence searching in data streams becomes more and more important for monitoring and mining stream data. Due to its capability of handling temporal distortions in sequences, Dynamic Time Warping (DTW) distance is a widely used similarity measure for time-series pattern matching. Unfortunately, because of the high computational complexity of DTW, no one has proposed efficient methods for online subsequence searching under DTW distance, especially over high speed data streams. In this paper, we observe that some important properties of DTW can be used to eliminate a lot of redundant computations. Based on these properties, an efficient batch filtering method for online subsequence searching in data streams is proposed. The experimental results show that when no global path constraint is used, the proposed method outperforms the best known method up to 25 times in terms of throughput. When global path constraint is considered, the proposed method can still outperform the rival method under most of the settings of the global path constraint, although our method does not exploit any information about the constraint.

#index 1206573
#* Efficient Data Authentication in an Environment of Untrusted Third-Party Distributors
#@ Mikhail J. Atallah;YounSun Cho;Ashish Kundu
#t 2008
#c 17
#! In the third-party model for the distribution of data, the trusted data creator or owner provides an untrusted party D with data and integrity verification (IV) items for that data. When a user U gets a subset of the data at D or is already in possession of that subset, U may request from D the IV items that make it possible for U to verify the integrity of its data: D must then provide u with the (hopefully small) number of needed IVs. Most of the published work in this area uses the Merkle tree or variants thereof. For the problem of 2-dimensional range data, the best published solutions require D to store O(n log n) IV items for a database of n items, and allow a user U to be sent only O(log n) of those IVs for the purpose of verifying the integrity of the data it receives from D (regardless of the size of U's query rectangle). For data that is modeled as a 2-dimensional grid (such as GIS or image data), this paper shows that better bounds are possible: The number of IVs stored at D (and the time it takes to compute them) can be brought down to O(n), and the number of IVs sent to U for verification can be brought down to a constant.

#index 1206574
#* OptRR: Optimizing Randomized Response Schemes for Privacy-Preserving Data Mining
#@ Zhengli Huang;Wenliang Du
#t 2008
#c 17
#! The randomized response (RR) technique is a promising technique to disguise private categorical data in Privacy-Preserving Data Mining (PPDM). Although a number of RR-based methods have been proposed for various data mining computations, no study has systematically compared them to find optimal RR schemes. The difficulty of comparison lies in the fact that to compare two PPDM schemes, one needs to consider two conflicting metrics: privacy and utility. An optimal scheme based on one metric is usually the worst based on the other metric. In this paper, we first describe a method to quantify privacy and utility. We formulate the quantification as estimate problems, and use estimate theories to derive quantification. We then use an evolutionary multi-objective optimization method to find optimal disguise matrices for the randomized response technique. The experimental results have shown that our scheme has a much better performance than the existing RR schemes.

#index 1206575
#* Maintaining Connectivity in Dynamic Multimodal Network Models
#@ Petko Bakalov;Erik Hoel;Wee-Liang Heng;Vassilis J. Tsotras
#t 2008
#c 17
#! Network data models are frequently used as a mechanism to describe the connectivity between spatial features in many existing and emerging GIS applications (location-based services, transportation design, navigational systems, etc.). Connectivity information is required for solving a wide range of location-based queries like finding the shortest path, service areas discovery, allocation, and distance matrix computation. Nevertheless, real-life networks are dynamic in nature since spatial features can be periodically modified. Such updates may change the connectivity relations with the other features and connectivity must be reestablished. Existing approaches are not suitable for a dynamic environment, since whenever a feature change occurs, the whole network connectivity has to be reconstructed from scratch. In this paper, we propose an efficient algorithm that incrementally maintains connectivity within a dynamic network. Our solution is based on the existing functionality (tables, joins, sorting algorithms) provided by a standard relational DBMS and has been implemented and tested and will be shipped with an upcoming release of the ESRI ArcGIS product.

#index 1206576
#* An Enhanced Extract-Transform-Load System for Migrating Data in Telecom Billing
#@ Himanshu Agrawal;Girish Chafle;Sunil Goyal;Sumit Mittal;Sougata Mukherjea
#t 2008
#c 17
#! Data migration has become a priority in many industries, spawned by a variety of business needs. Most of the existing tools for Extract, Transform and Load (ETL) process of data migration are piece-meal and do not present a complete solution. Moreover, while research has focused on the problem of Schema Mapping, a key step in the ETL process, most of the current algorithms do not perform well on real-world data. Researchers have suggested the use of Domain Knowledge to enhance schema mapping. In this paper, we use domain knowledge in an innovative manner to improve schema mapping in an `actual' industrial setting. Further, we take a comprehensive view of the data migration problem and present an end-to-end system for the ETL process, utilizing existing tools for each step and building connectors, wherever required. We focus on Data Migration for Telecom Billing and utilize domain knowledge captured in an ontology, a thesaurus and a set of rules to improve schema mapping. Experiments conducted on a real-life data demonstrate the effectiveness of our system and validate the utility of domain knowledge in data migration projects.

#index 1206577
#* A Scalable Scheme for Bulk Loading Large RDF Graphs into Oracle
#@ Souripriya Das;Eugene Inseok Chong;Zhe Wu;Melliyal Annamalai;Jagannathan Srinivasan
#t 2008
#c 17
#! The growth of RDF data makes it imperative that an efficient mechanism for bulk-loading RDF graphs be supported. Thus, the paper proposes a bulk-load scheme that allows fast loading of arbitrarily large RDF graphs into a database. Specifically, three modes of load are supported: i) loading into an empty RDF graph, ii) appending to a non-empty RDF graph, and iii) concurrent loads into multiple graphs. The bulk-load scheme is implemented as part of Oracle Database Semantic Technologies and the performance experiments conducted with a variety of RDF graphs (from UniProt and synthesized data of Lehigh University Benchmark) demonstrate the scalability of the approach. The paper outlines the challenges involved in bulk-loading of large RDF graphs, describes the bulk-load scheme, discusses its implementation, and presents a performance study.

#index 1206578
#* Orchid: Integrating Schema Mapping and ETL
#@ Stefan Dessloch;Mauricio A. Hernandez;Ryan Wisnesky;Ahmed Radwan;Jindan Zhou
#t 2008
#c 17
#! This paper describes Orchid, a system that converts declarative mapping specifications into data flow specifications (ETL jobs) and vice versa. Orchid provides an abstract operator model that serves as a common model for both transformation paradigms; both mappings and ETL jobs are transformed into instances of this common model. As an additional benefit, instances of this common model can be optimized and deployed into multiple target environments. Orchid is being deployed in FastTrack, a data transformation toolkit in IBM Information Server.

#index 1206579
#* Merging Hierarchies Using Object Placement
#@ Kai Zhao;Robert Ikeda;Hector Garcia-Molina
#t 2008
#c 17
#! Objects are often organized in a hierarchy to help in managing or browsing them. For example, products in a store can be divided by type (electronics, clothes, books, ...) and then by brand (Sony, Epson, Dockers, ...). Web pages at a site can also be placed in a hierarchy. For instance, a French tourist site may have categories cities, history, hotels, tours; within the cities category we have pages divided by city, and then by events, maps, restaurants. In this paper we study the problem of hierarchy integration, in particular, how to combine two related hierarchies into one, more comprehensive one. The need to integrate arises in many situations where the objects come from different systems. In our product hierarchy example above, we may want to provide a comparison shopping service that offers products from two stores; in our tourism example, we may want to build a meta-web-site that combines the resources of two or more French tourism sites. To simplify the problem, we study how to merge one hierarchy into a second known base hierarchy, by copying references to objects into the base hierarchy, and perhaps by adding some categories into the base hierarchy.

#index 1206580
#* Model Management Engine for Data Integration with Reverse-Engineering Support
#@ Michael N. Gubanov;Philip A. Bernstein;Alexander Moshchuk
#t 2008
#c 17
#! Model management is a high-level programming language designed to efficiently manipulate schemas and mappings. It is comprised of robust operators that combined in short programs can solve complex metadata-oriented problems in a compact way. For instance, countless enterprise data integration scenarios can be easily expressed in this high-level language thus saving hundreds of development man-hours. Here we present the first model management engine that has reverse-engineering support for data integration, which is one of the most pressing metadata-oriented problems. It merges two schemas based on the mappings between them and allows user to correct the result keeping all the mappings in sync automatically. For user it is much more convenient than determining which mappings to correct in order to get desired result. In addition, the engine supports restructuring merging which is important when the sources are structured differently and cannot be mapped directly. While making schema merging fully automatic is not yet possible, our work simplifies and automates this process to make it practical in complex data integration scenarios.

#index 1206581
#* On the Anonymization of Sparse High-Dimensional Data
#@ Gabriel Ghinita;Yufei Tao;Panos Kalnis
#t 2008
#c 17
#! Existing research on privacy-preserving data publishing focuses on relational data: in this context, the objective is to enforce privacy-preserving paradigms, such as k-anonymity and l-diversity, while minimizing the information loss incurred in the anonymizing process (i.e. maximize data utility). However, existing techniques adopt an indexing-or clustering-based approach, and work well for fixed-schema data, with low dimensionality. Nevertheless, certain applications require privacy-preserving publishing of transaction data (or basket data), which involves hundreds or even thousands of dimensions, rendering existing methods unusable. We propose a novel anonymization method for sparse high-dimensional data. We employ a particular representation that captures the correlation in the underlying data, and facilitates the formation of anonymized groups with low information loss. We propose an efficient anonymization algorithm based on this representation. We show experimentally, using real-life datasets, that our method clearly outperforms existing state-of-the-art in terms of both data utility and computational overhead.

#index 1206582
#* On Anti-Corruption Privacy Preserving Publication
#@ Yufei Tao;Xiaokui Xiao;Jiexing Li;Donghui Zhang
#t 2008
#c 17
#! This paper deals with a new type of privacy threat, called "corruption", in anonymized data publication. Specifically, an adversary is said to have corrupted some individuals, if s/he has already obtained their sensitive values before consulting the released information. Conventional generalization may lead to severe privacy disclosure in the presence of corruption. Motivated by this, we advocate an alternative anonymization technique that integrates generalization with perturbation and stratified sampling. The integration provides strong privacy guarantees, even if an adversary has corrupted any number of individuals. We verify the effectiveness of the proposed technique through experiments with real data.

#index 1206583
#* Similar Document Detection with Limited Information Disclosure
#@ Wei Jiang;Mummoorthy Murugesan;Chris Clifton;Luo Si
#t 2008
#c 17
#! Similar document detection plays important roles in many applications, such as file management, copyright protection, and plagiarism prevention. Existing protocols assume that the contents of files stored on a server (or multiple servers) are directly accessible. This assumption limits more practical applications, e.g., detecting plagiarized documents between two conferences, where submissions are confidential. We propose novel protocols to detect similar documents between two entities where documents cannot be openly shared with each other. We also conduct experiments to show the practical value of the proposed protocols.

#index 1206584
#* k-Anonymization Revisited
#@ Aristides Gionis;Arnon Mazza;Tamir Tassa
#t 2008
#c 17
#! In this paper we introduce new notions of k-type anonymizations. Those notions achieve similar privacy goals as those aimed by Sweenie and Samarati when proposing the concept of k-anonymization: an adversary who knows the public data of an individual cannot link that individual to less than k records in the anonymized table. Every anonymized table that satisfies k-anonymity complies also with the anonymity constraints dictated by the new notions, but the converse is not necessarily true. Thus, those new notions allow generalized tables that may offer higher utility than k-anonymized tables, while still preserving the required privacy constraints. We discuss and compare the new anonymization concepts, which we call (1, k )-, (k, k)- and global (1, k)-anonymizations, according to several utility measures. We propose a collection of agglomerative algorithms for the problem of finding such anonymizations with high utility, and demonstrate the usefulness of our definitions and our algorithms through extensive experimental evaluation on real and synthetic datasets.

#index 1206585
#* Flow Algorithms for Parallel Query Optimization
#@ Amol Deshpande;Lisa Hellerstein
#t 2008
#c 17
#! We address the problem of minimizing the response time of a multi-way join query using pipelined (inter-operator) parallelism, in a parallel or a distributed environment. We observe that in order to fully exploit the parallelism in the system, we must consider a new class of "interleaving" plans, where multiple query plans are used simultaneously to minimize the response time of a query (or to maximize the tuple-throughput of the system). We cast the query planning problem in this environment as a "flow maximization problem", and present polynomial-time algorithms that (statically) find the optimal set of plans to use for a given query, for a large class of multi-way join queries. Our proposed algorithms also naturally extend to query optimization over web services. Finally we present an extensive experimental evaluation that demonstrates both the need to consider such plans in parallel query processing and the effectiveness of our algorithms.

#index 1206586
#* Self-Join Size Estimation in Large-scale Distributed Data Systems
#@ Theoni Pitoura;Peter Triantafillou
#t 2008
#c 17
#! In this work we tackle the open problem of self-join size (SJS) estimation in a large-scale Distributed Data System, where tuples of a relation are distributed over data nodes which comprise an overlay network. Our contributions include adaptations of five well-known SJS estimation centralized techniques (coined sequential, cross-sampling, adaptive, bifocal, and sample-count) to the network environment and a novel technique which is based on the use of the Gini coefficient. We develop analyses showing how Gini estimations can lead to estimations of the underlying Zipfian or power-law value distributions. We further contribute distributed sampling algorithms that can estimate accurately and efficiently the Gini coefficient. Finally, we provide detailed experimental evidence testifying for the claimed increased accuracy, precision, and efficiency of the proposed SJS estimation method, compared to the other methods. The proposed approach is the only one to ensure high efficiency, precision, and accuracy regardless of the skew of the underlying data.

#index 1206587
#* Extracting Loosely Structured Data Records Through Mining Strict Patterns
#@ Yipu Wu;Jing Chen;Qing Li
#t 2008
#c 17
#! Extracting loosely structured data records (DRs) has wide applications in many domains, such as forum pattern recognition, blog data analysis, and books and news review analysis. Currently existing methods work well for strongly structured DRs only. In this paper, we address the problem of extracting loosely structured DRs through mining strict patterns. In our method, we utilize both content feature and tag tree feature to recognize the loosely structured DRs, and propose a new approach to extract the DRs automatically. Through experimental study we demonstrate that this method is both effective and robust in practice.

#index 1206588
#* ICDE 2008 Brief Author Index
#@ 
#t 2008
#c 17

#index 1206589
#* ICDE 2008 Detailed Author Index
#@ 
#t 2008
#c 17

#index 1206590
#* info-author
#@ 
#t 2008
#c 17

#index 1206591
#* In Memoriam Klaus R. Dittrich (1950 - 2007)
#@ 
#t 2008
#c 17

#index 1206592
#* ICDE 2008 General Chairs Message
#@ Malu Castellanos;Alejandro Buchmann;Krithi Ramamritham
#t 2008
#c 17

#index 1206593
#* ICDE 2008 PC Chairs Message
#@ Gustavo Alonso;Jose A. Blakeley;Arbee Chen
#t 2008
#c 17

#index 1206594
#* Human Computation
#@ Luis von Ahn
#t 2008
#c 17
#! Tasks like image recognition are trivial for humans, but continue to challenge even the most sophisticated computer programs. This talk discusses a paradigm for utilizing human processing power to solve problems that computers cannot yet solve. Traditional approaches to solving such problems focus on improving software. I advocate a novel approach: constructively channel human brainpower to perform the tasks that computers cannot yet perform. For example, the ESP Game, described in this talk, is an enjoyable online game - many people play over 40 hours a week - and when people play, they help label images on the Web with descriptive keywords. These keywords can be used to significantly improve the accuracy of image search. People play the game not because they want to help, but because they enjoy it

#index 1206595
#* The Database Architecture Jigsaw Puzzle
#@ Martin L. Kersten
#t 2008
#c 17
#! Each DBMS represents a solution in a design space covering hundreds of parameters. The sheer size of this space leaves large parts unexplored, but also requires courage. The open-source MonetDB system is used to exemplify the pitfalls and opportunities of such an exploration into the realm of column-stores. We illustrate the vistas of high-risk projects based on radical changes in the design parameters, e.g., database cracking for self-organization, informative query summaries and database storage rings where the database is on the move. The missing jigsaw pieces identified are important for real innovations and provide an inspiration for changing the legacy architecture embodied in (relational) database products.

#index 1206596
#* PhotoSpread: A Spreadsheet for Managing Photos
#@ Hector Garcia-Molina
#t 2008
#c 17

#index 1206597
#* Sideways Information Passing for Push-Style Query Processing
#@ Zachary G. Ives;Nicholas E. Taylor
#t 2008
#c 17
#! In many modern data management settings, data is queried from a central node or nodes, but is stored at remote sources. In such a setting it is common to perform "push-style" query processing, using multithreaded pipelined hash joins and bushy query plans to compute parts of the query in parallel; to avoid idling, the CPU can switch between them as delays are encountered. This works well for simple select-project-join queries, but increasingly, Web and integration applications require more complex queries with multiple joins and even nested subqueries. As we demonstrate in this paper, push-style execution of complex queries can be improved substantially via sideways information passing; push-style queries provide many opportunities for information passing that have not been studied in the past literature. We present adaptive information passing, a general runtime decision-making technique for reusing intermediate state from one query subresult to prune and reduce computation of other subresults. We develop two alternative schemes for performing adaptive information passing, which we study in several settings under a variety of workloads.

#index 1206598
#* COLR-Tree: Communication-Efficient Spatio-Temporal Indexing for a Sensor Data Web Portal
#@ Yanif Ahmad;Suman Nath
#t 2008
#c 17
#! We present COLR-Tree, an abstraction layer designed to support efficient spatio-temporal queries on live data gathered from a large collection of sensors. We use COLR-Tree in a publicly-available sensor web portal to separate the concerns of sensor data management from the web portal application. COLR-Tree uses two techniques to optimize end-to-end latencies of users' queries by minimizing expensive data collection from sensors. First, it uses a novel technique to effectively cache aggregate results computed over sensor data with different expiry times. Second, it incorporates an efficient one-pass sampling algorithm with its range lookup to utilize cached data and compensate for occasional unavailability of sensors. We evaluate our implementation of COLR-Tree on SQL Server 2005 with a real, large workload from Windows Live Local. Our experiments demonstrate that COLR-Tree significantly improves both the end-to-end query performance and the number of sensors accessed compared to existing techniques.

#index 1206599
#* Region Sampling: Continuous Adaptive Sampling on Sensor Networks
#@ Song Lin;Benjamin Arai;Dimitrios Gunopulos;Gautam Das
#t 2008
#c 17
#! Satisfying energy constraints while meeting performance requirements is a primary concern when a sensor network is being deployed. Many recent proposed techniques offer error bounding solutions for aggregate approximation but cannot guarantee energy spending. Inversely, our goal is to bound the energy consumption while minimizing the approximation error. In this paper, we propose an online algorithm, Region Sampling, for computing approximate aggregates while satisfying a pre-defined energy budget. Our algorithm is distinguished by segmenting a sensor network into partitions of non-overlapping regions and performing sampling and local aggregation for each region. The sampling energy cost rate and sampling statistics are collected and analyzed to predict the optimal sampling plan. Comprehensive experiments on real-world data sets indicate that our approach is at a minimum of 10% more accurate compared with the previously proposed solutions.

#index 1206600
#* Fast and Highly-Available Stream Processing over Wide Area Networks
#@ Jeong-Hyon Hwang;Ugur Cetintemel;Stan Zdonik
#t 2008
#c 17
#! We present a replication-based approach that realizes both fast and highly-available stream processing over wide area networks. In our approach, multiple operator replicas send outputs to each downstream replica so that it can use whichever data arrives first. To further expedite the data flow, replicas run independently, possibly processing data in different orders. Despite this complication, our approach always delivers what non-replicated processing would produce without failures. We call this guarantee replication transparency. In this paper, we first discuss semantic issues for replication transparency and extend stream-processing primitives accordingly. Next, we develop an algorithm that manages replicas at geographically dispersed servers. This algorithm strives to achieve the best latency guarantee, relative to the cost of replication. Finally, we substantiate the utility of our work through experiments on PlanetLab servers as well as simulations based on real network traces.

#index 1206601
#* Approximate Joins for Data-Centric XML
#@ Nikolaus Augsten;Michael Bohlen;Curtis Dyreson;Johann Gamper
#t 2008
#c 17
#! In data integration applications, a join matches elements that are common to two data sources. Often, however, elements are represented slightly different in each source, so an approximate join must be used. For XML data, most approximate join strategies are based on some ordered tree matching technique. But in data-centric XML the order is irrelevant: two elements should match even if their subelement order varies. In this paper we give a solution for the approximate join of unordered trees. Our solution is based on windowed pq-grams. We develop an efficient technique to systematically generate windowed pq-grams in a three-step process: sorting the unordered tree, extending the sorted tree with dummy nodes, and computing the windowed pq-grams on the extended tree. The windowed pq-gram distance between two sorted trees approximates the tree edit distance between the respective unordered trees. The approximate join algorithm based on windowed pq-grams is implemented as an equality join on strings which avoids the costly computation of the distance between every pair of input trees. Our experiments with synthetic and real world data confirm the analytic results and suggest that our technique is both useful and scalable.

#index 1206602
#* Detection of Shape Anomalies: A Probabilistic Approach Using Hidden Markov Models
#@ Zheng Liu;Jeffrey Xu Yu;Lei Chen; Di Wu
#t 2008
#c 17
#! We study the problem of detecting the shape anomalies in this paper. Our shape anomaly detection algorithm is performed on the one-dimensional representation (time series) of shapes, whose similarity is modeled by a generalized segmental hidden Markov model (HMM) under a scaling, translation and rotation invariant manner. Experimental results show that our proposed approach can find shape anomalies in a large collection of shapes effectively and efficiently.

#index 1206603
#* Monitoring Network Evolution using MDL
#@ Jure Ferlez;Christos Faloutsos;Jure Leskovec;Dunja Mladenic;Marko Grobelnik
#t 2008
#c 17
#! Given publication titles and authors, what can we say about the evolution of scientific topics and communities over time? Which communities shrunk, which emerged, and which split, over time? And, when in time were the turning points? We propose TimeFall, which can automatically answer these questions given a social network/graph that evolves over time. The main novelty of the proposed approach is that it needs no user-defined parameters, relying instead on the principle of Minimum Description Length (MDL), to extract the communities, and to find good cut-points in time when communities change abruptly: a cut-point is good, if it leads to shorter data description. We illustrate our algorithm on synthetic and large real datasets, and we show that the results of the TimeFall agree with human intuition.

#index 1206604
#* Database Exploration Using Join Paths
#@ Cecilia M. Procopiuc;Divesh Srivastava
#t 2008
#c 17
#! Complex database schemas are challenging to explore and query, due to the exponentially many sequences of join edges in the schema graph, not all of which result in valid join paths. The problem becomes even more difficult when tables exhibit structural heterogeneity, i.e., different join paths are meaningful for different subsets of tuples in the same table. In this paper, we propose effective ways to identify meaningful join paths in complex schemas, and to compute the probability with which different destination tables are reached via join paths from a given source table.

#index 1206605
#* What-if OLAP Queries with Changing Dimensions
#@ L. V. S. Lakshmanan;A. Russakovsky;V. Sashikanth
#t 2008
#c 17
#! In a data warehouse, real-world activities can trigger changes to dimensions and their hierarchical structure. E.g., organizations can be reorganized over time causing changes to reporting structure. Product pricing changes in select markets can result in changes to bundled options in those markets. Much of the previous work on trend analysis on data warehouses has mainly focused on efficient evaluation of complex aggregations (e.g., data cube) and data-driven hypothetical scenarios. In this paper, we consider hypothetical scenarios driven by changes to dimension hierarchies and introduce the notion of perspectives. Perspectives are parameters such as time or location that drive changes in other dimensions. We demonstrate how perspectives aid in capturing a whole suite of what-if analysis queries. We propose various semantics for OLAP queries under perspectives and develop techniques for the efficient evaluation of such queries. We have implemented our techniques on the Essbase OLAP engine which fundamentally supports changing dimensions, and conducted a comprehensive set of experiments. Our results demonstrate the feasibility, scalability, and utility of our techniques for evaluating what-if queries with perspectives.

#index 1206606
#* A General Framework for Fast Co-clustering on Large Datasets Using Matrix Decomposition
#@ Feng Pan;Xiang Zhang;Wei Wang
#t 2008
#c 17
#! Simultaneously clustering columns and rows (co- clustering) of large data matrix is an important problem with wide applications, such as document mining, microarray analysis, and recommendation systems. Several co-clustering algorithms have been shown effective in discovering hidden clustering structures in the data matrix. For a data matrix of m rows and n columns, the time complexity of these methods is usually in the order of mtimesn (if not higher). This limits their applicability to data matrices involving a large number of columns and rows. Moreover, an implicit assumption made by existing co-clustering methods is that the whole data matrix needs to be held in the main memory. In this paper, we propose a general framework, CRD, for co-clustering large datasets utilizing recently developed sampling- based matrix decomposition methods. The time complexity of our approach is linear in m and n. And it does not require the whole data matrix be in the main memory. Experimental results show that CRD achieves competitive accuracy to existing co-clustering methods but with much less computational cost.

#index 1206607
#* MUSE: Multi-Represented Similarity Estimation
#@ Hans-Peter Kriegel;Peter Kunath;Alexey Pryakhin;Matthias Schubert
#t 2008
#c 17
#! In modern multimedia databases, objects can be specified by a large variety of feature representations. In this paper, we present a novel technique for multi-represented similarity estimation. We transform the distance between two objects in each representation into so-called similarity and dissimilarity estimates which are used to derive a meaningful similarity score. To determine the parameters for our new similarity measure, we present methods with and without user feedback.

#index 1206608
#* FLAME: Shedding Light on Hidden Frequent Patterns in Sequence Datasets
#@ Sandeep Tata;Jignesh M. Patel
#t 2008
#c 17
#! Existing database sequence mining algorithms focus on mining for subsequences. However, for many emerging applications, the subsequence model is inadequate for detecting interesting patterns. Often, an approximate substring model better accommodates the notion of a noisy pattern. In this paper, we present a powerful new model for approximate pattern mining. We show that this model can be used to capture the notion of an approximate match for a variety of different applications. We also present a novel, suffix tree based pattern mining algorithm called FLAME and demonstrate that it is a fast, accurate, and scalable method for discovering hidden patterns in large sequence databases.

#index 1206609
#* Declarative, Domain-Specific Languages - Elegant Simplicity or a Hammer in Search of a Nail?
#@ Samuel Madden;Johannes Gehrke
#t 2008
#c 17

#index 1206610
#* Cloud Computing-Was Thomas Watson Right After All?
#@ Raghu Ramakrishnan
#t 2008
#c 17

#index 1206611
#* Scientific Data Management: An Orphan in the Database Community?
#@ Randal Burns;Susan B. Davidson;Yannis Ioannidis;Miron Livny;Jignesh M. Patel
#t 2008
#c 17

#index 1206612
#* Muse: Mapping Understanding and deSign by Example
#@ Bogdan Alexe;Laura Chiticariu;Renee J. Miller;Wang-Chiew Tan
#t 2008
#c 17
#! A fundamental problem in information integration is that of designing the relationships, called schema mappings, between two schemas. The specification of a semantically correct schema mapping is typically a complex task. Automated tools can suggest potential mappings, but few tools are available for helping a designer understand mappings and design alternative mappings. We describe Muse, a mapping design wizard that uses data examples to assist designers in understanding and refining a schema mapping towards the desired specification. We present novel algorithms behind Muse and show how Muse systematically guides the designer on two important components of a mapping design: the specification of the desired grouping semantics for sets of data and the choice among alternative interpretations for semantically ambiguous mappings. In every component, Muse infers the desired semantics based on the designer's actions on a short sequence of small examples. Whenever possible, Muse draws examples from a familiar database, thus facilitating the design process even further. We report our experience with Muse on some publicly available schemas.

#index 1206613
#* Usage-Based Schema Matching
#@ Hazem Elmeleegy;Mourad Ouzzani;Ahmed Elmagarmid
#t 2008
#c 17
#! Existing techniques for schema matching are classified as either schema-based, instance-based, or a combination of both. In this paper, we define a new class of techniques, called usage-based schema matching. The idea is to exploit information extracted from the query logs to find correspondences between attributes in the schemas to be matched. We propose methods to identify co-occurrence patterns between attributes in addition to other features such as their use in joins and with aggregate functions. Several scoring functions are considered to measure the similarity of the extracted features, and a genetic algorithm is employed to find the highest-score mappings between the two schemas. Our technique is suitable for matching schemas even when their attribute names are opaque. It can further be combined with existing techniques to obtain more accurate results. Our experimental study demonstrates the effectiveness of the proposed approach and the benefit of combining it with other existing approaches.

#index 1206614
#* Clip: a Visual Language for Explicit Schema Mappings
#@ Alessandro Raffio;Daniele Braga;Stefano Ceri;Paolo Papotti;Mauricio A. Hernandez
#t 2008
#c 17
#! Many data integration solutions in the market today include tools for schema mapping, to help users visually relate elements of different schemas. Schema elements are connected with lines, which are interpreted as mappings, i.e. high-level logical expressions capturing the relationship between source and target data-sets; these are compiled into queries and programs that convert source-side data instances into target-side instances. This paper describes Clip, an XML Schema mapping tool distinguished from existing tools in that mappings explicitly specify structural transformations in addition to value couplings. Since Clip maps hierarchical XML schemas, lines appear naturally nested. We describe the transformation semantics associated with our "lines" and how they combine to form mappings that are more expressive than those generated by Clio, a well-known mapping tool. Further, we extend Clio's mapping generation algorithms to generate Clip's mappings.

#index 1206615
#* Transformation-based Framework for Record Matching
#@ Arvind Arasu;Surajit Chaudhuri;Raghav Kaushik
#t 2008
#c 17
#! Today's record matching infrastructure does not allow a flexible way to account for synonyms such as "Robert" and "Bob" which refer to the same name, and more general forms of string transformations such as abbreviations. We propose a programmatic framework of record matching that takes such user-defined string transformations as input. To the best of our knowledge, this is the first proposal for such a framework. This transformational framework, while expressive, poses significant computational challenges which we address. We empirically evaluate our techniques over real data.

#index 1206616
#* Querying Data under Access Limitations
#@ Andrea Cali;Davide Martinenghi
#t 2008
#c 17
#! Data sources on the web are often accessible through web interfaces that present them as relational tables, but require certain attributes to be mandatorily selected, e.g., via a web form. In a scenario where we integrate a set of such sources, and we pose queries over them, the values needed to access a source may have to be retrieved from other sources that are possibly not even mentioned in the query: answering queries at best can then be done only with a potentially recursive query plan that gets all obtainable answers to the query. Since data sources are typically distributed over a network, a major cost indicator for the execution of a query plan is the number of accesses to remote sources. In this paper we present an optimization technique for conjunctive queries that produces a query plan that: (1) minimizes the number of accesses according to a strong notion of minimality; (2) excludes all sources that are not relevant for the query. We introduce Toorjah, a prototype system that answers queries posed on sources with limitations by means of optimized query plans. Toorjah adopts a strategy that is aimed to retrieve answers as early as possible during query processing, and to present them to the user as they are computed. We provide experimental evidence of the effectiveness of our optimization, by showing the reduction of the number of accesses in a large number of cases.

#index 1206617
#* Mining Search-Phrase Definitions from Item Descriptions
#@ Hung V. Nguyen;Hasan Davulcu
#t 2008
#c 17
#! In this paper, we develop a model for representing term dependence based on Markov Random Fields and present an approach based on Markov Chain Monte Carlo technique for generating phrase definitions. This approach can use a small corpus of keyword matching and a random sample of other product descriptions for an advertiser's search-phrase to effectively mine and rank alternative but highly relevant search-phrase definitions. These definitions, which are search-phrases themselves, can then be provided as alternative phrases to an advertiser.

#index 1206618
#* Rights Protection of Trajectory Datasets
#@ Claudio Lucchese;Michail Vlachos;Deepak Rajan;Philip S. Yu
#t 2008
#c 17
#! This work presents a technique of convincingly claiming ownership rights over a trajectory dataset. The presented methodology distorts imperceptibly a collection of sequences, effectively embedding a secret key, while retaining as well as possible the neighborhood of each object, which is vital for operations such as similarity search, classification or clustering.

#index 1206619
#* Privacy Preserving Joins
#@ Yaping Li;Minghua Chen
#t 2008
#c 17
#! In this paper, we design a system for mutually distrustful entities to perform privacy preserving joins, leveraging the power of a memory-limited secure coprocessor. Under this setting, we critique a questionable assumption in a previous privacy definition [1] that leads to unnecessary information leakage. We then remove the assumption and propose a new definition. Based on this definition, we propose three correct and provable secure algorithms to compute general joins of arbitrary predicates, by utilizing available cryptographic tools in a nontrivial way. We discuss different memory requirements of our proposed algorithms, and explore how to trade little privacy with significant performance improvement. In [2], we evaluate the performance of our algorithms by numerical examples. We also show the performance superiority of our approach over secure multi-party computation in [2].

#index 1206620
#* Link Privacy in Social Networks
#@ Aleksandra Korolova;Rajeev Motwani;Shubha U. Nabar;Ying Xu
#t 2008
#c 17
#! We consider a privacy threat to a social network in which the goal of an attacker is to obtain knowledge of a significant fraction of the links in the network. We formalize the typical social network interface and the information about links that it provides to its users in terms of lookahead. We consider a particular threat in which an attacker subverts user accounts to gain information about local neighborhoods in the network and pieces them together in order to build a global picture. We analyze, both experimentally and theoretically, the number of user accounts an attacker would need to subvert for a successful attack, as a function of his strategy for choosing users whose accounts to subvert and a function of the lookahead provided by the network. We conclude that such an attack is feasible in practice, and thus any social network that wishes to protect the link privacy of its users should take great care in choosing the lookahead of its interface, limiting it to 1 or 2, whenever possible.

#index 1206621
#* Protecting Databases from Query Flood Attacks
#@ Anna Cinzia Squicciarini;Ivan Paloscia;Elisa Bertino
#t 2008
#c 17
#! A typical Denial of Service attack against a DBMS may occur through a query flood, that is, a large number of queries and/or updates sent by a malicious subject or several colluding malicious subjects to a target database with the intention to hinder other subjects from being serviced. In this paper we present experimental results showing that such attacks indeed degrade the performance of the DBMS; our experiments are conducted on several well known DBMS. We then propose some simple yet effective techniques for detecting query-flood attacks and protecting a DBMS against them.

#index 1206622
#* Secure Delta-Publishing of XML Content
#@ Mohamed Nabeel;Elisa Bertino
#t 2008
#c 17
#! Many content distribution applications are characterized by frequent, incremental updates. Efficiency is not the only requirement in that security is also crucial for a large spectrum of applications. The goal of this work is to develop an approach for efficient and scalable dissemination of XML documents while assuring confidentiality, integrity and completeness without requiring the third-party publishers to be trusted. The key element of our approach to reduce the bandwidth requirements is to use delta messaging. Our approach takes every possible measure to minimize indirect information leakage by making the rest of the structure of XML documents to which clients do not have access oblivious. The experimental results show that our scheme is superior to conventional techniques of securing XML documents when the percentage of updates with respect to original documents is low.

#index 1206623
#* On the Optimal Selection of k in the k-Anonymity Problem
#@ Rinku Dewri;Indrajit Ray;Indrakshi Ray;Darrell Whitley
#t 2008
#c 17
#! When disseminating data involving human subjects, researchers have to weigh in the requirements of privacy of the individuals involved in the data. A model widely used for enhancing individual privacy is k-anonymity, where an individual data record is rendered similar to k - 1 other records in the data set by using generalization and/or suppression operations on the data attributes. The drawback of this model is that such transformations result in considerable loss of information that is proportional to the choice of k. Studies in this context have so far focused on minimizing the information loss for some given value of k. However, owing to the presence of outliers, a specified k value may or may not be obtainable. Further, an exhaustive analysis is required to determine a k value that fits the loss constraint specified by a data publisher. In this paper, we formulate a multi-objective optimization problem to illustrate that the decision on k can be much more informed than being a choice solely based on the privacy requirement. The optimization problem is intended to resolve the issue of data privacy when data suppression is not allowed in order to obtain a particular value of k. An evolutionary algorithm is employed here to provide this insight.

#index 1206624
#* Constant-Time Query Processing
#@ Vijayshankar Raman;Garret Swart;Lin Qiao;Frederick Reiss;Vijay Dialani;Donald Kossmann;Inderpal Narang;Richard Sidle
#t 2008
#c 17
#! Query performance in current systems depends significantly on tuning: how well the query matches the available indexes, materialized views etc. Even in a well tuned system, there are always some queries that take much longer than others. This frustrates users who increasingly want consistent response times to ad hoc queries. We argue that query processors should instead aim for constant response times for all queries, with no assumption about tuning. We present Blink, our first attempt at this goal, that runs every query as a table scan over a fully denormalized database, with hash group-by done along the way. To make this scan efficient, Blink uses a novel compression scheme that horizontally partitions tuples by frequency, thereby compressing skewed data almost down to entropy, even while producing long runs of fixed-length, easily-parseable values. We also present a scheme for evaluating a conjunction of range and equality predicates in SIMD fashion over compressed tuples, and different schemes for efficient hash-based aggregation within the L2 cache. A experimental study with a suite of arbitrary single block SQL queries over a TPCH-like schema suggests that constant-time queries can be efficient.

#index 1206625
#* A Hybrid Prediction Model for Moving Objects
#@ Hoyoung Jeung;Qing Liu;Heng Tao Shen;Xiaofang Zhou
#t 2008
#c 17
#! Existing prediction methods in moving objects databases cannot forecast locations accurately if the query time is far away from the current time. Even for near future prediction, most techniques assume the trajectory of an object's movements can be represented by some mathematical formulas of motion functions based on its recent movements. However, an object's movements are more complicated than what the mathematical formulas can represent. Prediction based on an object's trajectory patterns is a powerful way and has been investigated by several work. But their main interest is how to discover the patterns. In this paper, we present a novel prediction approach, namely The Hybrid Prediction Model, which estimates an object's future locations based on its pattern information as well as existing motion functions using the object's recent movements. Specifically, an object's trajectory patterns which have ad-hoc forms for prediction are discovered and then indexed by a novel access method for efficient query processing. In addition, two query processing techniques that can provide accurate results for both near and distant time predictive queries are presented. Our extensive experiments demonstrate that proposed techniques are more accurate and efficient than existing forecasting schemes.

#index 1206626
#* A Generic Framework for Continuous Motion Pattern Query Evaluation
#@ Petko Bakalov;Vassilis J. Tsotras
#t 2008
#c 17
#! We introduce a novel query type defined over streaming moving object data, namely, the Continuous Motion Pattern (CMP) Queries. A motion pattern is defined as a sequence of distinct spatial predicates, each attached to a temporal constraint. The spatial predicates can be of various types (range, nearest neighbor, etc.) The temporal constraints are relative to the current time instant and are used to specify the order of the spatial predicates on the time axis. A CMP query is continuously reevaluated over streaming spatiotemporal data, producing the moving objects which satisfy the query's motion pattern. We first introduce an easily maintainable indexing scheme for spatiotemporal streams that facilitates the evaluation of the spatial predicates over their temporal constraints. Using this scheme we propose a generic framework for efficiently answering a wide range of CMP queries. The effectiveness of our algorithms in reducing the query computation cost and I/O operations is revealed through a thorough experimental evaluation.

#index 1206627
#* Querying Complex Spatio-Temporal Sequences in Human Motion Databases
#@ Yueguo Chen;Shouxu Jiang;Beng Chin Ooi;Anthony K. H. Tung
#t 2008
#c 17
#! Content-based retrieval of spatio-temporal patterns from human motion databases is inherently nontrivial since finding effective distance measures for such data is difficult. These data are typically modelled as time series of high dimensional vectors which incur expensive storage and retrieval cost as a result of the high dimensionality. In this paper, we abstract such complex spatio-temporal data as a set of frames which are then represented as high dimensional categorical feature vectors. New distance measures and queries for high dimensional categorical time series are then proposed and efficient query processing techniques for answering these queries are developed. We conducted experiments using our proposed distance measures and queries on human motion capture databases. The results indicate that significant improvement on the efficiency of query processing of categorical time series (more than 10,000 times faster than that of the original motion sequences) can be achieved while guaranteeing the effectiveness of the search.

#index 1206628
#* Common Influence Join: A Natural Join Operation for Spatial Pointsets
#@ Man Lung Yiu;Nikos Mamoulis;Panagiotis Karras
#t 2008
#c 17
#! We identify and formalize a novel join operator for two spatial pointsets P and Q. The common influence join (CIJ) returns the pairs of points (p, q), pP, qQ, such that there exists a location in space, being closer to p than to any other point in P and at the same time closer to q than to any other point in Q. In contrast to existing join operators between pointsets (i.e., -distance joins and k-closest pairs), CIJ is parameter-free, providing a natural join result that finds application in marketing and decision support. We propose algorithms for the efficient evaluation of CIJ, for pointsets indexed by hierarchical multi-dimensional indexes. We validate the effectiveness and the efficiency of these methods via experimentation with synthetic and real spatial datasets. The experimental results show that a non-blocking algorithm, which computes intersecting pairs of Voronoi cells on-demand, is very efficient in practice, incurring only slightly higher I/O cost than the theoretical lower bound cost for the problem.

#index 1206629
#* Anonymizing Streaming Data for Privacy Protection
#@ Jianzhong Li;Beng Chin Ooi;Weiping Wang
#t 2008
#c 17
#! In many applications, transaction data arrive in the form of high speed data streams. These data contain a lot of information about customers, not just transactions, and thus have to be carefully managed to protect customers' privacy. This paper presents a novel method called SKY (Stream K-anonYmity) to continuously facilitate k-anonymity on data streams. Experimental results show that SKY is efficient and effective.

#index 1206630
#* Privately Updating Suppression and Generalization based k-Anonymous Databases
#@ Alberto Trombetta;Wei Jiang;Elisa Bertino;Lorenzo Bossi
#t 2008
#c 17
#! Alice, owner of a k-anonymous database, needs to determine whether her database, when inserted with a tuple owned by Bob, is still k-anonymous. Suppose that Bob is not allowed to access to the database because of data confidentiality and that Alice is not allowed to read Bob's tuple due to Bob's privacy concern. Under these assumptions, this paper proposes two protocols to check whether the database inserted with a tuple is still k-anonymous, without letting Alice and Bob know the contents of the tuple and the database respectively.

#index 1206631
#* InstantDB: Enforcing Timely Degradation of Sensitive Data
#@ Nicolas Anciaux;Luc Bouganim;Harold van Heerde;Philippe Pucheral;Peter M.  G. Apers
#t 2008
#c 17
#! People cannot prevent personal information from being collected by various actors. Several security measures are implemented on servers to minimize the possibility of a privacy violation. Unfortunately, even the most well defended servers are subject to attacks and however much one trusts a hosting organism/company, such trust does not last forever. We propose a simple and practical degradation model where sensitive data undergoes a progressive and irreversible degradation from an accurate state at collection time, to intermediate but still informative fuzzy states, to complete disappearance. We introduce the data degradation model and identify related technical challenges and open issues.

#index 1206632
#* CASTLE: A delay-constrained scheme for ks-anonymizing data streams
#@ Jianneng Cao;Barbara Carminati;Elena Ferrari;Kian Lee Tan
#t 2008
#c 17
#! Most of existing privacy preserving techniques, such as k-anonymity methods, are designed for static data sets. As such, they cannot be applied to streaming data which are continuous, transient and usually unbounded. Moreover, in streaming applications, there is a need to offer strong guarantees on the maximum allowed delay between an incoming data and its anonymized output. To cope with these requirements, in this paper, we present CASTLE (Continuously Anonymizing STreaming data via adaptive cLustEring), a cluster-based scheme that anonymizes data streams on-the-fly and, at the same time, ensures the freshness of the anonymized data by satisfying specified delay constraints. We further show how CASTLE can be easily extended to handle l-diversity [1]. Our extensive performance study shows that CASTLE is efficient and effective.

#index 1206633
#* Exponentially Decayed Aggregates on Data Streams
#@ Graham Cormode;Flip Korn;Srikanta Tirthapura
#t 2008
#c 17
#! In a massive stream of sequential events such as stock feeds, sensor readings, or IP traffic measurements, tuples pertaining to recent events are typically more important than older ones. It is important to compute various aggregates over such streams after applying a decay function which assigns weights to tuples based on their age. We focus on the computation of exponentially decayed aggregates in the form of quantiles and heavy hitters. Our techniques are based on extending existing data stream summaries, such as the q-digest [1] and the "space-saving" algorithm [2]. Our experiments confirm that our methods can be applied in practice, and have similar space and time costs to the non-decayed aggregate computation.

#index 1206634
#* Efficient Aggregate Computation over Data Streams
#@ Kanthi Nagaraj;K. V. M. Naidu;Rajeev Rastogi;Scott Satkin
#t 2008
#c 17
#! Cisco's NetFlow Collector (NFC) is a powerful example of a real-world product that supports multiple aggregate queries over a continuous stream of IP flow records. NFC enables a plethora of network management tasks like traffic demands estimation, application traffic profiling, etc. In this paper, we investigate two computation sharing techniques for enabling streaming applications such as NFC to scale to hundreds of queries. Our first technique instantiates certain intermediate aggregates which are then used to generate the final answers for input queries. Our second technique coalesces the filter conditions of similar queries and uses the coalesced filter to pre-filter stream data input to these queries. Using these techniques, we propose a heuristic to compute a good query plan and perform extensive simulations to show that our heuristic delivers a factor of over 3 performance improvement compared to a naive approach.

#index 1206635
#* Online Failure Forecast for Fault-Tolerant Data Stream Processing
#@ Xiaohui Gu;Spiros Papadimitriou;Philip S. Yu;Shu-Ping Chang
#t 2008
#c 17
#! In this paper, we present a new online failure forecast system to achieve predictive failure management for fault-tolerant data stream processing. Different from previous reactive or proactive approaches, predictive failure management employs failure forecast to perform informed and just-in-time preventive actions on abnormal components only. We employ stream-based online learning methods to continuously classify runtime operator state into normal, alert, or failure, based on collected feature streams. We have implemented the online failure forecast system as part of the IBM System S stream processing system. Our experiments show that the on-line failure forecast system can achieve good prediction accuracy for a range of stream processing software failures, while imposing low overhead to the stream system.

#index 1206636
#* Matching Schemas in Online Communities: A Web 2.0 Approach
#@ Robert McCann;Warren Shen;AnHai Doan
#t 2008
#c 17
#! When integrating data from multiple sources, a key task that online communities often face is to match the schemas of the data sources. Today, such matching often incurs a huge workload that overwhelms the relatively small set of volunteer integrators. In such cases, community members may not even volunteer to be integrators, due to the high workload, and consequently no integration systems can be built. To address this problem, we propose to enlist the multitude of users in the community to help match the schemas, in a Web 2.0 fashion. We discuss the challenges of this approach and provide initial solutions. Finally, we describe an extensive set of experiments on both real-world and synthetic data that demonstrate the utility of the approach.

#index 1206637
#* Validating Multi-column Schema Matchings by Type
#@ Bing Tian Dai;Nick Koudas;Divesh Srivastava;Anthony K.  H. Tung;Suresh Venkatasubramanian
#t 2008
#c 17
#! Validation of multi-column schema matchings is essential for successful database integration. This task is especially difficult when the databases to be integrated contain little overlapping data, as is often the case in practice (e.g., customer bases of different companies). Based on the intuition that values present in different columns related by a schema matching will have similar "semantic type", and that this can be captured using distributions over values ("statistical types"), we develop a method for validating 1-1 and compositional schema matchings. Our technique is based on three key technical ideas. First, we propose a generic measure for comparing two columns matched by a schema matching, based on a notion of information-theoretic discrepancy that generalizes the standard geometric discrepancy; this provides the basis for 1:1 matching. Second, we present an algorithm for "splitting" the string values in a column to identify substrings that are likely to match with the values in another column; this enables (multi-column) 1:m schema matching. Third, our technique provides an invalidation certificate if it fails to validate a schema matching. We complement our conceptual and algorithmic contributions with an experimental study that demonstrates the effectiveness and efficiency of our technique on a variety of database schemas and data sets.

#index 1206638
#* CARE: Finding Local Linear Correlations in High Dimensional Data
#@ Xiang Zhang;Feng Pan;Wei Wang
#t 2008
#c 17
#! Finding latent patterns in high dimensional data is an important research problem with numerous applications. Existing approaches can be summarized into 3 categories: feature selection, feature transformation (or feature projection) and projected clustering. Being widely used in many applications, these methods aim to capture global patterns and are typically performed in the full feature space. In many emerging biomedical applications, however, scientists are interested in the local latent patterns held by feature subsets, which may be invisible via any global transformation. In this paper, we investigate the problem of finding local linear correlations in high dimensional data. Our goal is to find the latent pattern structures that may exist only in some subspaces. We formalize this problem as finding strongly correlated feature subsets which are supported by a large portion of the data points. Due to the combinatorial nature of the problem and lack of monotonicity of the correlation measurement, it is prohibitively expensive to exhaustively explore the whole search space. In our algorithm, CARE, we utilize spectrum properties and effective heuristic to prune the search space. Extensive experimental results show that our approach is effective in finding local linear correlations that may not be identified by existing methods.

#index 1206639
#* Trajectory Outlier Detection: A Partition-and-Detect Framework
#@ Jae-Gil Lee;Jiawei Han;Xiaolei Li
#t 2008
#c 17
#! Outlier detection has been a popular data mining task. However, there is a lack of serious study on outlier detection for trajectory data. Even worse, an existing trajectory outlier detection algorithm has limited capability to detect outlying sub-trajectories. In this paper, we propose a novel partition-and-detect framework for trajectory outlier detection, which partitions a trajectory into a set of line segments, and then, detects outlying line segments for trajectory outliers. The primary advantage of this framework is to detect outlying sub-trajectories from a trajectory database. Based on this partition-and-detect framework, we develop a trajectory outlier detection algorithm TRAOD. Our algorithm consists of two phases: partitioning and detection. For the first phase, we propose a two-level trajectory partitioning strategy that ensures both high quality and high efficiency. For the second phase, we present a hybrid of the distance-based and density-based approaches. Experimental results demonstrate that TRAOD correctly detects outlying sub-trajectories from real trajectory data.

#index 1206640
#* A Framework for Clustering Uncertain Data Streams
#@ Charu C. Aggarwal;Philip S. Yu
#t 2008
#c 17
#! In recent years, uncertain data management applications have grown in importance because of the large number of hardware applications which measure data approximately. For example, sensors are typically expected to have considerable noise in their readings because of inaccuracies in data retrieval, transmission, and power failures. In many cases, the estimated error of the underlying data stream is available. This information is very useful for the mining process, since it can be used in order to improve the quality of the underlying results. In this paper we will propose a method for clustering uncertain data streams. We use a very general model of the uncertainty in which we assume that only a few statistical measures of the uncertainty are available. We will show that the use of even modest uncertainty information during the mining process is sufficient to greatly improve the quality of the underlying results. We show that our approach is more effective than a purely deterministic method such as the CluStream approach. We will test the approach on a variety of real and synthetic data sets and illustrate the advantages of the method in terms of effectiveness and efficiency.

#index 1206641
#* On Supporting Kleene Closure over Event Streams
#@ Daniel Gyllstrom;Jagrati Agrawal;Yanlei Diao;Neil Immerman
#t 2008
#c 17
#! Complex event patterns involving Kleene closure are finding application in a variety of stream environments for tracking and monitoring purposes. In this paper, we propose a compact language, SASE+, that can be used to define a wide variety of Kleene closure patterns, analyze the expressive power of the language, and outline an automata-based implementation for efficient Kleene closure evaluation over event streams.

#index 1206642
#* MOOLAP: Towards Multi-Objective OLAP
#@ Shyam Antony;Ping Wu;Divyakant Agrawal;Amr El Abbadi
#t 2008
#c 17
#! Aggregation is among the core functionalities of OLAP systems. Frequently, such queries are issued in decision support systems to identify interesting groups of data. When more than one aggregation function is involved and the notion of interest is not clearly defined, skyline queries provide a robust mechanism to capture the potentially interesting points where (i) users do not need to specify a ranking function and (ii) the result is independent of the dimension scales. To provide better exploration functionalities in OLAP systems, we propose to use skyline queries over aggregated data to identify the most interesting groups. Since aggregation functions have to be ad-hoc to cover a wide variety of user interests, the skyline over the aggregates has to be computed on the fly. Hence any algorithm to compute such a skyline must be fast and be able to progressively produce the result set with potential skyline groups being produced as early as possible. We explore a family of algorithms which try to consume only as many data records as are necessary to compute the skyline and design an optimal algorithm. We further refine the algorithm by taking into account systems issues such as disk behavior which are often ignored but have strong impact on real system performance. Experimental results validate the performance and progressive benefits of our algorithm.

#index 1206643
#* On Skylining with Flexible Dominance Relation
#@ Tian Xia;Donghui Zhang;Yufei Tao
#t 2008
#c 17
#! Given a set of d dimensional objects, a skyline query finds the objects ("skyline") that are not dominated by others. However, skylines do not always provide useful query results to users, and existing methods of various skyline queries have at least one of the following drawbacks: (1) the size of skyline objects can not be controlled, or can be only increased or only decreased but not both; (2) skyline objects do not have built-in ranks; (3) skylines do not reflect users' weights (preferences) at different dimensions. In this paper, we propose a unified approach, the epsiv-skyline, to effectively solve all three drawbacks. We explore the properties of epsiv-skylines and propose two different algorithms to compute epsiv-skylines.

#index 1206644
#* Designing Random Sample Synopses with Outliers
#@ Philipp Rosch;Rainer Gemulla;Wolfgang Lehner
#t 2008
#c 17
#! Random sampling is one of the most widely used means to build synopses of large datasets because random samples can be used for a wide range of analytical tasks. Unfortunately, the quality of the estimates derived from a sample is negatively affected by the presence of "outliers" in the data. In this paper, we show how to circumvent this shortcoming by constructing outlier-aware sample synopses. Our approach extends the well-known outlier indexing scheme to multiple aggregation columns.

#index 1206645
#* Efficiently Answering Probabilistic Threshold Top-k Queries on Uncertain Data
#@ Ming Hua;Jian Pei;Wenjie Zhang;Xuemin Lin
#t 2008
#c 17
#! In this paper, we propose a novel type of probabilistic threshold top-k queries on uncertain data, and give an exact algorithm. More details can be found in [4].

#index 1206646
#* Efficient Processing of Top-k Queries in Uncertain Databases
#@ Ke Yi;Feifei Li;George Kollios;Divesh Srivastava
#t 2008
#c 17
#! This work introduces novel polynomial-time algorithms for processing top-k queries in uncertain databases, under the generally adopted model of x-relations. An x-relation consists of a number of x-tuples, and each x-tuple randomly instantiates into one tuple from one or more alternatives. Our results significantly improve the best known algorithms for top-k query processing in uncertain databases, in terms of both running time and memory usage. Focusing on the single-alternative case, the new algorithms are orders of magnitude faster.

#index 1206647
#* Adjoined Dimension Column Clustering to Improve Data Warehouse Query Performance
#@ Xuedong Chen;Patrick O'Neil;Elizabeth O'Neil
#t 2008
#c 17
#! Columns of dimension tables that are frequently restricted in queries on a star schema database are materialized into the fact table by various means, depending on database product features. This makes it possible to create a multi-dimensional partitioning on the fact table, so that many queries restricting columns in hierarchies of the columns materialized will retrieve only a small number of cells of a rectangular subset of the partitioning. We believe this approach to be a previously undiscovered, though seemingly obvious, method to accelerate query performance.

#index 1206648
#* Adaptive Segmentation for Scientific Databases
#@ Milena Ivanova;Martin L. Kersten;Niels Nes
#t 2008
#c 17
#! In this paper we explore database segmentation in the context of a column-store DBMS targeted at a scientific database. We present a novel hardware- and scheme-oblivious segmentation algorithm, which learns and adapts to the workload immediately. The approach taken is to capitalize on (intermediate) query results, such that future queries benefit from a more appropriate data layout. The algorithm is implemented as an extension of a complete DBMS and evaluated against a real-life workload. It demonstrates significant performance gains without DBA assistance.

#index 1206649
#* Mining Approximate Order Preserving Clusters in the Presence of Noise
#@ Mengsheng Zhang;Wei Wang;Jinze Liu
#t 2008
#c 17
#! Subspace clustering has attracted great attention due to its capability of finding salient patterns in high dimensional data. Order preserving subspace clusters have been proven to be important in high throughput gene expression analysis, since functionally related genes are often co-expressed under a set of experimental conditions. Such co-expression patterns can be represented by consistent orderings of attributes. Existing order preserving cluster models require all objects in a cluster have identical attribute order without deviation. However, real data are noisy due to measurement technology limitation and experimental variability which prohibits these strict models from revealing true clusters corrupted by noise. In this paper, we study the problem of revealing the order preserving clusters in the presence of noise. We propose a noise-tolerant model called approximate order preserving cluster (AOPC). Instead of requiring all objects in a cluster have identical attribute order, we require that (1) at least a certain fraction of the objects have identical attribute order; (2) other objects in the cluster may deviate from the consensus order by up to a certain fraction of attributes. We also propose an algorithm to mine AOPC. Experiments on gene expression data demonstrate the efficiency and effectiveness of our algorithm.

#index 1206650
#* Direct Discriminative Pattern Mining for Effective Classification
#@ Hong Cheng;Xifeng Yan;Jiawei Han;Philip S. Yu
#t 2008
#c 17
#! The application of frequent patterns in classification has demonstrated its power in recent studies. It often adopts a two-step approach: frequent pattern (or classification rule) mining followed by feature selection (or rule ranking). However, this two-step process could be computationally expensive, especially when the problem scale is large or the minimum support is low. It was observed that frequent pattern mining usually produces a huge number of "patterns" that could not only slow down the mining process but also make feature selection hard to complete. In this paper, we propose a direct discriminative pattern mining approach, DDPMine, to tackle the efficiency issue arising from the two-step approach. DDPMine performs a branch-and-bound search for directly mining discriminative patterns without generating the complete pattern set. Instead of selecting best patterns in a batch, we introduce a "feature-centered" mining approach that generates discriminative patterns sequentially on a progressively shrinking FP-tree by incrementally eliminating training instances. The instance elimination effectively reduces the problem size iteratively and expedites the mining process. Empirical results show that DDPMine achieves orders of magnitude speedup without any downgrade of classification accuracy. It outperforms the state-of-the-art associative classification methods in terms of both accuracy and efficiency.

#index 1206651
#* Verifying and Mining Frequent Patterns from Large Windows over Data Streams
#@ Barzan Mozafari;Hetal Thakkar;Carlo Zaniolo
#t 2008
#c 17
#! Mining frequent itemsets from data streams has proved to be very difficult because of computational complexity and the need for real-time response. In this paper, we introduce a novel verification algorithm which we then use to improve the performance of monitoring and mining tasks for association rules. Thus, we propose a frequent itemset mining method for sliding windows, which is faster than the state-of-the-art methods - in fact, its running time that is nearly constant with respect to the window size entails the mining of much larger windows than it was possible before. The performance of other frequent itemset mining methods (including those on static data) can be improved likewise, by replacing their counting methods (e.g., those using hash trees) by our verification algorithm.

#index 1206652
#* On Signatures for Communication Graphs
#@ Graham Cormode;Flip Korn;S. Muthukrishnan;Yihua Wu
#t 2008
#c 17
#! Communications between individuals can be represented by (weighted, multi-) graphs. Many applications operate on communication graphs associated with telephone calls, emails, instant messages (IM), blogs, web forums, e-business relationships and so on. These applications include identifying repetitive fraudsters, message board aliases, multiusage of IP addresses, etc. Tracking electronic identities in communication networks can be achieved if we have a reliable "signature" for nodes and activities. While many examples of ad hoc signatures can be proposed for particular tasks, what is needed is a systematic study of the principles behind the usage of signatures for any task. We develop a formal framework for the use of signatures in communication graphs and identify three fundamental properties that are natural to signature schemes: persistence, uniqueness and robustness. We argue for the importance of these properties by showing how they impact a set of applications. We then explore several signature schemes - previously defined and new - in our framework and evaluate them on real data in terms of these properties. This provides insights into suitable signature schemes for desired applications. Finally, as case studies, we focus on two concrete applications in enterprise network traffic. We apply signature schemes to these problems and demonstrate their effectiveness.

#index 1206653
#* Robust Stratified Sampling Plans for Low Selectivity Queries
#@ Shantanu Joshi;Christopher Jermaine
#t 2008
#c 17
#! We consider the problem of estimating the result of an aggregate query with a very low selectivity. Traditional sampling techniques can be ineffective for such a problem since a small random sample is likely to miss most or even all of the records satisfying the restrictive selection predicate. Stratfied sampling is useful in this situation, but a key problem in applying stratified sampling effectively is identifying which strata are important and developing a sampling plan that favors those strata in a robust fashion. We develop a solution to this problem that combines any prior knowledge or expectation about the stratification with information obtained from pilot sampling in a principled Bayesian framework.

#index 1206654
#* QShuffler: Getting the Query Mix Right
#@ Mumtaz Ahmad;Ashraf Aboulnaga;Shivnath Babu;Kamesh Munagala
#t 2008
#c 17
#! The typical workload in a database system consists of a mixture of multiple queries of different types, running concurrently and interacting with each other. Hence, optimizing performance requires reasoning about query mixes and their interactions, rather than considering individual queries or query types. In this paper, we use such a reasoning approach to develop a query scheduler. We treat the database system as a black box and experimentally build a model to estimate the performance of different query mixes. Our scheduler uses this model to decide which query mixes to schedule, with the goal of maximizing throughput. We experimentally demonstrate the effectiveness of our scheduler using queries from the TPC-H benchmark on DB2.

#index 1206655
#* Adaptive Bitmap Indexes for Space-Constrained Systems
#@ Rishi Rakesh Sinha;Marianne Winslett;Kesheng Wu;Kurt Stockinger;Arie Shoshani
#t 2008
#c 17
#! Data management systems for "big science" often have tight memory and disk space constraints. In this paper, we introduce adaptive bitmap indexes, which conform to both space limits while dynamically adapting to the query load and offering excellent performance. So that adaptive bitmap indexes can use optimal bin boundaries, we show how to improve the scalability of optimal binning algorithms so that they can be used with real-world workloads. As the removal of false positives is the largest component of lookup time for a small-footprint bitmap index, we propose a novel way to materialize and drop auxiliary projection indexes, to eliminate the need to visit the data store to check for false positives. Our experiments with real-world data and queries show that adaptive bitmap indexes offer approximately 100-300% performance improvement (compared to standard binned bitmap indexes) at a cost of 5 MB of dedicated memory, under disk storage constraints that would cripple other indexes.

#index 1206656
#* Skyline-based Peer-to-Peer Top-k Query Processing
#@ Akrivi Vlachou;Christos Doulkeridis;Kjetil Norvag;Michalis Vazirgiannis
#t 2008
#c 17
#! Due to applications and systems such as sensor networks, data streams, and peer-to-peer (P2P) networks, data generation and storage become increasingly distributed. Therefore a challenging problem is to support best-match query processing in highly distributed environments. In this paper, we present a novel framework for top-k query processing in large-scale P2P networks, where the dataset is horizontally distributed to peers. Our proposed framework returns the exact results to the user, while minimizing the number of queried super-peers and transferred data. Through simulations we demonstrate the feasibility of our approach in terms of overall response time.

#index 1206657
#* Adaptive Approximate Similarity Searching through Metric Social Networks
#@ Jan Sedmidubsky;Stanislav Barton;Vlastislav Dohnal;Pavel Zezula
#t 2008
#c 17
#! Exploiting the concepts of social networking represents a novel approach to the approximate similarity query processing. We present a metric social network where relations between peers, giving similar results, are established on per-query basis. Based on the universal law of generalization, a new query forwarding algorithm is proposed. The same principle is used to manage query histories of individual peers with the possibility to tune the tradeoff between the extent of the history and the level of the query-answer approximation. All algorithms are tested on real data and real network of computers.

#index 1206658
#* Fixed-Precision Approximate Continuous Aggregate Queries in Peer-to-Peer Databases
#@ Farnoush Banaei-Kashani;Cyrus Shahabi
#t 2008
#c 17
#! In this paper, we outline our efficient sample-based approach to answer fixed-precision approximate continuous aggregate queries in peer-to-peer databases. We describe our approach in the context of Digest, a two-tier system we have developed for correct and efficient query answering by sampling. With Digest, at the top tier we develop a query evaluation engine that uses the samples collected from the peer-to-peer database to continually estimate the running result of the approximate continuous aggregate query with guaranteed precision. For efficient query evaluation, we propose an extrapolation algorithm that predicts the evolution of the running result and adapts the frequency of the continual sampling occasions accordingly to avoid redundant samples. We also introduce a repeated sampling algorithm that draws on the correlation between the samples at successive sampling occasions and exploits linear regression to minimize the number of the samples derived at each occasion. At the bottom tier, we introduce a distributed sampling algorithm for random sampling (uniform and nonuniform) from peer-to-peer databases with arbitrary network topology and tuple distribution. Our sampling algorithm is based on the Metropolis Markov Chain Monte Carlo method that guarantees randomness of the sample with arbitrary small variation difference with the desired distribution, while it is comparable to optimal sampling in sampling cost/time. We evaluate the efficiency of Digest via simulation using real data.

#index 1206659
#* Faster Join Enumeration for Complex Queries
#@ Guido Moerkotte;Thomas Neumann
#t 2008
#c 17
#! Most existing join ordering algorithms concentrate on join queries with simple join predicates and inner joins only, where simple predicates are those that involve exactly two relations. However, real queries may contain complex join predicates, i.e. predicates involving more than two relations. We show how to handle complex join predicates efficiently, by modeling the query graph as a hypergraph and reasoning about its connected subgraphs.

#index 1206660
#* Processing Event-Monitoring Queries in Sensor Networks
#@ Vassilis Stoumpos;Antonios Deligiannakis;Yannis Kotidis;Alex Delis
#t 2008
#c 17
#! In this paper we present algorithms for building and maintaining efficient collection trees that provide the conduit to disseminate data required for processing monitoring queries in a wireless sensor network. We introduce and formalize the notion of event monitoring queries and demonstrate that they can capture a large class of monitoring applications. We then show techniques which, using a small set of intuitive statistics, can compute collection trees that minimize important resources such as the number of messages exchanged among the nodes or the overall energy consumption. Our experiments demonstrate that our techniques can organize the data collection process while utilizing significantly lower resources than prior approaches.

#index 1206661
#* Training Linear Discriminant Analysis in Linear Time
#@ Deng Cai;Xiaofei He;Jiawei Han
#t 2008
#c 17
#! Linear Discriminant Analysis (LDA) has been a popular method for extracting features which preserve class separability. It has been widely used in many fields of information processing, such as machine learning, data mining, information retrieval, and pattern recognition. However, the computation of LDA involves dense matrices eigen-decomposition which can be computationally expensive both in time and memory. Specifically, LDA has O(mnt + t3) time complexity and requires O(mn + mt + nt) memory, where m is the number of samples, n is the number of features and t = min (m,n). When both m and n are large, it is infeasible to apply LDA. In this paper, we propose a novel algorithm for discriminant analysis, called Spectral Regression Discriminant Analysis (SRDA). By using spectral graph analysis, SRDA casts discriminant analysis into a regression framework which facilitates both efficient computation and the use of regularization techniques. Our theoretical analysis shows that SRDA can be computed with O(ms) time and O(ms) memory, where s(les n) is the average number of non-zero features in each sample. Extensive experimental results on four real world data sets demonstrate the effectiveness and efficiency of our algorithm.

#index 1206662
#* Efficient Computation of Diverse Query Results
#@ Erik Vee;Utkarsh Srivastava;Jayavel Shanmugasundaram;Prashant Bhat;Sihem Amer Yahia
#t 2008
#c 17
#! We study the problem of efficiently computing diverse query results in online shopping applications, where users specify queries through a form interface that allows a mix of structured and content-based selection conditions. Intuitively, the goal of diverse query answering is to return a representative set of top-k answers from all the tuples that satisfy the user selection condition. For example, if a user is searching for Honda cars and we can only display five results, we wish to return cars from five different Honda models, as opposed to returning cars from only one or two Honda models. A key contribution of this paper is to formally define the notion of diversity, and to show that existing score based techniques commonly used in web applications are not sufficient to guarantee diversity. Another contribution of this paper is to develop novel and efficient query processing techniques that guarantee diversity. Our experimental results using Yahoo! Autos data show that our proposed techniques are scalable and efficient.

#index 1206663
#* Toward Expressive and Scalable Sponsored Search Auctions
#@ David J. Martin;Johannes Gehrke;Joseph Y. Halpern
#t 2008
#c 17
#! Internet search results are a growing and highly profitable advertising platform. Search providers auction advertising slots to advertisers on their search result pages. Due to the high volume of searches and the users' low tolerance for search result latency, it is imperative to resolve these auctions fast. Current approaches restrict the expressiveness of bids in order to achieve fast winner determination, which is the problem of allocating slots to advertisers so as to maximize the expected revenue given the advertisers' bids. The goal of our work is to permit more expressive bidding, thus allowing advertisers to achieve complex advertising goals, while still providing fast and scalable techniques for winner determination.

#index 1206664
#* Lattice Histograms: a Resilient Synopsis Structure
#@ Panagiotis Karras;Nikos Mamoulis
#t 2008
#c 17
#! Despite the surge of interest in data reduction techniques over the past years, no method has been proposed to date that can always achieve approximation quality preferable to that of the optimal plain histogram for a target error metric. In this paper, we introduce the Lattice Histogram: a novel data reduction method that discovers and exploits any arbitrary hierarchy in the data, and achieves approximation quality provably at least as high as an optimal histogram for any data reduction problem. We formulate LH construction techniques with approximation guarantees for general error metrics. We show that the case of minimizing a maximum-error metric can be solved by a specialized, memory-sparing approach; we exploit this solution to design reduced-space heuristics for the general-error case. We develop a mixed synopsis approach, applicable to the space-efficient high-quality summarization of very large data sets. We experimentally corroborate the superiority of LHs in approximation quality over previous techniques with representative error metrics and diverse data sets.

#index 1206665
#* Efficient Merging and Filtering Algorithms for Approximate String Searches
#@ Chen Li;Jiaheng Lu;Yiming Lu
#t 2008
#c 17
#! We study the following problem: how to efficiently find in a collection of strings those similar to a given query string? Various similarity functions can be used, such as edit distance, Jaccard similarity, and cosine similarity. This problem is of great interests to a variety of applications that need a high real-time performance, such as data cleaning, query relaxation, and spellchecking. Several algorithms have been proposed based on the idea of merging inverted lists of grams generated from the strings. In this paper we make two contributions. First, we develop several algorithms that can greatly improve the performance of existing algorithms. Second, we study how to integrate existing filtering techniques with these algorithms, and show that they should be used together judiciously, since the way to do the integration can greatly affect the performance. We have conducted experiments on several real data sets to evaluate the proposed techniques.

#index 1206666
#* The Space Complexity of Processing XML Twig Queries Over Indexed Documents
#@ Mirit Shalem;Ziv Bar-Yossef
#t 2008
#c 17
#! Current twig join algorithms incur high memory costs on queries that involve child-axis nodes. In this paper we provide an analytical explanation for this phenomenon. In a first large-scale study of the space complexity of evaluating XPath queries over indexed XML documents we show the space to depend on three factors: (1) whether the query is a path or a tree; (2) the types of axes occurring in the query and their occurrence pattern; and (3) the mode of query evaluation (filtering, full-fledged, or "pattern matching"). Our lower bounds imply that evaluation of a large class of queries that have child-axis nodes indeed requires large space. Our study also reveals that on some queries there is a large gap between the space needed for pattern matching and the space needed for full-fledged evaluation or filtering. This implies that many existing twig join algorithms, which work in the pattern matching mode, incur significant space overhead. We present a new twig join algorithm that avoids this overhead. On certain queries our algorithm is exceedingly more space-efficient than existing algorithms, sometimes bringing the space down from linear in the document size to constant.

#index 1206667
#* XML Index Recommendation with Tight Optimizer Coupling
#@ Iman Elghandour;Ashraf Aboulnaga;Daniel C. Zilio;Fei Chiang;Andrey Balmin;Kevin Beyer;Calisto Zuzarte
#t 2008
#c 17
#! XML database systems are expected to handle increasingly complex queries over increasingly large and highly structured XML databases. An important problem that needs to be solved for these systems is how to choose the best set of indexes for a given workload. In this paper, we present an XML Index Advisor that solves this XML index recommendation problem and has the key characteristic of being tightly coupled with the query optimizer. We rely on the optimizer to enumerate index candidates and to estimate the benefit gained from potential index configurations. We expand the set of candidate indexes obtained from the query optimizer to include more general indexes that can be useful for queries other than those in the training workload. To recommend an index configuration, we introduce two new search algorithms. The first algorithm finds the best set of indexes for the specific training workload, and the second algorithm finds a general set of indexes that can benefit the training workload as well as other similar workloads. We have implemented our XML Index Advisor in a prototype version of IBM® DB2® 9, which supports both relational and XML data, and we experimentally demonstrate the effectiveness of our advisor using this implementation.

#index 1206668
#* A Motion-Aware Approach to Continuous Retrieval of 3D Objects
#@ Mohammed Eunus Ali;Rui Zhang;Egemen Tanin;Lars Kulik
#t 2008
#c 17
#! With recent advances in mobile computing technologies, mobile devices can now render 3D objects realistically. Many users of these devices such as tourists, mixed-reality gamers, and rescue officers, need real-time retrieval of 3D objects over a wireless network. Due to bandwidth and latency restrictions in mobile settings, efficient continuous retrieval of 3D objects remains a challenge. In this paper, we describe a motion-aware approach to this problem. We first introduce multi-resolution ion storage and retrieval methods for 3D data, which restrict access to only the necessary content based on the client's motion pattern. We then propose a motion-aware buffer management technique as well as an efficient index using multi-resolution representations of objects. Our experiments demonstrate the effectiveness of our solution to continuous retrieval of complex spatial data in mobile settings.

#index 1206669
#* Continuous Content-Based Copy Detection over Streaming Videos
#@ Ying Yan;Beng Chin Ooi;Aoying Zhou
#t 2008
#c 17
#! Digital videos are increasingly adopted in various multimedia applications where they are usually broadcasted or transmitted as video streams. Continuously monitoring copies on the fast and long streaming videos is gaining attention due to its importance in content and rights management. The problem of video copies detection on video streams is complicated by two issues. First, original videos may be edited, with their frames being reordered, to avoid detection. Second, there are many concurrent video streams and for each stream, there could be many continuous video copy monitoring queries. Efficient data stream algorithms are therefore essential for processing a large number of continuous queries on video streams. In this paper, we first define video sequence similarity that is robust with respect to changes of videos, and a hash-based video sketch for efficient computation of sequence similarity. We then present a novel bit vector signature of the sketch to achieve two optimization objectives: CPU cost and memory requirement. Finally, in order to handle multiple continuous queries simultaneously, we design an index structure for the query sequences. We implemented the system and use real videos for the experimental study. Experimental results confirm the efficiency and effectiveness of our proposed techniques.

#index 1206670
#* PermJoin: An Efficient Algorithm for Producing Early Results in Multi-join Query Plans
#@ Justin J. Levandoski;Mohamed E. Khalefa;Mohamed F. Mokbel
#t 2008
#c 17
#! This paper introduces an efficient algorithm for Producing Early Results in Multi-join query plans (PermJoin, for short). While most previous research focuses only on the case of a single join operator, PermJoin takes a radical step by addressing query plans with multiple join operators. PermJoin is optimized to maximize the early overall throughput and to adapt to fluctuations in data arrival rates. PermJoin is a non-blocking operator that is capable of producing join results even if one or more data sources are blocked due to slow or bursty network behavior. Furthermore, PermJoin distinguishes itself from all previous techniques as it: (1) employs a new flushing policy to write in-memory data to disk, once memory allotment is exhausted, in a way that helps increase the probability of producing early result throughput in multi-join queries, and (2) employs a novel state manager module that adaptively switches operators between joining in-memory data and disk-resident data in order to maximize overall throughput.

#index 1206671
#* An Architecture for Query Optimization in Sensor Networks
#@ Ixent Galpin;Christian Y. A. Brenninkmeijer;Farhana Jabeen;Alvaro A. A. Fernandes;Norman W. Paton
#t 2008
#c 17
#! We present a novel sensor network query processing architecture that (a) covers all the query optimization phases that are required to map a declarative query to executable code; and (b) does so for a more expressive query language than has heretofore been supported over sensor networks. The architecture is founded on the view that a sensor network truly is a distributed computing infrastructure, albeit a very constrained one. As such, we address the problem of how to develop a comprehensive optimizer for an expressive declarative continuous query language over acquisitional streams as one of finding extensions to a classical distributed query processing architecture that contend with the peculiarities of sensor networks as an environment for distributed computing.

#index 1206672
#* Optimizing Complex Event Processing over RFID Data Streams
#@ Qun Chen;Zhanhuai Li;Hailong Liu
#t 2008
#c 17
#! One research question crucial to RFID technology's wider adoption is how to efficiently transform sequences of RFID readings into meaningful business events. Contrary to traditional events, RFID readings are usually of high volume and velocity, and have the attributes representing their reading objects, occurrence times and spots. Based on these characteristics and the Non-deterministic Finite Automata(NFA) implementation framework, this paper studies the performance issues of RFID complex event processing and proposes corresponding optimization techniques. Our techniques include : (1) taking advantage of negation events or exclusiveness between events to prune intermediate results, thus reduce memory consumption; (2) with complex events' different selectivities, purposefully reordering the join operations between events to improve overall efficiency, thus achieve higher stream throughput; (3) utilizing the slot-based or B+-tree-based approach to optimize the processing performance with the time window constraint. We present these techniques' analytical results and validate their effectiveness through experiments.

#index 1206673
#* Efficient Data Interpretation and Compression over RFID Streams
#@ Richard Cocci;Thanh Tran;Yanlei Diao;Prashant Shenoy
#t 2008
#c 17
#! Despite its promise, RFID technology presents numerous challenges, including incomplete data, lack of location and containment information, and very high volumes. In this work, we present a novel data interpretation and compression substrate over RFID streams to address these challenges in enterprise supply-chain environments. Our results show that our inference techniques provide good accuracy while retaining efficiency, and our compression algorithm yields significant reduction in data volume.

#index 1206674
#* Outlier-Aware Data Aggregation in Sensor Networks
#@ Antonios Deligiannakis;Vassilis Stoumpos;Yannis Kotidis;Vasilis Vassalos;Alex Delis
#t 2008
#c 17
#! In this paper we discuss a robust aggregation framework that can detect spurious measurements and refrain from incorporating them in the computed aggregate values. Our framework can consider different definitions of an outlier node, based on a specified minimum support. Our experimental evaluation demonstrates the benefits of our approach.

#index 1206675
#* Efficient Query Processing in Large Traffic Networks
#@ Hans-Peter Kriegel;Peer Kroger;Peter Kunath;Matthias Renz;Tim Schmidt
#t 2008
#c 17
#! We present an original graph embedding to speedup distance-range and k-nearest neighbor queries on static and/or dynamic objects located on a (weighted) graph. Our method is used to compute a lower and upper bounding filter distance which approximates the true shortest path distance significantly better than traditional filters. In addition, we discuss how the computation of the exact shortest path distance in the refinement step can be boosted by using the embedded graph.

#index 1206676
#* Optimal-Nearest-Neighbor Queries
#@ Yunjun Gao;Jing Zhang;Gencai Chen;Qing Li;Shen Liu;Chun Chen
#t 2008
#c 17
#! Given two sets DA and DB of multidimensional objects, a spatial region R, and a critical distance dc, an optimal-nearest-neighbor (ONN) query retrieves outside R, the object in DB with maximum optimality. Let CAR (Sp, p) be the cardinality of the subset Sp of objects in DA which locate within R and are enclosed by the vicinity circle centered at p with radius dc. Then, an object o is said to be better than another one o' if (i) CAR (So, o) ≫ CAR (So', o'), or (ii) when CAR (So, o) = CAR (So', o') the sum of the weighted distance from each object in So to o is smaller than the sum of the weighted distance between every object in So' and o'. This type of queries is quite useful in many decision making applications. In this paper, we formalize the ONN query, develop the optimality metric, and propose several algorithms for finding optimal nearest neighbors efficiently. Our techniques assume that both DA and DB are indexed by R-trees. Extensive experiments demonstrate the efficiency and scalability of our proposed algorithms using both real and synthetic datasets.

#index 1206677
#* Fast Indexes and Algorithms for Set Similarity Selection Queries
#@ Marios Hadjieleftheriou;Amit Chandel;Nick Koudas;Divesh Srivastava
#t 2008
#c 17
#! Data collections often have inconsistencies that arise due to a variety of reasons, and it is desirable to be able to identify and resolve them efficiently. Set similarity queries are commonly used in data cleaning for matching similar data. In this work we concentrate on set similarity selection queries: Given a query set, retrieve all sets in a collection with similarity greater than some threshold. Various set similarity measures have been proposed in the past for data cleaning purposes. In this work we concentrate on weighted similarity functions like TF/IDF, and introduce variants that are well suited for set similarity selections in a relational database context. These variants have special semantic properties that can be exploited to design very efficient index structures and algorithms for answering queries efficiently. We present modifications of existing technologies to work for set similarity selection queries. We also introduce three novel algorithms based on the Threshold Algorithm, that exploit the semantic properties of the new similarity measures to achieve the best performance in theory and practice.

#index 1206678
#* Privacy: Theory meets Practice on the Map
#@ Ashwin Machanavajjhala;Daniel Kifer;John Abowd;Johannes Gehrke;Lars Vilhuber
#t 2008
#c 17
#! In this paper, we propose the first formal privacy analysis of a data anonymization process known as the synthetic data generation, a technique becoming popular in the statistics community. The target application for this work is a mapping program that shows the commuting patterns of the population of the United States. The source data for this application were collected by the U.S. Census Bureau, but due to privacy constraints, they cannot be used directly by the mapping program. Instead, we generate synthetic data that statistically mimic the original data while providing privacy guarantees. We use these synthetic data as a surrogate for the original data. We find that while some existing definitions of privacy are inapplicable to our target application, others are too conservative and render the synthetic data useless since they guard against privacy breaches that are very unlikely. Moreover, the data in our target application is sparse, and none of the existing solutions are tailored to anonymize sparse data. In this paper, we propose solutions to address the above issues.

#index 1206679
#* Auditing SQL Queries
#@ Rajeev Motwani;Shubha U. Nabar;Dilys Thomas
#t 2008
#c 17
#! We study the problem of auditing a batch of SQL queries: Given a forbidden view of a database that should have been kept confidential, a batch of queries that were posed over this database and answered, and a definition of suspiciousness, determine if the query batch is suspicious with respect to the forbidden view. We consider several notions of suspiciousness that span a spectrum both in terms of their disclosure detection guarantees and the tractability of auditing under them for different classes of queries. We identify a particular notion of suspiciousness, weak syntactic suspiciousness, that allows for an efficient auditor for a large class of conjunctive queries. The auditor can be used together with a specific set of forbidden views to detect disclosures of the association between individuals and their private attributes. Further it can also be used to prevent disclosures by auditing queries on the fly in an online setting. Finally, we tie in our work with recent research on query auditing and access control and relate the above definitions of suspiciousness to the notion of unconditional validity of a query introduced in database access control literature.

#index 1206680
#* Optimal Boolean Matrix Decomposition: Application to Role Engineering
#@ Haibing Lu;Jaideep Vaidya;Vijayalakshmi Atluri
#t 2008
#c 17
#! A decomposition of a binary matrix into two matrices gives a set of basis vectors and their appropriate combination to form the original matrix. Such decomposition solutions are useful in a number of application domains including text mining, role engineering as well as knowledge discovery. While a binary matrix can be decomposed in several ways, however, certain decompositions better characterize the semantics associated with the original matrix in a succinct but comprehensive way. Indeed, one can find different decompositions optimizing different criteria matching various semantics. In this paper, we first present a number of variants to the optimal Boolean matrix decomposition problem that have pragmatic implications. We then present a unified framework for modeling the optimal binary matrix decomposition and its variants using binary integer programming. Such modeling allows us to directly adopt the huge body of heuristic solutions and tools developed for binary integer programming. Although the proposed solutions are applicable to any domain of interest, for providing more meaningful discussions and results, in this paper, we present the binary matrix decomposition problem in a role engineering context, whose goal is to discover an optimal and correct set of roles from existing permissions, referred to as the role mining problem (RMP). This problem has gained significant interest in recent years as role based access control has become a popular means of enforcing security in databases. We consider several variants of the above basic RMP, including the min-noise RMP, delta-approximate RMP and edge-RMP. Solutions to each of them aid security administrators in specific scenarios. We then model these variants as Boolean matrix decomposition and present efficient heuristics to solve them.

#index 1206681
#* Efficient similarity search using the Earth Mover's Distance for large multimedia databases
#@ Ira Assent;Marc Wichterich;Tobias Meisen;Thomas Seidl
#t 2008
#c 17
#! Multimedia similarity search in large databases requires efficient query processing. The Earth Mover's Distance, introduced in computer vision, is successfully used as a similarity model in a number of small-scale applications. Its computational complexity hindered its adoption in large multimedia databases. We enable directly indexing the Earth Mover's Distance in structures such as the R-tree and the VA-file by providing the accurate `MinDist' function to any bounding rectangle in the index. We exploit the computational structure of the new MinDist to derive a new lower bound for the EMD MinDist which is assembled from quantized partial solutions yielding very fast query processing times. We prove completeness of our approach in a multistep scheme. Extensive experiments on real world data demonstrate the high efficiency.

#index 1206682
#* Continuous Intersection Joins Over Moving Objects
#@ Rui Zhang;Dan Lin;Kotagiri Ramamohanarao;Elisa Bertino
#t 2008
#c 17
#! The continuous intersection join query is computationally expensive yet important for various applications on moving objects. No previous study has specifically addressed this query type. We can adopt a naive algorithm or extend an existing technique (TP-Join) to process the query. However, they compute the answer for either too long or too short a time interval, which results in either a very large computation cost per object update or too frequent answer updates, respectively. This motivates us to optimize the query processing in the time dimension. In this study, we achieve this optimization by introducing the new concept of time-constrained (TC) processing. Further, TC processing enables a set of effective improvement techniques on traditional intersection join algorithms. With a thorough experimental study, we show that our algorithm outperforms the best adapted existing solution by several orders of magnitude.

#index 1206683
#* Multiple Materialized View Selection for XPath Query Rewriting
#@ Nan Tang;Jeffrey Xu Yu;M. Tamer Ozsu;Byron Choi;Kam-Fai Wong
#t 2008
#c 17
#! We study the problem of answering XPATH queries using multiple materialized views. Despite the efforts on answering queries using single materialized view, answering queries using multiple views remains relatively new. We address two important aspects of this problem: multiple-view selection and equivalent multiple-view rewriting. With regards to the first problem, we propose an NFA-based approach (called VFILTER) to filter views that cannot be used to answer a given query. We then present the criterion for multiple view/query answerability. Based on the output of VFILTER, we further propose a heuristic method to identify a minimal view set that can answer a given query. For the problem of multiple-view rewriting, we first refine the materialized fragments of each selected view (like pushing selection), we then join the refined fragments utilizing an encoding scheme. Finally, we extract the result of the query from the materialized fragments of a single view. Experiments show the efficiency of our approach.

#index 1206684
#* Explaining and Reformulating Authority Flow Queries
#@ Ramakrishna Varadarajan;Vagelis Hristidis;Louiqa Raschid
#t 2008
#c 17
#! Authority flow is an effective ranking mechanism for answering queries on a broad class of data. Systems have been developed to apply this principle on the Web (PageRank and topic sensitive PageRank), bibliographic databases (ObjectRank), and biological databases (Hubs of Knowledge project). However, these systems have the following drawbacks: (a) There is no way to explain to the user why a particular result received its current score; (b) The authority flow rates, which have been shown to dramatically affect the results' quality in ObjectRank, have to be set manually by a domain expert; (c) There is no query reformulation methodology to refine the query results according to the user's preferences. In this work, we address these shortcomings by introducing a framework and algorithms to explain query results and reformulate authority flow queries based on the user's feedback. The query reformulation process can be used to learn the user's preferences and automatically adjust the authority flow rates to facilitate personalized authority flow searching. We experimentally evaluate our algorithms in terms of performance and quality.

#index 1206685
#* An Efficient Algorithm for Answering Graph Reachability Queries
#@ Yangjun Chen;Yibin Chen
#t 2008
#c 17
#! Given a directed graph G, to check whether a node v is reachable from another node u through a path is often required. In a database system, such an operation is called a recursion computation or reachability checking and not efficiently supported. The reason for this is that the space to store the whole transitive closure of G is prohibitively high. In this paper, we address this issue and propose an 0(n2 + bnradic(b)) time algorithm to decompose a directed acyclic graph (DAG) into a minimized set of disjoint chains to facilitate reachability checking, where n is the number of the nodes and b is the DAG's width, defined to be the size of a largest node subset U of the DAG such that for every pair of nodes u, v isin U, there does not exist a path from u to v or from v to u. Using this algorithm, we are able to label a graph in 0(be) time and store all the labels in O(bn) space with O(logb) reachability checking time, where e is the number of the edges of the DAG. The method can also be extended to handle cyclic directed graphs. Experiments have been performed, showing that our method is promising.

#index 1206686
#* Summarizing Graph Patterns
#@ Yong Liu;Jianzhong Li;Hong Gao
#t 2008
#c 17
#! Several efficient frequent subgraph mining algorithms have been recently proposed. However, the number of frequent graph patterns generated by these graph mining algorithms may be too large to be effectively explored by users, especially when the support threshold is low. In this paper, we propose to summarize frequent graph patterns by a much smaller number of representative graph patterns. Several novel concepts such as delta-cover graph, jump value and delta-jump pattern are proposed for efficiently summarizing frequent graph patterns. Based on the fact that all delta-jump patterns must be representative graph patterns, we propose two efficient algorithms for summarizing frequent graph patterns, RP-FP and RP-GD. The RP-FP algorithm computes representative graph patterns from a set of closed frequent graph patterns, whereas the RP-GD algorithm directly mines representative graph patterns from graph databases. Experimental results show that RP-FP and RP-GD are able to obtain compact summarization in both real and synthetic graph databases. When the number of closed graph patterns is very large, RP-GD is much more efficient than RP-FP, while achieving comparable summarization quality.

#index 1206687
#* An Algebraic Approach to Rule-Based Information Extraction
#@ Frederick Reiss;Sriram Raghavan;Rajasekar Krishnamurthy;Huaiyu Zhu;Shivakumar Vaithyanathan
#t 2008
#c 17
#! Traditional approaches to rule-based information extraction (IE) have primarily been based on regular expression grammars. However, these grammar-based systems have difficulty scaling to large data sets and large numbers of rules. Inspired by traditional database research, we propose an algebraic approach to rule-based IE that addresses these scalability issues through query optimization. The operators of our algebra are motivated by our experience in building several rule-based extraction programs over diverse data sets. We present the operators of our algebra and propose several optimization strategies motivated by the text-specific characteristics of our operators. Finally we validate the potential benefits of our approach by extensive experiments over real-world blog data.

#index 1206688
#* Convoy Queries in Spatio-Temporal Databases
#@ Hoyoung Jeung;Heng Tao Shen;Xiaofang Zhou
#t 2008
#c 17
#! We introduce a convoy query that retrieves all convoys from historical trajectories, each of which consists of a set of objects that travelled closely during a certain time period. Convoy query is useful for many applications such as carpooling and traffic jam analysis, however, limited work has been done in the database community. This study proposes three efficient methods for discovering convoys. The main novelty of our methods is to approximate original trajectories by using line simplification methods and perform the discovery process over the simplified trajectories with bounded errors. Our experimental results confirm the effectiveness and efficiency of our methods.

#index 1206689
#* On High Dimensional Indexing of Uncertain Data
#@ Charu C. Aggarwal;Philip S. Yu
#t 2008
#c 17
#! In this paper, we will examine the problem of distance function computation and indexing uncertain data in high dimensionality for nearest neighbor and range queries. Because of the inherent noise in uncertain data, traditional distance function measures such as the Lq-metric and their probabilistic variants are not qualitatively effective. This problem is further magnified by the sparsity issue in high dimensionality. In this paper, we examine methods of computing distance functions for high dimensional data which are qualitatively effective and friendly to the use of indexes. In this paper, we show how to construct an effective index structure in order to handle uncertain similarity and range queries in high dimensionality. Typical range queries in high dimensional space use only a subset of the ranges in order to resolve the queries. Furthermore, it is often desirable to run similarity queries with only a subset of the large number of dimensions. Such queries are difficult to resolve with traditional index structures which use the entire set of dimensions. We propose query-processing techniques which use effective search methods on the index in order to compute the final results. We discuss the experimental results on a number of real and synthetic data sets in terms of effectiveness and efficiency. We show that the proposed distance measures are not only more effective than traditional Lq-norms, but can also be computed more efficiently over our proposed index structure.

#index 1206690
#* Pattern Matching over Cloaked Time Series
#@ Xiang Lian;Lei Chen;Jeffrey Xu Yu
#t 2008
#c 17
#! In many privacy preserving applications such as Location-Based Services (LBS), medical data analysis, and data sequence matching, users often deliberately disturb the original data in order to avoid the release of their private information. Although these disturbed cloaked data cannot reveal the privacy information of individual users, they can still help perform some data mining tasks such as data classification. In this paper, we study one important and fundamental query predicate, that is, to find the cloaked time series that are similar to a query pattern. In this paper, we formalize such similarity search problem over the cloaked time series, and propose a novel approach to index the cloaked series, which can facilitate the similarity query.

#index 1206691
#* Satisfying Complex Data Needs using Pull-Based Online Monitoring of Volatile Data Sources
#@ Haggai Roitman;Avigdor Gal;Louiqa Raschid
#t 2008
#c 17
#! Emerging applications on the Web require better management of volatile data in pull-based environments. In a pull based setting, data may be periodically removed from the server. Data may also become obsolete, no longer serving client needs. In both cases, we consider such data to be volatile. To model such constraints on data usability, and support complex user needs we define profiles to specify which data sources are to be monitored and when. Using a novel abstraction of execution intervals we model complex profiles that access simultaneously several servers to gain from the used data. Given some budgetary constraints (e.g., bandwidth), the paper formalizes the problem of maximizing completeness.

#index 1206692
#* Processing Diagnosis Queries: A Principled and Scalable Approach
#@ Shivnath Babu;Songyun Duan;Kamesh Munagala
#t 2008
#c 17
#! Many popular Web sites suffer occasional user-visible problems such as slow responses, blank pages or error messages being displayed, items not being added to shopping carts, database slowdowns, and others. Such deviations of systems from desired behavior, or failures, can cause user dissatisfaction and considerable loss of revenue. The scale, complexity, and dynamics of modern systems make it hard to track down the cause of failures manually. We address this problem through a new class of declarative queries, called diagnosis queries, that a system administrator or user can pose to pinpoint the cause of a failure. We describe how diagnosis queries are specified over system-monitoring data, and the challenges faced by current techniques to process these queries. We develop and evaluate a new algorithm, based on a combination of clustering and classification, to process diagnosis queries automatically, efficiently, and with good accuracy.

#index 1206693
#* Capturing Approximated Data Delivery Tradeoffs
#@ Haggai Roitman;Avigdor Gal;Louiqa Raschid
#t 2008
#c 17
#! This paper presents a middleware data delivery setting with a proxy that is required to maximize the completeness of captured updates, specified in its clients' profiles, while minimizing at the same time the delay in delivering the updates to clients. The two objectives may conflict when the monitoring budget is limited. Therefore, any solution should consider this tradeoff in satisfying both objectives. We term this problem the "proxy dilemma" and formalize it as a biobjective optimization problem. Such problem occurs in many contemporary applications, such as mobile and sensor networks, and poses scalability challenges in delivering up-to-date data from remote resources to meet client specifications. We present a Pareto set as a formal solution to the proxy dilemma. We discuss the complexity of generating a Pareto set for the proxy dilemma and suggest an approximation scheme to this problem.

#index 1206694
#* Similarity Search in Arbitrary Subspaces Under Lp-Norm
#@ Xiang Lian;Lei Chen
#t 2008
#c 17
#! Similarity search has been widely used in many applications such as information retrieval, image data analysis, and time-series matching. Specifically, a similarity query retrieves all data objects in a data set that are similar to a given query object. Previous work on similarity search usually consider the search problem in the full space. In this paper, however, we propose a novel problem, subspace similarity search, which finds all data objects that match with a query object in the subspace instead of the original full space. In particular, the query object can specify arbitrary subspace with arbitrary number of dimensions. Since traditional approaches for similarity search cannot be applied to solve the proposed problem, we introduce an efficient and effective pruning technique, which assigns scores to data objects with respect to pivots and prunes candidates via scores. We propose an effective multipivot-based method to pre-process data objects by selecting appropriate pivots, where the entire procedure is guided by a formal cost model, such that the pruning power is maximized. Finally, scores of each data object are organized in sorted list to facilitate an efficient subspace similarity search. Extensive experiments have verified the correctness of our cost model and demonstrated the efficiency and effectiveness of our proposed approach for the subspace similarity search.

#index 1206695
#* Nearest Neighbor Retrieval Using Distance-Based Hashing
#@ Vassilis Athitsos;Michalis Potamias;Panagiotis Papapetrou;George Kollios
#t 2008
#c 17
#! A method is proposed for indexing spaces with arbitrary distance measures, so as to achieve efficient approximate nearest neighbor retrieval. Hashing methods, such as Locality Sensitive Hashing (LSH), have been successfully applied for similarity indexing in vector spaces and string spaces under the Hamming distance. The key novelty of the hashing technique proposed here is that it can be applied to spaces with arbitrary distance measures, including non-metric distance measures. First, we describe a domain-independent method for constructing a family of binary hash functions. Then, we use these functions to construct multiple multibit hash tables. We show that the LSH formalism is not applicable for analyzing the behavior of these tables as index structures. We present a novel formulation, that uses statistical observations from sample data to analyze retrieval accuracy and efficiency for the proposed indexing method. Experiments on several real-world data sets demonstrate that our method produces good trade-offs between accuracy and efficiency, and significantly outperforms VP-trees, which are a well-known method for distance-based indexing.

#index 1206696
#* On Monitoring the top-k Unsafe Places
#@ Donghui Zhang;Yang Du;Ling Hu
#t 2008
#c 17
#! In a city, protecting units like police cars move around and protect places such as banks and residential buildings. Different places may have different requirements in how many protecting units should be nearby. If any place has less protecting units around than it requires, it is an unsafe place. This paper studies the Continuous Top-k Unsafe Places (CTUP) query, which continuously monitors the k least safe places while the protecting units keep sending their location updates to the server. The CTUP query is a novel addition to the family of continuous location-based queries, an emerging area due to the recent advances in dynamic location-aware environments. Solutions to existing continuous location-based queries and to the traditional top-k queries do not apply. This paper proposes two solutions to this new query, the BasicCTUP scheme and the OptCTUP scheme. Experiments are conducted to evaluate the proposed solutions.

#index 1206697
#* Compact Similarity Joins
#@ Brent Bryan;Frederick Eberhardt;Christos Faloutsos
#t 2008
#c 17
#! Similarity joins have attracted significant interest, with applications in Geographical Information Systems, astronomy, marketing analyzes, and anomaly detection. However, all the past algorithms, although highly fine-tuned, suffer an output explosion if the query range is even moderately large relative to the local data density. Under such circumstances, the response time and the search effort are both almost quadratic in the database size, which is often prohibitive. We solve this problem by providing two algorithms that find a compact representation of the similarity join result, while retaining all the information in the standard join. Our algorithms have the following characteristics: (a) they are at least as fast as the standard similarity join algorithm, and typically much faster, (b) they generate significantly smaller output, (c) they provably lose no information, (d) they scale well to large data sets, and (e) they can be applied to any of the standard tree data structures. Experiments on real and realistic point-sets show that our algorithms are up to several orders of magnitude faster.

#index 1206698
#* Standing Out in a Crowd: Selecting Attributes for Maximum Visibility
#@ Muhammed Miah;Gautam Das;Vagelis Hristidis;Heikki Mannila
#t 2008
#c 17
#! In recent years, there has been significant interest in development of ranking functions and efficient top-k retrieval algorithms to help users in ad-hoc search and retrieval in databases (e.g., buyers searching for products in a catalog). In this paper we focus on a novel and complementary problem: how to guide a seller in selecting the best attributes of a new tuple (e.g., new product) to highlight such that it stands out in the crowd of existing competitive products and is widely visible to the pool of potential buyers. We develop several interesting formulations of this problem. Although these problems are NP-complete, we can give several exact algorithms as well as approximation heuristics that work well in practice. Our exact algorithms are based on Integer Programming (IP) formulations of the problems, as well as on adaptations of maximal frequent itemset mining algorithms, while our approximation algorithms are based on greedy heuristics. We conduct a performance study illustrating the benefits of our methods on real as well as synthetic data.

#index 1206699
#* Fast Graph Pattern Matching
#@ Jiefeng Cheng;Jeffrey Xu Yu;Bolin Ding;Philip S. Yu;Haixun Wang
#t 2008
#c 17
#! Due to rapid growth of the Internet technology and new scientific/technological advances, the number of applications that model data as graphs increases, because graphs have high expressive power to model complicated structures. The dominance of graphs in real-world applications asks for new graph data management so that users can access graph data effectively and efficiently. In this paper, we study a graph pattern matching problem over a large data graph. The problem is to find all patterns in a large data graph that match a user-given graph pattern. We propose a new two-step R-join (reachability join) algorithm with filter step and fetch step based on a cluster-based join-index with graph codes. We consider the filter step as an R-semijoin, and propose a new optimization approach by interleaving R-joins with R-semijoins. We conducted extensive performance studies, and confirm the efficiency of our proposed new approaches.

#index 1206700
#* Stop Chasing Trends: Discovering High Order Models in Evolving Data
#@ Shixi Chen;Haixun Wang;Shuigeng Zhou;Philip S. Yu
#t 2008
#c 17
#! Many applications are driven by evolving data - patterns in Web traffic, program execution traces, network event logs, etc., are often non-stationary. Building prediction models for evolving data becomes an important and challenging task. Currently, most approaches work by "chasing trends", that is, they keep learning or updating models from the evolving data, and use these impromptu models for online prediction. In many cases, this proves to be both costly and ineffective - much time is wasted on re-learning recurring concepts, yet the classifier may remain one step behind the current trend all the time. In this paper, we propose to mine high-order models in evolving data. More often than not, there are a limited number of concepts, or stable distributions, in the data stream, and concepts switch between each other constantly. We mine all such concepts offline from a historical stream, and build high quality models for each of them. At run time, combining historical concept change patterns and cues provided by an online training stream, we find the most likely current concept and use its corresponding models to classify data in an unlabeled stream. The primary advantage of the high-order model approach is its high accuracy. Experiments show that in benchmark datasets, classification error of the high-order model is only a small fraction of that of the current best approaches. Another important benefit is that, unlike state-of-the-art approaches, our approach does not require users to tune any parameters to achieve a satisfying result on streams of different characteristics.

#index 1206701
#* Efficient Information Extraction over Evolving Text Data
#@ Fei Chen;AnHai Doan;Jun Yang;Raghu Ramakrishnan
#t 2008
#c 17
#! Most current information extraction (IE) approaches have considered only static text corpora, over which we typically have to apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and to keep extracted information up to date, we often must apply IE repeatedly, to consecutive corpus snapshots. We describe Cyclex, an approach that efficiently executes such repeated IE, by recycling previous IE efforts. Specifically, given a current corpus snapshot U, Cyclex identifies text portions of U that also appear in the previous corpus snapshot V. Since Cyclex has already executed IE over V, it can now recycle the IE results of these parts, by combining these results with the results of executing IE over the remaining parts of U, to produce the complete IE results for U. Realizing Cyclex raises many challenges, including modeling information extractors, exploring the trade-off between runtime and completeness in identifying overlapping text, and making informed, cost-based decisions between redoing IE from scratch and recycling previous IE results. We describe initial solutions to these challenges, and experiments over two real-world data sets that demonstrate the utility of our approach.

#index 1206702
#* NAGA: Searching and Ranking Knowledge
#@ Gjergji Kasneci;Fabian M. Suchanek;Georgiana Ifrim;Maya Ramanath;Gerhard Weikum
#t 2008
#c 17
#! The Web has the potential to become the world's largest knowledge base. In order to unleash this potential, the wealth of information available on the Web needs to be extracted and organized. There is a need for new querying techniques that are simple and yet more expressive than those provided by standard keyword-based search engines. Searching for knowledge rather than Web pages needs to consider inherent semantic structures like entities (person, organization, etc.) and relationships (isA, locatedIn, etc.). In this paper, we propose NAGA, a new semantic search engine. NAGA builds on a knowledge base, which is organized as a graph with typed edges, and consists of millions of entities and relationships extracted from Web-based corpora. A graph-based query language enables the formulation of queries with additional semantic information. We introduce a novel scoring model, based on the principles of generative language models, which formalizes several notions such as confidence, informativeness and compactness and uses them to rank query results. We demonstrate NAGA's superior result quality over state-of-the-art search engines and question answering systems.

#index 1206703
#* TALE: A Tool for Approximate Large Graph Matching
#@ Yuanyuan Tian;Jignesh M. Patel
#t 2008
#c 17
#! Large graph datasets are common in many emerging database applications, and most notably in large-scale scientific applications. To fully exploit the wealth of information encoded in graphs, effective and efficient graph matching tools are critical. Due to the noisy and incomplete nature of real graph datasets, approximate, rather than exact, graph matching is required. Furthermore, many modern applications need to query large graphs, each of which has hundreds to thousands of nodes and edges. This paper presents a novel technique for approximate matching of large graph queries. We propose a novel indexing method that incorporates graph structural information in a hybrid index structure. This indexing technique achieves high pruning power and the index size scales linearly with the database size. In addition, we propose an innovative matching paradigm to query large graphs. This technique distinguishes nodes by their importance in the graph structure. The matching algorithm first matches the important nodes of a query and then progressively extends these matches. Through experiments on several real datasets, this paper demonstrates the effectiveness and efficiency of the proposed method.

#index 1206704
#* Skippy: Enabling Long-Lived Snapshots of the Long-Lived Past
#@ Ross Shaull;Liuba Shrira;Hao Xu
#t 2008
#c 17
#! Decreasing disk costs have made it practical to retain long-lived snapshots, enabling new applications that analyze past states and infer about future states. Current approaches offer no satisfactory way to organize long-lived snapshots because they disrupt the database in either short or long run. Split snapshots are a recent approach that overcomes some of the limitations. An unsolved problem has been how to support efficient application code access to arbitrarily long-lived snapshots. We describe Skippy, a new approach that solves this problem. Performance evaluation of Skippy, based on theoretical analysis and experimental measurements, indicates that the new approach is effective and efficient.

#index 1206705
#* Adjourn State Concurrency Control Avoiding Time-Out Problems in Atomic Commit Protocols
#@ Sebastian Obermeier;Stefan Bottcher;Martin Hett;Panos K. Chrysanthis;George Samaras
#t 2008
#c 17
#! The use of atomic commit protocols in mobile adhoc networks involves difficulties in setting up reasonable time-outs for aborting a pending distributed transaction. This paper presents the non-blocking Adjourn State, a concurrency control modification which makes time-outs in an atomic commit protocol for aborting a transaction unnecessary. Further, it enhances concurrency among transactions performing conflicting accesses to resources used by completed distributed transactions waiting for the commit protocol to be initiated.

#index 1206706
#* Probabilistic Event Extraction from RFID Data
#@ Nodira Khoussainova;Magdalena Balazinska;Dan Suciu
#t 2008
#c 17
#! We present PEEX, a system that enables applications to define and extract meaningful probabilistic high-level events from RFID data. PEEX effectively copes with errors in the data and the inherent ambiguity of event extraction.

#index 1206707
#* Mobile Filter: Exploring Migration of Filters for Error-Bounded Data Collection in Sensor Networks
#@ Dan Wang;Jianliang Xu;Jiangchuan Liu;Feng Wang
#t 2008
#c 17
#! In wireless sensor networks, filters, which suppress data update reports within predefined error bounds, effectively reduce the traffic volume for continuous data collection. All prior filter designs, however, are stationary in the sense that each filter is attached to a specific sensor node and remains stationary over its lifetime. In this paper, we propose mobile filter, a novel design that explores migration of filters to maximize overall traffic reduction. A mobile filter moves upstream along the data collection path, with its residual size being updated according to the collected data. Intuitively, this migration extracts and relays unused filters, leading to more proactive suppressing of update reports. While extra communications are needed to move filters, we show through probabilistic analysis that the overhead is outrun by the gain from suppressing more data updates.

#index 1206708
#* PageChaser: A Tool for the Automatic Correction of Broken Web Links
#@ Atsuyuki Morishima;Akiyoshi Nakamizo;Toshinari Iida;Shigeo Sugimoto;Hiroyuki Kitagawa
#t 2008
#c 17
#! PageChaser is a system that monitors links between Web pages and searches for the new locations of moved Web pages when it finds broken links. The problem of searching for moved pages is different from typical information retrieval problems. First, it is impossible to identify the final destination until the page is actually moved, so the index-server approach is not necessarily effective. Secondly, there is a large bias about where the new address is likely to be and crawler-based solutions can be effectively implemented, avoiding the need to search the entire Web. PageChaser incorporates a comprehensive set of heuristics, some of which are novel, in a single unified framework. This paper explains the underlying ideas behind the design and development of PageChaser.

#index 1206709
#* Index Design for Dynamic Personalized PageRank
#@ Amit Pathak;Soumen Chakrabarti;Manish Gupta
#t 2008
#c 17
#! Personalized page rank, related to random walks with restarts and conductance in resistive networks, is a frequent search paradigm for graph-structured databases. While efficient batch algorithms exist for static whole-graph page rank, interactive query-time personalized page rank has proved more challenging. Here we describe how to select and build indices for a popular class of page rank algorithms, so as to provide real-time personalized page rank and smoothly trade off between index size, preprocessing time, and query speed. We achieve this by developing a precise, yet efficiently estimated performance model for personalized page rank query execution. We use this model in conjunction with a query workload in a cost-benefit type index optimizer. On millions of queries from CiteSeer and its data graphs with 74-320 thousand nodes, our algorithm runs 50-400 x faster than whole-graph page rank, the gap growing with graph size. Index size is 10-20% of a text index. Ranking accuracy is above 94%.

#index 1206710
#* Quality-Aware Retrieval of Data Objects from Autonomous Sources for Web-Based Repositories
#@ Houtan Shirani-Mehr;Chen Li;Gang Liang;Michal Shmueli-Scheuer
#t 2008
#c 17

#index 1206711
#* Efficient Discovery of Authoritative Resources
#@ Ravi Kumar;Kevin Lang;Cameron Marlow;Andrew Tomkins
#t 2008
#c 17
#! Given a dynamic corpus whose content and attention are changing on a daily basis, is it possible to collect and maintain the high-quality resources with a minimal investment? We address two problems that arise from this question for hyperlinked corpora such as web pages or blogs: how to efficiently discover the correct set of authoritative resources given a fixed network, and how to track these resources over time as new entrants arrive, old standbys depart, and existing participants change roles.

#index 1206712
#* SpaceTwist: Managing the Trade-Offs Among Location Privacy, Query Performance, and Query Accuracy in Mobile Services
#@ Man Lung Yiu;Christian S. Jensen;Xuegang Huang;Hua Lu
#t 2008
#c 17
#! In a mobile service scenario, users query a server for nearby points of interest but they may not want to disclose their locations to the service. Intuitively, location privacy may be obtained at the cost of query performance and query accuracy. The challenge addressed is how to obtain the best possible performance, subjected to given requirements for location privacy and query accuracy. Existing privacy solutions that use spatial cloaking employ complex server query processing techniques and entail the transmission of large quantities of intermediate result. Solutions that use transformation-based matching generally fall short in offering practical query accuracy guarantees. Our proposed framework, called SpaceTwist, rectifies these shortcomings for k nearest neighbor (kNN) queries. Starting with a location different from the user's actual location, nearest neighbors are retrieved incrementally until the query is answered correctly by the mobile terminal. This approach is flexible, needs no trusted middleware, and requires only well-known incremental NN query processing on the server. The framework also includes a server-side granular search technique that exploits relaxed query accuracy guarantees for obtaining better performance. The paper reports on empirical studies that elicit key properties of SpaceTwist and suggest that the framework offers very good performance and high privacy, at low communication cost.

#index 1206713
#* Never Walk Alone: Uncertainty for Anonymity in Moving Objects Databases
#@ Osman Abul;Francesco Bonchi;Mirco Nanni
#t 2008
#c 17
#! Preserving individual privacy when publishing data is a problem that is receiving increasing attention. According to the fc-anonymity principle, each release of data must be such that each individual is indistinguishable from at least k - 1 other individuals. In this paper we study the problem of anonymity preserving data publishing in moving objects databases. We propose a novel concept of k-anonymity based on co-localization that exploits the inherent uncertainty of the moving object's whereabouts. Due to sampling and positioning systems (e.g., GPS) imprecision, the trajectory of a moving object is no longer a polyline in a three-dimensional space, instead it is a cylindrical volume, where its radius delta represents the possible location imprecision: we know that the trajectory of the moving object is within this cylinder, but we do not know exactly where. If another object moves within the same cylinder they are indistinguishable from each other. This leads to the definition of (k,delta) -anonymity for moving objects databases. We first characterize the (k, delta)-anonymity problem and discuss techniques to solve it. Then we focus on the most promising technique by the point of view of information preservation, namely space translation. We develop a suitable measure of the information distortion introduced by space translation, and we prove that the problem of achieving (k,delta) -anonymity by space translation with minimum distortion is NP-hard. Faced with the hardness of our problem we propose a greedy algorithm based on clustering and enhanced with ad hoc pre-processing and outlier removal techniques. The resulting method, named NWA (Never Walk .Alone), is empirically evaluated in terms of data quality and efficiency. Data quality is assessed both by means of objective measures of information distortion, and by comparing the results of the same spatio-temporal range queries executed on the original database and on the (k, delta)-anonymized one. Experimental - results show that for a wide range of values of delta and k, the relative error introduced is kept low, confirming that NWA produces high quality (k, delta)-anonymized data.

#index 1206714
#* On Unifying Privacy and Uncertain Data Models
#@ Charu C. Aggarwal
#t 2008
#c 17
#! The problem of privacy-preserving data mining has been studied extensively in recent years because of the increased amount of personal information which is available to corporations and individuals. Most privacy transformations use some form of data perturbation or representational ambiguity in order to reduce the risk of identification. The final results from privacy transformation methods often require the underlying applications to be modified in order to work with the new representation of the data. Since the end results of privacy-transformation methods have not been standardized, the required modifications may vary with the method used for the privacy transformation. In some cases, it can be an enormous effort to re-design applications to work with the anonymized data. While the results of privacy-transformation methods are a natural form of uncertain data, the two problems have generally been studied independently. In this paper, we make a first attempt to unify the two fields, and propose a privacy transformation for which existing uncertain data management tools can be directly used. This is a great advantage, since it means that the wide spectrum of research available for uncertain data management can also be used for privacy-preserving data mining. We propose an uncertain version of the k-anonymity model which is related to the well known deterministic model of k-anonymity. The uncertain version of the k-anonymity model has the additional feature of introducing greater uncertainty for the adversary over an equivalent deterministic model.

#index 1206715
#* Efficient Construction of Compact Shedding Filters for Data Stream Processing
#@ Bugra Gedik;Kun-Lung Wu;Philip S. Yu
#t 2008
#c 17
#! High-volume source streams, coupled with fluctuating rates, necessitate adaptive load shedding in data stream processing. When ignored, a continual query (CQ) server may randomly drop items, when its capacity is inadequate to handle the arriving data, and degrade the quality of the query results. To alleviate this problem, filters can be used at the source nodes. However, regular source filtering in itself is not sufficient to prevent random dropping, because the amount of data passing through the filters can still surpass the server's capacity. In this case, intelligent load shedding can be applied by the source filters to minimize the degradation in result quality. In this paper, we introduce a novel type of load-shedding source filters, called Non-uniformly Regulated (NR) sifters. An NR sifter judiciously applies varying amounts of load shedding to different regions of the data space within the sifter. We formulate the problem of constructing NR sifters as an optimization one. NR sifters are compact and quickly configurable, allowing frequent adaptations, and provide fast lookup for deciding if a data item should be dropped. We structure NR sifters as a set of (sifter region, drop threshold) pairs to achieve compactness, develop query consolidation techniques to enable quick construction, and introduce flexible space partitioning mechanisms to realize fast lookup.

#index 1206716
#* Probabilistic Verifiers: Evaluating Constrained Nearest-Neighbor Queries over Uncertain Data
#@ Reynold Cheng;Jinchuan Chen;Mohamed Mokbel;Chi-Yin Chow
#t 2008
#c 17
#! In applications like location-based services, sensor monitoring and biological databases, the values of the database items are inherently uncertain in nature. An important query for uncertain objects is the Probabilistic Nearest-Neighbor Query (PNN), which computes the probability of each object for being the nearest neighbor of a query point. Evaluating this query is computationally expensive, since it needs to consider the relationship among uncertain objects, and requires the use of numerical integration or Monte-Carlo methods. Sometimes, a query user may not be concerned about the exact probability values. For example, he may only need answers that have sufficiently high confidence. We thus propose the Constrained Nearest-Neighbor Query (C-PNN), which returns the IDs of objects whose probabilities are higher than some threshold, with a given error bound in the answers. The C-PNN can be answered efficiently with probabilistic verifiers. These are methods that derive the lower and upper bounds of answer probabilities, so that an object can be quickly decided on whether it should be included in the answer. We have developed three probabilistic verifiers, which can be used on uncertain data with arbitrary probability density functions. Extensive experiments were performed to examine the effectiveness of these approaches.

#index 1206717
#* Fast and Simple Relational Processing of Uncertain Data
#@ Lyublena Antova;Thomas Jansen;Christoph Koch;Dan Olteanu
#t 2008
#c 17
#! This paper introduces U-relations, a succinct and purely relational representation system for uncertain databases. U-relations support attribute-level uncertainty using vertical partitioning. If we consider positive relational algebra extended by an operation for computing possible answers, a query on the logical level can be translated into, and evaluated as, a single relational algebra query on the U-relational representation. The translation scheme essentially preserves the size of the query in terms of number of operations and, in particular, number of joins. Standard techniques employed in off-the-shelf relational database management systems are effective for optimizing and processing queries on U-relations. In our experiments we show that query evaluation on U-relations scales to large amounts of data with high degrees of uncertainty.

#index 1206718
#* Sharoes: A Data Sharing Platform for Outsourced Enterprise Storage Environments
#@ Aameek Singh;Ling Liu
#t 2008
#c 17
#! With fast paced growth of digital data and exploding storage management costs, enterprises are looking for new ways to effectively manage their data. One such cost-effective paradigm is the Storage-as-a-Service model, in which enterprises outsource their storage to a storage service provider (SSP) by storing data at a remote SSP-managed site and accessing it over a high speed network. Often for a variety of reasons, enterprises find it unacceptable to fully trust the SSP and prefer to store data in an encrypted form. This typically limits collaboration and data sharing among enterprise users due to complex key management and access control challenges. In this paper, we propose a platform called SHAROES that provides data sharing capability over such outsourced storage environments. SHAROES provide rich *nix-like data sharing semantics over SSP stored data, without trusting the SSP for data confidentiality or access control. SHAROES is unique in its ability in reducing user involvement during setup and operation through the use of in-band key management and allows a near-seamless transition of existing storage environments to the new model. It is also superior in performance by minimizing the use of expensive public-key cryptography in metadata management. We present the architecture and implementation of various SHAROES components and our experiments demonstrate performance superior to other proposals by over 40% on a number of benchmarks.

#index 1206719
#* Accelerating Lookups in P2P Systems using Peer Caching
#@ Supratim Deb;Prakash Linga;Rajeev Rastogi;Anand Srinivasan
#t 2008
#c 17
#! Many structured peer-to-peer (P2P) systems have been proposed as distributed hash tables (DHTs) for fast and efficient lookup of queries. In this paper, we propose a novel technique for improving average lookup times in P2P systems by caching additional neighbor pointers based on peer access frequencies. In particular, we address the problem of each peer choosing the k best pointers to store (in addition to its index pointers) to minimize the average query lookup times. We focus on two popular P2P systems, namely Pastry and Chord: we exploit the inherent structure of these systems to develop efficient, scalable algorithms for optimally choosing the k additional pointers. Simulations with Chord and Pastry demonstrate that our algorithms are very effective in reducing the lookup times significantly. Our approach can be used in tandem with other techniques such as item caching and replication, and is particularly useful for applications such as name services in mobile environments or location services, where we can expect a low churn rate for peers and a relatively higher churn rate for items.

#index 1206720
#* Diagnosing Estimation Errors in Page Counts Using Execution Feedback
#@ Surajit Chaudhuri;Vivek Narasayya;Ravishankar Ramamurthy
#t 2008
#c 17
#! Errors in estimating page counts can lead to poor choice of access methods and in turn to poor quality plans. Although there is past work in using execution feedback for accurate cardinality estimation, the problem of inaccurate estimation of page counts has not been addressed. In this paper, we present novel mechanisms for diagnosing errors in page count by monitoring query execution at low overhead. Detection of inaccuracy in the optimizer estimates of page count can be leveraged by database administrators to improve plan quality. We have prototyped our techniques in the Microsoft SQL Server engine, and our experiments demonstrate the ability to estimate page counts accurately using execution feedback with low overhead. For queries on several real world databases, we observe significant improvement in plan quality when page counts obtained from execution feedback are used instead of the traditional optimizer estimations.

#index 1206721
#* Automatically Extracting Form Labels
#@ Hoa Nguyen;Eun Yong Kang;Juliana Freire
#t 2008
#c 17
#! We describe a machine-learning-based approach for extracting attribute labels from Web form interfaces. Having these labels is a requirement for several techniques that attempt to retrieve and integrate data that reside in online databases and that are hidden behind form interfaces, including schema matching and clustering, and hidden-Web crawlers. Whereas previous approaches to this problem have relied on heuristics and manually specified extraction rules, our technique makes use of learning classifiers to identify form labels. Our preliminary experiments show this approach is promising and has high accuracy.

#index 1206722
#* Answering Keyword Queries on XML Using Materialized Views
#@ Ziyang Liu;Yi Chen
#t 2008
#c 17
#! Answering queries using materialized views has been well studied in the context of structured queries and has shown significant performance benefits. Despite the popularity of keyword search over XML data, it is an open problem whether materialized views can be leveraged for query evaluation. In this paper, we investigate this problem and present techniques for answering keyword queries using a minimal number of materialized views. Experimental evaluation demonstrates the efficiency of the proposed techniques.

#index 1206723
#* An Inflationary Fixed Point Operator in XQuery
#@ Loredana Afanasiev;Torsten Grust;Maarten Marx;Jan Rittinger;Jens Teubner
#t 2008
#c 17
#! We introduce a controlled form of recursion in XQuery, an inflationary fixed point operator, familiar from the context of relational databases. This operator imposes restrictions on the expressible types of recursion, but we show that it is sufficiently versatile to capture a wide range of interesting use cases, including Regular XPath and its core transitive closure operator. While the optimization of general user-defined recursive functions in XQuery appears elusive, we describe how inflationary fixed points can be efficiently evaluated, provided that the recursive XQuery expressions are distributive. We test distributivity syntactically and algebraically, and provide experimental evidence that XQuery processors can benefit substantially from this mode of evaluation.

#index 1206724
#* Grouping and Optimization of XPath Expressions in System RX
#@ Andrey Balmin;Fatma Ozcan;Ashutosh Singh;Edison Ting
#t 2008
#c 17
#! Several XML DBMS support XQuery and/or SQL/XML languages, which are based on navigational primitives in the form of XPath expressions. Typically, these systems either model each XPath step as a separate query plan operator, or employ holistic approaches that can evaluate multiple steps of a single XPath expression. There have also been proposals to execute as many XPath expressions as possible within a single FLWOR block simultaneously in a data streaming context. We observe in our System-RX prototype that blindly combining all possible XPath expressions for concurrent execution can result in significant performance degradation. We identify two main problems. First, the simple strategy of grouping all XPath expressions on a single document does not always work if the query involves more than one data source or has nested query blocks. Second, merging XPath expressions may result in unnecessary execution of branches that can be filtered by predicates in other branches or elsewhere in the query. To rectify these problems, we develop a combination of heuristic-based rewrite transformations, to decide which XPath expressions should be grouped for concurrent evaluation, and cost-based optimization to globally order the groups within the query execution plan, and locally order the branches within individual groups. Experimental evaluation confirms that selectively grouping multiple XPath expressions allows for better query evaluation performance and reduces the query optimization complexity.

#index 1206725
#* Hierarchical Indexing Approach to Support XPath Queries
#@ Nan Tang;Jeffrey Xu Yu;M. Tamer Ozsu;Kam-Fai Wong
#t 2008
#c 17
#! We study new hierarchical indexing approach to process XPATH queries. Here, a hierarchical index consists of index entries that are pairs of queries and their (full/partial) answers (called extents). With such an index, XPATH queries can be processed to extract the results if they match the queries maintained in those index entries. Existing XML path indexing approaches support either child-axis (/) only, or additional descendant-or-self-axis (//) but only in the query root. Different from them, we propose a novel indexing approach to process a large fragment of XPATH queries, which may use /, //, and wildcards (*). The key issues are how to reduce the number of index entries and how to maintain non-overlapping extents among index entries. We show how to compress such index and how to evaluate XPATH queries on it. Experiments show the efficiency of our approaches.

#index 1206726
#* A Query Processing Architecture for an XML Data Warehouse
#@ Nuwee Wiwatwattana;H. V. Jagadish
#t 2008
#c 17
#! Data warehousing accounts for a significant fraction of database use today. As XML becomes ever more popular, more and more XML data finds its way into data warehouse repositories. This paper examines the modeling mismatch between the tree structure of XML data model and the multidimensional model of a typical data warehouse, and proposes an XML warehouse model based on the Multi-Colored Trees (MCT) logical data model that resolves the modeling issue naturally. Furthermore, this data model ameliorates some well-known modeling limitations of the XML data warehouse. To cope with ad-hoc OLAP queries, we extend bitmap join indices to the XML context. We then tackle the difference between the bit-map and the stack-based structural join processing paradigm popular in XML query processing, permitting both styles of query processing to be used seamlessly in consort to evaluate queries. We demonstrate experimentally the benefit of bitmap join indices for typical queries, and particularly those with low cardinality or high selectivity.

#index 1206727
#* A Security Punctuation Framework for Enforcing Access Control on Streaming Data
#@ Rimma V. Nehme;Elke A. Rundensteiner;Elisa Bertino
#t 2008
#c 17
#! The management of privacy and security in the context of data stream management systems (DSMS) remains largely an unaddressed problem to date. Unlike in traditional DBMSs where access control policies are persistently stored on the server and tend to remain stable, in streaming applications the contexts and with them the access control policies on the real-time data may rapidly change. A person entering a casino may want to immediately block others from knowing his current whereabouts. We thus propose a novel "stream-centric" approach, where security restrictions are not persistently stored on the DSMS server, but rather streamed together with the data. Here, the access control policies are expressed via security constraints (called security punctuations, or short, sps) and are embedded into data streams. The advantages of the sp model include flexibility, dynamicity and speed of enforcement. DSMSs can adapt to not only data-related but also security-related selectivities, which helps reduce the waste of resources, when few subjects have access to data. We propose a security-aware query algebra and new equivalence rules together with cost estimations to guide the security-aware query plan optimization. We have implemented the sp framework in a real DSMS. Our experimental results show the validity and the performance advantages of our sp model as compared to alternative access control enforcement solutions for DSMSs.

#index 1206728
#* Randomized Synopses for Query Assurance on Data Streams
#@ Ke Yi;Feifei Li;Marios Hadjieleftheriou;George Kollios;Divesh Srivastava
#t 2008
#c 17
#! The overwhelming flow of information in many data stream applications forces many companies to outsource to a third-party the deployment of a Data Stream Management System (DSMS) for performing desired computations. Remote computations intrinsically raise issues of trust, making query execution assurance on data streams a problem with practical implications. Consider a client observing the same data stream as a remote server (e.g., network traffic), that registers a continuous query on the server's DSMS, and receives answers upon request. The client needs to verify the integrity of the results using significantly fewer resources than evaluating the query locally. Towards that goal, we propose a probabilistic algorithm for selection and aggregate/group-by queries, that uses constant space irrespective of the result-set size, has low update cost, and arbitrarily small probability of failure. We generalize this algorithm to allow some tolerance on the number of errors permitted (irrespective of error magnitude), and also discuss the hardness of permitting arbitrary errors of small magnitude. We also perform an empirical evaluation using live network traffic.

#index 1206729
#* LOCUST: An Online Analytical Processing Framework for High Dimensional Classification of Data Streams
#@ Charu C. Aggarwal;Philip S. Yu
#t 2008
#c 17
#! In recent years, data streams have become ubiquitous because of advances in hardware and software technology. The ability to adapt conventional mining problems to data streams is a great challenge in a data stream environment. Many data streams are inherently high dimensional, which creates a special challenge for data mining algorithms. In this paper, we consider the problem of classification of high dimensional data streams. For the high dimensional case, even traditional classifiers do not work very well on fixed data sets. We discuss a number of insights for the intractability of the high dimensional case. We use these insights to propose a new classification method (LOCUST) which avoids many of these weaknesses. The key is to develop a subspace-based instance centered classification approach which can be implemented efficiently for a fast data stream. We propose a methodology to effectively process the data stream in an organized way, so that the intermediate data structures can be used to sample locally discriminative subspaces for the classification process. We show that LOCUST is able to work effectively in the high dimensional case, and is also flexible in terms of increased robustness with greater resource availability.

#index 1206730
#* Dynamic Materialization of Query Views for Data Warehouse Workloads
#@ Thomas Phan;Wen-Syan Li
#t 2008
#c 17
#! A materialized view, or Materialized Query Table (MQT), is an auxiliary table with precomputed data that can be used to significantly improve the performance of a database query. Previous research efforts have focused onfinding the best candidate MQT set, with a common static heuristic being to greedily pre-materialize the MQTs prior to executing the workload. While this approach is sound when the size of the MQT set on disk is small, it will not be able to pre-materialize all MQTs and indexes when faced with real-world disk limits and view maintenance costs, and thus a static heuristic will fail to exploit the potentially large benefits of those MQTs not selected for materialization. In this paper we present an automated, dynamic MQT management scheme that materializes views and creates indexes in an on-demand fashion as a workload executes and manages them with an LRU cache. In order to maximize the benefit of executing queries with MQTs, the scheme makes an adaptive tradeoff between the MQT materializations, the base table accesses, and the benefit of MQT hits in the cache. To find the workload permutation that produces the overall highest net benefit, we use a genetic algorithm to search the N! solution space, and to avoid materializing seldom-used MQTs, we prune the set of MQT candidates.

#index 1206731
#* RiTE: Providing On-Demand Data for Right-Time Data Warehousing
#@ Christian Thomsen;Torben Bach Pedersen;Wolfgang Lehner
#t 2008
#c 17
#! Data warehouses (DWs) have traditionally been loaded with data at regular time intervals, e.g., monthly, weekly, or daily, using fast bulk loading techniques. Recently, the trend is to insert all (or only some) new source data very quickly into DWs, called near-realtime DWs (right-time DWs). This is done using regular INSERT statements, resulting in too low insert speeds. There is thus a great need for a solution that makes inserted data available quickly, while still providing bulk-load insert speeds. This paper presents RiTE ("Right-Time ETL"), a middleware system that provides exactly that. A data producer (ETL) can insert data that becomes available to data consumers on demand. RiTE includes an innovative main-memory based catalyst that provides fast storage and offers concurrency control. A number of policies controlling the bulk movement of data based on user requirements for persistency, availability, freshness, etc. are supported. The system works transparently to both producer and consumers. The system is integrated with an open source DBMS, and experiments show that it provides "the best of both worlds", i.e., INSERT-like data availability, but with bulk-load speeds (up to 10 times faster).

#index 1206732
#* Exploiting Lineage for Confidence Computation in Uncertain and Probabilistic Databases
#@ Anish Das Sarma;Martin Theobald;Jennifer Widom
#t 2008
#c 17
#! We study the problem of computing query results with confidence values in ULDBs: relational databases with uncertainty and lineage. ULDBs, which subsume probabilistic databases, offer an alternative decoupled method of computing confidence values: Instead of computing confidences during query processing, compute them afterwards based on lineage. This approach enables a wider space of query plans, and it permits selective computations when not all confidence values are needed. This paper develops a suite of algorithms and optimizations for a broad class of relational queries on ULDBs. We provide confidence computation algorithms for single data items, as well as efficient batch algorithms to compute confidences for an entire relation or database. All algorithms incorporate memoization to avoid redundant computations, and they have been implemented in the Trio prototype ULDB database system. Performance characteristics and scalability of the algorithms are demonstrated through experimental results over a large synthetic dataset.

#index 1206733
#* Greedy List Intersection
#@ Robert Krauthgamer;Aranyak Mehta;Vijayshankar Raman;Atri Rudra
#t 2008
#c 17
#! A common technique for processing conjunctive queries is to first match each predicate separately using an index lookup, and then compute the intersection of the resulting rowid lists, via an AND-tree. The performance of this technique depends crucially on the order of lists in this tree: it is important to compute early the intersections that will produce small results. But this optimization is hard to do when the data or predicates have correlation. We present a new algorithm for ordering the lists in an AND-tree tree by sampling the intermediate intersection sizes. We prove that our algorithm is near-optimal and validate its effectiveness experimentally on datasets with a variety of distributions.

#index 1206734
#* Handling Non-linear Polynomial Queries over Dynamic Data
#@ Shetal Shah;Krithi Ramamritham
#t 2008
#c 17
#! Applications that monitor functions over rapidly and unpredictably changing data, express their needs as continuous queries. Our focus is on a rich class of queries, expressed as polynomials over multiple data items. Given a set of polynomial queries at a coordinator C, and a user-specified accuracy bound (tolerable imprecision) for each query, we address the problem of assigning data accuracy bounds or filters to the source of each data item. Assigning data accuracy bounds for non-linear queries poses special challenges. Unlike linear queries, data accuracy bounds for non-linear queries depend on the current values of data items and hence need to be recomputed frequently. So, we seek an assignment such that a) if the value of each data item at C is within its data accuracy bound then the value of each query is also within its accuracy bound, b) the number of data refreshes sent by sources to C to meet the query accuracy bounds, is as low as possible, and c) the number of times the data accuracy bounds need to be recomputed is as low as possible. In this paper, we couple novel ideas with existing optimization techniques to derive such an assignment.

#index 1206735
#* Database Support for Probabilistic Attributes and Tuples
#@ Sarvjeet Singh;Chris Mayfield;Rahul Shah;Sunil Prabhakar;Susanne Hambrusch;Jennifer Neville;Reynold Cheng
#t 2008
#c 17
#! The inherent uncertainty of data present in numerous applications such as sensor databases, text annotations, and information retrieval motivate the need to handle imprecise data at the database level. Uncertainty can be at the attribute or tuple level and is present in both continuous and discrete data domains. This paper presents a model for handling arbitrary probabilistic uncertain data (both discrete and continuous) natively at the database level. Our approach leads to a natural and efficient representation for probabilistic data. We develop a model that is consistent with possible worlds semantics and closed under basic relational operators. This is the first model that accurately and efficiently handles both continuous and discrete uncertainty. The model is implemented in a real database system (PostgreSQL) and the effectiveness and efficiency of our approach is validated experimentally.

#index 1206736
#* Scalable Rule-Based Gene Expression Data Classification
#@ Mark A. Iwen;Willis Lang;Jignesh M. Patel
#t 2008
#c 17
#! Current state-of-the-art association rule-based classifiers for gene expression data operate in two phases: (i) Association rule mining from training data followed by (ii) Classification of query data using the mined rules. In the worst case, these methods require an exponential search over the subset space of the training data set's samples and/or genes during at least one of these two phases. Hence, existing association rule-based techniques are prohibitively computationally expensive on large gene expression datasets. Our main result is the development of a heuristic rule-based gene expression data classifier called Boolean Structure Table Classification (BSTC). BSTC is explicitly related to association rule-based methods, but is guaranteed to be polynomial space/time. Extensive cross validation studies on several real gene expression datasets demonstrate that BSTC retains the classification accuracy of current association rule-based methods while being orders of magnitude faster than the leading classifier RCBT on large datasets. As a result, BSTC is able to finish table generation and classification on large datasets for which current association rule-based methods become computationally infeasible. BSTC also enjoys two other advantages over association rule-based classifiers: (i) BSTC is easy to use (requires no parameter tuning), and (ii) BSTC can easily handle datasets with any number of class types. Furthermore, in the process of developing BSTC we introduce a novel class of boolean association rules which have potential applications to other data mining problems.

#index 1206737
#* A Clustered Index Approach to Distributed XPath Processing
#@ Georgia Koloniari;Evaggelia Pitoura
#t 2008
#c 17
#! Supporting top-k queries over distributed collections of schemaless XML data poses two challenges. While XML supports expressive query languages such as XPath and XQuery, these languages require schema knowledge so as to write an appropriate query which may not be available in distributed systems with autonomous and dynamic sources. Thus, there is a need for approximate query processing. Furthermore, retrieving the top-k results incurs large communication and processing cost, since partial result lists from numerous sites need to be combined and ranked to assembly the top-k answers. To address both of these issues, we present an approach for approximate XPath processing over distributed collections of XML data based on a clustered path index, where data is grouped based on structural information. Our method gradually generalizes a query by applying a set of structural transformations to it and the retrieved results are ranked based on the edit distance between two path expressions. A compact indexing data structure is used to reduce the index construction cost. Our experimental results show that our approach significantly reduces the communication cost for retrieving the top-k results, while maintaining a low construction cost for the clustered index.

#index 1206738
#* AxPRE Summaries: Exploring the (Semi-)Structure of XML Web Collections
#@ Mariano P. Consens;Flavio Rizzolo;Alejandro A. Vaisman
#t 2008
#c 17
#! This paper introduces AxPRE summaries, a formalism that allows exploring the (semi-)structure of large XML collections. AxPRE summaries are implemented in a tool, DescribeX, that supports visualizing XML collections via summaries that can be interactively refined using a powerful and descriptive axis path regular expression language. Experimental results on gigabyte collections have shown that this flexibility does not come at the expense of efficiency.

#index 1206739
#* Correlation-based Attribute Outlier Detection in XML
#@ Judice L.  Y. Koh;Mong Li Lee;Wynne Hsu;Wee Tiong Ang
#t 2008
#c 17
#! Compared to relational data models, the hierarchical structure of semi-structured data such as XML provides semantically meaningful neighbourhoods advancing data cleaning problems such as outlier detection. In this paper, we introduce the concept of correlated subspace that leverages on the hierarchical relationships between XML attributes to provide contextually informative neighbourhoods for attribute outlier detection. We also design two correlation-based attribute outlier metrics for XML, namely the xO-Measure and xQ-Measure. The effectiveness of our XML outlier detection approach is supported with experimental results.

#index 1206740
#* Ontology-Aware Search on XML-based Electronic Medical Records
#@ Fernando Farfan;Vagelis Hristidis;Anand Ranganathan;Redmond P. Burke
#t 2008
#c 17
#! As the use of Electronic Medical Records (EMRs) becomes more widespread, so does the need for effective information discovery on them. Recently proposed EMR standards are XML-based, having as a key characteristic the frequent use of ontological references, i.e., ontological concept codes appear as XML elements and are used to associate portions of the EMR document with concepts defined in a domain ontology. In this paper we present the XOntoRank system which tackles the problem of ontology-aware keyword search on XML documents with a particular focus on EMR XML documents. Our running examples and experiments use the Health Level Seven (HL7) Clinical Document Architecture (CDA) Release 2.0 standard of EMR representation and the Systematized Nomenclature of Human and Veterinary Medicine (SNOMED) ontology, although the presented techniques and results are applicable to any EMR hierarchical format and any ontology that defines concepts and relationships.

#index 1206741
#* Query-Aware Partitioning for Monitoring Massive Network Data Streams
#@ Theodore Johnson;S. Muthukrishnan;Vladislav Shkapenyuk;Oliver Spatscheck
#t 2008
#c 17
#! Data Stream Management Systems (DSMS) are gaining acceptance for applications that need to process very large volumes of data in real time. The load generated by such applications frequently exceeds by far the computation capabilities of a single centralized server. In particular, a single-server instance of our DSMS, Gigascope, cannot keep up with the processing demands of the new OC-786 networks, which can generate more than 100 million packets per second. In this paper, we explore a mechanism for the distributed processing of very high speed data streams. Existing distributed DSMSs employ two mechanisms for distributing the load across the participating machines: partitioning of the query execution plans and partitioning of the input data stream in a query-independent fashion. However, for a large class of queries, both approaches fail to reduce the load as compared to centralized system, and can even lead to an increase in the load. In this paper we present an alternative approach - query-aware data stream partitioning that allows for more efficient scaling. We have developed methods for analyzing any given query node to determine a partition strategy, reconcile potentially conflicting requirements that different queries in a query set place on partitioning, and to choose an optimal partitioning which minimizes overall communication costs.

#index 1206742
#* Optimizing Hierarchical Access in OLAP Environment
#@ Lipyeow Lim;Bishwaranjan Bhattacharjee
#t 2008
#c 17
#! In Online Analytic Processing (OLAP) deployments, different users, lines of businesses and business units often create adhoc aggregation hierarchies tailor-made for specific reporting or analytical applications. As a result, a large number of these application specific hierarchies accumulate over time. System administrators typically are not able to optimize all these hierarchical accesses by hand due to the large number of hierarchies. However, many optimization opportunities exist due to the significant amount of overlap between some hierarhies. In this paper, we sketch a novel method for optimizing OLAP aggregation queries using precomputed aggregates on other overlapping hierarchies. Our method detects common sub-structures among hierarchies and provides a rewriting algorithm to exploit any precomputations on these shared sub-structures.

#index 1206743
#* Frequency Estimation over Sliding Windows
#@ Linfeng Zhang;Yong Guan
#t 2008
#c 17
#! Capturing characteristics of large data streams has received considerable attention. The constraints in space and time often restrict the data stream processing to only one pass. Furthermore, processing data streams over sliding windows makes the problem more difficult and challenging. In this paper, we address the problem of estimating -approximate frequency in data streams over sliding windows. We are the first who propose an efficient algorithm which can achieve O(1/) space requirement and only need O(1) running time to process each item in the data stream and to answer a query.

#index 1206744
#* Parallel Evaluation of Composite Aggregate Queries
#@ Lei Chen;Christopher Olston;Raghu Ramakrishnan
#t 2008
#c 17
#! Aggregate measures summarizing subsets of data are valuable in exploratory analysis and decision support, especially when dependent aggregations can be easily specified and computed. A novel class of queries, called composite subset measures, was previously introduced to allow correlated aggregate queries to be easily expressed. This paper considers how to evaluate composite subset measure queries using a large distributed system. We describe a cross-node data redistribution strategy that takes into account the nested structure of a given query. The main idea is to group data into blocks in "cube space", such that aggregations can be generated locally within each block, leveraging previously proposed optimizations per-block. The partitioning scheme allows overlap among blocks so that sliding window aggregation can be handled. Furthermore, it also guarantees that the final answer is the union of local results with no duplication and there is no need for the expensive data combination step. We identify the most important partitioning parameters and propose an optimization algorithm. We also demonstrate effectiveness of the optimizer to minimize the query response time.

#index 1206745
#* Injector: Mining Background Knowledge for Data Anonymization
#@ Tiancheng Li;Ninghui Li
#t 2008
#c 17
#! Existing work on privacy-preserving data publishing cannot satisfactorily prevent an adversary with background knowledge from learning important sensitive information. The main challenge lies in modeling the adversary's background knowledge. We propose a novel approach to deal with such attacks. In this approach, one first mines knowledge from the data to be released and then uses the mining results as the background knowledge when anonymizing the data. The rationale of our approach is that if certain facts or background knowledge exist, they should manifest themselves in the data and we should be able to find them using data mining techniques. One intriguing aspect of our approach is that one can argue that it improves both privacy and utility at the same time, as it both protects against background knowledge attacks and better preserves the features in the data. We then present the Injector framework for data anonymization. Injector mines negative association rules from the data to be released and uses them in the anonymization process. We also develop an efficient anonymization algorithm to compute the injected tables that incorporates background knowledge. Experimental results show that Injector reduces privacy risks against background knowledge attacks while improving data utility.

#index 1206746
#* Automatic Extraction of Useful Facet Hierarchies from Text Databases
#@ Wisam Dakka;Panagiotis G. Ipeirotis
#t 2008
#c 17
#! Databases of text and text-annotated data constitute a significant fraction of the information available in electronic form. Searching and browsing are the typical ways that users locate items of interest in such databases. Faceted interfaces represent a new powerful paradigm that proved to be a successful complement to searching. Thus far the identification of the facets was either a manual procedure or relied on apriori knowledge of the facets that can potentially appear in the underlying collection. In this paper we present an unsupervised technique for automatic extraction of facets useful for browsing text databases. In particular we observe through a pilot study that facet terms rarely appear in text documents showing that we need external resources to identify useful facet terms. For this we first identify important phrases in each document. Then we expand each phrase with "context" phrases using external resources such as WordNet and Wikipedia causing facet terms to appear in the expanded database. Finally we compare the term distributions in the original database and the expanded database to identify the terms that can be used to construct browsing facets. Our extensive user studies using the Amazon Mechanical Turk service show that our techniques produce facets with high precision and recall that are superior to existing approaches and help users locate interesting items faster.

#index 1206747
#* A Sampling-Based Approach to Information Recovery
#@ Junyi Xie;Jun Yang;Yuguo Chen;Haixun Wang;Philip S. Yu
#t 2008
#c 17
#! There has been a recent resurgence of interest in research on noisy and incomplete data. Many applications require information to be recovered from such data. Ideally, an approach for information recovery should have the following features. First, it should be able to incorporate prior knowledge about the data, even if such knowledge is in the form of complex distributions and constraints for which no close-form solutions exist. Second, it should be able to capture complex correlations and quantify the degree of uncertainty in the recovered data, and further support queries over such data. The database community has developed a number of approaches for information recovery, but none is general enough to offer all above features. To overcome the limitations, we take a significantly more general approach to information recovery based on sampling. We apply sequential importance sampling, a technique from statistics that works for complex distributions and dramatically outperforms naive sampling when data is constrained. We illustrate the generality and efficiency of this approach in two application scenarios: cleansing RFID data, and recovering information from published data that has been summarized and randomized for privacy.

#index 1206748
#* Mining (Social) Network Graphs to Detect Random Link Attacks
#@ Nisheeth Shrivastava;Anirban Majumder;Rajeev Rastogi
#t 2008
#c 17
#! Modern communication networks are vulnerable to attackers who send unsolicited messages to innocent users, wasting network resources and user time. Some examples of such attacks are spam emails, annoying tele-marketing phone calls, viral marketing in social networks, etc. Existing techniques to identify these attacks are tailored to certain specific domains (like email spam filtering), but are not applicable to a majority of other networks. We provide a generic abstraction of such attacks, called the Random Link Attack (RLA), that can be used to describe a large class of attacks in communication networks. In an RLA, the malicious user creates a set of false identities and uses them to communicate with a large, random set of innocent users. We mine the social networking graph extracted from user interactions in the communication network to find RLAs. To the best of our knowledge, this is the first attempt to conceptualize the attack definition, applicable to a variety of communication networks. In this paper, we formally define RLA and show that the problem of finding an RLA is NP-complete. We also provide two efficient heuristics to mine subgraphs satisfying the RLA property; the first (GREEDY) is based on greedy set-expansion, and the second (TRWALK) on randomized graph traversal. Our experiments with a real-life data set demonstrate the effectiveness of these algorithms.

#index 1206749
#* A Hybrid Approach to Private Record Linkage
#@ Ali Inan;Murat Kantarcioglu;Elisa Bertino;Monica Scannapieco
#t 2008
#c 17
#! Real-world entities are not always represented by the same set of features in different data sets. Therefore matching and linking records corresponding to the same real-world entity distributed across these data sets is a challenging task. If the data sets contain private information, the problem becomes even harder due to privacy concerns. Existing solutions of this problem mostly follow two approaches: sanitization techniques and cryptographic techniques. The former achieves privacy by perturbing sensitive data at the expense of degrading matching accuracy. The later, on the other hand, attains both privacy and high accuracy under heavy communication and computation costs. In this paper, we propose a method that combines these two approaches and enables users to trade off between privacy, accuracy and cost. Experiments conducted on real data sets show that our method has significantly lower costs than cryptographic techniques and yields much more accurate matching results compared to sanitization techniques, even when the data sets are perturbed extensively.

#index 1206750
#* Querying and Managing Provenance through User Views in Scientific Workflows
#@ Olivier Biton;Sarah Cohen-Boulakia;Susan B. Davidson;Carmem S. Hara
#t 2008
#c 17
#! Workflow systems have become increasingly popular for managing experiments where many bioinformatics tasks are chained together. Due to the large amount of data generated by these experiments and the need for reproducible results, provenance has become of paramount importance. Workflow systems are therefore starting to provide support for querying provenance. However, the amount of provenance information may be overwhelming, so there is a need for abstraction mechanisms to help users focus on the most relevant information. The technique we pursue is that of "user views." Since bioinformatics tasks may themselves be complex sub-workflows, a user view determines what level of sub-workflow the user can see, and thus what data and tasks are visible in provenance queries. In this paper, we formalize the notion of user views, demonstrate how they can be used in provenance queries, and give an algorithm for generating a user view based on which tasks are relevant for the user. We then describe our prototype and give performance results. Although presented in the context of scientific workflows, the technique applies to other data-oriented workflows.

#index 1206751
#* Spatial Outsourcing for Location-based Services
#@ Yin Yang;Stavros Papadopoulos;Dimitris Papadias;George Kollios
#t 2008
#c 17
#! The embedding of positioning capabilities in mobile devices and the emergence of location-based applications have created novel opportunities for utilizing several types of multi-dimensional data through spatial outsourcing. In this setting, a data owner (DO) delegates its data management tasks to a location-based service (LBS) that processes queries originating from several clients/ subscribers. Because the LBS is not the real owner of the data, it must prove (to each client) the correctness of query output using an authenticated structure signed by the DO. Currently there is very narrow selection of multi-dimensional authenticated structures, among which the VR-tree is the best choice. Our first contribution is the MR-tree, a novel index suitable for spatial outsourcing. We show, analytically and experimentally, that the MR-tree outperforms the VR-tree, usually by orders of magnitude, on all performance metrics, including construction cost, index size, query and verification overhead. Motivated by the fact that successive queries by the same mobile client exhibit locality, we also propose a synchronized caching technique that utilizes the results of previous queries to reduce the size of the additional information sent to the client for verification purposes.

#index 1206752
#* P-Cube: Answering Preference Queries in Multi-Dimensional Space
#@ Dong Xin;Jiawei Han
#t 2008
#c 17
#! Many new applications that involve decision making need online (i.e., OLAP-styled) preference analysis with multi-dimensional boolean selections. Typical preference queries includes top-k queries and skyline queries. An analytical query often comes with a set of boolean predicates that constrain a target subset of data, which, may also vary incrementally by drilling/rolling operators. To efficiently support preference queries with multiple boolean predicates, neither boolean-then-preference nor preference-then-boolean approach is satisfactory. To integrate boolean pruning and preference pruning in a unified framework, we propose signature, a new materialization measure for multi-dimensional group-bys. Based on this, we propose P-Cube (i.e., data cube for preference queries) and study its complete life cycle, including signature generation, compression, decomposition, incremental maintenance and usage for efficient on-line analytical query processing. We present a signature-based progressive algorithm that is able to simultaneously push boolean and preference constraints deep into the database search. Our performance study shows that the proposed method achieves at least one order of magnitude speed-up over existing approaches.

#index 1206753
#* Efficient Rewriting Algorithms for Preference Queries
#@ Periklis Georgiadis;Ioannis Kapantaidakis;Vassilis Christophides;Elhadji Mamadou Nguer;Nicolas Spyratos
#t 2008
#c 17
#! Preference queries are crucial for various applications (e.g. digital libraries) as they allow users to discover and order data of interest in a personalized way. In this paper, we define preferences as preorders over relational attributes and their respective domains. Then, we rely on appropriate linearizations to provide a natural semantics for the block sequence answering a preference query. Moreover, we introduce two novel rewriting algorithms (called LBA and TBA) which exploit the semantics of preference expressions for constructing progressively each block of the answer. We demonstrate experimentally the scalability and performance gains of our algorithms (up to 3 orders of magnitude) for variable database and result sizes, as well as for preference expressions of variable size and structure. To the best of our knowledge, LBA and TBA are the first algorithms for evaluating efficiently arbitrary preference queries over voluminous databases.

#index 1206754
#* A Fast Similarity Join Algorithm Using Graphics Processing Units
#@ Michael D. Lieberman;Jagan Sankaranarayanan;Hanan Samet
#t 2008
#c 17
#! A similarity join operation A BOWTIEepsiv B takes two sets of points A, B and a value epsiv isin Ropf, and outputs pairs of points p isin A,q isin B, such that the distance D(p, q) les epsiv. Similarity joins find use in a variety of fields, such as clustering, text mining, and multimedia databases. A novel similarity join algorithm called LSS is presented that executes on a graphics processing unit (GPU), exploiting its parallelism and high data throughput. As GPUs only allow simple data operations such as the sorting and searching of arrays, LSS uses these two operations to cast a similarity join operation as a GPU sort-and-search problem. It first creates, on the fly, a set of space-filling curves on one of its input datasets, using a parallel GPU sort routine. Next, LSS processes each point p of the other dataset in parallel. For each p, it searches an interval of one of the space-filling curves guaranteed to contain all the pairs in which p participates. Using extensive theoretical and experimental analysis, LSS is shown to offer a good balance between time and work efficiency. Experimental results demonstrate that LSS is suitable for similarity joins in large high-dimensional datasets, and that it performs well when compared against two existing prominent similarity join methods.

#index 1206755
#* Automatic Result Verification for the Functional Testing of a Query Language
#@ Carsten Binnig;Donald Kossmann;Eric Lo;Angel Saenz-Badillos
#t 2008
#c 17
#! Functional testing of a query language is a challenging task in practice. In order to reveal errors in the query processing functionality, it is necessary to verify the actual result of a test query with the expected correct result. However, automatically computing the expected query result of an arbitrary test query is not trivial. One solution is to first generate a set of test database instances and test queries and then to compute the expected result for each test query over the individual test database instances. The problem of this solution is that many test queries might return an empty query result, which is not interesting for the functional testing of a query language. In this paper, we present a new approach to verify the result of a test query so as to facilitate the functional testing of a query language. Instead of first generating the database instance and then computing the expected result for each test query, we first create one or more interesting expected results for a given test query and then generate a test database instance for each combination of a test query and an expected result individually which returns the expected result if the test query is executed correctly. That way, we enable the verification of the actual result and allow an explicit definition of interesting test cases for the functional testing of a query language.

#index 1206756
#* Improving Information Access for a Community of Practice Using Business Process as Context
#@ Yu Deng;Murthy Devarakonda;Nithya Rajamani;Wlodek Zadrozny
#t 2008
#c 17
#! This paper addresses the important problem of finding relevant information in the context of a business process. It presents an information access solution called EIL (Enterprise Information Leverage) which combines information extraction and semantic search to support information needs of professionals selling IT services. EIL leverages structured and unstructured data using novel architecture and special purpose algorithms. Our approach is to organize information around business activities (e.g. a sale), and the system supports semantic concept based information retrieval by utilizing both database query and document search where the relevant business activities act as a contextual constraint. We experimentally show that this approach is promising for reducing noise in search results. EIL is currently under pilot deployment in one of the IBM services sales units.

#index 1206757
#* DescribeX: Interacting with AxPRE Summaries
#@ M. S. Ali;Mariano P. Consens;Shahan Khatchadourian;Flavio Rizzolo
#t 2008
#c 17
#! DescribeX is a visual, interactive tool for exploring the underlying structure of an XML collection. DescribeX implements a framework for creating XML summaries described using axis path regular expressions (abbreviated AxPRE). AxPRE's capture all the bisimilarity-based proposals in the summary literature and they can be used to define new and more expressive summaries. This demonstration shows how DescribeX helps to analyze diverse XML collections in one particular scenario: the analysis of protein-protein interaction XML data from multiple providers that conform to the PSI-MI schema.

#index 1206758
#* COSTA: Adaptive Indexing for Terms in a Large-scale Distributed System
#@ Aoying Zhou;Rong Zhang;Quang Hieu Vu;Weining Qian
#t 2008
#c 17

#index 1206759
#* IMPrECISE: Good-is-good-enough data integration
#@ Ander de Keijzer;Maurice van Keulen
#t 2008
#c 17
#! IMPrECISE is an XQuery module that adds probabilistic XML functionality to an existing XML DBMS, in our case MonetDB/XQuery. We demonstrate probabilistic XML and data integration functionality of IMPrECISE. The prototype is configurable with domain knowledge such that the amount of uncertainty arising during data integration is reduced to an acceptable level, thus obtaining a "good is good enough" data integration with minimal human effort.

#index 1206760
#* SPARK: A Keyword Search Engine on Relational Databases
#@ Yi Luo;Wei Wang;Xuemin Lin
#t 2008
#c 17

#index 1206761
#* Astoria: A Programming Model for Data on the Web
#@ Pablo Castro;Anil Nori
#t 2008
#c 17
#! Modern web applications built using technologies such as AJAX, Adobe Flash and Microsoft Silverlight interact with data in a different way compared to previous-generation applications. Data is now a first-class construct that is exchanged over the web independently from presentation information. Astoria is both a set of patterns and an actual implementation of a programming interface for data in the web. Astoria exposes data in a database over an HTTP interface and follows the REST architectural style for exploring and manipulating data. We demonstrate the URI patterns used by Astoria and their associated semantics, and the various data formats supported by the system. We explore specific requirements for the formats such as hyper-linking and usability for both retrieval and updates. We also demonstrate step by step how developers use Astoria to create custom data services on top of existing databases.

#index 1206762
#* Environmental Tomography: Ubiquitous Sensing with Mobile Devices
#@ Stacy Patterson;Bassam Bamieh;Amr El Abbadi
#t 2008
#c 17
#! The ubiquitous nature of mobile phones, which are location-aware devices, presents a unique platform for large-scale computing applications. In particular, if mobile phones are coupled with sensors, they can be used for detection and monitoring of environmental phenomena such as pollution and radiation. In this demonstration, we present Environmental To-mography, a system for ubiquitous environmental sensing with mobile devices. Aggregate sensor measurements are collected by the devices along fixed paths such as roads, and these aggregates are used to reconstruct an estimate of the distribution of the underlying physical phenomenon. Our system is robust to the dynamic characteristics of mobile networks and also preserves the privacy of mobile user locations. We demonstrate a prototype that generates estimate distributions from user specified data collection paths and underlying data distributions. The accuracy of the reconstructed distributions is illustrated both numerically and graphically.

#index 1206763
#* Preserving Privacy in Social Networks Against Neighborhood Attacks
#@ Bin Zhou;Jian Pei
#t 2008
#c 17
#! Recently, as more and more social network data has been published in one way or another, preserving privacy in publishing social network data becomes an important concern. With some local knowledge about individuals in a social network, an adversary may attack the privacy of some victims easily. Unfortunately, most of the previous studies on privacy preservation can deal with relational data only, and cannot be applied to social network data. In this paper, we take an initiative towards preserving privacy in social network data. We identify an essential type of privacy attacks: neighborhood attacks. If an adversary has some knowledge about the neighbors of a target victim and the relationship among the neighbors, the victim may be re-identified from a social network even if the victim's identity is preserved using the conventional anonymization techniques. We show that the problem is challenging, and present a practical solution to battle neighborhood attacks. The empirical study indicates that anonymized social networks generated by our method can still be used to answer aggregate network queries with high accuracy.

#index 1206764
#* Increasing the Expressivity of Conditional Functional Dependencies without Extra Complexity
#@ Loreto Bravo;Wenfei Fan;Floris Geerts;Shuai Ma
#t 2008
#c 17
#! The paper proposes an extension of CFDs [1], referred to as extended Conditional Functional Dependencies (eCFDs). In contrast to CFDs, eCFDs specify patterns of semantically related values in terms of disjunction and inequality, and are capable of catching inconsistencies that arise in practice but cannot be detected by CFDs. The increase in expressive power does not incur extra complexity: we show that the satisfiability and implication analyses of eCFDs remain NP - complete and coNP -complete, respectively, the same as their CFDs counterparts. In light of the intractability, we present an algorithm that approximates the maximum number of eCFDs that are satisfiable. In addition, we revise SQL techniques for detecting CFD violations, and show that violations of multiple eCFDs can be captured via a single pair of SQL queries. We also introduce an incremental SQL technique for detecting eCFD violations in response to database updates. We experimentally verify the effectiveness and efficiency of our SQL -based detection methods.

#index 1206765
#* Efficient Constraint Monitoring Using Adaptive Thresholds
#@ Srinivas Kashyap;Jeyashankher Ramamirtham;Rajeev Rastogi;Pushpraj Shukla
#t 2008
#c 17
#! Detecting constraint violations in large-scale distributed systems has recently attracted plenty of attention from the research community due to its varied applications (security, network monitoring, etc.). Communication efficiency of these systems is a critical concern and determines their practicality. In this paper, we introduce a new set of methods called non-zero slack schemes to implement distributed SUM queries efficiently. We show, both analytically and empirically, that these methods can lead to a considerable reduction in the amount of communication. We propose three adaptive non-zero slack schemes that adapt to changing data distributions; our best scheme is a lightweight reactive scheme that probabilistically adjusts local constraints based on the occurrence of certain events (using only a periodic probability estimation). We conduct an extensive experimental study using real-life and synthetic data sets, and show that our non-zero slack schemes incur significantly less communication overhead compared to the state of the art zero slack scheme (over a 60% savings).

#index 1206766
#* Dominant Graph: An Efficient Indexing Structure to Answer Top-K Queries
#@ Lei Zou;Lei Chen
#t 2008
#c 17
#! Given a record set D and a query score function F, a top-k query returns k records from D, whose values of function F on their attributes are the highest. In this paper, we investigate the intrinsic connection between top-k queries and dominant relationship between records, and based on which, we propose an efficient layer-based indexing structure, Dominant Graph (DG), to improve the query efficiency. Specifically, DG is built offline to express the dominant relationship between records and top-k query is implemented as a graph traversal problem, i.e. Traveler algorithm. We prove theoretically that the size of search space (that is the number of retrieved records from the record set to answer top-k query) in our basic algorithm is directly related to the cardinality of skyline points in the record set (see Theorem 3.2). Based on the cost analysis, we propose the optimization technique, pseudo record, to improve the search efficiency. In order to handle the top-k query in the high dimension record set, we also propose N-Way Traveler algorithm. Finally, extensive experiments demonstrate that our proposed methods have significant improvement over its counterparts, including both classical and state art of top-k algorithms. For example, the search space in our algorithm is less than 1/5 of that in AppRI (Xin et al., 2006), one of state art of top-k algorithms. Furthermore, our method can support any aggregate monotone query function.

#index 1206767
#* Parallel Distributed Processing of Constrained Skyline Queries by Filtering
#@ Bin Cui;Hua Lu;Quanqing Xu;Lijiang Chen;Yafei Dai;Yongluan Zhou
#t 2008
#c 17
#! Skyline queries are capable of retrieving interesting points from a large data set according to multiple criteria. Most work on skyline queries so far has assumed a centralized storage, whereas in practice relevant data are often distributed among geographically scattered sites. In this work, we tackle constrained skyline queries in large-scale distributed environments without the assumption of any overlay structures, and propose a novel algorithm named PaDSkyline (Parallel Distributed Skyline query processing). PaDSkyline significantly shortens the response time by performing parallel processing over site groups produced by a partition algorithm. Within each group, it locally optimizes the query processing over distributed sites. It also drastically enhances the network transmission efficiency by performing early reduction of skyline candidates with deliberately selected multiple filtering points. Results of extensive experiments demonstrate the efficiency and robustness of our proposals.

#index 1206768
#* MobiQual: QoS-aware Load Shedding in Mobile CQ Systems
#@ Bugra Gedik;Kun-Lung Wu;Philip S. Yu;Ling Liu
#t 2008
#c 17
#! Freshness and accuracy are two key measures of quality of service (QoS) in location-based, mobile continual queries (CQs). However, it is often difficult to provide both fresh and accurate CQ results due to (a) limited resources in computing and communication and (b) fast-changing load conditions caused by continuous mobile node movement. Thus a key challenge for a mobile CQ system is: How do we achieve the highest possible quality of the query results, in both freshness and accuracy, with currently available resources under changing load conditions? In this paper, we formulate this problem as a load shedding one, and develop MobiQual - a QoS-aware framework for performing both update load shedding and query load shedding. The design of MobiQual highlights three important features. (1) Differentiated load shedding: Different amounts of query and update load shedding are applied to different groups of queries and mobile nodes, respectively. (2) Per-query QoS specifications: The overall freshness and accuracy of the query results are maximized with individualized QoS specifications. (3) Low-cost adaptation: MobiQual dynamically adapts, with a minimal overhead, to changing load conditions and available resources. We show that, through a careful combination of update and query load shedding, the MobiQual approach leads to much higher freshness and accuracy in the query results in all cases, compared to existing approaches.

#index 1206769
#* Approximate Clustering on Distributed Data Streams
#@ Qi Zhang;Jinze Liu;Wei Wang
#t 2008
#c 17
#! We investigate the problem of clustering on distributed data streams. In particular, we consider the k-median clustering on stream data arriving at distributed sites which communicate through a routing tree. Distributed clustering on high speed data streams is a challenging task due to limited communication capacity, storage space, and computing power at each site. In this paper, we propose a suite of algorithms for computing (1 + epsiv) -approximate k-median clustering over distributed data streams under three different topology settings: topology-oblivious, height-aware, and path-aware. Our algorithms reduce the maximum per node transmission to polylog N (opposed to Omega(N) for transmitting the raw data). We have simulated our algorithms on a distributed stream system with both real and synthetic datasets composed of millions of data. In practice, our algorithms are able to reduce the data transmission to a small fraction of the original data. Moreover, our results indicate that the algorithms are scalable with respect to the data volume, approximation factor, and the number of sites.

#index 1206770
#* Handling Uncertain Data in Array Database Systems
#@ Tingjian Ge;Stan Zdonik
#t 2008
#c 17
#! Scientific and intelligence applications have special data handling needs. In these settings, data does not fit the standard model of short coded records that had dominated the data management area for three decades. Array database systems have a specialized architecture to address this problem. Since the data is typically an approximation of reality, it is important to be able to handle imprecision and uncertainty in an efficient and provably accurate way. We propose a discrete approach for value distributions and adopt a standard metric (i.e., variation distance) in probability theory to measure the quality of a result distribution. We then propose a novel algorithm that has a provable upper bound on the variation distance between its result distribution and the "ideal" one. Complementary to that, we advocate the usage of a "statistical mode" suitable for the results of many queries and applications, which is also much more efficient for execution. We show how the statistical mode also presents interesting predicate evaluation strategies. In addition, extensive experiments are performed on real world datasets to evaluate our algorithms.

#index 1206771
#* Just-In-Time Processing of Continuous Queries
#@ Yin Yang;Dimitris Papadias
#t 2008
#c 17
#! In a data stream management system, a continuous query is processed by an execution plan consisting of multiple operators connected via the "consumer-producer" relationship, i.e., the output of an operator (the "producer") feeds to another downstream operator (the "consumer") as input. Existing techniques execute each operator separately and push all results to its consumers, without considering whether the consumers need them. Consequently, considerable CPU and memory resources are wasted on producing and storing useless intermediate results. Motivated by this, we propose just-in-time (JIT) processing, a novel methodology that enables a consumer to return feedback expressing its current demand to the producer. The latter selectively generates results based on this information. We show, through extensive experiments, that JIT achieves significant savings in terms of both CPU time and memory consumption.

#index 1206772
#* Online Filtering, Smoothing and Probabilistic Modeling of Streaming data
#@ Bhargav Kanagal;Amol Deshpande
#t 2008
#c 17
#! In this paper, we address the problem of extending a relational database system to facilitate efficient real-time application of dynamic probabilistic models to streaming data. We use the recently proposed abstraction of model-based views for this purpose, by allowing users to declaratively specify the model to be applied, and by presenting the output of the models to the user as a probabilistic database view. We support declarative querying over such views using an extended version of SQL that allows for querying probabilistic data. Underneath we use particle filters, a class of sequential Monte Carlo algorithms, to represent the present and historical states of the model as sets of weighted samples (particles) that are kept up-to-date as new data arrives. We develop novel techniques to convert the queries on the model-based view directly into queries over particle tables, enabling highly efficient query processing. Finally, we present experimental evaluation of our prototype implementation over several synthetic and real datasets, that demonstrates the feasibility of online modeling of streaming data using our system and establishes the advantages of tight integration between dynamic probabilistic models and databases.

#index 1206773
#* OptimAX: efficient support for data-intensive mash-ups
#@ S. Abiteboul;I. Manolescu;S. Zoupanos
#t 2008
#c 17
#! Mash-ups are being used in various Web-based applications of Web 2.0 which combine instantly information from different sources. Active XML (AXML, in short) language is a tool for decentralized, data-centric Web service integration. AXML document includes calls to services that may be either simple request-responses either long running subscriptions. Being fully composable and allowing resource sharing makes AXML ideal for mash-up style integration. In this demo we present how AXML can be used as a specification, optimization and distributed execution language for dynamic distributed mashups in varied P2P settings. We also demonstrate our AXML optimizer's (OptimAX) optimization rules and rewriting engine with a help of GUI.

#index 1206774
#* Graphitti: An Annotation Management System for Heterogeneous Objects
#@ Sandeep Gupta;Christopher Condit;Amarnath Gupta
#t 2008
#c 17
#! Annotation is the process of supplementing data with additional information that was not part of the actual observation, but reflects post-facto comments and associations made by a user who analyzes the data. While annotation management systems are emerging in the field of relational data, such systems for scientific applications, where there is a wide heterogeneity in the types of annotable data, are almost nonexistent. In this demonstration paper, we describe Graphitti, a tool that (i) allows a user to annotate a wide variety of scientific data, and (ii) allows a user to query data and their annotations in a seamless manner.

#index 1206775
#* Distributed Monitoring of Peer-to-Peer Systems
#@ Serge Abiteboul;Bogdan Marinoiu;Pierre Bourhis
#t 2008
#c 17
#! Observing highly dynamic Peer-to-Peer systems is essential for many applications such as fault management or business processing. We demonstrate P2PMonitor, a P2P system for monitoring such systems. Alerters deployed on the monitored peers are designed to detect particular kinds of local events. They generate streams of XML data that form the primary sources of information for P2PMonitor. The core of the system is composed of processing components implementing the operators of an algebra over data streams. From a user viewpoint, monitoring a P2P system can be as simple as querying an XML document. The document is an ActiveXML document that aggregates a (possibly very large) number of streams generated by alerters on the monitored peers. Behind the scene, P2PMonitor compiles the monitoring query into a distributed monitoring plan, deploys alerters and stream algebra processors and issues notifications that are sent to users. The system functionalities are demonstrated by simulating the supply chain of a large company.

#index 1206776
#* SMART: A System for Online Monitoring Large Volumes of Network Traffic
#@ Aoying Zhou;Ying Yan;Xueqing Gong;Jianlong Chang;Dai Dai
#t 2008
#c 17
#! Network traffic monitoring have been gaining attentions due to its importance in telecom industry. However, the monitoring systems deployed in telecom operators are usually too slow because of their disk-based processing approach. To address this problem, an online network traffic monitoring system, named SMART, is designed and developed. The system converts different formats of raw Netflow data (Netflow IPv5, IPv7 and IPv9) to user-defined control flows through combination and filtering. It can compute top-k frequent flows with sliding window, detects burst on arbitrary attributes, and presents results visually to users. The system could be used to replace the traditional offline monitoring system used in Shanghai Telecom. In its daily operation, it is shown that the processing speed achieves 30,000 flows per second. The basis of advanced streaming algorithms and design of robust system architecture enable SMART to achieve good performance.

#index 1206777
#* Toward Simulation-Based Optimization in Data Stream Management Systems
#@ Christoph Heinz;Jurgen Kramer;Tobias Riemenschneider;Bernhard Seeger
#t 2008
#c 17
#! Our demonstration introduces a novel system architecture which massively facilitates optimization in data stream management systems (DSMS). The basic idea is to decouple optimization from the operative system by means of a secondary optimization system, which bears the burden of determining new query plans. Within the secondary system, which typically runs on a separate machine, we utilize suitable statistical models of the original data streams to simulate them. As the simulation can run at much faster rates, we are able to examine and assess new query plans in a shorter period of time without running the risk of deteriorating the original plan; we only migrate practically approved plans into the operative system. In our demonstration, we will present our prototypical implementation of this optimization architecture. We will demonstrate the interaction between primary and secondary system as well as the key features of the whole optimization process.

#index 1206778
#* PGM/F: A Framework for the Optimization of Data Processing in Business Processes
#@ Marko Vrhovnik;Oliver Suhre;Stephan Ewen;Holger Schwarz
#t 2008
#c 17
#! Workflow languages like BPEL are broadly adopted by industry to integrate the heterogeneous applications and data stores of an enterprise. Leading vendors provide extensions to BPEL that allow for a tight integration of data processing capabilities into the process logic. Business processes exploiting these capabilities show a remarkable potential for optimization. In this demonstration, we present PGM/F, a framework for the optimization of data processing in such business processes.

#index 1206779
#* MED: A Multimedia Event Database for 3D Crime Scene Representation and Analysis
#@ Marcin Kwietniewski;Stephanie Wilson;Anna Topol;Sunbir Gill;Jarek Gryz;Michael Jenkin;Piotr Jasiobedzki;Ho-Kong Ng
#t 2008
#c 17
#! The development of sensors capable of obtaining 3D scans of crime scenes is revolutionizing the ways in which crime scenes can be analyzed and at the same time is driving the need for the development of sophisticated tools to represent and store this data. Here we describe the design of a multimedia database suitable for representing and reasoning about crime scene data. The representation is grounded in the physical environment that makes up the crime scene and provides mechanisms for representing both traditional (forms-based) data as well as 3D scan and other complex spatial data.

#index 1206780
#* Skyline Query Processing for Incomplete Data
#@ Mohamed E. Khalefa;Mohamed F. Mokbel;Justin J. Levandoski
#t 2008
#c 17
#! Recently, there has been much interest in processing skyline queries for various applications that include decision making, personalized services, and search pruning. Skyline queries aim to prune a search space of large numbers of multi-dimensional data items to a small set of interesting items by eliminating items that are dominated by others. Existing skyline algorithms assume that all dimensions are available for all data items. This paper goes beyond this restrictive assumption as we address the more practical case of involving incomplete data items (i.e., data items missing values in some of their dimensions). In contrast to the case of complete data where the dominance relation is transitive, incomplete data suffer from non-transitive dominance relation which may lead to a cyclic dominance behavior. We first propose two algorithms, namely, "Replacement" and "Bucket" that use traditional skyline algorithms for incomplete data. Then, we propose the "ISkyline" algorithm that is designed specifically for the case of incomplete data. The "ISkyline" algorithm employs two optimization techniques, namely, virtual points and shadow skylines to tolerate cyclic dominance relations. Experimental evidence shows that the "ISkyline" algorithm significantly outperforms variations of traditional skyline algorithms.

#index 1206781
#* Top-k Spatial Joins of Probabilistic Objects
#@ Vebjorn Ljosa;Ambuj K. Singh
#t 2008
#c 17
#! Probabilistic data have recently become popular in applications such as scientific and geospatial databases. For images and other spatial datasets, probabilistic values can capture the uncertainty in extent and class of the objects in the images. Relating one such dataset to another by spatial joins is an important operation for data management systems. We consider probabilistic spatial join (PSJ) queries, which rank the results according to a score that incorporates both the uncertainties associated with the objects and the distances between them. We present algorithms for two kinds of PSJ queries: Threshold PSJ queries, which return all pairs that score above a given threshold, and top-k PSJ queries, which return the k top-scoring pairs. For threshold PSJ queries, we propose a plane sweep algorithm that, because it exploits the special structure of the problem, runs in O(n (log n + k)) time, where n is the number of points and k is the number of results. We extend the algorithms to 2-D data and to top-k PSJ queries. To further speed up top-k PSJ queries, we develop a scheduling technique that estimates the scores at the level of blocks, then hands the blocks to the plane sweep algorithm. By finding high-scoring pairs early, the scheduling allows a large portion of the datasets to be pruned. Experiments demonstrate speed-ups of two orders of magnitude.

#index 1206782
#* The Cost of Serializability on Platforms That Use Snapshot Isolation
#@ Mohammad Alomari;Michael Cahill;Alan Fekete;Uwe Rohm
#t 2008
#c 17
#! Several common DBMS engines use the multiversion concurrency control mechanism called Snapshot Isolation, even though application programs can experience non-serializable executions when run concurrently on such a platform. Several proposals exist for modifying the application programs, without changing their semantics, so that they are certain to execute serializably even on an engine that uses SI. We evaluate the performance impact of these proposals, and find that some have limited impact (only a few percent drop in throughput at a given multi-programming level) while others lead to much greater reduction in throughput of up-to 60% in high contention scenarios. We present experimental results for both an open-source and a commercial engine. We relate these to the theory, giving guidelines on which conflicts to introduce so as to ensure correctness with little impact on performance.

#index 1206783
#* Network-Aware Join Processing in Global-Scale Database Federations
#@ Xiaodan Wang;Randal Burns;Andreas Terzis;Amol Deshpande
#t 2008
#c 17
#! We introduce join scheduling algorithms that employ a balanced network utilization metric to optimize the use of all network paths in a global-scale database federation. This metric allows algorithms to exploit excess capacity in the network, while avoiding narrow, long-haul paths. We give a two-approximate, polynomial-time algorithm for serial (left-deep) join schedules. We also present extensions to this algorithm that explore parallel schedules, reduce resource usage, and define trade-offs between computation and network utilization. We evaluate these techniques within the SkyQuery federation of Astronomy databases using spatial-join queries submitted by SkyQuery's users. Experiments show that our algorithms realize near-optimal network utilization with minor computational overhead.

#index 1206784
#* Robust Runtime Optimization of Data Transfer in Queries over Web Services
#@ Anastasios Gounaris;Christos Yfoulis;Rizos Sakellariou;Marios D. Dikaiakos
#t 2008
#c 17
#! Self-managing solutions have recently attracted a lot of interest from the database community. The need for self-* properties is more evident in distributed applications comprising heterogeneous and autonomous databases and functionality providers. Such resources are typically exposed as Web Services (WSs), which encapsulate remote DBMSs and functions called from within database queries. In this setting, database queries are over WSs, and the data transfer cost becomes the main bottleneck. To reduce this cost, data is shipped to and from WSs in chunks; however the optimum chunk size is volatile, depending on both the resources' runtime properties and the query. In this paper we propose a robust control theoretical solution to the problem of optimizing the data transfer in queries over WSs, by continuously tuning at runtime the block size and thus tracking the optimum point. Also, we develop online system identification mechanisms that are capable of estimating the optimum block size analytically. Both contributions are evaluated via both empirical experimentation in a real environment and simulations, and have been proved to be more effective and efficient than static solutions.

#index 1206785
#* Butterfly: Protecting Output Privacy in Stream Mining
#@ Ting Wang;Ling Liu
#t 2008
#c 17
#! Privacy preservation in data mining demands protecting both input and output privacy. The former refers to sanitizing the raw data itself before performing mining. The latter refers to preventing the mining output (model/pattern) from malicious pattern-based inference attacks. The preservation of input privacy does not necessarily lead to that of output privacy. This work studies the problem of protecting output privacy in the context of frequent pattern mining over data streams. After exposing the privacy breaches existing in current stream mining systems, we propose Butterfly, a light-weighted countermeasure that can effectively eliminate these breaches without explicitly detecting them, meanwhile minimizing the loss of the output accuracy. We further optimize the basic scheme by taking account of two types of semantic constraints, aiming at maximally preserving utility-related semantics while maintaining the hard privacy and accuracy guarantee. We conduct extensive experiments over real-life datasets to show the effectiveness and efficiency of our approach.

#index 1206786
#* XStream: a Signal-Oriented Data Stream Management System
#@ Lewis Girod;Yuan Mei;Ryan Newton;Stanislav Rost;Arvind Thiagarajan;Hari Balakrishnan;Samuel Madden
#t 2008
#c 17
#! Sensors capable of sensing phenomena at high data rates on the order of tens to hundreds of thousands of samples per second are now widely deployed in many industrial, civil engineering, scientific, networking, and medical applications. In aggregate, these sensors easily generate several million samples per second that must be processed within milliseconds or seconds. The computation required includes both signal processing and event stream processing. XStream is a stream processing system for such applications. XStream introduces a new data type, the signal segment, which allows applications to manipulate isochronous (regularly spaced in time) collections of sensor samples more conveniently and efficiently than the asynchronous representation used in previous work. XStream includes a memory manager and scheduler optimizations tuned for processing signal segments at high speeds. In benchmark comparisons, we show that XStream outperforms a leading commercial stream processing system by more than three orders of magnitude. On one application, the commercial system processed 72.7 Ksamples/sec, while XStream processed 97.6 Msamples/sec.

#index 1206787
#* Optimizing Star Join Queries for Data Warehousing in Microsoft SQL Server
#@ Cesar A. Galindo-Legaria;Torsten Grabs;Sreenivas Gukal;Steve Herbert;Aleksandras Surna;Shirley Wang;Wei Yu;Peter Zabback;Shin Zhang
#t 2008
#c 17
#! As mainstream data warehouses are growing into the multi-terabyte range, adequate performance for decision support queries remains challenging for database query processors. Proper choice of query plan is essential in data warehouses where fact tables often store billions of rows. This paper discusses query optimization and execution strategies that Microsoft SQL Server employs for decision support queries in dimensionally modeled relational data warehouses. Our approach is based on pattern matching to detect typical star query patterns. When matching the pattern, the optimizer generates additional query plan alternatives specifically optimized for data warehouse performance. For high selectivity queries, the plans use nested loops joins and seeks. Medium selectivity queries in turn rely on right-deep hash joins with bitmap filters. Bitmap filters perform semi-join reductions to efficiently prune out non-qualifying rows early. Final plan choice is left for cost-based optimization which also compares the data warehouse specific plans against conventional query plans. We conducted an extensive experimental investigation using both synthetic workloads and several customer workloads. As our results show, the new plan shapes and execution strategies yield significant performance improvements across the targeted workloads as compared to earlier versions of Microsoft SQL Server.

#index 1206788
#* Integration of Server, Storage and Database Stack: Moving Processing Towards Data
#@ Lin Qiao;Vijayshankar Raman;Inderpal Narang;Prashant Pandey;David Chambliss;Gene Fuh;James Ruddy;Ying-Lin Chen;Kou-Horng Yang;Fen-Ling Lin
#t 2008
#c 17
#! Storage architecture includes more and more processing power for increasing requirement of reliability, managibility and scalability. For example, an IBM storage server is equipped with 4 or 8 state-of-the-art processors and gigabytes of memories. This trend enables analyzing data locally inside a storage server. Processing data locally is appealing under the following circumstances: 1) huge reduction of data flowing to the host, 2) reduction of CPU consumption on host. Accordingly, the benefits are 1) less data traffic through IO channel to the host, 2) better utilization of host bufferpool, and 3) enabling more workload on the host. One crucial task is to understand how DBMS can benefit from such hardware. That is to identify which database operations are beneficial to be offloaded given a query workload in a particular setting. For certain operations, we establish value proposition via various approaches and show the analytical and experimental results. In particular, starjoin queries are commonly used in business warehouses. We propose to offload a portion of a starjoin query from host to the POWER5 P processors on a storage server, which dramatically reduces the amount of channel IO and host CPU consumption. Moreover, the query elapsed time is improved via the exploitation of the state-of-the-art P processors on a storage server.

#index 1206789
#* Accurate and Efficient Inter-Transaction Dependency Tracking
#@ Tzi-cker Chiueh;Shweta Bajpai
#t 2008
#c 17
#! A reparable database management system has the ability to automatically undo the set of transactions that are corrupted by a human error or malicious attack. The key technical challenge to building repairable database management systems is how to accurately and efficiently keep track of inter-transaction dependencies due to data sharing through a database or through an application. In this paper, we present the design, implementation and evaluation of the inter-transaction dependency tracking mechanisms used in a repairable database management system called Blastema, which adds fast repairability in a portable way to a commercial DBMS, Oracle 9.2.0. Compared with other repairable DBMSs, Blastema eliminates false positive dependencies using fine-grained inter-transaction dependency tracking, and is able to successfully recognize two major sources of false negative dependencies, phantom dependencies and dependencies among transactions due to application data sharing. With these advanced inter-transaction dependency tracking mechanisms, Blastema significantly improves the availability of modern DBMSs by facilitating and sometimes even automating the damage repair process after a human error or a malicious attack. Performance measurements on a fully operational Blastema prototype run under the TPC-C benchmark show that the average run-time throughput penalty of the proposed inter-transaction dependency tracking mechanisms is less than 18%.

#index 1206790
#* Fuzzy Multi-Dimensional Search in the Wayfinder File System
#@ Christopher Peery;Wei Wang;Amelie Marian;Thu D. Nguyen
#t 2008
#c 17
#! With the explosion in the amount of semi-structured data users access and store, there is a need for complex search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools usually index text content, allowing for some IR-style ranking on the textual part of the query, but only consider structure (e.g., file directory) and metadata (e.g., date, file type) as filtering conditions. We propose a novel multi-dimensional querying approach to semi-structured data searches in personal information systems by allowing users to provide fuzzy structure and metadata conditions in addition to traditional keyword conditions. The provided query interface is more comprehensive than content-only searches as it considers three query dimensions (content, structure, metadata) in the search. We have implemented our proposed approach in the Wayfinder file system. In this demo, we will use this implementation to both present an overview of the unified scoring framework underlying the fuzzy multi-dimensional querying approach and demonstrate its potential in improving search results.

#index 1206791
#* DIPBench Toolsuite: A Framework for Benchmarking Integration Systems
#@ Matthias Bohm;Dirk Habich;Wolfgang Lehner;Uwe Wloka
#t 2008
#c 17
#! So far the optimization of integration processes between heterogeneous data sources is still an open challenge. A first step towards sufficient techniques was the specification of a universal benchmark for integration systems. This DIPBench allows to compare solutions under controlled conditions and would help generate interest in this research area. However, we see the requirement for providing a sophisticated toolsuite in order to minimize the effort for benchmark execution. This demo illustrates the use of the DIPBench toolsuite. We show the macro-architecture as well as the micro-architecture of each tool. Furthermore, we also present the first reference benchmark implementation using a federated DBMS. Thereby, we discuss the impact of the defined benchmark scale factors. Finally, we want to give guidance on how to benchmark other integration systems and how to extend the toolsuite with new distribution functions or other functionalities.

#index 1206792
#* Managing Biological Data using bdbms
#@ Mohamed Y. Eltabakh;Mourad Ouzzani;Walid G. Aref;Ahmed K. Elmagarmid;Yasin Laura-Silva;Muhammad U. Arshad;David Salt;Ivan Baxter
#t 2008
#c 17
#! We demonstrate bdbms, an extensible database engine for biological databases. bdbms started on the observation that database technology has not kept pace with the specific requirements of biological databases and that several needed key functionalities are not supported at the engine level. While bdbms aims at supporting several of these functionalities, this demo focuses on: (1) Annotation and provenance management including storage, indexing, querying, and propagation, (2) Local dependency tracking of dependencies and derivations among data items, and (3) Update authorization to support data curation. We demonstrate how bdbms enables biologists to manipulate their databases, annotations, and derivation information in a unified database system using the Purdue Ionomics Information Management System (PiiMS) as a case study.

#index 1206793
#* Mining Gene Expression Database for Primary Human Disease Tissues
#@ Andrew Campen;Yuni Xia;Dan Rigsby;Ying Guo;Xingdong Feng;Eric W. Su;Mathew Palakal;Shuyu Li
#t 2008
#c 17
#! Studies of gene expression in primary human disease tissue often span several years in order to achieve reasonably large sample sizes and to collect patient clinical information making this data particularly valuable. Due to the lack of a central repository, this data has only been available through disparate and non-publicly accessible sources following publication. We developed Disease-to-Gene Expression Mapper (D-GEM) as a publically accessible database and data mining toolbox for microarray data of human primary disease tissue. A statistical pipeline has also been implemented to identify genes over-expressed in disease tissue samples in comparison with normal control samples, or genes whose expression values are associated with clinical parameters such as patient survival rate. One potential application of this data is the identification of pathway specific cancer prognosis markers. By applying a novel, gene signatures for cancer prognosis in the context of known biological pathways in cancer development were identified and confirmed.

#index 1206794
#* Mining Views: Database Views for Data Mining
#@ Hendrik Blockeel;Toon Calders;Elisa Fromont;Bart Goethals;Adriana Prado
#t 2008
#c 17
#! We present a system towards the integration of data mining into relational databases. To this end, a relational database model is proposed, based on the so called virtual mining views. We show that several types of patterns and models over the data, such as itemsets, association rules and decision trees, can be represented and queried using a unifying framework.

#index 1206795
#* PictureBook: A Text-and-Image Summary System for Web Search Result
#@ Mei Wang;Hongtao Xu;Guoyu Hao;Xiangdong Zhou;Wei Wang;Qi Zhang;Baile Shi
#t 2008
#c 17
#! Search engine technology plays an important role in Web information retrieval. However, with Internet information explosion, traditional searching techniques cannot provide satisfactory result due to problems such as huge number of result Web pages, unintuitive ranking, etc. Therefore, the reorganization and post-processing of Web search results have been extensively studied to help user effectively obtain useful information. Previous studies mainly focused on Web page clustering, document summary, visualization of search results, etc, which are applied separately to either text or image search. In this paper, we propose a demo to illustrate a new Web search result summary system - PictureBook, which combines text and image retrieval using techniques of multiple document summarization and image semantics analysis. Particularly, audience can interactively investigate the effect of the combined text and image summary in Web information searching and knowledge acquisition. We also introduce our new image semantic analysis method based on generalized discriminant analysis(GDA).

#index 1206796
#* XML processing in DHT networks
#@ Serge Abiteboul;Ioana Manolescu;Neoklis Polyzotis;Nicoleta Preda;Chong Sun
#t 2008
#c 17
#! We study the scalable management of XML data in P2P networks based on distributed hash tables (DHTs). We identify performance limitations in this context, and propose an array of techniques to lift them. First, we adapt the DHT platform's index store and communication primitives to the needs of massive data processing. Second, we introduce a distributed hierarchical index and associated efficient algorithms to speed up query processing. Third, we present an innovative, XML-specific flavor of Bloom filters, to reduce data transfers entailed by query processing. Our approach is fully implemented in the KadoP system, used in a real-life software manufacturing application. Our experiments demonstrate the benefits of the proposed techniques.

#index 1206797
#* Efficient Processing of XML Update Streams
#@ Leonidas Fegaras
#t 2008
#c 17
#! This paper introduces a framework for processing continuous, exact queries over continuous update XML streams. Instead of eagerly performing the updates on cached portions of the stream, we propagate the updates through the query evaluation pipeline, all the way to the result display, which prints the query answers. That way, the result display prints the query results continuously, replacing old results with new. The novelty of our approach is in the use of this processing framework to unblock operations and reduce buffering by letting the operations themselves embed new updates into the stream that retroactively perform the blocking parts of the operation. Based on this framework, we present novel methods for unblocking a number of important blocking/unbounded stream operations in XQuery using a small memory footprint, such as concatenation, general predicates, descendant-or-self and backward axes, and sorting.

#index 1206798
#* XML Prefiltering as a String Matching Problem
#@ Christoph Koch;Stefanie Scherzinger;Michael Schmidt
#t 2008
#c 17
#! We propose a new technique for the efficient search and navigation in XML documents and streams. This technique takes string matching algorithms designed for efficient search in flat strings into the second dimension to navigate in tree structured data. We consider the important XML data management task of prefiltering XML documents (also called XML projection) as an application for our approach. Different from existing prefiltering schemes we usually process only fractions of the input and get by with very economical consumption of both main memory and processing time. Our experiments reveal that already on low-complexity problems such as XPath filtering inmemory query engines can experience speed-ups by two orders of magnitude.

#index 1206799
#* Optimizing SQL Queries over Text Databases
#@ Alpa Jain;AnHai Doan;Luis Gravano
#t 2008
#c 17
#! Text documents often embed data that is structured in nature, and we can expose this structured data using information extraction technology. By processing a text database with information extraction systems, we can materialize a variety of structured "relations," over which we can then issue regular SQL queries. A key challenge to process SQL queries in this text-based scenario is efficiency: information extraction is time-consuming, so query processing strategies should minimize the number of documents that they process. Another key challenge is result quality: in the traditional relational world, all correct execution strategies for a SQL query produce the same (correct) result; in contrast, a SQL query execution over a text database might produce answers that are not fully accurate or complete, for a number of reasons. To address these challenges, we study a family of select-project-join SQL queries over text databases, and characterize query processing strategies on their efficiency and - critically - on their result quality as well. We optimize the execution of SQL queries over text databases in a principled, cost-based manner, incorporating this tradeoff between efficiency and result quality in a user-specific fashion. Our large-scale experiments- over real data sets and multiple information extraction systems - show that our SQL query processing approach consistently picks appropriate execution strategies for the desired balance between efficiency and result quality.

#index 1206800
#* Building Community Wikipedias: A Machine-Human Partnership Approach
#@ Pedro DeRose;Xiaoyong Chai;Byron J. Gao;Warren Shen;AnHai Doan;Philip Bohannon;Xiaojin Zhu
#t 2008
#c 17
#! The rapid growth of Web communities has motivated many solutions for building community data portals. These solutions follow roughly two approaches. The first approach (e.g., Libra, Citeseer, Cimple) employs semi-automatic methods to extract and integrate data from a multitude of data sources. The second approach (e.g., Wikipedia, Intellipedia) deploys an initial portal in wiki format, then invites community members to revise and add material. In this paper we consider combining the above two approaches to building community portals. The new hybrid machine-human approach brings significant benefits. It can achieve broader and deeper coverage, provide more incentives for users to contribute, and keep the portal more up-to-date with less user effort. In a sense, it enables building "community wikipedias", backed by an underlying structured database that is continuously updated using automatic techniques. We outline our ideas for the new approach, describe its challenges and opportunities, and provide initial solutions. Finally, we describe a real-world implementation and preliminary experiments that demonstrate the utility of the new approach.

#index 1206801
#* Keyword Search on Spatial Databases
#@ Ian De Felipe;Vagelis Hristidis;Naphtali Rishe
#t 2008
#c 17
#! Many applications require finding objects closest to a specified location that contains a set of keywords. For example online yellow pages allow users to specify an address and a set of keywords. In return the user obtains a list of businesses whose description contains these keywords ordered by their distance from the specified address. The problems of nearest neighbor search on spatial data and keyword search on text data have been extensively studied separately. However to the best of our knowledge there is no efficient method to answer spatial keyword queries that is queries that specify both a location and a set of keywords. In this work we present an efficient method to answer top-k spatial keyword queries. To do so we introduce an indexing structure called IR2-Tree (Information Retrieval R-Tree) which combines an R-Tree with superimposed text signatures. We present algorithms that construct and maintain an IR2-Tree and use it to answer top-k spatial keyword queries. Our algorithms are experimentally compared to current methods and are shown to have superior performance and excellent scalability.

#index 1206802
#* Native Web Browser Enabled SVG-based Collaborative Multimedia Annotation for Medical Images
#@ Fusheng Wang;Cornelius Rabsch;Peiya Liu
#t 2008
#c 17
#! Image annotation becomes increasingly important for clinical applications and medical research. In particular, collaborative image annotations can harness the collective intelligence from distributed experts. There are several challenges to support collaborative medical image annotations: i) Medical image annotation includes not only metadata annotation but also multimedia annotation, such as graphical annotation. The latter often requires a heavy-duty tool, which can be difficult to realize in a distributed environment; ii) Annotations need to be well modeled for easy exchange and support of queries, i.e., there is a gap between image annotation and content retrieval; and iii) An annotation platform is needed to provide authoring tools and the collaborative infrastructure. Meanwhile, the Web is evolving quickly on supporting interaction, participation and collaboration enabled by Web 2.0 technologies. Among them, Scalable Vector Graphics (SVG) now becomes a standard language for vector graphics on the Web natively supported by latest Web browsers. In our work, we develop a collaborative image annotation platform, which provides: i) a flexible data model to support both metadata and multimedia annotations on 2-D medical images; ii) SVG based implementation of the data model that can support complex textual, spatial, and collaborative queries on annotations with XQuery; iii) a lightweight native Web browser enabled annotation authoring tool without any plugin needed; and iv) an architecture that provides authoring, storing, querying, and exchanging of annotations, and supports Web-based collaboration.

#index 1206803
#* XQSE: An XQuery Scripting Extension for the AquaLogic Data Services Platform
#@ Vinayak Borkar;Michael Carey;Daniel Engovatov;Dmitry Lychagin;Till Westmann;Warren Wong
#t 2008
#c 17
#! The AquaLogic Data Services Platform (ALDSP) is a BEA middleware platform for creating services that access and manipulate information drawn from multiple heterogeneous sources of data. The integration logic for read services is specified declaratively using the XQuery language. ALDSP 3.0, available in December 2007, includes a new XQuery-based Scripting Extension - XQSE - that enables developers to write procedural as well as declarative logic without leaving the XQuery world. In this paper, we describe the XQSE extensions to XQuery and show how they help to support important new classes of data services in ALDSP 3.0.

#index 1206804
#* Implementing an Inference Engine for RDFS/OWL Constructs and User-Defined Rules in Oracle
#@ Zhe Wu;George Eadon;Souripriya Das;Eugene Inseok Chong;Vladimir Kolovski;Melliyal Annamalai;Jagannathan Srinivasan
#t 2008
#c 17
#! This Inference Engines are an integral part of Semantic Data Stores. In this paper, we describe our experience of implementing a scalable inference engine for Oracle Semantic Data Store. This inference engine computes production rule based entailment of one or more RDFS/OWL encoded semantic data models. The inference engine capabilities include i) inferencing based on semantics of RDFS/OWL constructs and user-defined rules, ii) computing ancillary information (namely, semantic distance and proof) for inferred triples, and iii) validation of semantic data model based on RDFS/OWL semantics. A unique aspect of our approach is that the inference engine is implemented entirely as a database application on top of Oracle Database. The paper describes the inferencing requirements, challenges in supporting a sufficiently expressive set of RDFS/OWL constructs, and techniques adopted to build a scalable inference engine. A performance study conducted using both native and synthesized semantic datasets demonstrates the effectiveness of our approach.

#index 1206805
#* Towards Declarative Queries on Adaptive Data Structures
#@ Nicolas Bruno;Pablo Castro
#t 2008
#c 17
#! In this work we look at combining emerging technologies in programming languages with traditional query processing techniques to provide support for efficient execution of declarative queries over adaptive data structures. We first explore available technologies such as Language-Integrated Query, or LINQ (which enables declarative queries in programming languages) and the ADO.NET DataSet classes (which provide various efficient alternatives to manipulate data in procedural terms). Unfortunately, combining the good features in both technologies is not straightforward, since LINQ over DataSets results by default in execution plans that do not exploit the specific characteristics of the data structures. To address this limitation, we introduce a lightweight optimizer that dynamically chooses appropriate execution strategies for declarative queries on DataSets based on their internal structure. To further enable declarative programming, we introduce a component that dynamically reorganizes the internal representation of DataSets, so that they automatically respond to workload changes. We experimentally showcase the features of our approach.

#index 1206806
#* Filtered Indices and Their Use in Flexible Schema Scenarios
#@ Srini Acharya;Cesar Galindo-Legaria;Milind Joshi;Babu Krishnaswamy;Stefano Stefani;Pawel Terlecki
#t 2008
#c 17
#! Efficient and convenient handling of heterogeneous data is a current challenge for data management systems. In this paper, we discuss several common relational approaches to represent heterogeneity and argue for a design based on a single wide-table, referred to as a flexible schema. For this scenario, we focus on partial indexation and its support for efficient data storage and processing. Filtered indices provide partial indexation functionality in the Microsoft SQL Server product. We describe here the implementation of this feature, including index utilization in queries, index maintenance and query parameterization issues. Our performance experiments validate the expected benefits of the approach in our implementation.

#index 1206807
#* An Overview of SQL Support in Workflow Products
#@ Marko Vrhovnik;Holger Schwarz;Sylvia Radeschutz;Bernhard Mitschang
#t 2008
#c 17
#! Over the last years, data management products as well as workflow products have established themselves as indispensable building blocks for advanced IT systems in almost all application areas. Recently, many vendors have created innovative product extensions that combine service-oriented frameworks with powerful workflow and data management capabilities. In this paper, we discuss several workflow products from different vendors with a specific focus on their SQL support. We provide a comparison based on a set of important data management patterns and illustrate the characteristics of various approaches by means of a running example.

#index 1206808
#* Enhanced Business Intelligence using EROCS
#@ M. Bhide;V. Chakravarthy;A. Gupta;H. Gupta;M. Mohania;K. Puniyani;P. Roy;S. Roy;V. Sengar
#t 2008
#c 17
#! The EROCS technology automatically links unstructured data with relevant structured data from an external relational database. We demonstrate how EROCS can be used for enhancing Business Intelligence by allowing OLAP tools to analyze structured and unstructured data in a consolidated manner. Our demonstration showcases the use of EROCS in exploiting latent information in customer emails, which helps in building a complete view of the customer. This results in new insights about the business which are not possible with the existing state of the art.

#index 1206809
#* T-Time: Threshold-Based Data Mining on Time Series
#@ Johannes ABfalg;Hans-Peter Kriegel;Peer Kroger;Peter Kunath;Alexey Pryakhin;Matthias Renz
#t 2008
#c 17
#! Mining time series data is an important approach for the analysis in many application areas as diverse as biology, environmental research, medicine, or stock chart analysis. As nearly all data mining tasks on this kind of data depend on a distance function between two time series, a huge number of such functions has been developed during the last decades. The introduction of threshold-based distance functions presented a new concept of time series similarity and these functions were applied to data mining techniques on a wide spectrum of time series data. In this demonstration, we present the Java toolkit T-Time which is able to perform several data mining tasks for a complete range of threshold values in an interactive way. The results are visually presented in a very concise way so that the user can easily identify important threshold values. Combined with domain-specific knowledge, these pivotal values can yield novel insights beyond the means of the underlying data mining techniques the analysis is based on.

#index 1206810
#* RAD: A Scalable Framework for Annotator Development
#@ Sanjeet Khaitan;Ganesh Ramakrishnan;Sachindra Joshi;Anup Chalamalla
#t 2008
#c 17
#! Developments in semantic search technology have motivated the need for efficient and scalable entity annotation techniques. We demonstrate RAD: a tool for Rapid Annotator Development on a document collection. RAD builds on a recent approach [1] that translates entity annotation rules into equivalent operations on the inverted index of the collection, to directly generate an annotation index (which can be used in search applications). To make the framework scalable, we use an industrial strength indexer, Lucene [2] and introduce some modifications to its API. The index also serves as a suitable representation for making quick comparisons with an indexed ground truth of annotations on the same collection to evaluate precision and recall of the annotations. RAD achieves at least an order of magnitude speedup over the standard approach of annotating a document-at-a-time as adopted by GATE [3]. The speedup factor increases with increase in the size of the collection, making RAD scalable. We cache intermediate results from the index operations, enabling quick update of the annotation index as well as speedy evaluation when rules are modified. This makes RAD suitable for rapid and interactive development of annotators.

#index 1206811
#* SPOT: A System for Detecting Projected Outliers From High-dimensional Data Streams
#@ Ji Zhang;Qigang Gao;Hai Wang
#t 2008
#c 17
#! In this paper, we present a new technique, called Stream Projected Ouliter deTector (SPOT), to deal with outlier detection problem in high-dimensional data streams. SPOT is unique in a number of aspects. First, SPOT employs a novel window-based time model and decaying cell summaries to capture statistics from the data stream. Second, Sparse Scubspace Template (SST), a set of top sparse subspaces obtained by unsupervised and/or supervised learning processes, is constructed in SPOT to detect projected outliers effectively. Multi-Objective Genetic Algorithm (MOGA) is employed as an effective search method in unsupervised learning for finding outlying subspaces from training data. Finally, SST is able to carry out online self-evolution to cope with dynamics of data streams. This paper provides details on the motivation and technical challenges of detecting outliers from high-dimensional data streams, present an overview of SPOT, and give the plans for system demonstration of SPOT.

#index 1206812
#* Why Can't I Find My Data the Way I Find My Dinner?
#@ David Carlson
#t 2009
#c 17
#! The success of the International Polar Year, particularly its unprecedented breadth of science, shines a bright light on the challenges and opportunities of modern information technology. IPY researchers produce data ranging from genetic sequences to cosmic neutrino energy levels, over a broad range of time and space scales, in disparate coordinate systems, and with varied traditions and experience of data sharing and data management. We can not say with confidence where these invaluable data will reside for use by future generations of polar and planetary researchers. Meanwhile, smart search engines, pattern recognition and data mining tools, multi-gigabyte personal storage devices, and advanced animation capabilities, coupled with almost unlimited mobile bandwidth, offer expansive and amazing information access to many citizens. However, this array of access technology remains certainly beyond the means, and often not even in the plans, of most scientific data centres. What changes in strategy, funding and individual and collective behaviour need to occur to allow me to browse, view and access IPY data on my iTouch?

#index 1206813
#* Data Management in the Cloud
#@ Raghu Ramakrishnan
#t 2009
#c 17
#! We are in the midst of a computing revolution. As the cost of provisioning hardware and software stacks grows, and the cost of securing and administering these complex systems grows even faster, we're seeing a shift towards computing clouds. Clouds are essentially services accessed over a network, and offer developers scalable, robust computing infrastructure on a "pay as you go" basis, with the ability to dynamically adjust the amount of "rented" resources, and thereby, the bill. For cloud service providers, there is efficiency from amortizing costs and averaging usage peaks. Internet portals like Yahoo! have long offered application services, such as email for individuals and organizations. Companies are now offering services such as storage and compute cycles, enabling higher-level services to be built on top. In this talk, I will discuss Yahoo!'s vision of cloud computing, and describe some of the key initiatives, highlighting the technical challenges involved in designing hosted, multi-tenanted data management systems.

#index 1206814
#* Modeling and Integrating Background Knowledge in Data Anonymization
#@ Tiancheng Li;Ninghui Li;Jian Zhang
#t 2009
#c 17
#! Recent work has shown the importance of considering the adversary's background knowledge when reasoning about privacy in data publishing. However, it is very difficult for the data publisher to know exactly the adversary's background knowledge. Existing work cannot satisfactorily model background knowledge and reason about privacy in the presence of such knowledge.This paper presents a general framework for modeling the adversary's background knowledge using kernel estimation methods. This framework subsumes different types of knowledge (e.g., negative association rules) that can be mined from the data. Under this framework, we reason about privacy using Bayesian inference techniques and propose the skyline (B, t)-privacy model, which allows the data publisher to enforce privacy requirements to protect the data against adversaries with different levels of background knowledge. Through an extensive set of experiments, we show the effects of probabilistic background knowledge in data anonymization and the effectiveness of our approach in both privacy protection and utility preservation.

#index 1206815
#* Deriving Private Information from Association Rule Mining Results
#@ Zutao Zhu;Guan Wang;Wenliang Du
#t 2009
#c 17
#! Data publishing can provide enormous benefits to the society. However, due to privacy concerns, data cannot be published in their original forms. Two types of data publishing can address the privacy issue: one is to publish the sanitized version of the original data, and the other is to publish the aggregate information from the original data, such as data mining results. There have been extensive studies to understand the privacy consequence in the first approach, but there is not much investigation on the privacy consequence of publishing data mining results, although, it is well believed that publishing data mining results can lead to the disclosure of private information. We propose a systematic method to study the privacy consequence of data mining results. Based on a well-established theory, the principle of maximum entropy, we have developed a method to precisely quantify the privacy risk when data mining results are published. We take the association rule mining as an example in this paper, and demonstrate how we quantify the privacy risk based on the published association rules. We have conducted experiments to evaluate the effectiveness and performance of our method. We have drawn several interesting observations from our experiments.

#index 1206816
#* Light-Weight, Runtime Verification of Query Sources
#@ Tingjian Ge;Stan Zdonik
#t 2009
#c 17
#! Modern database systems increasingly make use of networked storage. This storage can be in the form of SAN's or in the form of shared-nothing nodes in a cluster. One type of attack on databases is arbitrary modification of data in a database through the file system, bypassing database access control. Additionally, for many applications, ensuring strict and definite authenticity of query source and results is required or highly desirable. In this paper, we propose a lightweight approach for verifying the minimum information that a database server needs from the storage system to execute a query. The verification is definite and produces high confidence results because of its online manner (i.e., the information is verified right before it is used). It is lightweight in three ways: (1) We use the Merkle hash tree data structure and fast cryptographic hash functions to ensure the verification itself is fast and secure; (2) We verify the minimum number of bytes needed to ensure the authenticity of the source related to the query result; and (3) We achieve high concurrency of multiple reader and writer transactions and avoid delays due to locking by using the compare-and-swap primitive. We then prove the correctness and progress guarantees of the algorithms using concepts from the theory of distributed computing. We also analyze the performance of the algorithm. Finally, we perform a comprehensive empirical study on various parameter choices and on the system performance and concurrency with our approaches.

#index 1206817
#* STAR: Steiner-Tree Approximation in Relationship Graphs
#@ Gjergji Kasneci;Maya Ramanath;Mauro Sozio;Fabian M. Suchanek;Gerhard Weikum
#t 2009
#c 17
#! Large graphs and networks are abundant in modern information systems: entity-relationship graphs over relational data or Web-extracted entities, biological networks, social online communities, knowledge bases, and many more. Often such data comes with expressive node and edge labels that allow an interpretation as a semantic graph, and edge weights that reflect the strengths of semantic relations between entities. Finding close relationships between a given set of two, three, or more entities is an important building block for many search, ranking, and analysis tasks. From an algorithmic point of view, this translates into computing the best Steiner trees between the given nodes, a classical NP-hard problem. In this paper, we present a new approximation algorithm, coined STAR, for relationship queries over large relationship graphs. We prove that for n query entities, STAR yields an O(log(n))-approximation of the optimal Steiner tree in pseudopolynomial run-time, and show that in practical cases the results returned by STAR are qualitatively comparable to or even better than the results returned by a classical 2-approximation algorithm. We then describe an extension to our algorithm to return the top-k Steiner trees. Finally, we evaluate our algorithm over both main-memory as well as completely diskresident graphs containing millions of nodes. Our experiments show that in terms of efficiency STAR outperforms the best state-of-the-art database methods by a large margin, and also returns qualitatively better results.

#index 1206818
#* A Latent Topic Model for Complete Entity Resolution
#@ Liangcai Shu;Bo Long;Weiyi Meng
#t 2009
#c 17
#! In bibliographies like DBLP and Citeseer, there are three kinds of entity-name problems that need to be solved. First, multiple entities share one name, which is called the name sharing problem. Second, one entity has different names, which is called the name variant problem. Third, multiple entities share multiple names, which is called the name mixing problem. We aim to solve these problems based on one model in this paper. We call this task complete entity resolution. Different from previous work, our work use global information based on data with two types of information, words and author names. We propose a generative latent topic model that involves both author names and words — the LDA-dual model, by extending the LDA (Latent Dirichlet Allocation) model. We also propose a method to obtain model parameters that is global information. Based on obtained model parameters, we propose two algorithms to solve the three problems mentioned above. Experimental results demonstrate the effectiveness and great potential of the proposed model and algorithms.

#index 1206819
#* Distance-Based Representative Skyline
#@ Yufei Tao;Ling Ding;Xuemin Lin;Jian Pei
#t 2009
#c 17
#! Given an integer $k$, a {\em representative skyline} contains the $k$ skyline points that best describe the tradeoffs among different dimensions offered by the full skyline. Although this topic has been previously studied, the existing solution may sometimes produce $k$ points that appear in an arbitrarily tiny cluster, and therefore, fail to be representative. Motivated by this, we propose a new definition of representative skyline that minimizes the distance between a non-representative skyline point and its nearest representative. We also study algorithms for computing distance-based representative skylines. In 2D space, there is a dynamic programming algorithm that guarantees the optimal solution. For dimensionality at least 3, we prove that the problem is NP-hard, and give a 2-approximate polynomial time algorithm. Using a multidimensional access method, our algorithm can directly report the representative skyline, without retrieving the full skyline. We show that our representative skyline not only better captures the contour of the entire skyline than the previous method, but also can be computed much faster.

#index 1206820
#* Similarity Group-By
#@ Yasin N. Silva;Walid G. Aref;Mohamed H. Ali
#t 2009
#c 17
#! Group-by is a core database operation that is used extensively in OLTP, OLAP, and decision support systems. In many application scenarios, it is required to group similar but not necessarily equal values. In this paper we propose a new SQL construct that supports similarity-based Group-by (SGB). SGB is not a new clustering algorithm, but rather is a practical and fast similarity grouping query operator that is compatible with other SQL operators and can be combined with them to answer similarity-based queries efficiently. In contrast to expensive clustering algorithms, the proposed similarity group-by operator maintains low execution times while still generating meaningful groupings that address many application needs. The paper presents a general definition of the similarity group-by operation and gives three instances of this definition. The paper also discusses how optimization techniques for the regular group-by can be extended to the case of SGB. The proposed operators are implemented inside PostgreSQL. The performance study shows that the proposed similarity-based group-by operators have good scalability properties with at most only 25% increase in execution time over the regular group-by.

#index 1206821
#* Top-k Set Similarity Joins
#@ Chuan Xiao;Wei Wang;Xuemin Lin;Haichuan Shang
#t 2009
#c 17
#! Similarity join is a useful primitive operation underlying many applications, such as near duplicate Web page detection, data integration, and pattern recognition. Traditional similarity joins require a user to specify a similarity threshold. In this paper, we study a variant of the similarity join, termed top-k set similarity join. It returns the top-k pairs of records ranked by their similarities, thus eliminating the guess work users have to perform when the similarity threshold is unknown before hand. An algorithm, topk-join, is proposed to answer top-k similarity join efficiently. It is based on the prefix filtering principle and employs tight upper bounding of similarity values of unseen pairs. Experimental results demonstrate the efficiency of the proposed algorithm on large-scale real datasets.

#index 1206822
#* Finding Actionable Knowledge via Automated Comparison
#@ Lei Zhang;Bing Liu;Jeffrey Benkler;Chi Zhou
#t 2009
#c 17
#! The problem of finding interesting and actionable patterns is a major challenge in data mining. It has been studied by many data mining researchers. The issue is that data mining algorithms often generate too many patterns, which make it very hard for the user to find those truly useful ones. Over the years many techniques have been proposed. However, few have made it to real-life applications. At the end of 2005, we built a data mining system for Motorola (called Opportunity Map) to enable the user to explore the space of a large number of rules in order to find actionable knowledge. The approach is based on the concept of rule cubes and operations on rule cubes. A rule cube is similar to a data cube, but stores rules. Since its deployment, some issues have also been identified during the regular use of the system in Motorola. One of the key issues is that although the operations on rule cubes are flexible, each operation is primitive and has to be initiated by the user. Finding a piece of actionable knowledge typically involves many operations and intense visual inspections, which are labor-intensive and time-consuming. From interactions with our users, we identified a generic problem that is crucial for finding actionable knowledge. The problem involves extensive comparison of sub-populations and identification of the cause of their differences. This paper first defines the problem and then proposes an effective method to solve the problem automatically. To the best of our knowledge, there is no reported study of this problem. The new method has been added to the Opportunity Map system and is now in daily use in Motorola.

#index 1206823
#* Updates in the AquaLogic Data Services Platform
#@ Michael Blow;Vinayak Borkar;Michael Carey;Christopher Hillery;Alexander Kotopoulis;Dmitry Lychagin;Radu Preotiuc-Pietro;Panagiotis Reveliotis;Joshua Spiegel;Till Westmann
#t 2009
#c 17
#! The BEA AquaLogic Data Services Platform (ALDSP) is a middleware platform for creating services that integrate and manipulate information from disparate enterprise data sources. This paper provides a technical overview of the all-new update support in ALDSP 3.0, released in January 2008. It describes the update side of data services, our unique model for making update automation transparent and flexible, and the use of the XQuery Scripting Extension (XQSE) for further customizing the system's default handling of updates. It also gives an overview of the ALDSP update processing machinery, including the automatic generation of update maps from read functions, translation of update maps into Update Virtual Machine (UVM) programs, the UVM instruction interpreter, and SQL generation for updates to data drawn from relational data sources.

#index 1206824
#* Web Query Recommendation via Sequential Query Prediction
#@ Qi He;Daxin Jiang;Zhen Liao;Steven C.  H. Hoi;Kuiyu Chang;Ee-Peng Lim;Hang Li
#t 2009
#c 17
#! Web query recommendation has long been considered a key feature of search engines. Building a good Web query recommendation system, however, is very difficult due to the fundamental challenge of predicting users' search intent, especially given the limited user context information. In this paper, we propose a novel "sequential query prediction" approach that tries to grasp a user's search intent based on his/her past query sequence and its resemblance to historical query sequence models mined from massive search engine logs. Different query sequence models were examined, including the naive variable length N-gram model, Variable Memory Markov (VMM) model, and our proposed Mixture Variable Memory Markov (MVMM) model. Extensive experiments were conducted to benchmark our sequence prediction algorithms against two conventional pairwise approaches on large-scale search logs extracted from a commercial search engine. Results show that the sequence-wise approaches significantly outperform the conventional pair-wise ones in terms of prediction accuracy. In particular, our MVMM approach, consistently leads the pack, making it an effective and practical approach towards Web query recommendation.

#index 1206825
#* rFEED: A Mixed Workload Scheduler for Enterprise Data Warehouses
#@ Abhay Mehta;Chetan Gupta;Song Wang;Umesh Dayal
#t 2009
#c 17
#! A typical online Business Intelligence (BI) workload consists of a combination of short, less intensive queries, along with long, resource intensive queries. As such, the longest queries in a typical BI workload may take several orders of magnitude more time to execute, compared with the shortest queries in the workload. This makes it challenging to design a good Mixed Workload Scheduler (MWS). In this paper we first define the design criteria that make a 'good' MWS. We then use these criteria to design rFEED, a MWS that is fair, effective, efficient, and differentiated. We simulate real workloads and compare our rFEED MWS with models of the current best of breed commercial systems. We show that the rFEED MWS works extremely well.

#index 1206826
#* An Internet-Scale Service for Publishing and Locating XML Documents
#@ Praveen Rao;Bongki Moon
#t 2009
#c 17
#! In recent years, there has been a growing interest for peer-to-peer (P2P) based computing and applications. One of the most important challenges in P2P environments is to quickly locate relevant data across many participating peers. In this demonstration, we present psiX, which is an Internet-scale service for publishing and locating XML documents. This service runs on several PlanetLab nodes geographically spread across the globe. The psiX system adopts a suite of new techniques for XML indexing and pattern matching in a P2P network, namely, (a) representing XML documents and XPath queries compactly via algebraic signatures, (b) searching signatures of documents and value summaries indexed using distributed hierarchical indexesbuilt over a Distributed Hash Table (DHT), and (c) gracefully adapting to failures while running on the Internet, where failures are a norm rather than an exception.

#index 1206827
#* DynaCet: Building Dynamic Faceted Search Systems over Databases
#@ Senjuti Basu Roy;Haidong Wang;Ullas Nambiar;Gautam Das;Mukesh Mohania
#t 2009
#c 17
#! Extracting information and insights from large databases is a time-consuming activity and has received considerable research attention recently. In this demo, we present DynaCet - a domain independent system that provides effective minimum-effort based dynamic faceted search solutions over enterprise databases. At every step, Dynacet suggests facets depending on the user response in the previous step. Facets are selected based on their ability to rapidly drill down to the most promising tuples, as well as on the ability of the user to provide desired values for them. The benefits provided include faster access to information stored in databases while taking into consideration the variance in user knowledge and preferences.

#index 1206828
#* Auditing a Database under Retention Restrictions
#@ Wentian Lu;Gerome Miklau
#t 2009
#c 17
#! Auditing the changes to a database is critical for identifying malicious behavior, maintaining data quality, and improving system performance. But an accurate audit log is a historical record of the past that can also pose a serious threat to privacy. Policies which limit data retention conflict with the goal of accurate auditing, and data owners have to carefully balance the need for policy compliance with the goal of accurate auditing. In this paper, we provide a framework for auditing the changes to a database system while respecting data retention policies. Our framework includes a historical data model that supports flexible audit queries, along with a language for retention policies that hide individual attribute values or remove entire tuples from history. Under retention policies, the audit history is partially incomplete. We formalize the meaning of audit queries on the protected history, which can include imprecise results. We implement policy application and query answering efficiently in a standard relational system, and characterize (both theoretically and experimentally) the cases where accurate auditing can be achieved under retention restrictions.

#index 1206829
#* ApproxRank: Estimating Rank for a Subgraph
#@ Yao Wu;Louiqa Raschid
#t 2009
#c 17
#! Customized semantic query answering, personalized search, focused crawlers and localized search engines frequently focus on ranking the pages contained within a subgraph of the global Web graph. The challenge for these applications is to compute PageRank-style scores efficiently on the subgraph, i.e., the ranking must reflect the global link structure of the Web graph but it must do so without paying the high overhead associated with a global computation. We propose a framework of an exact solution and an approximate solution for computing ranking on a subgraph.The IdealRank algorithm is an exact solution with the assumption that the scores of external pages are known. We prove that the IdealRank scores for pages in the subgraph converge. Since the PageRank-style scores of external pages may not typically be available, we propose the ApproxRank algorithm to estimate scores for the subgraph. Both IdealRank and ApproxRank represent the set of external pages with an external node $\Lambda$ and extend the subgraph with links to $\Lambda$. They also modify the PageRank-style transition matrix with respect to $\Lambda$.We analyze the $L_1$ distance between IdealRank scores and ApproxRank scores of the subgraph and show that it is within a constant factor of the $L_1$ distance of the external pages (e.g., the true PageRank scores and uniform scores assumed by ApproxRank). We compare ApproxRank and a stochastic complementation approach (SC), a current best solution for this problem, on different types of subgraphs. ApproxRank has similar or superior performance to SC and typically improves on the runtime performance of SC by an order of magnitude or better. We demonstrate that ApproxRank provides a good approximation to PageRank for a variety of subgraphs.

#index 1206830
#* BinRank: Scaling Dynamic Authority-Based Search Using Materialized SubGraphs
#@ Heasoo Hwang;Andrey Balmin;Berthold Reinwald;Erik Nijkamp
#t 2009
#c 17
#! Dynamic authority-based keyword search algorithms, such as ObjectRank and personalized PageRank, leverage semantic link information to provide high quality, high recall search in databases and the Web. Conceptually, these algorithms require a query-time PageRank-style iterative computation over the full graph. This computation is too expensive for large graphs, and not feasible at query time. Alternatively, building an index of pre-computed results for some or all keywords involves very expensive preprocessing. We introduce BinRank, a system that approximates ObjectRank results by utilizing a hybrid approach inspired by materialized views in traditional query processing. We materialize a number of relatively small subsets of the data graph in such a way that any keyword query can be answered by running ObjectRank on only one of the sub-graphs. BinRank generates the sub-graphs by partitioning all the terms in the corpus based on their co-occurrence, executing ObjectRank for each partition using the terms to generate a set of random walk starting points, and keeping only those objects that receive non-negligible scores. The intuition is that a sub-graph that contains all objects and links relevant to a set of related terms should have all the information needed to rank objects with respect to one of these terms. We demonstrate that BinRank can achieve sub-second query execution time on the English Wikipedia dataset, while producing high quality search results that closely approximate the results of ObjectRank on the original graph. The Wikipedia link graph contains about 10^8 edges, which is at least two orders of magnitude larger than what prior state of the art dynamic authority-based search systems have been able to demonstrate. Our experimental evaluation investigates the trade-off between query execution time, quality of the results, and storage requirements of BinRank.

#index 1206831
#* AJAX Crawl: Making AJAX Applications Searchable
#@ Cristian Duda;Gianni Frey;Donald Kossmann;Reto Matter;Chong Zhou
#t 2009
#c 17
#! Current search engines such as Google and Yahoo! are prevalent for searching the Web. Search on dynamic client-side Web pages is, however, either inexistent or far from perfect, and not addressed by existing work, for example on Deep Web. This is a real impediment since AJAX and Rich Internet Applications are already very common in the Web. AJAX applications are composed of states which can be seen by the user, but not by the search engine, and changed by the user using client-side events. Current search engines either ignore AJAX applications or produce false negatives. The reason is that crawling client-side code is a difficult problem that cannot be solved naively by invoking user events. The challenges are: lack of caching, duplicate states detection, very granular events, reducing the number of AJAX calls and infinite event invocation. This paper sets the stage for this new search challenge and proposes a solution: it shows how an AJAX Web application can be crawled in the granularity of the application states. A model of AJAX Web sites is presented. An AJAX Crawler and optimizations for caching and duplicate elimination are defined, and finally, the gain in search result quality and corresponding performance price are evaluated on YouTube, a real AJAX application.

#index 1206832
#* Best-Effort Top-k Query Processing Under Budgetary Constraints
#@ Michal Shmueli-Scheuer;Chen Li;Yosi Mass;Haggai Roitman;Ralf Schenkel;Gerhard Weikum
#t 2009
#c 17
#! We consider a novel problem of top-k query processing under budget constraints. We provide both a framework and a set of algorithms to address this problem. Existing algorithms for top-k processing are budget-oblivious, i.e., they do not take budget constraints into account when making scheduling decisions, but focus on the performance to compute the final top-k results. Under budget constraints, these algorithms therefore often return results that are a lot worse than the results that can be achieved with a clever, budget-aware scheduling algorithm. This paper introduces novel algorithms for budget-aware top-k processing that produce results that have a significantly higher quality than those of state-of-the-art budget-oblivious solutions.

#index 1206833
#* Aggregate Query Answering under Uncertain Schema Mappings
#@ Avigdor Gal;Maria Vanina Martinez;Gerardo I. Simari;V. S. Subrahmanian
#t 2009
#c 17
#! Recent interest in managing uncertainty in data integration has led to the introduction of probabilistic schema mappings and the use of probabilistic methods to answer queries across multiple databases using two semantics: by-table and by-tuple. In this paper, we develop three possible semantics for aggregate queries: the range, distribution, and expected value semantics, and show that these three semantics combine with the by-table and by-tuple semantics in six ways. We present algorithms to process COUNT, AVG, SUM, MIN, and MAX queries under all six semantics and develop results on the complexity of processing such queries under all six semantics. We show that computing COUNT is in PTIME for all six semantics and computing SUM is in PTIME for all but the by-tuple/distribution semantics. Finally, we show that AVG, MIN, and MAX are PTIME computable for all by-table semantics and for the by-tuple/range semantics.We developed a prototype implementation and experimented with both real-world traces and simulated data. We show that, as expected, naive processing of aggregates does not scale beyond small databases with a small number of mappings. The results also show that the polynomial time algorithms are scalable up to several million tuples as well as with a large number of mappings.

#index 1206834
#* Large-Scale Deduplication with Constraints Using Dedupalog
#@ Arvind Arasu;Christopher Ré;Dan Suciu
#t 2009
#c 17
#! We present a declarative framework for collective deduplication of entity references in the presence of constraints. Constraints occur naturally in many data cleaning domains and can improve the quality of deduplication. An example of a constraint is "each paper has a unique publication venue''; if two paper references are duplicates, then their associated conference references must be duplicates as well. Our framework supports collective deduplication, meaning that we can dedupe both paper references and conference references collectively in the example above. Our framework is based on a simple declarative Datalog-style language with precise semantics. Most previous work on deduplication either ignoreconstraints or use them in an ad-hoc domain-specific manner. We also present efficient algorithms to support the framework. Our algorithms have precise theoretical guarantees for a large subclass of our framework. We show, using a prototype implementation, that our algorithms scale to very large datasets. We provide thoroughexperimental results over real-world data demonstrating the utility of our framework for high-quality and scalable deduplication.

#index 1206835
#* Recommending Join Queries via Query Log Analysis
#@ Xiaoyan Yang;Cecilia M. Procopiuc;Divesh Srivastava
#t 2009
#c 17
#! Complex ad hoc join queries over enterprise databases are commonly used by business data analysts to understand and analyze a variety of enterprise-wide processes. However, effectively formulating such queries is a challenging task for human users, especially over databases that have large, heterogeneous schemas. In this paper, we propose a novel approach to automatically create join query recommendations based on input-output specifications (i.e.,input tables on which selection conditions are imposed, and output tables whose attribute values must be in the result of the query).The recommended join query graph includes (i) "intermediate'' tables, and (ii) join conditions that connect the input and output tables via the intermediate tables. Our method is based on analyzing an existing query log over the enterprise database. Borrowing from program slicing techniques, which extract parts of a program that affect the value of a given variable, we first extract "query slices'' from each query in the log. Given a user specification, we then re-combine appropriate slices to create a new join query graph, which connects the sets of input and output tables via the intermediate tables. We propose and study several quality measures to enable choosing a good join query graph among the many possibilities. Each measure expresses an intuitive notion that there should be sufficient evidence in the log to support our recommendation of the join query graph. We conduct an extensive study using the log of an actual enterprise database system to demonstrate the viability of our novel approach for recommending join queries.

#index 1206836
#* Resolution-Aware Query Answering for Business Intelligence
#@ Yannis Sismanis;Ling Wang;Ariel Fuxman;Peter J. Haas;Berthold Reinwald
#t 2009
#c 17
#! Entity uncertainty is an unavoidable problem in modern enterprise databases, resulting from integration of data over multiple sources. In traditional warehousing, the administrator, during an ETL process, manually and laboriously resolves inconsistent data records to discover "true'' entities(customers, products, etc.) and identify their "correct'' attribute values. At any time point, however, the current entity resolution is merely a best guess, and OLAP query results based on this resolution are inherently imprecise. We propose a new approach that maintains the data in an unresolved state, and dynamically deals with entity uncertainty at query time. We enhance the traditional OLAP model to return not a single query answer, but rather upper and lower bounds on each OLAP aggregate. This approach avoids expensive entity-resolution processing, and serves to identify potential risks when making business decisions based on the results of OLAP queries. By focusing on bounds, rather than probability distributions, we can easily and efficiently process roll-up and group-by aggregation queries over all of the core aggregation functions. Moreover, our approach can be readily implemented in an existing RDBMS using SQL queries, and does not require the user to specify explicit probabilities for alternative entity resolutions. Experiments show that the overhead of our new OLAP functionality is small over a wide range of scenarios.

#index 1206837
#* Flexible Recommendations for Course Planning
#@ Georgia Koutrika;Benjamin Bercovitz;Robert Ikeda;Filip Kaliszan;Henry Liou;Hector Garcia-Molina
#t 2009
#c 17
#! Most recommendation methods are "hard-wired" into the system and support only fixed recommendations. The purpose of this demo is to show the expressivity of flexible recommendation workflows, how flexible recommendations can be processed over relational data, and to show flexible recommendations in action through a real system used for course planning.

#index 1206838
#* GuruMine: A Pattern Mining System for Discovering Leaders and Tribes
#@ Amit Goyal;Byung-Won On;Francesco Bonchi;Laks V.  S. Lakshmanan
#t 2009
#c 17
#! In this demo we introduce GuruMine, a pattern mining system for the discovery of leaders, i.e., influential users in social networks, and their tribes, i.e., a set of users usually influenced by the same leader over several actions. GuruMine is built upon a novel pattern mining framework for leaders discovery, that we introduced in [1]. In particular, we consider social networks where users perform actions. Actions may be as simple as tagging resources (urls) as in del.icio.us, rating songs as in Yahoo! Music, or movies as in Yahoo! Movies, or users buying gadgets such as cameras, handholds, etc. and blogging a review on the gadgets. The assumption is that actions performed by a user can be seen by their network friends. Users seeing their friends actions are sometimes tempted to perform those actions. On the basis of the propagation of such influence, in [1] we provided various notion of leaders and developed algorithms for their efficient discovery. GuruMine provides users with a friendly graphical interface for selecting the actions of interest, and the kind of leaders to mine. The set of parameters driving the pattern discovery process can be iteratively refined, and the result is updated, if possible without incurring a completely new computation. Once a set of leaders has been extracted, GuruMine can easily validate them on a set of actions unseen during the pattern mining, by analyzing the portion of network reached by the influence of the selected leaders on the unseen actions. GuruMine also offers various visualizations over the social networks: the propagation of an action, the leaders, their tribes, and the interactions between different leaders and tribes. In this demo we will show: (i) how the pattern mining process can be driven towards the discovery of a good set of leaders, (ii) the ease of use of GuruMine system, and (iii) its outstanding performances on large real-world social networks and actions databases.

#index 1206839
#* I3DC: Interactive Three-Dimensional Cubes
#@ Ke Yang;Yinan Li;Qiong Luo;Pedro V. Sander;Jiaoying Shi
#t 2009
#c 17
#! We present the I3DC system prototype, which constructs the cube for tens of millions of data items within milliseconds, and provides high-quality cube visualization as well as highly-interactive OLAP operations. Our approach is based on a novel blending-as-aggregation (BAA) algorithm that maps distributive OLAP aggregations to the intrinsic rendering mechanisms of the GPU. Our system runs entirely on the GPU and requires no precomputations.

#index 1206840
#* MatchUp: Autocompletion for Mashups
#@ Serge Abiteboul;Ohad Greenshpan;Tova Milo;Neoklis Polyzotis
#t 2009
#c 17
#! A mashup is a Web application that integrates data, computation and GUI provided by several systems into a unique tool. The concept originated from the understanding that the number of applications available on the Web and the need for combining them to meet user requirements, are growing very rapidly. This demo presents MatchUp, a system that supports rapid, on-demand, intuitive development of mashups, based on a novel autocompletion mechanism. The key observation guiding the development of MatchUp is that mashups developed by different users typically share common characteristics; they use similar classes of mashup components and glue them together in a similar manner. MatchUp exploits these similarities to predict, given a user's partial mashup specification, what are the most likely potential completions (missing components and connection between them) for the specification. Using a novel ranking algorithm, users are then offered top-k completions from which they choose and refine according to their needs.

#index 1206841
#* SmallBlue: Social Network Analysis for Expertise Search and Collective Intelligence
#@ Ching-Yung Lin;Nan Cao;Shi Xia Liu;Spiros Papadimitriou;Jimeng Sun;Xifeng Yan
#t 2009
#c 17
#! SmallBlue is a social networking application that unlocks the valuable business intelligence of 'who knows what?', 'who knows whom?' and 'who knows what about whom' within an organization, without requiring explicit involvement of individuals. The aim of SmallBlue is to locate knowledgeable colleagues, communities, and knowledge networks in companies. The suite also helps users manage their personal networks, and reach out to their extended network (the friends of their friends) to find and access expertise and information.

#index 1206842
#* WEST: Modern Technologies for Web People Search
#@ Dmitri V. Kalashnikov;Zhaoqi Chen;Rabia Nuray-Turan;Sharad Mehrotra;Zheng Zhang
#t 2009
#c 17
#! In this paper we describe WEST (Web Entity Search Technologies) system that we have developed to improve people search over the Internet. Recently the problem of Web People Search (WePS) has attracted significant attention from both the industry and academia. In the classic formulation of WePS problem the user issues a query to a web search engine that consists of a name of a person of interest. For such a query, a traditional search engine such as Yahoo or Google would return webpages that are related to any people who happened to have the queried name. The goal of WePS, instead, is to output a set of clusters of webpages, one cluster per each distinct person, containing all of the webpages related to that person. The user then can locate the desired cluster and explore the webpages it contains.

#index 1206843
#* WISE: A Workflow Information Search Engine
#@ Qihong Shao;Peng Sun;Yi Chen
#t 2009
#c 17
#! Workflows are widely used for representing business processes, web services, scientific experiments, and activities in daily life, like recipes. There is an increasing need for people to search a workflow repository using keywords and retrieve the relevant ones according to their interests. A workflow hierarchy is a three dimensional object containing multi-resolution abstraction views on the same workflow. This unique structure poses a new set of challenges compared to keyword search on tree or graph structures which are typically found in XML or relational data. In this demonstration, we present an effective workflow search engine, WISE, which returns informative and concise search results, defined as the minimal views of the most specific workflow hierarchies containing matching keywords.

#index 1206844
#* A Data Structure for Sponsored Search
#@ Arnd Christian König;Kenneth Church;Martin Markov
#t 2009
#c 17
#! Inverted files have been very successful for document retrieval, but sponsored search is different. Inverted files are designed to find documents that match the query (all the terms in the query need to be in the document, but not vice versa). For sponsored search, ads are associated with bids. When a user issues a search query, bids are typically matched to the query using broad-match semantics: all the terms in the bid need to be in the query (but not vice versa). This means that the roles of the query and the bid/document are reversed in sponsored search, in turn making standard retrieval techniques based on inverted indexes ill-suited for sponsored search. This paper proposes novel index structures and query processing algorithms for sponsored search. We evaluate these structures using a real corpus of 180 million advertisements.

#index 1206845
#* A Framework for Clustering Massive-Domain Data Streams
#@ Charu Aggarwal
#t 2009
#c 17
#! In this paper, we will examine the problem of clustering massive domain data streams. Massive-domain data streams are those in which the number of possible domain values for each attribute are very large and cannot be easily tracked for clustering purposes. Some examples of such streams include IP-address streams, credit-card transaction streams, or streams of sales data over large numbers of items. In such cases, it is well known that even simple stream operations such as counting can be extremely difficult because ofthe difficulty in maintaining summary information over the different discrete values. The task of clustering is significantly more challenging in such cases, since the intermediate statistics for the different clusters cannot be maintained efficiently. In this paper, we propose a method for clustering massive-domain data streams with the use of sketches. We prove probabilistic results which show that a sketch-based clustering method can provide similar results to an infinite-space clustering algorithm with high probability. We present experimental results which validate these theoretical results, and show that it is possible to approximate the behavior of an infinite-space algorithm accurately.

#index 1206846
#* Self-Tuning, Bandwidth-Aware Monitoring for Dynamic Data Streams
#@ Navendu Jain;Praveen Yalagandula;Mike Dahlin;Yin Zhang
#t 2009
#c 17
#! We present SMART, a self-tuning, bandwidth-aware monitoring system that maximizes result precision of continuous aggregate queries over dynamic data streams. While prior approaches minimize bandwidth cost under fixed precision constraints, they may still overload a monitoring system during traffic bursts. To facilitate practical deployment of monitoring systems, SMART therefore bounds the worst-case bandwidth cost for overload resilience. The primary challenge for SMART is how to dynamically select updates at each node to maximize query precision while keeping per-node monitoring bandwidth below a specified budget. To address this challenge, SMART’s hierarchical algorithm (1) allocates bandwidth budgets in an ear-optimal manner to maximize global precision and (2) selftunes bandwidth settings to improve precision under dynamic workloads. Our prototype implementation of SMART provides key solutions to (a) prioritize pending updates for multi-attribute queries, (b) build bounded fan-in, load-aware aggregation trees to improve accuracy, and (c) combine temporal batching with arithmetic filtering to reduce load and to quantify result staleness. Our evaluation using simulations and a network monitoring application shows that SMART incurs low overheads, improves accuracy by up to an order of magnitude compared to uniform bandwidth allocation, and performs close to the optimal algorithm under modest bandwidth budgets.

#index 1206847
#* Towards Efficient Processing of General-Purpose Joins in Sensor Networks
#@ Mirco Stern;Erik Buchmann;Klemens Böhm
#t 2009
#c 17
#! Join processing in wireless sensor networks is difficult: As the tuples can be arbitrarily distributed within the network, matching pairs of tuples is communication intensive and costly in terms of energy. Current solutions only work well with specific placements of the nodes and/or make restrictive assumptions. In this paper, we present SENS-Join, an efficient general-purpose join method for sensor networks. To obtain efficiency, SENS-Join does not ship tuples that do not join, based on a filtering step. Our main contribution is the design of this filtering step which is highly efficient in order not to exhaust the potential savings. We demonstrate the performance of SENS-Join experimentally: The overall energy consumption can be reduced by more than 80%, as compared to the state-of-the-art approach. The per node energy consumption of the most loaded nodes can be reduced by more than an order of magnitude.

#index 1206848
#* Forward Decay: A Practical Time Decay Model for Streaming Systems
#@ Graham Cormode;Vladislav Shkapenyuk;Divesh Srivastava;Bojian Xu
#t 2009
#c 17
#! Temporal data analysis in data warehouses and datastreaming systems often uses time decay to reduce the importance of older tuples, without eliminating their influence, on the results of the analysis. While exponential time decay is commonly used in practice, other decay functions (e.g. polynomial decay) are not, even though they have been identified as useful. We argue that this is because the usual definitions of time decay are "backwards": the decayed weight of a tuple is based on its age, measured backward from the current time. Since this age is constantly changing, such decay is too complex and unwieldy for scalable implementation. In this paper, we propose a new class of "forward" decay functions based on measuring forward from a fixed point in time. We show that this model captures the more practical models already known, such as exponential decay and landmark windows, but also includes a wide class of other types of time decay. We provide efficient algorithms to compute a variety of aggregates and draw samples under forward decay, and show that these are easy to implement scalably. Further, we provide empirical evidence that these can be executed in a production data stream management system with little or no overhead compared to the undecayed computations. Our implementation required no extensions to the query language or the DSMS, demonstrating that forward decay represents a practical model of time decay for systems that deal with time-based data.

#index 1206849
#* Another Outlier Bites the Dust: Computing Meaningful Aggregates in Sensor Networks
#@ Antonios Deligiannakis;Yannis Kotidis;Vasilis Vassalos;Vassilis Stoumpos;Alex Delis
#t 2009
#c 17
#! Recent work has demonstrated that readings provided by commodity sensor nodes are often of poor quality. In order to provide a valuable sensory infrastructure for monitoring applications, we first need to devise techniques that can withstand "dirty" and unreliable data during query processing. In this paper we present a novel aggregation framework that detects suspicious measurements by outlier nodes and refrains from incorporating such measurements in the computed aggregate values. We consider different definitions of an outlier node, based on the notion of a user-specified minimum support, and discuss techniques for properly routing messages in the networkin order to reduce the bandwidth consumption and the energy drain during the query evaluation. In our experiments using real and synthetic traces we demonstrate that: (i) a straightforward evaluation of a user aggregate query leads to practically meaningless results due to the existence of outliers; (ii) our techniques can detect and eliminate spurious readings without any application specific knowledge of what constitutes normal behavior; (iii) the identification of outliers, when performed inside the network, significantly reduces bandwidth and energy drain compared to alternative methods that centrally collect and analyze all sensory data; and (iv) we can significantly reduce the cost of the aggregation process by utilizing simple statistics on outlier nodes and reorganizing accordingly the collection tree.

#index 1206850
#* Online Anomaly Prediction for Robust Cluster Systems
#@ Xiaohui Gu;Haixun Wang
#t 2009
#c 17
#! In this paper, we present a stream-based mining algorithm for online anomaly prediction. Many real-world applications such as data stream analysis requires continuous cluster operation. Unfortunately, today's large-scale cluster systems are still vulnerable to various software and hardware problems. System administrators are often overwhelmed by the tasks of correcting various system anomalies such as processing bottlenecks (i.e., full stream buffers), resource hot spots, and service level objective (SLO) violations. Our anomaly prediction scheme raises early alerts for impending system anomalies and suggests possible anomaly causes. Specifically, we employ Bayesian classification methods to capture different anomaly symptoms and infer anomaly causes. Markov models are introduced to capture the changing patterns of different measurement metrics. More importantly, our scheme combines Markov models and Bayesian classification methods to predict when a system anomaly will appear in the foreseeable future and what are the possible anomaly causes. To the best of our knowledge, our work provides the first stream-based mining algorithm for predicting system anomalies. We have implemented our approach within the IBM System S distributed stream processing cluster, and conducted case study experiments using fully implemented distributed data analysis applications processing real application workloads. Our experiments show that our approach efficiently predicts and diagnoses severalbottleneck anomalies with high accuracy while imposing low overhead to the cluster system.

#index 1206851
#* Fa: A System for Automating Failure Diagnosis
#@ Songyun Duan;Shivnath Babu;Kamesh Munagala
#t 2009
#c 17
#! Failures of Internet services and enterprise systems lead to user dissatisfaction and considerable loss of revenue. Since manual diagnosis is often laborious and slow, there is considerable interest in tools that can diagnose the cause of failures quickly and automatically from system-monitoring data. This paper identifies two key data-mining problems arising in a platform for automated diagnosis called {\em Fa}. Fa uses monitoring data to construct a database of{\em failure signatures} against which data from undiagnosed failures can be matched. Two novel challenges we address are to make signatures robust to the noisy monitoring data in production systems, and to generate reliable confidence estimates for matches. Fa uses a new technique called {\em anomaly-based clustering} when the signature database has no high-confidence match for an undiagnosed failure. This technique clusters monitoring data based on how it differs from the failure data, and pinpoints attributes linked to the failure. We show the effectiveness of Fa through a comprehensive experimental evaluation based on failures from a production setting, a variety of failures injected in a testbed, and synthetic data.

#index 1206852
#* Online Interval Skyline Queries on Time Series
#@ Bin Jiang;Jian Pei
#t 2009
#c 17
#! In many applications, we need to analyze a large number of time series. Segments of time series demonstrating dominating advantages over others are often of particular interest. In this paper, we advocate interval skyline queries, a novel type of time series analysis queries. For a set of time series and a given time interval [i : j], an interval skyline query returns the time series which are not dominated by any other time series in the interval. We illustrate the usefulness of interval skyline queries in applications. Moreover, we develop an on-the-fly method and a view-materialization method to online answer interval skyline queries on time series. The on-the-fly method keeps the minimum and the maximum values of the time series using radix priority search trees and sketches, and computes the skyline at the query time. The view-materialization method maintains the skylines over all intervals in a compact data structure. Through theoretical analysis and extensive experiments, we show that both methods only require linear space and are efficient in query answering as well as incremental maintenance.

#index 1206853
#* ACStream: Enforcing Access Control over Data Streams
#@ Jianneng Cao;Barbara Carminati;Elena Ferrari;Kian-Lee Tan
#t 2009
#c 17
#! In this demo proposal, we illustrate ACStream, a system built on top of Stream Base [1], to specify and enforce access control policies over data streams. ACStream supports a very flexible role-based access control model specifically designed to protect against unauthorized access to streaming data. The core component of ACStream is a query rewriting mechanism that, by exploiting a set of secure operators proposed by us in [2], rewrites a user query in such a way that it does not violate the specified access control policies during its execution. The demo will show how policies modelling a variety of access control requirements can be easily specified and enforced using ACStream.

#index 1206854
#* Automated Diagnosis of System Failures with Fa
#@ Songyun Duan;Shivnath Babu
#t 2009
#c 17
#! Failures of Internet services and enterprise systems lead to user dissatisfaction and considerable loss of revenue. Since manual diagnosis is often laborious and slow, there is considerable interest in tools that can diagnose the cause of failures quickly and automatically from system-monitoring data. Fa uses monitoring data to construct a database of {\em failure signatures} against which data from undiagnosed failures can be matched. Two novel challenges we address are to make signatures robust to the noisy monitoring data in production systems, and to generate reliable confidence estimates for matches. Fa uses a new technique called {\em anomaly-based clustering} when thesignature database has no high-confidence match for an undiagnosed failure. This technique clusters monitoring data based on how it differs from the failure data, and pinpoints attributes linked to the failure. We show the effectiveness of Fa through a comprehensive experimental evaluation based on failures from a production setting, a variety of failures injected in a testbed, and synthetic data.

#index 1206855
#* KSpot: Effectively Monitoring the K Most Important Events in a Wireless Sensor Network
#@ Panayiotis Andreou;Demetrios Zeinalipour-Yazti;Martha Vassiliadou;Panos K. Chrysanthis;George Samaras
#t 2009
#c 17
#! This demo presents a graphical user interface and ranking system, coined KSpot, for effectively monitoring the K highest-ranked answers to a query Q in a Wireless Sensor Network. KSpot deploys state-of-the-art distributed Top-k query processing algorithms in order to realize both snapshot queries and historic queries minimizing the consumption of system resources and prolonging the lifetime of the deployed sensor network. Additionally, KSpot is user-friendly and customizable featuring an intuitive user interface that enables a user to express declarative SQL-like queries over any ad-hoc scenario and to display the results graphically as opposed to the traditional tabular representation. To demonstrate the applicability of our system during the conference, we will continuously identify the K conference rooms with the highest sound level and display them such that conference attendees will be able to quickly determine the rooms with the most active discussions. The demo will also allow attendees to customize the system by changing the target scenario (e.g., by adapting the K value, the sensed parameter, etc.) Finally, we will present KSpot's system panel which continuously displays the savings in energy and messages that our system yields.

#index 1206856
#* Environmental Monitoring 2.0
#@ Sebastian Michel;Ali Salehi;Liqian Luo;Nicholas Dawes;Karl Aberer;Guillermo Barrenetxea;Mathias Bavay;Aman Kansal;K. Ashwin Kumar;Suman Nath;Marc Parlange;Stewart Tansley;Catharine van Ingen;Feng Zhao;Yongluan Zhou
#t 2009
#c 17
#! A sensor network data gathering and visualization infrastructure is demonstrated, comprising of Global Sensor Networks (GSN) middleware and Microsoft SensorMap. Users are invited to actively participate in the process of monitoring real-world deployments and can inspect measured data in the form of contour plots overlayed onto a high resolution map and a digital topographic model. Users can go back in time virtually to search for interesting events or simply to visualize the temporal dependencies of the data. The system presented is not only interesting and visually enticing for non-expert users but brings substantial benefits to environmental scientists. The easily installed data acquisition component as well as the powerful data sharing and visualization platform opens up new ground in collaborative data gathering and interpretation in the spirit of Web 2.0 applications.

#index 1206857
#* Online Near-Duplicate Video Clip Detection and Retrieval: An Accurate and Fast System
#@ Zi Huang;Liping Wang;Heng Tao Shen;Jie Shao;Xiaofang Zhou
#t 2009
#c 17
#! Video search has become a compelling research topic in recent years, due to the proliferation of online video uploading/sharing sites and the exponential explosion of video data. In this demonstration, we showcase a Web-based integrated platform which performs online detection of near-duplicate occurrences over continuous video streams, as well as retrieval of near-duplicate clips from segmented video collections. In particular, our method to detect relevant subsequences in a streaming video is characterized by a novel one-dimensional distance trajectory capturing the changes of consecutive frames. Such a trajectory is further represented by a sequence of compact signatures. An effective similarity measure is devised to compare the trajectory with multiple query videos. This system shows a number of new features compared with our previous prototype.

#index 1206858
#* PLUS: A Message-Efficient Prototype for Location-Based Applications
#@ Yu-Ling Hsueh;Roger Zimmermann;Wei-Shinn Ku;Haojun Wang;Chung-Dau Wang
#t 2009
#c 17
#! The PLUS system is designed to efficiently track moving object locations on a road network and execute continuous spatial queries in support of location-based services. PLUS implements a novel lazy position update mechanism that significantly reduces the communication overhead and server indexing load related to frequent location updates in moving object and moving query scenarios. The contribution of this demo is to present how the lazy position update scheme can achieve message-efficiency under various conditions which can be interactively set via user-selectable parameters in a graphical user interface.

#index 1206859
#* Unified Declarative Platform for Secure Netwoked Information Systems
#@ Wenchao Zhou;Yun Mao;Boon Thau Loo;Martín Abadi
#t 2009
#c 17
#! We present a unified declarative platform for specifying, implementing, and analyzing secure networked information systems. Our work builds upon techniques from logic-based trust management systems, declarative networking, and data analysis via provenance. We make the following contributions. First, we propose the Secure Network Datalog (SeNDlog) language that unifies Binder, a logic-based language for access control in distributed systems, and Network Datalog, a distributed recursive query language for declarative networks. SeNDlog enables network routing, information systems, and their security policies to be specified and implemented within a common declarative framework. Second, we extend existing distributed recursive query processing techniques to execute SeNDlog programs that incorporate authenticated communication among untrusted nodes. Third, we demonstrate that distributed network provenance can be supported naturally within our declarative framework for network security analysis and diagnostics. Finally, using a local cluster and the PlanetLab testbed, we perform a detailed performance study of a variety of secure networked systems implemented using our platform.

#index 1206860
#* An Architecture for Regulatory Compliant Database Management
#@ Soumyadeb Mitra;Marianne Winslett;Richard T. Snodgrass;Shashank Yaduvanshi;Sumedh Ambokar
#t 2009
#c 17
#! Spurred by financial scandals and privacy concerns, governments worldwide have moved to ensure confidence in digital records by regulating their retention and deletion. These requirements have led to a huge market for compliance storage servers, which ensure that data are not shredded or altered before the end of their mandatory retention period. These servers preserve unstructured and semi-structured data at a file-level granularity: email, spreadsheets, reports, instant messages. In this paper, we extend this level of protection to structured data residing in relational databases. We propose a compliant DBMS architecture and two refinements that illustrate the additional security that one can gain with only a slight performance penalty, with almost no modifications to the DBMS kernel. We evaluate our proposed architecture through experiments with TPC-C on a high-performance DBMS, and show that the runtime overhead for transaction processing is approximately 10\% in typical configurations.

#index 1206861
#* Perm: Processing Provenance and Data on the Same Data Model through Query Rewriting
#@ Boris Glavic;Gustavo Alonso
#t 2009
#c 17
#! Data provenance is information that describes how a given data item was produced. The provenance includes source and intermediate data as well as the transformations involved in producing the concrete data item. In the context of a relational databases, the source and intermediate dataitems are relations, tuples and attribute values. The transformations are SQL queries and/or functions on the relational data items. Existing approaches capture provenance information by extending the underlying data model. This has the intrinsic disadvantage that the provenance must be stored and accessed using a different model than the actual data. In this paper, we present an alternative approach that uses query rewriting to annotate result tuples with provenance information. The rewritten query and its result use the same model and can, thus, be queried, stored and optimized using standard relational database techniques. In the paper we formalize the query rewriting procedures, prove their correctness, and evaluate a first implementation of the ideas using PostgreSQL. As the experiments indicate, our approach efficiently provides provenance information inducing only a small overhead on normal operations.

#index 1206862
#* Join Optimization of Information Extraction Output: Quality Matters!
#@ Alpa Jain;Panagiotis G. Ipeirotis;AnHai Doan;Luis Gravano
#t 2009
#c 17
#! Information extraction (IE) systems are trained to extract specific relations from text databases. Real-world applications often require that the output of multiple IE systems be joined to produce the data of interest. To optimize the execution of a join of multiple extracted relations, it is not sufficient to consider only execution time. In fact, the quality of the join output is of critical importance: unlike in the relational world, different join execution plans can produce join results of widely different quality whenever IE systems are involved. In this paper, we develop a principled approach to understand, estimate, and incorporate output quality into the join optimization process over extracted relations. We argue that the output quality is affected by (a) the configuration of the IE systems used to process documents, (b) the document retrieval strategies used to retrieve documents, and (c) the actual join algorithm used. Our analysis considers several alternatives for these factors, and predicts the output quality---and, of course, the execution time---of the alternate execution plans. We establish the accuracy of our analytical models, as well as study the effectiveness of a quality-aware join optimizer, with a large-scale experimental evaluation over real-world text collections and state-of-the-art IE systems.

#index 1206863
#* STAIRS: Towards Efficient Full-Text Filtering and Dissemination in a DHT Environment
#@ Weixiong Rao;Ada Wai-Chee Fu;Lei Chen;Hanhua Chen
#t 2009
#c 17
#! Nowadays contents in Internet like weblogs, wikipedia and news sites become "live". How to notify and provide users with the relevant contents becomes a challenge. Unlike conventional Web search technology or the RSS feed, this paper envisions a personalized full-text content filtering and dissemination system in a highly distributed environment such as a Distributed Hash Table (DHT). Users can subscribe to their interested contents by specifying some terms and threshold values for filtering. Then, published contents will be disseminated to the associated subscribers. We propose a novel and simple framework of filter registration and content publication, STAIRS. By the new framework, we propose three algorithms (default forwarding, dynamic forwarding and adaptive forwarding) to reduce the forwarding cost and false dismissal rate; meanwhile, the subscriber can receive the desired contents with no duplicates. In particular, the adaptive forwarding utilizes the filter information to significantly reduce the forwarding cost. Experiments based on two real query logs and two real datasets show the effectiveness of our proposed framework.

#index 1206864
#* Efficient Mining of Closed Repetitive Gapped Subsequences from a Sequence Database
#@ Bolin Ding;David Lo;Jiawei Han;Siau-Cheng Khoo
#t 2009
#c 17
#! There is a huge wealth of sequence data available, for example, customer purchase histories, program execution traces, DNA, and protein sequences. Analyzing this wealth of data to mine important knowledge is certainly a worthwhile goal.In this paper, as a step forward to analyzing patterns in sequences, we introduce the problem of mining closed repetitive gapped subsequences and propose efficient solutions. Given a database of sequences where each sequence is an ordered list of events, the pattern we would like to mine is called repetitive gapped subsequence, which is a subsequence (possibly with gaps between two successive events within it) of some sequences in the database. We introduce the concept of repetitive support to measure how frequently a pattern repeats in the database. Different from the sequential pattern mining problem, repetitive support captures not only repetitions of a pattern in different sequences but also the repetitions within a sequence. Given a userspecified support threshold min_sup, we study finding the set of all patterns with repetitive support no less than min_sup. To obtain a compact yet complete result set and improve the efficiency, we also study finding closed patterns. Efficient mining algorithms to find the complete set of desired patterns are proposed based on the idea of instance growth. Our performance study on various datasets shows the efficiency of our approach. A case study is also performed to show the utility of our approach.

#index 1206865
#* Efficient Processing of Warping Time Series Join of Motion Capture Data
#@ Yueguo Chen;Gang Chen;Ke Chen;Beng Chin Ooi
#t 2009
#c 17
#! Discovering non-trivial matching subsequences from two time series is very useful in synthesizing novel time series. This can be applied to applications such as motion synthesis where smooth and natural motion sequences are often required to be generated from existing motion sequences. We first address this problem by defining it as a problem of l-e-join over two time series. Given two time series, the goal of l-e-join is to find those non-trivial matching subsequences by detecting maximal l-connections from the e-matching matrix of the two time series. Given a querying motion sequence, the l-e-join can be applied to retrieve all connectable motion sequences from a database of motion sequences. To support efficient l-e-join of time series, we propose a two-step filter-and-refine algorithm, called Warping Time Series Join (WTSJ) algorithm. The filtering step serves to prune those sparse regions of the e-matching matrix where there are no maximal l-connections without incurring costly computation. The refinement step serves to detect closed l-connections within regions that cannot be pruned by the filtering step. To speed up the computation of e-matching matrix, we propose a block-based time series summarization method, based on which the block-wise e-matching matrix is first computed. Lots of pairwise distance computation of elements can then be avoided by applying the filtering algorithm on the block-wise e-matching matrix.Extensive experiments on l-e-join of motion capture sequences are conducted. The results confirm the efficiency and effectiveness of our proposed algorithm in processing l-e-join of motion capture time series.

#index 1206866
#* Probabilistic Skyline Operator over Sliding Windows
#@ Wenjie Zhang;Xuemin Lin;Ying Zhang;Wei Wang;Jeffrey Xu Yu
#t 2009
#c 17
#! Skyline computation has many applications including multi-criteria decision making. In this paper, we study the problem of efficient processing of continuous skyline queries over sliding windows on uncertain data elements regarding given probability thresholds. We first characterize what kind of elements we need to keep in our query computation. Then we show the size of dynamically maintained candidate set and the size of skyline. We develop novel, efficient techniques to process a continuous, probabilistic skyline query. Finally, we extend our techniques to the applications where multiple probability thresholds are given or we want to retrieve "top-k" skyline data objects. Our extensive experiments demonstrate that the proposed techniques are very efficient and handle a high-speed data stream in real time.

#index 1206867
#* Supporting Generic Cost Models for Wide-Area Stream Processing
#@ Olga Papaemmanouil;Ugur Çetintemel;John Jannotti
#t 2009
#c 17
#! Existing stream processing systems are optimized for a specific metric, which may limit their applicability to diverse applications and environments. This paper presents XFlow, a generic data stream collection, processing, and dissemination system that addresses this limitation efficiently. XFlow can express and optimize a variety of optimization metrics and constraints by distributing stream processing queries across a wide-area network. It uses metric-independent decentralized algorithms that work on localized, aggregated statistics, while avoiding local optima. To facilitate light-weight dynamic changes on the query deployment, XFlow relies on a loosely-coupled, flexible architecture consisting of multiple publish-subscribe overlay trees that can gracefully scale and adapt to changes to network and workload conditions. Based on the desired performance goals, the system progressively refines the query deployment, the structure of the overlay trees, as well as the statistics collection process. We provide an overview of XFlow's architecture and discuss its decentralized optimization model. We demonstrate its flexibility and the effectiveness using real-world streams and experimental results obtained from XFlow's deployment on PlanetLab. The experiments reveal that XFlow can effectively optimize various performance metrics in the presence of varying network and workload conditions.

#index 1206868
#* V*-kNN: An Efficient Algorithm for Moving k Nearest Neighbor Queries
#@ Sarana Nutanong;Rui Zhang;Egemen Tanin;Lars Kulik
#t 2009
#c 17
#! This demonstration program presents the V*-kNN algorithm, an efficient algorithm to process moving k nearest neighbor queries (MkNN). The V*-kNN algorithm is based on a safe-region concept called the V*-Diagram. By incrementally maintaining the V*-Diagram, V*-kNN continuously provides accurate MkNN query results and supports dynamically changing values of k. Our approach exploits information regarding the current location of the query point and the search space in addition to the data objects. As a result, the V*-kNN has much smaller IO and computation costs than existing methods.

#index 1206869
#* The PRISM Workwench: Database Schema Evolution without Tears
#@ Carlo A. Curino;Hyun J. Moon;MyungWon Ham;Carlo Zaniolo
#t 2009
#c 17
#! Information Systems are subject to a perpetual evolution, which is particularly pressing in Web Information Systems, due to their distributed and often collaborative nature. Such continuous adaptation process, comes with a very high cost, because of the intrinsic complexity of the task and the serious ramiﬁcations of such changes upon database-centric Information System softwares. Therefore, there is a need to automate and simplify the schema evolution process and to ensure predictability and logical independence upon schema changes. Current relational technology makes it easy to change the database content or to revise the underlaying storage and indexes but does little to support logical schema evolution which nowadays remains poorly supported by commercial tools. The PRISM system demonstrates a major new advance toward automating schema evolution (including query mapping and database conversion), by improving predictability, logical independence, and auditability of the process. In fact, PRISM exploits recent theoretical results on mapping composition, invertibility and query rewriting to provide DB Administrators with an intuitive, operational workbench usable in their everyday activities—thus enabling graceful schema evolution. In this demonstration, we will show (i) the functionality of PRISM and its supportive AJAX interface, (ii) its architecture built upon a simple SQL–inspired language of Schema Modiﬁcation Operators, and (iii) we will allow conference participants to directly interact with the system to test its capabilities. Finally, some of the most interesting evolution steps of popular Web Information Systems, such as Wikipedia, will be reviewed in a brief "Saga of Famous Schema Evolutions".

#index 1206870
#* SbQA: A Self-Adaptable Query Allocation Process
#@ Jorge-Arnulfo Quiané-Ruiz;Philippe Lamarre;Patrick Valduriez
#t 2009
#c 17
#! We present a flexible query allocation framework, called Satisfaction-based Query Allocation (SbQA for short), for distributed information systems where both consumers and providers (the participants) have special interests towards queries. A particularity of SbQA is that it allocates queries while considering both query load and participants' interests. To be fair, it dynamically trades consumers' interests for providers' interests based on their satisfaction. In this demo we illustrate the flexibility and efficiency of SbQA to allocate queries on the Berkeley Open Infrastructure for Network Computing (BOINC). We also demonstrate that SbQA is self-adaptable to the participants' expectations. Finally, we demonstrate that SbQA can be adapted to different kinds of applications by varying its parameters.

#index 1206871
#* FRISK: Keyword Query Cleaning and Processing in Action
#@ Ken Q. Pu;Xiaohui Yu
#t 2009
#c 17
#! In this demo, we will showcase the prototype system FRISK (for Finding Relational Views Using Structured Keyword Queries) for supporting keyword queries in relational databases. Two salient features that set our prototype apart from existing systems and will be demonstrated are: (1) It supports keyword query cleaning; and (2) It offers an efficient way to search for the proper data subspace related to the keyword query.

#index 1206872
#* XBLEND: Visual XML Query Formulation Meets Query Processing
#@ Zhou Yong;Sourav S. Bhowmick;Erwin Leonardi;K. G. Widjanarko
#t 2009
#c 17
#! Due to the complexity of XML query languages, the need for visual query interfaces that can reduce the burden of query formulation is fundamental to the spreading of XML to wider community. We present a RDBMS-based XML query evaluation system, called XBLEND, that takes a novel and on traditional approach to improving query performance by blending visual query formulation and query processing. It exploits the latency offered by GUI-based visual query formulation to prefetch portions of the query results. The basic idea is that we prefetch constituent path expressions, store the synopsis of intermediary results, reuse them when connective is added or "Run" is pressed. In our demonstration we show that our system exhibits promising performance in evaluating XML queries and show its usefulness in life sciences domain.

#index 1206873
#* Shaman: A Self-Healing Database System
#@ Songyun Duan;Peter Franklin;Vamsidhar Thummala;Dongdong Zhao;Shivnath Babu
#t 2009
#c 17
#! A self-healing database system is a grand-challenge vision where the system will detect, diagnose, and repair software/hardware failures automatically. We identify performance-availability problems (PAPs) as an important target for self-healing, and develop techniques that enable database systems to recover automatically from common PAPs. PAPs can be caused by many factors in database systems, e.g., contention for hardware (e.g., CPU, memory) and software resources (e.g., locks), inappropriate setting of configuration parameters (e.g., undersized buffer pool), and poor physical design (e.g., missing indexes). Many "tuning knobs" (mechanisms) are available to recover from these failures online. We propose to demonstrate the Shaman database system that implements policies to invoke these mechanisms automatically, efficiently, and correctly by addressing two challenges: (i) which knob to turn? and (ii) by how much? Shaman uses a mix of analytical modeling (queuing networks) and statistical learning (planned experiments and regression) techniques to estimate the benefit and cost associated with the tuning of each knob.

#index 1206874
#* iVA-File: Efficiently Indexing Sparse Wide Tables in Community Systems
#@ Boduo Li;Mei Hui;Jianzhong Li;Hong Gao
#t 2009
#c 17
#! In community web management systems (CWMS), storage structures inspired by universal tables are being used increasingly to manage sparse datasets. Such a sparse wide table (SWT) typically embodies thousands of attributes, with many of them being undefined in each tuple, and low-dimensional structured similarity search on a combination of numerical and text attributes is a common operation. However, many properties of such wide tables and their associated Web 2.0 services render most multi-dimensional indexing structures irrelevant. Recent studies in this area have mainly focused on improving the storage efficiency and efficient deployment of inverted indices; so far no new index has been proposed for indexing SWTs. The inverted index is fast for scanning but not efficient in reducing random accesses to the data file as it captures little information about the content of attribute values. In this paper, we propose the iVA-file that works on the basis of approximate contents and keeps scanning efficiency within a bounded range. We introduce the nG-signature to approximately represent data strings and improve the existing approximate vectors for numerical values. We also propose an efficient query processing strategy for the iVA-file, which is different from strategies used for existing scan-based indices. To enable the use of different metrics of distance between a query and a tuple that may vary from application to application, the iVA-file has been designed to be metric-oblivious and to provide efficient filter-and-refine search based on any rational metric. Extensive experiments on real datasets show that the iVA-file outperforms existing proposals in query efficiency significantly, at the same time, keeps a good update speed.

#index 1206875
#* SP^2Bench: A SPARQL Performance Benchmark
#@ Michael Schmidt;Thomas Hornung;Georg Lausen;Christoph Pinkel
#t 2009
#c 17
#! Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evaluation strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP^2Bench, a publicly available, language-specific SPARQL performance benchmark. SP^2Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellationsand RDF access patterns. As a proof of concept, we apply SP^2Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results

#index 1206876
#* Weighted Proximity Best-Joins for Information Retrieval
#@ Risi Thonangi;Hao He;Anhai Doan;Haixun Wang;Jun Yang
#t 2009
#c 17
#! We consider the problem of efficiently computing weighted proximity best-joins over multiple lists, with applications in information retrieval and extraction. We are given a multi-term query, and for each query term, a list of all its matches with scores, sorted by locations. The problem is to find the overall best matchset, consisting of one match from each list, such that the combined score according to a scoring function is maximized. We study three types of functions that consider both individual match scores and proximity of match locations in scoring a matchset. We present algorithms that exploit the properties of the scoring functions in order to achieve time complexities linear in the size of the match lists. Experiments show that these algorithms greatly outperform the naive algorithm based on taking the cross product of all match lists. Finally, we extend our algorithms for an alternative problem definition applicable to information extraction, where we need to find all good matchsets in a document.

#index 1206877
#* Access Methods for Markovian Streams
#@ Julie Letchner;Christopher Re;Magdalena Balazinska;Matthai Philipose
#t 2009
#c 17
#! Model-based views have recently been proposed as an effective method for querying noisy sensor data. Commonly used models from the AI literature (e.g., the hidden Markov model) expose to applications a stream of probabilistic and correlated state estimates computed from the sensor data. Many applications want to detect sophisticated patterns of states from these Markovian streams. Such queries are called event queries. In this paper, we present a new Markovian stream storage manager, Caldera. We develop and evaluate Caldera as a component of Lahar, a Markovian stream event query processing system developed in previous work. At the heart of Caldera is a set of access methods for Markovian streams that can improve event query performance by orders of magnitude compared to existing techniques, which must scan the entire stream. Our access methods use new adaptations of traditional B+ tree indexes, and a new index, called the Markov-chain index. They efficiently extract only the relevant timesteps from a stream, while retaining the stream's Markovian properties. We have implemented our prototype system on BDB and demonstrate its effectiveness on both synthetic data and real data from a building-wide RFID deployment.

#index 1206878
#* Shared Winner Determination in Sponsored Search Auctions
#@ David J. Martin;Joseph Y. Halpern
#t 2009
#c 17
#! Sponsored search auctions form a multibillion dollar industry. Search providers auction advertisement slots on search result pages to advertisers who are charged only if the end-user clicks on the advertiser's ad. The high volume of searches presents an opportunity for sharing the workrequired to resolve multiple auctions that occur simultaneously. We provide techniques for efficiently resolving sponsored search auctions involving large numbers of advertisers, with a focus on two issues: sharing work between multiple search auctions using shared aggregation and shared sort, and dealing with budget uncertainty arising from ads that have been displayed from previous auctions but have not received clicks yet.

#index 1206879
#* Probabilistic Inference over RFID Streams in Mobile Environments
#@ Thanh Tran;Charles Sutton;Richard Cocci;Yanming Nie;Yanlei Diao;Prashant Shenoy
#t 2009
#c 17
#! Recent innovations in RFID technology are enabling large-scale cost-effective deployments in retail, healthcare, pharmaceuticals and supply chain management. The advent of mobile or handheld readers adds significant new challenges to RFID stream processing due to the inherent reader mobility, increased noise, and incomplete data. In this paper, we address the problem of translating noisy, incomplete raw streams from mobile RFID readers into clean, precise event streams with location information. Specifically we propose a probabilistic model to capture the mobility of the reader, object dynamics, and noisy readings. Our model can self-calibrate by automatically estimating key parameters from observed data. Based on this model, we employ a sampling-based technique called particle filtering to infer clean, precise information about object locations from raw streams from mobile RFID readers. Since inference based on standard particle filtering is neither scalable nor efficient in our settings, we propose three enhancements---particle factorization, spatial indexing, and belief compression---for scalable inference over large numbers of objects and high-volume streams. Our experiments show that our approach can offer 49\% error reduction over a state-of-the-art data cleaning approach such as SMURF while also being scalable and efficient.

#index 1206880
#* Recursive Computation of Regions and Connectivity in Networks
#@ Mengmeng Liu;Nicholas E. Taylor;Wenchao Zhou;Zachary G. Ives;Boon Thau Loo
#t 2009
#c 17
#! In recent years, the data management community has begun to consider situations in which data access is closely tied to network routing and distributed acquisition: examples include, sensor networks that execute queries about reachable nodes or contiguous regions, declarative networks that maintain information about shortest paths and reachable endpoints, and distributed and peer-to-peer stream systems that detect associations (e.g., transitive relationships) among data at the distributed sources. In each case, the fundamental operation is to maintain a view over dynamic network state. This view is typically distributed, recursive, and may contain aggregation, e.g., describing transitive connectivity, shortest paths, least costly paths, or region membership.Surprisingly, solutions to computing such views are often domain-specific, expensive, and incomplete. In this paper, we recast the problem as one of incremental recursive view maintenance in the presence of distributed streams of updates to tuples: new stream data becomes insert operations and tuple expirations become deletions. We develop a set of techniques that maintain compact information about tuple derivability or data provenance. We complement this with techniques to reduce communication: aggregate selections to prune irrelevant aggregation tuples, provenance-aware operators that can determine when tuples are no longer derivable and remove them from their state, and shipping operators that greatly reduce the tuple and provenance information being propagated while still maintaining correct answers. We validate our work in a distributed setting with sensor and network router queries, showing significant gains in communication overhead without sacrificing performance.

#index 1206881
#* Non-Exposure Location Anonymity
#@ Haibo Hu;Jianliang Xu
#t 2009
#c 17
#! Location cloaking has been proposed and well studied to protect user privacy. It blurs the accurate user location (i.e., a point withcoordinates) and replaces it with a well-shaped cloaked region (usually a circle or a rectangle). However, to obtain such a cloaked region, all existing cloaking algorithms require to know the accurate locations of all users. Since such information is exactly what the user wants to hide, these algorithms can work only if all parties involved in the cloaking process are trusted. However, in practice this assumption rarely holds as any of these parties could be malicious. Therefore, location cloaking without exposing the accurate user location to any party is urgently needed. In this paper, we present such a non-exposure cloaking algorithm. It is designed for k-anonymity and cloaking is performed based on the proximity information among mobile users, instead of directly on their coordinates. We decompose the problem into two subproblems --- proximity minimum k-clustering and secure bounding, and develop distributed algorithms for both of them. Experimental results consistently show that these algorithms are efficient and robust under various proximity topologies and system settings.

#index 1206882
#* Privacy Preserving Publishing on Multiple Quasi-identifiers
#@ Jian Pei;Yufei Tao;Jiexing Li;Xiaokui Xiao
#t 2009
#c 17
#! In some applications of privacy preserving data publishing, a practical demand is to publish a data set on multiple quasi-identifiers for multiple users simultaneously, which poses several challenges. Can we generate one anonymized version of the data so that the privacy preservation requirement like $k$-anonymity is satisfied for all users and the information loss is reduced as much as possible? In this paper, we identify and tackle the novel problem by an elegant solution.The full paper is available at http://www.cs.sfu.ca/~jpei/publications/butterfly-tr.pdf

#index 1206883
#* FF-Anonymity: When Quasi-identifiers Are Missing
#@ Ke Wang;Yabo Xu;Ada W.  C. Fu;Raymond C.  W. Wong
#t 2009
#c 17
#! Existing approaches on privacy-preserving data publishing rely on the assumption that data can be divided into quasi-identifier attributes (QI) and sensitive attribute (SA). This assumption does not hold when an attribute has both sensitive values and identifying values, which is typically the case. In this paper, we study how such attributes would impact the privacy model and data anonymization. We identify a new form of attacks, called "freeform attacks", that occur on such data without explicit QI attributes and SA attributes. We present a framework for modeling identifying/sensitive information at the value level, define a problem to eliminate freeform attacks, and outline an efficient solution.

#index 1206884
#* QuickStart: An Upfront Client-Based Design Advisor for Parallel Data Warehouses
#@ Malu Castellanos;Ivo Jimenez;Neal Coddington;Hans Zeller;Steven Whang;Umeshwar Dayal
#t 2009
#c 17
#! QuickStart is a tool to automate the physical design of data warehouses for HP's Neoview system. It has been researched and prototyped at HP Labs with close interaction from Neoview design experts. It embodies heuristics and best practices of the experts to search for candidate physical features and uses cost calculations to recommend the features that result in good designs. It has some unique characteristics that differentiate it from other physical design advisors. In particular, it is the only advisor that is client-based, does not require a DBMS server installation and can work off a laptop by simply connecting to the customers' flat files. Another unique characteristic of QuickStart is that it provides the rationale for its recommendations.

#index 1206885
#* Efficient and Robust Database Support for Data-Intensive Applications in Dynamic Environments
#@ Jon Olav Hauglid;Kjetil Nørvåg;Norvald H. Ryeng
#t 2009
#c 17
#! Requirements from new types of applications call for new database system solutions. Computational science applications performing distributed computations on Grid networks with requirements for efficient storage and query solutions are now emerging. For this purpose we have developed DASCOSA-DB, a P2P-based distributed database system, which in addition to providing location-transparent storage and querying, also includes novel features like efficient partial restartof queries and redistribution of query operators in the context of failure, dynamic refragmentation and allocation, and distributed semantic caching. In this demo, the novel features will be demonstrated, combined with a more general description of the architecture and demonstration of the distributed query processing capabilities.

#index 1206886
#* Galaxy: Encouraging Data Sharing among Sources with Schema Variants
#@ Peter Mork;Len Seligman;Arnon Rosenthal;Michael Morse;Chris Wolf;Jeff Hoyt;Ken Smith
#t 2009
#c 17
#! This demonstration presents Galaxy, a schema manager that facilitates easy and correct data sharing among autonomous but related, evolving data sources. Galaxy reduces heterogeneity by helping database developers identify, reuse, customize, and advertise related schema components. The central idea is that as schemata are customized, Galaxy maintains a derivation graph, and exploits it for data exchange, discovery, and multi-database query over the "galaxy" of related data sources. Using a set of schemata from the biomedical domain, we demonstrate how Galaxy facilitates schema and data sharing.

#index 1206887
#* Hippocratic PostgreSQL
#@ Jalaja Padma;Yasin N. Silva;Muhammad U. Arshad;Walid G. Aref
#t 2009
#c 17
#! Privacy preservation has become an important requirement in information systems that deal with personal data. In many cases this requirement is imposed by laws that recognize the right of data owners to control whom their information is shared with and the purposes for which it can be shared. Hippocratic databases have been proposed as an answer to this privacy requirement; they extend the architecture of standard DBMSs with components that ensure personal data is handled in compliance with its associated privacy definitions. Previous work in Hippocratic databases has proposed the design of some of these components. Unfortunately, there has not been much work done to implement these components as an integral part of a DBMS and study the problems faced to realize the Hippocratic databases. The main goal of the 'Hippocratic PostgreSQL' project is to perform this implementation and study. The project includes the implementation of components to support limited disclosure, limited retention time, and management of multiple policies and policy versions. This demo presents the use of these components both from a terminal-based SQL command interface and through a Web-based healthcare application that makes use of the implemented database-level privacy features. Hippocratic PostgreSQL has the novel feature of augmenting both k-anonymity and generalization hierarchies into the Hippocratic DBMS engine functionality. Several interesting problems emerge as a result and their solutions are presented in the context of this demo.

#index 1206888
#* Weighted Set Similarity: Queries and Updates
#@ Divesh Srivastava
#t 2009
#c 17
#! Consider a universe of items, each of which is associated with a weight, and a database consisting of subsets of these items. Given a query set, a weighted set similarity query identifies either (i) all sets in the database whose normalized similarity to the query set is above a pre-specified threshold, or (ii) the sets in the database with the k highest similarity values to the query set. Weighted set similarity queries are useful in applications like data cleaning and integration for finding approximate matches in the presence of typographical mistakes, multiple formatting conventions, transformation errors, etc. We show that this problem has semantic properties that can be exploited to design index structures that support efficient algorithms for answering queries; these algorithms can achieve arbitrarily stronger pruning than the family of Threshold Algorithms. We describe how these index structures can beefficiently updated using lazy propagation in a way that gives strict guarantees on the quality of subsequent query answers. Finally, we illustrate that our proposed ideas work well in practice for real datasets.

#index 1206889
#* Incremental Reverse Nearest Neighbor Ranking
#@ Hans-Peter Kriegel;Peer Kröger;Matthias Renz;Andreas Züfle;Alexander Katzdobler
#t 2009
#c 17
#! In this paper, we formalize the novel concept of incremental reverse nearest neighbor ranking and suggest an original solution for this problem. We propose an efficient approach for reporting the results incrementally without the need to restart the search from scratch. Our approach can be applied to a multi-dimensional feature database which is hierarchically organized by any R-tree like index structure. Our solution does not assume any preprocessing steps which makes it applicable for dynamic environments where updates of the database frequently occur. Our experiments show that our approach reports the ranking results with much less page accesses than existing approaches designed for traditional reverse nearest neighbor search applied to the ranking problem.

#index 1206890
#* Keyword Search over Dynamic Categorized Information
#@ Manish Bhide;Venkatesan T. Chakaravarthy;Krithi Ramamritham;Prasan Roy
#t 2009
#c 17
#! Consider an information repository whose content is categorized. A data item (in the repository) can belong to multiple categories and new data is continuously added to the system. In this paper, we describe a system, CS*, which takes a keyword query and returns the relevant top-K categories. In contrast, traditional keyword search returns the top-K documents (i.e., data items) relevant to a user query. The need to dynamically categorize new data and also update the meta-data required for fast responses to user queries poses interesting challenges. The brute force approach of updating the meta-data by comparing each new data item with all the categories is impractical due to (i) the large cost involved in finding the categories associated with a data item and (ii) the high rate of arrival of new data items. We show that a sampling based approach which provides statistical guarantees on the reported results is also impracticable. We hence develop the CS* approach whose effectiveness results from its ability to focus on a strategically chosen subset of categories on the one hand and a subset of new data on the other. Given a query, CS* finds the top-K categories with high accuracy even in time-constrained situations. An experimental evaluation of the CS* system using real world data shows that it can easily achieve accuracy in excess of 90%, whereas other approaches demand at least 57% more resources (i.e., processing power), for providing similar results. Our experimental results also show that, contrary to expectations, if the rate of arrival of data items doubles, whereas CS* continues to provide high accuracy without a significant increase in resources, other approaches require more than double the number of resources.

#index 1206891
#* Deductive Framework for Programming Sensor Networks
#@ Himanshu Gupta;Xianjin Zhu;Xiang Xu
#t 2009
#c 17
#! Developing powerful paradigms for programming sensor networks is critical to realize the full potential of sensor networks as collaborative data processing engines. %In this article, we motivate and develop a deductive framework for programming sensor networks, extending the prior vision of viewing sensor network as a distributed database. The deductive programming approach is declarative, very expressive, and amenable to automatic optimizations. Such a framework allows users to program sensor network applications at a high-level without worrying about the low-level tedious details. %Our system translates a given deductive program to efficient distributed code that runs on individual nodes. To facilitate the above translation, we develop techniques for {\em distributed and asynchronous} evaluation of deductive programs in sensor networks. Our techniques generalize to recursive programs without negations, arbitrary non-recursive programs with negations, and in general to arbitrary "locally non-recursive'' programs with function symbols. %We present performance results on TOSSIM, a network simulator, and a small network testbed.

#index 1206892
#* Histograms and Wavelets on Probabilistic Data
#@ Graham Cormode;Minos Garofalakis
#t 2009
#c 17
#! There is a growing realization that uncertain information is a first-class citizen in modern database management. As such, we need techniques to correctly and efficiently process uncertain data in database systems. In particular, data reduction techniques that can produce concise, accurate synopses of large probabilistic relations are crucial. Similar to their deterministic relation counterparts, such compact probabilistic data synopses can form the foundation for human understanding and interactive data exploration, probabilistic query planning and optimization, and fast approximate query processing in probabilistic database systems. In this paper, we introduce definitions and algorithms for building histogram- and Haar wavelet-based synopses on probabilistic data. The core problem is to choose a set of histogram bucket boundaries or wavelet coefficients to optimize the accuracy of the approximate representation of a collection of probabilistic tuples under a given error metric. For a variety of different error metrics, we devise efficient algorithms that construct optimal or near optimal size B histogram and wavelet synopses. This requires careful analysis of the structure of the probability distributions, and novel extensions of known dynamic programming-based techniques for the deterministic domain. Our experiments show that this approach clearly outperforms simple ideas, such as building summaries for samples drawn from the data distribution, while taking equal or less time.

#index 1206893
#* Semantics of Ranking Queries for Probabilistic Data and Expected Ranks
#@ Graham Cormode;Feifei Li;Ke Yi
#t 2009
#c 17
#! When dealing with massive quantities of data, top-k queries are a powerful technique for returning only the k most relevant tuples for inspection, based on a scoring function. The problem of efficiently answering such ranking queries has been studied and analyzed extensively within traditional database settings. The importance of the top-k is perhaps even greater in probabilistic databases, where a relation can encode exponentially many possible worlds. There have been several recent attempts to propose definitions and algorithms for ranking queries over probabilistic data. However, these all lack many of the intuitive properties of a top-k over deterministic data. Specifically, we define a number of fundamental properties, including exact-k, containment, unique-rank, value-invariance, and stability, which are all satisfied by ranking queries on certain data. We argue that all these conditions should also be fulfilled by any reasonable definition for ranking uncertain data. Unfortunately, none of the existing definitions is able to achieve this. To remedy this shortcoming, this work proposes an intuitive new approach of expected rank. This uses the well-founded notion of the expected rank of each tuple across all possible worlds as the basis of the ranking. We are able to prove that, in contrast to all existing approaches, the expected rank satisfies all the required properties for a ranking query. We provide efficient solutions to compute this ranking across the major models of uncertain data, such as attribute-level and tuple-level uncertainty. For an uncertain relation of N tuples, the processing cost is O(N logN)—no worse than simply sorting the relation. In settings where there is a high cost for generating each tuple in turn, we provide pruning techniques based on probabilistic tail bounds that can terminate the search early and guarantee that the top-k has been found. Finally, a comprehensive experimental study confirms the effectiveness of our approach.

#index 1206894
#* Outsourcing Search Services on Private Spatial Data
#@ Man Lung Yiu;Gabriel Ghinita;Christian S. Jensen;Panos Kalnis
#t 2009
#c 17
#! Social networking and content sharing service providers, e.g., Facebook and Google Maps, enable their users to upload and share a variety of user-generated content, including location data such as points of interest. Users wish to share location data through an (untrusted) service provider such that trusted friends can perform spatial queries on the data. We solve the problem by transforming the location data before uploading them. We contribute spatial transformations that re-distribute locations in space and a transformation that employs cryptographic techniques. The data owner selects transformation keys and shares them with the trusted friends. Without the keys, it is infeasible for an attacker to reconstruct the exact original data points from the transformed points. These transformations achieve different tradeoffs between query efficiency and data security. In addition, we describe an attack model for studying the security properties of the transformations. Empirical studies suggest that the proposed methods are secure and efficient.

#index 1206895
#* Processing Group Nearest Group Query
#@ Ke Deng;Hu Xu;Shazia Sadiq;Yansheng Lu;Gabriel Pui Cheong Fung;Heng Tao Shen
#t 2009
#c 17
#! Given a data point set D, a query point set Q and an integer k, the Group Nearest Group (GNG) query finds a subset of points from D, ω (|ω| ≤ k), such that the total distance from all points in Q to the nearest point in ω is no greater than any other subset of points in D, ω(|ω| ≤ k). GNG query can be found in many real applications. In this paper, Exhaustive Hierarchical Combination algorithm (EHC) and Subset Hierarchial Refinement algorithm (SHR) are developed for GNG query processing. The superiority of SHR in terms of efficiency and quality compared to existing algorithms developed originally for data clustering is demonstrated.

#index 1206896
#* Separating Authentication from Query Execution in Outsourced Databases
#@ Stavros Papadopoulos;Dimitris Papadias;Weiwei Cheng;Kian-Lee Tan
#t 2009
#c 17
#! In the database outsourcing paradigm, a data owner (DO) delegates its DBMS administration to a specialized service provider (SP) that receives and processes queries from clients. The traditional outsourcing model (TOM) requires that the DO and the SP maintain authenticated data structures to enable authentication of query results. In this paper, we present SAE, a novel outsourcing model that separates authentication from query execution. Specifically, the DO does not perform any task except for maintaining its dataset (if there are updates). The SP only stores the DO's dataset and computes the query results using a conventional DBMS. All security-related tasks are outsourced to a separate trusted entity (TE), which maintains limited authentication information about the original dataset. A client contacts the TE when it wishes to establish the correctness of a result returned by the SP. The TE efficiently generates a verification token of negligible size. The client can verify the token with minimal cost. SAE eliminates the participation of the DO and the SP in the authentication process, and outperforms TOM in every aspect, including processing cost for all parties involved, communication overhead, query response time and ease of implementation in practical applications.

#index 1206897
#* On High Dimensional Projected Clustering of Uncertain Data Streams
#@ Charu C. Aggarwal
#t 2009
#c 17
#! In this paper, we will study the problem of projected clustering of uncertain data streams. The use of uncertainty is especially important in the high dimensional scenario, because the sparsity property of high dimensional data is aggravated by the uncertainty. The uncertainty information is important for not only the determination of the assignment of data points to clusters, but also that of the valid projections across which the data is naturally clustered. The problem is especially challenging in the case where the data is not available on disk and arrives in the form of a fast stream. In such cases, the one-pass constraint in data stream computation poses special challenges to the algorithmic sophistication required for incorporating uncertainty information into the high dimensional computations. We will show that the projected clustering problem can be effectively solved in the context of uncertain data streams.

#index 1206898
#* Expressive Location-Based Continuous Query Evaluation with Binary Decision Diagrams
#@ Zhengdao Xu;Hans-Arno Jacobsen
#t 2009
#c 17
#! Many location-based services require rich and expressive query language support for filtering large amounts of information over thousands of concurrently executing continuous queries. Parts of these queries may overlap or logically depend on each other suggesting the possibility to amortize the query execution over shared sub-queries and prune query execution according to dependencies to achieve real-time processing requirements inherent to many location-based applications. In this paper spatio-temporal queries constitute location constraints monitored by applications. We develop the Constraint Combination Binary Decision Diagrams (CCBDD), an efficient location constraint matching algorithm, and query indexing based on Binary Decision Diagrams. With CCBDD, redundant computations in shared sub-queries are avoided, and query dependencies are identified and pruned. Empirical results show that the CCBDD structure greatly improves matching performance with shared query execution and economical memory use.

#index 1206899
#* Self Organizing Semantic Topologies in P2P Data Integration Systems
#@ Ami Eyal;Avigdor Gal
#t 2009
#c 17
#! A semantic topology is a peer overlay network connected via semantic links, constructed using schema mappings and used for peer querying. The large-scale and dynamic environments of P2P networks dictate the use of automatic schema matching, which was shown to carry with it a degree of uncertainty. Therefore, peers prefer network topologies that improve their ability to answer queries effectively, by reducing uncertainty. We introduce a model for a peer database management system that manages the inherent uncertainty of automatic schema matching, the amplification of this uncertainty over transitive mappings, and its impact on query processing. We then briefly present the research challenges involving a dynamic topology setting where peers can change their neighbor set selection.

#index 1206900
#* Continuous Skylining on Volatile Moving Data
#@ Mu-Woong Lee;Seung-won Hwang
#t 2009
#c 17
#! A dynamic skyline query retrieves the moving data objects that are not spatially dominated by any other object with respect to a given query point. Existing efforts on supporting such queries, however, supports location as a single dynamic attribute and one or more static dimensions. In a clear contrast, this paper focuses on the continuous skyline computation on moving data with an arbitrary number of dynamic queriable dimensions, e.g., to model both location and its volatility, with and without static dimension. Toward the goal, we investigate the relative positions and velocities of the initial skyline points with respect to the query, to derive a search region for skyline candidates. After retrieving these candidates, we further prune out some candidates and examine their spatial relations to monitor the changes in the skyline.

#index 1206901
#* Visualized Elucidations of Ranking by Exploiting Object Relations
#@ Xinpeng Zhang;Yasuhito Asano;Masatoshi Yoshikawa
#t 2009
#c 17
#! For understanding human activity, a useful approach is to rank people according to the strength of their relations to a specified person. Similarly, rankings of objects based on such a relation are used in several fields. Several methods have been proposed for computing the strength of a relation between two objects. These methods do not present a reason why an object has a stronger relation to a specified object than another object has. Therefore, it is difficult for a user to understand a ranking obtained using these methods. On the other hand, the authors recently proposed a method for computing the strength of a relation through mining objects to elucidate the relation. We propose a ranking tool based on this method to afford a better understanding of a ranking. Our ranking tool has the following three characteristics for understanding a ranking: (1) it visualizes a relation by displaying objects that are important for elucidating the relation; (2) it classifies ranked objects into several groups, e.g., the two groups of "petroleum exporting countries'' and "petroleum consuming countries'' for ranking countries based on their respective relations to petroleum; and (3) it visualizes a reason explaining why an object has a stronger relation to a specified object than another object has. We explain novel ideas used in our ranking tool for understanding a ranking based on relations, and claim how effective the features of our tool are by presenting examples.

#index 1206902
#* Ranking of Object Summaries
#@ Georgios John Fakas;Zhi Cai
#t 2009
#c 17
#! A previously proposed Keyword Search paradigm produces, as a query result, a ranked list of Object Summaries (OSs); each OS summarizes all data held in a relational database about a particular Data Subject (DS). This paper further investigates the ranking of OSs and their tuples as to facilitate (1) the top-k ranking of OSs and also (2) the generation of partial size-l OSs (i.e. comprised of the l most important tuples). Therefore, a global Importance score for each tuple of the database (denoted as Im(ti)) is investigated and quantified. For this purpose, ValueRank (an extension of ObjectRank) is introduced which facilitates the estimation of scores for arbitrary databases (in contrast to PageRank-style techniques that are only effective on bibliographic databases). In addition, a variation of Combined functions are investigated for assigning an Importance score to an OS (denoted as Im(OS)) and a local Importance score of their tuples (denoted as Im(OS, ti)). Preliminary experimental evaluation on DBLP and Northwind Databases is presented.

#index 1206903
#* Improving the Effectiveness of XML Retrieval with User Navigation Models
#@ M. S. Ali;Mariano P. Consens;Bassam Helou
#t 2009
#c 17
#! Structured documents (predominantly encoded in XML) utilize markup dialects for several purposes, such as conveying logical structure, or providing rendering instructions. XML structure can also help users to navigate within documents to satisfy their information needs. However, including the user's structural preferences in the ranking of retrieved elements remains a key challenge in XML retrieval. In this paper, we propose an approach for including structural preferences in the ranking of XML elements by improving the structural relevance (SR) of results. SR is an evaluation measure which relies on graphical navigation models to capture the structural preferences of users. We propose several algorithms to post-process search engine output to improve the SR of the output. Experimental results (using data, assessments, and search engines from INEX 2007 and 2008) demonstrate the effect of different combinations of post-processing algorithms and navigation models on the effectiveness of systems.

#index 1206904
#* Jelly: A Language for Building Community-Centric Information Exploration Applications
#@ Sihem Amer-Yahia;Jian Huang;Cong Yu
#t 2009
#c 17
#! Social content sites [3], which integrate traditional content sites (e.g., Yahoo! Travel) with social network features, have recently emerged as a significant new trend on the Web. Users on those sites share content and form various communities based on explicit friendships or shared interests. However, the existing information exploration mechanisms rarely leverage the rich community structure. In this work, we aim to unlock the value of social content sites by helping developers specify community-based information exploration strategies in a flexible and declarative way. Our solution makes use of two key notions, topics and communities, in order to identify socially and semantically relevant information for users. Specifically, we propose JELLY as a language for developing community-centric information exploration applications. JELLY provides several primitives which exploit both content and user behavior in social content sites in order to help users explore relevant content. The topic generation primitive is used to extract topics from tags. The community extraction primitive enables building different user communities. The information discovery primitive helps customize content relevance by combining a user’s query and profile, as well as insights from related communities. Finally, the information explanation primitive offers valuable social provenance to help users better understand the returned content. We describe JELLY’s data model and language, and its application to building a system for finding socially relevant travel destinations in Yahoo! Travel.

#index 1206905
#* Ranking with Uncertain Scores
#@ Mohamed A. Soliman;Ihab F. Ilyas
#t 2009
#c 17
#! Large databases with uncertain information are becoming more common in many applications including data integration, location tracking, and Web search. In these applications, ranking records with uncertain attributes needs to handle new problems that are fundamentally different from conventional ranking. Specifically, uncertainty in records' scores induces a partial order over records, as opposed to the total order that is assumed in the conventional ranking settings. In this paper, we present a new probabilistic model, based on partial orders, to encapsulate the space of possible rankings originating from score uncertainty. Under this model, we formulate several ranking query types with different semantics. We describe and analyze a set of efficient query evaluation algorithms. We show that our techniques can be used to solve the problem of rank aggregation in partial orders. In addition, we design novelsampling techniques to compute approximate query answers. Our experimental evaluation uses both real and synthetic data. The experimental study demonstrates the efficiency and effectiveness of our techniques in different settings.

#index 1206906
#* Leveraging COUNT Information in Sampling Hidden Databases
#@ Arjun Dasgupta;Nan Zhang;Gautam Das
#t 2009
#c 17
#! A large number of online databases are hidden behind form-like interfaces which allow users to execute search queries by specifying selection conditions in the interface. Most of these interfaces return restricted answers (e.g., only top-k of the selected tuples), while many of them also accompany each answer with the COUNT of the selected tuples. In this paper, we propose techniques which leverage the COUNT information to efﬁciently acquire unbiased samples of the hidden database. We also discuss variants for interfaces which do not provide COUNTinformation. We conduct extensive experiments to illustrate the efﬁciency and accuracy of our techniques.

#index 1206907
#* A Robust Technique to Ensure Serializable Executions with Snapshot Isolation DBMS
#@ Mohammad Alomari;Alan Fekete;Uwe Röhm
#t 2009
#c 17
#! Snapshot Isolation (SI) is a popular concurrency control mechanism that has been implemented by many commercial and open-source platforms (e.g. Oracle, Postgre SQL, and MS SQL Server 2005). Unfortunately, SI can result in nonserializable execution, in which database integrity constraints can be violated. The literature reports some techniques to ensure that all executions are serializable when run in an engine that uses SI for concurrency control. These modify the application by introducing conflicting SQL statements. However, with each of these techniques the DBA has to make a choice among possible transactions to modify — and as we previously showed, making a bad choice of which transactions to modify can come with a hefty performance reduction. In this paper we propose a novel technique called ELM to introduce conflicts in a separate lock-manager object. Experiments with two platforms show that ELM has peak performance which is similar to SI, no matter which transactions are chosen for modification. That is, ELM is much less vulnerable from poor DBA choices than the previous techniques.

#index 1206908
#* Transaction Support for Log-Based Middleware Server Recovery
#@ Rui Wang;Betty Salzberg;David Lomet
#t 2009
#c 17
#! We have developed log-based recovery for middleware servers that access back-end transaction systems (DBMSs). Transactional consistency is provided between in-memory state stored in middleware servers and persistent state stored in transaction systems. A new logging method called results logging is exploited to ensure coordinated recovery of in-memory state with persistent database state. Results logging incurs low logging overhead for middleware servers and requires little or no modification to existing transaction systems. This makes our approach a practical coordinated recovery technique.

#index 1206909
#* Adaptive Scheduling of Web Transactions
#@ Shenoda Guirguis;Mohamed A. Sharaf;Panos K. Chrysanthis;Alexandros Labrinidis;Kirk Pruhs
#t 2009
#c 17
#! In highly interactive dynamic web database systems, user satisfaction determines their success. In such systems, user requested web pages are dynamically created by executing a number of database queries or web transactions. In this paper, we model the interrelated transactions generating a web page as workflows and quantify the user satisfaction by associating dynamic web pages with soft-deadlines. Further, we model the importance of transactions in generating a page by associating different weights to transactions. Using this framework, system success is measured in terms of minimizing the deviation from the deadline (i.e., tardiness) and also minimizing the weighted such deviation (i.e., weighted tardiness). In order to efficiently support the materialization of dynamic web pages, we propose ASETS∗, which is a parameter-free adaptive scheduling algorithm that automatically adapts to, not only system load, but also transactions' characteristics (i.e., interdependencies, deadlines and weights). ASETS∗ prioritizes the execution of transactions with the objective of minimizing weighted tardiness. It is also capable of balancing the tradeoff between optimizing average- and worst-case performance when needed. The performance advantages of ASETS∗ are experimentally demonstrated.

#index 1206910
#* Top-k Exploration of Query Candidates for Efficient Keyword Search on Graph-Shaped (RDF) Data
#@ Thanh Tran;Haofen Wang;Sebastian Rudolph;Philipp Cimiano
#t 2009
#c 17
#! Keyword queries enjoy widespread usage as they represent an intuitive way of specifying information needs. Recently, answering keyword queries on graph-structured data has emerged as an important research topic. The prevalent approaches build on dedicated indexing techniques as well as search algorithms aiming at finding substructures that connect the data elements matching the keywords. In this paper, we introduce a novel keyword search paradigm for graph-structured data, focusing in particular on the RDF data model. Instead of computing answers directly as in previous approaches, we first compute queries from the keywords, allowing the user to choose the appropriate query, and finally, process the query using the underlying database engine. Thereby, the full range of database optimization techniques can be leveraged for query processing. For the computation of queries, we propose a novel algorithm for the exploration of top-k matching subgraphs. While related techniques search the best answer trees, our algorithm is guaranteed to compute all k subgraphs with lowest costs, including cyclic graphs. By performing exploration only on a summary data structure derived from the data graph, we achieve promising performance improvements compared to other approaches.

#index 1206911
#* Reachability Indexes for Relational Keyword Search
#@ Alexander Markowetz;Yin Yang;Dimitris Papadias
#t 2009
#c 17
#! Due to its considerable ease of use, relational keyword search (R-KWS) has become increasingly popular. Its simplicity, however, comes at the cost of intensive query processing. Specifically, R-KWS explores a vast search space, comprised of all possible combinations of keyword occurrences in any attribute of every table. Existing systems follow two general methodologies for query processing: (i) graph based, which traverses a materialized data graph, and (ii) operator based, which executes relational operator trees on an underlying DBMS. In both cases, computations are largely wasted on graph traversals or operator tree executions that fail to return results. Motivated by this observation, we introduce a comprehensive framework for reachability indexing that eliminates such fruitless operations. We describe a range of indexes that capture various types of join reachability. Extensive experiments demonstrate that the proposed techniques significantly improve performance, often by several orders of magnitude.

#index 1206912
#* XQuery Join Graph Isolation: Celebrating 30+ Years of XQuery Processing Technology
#@ Torsten Grust;Manuel Mayr;Jan Rittinger
#t 2009
#c 17
#! A purely relational account of the true XQuery semantics can turn any relational database system into an XQuery processor. Compiling nested expressions of the fully compositional XQuery language, however, yields odd algebraic plan shapes featuring scattered distributions of join operators that currently overwhelm commercial SQL query optimizers. This work rewrites such plans before submission to the relational database back-end. Once cast into the shape of join graphs, we have found off-the-shelf relational query optimizers — the B-tree indexing subsystem and join tree planner, in particular — to cope and even be autonomously capable of "reinventing" advanced processing strategies that have originally been devised specifically for the XQuery domain, e.g., XPath step reordering, axis reversal, and path stitching. Performance assessments provide evidence that relational query engines are among the most versatile and efficient XQuery processors readily available today.

#index 1206913
#* Distinct Counting with a Self-Learning Bitmap
#@ Aiyou Chen;Jin Cao
#t 2009
#c 17
#! Estimating the number of distinct values is a fundamental problem in database that has attracted extensive research over the past two decades, due to its wide applications (especially in the Internet). Many algorithms have been proposed via sampling or sketching for obtaining statistical estimates that only require limited computing and memory resources. However, their performance in terms of relative estimation accuracy usually depends on the unknown cardinalities. In this paper, we address the following question: can a distinct counting algorithm have uniformly reliable performance, i.e. constant relative estimation errors for unknown cardinalities in a wide range, say from tens to millions? We propose a self-learning bitmap algorithm (S-bitmap) to answer this question. The S-bitmap is a bitmap obtained via a novel adaptive sampling process, where the bits corresponding to the sampled items are set to 1, and the sampling rates are learned from the number of distinct items already passed and reduced sequentially as more bits are set to 1.A unique property of S-bitmap is that its relative estimation error is truly stabilized, i.e. invariant to unknown cardinalities in a prescribed range. We demonstrate through both theoretical and empirical studies that with a given memory requirement, S-bitmap is not only uniformly reliable but more accurate than state-of-the-art algorithms such as the multiresolution bitmap \cite{bitmap:2006} and Hyper LogLog algorithms \cite{flajolet.et.al.07} under common practice settings.

#index 1206914
#* Holistic Query Transformations for Dynamic Web Applications
#@ Amit Manjhi;Charles Garrod;Bruce M. Maggs;Todd C. Mowry;Anthony Tomasic
#t 2009
#c 17
#! A promising approach to scaling Web applications is to distribute the server infrastructure on which they run. This approach, unfortunately, can introduce latency between the application and database servers, which in turn increases the network latency of Web interactions for the clients (end users). In this paper we introduce the concept of source-to-source holistic transformations---transformations that seek to optimize both the application code and the database requests made by it, to reduce clientlatency. As examples of our concept, we propose and evaluate two source-to-source holistic transformations that focus on hiding the latencies of database queries. We argue that opportunities for applying these transformations will continue to exist in Web applications. We then present algorithms for automating these transformations in asource-to-source compiler. Finally, we evaluate the effect of these two transformations on three realistic Web benchmark applications, both in the traditional centralized setting and a distributed setting.

#index 1206915
#* A Concise Representation of Range Queries
#@ Ke Yi;Xiang Lian;Feifei Li;Lei Chen
#t 2009
#c 17
#! With the advance of wireless communication technology, it is quite common for people to view maps or get related services from the handheld devices, such as mobile phones and PDAs. Range queries, as one of the most commonly used tools, are often posed by the users to retrieve needful information from a spatial database. However, due to the limits of communication bandwidth and hardware power of handheld devices, displaying all the results of a range query on a handheld device is neither communication efficient nor informative to the users. This is simply because that there are often too many results returned from a range query. In view of this problem, we present a novel idea that a concise representation of a specified size for the range query results, while incurring minimal information loss, shall be computed and returned to the user. Such a concise range query not only reduces communication costs, but also offers better usability to the users, providing an opportunity for interactive exploration. The usefulness of the concise range queries is confirmed by comparing it with other possible alternatives, such as sampling and clustering. Then we propose algorithms to find a good concise representation.

#index 1206916
#* SoQL: A Language for Querying and Creating Data in Social Networks
#@ Royi Ronen;Oded Shmueli
#t 2009
#c 17
#! We present SoQL (SOcial networks Query Language), a new language for querying and creating data in social networks. The language is designed to meet the growing need of social networks participants to efficiently manage the large, and quickly growing, amounts of data available to them, as well as automate processes of creating new data. This need is increasingly pressing as social networks gradually become an important working tool for business development and management. SoQL is a step in the direction of meeting the challenges of providing an expressive querying mechanism and automating processes in social networks.SoQL is an SQL-like language which enables the user to retrieve paths to other participants in the network, and use a retrieved path in order to attempt to create a connection with the participant at the end of the path. The language can specify complex conditions that a desired path should satisfy. The language also supports retrieving a group of participants which satisfy conditions as a group, and connecting its members to each other. SoQL uses the path and group as data types. This work presents the SoQL language and discusses implementation issues.

#index 1206917
#* On Protecting Private Information in Social Networks: A Proposal
#@ Bo Luo;Dongwon Lee
#t 2009
#c 17
#! As online social networks get more popular, it becomes increasingly critical to preserve user privacy in such networks. In this paper, we propose our preliminary results on defining and tackling information aggregation attacks over online social networks. We first introduce three major threats towards private information in online social networks. We conceptually model private information into multilevel and discretionary models. Then, we articulate information aggregation attacks under discretionary model. Finally, we present our preliminary design of "privacy monitor," a framework that allows users to define their own privacy scheme, and track their actual privacy disclosure to check for any unwanted leakage.

#index 1206918
#* Contents-Based Analysis of Community Formation and Evolution in Blogspace
#@ Seok-Chul Baek;Sukwon Kang;Hyung Noh;Sang-Wook Kim
#t 2009
#c 17
#! Blogspace is a primary example of online social networks. In blogspace, there are a number of communities, each of which consists of members having dense relationships with one another. In this paper, we address formation and evolution of blog communities. We first make two claims: (1) A high level of contents similarity of blogs increases the likelihood of their belonging to the same community in a blog network; (2) A high level of contents similarity of communities increases the possibility of their being merged into a bigger community in the future. We verify these claims by analyzing a large volume of real-world blog data of 2 millions bloggers through extensive experiments. Our results provide new insights towards subsequent research on formation and evolution of communities in a blog network.

#index 1206919
#* Actively Building Private Recommender Networks for Evolving Reliable Relationships
#@ Ira Assent
#t 2009
#c 17
#! Recommender systems have been successfully using information from social networks to improve the quality of results for the targeted users. In this work, we propose a novel model that allows users to actively cultivate their recommender network. Building on existing recommender systems, we suggest providing users with transparent information on users who might be able to suggest relevant items to their taste. Ensuring that users may keep their desired privacy level, this framework allows users to make anonymous contacts. In this way, the recommender system not only learns user taste, but makes these learned preferences transparent and editable. As more and more relevant recommendations by anonymous contacts are made, the recommender network evolves and builds trust between reliable contacts that share common interests.

#index 1206920
#* Social Streams Blog Crawler
#@ Matthew Hurst;Alexey Maykov
#t 2009
#c 17
#! Weblogs, and other forms of social media, differ from traditional web content in many ways. One of the most important differences is the highly temporal nature of the content. Applications that leverage social media content must, to be effective, have access to this data with minimal publication/acquisition latency. An effective weblog crawler should satisfy the following requirements: low latency, highly scalable, high data quality and appropriate network politeness. In this paper, we outline the weblog crawler implemented in the social streams project and summarize the challenges faced during development.

#index 1206921
#* Preprocessing Uncertain User Profile Data: Inferring User's Actual Age from Ages of the User's Neighbors
#@ Sung Hyuk Park;Sang Pil Han;Soon Young Huh;Hojin Lee
#t 2009
#c 17
#! User profile data (for example, age and sex) is usually self-reported by users, so it is prone to human errors or biases. For example, a user can be reluctant to provide a company with private information such as his/her actual age upon subscription, thus the user either does not fill in the age column or put in some random numbers to avoid unwanted privacy intrusion. However, inaccurate or uncertain user profile data undermines the integrity of a company's marketing or operational intelligence. Targeting customers based on uncertain user profile data will not as effective as targeting customers based on accurate user profile data. Thus companies perform preprocessing on user profile data as part of effort to maintain the accuracy of their user profile data. This paper presents a study of preprocessing uncertain user profile data based on a proposed simple collaborative learning algorithm. We demonstrate that a user's accurate profile information can be inferred from profile information of the user's social network neighbors. Particularly, we address the issue of how a communication service company can verify whether a user's reported age is true or not. We implement a simple collaborative learning algorithm using mobile network data. The dataset contains anonymized user data from a large Korean mobile company, capturing 174,071 users' demographic profiles and their communication histories. To construct a mobile social network among users, we collect 3G voice call histories including 561,787 unique call receivers who belong to the same service carrier. Results reveal that the prediction accuracy of the proposed method based on voice network data is 97% which is very high compared to 53%, the best accuracy by among competing methods and indicates that our method effectively detects users with great discrepancy between self-reported age and actual age.

#index 1206922
#* BP-Wrapper: A System Framework Making Any Replacement Algorithms (Almost) Lock Contention Free
#@ Xiaoning Ding;Song Jiang;Xiaodong Zhang
#t 2009
#c 17
#! In a high-end database system, the execution concurrency level rises continuously in a multiprocessor environment due to the increase in number of concurrent transactions and the introduction of multi-core processors. A new challenge for buffer management to address is to retain its scalability in responding to the highly concurrent data processing demands and environment. The page replacement algorithm, a major component in the buffer management, can seriously degrade the system's performance if the algorithm is not implemented in a scalable way. A lock-protected data structure is used in most replacement algorithms, where high contention is caused by concurrent accesses. A common practice is to modify a replacement algorithm to reduce the contention, such as to approximate the LRU replacement with the clock algorithm. Unfortunately, this type of modification usually hurts hit ratios of original algorithms. This problem may not exist or can be tolerated in an environment of low concurrency, thus has not been given enough attention for a long time. In this paper, instead of making a trade-off between the high hit ratio of a replacement algorithm and the low lock contention of its approximation, we propose a system framework, called BP-Wrapper, that (almost) eliminates lock contention for any replacement algorithm without requiring any changes to the algorithm. In BP-Wrapper, we use batching and prefetching techniques to reduce lock contention and to retain high hit ratio. The implementation of BP-Wrapper in PostgreSQL version 8.2 adds only about 300 lines of C code. It can increase the throughput up to two folds compared with the replacement algorithms with lock contention when running TPC-C-like and TPC-W-like workloads.

#index 1206923
#* Sketching Sampled Data Streams
#@ Florin Rusu;Alin Dobra
#t 2009
#c 17
#! Sampling is used as a universal method to reduce the running time of computations -- the computation is performed on a much smaller sample and then the result is scaled to compensate for the difference in size. Sketches are a popular approximation method for data streams and they proved to be useful for estimating frequency moments and aggregates over joins. A possibility to further improve the time performance of sketches is to compute the sketch over a sample of the stream rather than the entire data stream.In this paper we analyze the behavior of the sketch estimator when computed over a sample of the stream, not the entire data stream, for the size of join and the self-join size problems. Our analysis is developed for a generic sampling process. We instantiate the results of the analysis for all three major types of sampling -- Bernoulli sampling which is used for load shedding, sampling with replacement which is used to generate i.i.d. samples from a distribution, and sampling without replacement which is used by online aggregation engines -- and compare these particular results with the results of the basic sketch estimator. Our experimental results show that the accuracy of the sketch computed over a small sample of the data is, in general, close to the accuracy of the sketch estimator computed over the entire data even when the sample size is only $10\%$ or less of the dataset size. This is equivalent to a speed-up factor of at least $10$ when updating the sketch.

#index 1206924
#* Continuous Subgraph Pattern Search over Graph Streams
#@ Changliang Wang;Lei Chen
#t 2009
#c 17
#! Search over graph databases has attracted much attention recently due to its usefulness in many fields, such as the analysis of chemical compounds, intrusion detection in network traffic data, and pattern matching over users' visiting logs. However, most of the existing work focuses on search over static graph databases while in many real applications graphs are changing over time. In this paper we investigate a new problem on continuous subgraph pattern search under the situation where multiple target graphs are constantly changing in a stream style, namely the subgraph pattern search over graph streams. Obviously the proposed problem is a continuous join between query patterns and graph streams where the join predicate is the existence of subgraph isomorphism. Due to the NP-completeness of subgraph isomorphism checking, to achieve the real time monitoring of the existence of certain subgraph patterns, we would like to avoid using subgraph isomorphism verification to find the exact query-stream subgraph isomorphic pairs but to offer an approximate answer that could report all probable pairs without missing any of the actual answer pairs. In this paper we propose a light-weight yet effective feature structure called Node-Neighbor Tree to filter false candidate query-stream pairs. To reduce the computational cost, we further project the feature structures into a numerical vector space and conduct dominant relationship checking in the projected space. We propose two methods to efficiently check dominant relationships and substantiate our methods with extensive experiments.

#index 1206925
#* A Spreadsheet Algebra for a Direct Data Manipulation Query Interface
#@ Bin Liu;H. V. Jagadish
#t 2009
#c 17
#! A spreadsheet-like "direct manipulation" interface is more intuitive for many non-technical database users compared to traditional alternatives, such as visual query builders. The construction of such a direct manipulation interfacemay appear straightforward, but there are some significant challenges. First, individual direct manipulation operations cannot be too complex, so expressive power has to be achieved through composing (long) sequences of small operations. Second, all intermediate results are visible to the user, so grouping and ordering are material after every small step. Third, users often find the need to modify previously specified queries. Since manipulations are specified one step at a time, there is no actual queryexpression to modify. Suitable means must be provided to address this need. Fourth, the order in which manipulations are performed by the user should not affect the results obtained, to avoid user confusion. We address the aforementioned challenges by designing a new spreadsheet algebra that: i) operates on recursively grouped multi-sets, ii) contains a selectively designed set of operators capable of expressing at least all single-block SQL queries and can be intuitively implemented in a spreadsheet, iii) enables query modification by the notion of modifiable query state, and iv) requires no ordering in unary data manipulation operators since they are all designed to commute. We built a prototype implementation of the spreadsheet algebra and show, through user studies with non-technical subjects, that the resultant query interface is easier to use than a standard commercial visual query builder.

#index 1206926
#* Progressive Keyword Search in Relational Databases
#@ Guoliang Li;Xiaofang Zhou;Jianhua Feng;Jianyong Wang
#t 2009
#c 17
#! A common approach to performing keyword search over relational databases is to find the minimum Steiner trees in database graphs. These methods, however, are rather expensive as the minimum Steiner tree problem is known to be NP-hard. Further, these methods cannot benefit from DBMS capabilities. We propose a new concept called Compact Steiner Tree (CSTree), which can be used to approximate the Steiner tree problem for answering top-k keyword queries efficiently. We propose a structure-aware index, together with an effective ranking mechanism for fast, progressive and accurate retrieval of $top$-$k$ highest ranked CSTrees. The proposed techniques can be implemented using a standard RDBMS to benefit from its indexing and query processing capability. The experimental results show that our method achieves high search efficiency and result quality comparing to existing state-of-the-art approaches.

#index 1206927
#* An Incremental Threshold Method for Continuous Text Search Queries
#@ Kyriakos Mouratidis;HweeHwa Pang
#t 2009
#c 17
#! A text filtering system monitors a stream of incoming documents, to identify those that match the interest profiles of its users. The user interests are registered at a server as continuous text search queries. The server constantly maintains for each query a ranked result list, comprising the recent documents (drawn from a sliding window) with the highest similarity to the query. Such a system underlies many text monitoring applications that need to cope with heavy document traffic, such as news and email monitoring. In this paper, we propose the first solution for processing continuous text queries efficiently. Our objective is to support a large number of user queries while sustaining high document arrival rates. Our solution indexes the streamed documents with a structure based on the principles of the inverted file, and processes document arrival and expiration events with an incremental threshold-based method. Using a stream of real documents, we experimentally verify the efficiency of our approach, which is at least an order of magnitude faster than a competitor constructed from existing techniques.

#index 1206928
#* A Subspace Symbolization Approach to Content-Based Video Search
#@ Xiangmin Zhou;Xiaofang Zhou;Athman Bouguettaya;John A. Taylor
#t 2009
#c 17
#! We propose a subspace symbolization approach, namely SUDS, for content-based search on very large video databases. The novelty of SUDS is that it explores the data distribution in subspaces to build a visual dictionary. With this dictionary, the video data are processed using string matching techniques with two-step data simplification. A compact video representation model is developed by transforming each keyframe into a word that is a series of symbols in the dominant subspaces. Then, we present an innovative similarity measure called ED, which draws from the concept of the edit distance on strings to conduct video matching. The experimental results demonstrate the higheffectiveness of SUDS with optimal parameters.

#index 1206929
#* Evaluating TOP-K Queries over Business Processes
#@ Daniel Deutch;Tova Milo
#t 2009
#c 17
#! A Business Process (BP) consists of some business activities undertaken by one or more organizations in pursuit of some business goal. Tools for querying and analyzing BP specifications are extremely valuable for companies as they allow to optimize the BP, identify potential problems, and reduce operational costs. In particular, given a BP specification, identifying the top-k execution flows that are most likely to occur in practice out of those satisfying the query criteria, is crucial for various applications. To address this need, we introduce in this paper the notion of {\em likelihood} for BP execution flows, and study top-k query evaluation (finding the $k$ most likely matches) for queries over BP specifications. We analyze the complexity of query evaluation in this context and present novel algorithms for computing top-k query results. To our knowledge, this is the first paper that studies such top-k query evaluation for BP specifications.

#index 1206930
#* Scalable Keyword Search on Large Data Streams
#@ Lu Qin;Jeffrey Xu Yu;Lijun Chang;Yufei Tao
#t 2009
#c 17
#! It is widely realized that the integration of information retrieval (IR) and database (DB) techniques provides users with a broad range of high quality services. A new challenging issue along the same direction is IR-styled m-keyword query processing in a RDBMS framework over an open-ended relational data stream. The capability of supporting m-keyword queries over a relational data stream makes it possible for users to monitor events, that are implicitly interrelated, over a relational data stream in a timely manner. In brief, the problem is to find all connected trees whose size is less than or equal to a user-given threshold in terms of number of nodes for a m-keyword query, {k1, k2, · · · , km}, over a relational data stream on a database schema GS. The difficulty of the problem is related to the number of costly joins to be processed over time, which is affected by the parameters such as the number of keywords (m), the maximum size of connected trees (Tmax), as well as the complexity of the database schema when it is viewed as a schema graph (GS). In this paper, we propose a new demand-driven approach to process such a query over a high speed data stream. We show that we can significantly reduce the number of intermediate results when processing joins over a data stream, and therefore can achieve high efficiency.

#index 1206931
#* Visible Reverse k-Nearest Neighbor Queries
#@ Yunjun Gao;Baihua Zheng;Gencai Chen;Wang-Chien Lee;Ken C.  K. Lee;Qing Li
#t 2009
#c 17
#! Reverse nearest neighbor (RNN) queries have a broad application base such as decision support, profile-based marketing, resource allocation, data mining, etc. Previous work on RNN search does not take obstacles into consideration. In the real world, however, there are many physical obstacles (e.g., buildings, blindages, etc.), and their presence may affect the visibility/distance between two objects. In this paper, we introduce a novel variant of RNN queries, namely visible reverse nearest neighbor (VRNN) search, which considers the obstacle influence on the visibility of objects. Given a data set P, an obstacle set O, and a query point q, a VRNN query retrieves the points in P that have q as their nearest neighbor and are visible to q. We propose an efficient algorithm for VRNN query processing, assuming that both P and O are indexed by R-trees. Our method does not require any pre-processing, and employs half-plane property and visibility check to prune the search space.

#index 1206932
#* Clustering Uncertain Data with Possible Worlds
#@ Peter Benjamin Volk;Frank Rosenthal;Martin Hahmann;Dirk Habich;Wolfgang Lehner
#t 2009
#c 17
#! The topic of managing uncertain data has been explored in many ways. Different methodologies for data storage and query processing have been proposed. As the availability of management systems grows, the research on analytics of uncertain data is gaining in importance. Similar to the challenges faced in the field of data management, algorithms for uncertain data mining also have a high performance degradation compared to their certain algorithms. To overcome the problem of performance degradation, the MCDB approach was developed for uncertain data management based on the possible world scenario. As this methodology shows significant performance and scalability enhancement, we adopt this method for the field of mining on uncertain data. In this paper, we introduce a clustering methodology for uncertain data and illustrate current issues with this approach within the field of clustering uncertain data.

#index 1206933
#* A Rule-Based Classification Algorithm for Uncertain Data
#@ Biao Qin;Yuni Xia;Sunil Prabhakar;Yicheng Tu
#t 2009
#c 17
#! Data uncertainty is common in real-world applications due to various causes, including imprecise measurement, network latency, outdated sources and sampling errors. These kinds of uncertainty have to be handled cautiously, or else the mining results could be unreliable or even wrong. In this paper, we propose a new rule-based classification and prediction algorithm called uRule for classifying uncertain data. This algorithm introduces new measures for generating, pruning and optimizing rules. These new measures are computed considering uncertain data interval and probability distribution function. Based on the new measures, the optimal splitting attribute and splitting value can be identified and used for classification and prediction. The proposed uRule algorithm can process uncertainty in both numerical and categorical data. Our experimental results show that uRule has excellent performance even when data is highly uncertain.

#index 1206934
#* Tracking High Quality Clusters over Uncertain Data Streams
#@ Chen Zhang;Ming Gao;Aoying Zhou
#t 2009
#c 17
#! Recently, data mining over uncertain data streams has attracted a lot of attentions because of the widely existed imprecise data generated from a variety of streaming applications. In this paper, we try to resolve the problem of clustering over uncertain data streams. Facing uncertain tuples with different probability distributions, the clustering algorithm should not only consider the tuple value but also emphasis on its uncertainty. To fulfill these dual purposes, a metric named tuple uncertainty will be integrated into the overall procedure of clustering. Firstly, we survey uncertain data model and propose our uncertainty measurement and corresponding properties. Secondly, based on such uncertainty quantification method, we provide a two phase stream clustering algorithm and elaborate implementation detail. Finally, performance experiments over a number of real and synthetic data sets demonstrate the effectiveness and efficiency of our method.

#index 1206935
#* Holistically Twig Matching in Probabilistic XML
#@ Yawen Li;Guoren Wang;Junchang Xin;Ende Zhang;Zeling Qiu
#t 2009
#c 17
#! Traditional databases manage only deterministic information, but now many applications that use databases involve uncertain data. For example, it is infeasible for a sensor database to contain only the exact value of each sensor at all points in time. The uncertainty is inherent in these systems due to measurement and sampling errors, and resource limitations. This paper aims at the query processing algorithm of twig patterns on probabilistic XML documents. The existing algorithms evaluate twig patterns in a traversal way. The main shortcoming of this way is scanning the whole probabilistic XML document to get the final results. In this paper, we first represent a probabilistic XML document in the form of probabilistic tag streams and then match them in a holistic way. Extensive experiments are conducted and show that the proposed holistic way has the higher performance than the traversal way.

#index 1206936
#* Effective Feature Selection on Data with Uncertain Labels
#@ Bo Wang;Yan Jia;Yi Han;Weihong Han
#t 2009
#c 17
#! Nowadays, various learning technologies are required on uncertain data. As an important pre-processing step in data mining, feature selection needs to consider this vagueness or uncertainty. In this paper, we propose a novel algorithm to evaluate the correlation between features and uncertain class labels on the basis of Hilbert-Schmidt Independence Criterion. Consequently, the features can be ranked according to this criterion. Experimental results on extensive datasets demonstrate the benefits of our method.

#index 1206937
#* Mining of Frequent Itemsets from Streams of Uncertain Data
#@ Carson Kai-Sang Leung;Boyu Hao
#t 2009
#c 17
#! Frequent itemset mining plays an essential role in the mining of various patterns and is in demand in many real-life applications. Hence, mining of frequent itemsets has been the subject of numerous studies since its introduction. Generally, most of these studies find frequent itemsets from traditional transaction databases, in which the content of each transaction--namely, items--is definitely known and precise. However, there are many real-life situations in which ones are uncertain about the content of transactions. This calls for the mining of uncertain data. Moreover, due to advances in technology, a flood of precise or uncertain data can be produced in many situations. This calls for the mining of data streams. To deal with these situations, we propose two tree-based mining algorithms to efficiently find frequent itemsets from streams of uncertain data, where each item in the transactions in the streams is associated with an existential probability. Experimental results show the effectiveness of our algorithms in mining frequent itemsets from streams of uncertain data.

#index 1206938
#* Using Anonymized Data for Classification
#@ Ali Inan;Murat Kantarcioglu;Elisa Bertino
#t 2009
#c 17
#! In recent years, anonymization methods have emerged as an important tool to preserve individual privacy when releasing privacy sensitive data sets. This interest in anonymization techniques has resulted in a plethora of methods for anonymizing data under different privacy and utility assumptions. At the same time, there has been little research addressing how to effectively use the anonymized data for data mining in general and for distributed data mining in particular. In this paper, we propose a new approach for building classifiers using anonymized data by modeling anonymized data as uncertain data. In our method, we do not assume any probability distribution over the data. Instead, we propose collecting all necessary statistics during anonymization and releasing these together with the anonymized data. We show that releasing such statistics does not violate anonymity. Experiments spanning various alternatives both in local and distributed data mining settings reveal that our method performs better than heuristic approaches for handling anonymized data.

#index 1206939
#* Decision Trees for Uncertain Data
#@ Smith Tsang;Ben Kao;Kevin Y. Yip;Wai-Shing Ho;Sau Dan Lee
#t 2009
#c 17
#! Traditional decision tree classifiers work with data whose values are known and precise. We extend such classifiers to handle data with uncertain information, which originates from measurement/quantisation errors, data staleness, multiple repeated measurements, etc. The value uncertainty is represented by multiple values forming a probability distribution function (pdf). We discover that the accuracy of a decision tree classifier can be much improved if the whole pdf, rather than a simple statistic, is taken into account. We extend classical decision tree building algorithms to handle data tuples with uncertain values. Since processing pdf's is computationally more costly, we propose a series of pruning techniques that can greatly improve the efficiency of the construction of decision trees.

#index 1206940
#* Finding Time-Lagged 3D Clusters
#@ Xin Xu;Ying Lu;Kian-Lee Tan;Anthony K.  H. Tung
#t 2009
#c 17
#! Existing 3D clustering algorithms on gene times sample times time expression data do not consider the time lags between correlated gene expression patterns. Besides, they either ignore the correlation on time subseries, or disregard the continuity of the time series, or only validate pure shifting or pure scaling coherent patterns instead of the general shifting and-scaling patterns. In this paper, we propose a novel 3D cluster model, S2D3 Cluster, to address these problems, where S2 reflects the shifting-and-scaling correlation and D3 the 3-Dimensional gene times sample times time data. Within the S2D3 Cluster model, expression levels of genes are shifting-and-scaling coherent in both sample subspace and time subseries with arbitrary time lags. We develop a 3D clustering algorithm, LagMiner, for identifying interesting S2D3 Clusters that satisfy the constraints of regulation (gamma), coherence (gamma), minimum gene number (MinG), minimum sample subspace size (MinS) and minimum time periods length (MinT). Experimental results on both synthetic and real-life datasets show that LagMiner is effective, scalable and parameter-robust. While we use gene expression data in this paper, our model and algorithm can be applied on any other data where both spatial and temporal coherence are pursued.

#index 1206941
#* Contextual Ranking of Keywords Using Click Data
#@ Utku Irmak;Vadim von Brzeski;Reiner Kraft
#t 2009
#c 17
#! The problem of automatically extracting the most interesting and relevant keyword phrases in a document has been studied extensively as it is crucial for a number of applications. These applications include contextual advertising, automatic text summarization, and user-centric entity detection systems. All these applications can potentially benefit from a successful solution as it enables computational efficiency (by decreasing the input size), noise reduction, or overall improved user satisfaction.In this paper, we study this problem and focus on improving the overall quality of user-centric entity detection systems. First, we review our concept extraction technique, which relies on search engine query logs. We then define a new feature space to represent the interestingness of concepts, and describe a new approach to estimate their relevancy for a given context. We utilize click through data obtained from a large scale user-centric entity detection system - Contextual Shortcuts - to train a model to rank the extracted concepts, and evaluate the resulting model extensively again based on their click through data. Our results show that the learned model outperforms the baseline model, which employs similar features but whose weights are tuned carefully based on empirical observations, and reduces the error rate from 30.22% to 18.66%.

#index 1206942
#* Power Hints for Query Optimization
#@ Nicolas Bruno;Surajit Chaudhuri;Ravi Ramamurthy
#t 2009
#c 17
#! Commercial database systems expose query hints to address situations in which the optimizer chooses a poor plan for a given query. However, current query hints are not flexible enough to deal with a variety of non-trivial scenarios. In this paper, we introduce a hinting framework that enables the specification of rich constraints to influence the optimizer to pick better plans. We show that while our framework unifies previous approaches, it goes considerably beyond existing hinting mechanisms, and can be implemented efficiently with moderate changes to current optimizers.

#index 1206943
#* Double Index NEsted-Loop Reactive Join for Result Rate Optimization
#@ Mihaela A. Bornea;Vasilis Vassalos;Yannis Kotidis;Antonios Deligiannakis
#t 2009
#c 17
#! Adaptive join algorithms have recently attracted alot of attention in emerging applications where data is provided by autonomous data sources through heterogeneous network environments. Their main advantage over traditional join techniques is that they can start producing join results as soon as the first input tuples are available, thus improving pipelining by smoothing join result production and by masking source or network delays. In this paper we propose Double Index NEsted loops Reactive join (DINER), a new adaptive join algorithm forresult rate maximization. DINER combines two key elements: an intuitive flushing policy that aims to increase the productivity of in-memory tuples in producing results during the online phase of the join, and a novel re-entrant join technique that allows the algorithm to rapidly switch between processing in-memory and disk-resident tuples, thus better exploiting temporary delayswhen new data is not available. Our experiments using realand synthetic data sets demonstrate that DINER outperformsprevious adaptive join algorithms in producing result tuples at a significantly higher rate, while making better use of the available memory.

#index 1206944
#* Scheduling Updates in a Real-Time Stream Warehouse
#@ Lukasz Golab;Theodore Johnson;Vladislav Shkapenyuk
#t 2009
#c 17
#! This paper discusses updating a data warehouse that collects near-real-time data streams from a variety of external sources. The objective is to keep all the tables and materialized views up-to-date as new data arrive over time. We define the notion of data staleness, formalize the problem of scheduling updates in a way that minimizes average data staleness, and present scheduling algorithms designed to handle the complex environment of a real-time stream warehouse. A novel feature of our scheduling framework is that it considers the effect of an update on the staleness of the underlying tables rather than any property of the update job itself (such as deadline).

#index 1206945
#* AURA: Enabling Attribute-Based Spatial Search in RFID Rich Environments
#@ Tejas A. Bapat;K. Selçuk Candan;V. Snehith Cherukuri;Hari Sundaram
#t 2009
#c 17
#! In this paper, we introduce AURA, a novel framework for enriching the physical environment with information about objects and activities in order to support searches in the physical world. The goal is to enable individuals to use the environment in which they function as a living (short-term) memory of their activities and of the objects with which they interact in this environment. In order to act as a memory, the physical environment must be transparently embedded with relevant information and made accessible by in-situ search mechanisms. We achieve this embedding through innovative algorithms that leverage a collection of parasitic RFID tags distributed in the environment to act as a distributed storage cloud. Information about the activities of the users and objects with which they interact are encoded and stored, in a decentralized way, on these RFID tags to support attribute-based search. A novel auraProp algorithm disseminates information in the environment and a complementary auraSearch algorithm implements spatial searches for physical objects in the environment. Parasitic RFID tags are not self-powered and thus cannot communicate among each other. AURA leverages human movement in the environment to propagate information: as they move in the environment, users not only leave traces (or auras) of their own activities, but also help further disseminate auras of prior activities in the same space. AURA relies on a novel signature based information dissemination mechanism and a randomized information erasure scheme to ensure that the extremely limited storage spaces available on the RFID tags are used effectively. The erasure scheme also helps create an information gradient in the physical environment, which the auraSearch algorithm uses to direct the user towards the object of interest.

#index 1206946
#* Web Monitoring 2.0: Crossing Streams to Satisfy Complex Data Needs
#@ Haggai Roitman;Avigdor Gal;Louiqa Raschid
#t 2009
#c 17
#! Web Monitoring 2.0 supports the complex information needs of clients who probe multiple information sources and generate mashups by integrating across these volatile streams. A proxy that aims at satisfying multiple customized client profiles will face a scalability challenge in trying to maximize the number of clients served while at the same time fully satisfying complex client needs. In this paper, we introduce an abstraction of complex execution intervals, a combination of time intervals and information streams, to capture complex client needs. Given some budgetary constraints (e.g., bandwidth), we present offline algorithmic solutions for the problem of maximizing completeness of capturing complex profiles.

#index 1206947
#* Exploiting Domain Knowledge to Improve Biological Significance of Biclusters with Key Missing Genes
#@ Jin Chen;Liping Ji;Wynne Hsu;Kian-Lee Tan;Seung Y. Rhee
#t 2009
#c 17
#! In an era of increasingly complex biological datasets, one of the key steps in gene functional analysis comes from clustering genes based on co-expression. Biclustering algorithms can identify gene clusters with local co-expressed patterns, which are more likely to define genes functioning together than global clustering methods. However, these algorithms are not effective in uncovering gene regulatory networks because the mined biclusters lack genes that may be critical in the function but may not be co-expressed with the clustered genes. In this paper, we introduce a biclustering method called SKeleton Biclustering (SKB), which builds high quality biclusters from microarray data, creates relationships among the biclustered genes based on Gene Ontology annotations, and identifies genes that are missing in the biclusters. SKB thus defines inter-bicluster and intra-bicluster functional relationships. The delineation of functional relationships and incorporation of such missing genes may help biologists to discover biological processes that are important in a given study and provides clues for how the processes may be functioning together. Experimental results show that, with SKB, the biological significance of the biclusters is considerably improved.

#index 1206948
#* On the Efficiency of Provenance Queries
#@ Anastasios Kementsietsidis;Min Wang
#t 2009
#c 17
#! While models for data provenance have been extensively studied in the literature, the efficient evaluation of the resulting provenance queries remains an open problem. Traditional query optimization techniques, like the use of general-purpose indexes, or the materialization of provenance data, fail on different fronts to address the problem. Provenance-specific optimization techniques, like the use of customized indexes, similarly prove inadequate since the techniques are bound to specific provenance models. Therefore, the need to develop generic provenance-aware techniques quickly becomes apparent.In this paper, we argue for such a generic technique in the form of a provenance index structure that can be used to efficiently evaluate provenance queries ina variety of contexts. By highlighting the limitations of existing techniques, we identify the set of key properties of the generic index, including a novel property called duality which guarantees that the single index can evaluate both backward provenance queries (which data items from a set I are associated with an item from set O) and forward provenance queries (which items from O are associated with an item from I).

#index 1206949
#* An Integrated Approach to Performance Monitoring for Autonomous Tuning
#@ Alexander Thiem;Kai-Uwe Sattler
#t 2009
#c 17
#! With an ever growing complexity and data volume, the administration of today's relational database management systems has become one of the most important cost factors in their operation. Dynamic workloads and shifting demands require continuous effort from the DBA to deliver adequate performance. The goal of a modern DBMS must be to support the DBA's work with automated processes and workflows that facilitate quick and precise decisions. In this paper, we present the concept of an integrated performance monitoring in the Ingres DBMS that provides long-term collection of information valuable for performance tuning, problem identification and prediction. The approach of enhancing the DBMS core with monitoring features rather than adding an additional watchdog on top of the system leads to a high data resolution while still having only a minimal overhead. This concept was successfully prototyped in Ingres with a very small overhead for most usage scenarios. The prototype is able to collect and analyze data and to give useful recommendations on the physical database design to improve overall performance of the DBMS.

#index 1206950
#* Online Tuning of Aggregation Tables for OLAP
#@ Katja Hose;Daniel Klan;Kai-Uwe Sattler
#t 2009
#c 17
#! Materializing results from complex aggregation queries helps to significantly improve response times in OLAP servers. This problem is known as the view selection problem: choosing the optimal set of aggregation tables (called configuration) for a given workload. In this paper we present an online approach for adjusting the configuration dynamically to the current workload. This approach is implemented as part of an open source OLAP server and acts on the level of multidimensional MDX queries. The work presents the details of cost estimation and optimization of the system demonstrated in [10] and extends it by an online tuning strategy.

#index 1206951
#* An Economic Model for Self-Tuned Cloud Caching
#@ Debabrata Dash;Verena Kantere;Anastasia Ailamaki
#t 2009
#c 17
#! Cloud computing, the new trend for service infrastructures requires user multi-tenancy as well as minimal capital expenditure. In a cloud that services large amounts of data that are massively collected and queried, such as scientific data, users typically pay for query services. The cloud supports caching of data in order to provide quality query services. User payments cover query execution costs and maintenance of cloud infrastructure, and incur cloud profit. The challenge resides in providing efficient and resource-economic query services while maintaining a profitable cloud. In this work we propose an economic model for self-tuned cloud caching targeting the service of scientific data. The proposed economy is adapted to policies that encourage high-quality individual and overall query services but also brace the profit of the cloud. We propose a cost model that takes into account all possible query and infrastructure expenditure. The experimental study proves that the proposed solution is viable for a variety of workloads and data.

#index 1206952
#* Self-Tuning for SQL Performance in Oracle Database 11g
#@ Peter Belknap;Benoit Dageville;Karl Dias;Khaled Yagoub
#t 2009
#c 17
#! Commercial database customers across the board list SQL performance tuning as one of the most time-consuming tasks for database administrators (DBAs). The 10g Oracle Database provides a feature called the SQL Tuning Advisor to simplify the task. The 11g release adds a new database feature, called Automatic SQL Tuning, that closes the feedback loop for the first time, fully automating the SQL tuning workflow and solving some SQL performance problems without any DBA intervention.

#index 1206953
#* A Benchmark for Online Index Selection
#@ Karl Schnaitter;Neoklis Polyzotis
#t 2009
#c 17
#! Online approaches to physical design tuning have received considerable attention in the recent literature, with a focus on the problem of online index selection. However, it is difficult to draw conclusions on the relative merits of the proposed techniques, as they have been evaluated in isolation using different methodologies. In this paper, we make two concrete contributions to address this issue. First, we propose a benchmark for evaluating the performance of an online tuning algorithm in a principled fashion. Second, using the benchmark, we present a comparison of two representative online tuning algorithms that are implemented in the same database system. The results provide interesting insights on the behavior of these algorithms and validate the usefulness of the proposed benchmark.

#index 1206954
#* Support Multi-version Applications in SaaS via Progressive Schema Evolution
#@ Jianfeng Yan;Bo Zhang
#t 2009
#c 17
#! Update of applications in SaaS is expected to be a continuous efforts and cannot be done overnight or over the weekend. In such migration efforts, users are trained and shifted from a existing version to a new version successively. There is a long period of time when both versions of applications co-exist. Supporting two systems at the same time is not a cost efficient option and two systems may suffer from slow response time due to continuous synchronization between two systems. In this paper, we focus on how to enable progressive migration of multi-version applications in SaaS via evolving schema. Instead of maintain two systems, our solution is to maintain an intermediate schema that is optimized for mixed workloads for new and old applications. With a application migration schedule, an genetic algorithm is used to find out the more effective intermediated schema as well as migration paths and schedule. A key advantage of our approach is optimum performance during the long migration period while maintaining the same level of data movement required by the migration. We evaluated the proposed progressive migration approach on a TPCW workload and results validated its effectiveness of across a variety of scenarios; Experimental results demonstrate that our incremental migration proposed in this paper could bring about 200% performance gain as compared to the existing system.

#index 1206955
#* Join Reordering by Join Simulation
#@ Chaitanya Mishra;Nick Koudas
#t 2009
#c 17
#! We introduce a framework for reordering join pipelines at runtime in a database system. This framework incorporates novel techniques for simulating the execution of a join pipeline using random samples and statistical summaries. Our simulation techniques provide accurate runtime cardinality estimates along all alternative execution paths of a join pipeline. These estimates are then utilized to compare costs of alternative execution paths in a dynamic fashion, and reorder the pipeline if a better alternative path is found. We describe simulation techniques for pipelines of different kinds of join operators. We present an experimental evaluation of a prototype implementation of our framework in an open source data manager. The results demonstrate the feasibility and utility of the approach presented herein.

#index 1206956
#* PSALM: Cardinality Estimation in the Presence of Fine-Grained Access Controls
#@ Huaxin Zhang;Ihab F. Ilyas;Kenneth Salem
#t 2009
#c 17
#! In database systems that support fine-grained access controls, each user has access rights that determine which tuples are accessible and which are inaccessible. Queries are answered as if the inaccessible tuples are not present in the database. Thus, users with different access rights may get different answers to a given query. To process queries efficiently in the presence of fine-grained access controls, the database system needs accurate estimates of the number of tuples that are both accessible according to the access rights of the submitting user and relevant according to the selection predicates in the query. In this paper, we present PSALM, a sampling-based cardinality estimation technique for use in the presence of fine-grained access controls. Our technique exploits the fact that access rights are relatively static and are common to all queries that are evaluated on behalf of a particular user. We show that PSALM provides more accurate estimates than techniques that do not exploit knowledge of access rights.

#index 1206957
#* Effective XML Keyword Search with Relevance Oriented Ranking
#@ Zhifeng Bao;Tok Wang Ling;Bo Chen;Jiaheng Lu
#t 2009
#c 17
#! Inspired by the great success of information retrieval (IR) style keyword search on the web, keyword search on XML has emerged recently. The difference between text database and XML database results in three new challenges: (1) Identify the user search intention, i.e. identify the XML node types that user wants to search for and search via. (2) Resolve keyword ambiguity problems: a keyword can appear as both a tag name and a text value of some node; a keyword can appear as the text values of different XML node types and carry different meanings. (3) As the search results are sub-trees of the XML document, new scoring function is needed to estimate its relevance to a given query. However, existing methods cannot resolve these challenges, thus return low result quality in term of query relevance. In this paper, we propose an IR-style approach which basically utilizes the statistics of underlying XML data to address these challenges. We first propose specific guidelines that a search engine should meet in both search intention identification and relevance oriented ranking for search results. Then based on these guidelines, we design novel formulae to identify the search for nodes and search via nodes of a query, and present a novel XML TF*IDF ranking strategy to rank the individual matches of all possible search intentions. Lastly, the proposed techniques are implemented in an XML keyword search engine called XReal, and extensive experiments show the effectiveness of our approach.

#index 1206958
#* Distributed Structural Relaxation of XPath Queries
#@ Georgia Koloniari;Evaggelia Pitoura
#t 2009
#c 17
#! Due to the structural heterogeneity of XML, queries are often interpreted approximately. This is achieved by relaxing the query and ranking the results based on their relevance to the original query. Query relaxation over distributed XML repositories may incur large communication costs, since partial result lists from different sites need to be gathered and ranked to assembly the overall top-k results. To process such queries efficiently, we propose using a distributed clustered index to group documents based on their structural similarity. The clustered index proves to be very effective in reducing the sizes of the partial lists that need to be combined. Furthermore, it can be used as the basis of a pay-as-you-go approach, where clusters of documents are accessed gradually providing the user with increasingly improving results. To reduce the cost of constructing and maintaining the clustered index, we use a compact data structure that trades-off accuracy for storage and communication efficiency. The index is also used for selectivity estimation so that query relaxation is geared towards the most promising structural transformations. Our experimental results show that our approach significantly reduces the communication cost for retrieving the top-k results, while maintaining a low construction cost for the clustered index.

#index 1206959
#* Sketch-Based Summarization of Ordered XML Streams
#@ Veronica Mayorga;Neoklis Polyzotis
#t 2009
#c 17
#! In this paper, we tackle the problem of approximately answering a continuous aggregate query over an XML stream using limited memory. This problem is key in the development of tools for the on-line monitoring and analysis of streaming XML data, such as complex event streams, RSS feeds, or workflow traces. We introduce a novel technique that supports XML queries with any combination of the common XPath axes, namely, ancestor, descendant, parent, child, following, preceding, following-sibling, and preceding-sibling. At the heart of our approach lies an efficient transform that reduces a continuous XML query to an equi-join query over relational streams. We detail the transform and discuss its integration with randomized sketches as a basic mechanism to estimate the result of the XML query. We further enhance this mechanism with structural sieving, a technique that takes advantage of the XML data and query characteristics in order to improve the accuracy of the sketch-based approximation. We present an extensive experimental study on real-life and synthetic data sets that validates the effectiveness of our approach and demonstrates its advantages over existing techniques.

#index 1206960
#* Using Semantics for Speech Annotation of Images
#@ Chaitanya Desai;Dmitri V. Kalashnikov;Sharad Mehrotra;Nalini Venkatasubramanian
#t 2009
#c 17
#! Digital cameras and multimedia capture devices are becoming increasingly popular to take pictures. Annotating these pictures is important to support their browsing and retrieval. Fully automatic image annotation techniques typically rely entirely on visual properties of the image. The state of the art image annotation systems of this kind work well in detecting generic object classes: car, horse, motorcycle, airplane, etc. However, certain characteristics of the image are hard to capture using strictly the visual properties. These include location (Paris, California, San Francisco, etc), event (birthday, wedding, graduation ceremony, etc), people (John, Jane, brother, etc) and abstract qualities referring to objects in the image (beautiful, funny, sweet, etc) among others. The more conventional method of annotation that relies completely on human input has several limitations as well. Typing tags using the keypads of such devices can be cumbersome and error-prone. Secondly, delay in tagging may result in a loss of context in which the picture was taken (e.g., user may not remember the names of the people/structures in the image). This presents an opportunity for using speech as a modality to annotate images and/or other multimedia content. Most camera devices have a built-in microphone. In principle, some of the challenges associated with both, fully automatic annotation as well as manual tagging can be alleviated if the user were to use speech as a medium of annotation. Ideally, the user would take a picture and speak the desired tags into the device's microphone. A speech recognizer would transcribe the audio signal into text. The speech to text transcription can happen either on the device itself or be done on a remote machine. The transcribed text can be used as tags for the image, exactly as the user intended. One of the biggest bottlenecks facing such systems is the accuracy of the underlying speech recognizer. Even speaker dependent recognition systems can make mistakes in noisy environments. If the recognizer's output is considered as is for annotation, then poor recognition will lead to poor quality tags. Our work tries to address this issue by incorporating outside semantic knowledge to improve interpretation of the recognizer's output, as opposed to blindly believing what the recognizer suggests. To improve interpretation of speech output, we exploit the fact that most speech recognizers provide alternate hypotheses for each utterance. The main contribution of this paper is our approach for annotating images using speech as the input modality. The approach employs a probabilistic model for computing the joint probability of a given combination of tags using a Maximum Entropy solution. The extensive empirical evaluation demonstrates the advantage of the proposed solution, that leads to a significant improvement of quality of speech annotation.

#index 1206961
#* Discovering Conditional Functional Dependencies
#@ Wenfei Fan;Floris Geerts;Laks V.  S. Lakshmanan;Ming Xiong
#t 2009
#c 17
#! This paper investigates the discovery of conditional functional dependencies (CFDs). CFDs are a recent extension of functional dependencies (FDs) by supporting patterns of semantically related constants, and can be used as rules for cleaning relational data. However, finding CFDs is an expensive process that involves intensive manual effort. To effectively identify data cleaning rules, we develop techniques for discovering CFDs from sample relations. We provide three methods for CFD discovery. The first, referred to as CFDMiner, is based on techniques for mining closed itemsets, and is used to discover constant CFDs, namely, CFDs with constant patterns only. The other two algorithms are developed for discovering general CFDs. The first algorithm, referred to as CTANE, is a levelwise algorithm that extends TANE, a well-known algorithm for mining FDs. The other, referred to as FastCFD, is based on the depthfirst approach used in FastFD, a method for discovering FDs. It leverages closed-itemset mining to reduce search space. Our experimental results demonstrate the following. (a) CFDMiner can be multiple orders of magnitude faster than CTANE and FastCFD for constant CFD discovery. (b) CTANE works well when a given sample relation is large, but it does not scale well with the arity of the relation. (c) FastCFD is far more efficient than CTANE when the arity of the relation is large.

#index 1206962
#* Integrating and Ranking Uncertain Scientific Data
#@ Landon Detwiler;Wolfgang Gatterbauer;Brent Louie;Dan Suciu;Peter Tarczy-Hornoch
#t 2009
#c 17
#! Mediator-based data integration systems resolve exploratory queries by joining data elements across sources. In the presence of uncertainties, such multiple expansions can quickly lead to spurious connections and incorrect results. The BioRank project investigates formalisms for modeling uncertainty during scientific data integration and for ranking uncertain query results. Our motivating application is protein function prediction. In this paper we show that: (i) explicit modeling of uncertainties as probabilities increases our ability to predict less-known or previously unknown functions (though it does not improve predicting the well-known). This suggests that probabilistic uncertainty models offer utility for scientific knowledge discovery; (ii) small perturbations in the input probabilities tend to produce only minor changes in the quality of our result rankings. This suggests that our methods are robust against slight variations in the way uncertainties are transformed into probabilities; and (iii) several techniques allow us to evaluate our probabilistic rankings efficiently. This suggests that probabilistic query evaluation is not as hard for real-world problems as theory indicates.

#index 1206963
#* Adaptive Multi-join Query Processing in PDBMS
#@ Sai Wu;Quang Hieu Vu;Jianzhong Li;Kian-Lee Tan
#t 2009
#c 17
#! Traditionally, distributed databases assume that the small) set of nodes participating in a query is known apriori, the data is well placed, and the statistics are readily available. However, these assumptions are no longer valid in a Peer-based DataBase Management System (PDBMS). As such, it is a challenge to process and optimize queries in a PDBMS. In this paper, we present our distributed solution to this problem for multi-way join queries. Our approach first processes a multi-way join query based on an initial query evaluation plan (generated using statistical data that may be obsolete or inaccurate); as the query is beingprocessed, statistics obtained on-the-fly are used to (continuously) refine the current plan dynamically into a more effective one. We have conducted an extensive performance study which shows that our adaptive query processing strategy can reduce the network traffic significantly.

#index 1206964
#* Database Management as a Service: Challenges and Opportunities
#@ Divyakant Agrawal;Amr El Abbadi;Fatih Emekci;Ahmed Metwally
#t 2009
#c 17
#! Data outsourcing or database as a service is a new paradigm for data management in which a third party service provider hosts a database as a service. The service provides data management for its customers and thus obviates the need for the service user to purchase expensive hardware and software, deal with software upgrades and hire professionals for administrative and maintenance tasks. Since using an external database service promises reliable data storage at a low cost it is very attractive for companies. Such a service would also provide universal access, through the Internet to private data storedat reliable and secure sites. A client would store their data, and not need to carry their data with them as they travel. They would also not need to log remotely to their home machines, which may suffer from crashes and be unavailable. However, recent governmental legislations, competition among companies, and database thefts mandate companies to use secure and privacy preserving data management techniques. The data provider, therefore, needs to guarantee that the data is secure, be able to execute queries on the data, and the results of the queries must also be secure and not visible to the data provider. Current research has been focused only on how to index and query encrypted data. However, querying encrypted data is computationally very expensive. \emph{Providing an efficient trust mechanism} to push both database service providers and clients to behave honestly has emerged as one of the most important problem before data outsourcing to become a viable paradigm. In this paper, we describe scalable privacy preserving algorithms for data outsourcing. Instead of encryption, which is computationally expensive, we use distribution on multiple data provider sites and information theoretically proven secret sharing algorithms as the basis for privacy preserving outsourcing. The technical contributions of this paper is the establishment and development of a framework for efficient fault-tolerant scalable and theoretically secure privacy preserving data outsourcing that supports a diversity of database operations executed on different types of data, which can even leverage publicly available data sets.

#index 1206965
#* Adaptive Parallelization of Queries over Dependent Web Service Calls
#@ Manivasakan Sabesan;Tore Risch
#t 2009
#c 17
#! We have developed a system to process database queries over composed data providing web services. The queries are transformed into execution plans containing an operator that invokes any web service for given arguments. A common pattern in these query execution plans is that the output of one web service call is the input for another, etc. The challenge addressed in this paper is to develop methods to speed up such dependent calls in queries by parallelization. Since web service calls incur high-latency and message set-up costs, a naïve approach making the calls sequentially is time consuming and parallel invocations of the web service calls should improve the speed. Our approach automatically parallelizes the web service calls by starting separate query processes, each managing a parameterized sub-query, a plan function, for different parameter tuples. For a given query, the query processes are automatically arranged in a multi-level process tree where plan functions are called in parallel. The parallel plan is defined in terms of an algebra operator, FF_APPLYP, to ship in parallel to other query processes the same plan function for different parameters. By using FF_APPLYP we first investigated ways to set up different process trees manually. We concluded from our experiments that the best performing query execution plan is an almost balanced bushy tree. To automatically achieve the optimal process tree we modified FF_APPLYP to an operator AFF_APPLYP that adapts a parallel plan locally in each query process until an optimized performance is achieved. AFF_APPLYP starts with a binary process tree. During execution each query process in the tree makes local decisions to expand or shrink its process sub-tree by comparing the average time to process each incoming tuple. The query execution time obtained with AFF_APPLYP is shown to be close to the best time achieved by manually built query process trees.

#index 1206966
#* Towards Composition as a Service - A Quality of Service Driven Approach
#@ Florian Rosenberg;Philipp Leitner;Anton Michlmayr;Predrag Celikovic;Schahram Dustdar
#t 2009
#c 17
#! Software as a Service (SaaS) and the possibility to compose Web services provisioned over the Internet are important assets for a service-oriented architecture (SOA). However, the complexity and time for developing and provisioning a composite service is very high and it is generally an error-prone task. In this paper we address these issues by describing a semi-automated "Composition as a Service'' (CaaS) approach combined with a domain-specific language called VCL (Vienna Composition Language). The proposed approach facilitates rapid development and provisioning of composite services by specifying what to compose in a constraint-hierarchy based way using VCL. Invoking the composition service triggers the composition process and upon success the newly composed service is immediately deployed and available. This solution requires no client-side composition infrastructure because it is transparently encapsulated in the CaaS infrastructure.

#index 1206967
#* Universal Resource Lifecycle Management
#@ Marcos Báez;Fabio Casati;Maurizio Marchese
#t 2009
#c 17
#! This paper presents a model and a tool that allows Web users to define, execute, and manage lifecycles for any artifact available on the Web. In the paper we show the need for lifecycle management of Web artifacts, and we show in particular why it is important that non-programmers are also able to do this. We then discuss why current models do not allow this, and we present a model and a system implementation that achieves lifecycle management for any URI-identifiable and accessible object. The most challenging parts of the work lie in the definition of a simple but universal model and system (and in particular in allowing universality and simplicity to coexist) and in the ability to hide from the lifecycle modeler the complexity intrinsic in having to access and manage a variety of resources, which differ in nature, in the operations that are allowed on them, and in the protocols and data formats required to access them.

#index 1206968
#* Flexible XML Querying Using Skyline Semantics
#@ Sara Cohen;Maayan Shiloach
#t 2009
#c 17
#! Preferences over results of an XML query are of two distinct flavors. First, the user may prefer results which contain desired values, e.g., lower prices, favorite foods, higher ratings. Second, the user may prefer results with a certain structure, e.g., existence of a "discount" node, existence of an edge (and not only a path) between "departure" and "arrival" nodes. The first type of preference has been studied extensively over relational data, using skyline semantics, but has barely been considered for XML. The second type of preference has been studied for XML in the context of inexact querying, using scoring functions to rank results. This paper presents a query language for XML that incorporates both value-based and structural desires. Skyline semantics is used to determine optimal results. Algorithms for query evaluation under skyline semantics are presented and experimentation proves efficiency. The paper is novel in three aspects. First, it considers skyline querying over XML data values, and not over values in a relational database. Second, it presents a method for inexact querying of the structure of XML that is based on computing a skyline, instead of using scoring functions. Third, it combines both types of user preference into a single language. These facets join together to yield a versatile language for flexible querying of XML.

#index 1206969
#* Efficient Distribution of Full-Fledged XQuery
#@ Ying Zhang;Nan Tang;Peter Boncz
#t 2009
#c 17
#! We investigate techniques to automatically decompose any XQuery query into subqueries, that can be executed near their data sources; i.e., function-shipping. In this scenario, the subqueries being executed remotely may have XML node-valued parameters or results, that must be shipped in some way. The main challenge addressed here is to ensure that the decomposed queries properly respect XML node identity and preserve structural properties, when (parts of) XML nodes are sent over the network, effectively copying them. We start by precisely characterizing the conditions, under which pass-by-value parameter passing causes semantic differences between remote execution of an XQuery expression and its local execution. We then formulate a conservative strategy that effectively avoids decomposition in such cases. To broaden the possibilities of query distribution, we extend the pass-by value semantics to a pass-by-fragment semantics, which keeps better track of node identities and structural properties. The pass-by-fragment semantics is subsequently refined to a pass-by projection semantics by means of a novel runtime XML projection technique, which safely eliminates most semantic differences between the local and remote execution of an XQuery expression, and strongly reduces message sizes. The proposed techniques are implemented in XRPC, a simple yet efficient XQuery extension that enables function-shipping by adding a Remote Procedure Call mechanism to XQuery. Experiments on MonetDB/XQuery establish the performance potential of our XQuery decomposition techniques.

#index 1206970
#* X-CSR: Dataflow Optimization for Distributed XML Process Pipelines
#@ Daniel Zinn;Shawn Bowers;Timothy McPhillips;Bertram Ludäscher
#t 2009
#c 17
#! XML process networks are a simple, yet powerful programming paradigm for loosely coupled, coarse-grained dataflow applications such as data-centric scientific workflows. We describe a framework called Delta-XML that is well-suited for applications in which pipelines of data processors modify parts ("deltas'') of XML data collections while keeping the overall collection structure intact. We show how to optimize the execution of Delta-XML process networks by minimizing the data shipping cost in distributed settings. This X-CSR** optimization employs static type inference based on XML Schema to determine the XML stream fragments that are relevant to a processor, allowing irrelevant fragments to be bypassed ("shipped'') to downstream pipeline steps. Finally, we present evaluation results for a real-world scientific workflow, which shows the practical feasibility of X-CSR. A long version of this paper is available as technical report (http://www.cs.ucdavis.edu/research/tech-reports/2008/CSE-2008-15.pdf).** X-CSR: _X_ML _C_ut, _S_hip, and _R_eassemble; pronounced "X-scissor''

#index 1206971
#* Improving Transaction-Time DBMS Performance and Functionality
#@ David B. Lomet;Feifei Li
#t 2009
#c 17
#! Immortal DB is a transaction time database system that is built into a commercial database system rather than being layered on top. This enables it to have performance that is very close to the performance of an unversioned current time database system. Achieving such competitive performance is essential for wide acceptance of this temporal functionality. In this paper we describe further performance improvements in two critical dimensions. First Immortal DB range search performance is improved for current time data via improved current version storage utilization, making this performance essentially the same as unversioned performance. Second, Immortal DB update performance is increased by further reducing the cost for the timestamping of versions. Finally, we show how a simple modification, integrated into the timestamping mechanism, can provide a foundation for auditing database activity. Our algorithms have been incorporated into a commercial database engine and experiments using this database engine demonstrate the effectiveness of our approach.

#index 1206972
#* Confidence-Aware Join Algorithms
#@ Parag Agrawal;Jennifer Widom
#t 2009
#c 17
#! In uncertain and probabilistic databases, confidence values (or probabilities) are associated with each data item. Confidence values are assigned to query results based on combining confidences from the input data. Users may wish to apply a threshold on result confidence values, ask for the "top-$k$'' results by confidence, or obtain results sorted by confidence. Efficient algorithms for these types of queries can be devised by exploiting properties of the input data and the combining functions for result confidences. Previous algorithms for these problems assumed sufficient memory was available for processing. In this paper, we address the problem of processing all three types of queries when sufficient memory is not available, minimizing retrieval cost. We present algorithms, theoretical guarantees, and experimental evaluation.

#index 1206973
#* Finding Equivalent Rewritings with Exact Views
#@ Michael Compton
#t 2009
#c 17
#! This paper presents a study of equivalent rewritings for conjunctive queries and views: that is, queries and views expressed in terms of database predicates, but not built-in predicates. It analyses conjunctive queries and views with an exact, set semantics, corrects some errors in prior work and shows necessary and sufficient conditions on the query and views for an equivalent rewriting to exist, including showing when a view covers part of a query. The paper presents a sound and complete algorithm for finding equivalent rewritings.

#index 1206974
#* Uncertain Location Based Range Aggregates in a Multi-dimensional Space
#@ Ying Zhang;Xuemin Lin;Yufei Tao;Wenjie Zhang
#t 2009
#c 17
#! Uncertain data are inherent in many applications such as environmental surveillance and quantitative economics research. Recently, considerable research efforts have been put into the field of analysing uncertain data. In this paper, we study the problem of processing the uncertain location based range aggregate in a multi-dimensional space. We first formally introduce the problem, then propose a general filtering-and-verification framework to solve the problem. Two filtering techniques, named STF and PCR respectively, are proposed to significantly reduce the verification cost.

#index 1206975
#* Efficient Evaluation of Multiple Preference Queries
#@ Leong Hou U;Nikos Mamoulis;Kyriakos Mouratidis
#t 2009
#c 17
#! Consider multiple users searching for a hotel room, based on size, cost, distance to the beach, etc. Users may have variable preferences expressed by different weights on the attributes of the searched objects. Although individual preference queries can be evaluated by selecting the object in the database with the highest aggregate score, in the case of multiple requests at the same time, a single object cannot be assigned to more than one users. The challenge is to compute a fair 1-1 matching between the queries and a subset of the objects. We model this as a stable-marriage problem and propose an efficient technique for its evaluation. Our algorithm is an iterative process, which finds at each step the query-object pair with the highest score and removes it from the problem. This is done efficiently by maintaining and matching the skyline of the remaining objects with the remaining queries at each step. An experimental evaluation with synthetic and real data confirms the effectiveness of our method.

#index 1206976
#* iBroker: An Intelligent Broker for Ontology Based Publish/Subscribe Systems
#@ Myung-Jae Park;Chin-Wan Chung
#t 2009
#c 17
#! In this paper, we present iBroker, an Intelligent Broker for ontology based publish/subscribe systems which syntactically and semantically match incoming OWL data to multiple user profiles. iBroker effectively manages user profiles based on the semantics of query patterns in user profiles formulated in SPARQL. iBroker uses a semantic matching algorithm to efficiently process OWL data and generate the complete results for user profiles, considering the core semantics of OWL. Experimental results demonstrate that iBroker is more efficient and scalable compared to an existing broker for ontology based publish/subscribe systems.

#index 1206977
#* Extracting Web Query Interfaces Based on Form Structures and Semantic Similarity
#@ Jun Hong;Zhongtian He;David A. Bell
#t 2009
#c 17
#! Web databases are now pervasive. Such a database can be accessed via its query interface (usually HTML query form) only. Extracting Web query interfaces is a critical step in data integration across multiple Web databases, which creates a formal representation of a query form by extracting a set of query conditions in it. This paper presents a novel approach to extracting Web query interfaces. In this approach, a generic set of query condition rules are created to define query conditions that are semantically equivalent to SQL search conditions. Query condition rules represent the semantic roles that labels and form elements play in query conditions, and how they are hierarchically grouped into constructs of query conditions. To group labels and form elements in a query form, we explore both their structural proximity in the hierarchy of structures in the query form, which is captured by a tree of nested tags in the HTML codes of the form, and their semantic similarity, which is captured by various short texts used in labels, form elements and their properties. We have implemented the proposed approach and our experimental results show that the approach is highly effective.

#index 1206978
#* Efficient Indices Using Graph Partitioning in RDF Triple Stores
#@ Ying Yan;Chen Wang;Aoying Zhou;Weining Qian;Li Ma;Yue Pan
#t 2009
#c 17
#! With the advance of the Semantic Web, varying RDF data were increasingly generated, published, queried, and reused via the Web. For example, the DBpedia, a community effort to extract structured data from Wikipedia articles, broke 100 million RDF triples in its latest release. Initiated by Tim Berners-Lee,likewise, the Linking Open Data (LOD) project has published and interlinked many open licence datasets which consisted of over 2 billion RDF triples so far. In this context, fast query response over such large scaled data would be one of the challenges to existing RDF data stores. In this paper, we propose a novel triple indexing scheme to help RDF query engine fast locate the instances within a small scope. By considering the RDF data as a graph, we would partition the graph into multiple subgraph pieces and store them individually, over which a signature tree would be built up to index the URIs. When a query arrives, the signature tree index is used to fast locate the partitions that might include the matches of the query by its constant URIs. Our experiments indicate that the indexing scheme dramatically reduces the query processing time in most cases because many partitions would be early filtered out and the expensive exact matching is only performed over a quite small scope against the original dataset.

#index 1206979
#* Secure Enterprise Services Consumption for SaaS Technology Platforms
#@ Yuecel Karabulut;Ike Nassi
#t 2009
#c 17
#! Over recent years there has been increased level of discussion on utility pricing for software. The focus of these discussions is to create new operating cost models where the unit costs are directly tied to the business operations to which they contribute. While creating a fine-grained operating cost model is very important for software solutions such as SaaS, the anticipated technology platforms will need to rely on a set of security mechanisms in order to provide a secure and trustworthy service consumption environment. We present an architecture for secure enterprise services consumption management system and a protocol for secure service consumption for service-oriented technology platforms. Our approach is performance sensitive and utilizes a novel combination of asymmetric and symmetric cryptography, and capability based access control. Access to technology platform services is regulated based on the permissions encoded in cryptographic capability tokens. In this paper we report a work in progress.

#index 1206980
#* Study of Web Delivered Services Support Platform
#@ Changjie Guo
#t 2009
#c 17
#! Recently, On-demand software and subscription based business model has been touted as the wave of the future to replace the traditional licensed software. Web delivered applications are perfect SaaS (software as a service) because of the characteristics like easy-to-use, central management, and global deliverable etc. However, this novel business model also involve many different roles owning their specific value proposition and needs. This talk will review the challenges and introduce our proposed technical solution and experience in the industry practice, to set up an effective and profitable SaaS ecosystem for the large-scale service provider to deliver internet-based business services to a huge amounts of clients by working with plenty of service vendors.

#index 1206981
#* Design Patterns for Internet-Scale Services
#@ Jinquan Dai
#t 2009
#c 17
#! There are dramatic differences between delivering software as services in the cloud for millions to use, versus distributing software as bits for millions to run on their PCs.First, "the data center is the computer". Services are developed to run on distributed data centers for high scalability, high availability and low latency. The data centers have a scale-out shared-nothing architecture (made up of thousands of independent, unreliable servers) and are distributed in geography. In addition, there is also a move to many small, cheap, independent and flaky data centers for cost and energy effectiveness. Therefore, the service needs to be programmed to have its data set to be partitioned and spread around independent servers, and to be replicated across independent servers and data centers.Second, "the application is always offline". Despite increasingly ubiquitous wireless connectivity, clients will be occasionally disconnected. In addition, due to increasingly ubiquitous service compositions and data integrations (e.g., SOA, mashup and widget), remote services or data will be occasionally unavailable to the applications (either in the cloud or on the clients). Consequently, the application needs to be programmed to work with an assumption that it is always offline, by making progress based on its local states as much as possible, and by interacting with remote partners asynchronously to reconcile their states.Therefore, Internet-scale services need new design patterns and programming models for the partitioned data set with many copies that are changed independently. This is a huge software challenge. Big websites spend 70% of their efforts on undifferentiated heavy lifting (e.g., partitioning, replication and scaling) versus 30% on differentiated value (feature) creation. This talk will review the challenges for Internet-scale services and some of the emerging solutions to address those challenges, based on our experience in building Internet-scale service platforms as well as the industry best practices.

#index 1206982
#* SAP Business ByDesign
#@ Yuebo Zhou
#t 2009
#c 17
#! On-demand is a variant of SaaS (Software as a Service) with software which is specifically built for one-to-many hosting. SAP Business ByDesign is one of the most complete and adaptable on-demand business solutions designed to liberate mid size companies from the restrictions of traditional IT based on purely SOA (Service-Oriented Architecture). SAP addresses itself as a hosting vendor to provide on-demand business as of now. On-demand changes the technology and the business in particular areas of- Architecture which is designed for value, multi-tenancy enabling, configurable and customizable through the web, no installation, short setup, short configuration, painless/frequent upgrade, abilities taken for granted (security,availability, stability, robustness, performance and desirable with Web 2.0 technology for UI/Business logic.- Business model with different pricing model, various customer perceptions of risks, new cost structure and partnering via web methodologies.- Operational structure to enable network-based management of the software, centrally managed activities rather than at each customer side, logistics in web time and high visibility into hosting IT and quality issues from customer perspective. This talk will review the on-demand business model that SAP Business ByDesign takes, focusing on the typical requirements of a hosting provider especially in operational aspects, landscape of an on-demand environment, costing factors, etc. In addition, the talk covers Business ByDesign technical platforms as well as development/deployment/consuming lifecycle.

#index 1206983
#* Frontiers in Information and Software as Services
#@ K. Selçuk Candan;Wen-Syan Li;Thomas Phan;Minqi Zhou
#t 2009
#c 17
#! The high cost of creating and maintaining software and hardware infrastructures for delivering services to businesses has led to a notable trend toward the use of third-party service providers, which rent out network presence, computation power, and data storage space to clients with infrastructural needs. These third party service providers can act as data stores as well as entire software suites for improved availability and system scalability, reducing small and medium businesses' burden of managing complex infrastructures. This is called information/application outsourcing or software as a service (SaaS). Emergence of enabling technologies, such as service oriented architectures (SOA), virtual machines, and cloud computing, contribute to this trend. Scientific Grid computing, on-line software services, and business service networks are typical examples leveraging database and software as service paradigm. In this paper, we survey the technologies used to enable SaaS paradigm as well as the current offerings on the market. We also outline research directions in the field.

#index 1206984
#* Predicting Multiple Metrics for Queries: Better Decisions Enabled by Machine Learning
#@ Archana Ganapathi;Harumi Kuno;Umeshwar Dayal;Janet L. Wiener;Armando Fox;Michael Jordan;David Patterson
#t 2009
#c 17
#! One of the most challenging aspects of managing a very large data warehouse is identifying how queries will behave before they start executing. Yet knowing their performance characteristics --- their runtimes and resource usage --- can solve two important problems. First, every database vendor struggles with managing unexpectedly long-running queries. When these long-running queries can be identified before they start, they can be rejected or scheduled when they will not cause extreme resource contention for the other queries in the system. Second, deciding whether a system can complete a given workload in a given time period (or a bigger system is necessary) depends on knowing the resource requirements of the queries in that workload. We have developed a system that uses machine learning to accurately predict the performance metrics of database queries whose execution times range from milliseconds to hours. For training and testing our system, we used both real customer queries and queries generated from an extended set of TPC-DS templates. The extensions mimic queries that caused customer problems. We used these queries to compare how accurately different techniques predict metrics such as elapsed time, records used, disk I/Os, and message bytes. The most promising technique was not only the most accurate, but also predicted these metrics simultaneously and using only information available prior to query execution. We validated the accuracy of this machine learning technique on a number of HP Neoview configurations. We were able to predict individual query elapsed time within 20% of its actual time for 85% of the test queries. Most importantly, we were able to correctly identify both the short and long-running (up to two hour) queries to inform workload management and capacity planning.

#index 1206985
#* Space-Constrained Gram-Based Indexing for Efficient Approximate String Search
#@ Alexander Behm;Shengyue Ji;Chen Li;Jiaheng Lu
#t 2009
#c 17
#! Answering approximate queries on string collections is important in applications such as data cleaning, query relaxation, and spell checking, where inconsistencies and errors exist in user queries as well as data. Many existing algorithms use gram-based inverted-list indexing structures to answer approximate string queries. These indexing structures are "notoriously" large compared to the size of their original string collection. In this paper, we study how to reduce the size of such an indexing structure to a given amount of space, while retaining efﬁcient query processing. We ﬁrst study how to adopt existing inverted-list compression techniques to solve our problem. Then, we propose two novel approaches for achieving the goal: one is based on discarding gram lists, and one is based on combining correlated lists. They are both orthogonal to existing compression techniques, exploit a unique property of our setting, and offer new opportunities for improving query performance. For each approach we analyze its effect on query performance and develop algorithms for wisely choosing lists to discard or combine. Our extensive experiments on real data sets show that our approaches provide applications the ﬂexibility in deciding the tradeoff between query performance and indexing size, and can outperform existing compression techniques. An interesting and surprising ﬁnding is that while we can reduce the index size signiﬁcantly (up to 60% reduction) with tolerable performance penalties, for 20-40% reductions we can even improve query performance compared to original indexes.

#index 1206986
#* Exploring a Few Good Tuples from Text Databases
#@ Alpa Jain;Divesh Srivastava
#t 2009
#c 17
#! Information extraction from text databases is a useful paradigm to populate relational tables and unlock the considerable value hidden in plain-text documents. However, information extraction can be expensive, due to various complex text processing steps necessary in uncovering the hidden data. There are a large number of text databases available, and not every text database is necessarily relevant to every relation. Hence, it is important to be able to quickly explore the utility of running an extractor for a specific relation over a given text database before carrying out the expensive extraction task. In this paper, we present a novel exploration methodology of {\em finding a few good tuples} for a relation that can be extracted from a database which allows for judging the relevance of the database for the relation. Specifically, we propose the notion of a good(k, $\ell$) query as one that can return any $k$ tuples for a relation among the top-$\ell$ fraction of tuples ranked by their aggregated confidence scores, provided by the extractor; if these tuples have high scores, the database can be determined as relevant to the relation. We formalize the access model for information extraction, and investigate efficient query processing algorithms for good(k, $\ell$) queries, which do not rely on any prior knowledge about the extraction task or the database. We demonstrate the viability of our algorithms using a detailed experimental study with real text databases.

#index 1206987
#* SPROUT: Lazy vs. Eager Query Plans for Tuple-Independent Probabilistic Databases
#@ Dan Olteanu;Jiewen Huang;Christoph Koch
#t 2009
#c 17
#! A paramount challenge in probabilistic databases is the scalable computation of confidences of tuples in query results. This paper introduces an efficient secondary-storage operator for exact computation of queries on tuple-independent probabilistic databases. We consider the conjunctive queries without self-joins that are known to be tractable on any tuple-independent database, and queries that are not tractable in general but become tractable on probabilistic databases restricted by functional dependencies. Our operator is semantically equivalent to a sequence of aggregations and can be naturally integrated into existing relational query plans. As a proof of concept, we developed an extension of the PostgreSQL 8.3.3 query engine called SPROUT. We study optimizations that push or pull our operator or parts thereof past joins. The operator employs static information, such as the query structure and functional dependencies, to decide which constituent aggregations can be evaluated together in one scan and how many scans are needed for the overall confidence computation task. A case study on the TPC-H benchmark reveals that most TPC-H queries obtained by removing aggregations can be evaluated efficiently using our operator. Experimental evaluation on probabilistic TPC-H data shows substantial efficiency improvements when compared to the state of the art.

#index 1206988
#* Privacy-Preserving Singular Value Decomposition
#@ Shuguo Han;Wee Keong Ng;Philip S. Yu
#t 2009
#c 17
#! In this paper, we propose secure protocols to perform Singular Value Decomposition (SVD) for two parties over horizontally and vertically partitioned data. We propose various secure building blocks for the computations of QR algorithm so that it is privacy-preserving. Some of the proposed secure building blocks include Secure Matrix Multiplication, $(x+y)^{-1}$, and $\sqrt{x+y}$. Together, they allow us to derive Privacy-Preserving SVD (PPSVD) based on a privacy-preserving QR algorithm. Finally we conduct experiments to evaluate the proposed secure building blocks and protocols. The results show that the proposed protocols for SVD achieve high accuracy for matrices of small and medium size.

#index 1206989
#* OPAQUE: Protecting Path Privacy in Directions Search
#@ Ken C.  K. Lee;Wang-Chien Lee;Hong Va Leong;Baihua Zheng
#t 2009
#c 17
#! Directions search returns the shortest path from a source to a destination on a road network. However, the search interests of users may be exposed to the service providers, thus raising privacy concerns. For instance, a path query that finds a path from a resident address to a clinic may lead to a deduction about "who is related to what disease". To protect user privacy from accessing directions search services, we introduce the OPAQUE system, which consists of two major components: (1) an obfuscator that formulates obfuscated path queries by mixing true and fake sources/destinations; and (2) an obfuscated path query processor installed in the server for obfuscated path query processing. OPAQUE reduces the likelihood of path queries being revealed and allows retrieval of requested paths. We propose two types of obfuscated path queries, namely, independently obfuscated path query and shared obfuscated path query to strike a balance between privacy protection strength and query processing overhead, and to enhance privacy protection against collusion attacks.

#index 1206990
#* Metric Functional Dependencies
#@ Nick Koudas;Avishek Saha;Divesh Srivastava;Suresh Venkatasubramanian
#t 2009
#c 17
#! When merging data from various sources, it is often the case that small variations in data format and interpretation cause traditional functional dependencies (FDs) to be violated, without there being an intrinsic violation of semantics. Examples include differing address formats, or different reported latitude/longitudes for a given address. In this paper, we define metric functional dependencies, which strictly generalize traditional FDs by allowing small differences (controlled by a metric) in values of the consequent attribute of an FD. We present efficient algorithms for the verification problem: determining whether a given metric FD holds for a given relation. We experimentally demonstrate the validity and efficiency of our approach on various data sets that lie in multidimensional spaces.

#index 1206991
#* A General Proximity Privacy Principle
#@ Ting Wang;Shicong Meng;Bhuvan Bamba;Ling Liu;Calton Pu
#t 2009
#c 17
#! As an important privacy threat in anonymized data publication, the proximity breach is gaining increasing attention recently. Such breach occurs when an adversary concludes with high confidence that the sensitive value of a victim individual falls in a set of proximate values, even though with low confidence about the exact value. Most existing research efforts focus on the case of publishing data of specific types, e.g., (1) categorical sensitive data (different values have no sense of proximity) or (2) numerical sensitive data (different values can be strictly ordered), while failing to address the privacy threats for a much wider range of data models, where the similarity of specific values is defined by arbitrary functions. In this work, we study the problem of protecting \texts c{general proximity privacy}, with findings applicable to most existing data models. Specifically, we counter the attacks by introducing a novel privacy principle, ($\epsilon$, $\delta$)-dissimilarity. It requires that each sensitive value in a QI-group $G$ must be "dissimilar'' to at least $\delta$ percent of all other values in $G$, while the similarity is measured by $\epsilon$. We prove that ($\epsilon$, $\delta$)-dissimilarity, used in conjunction with $k$-anonymity, provides effective protection against linking attacks in terms of both exact-association and proximate-association. Furthermore, We present a theoretical analysis regarding the satisfiability of this principle.

#index 1206992
#* Efficient Private Record Linkage
#@ Mohamed Yakout;Mikhail J. Atallah;Ahmed Elmagarmid
#t 2009
#c 17
#! Record linkage is the computation of the associations among records of multiple databases. It arises in contexts like the integration of such databases, online interactions and negotiations, and many others. The autonomous entities who wish to carry out the record matching computation are often reluctant to fully share their data. In such a framework where the entities are unwilling to share data with each other, the problem of carrying out the linkage computation without full data exchange has been called private record linkage. Previous private record linkage techniques have made use of a third party. We provide efficient techniques for private record linkage that improve on previous work in that (i) they make no use of a third party; (ii) they achieve much better performance than that of previous schemes in terms of execution time and quality of output (i.e., practically without false negatives and minimal false positives). Our software implementation provides experimental validation of our approach and the above claims.

#index 1206993
#* BioNav: Effective Navigation on Query Results of Biomedical Databases
#@ Abhijith Kashyap;Vagelis Hristidis;Michalis Petropoulos;Sotiria Tavoulari
#t 2009
#c 17
#! Search queries on biomedical databases like PubMed often return a large number of results, only a small subset of which is relevant to the user. Ranking and categorization, which can also be combined, have been proposed to alleviate this information overload problem. Results categorization for biomedical databases is the focus of this work. A natural way to organize biomedical citations is according to their MeSH annotations, a comprehensive concept hierarchy used by PubMed. In this paper, we present the BioNav system, a novel search interface that enables the user to navigate large number of query results by organizing them using the MeSH concept hierarchy. First, the query results are organized into a navigation tree. Previous works expand the hierarchy in a predefined static manner. In contrast, BioNav uses an intuitive navigation cost model to decide what concepts to display at each step. Another difference from previous works is that the hierarchy is not strictly displayed level-by-level.

#index 1206994
#* Distance Oracles for Spatial Networks
#@ Jagan Sankaranarayanan;Hanan Samet
#t 2009
#c 17
#! The popularity of location-based services and the need to do real-time processing on them has led to an interest in performing queries on transportation networks, such as finding shortest paths and finding nearest neighbors. The challenge is that these operations involve the computation of distance along a spatial network rather than "as the crow flies." In many applications an estimate of the distance is sufficient, which can be achieved by use of an oracle. An approximate distance oracle is proposed for spatial networks that exploits the coherence between the spatial position of vertices and the network distance between them. Using this observation, a distance oracle is introduced that is able to obtain the epsilon-approximate network distance between two vertices of the spatial network. The network distance between every pair of vertices in the spatial network is efficiently represented by adapting the well-separated pair technique to spatial networks. Initially, use is made of an epsilon-approximate distance oracle of size O( n / epsilon^d ) that is capable of retrieving the approximate network distance in O(logn) time using a B-tree. The retrieval time can be theoretically reduced further to O(1) time by proposing another e-approximate distance oracle of size O(n log n / epsilon^d) that uses a hash table. Experimental results indicate that the proposed technique is scalable and can be applied to sufficiently large road networks. A 10%-approximate oracle (epsilon = 0.1) on a large network yielded an average error of 0.9% with 90% of the answers making an error of 2% or less and an average retrieval timeof 68µ seconds. Finally, a strategy for the integration of the distance oracle into any relational database system as well as using it to perform a variety of spatial queries such as region search, k-nearest neighbor search, and spatial joins on spatial networks is discussed.

#index 1206995
#* Reverse Furthest Neighbors in Spatial Databases
#@ Bin Yao;Feifei Li;Piyush Kumar
#t 2009
#c 17
#! Given a set of points $P$ and a query point $q$, the {\em reverse furthest neighbor} (\trfn) query fetches the set of points $p \in P$ such that $q$ is their furthest neighbor among all points in $P\cup\{q\}$. This is the monochromatic \trfn (\mrfn) query. Another interesting version of \trfn query is the {\em bichromatic reverse furthest neighbor} (\brfn) query. Given a set of points $P$, a query set $Q$ and a query point $q\in Q$, a \brfn query fetches the set of points $p\in P$ such that $q$ is the furthest neighbor of $p$ among all points in $Q$. The \trfn query has many interesting applications in spatial databases and beyond. For instance, given a large residential database (as $P$) and a set of potential sites (as $Q$) for building a chemical plant complex, the construction site should be selected as the one that has the maximum number of reverse furthest neighbors. This is an instance of the \brfn query. This paper presents the challenges associated with such queries and proposes efficient, R-tree based algorithms for both monochromatic and bichromatic versions of the \trfn queries. We analyze properties of the \trfn query that differentiate it from the widely studied reverse nearest neighbor queries and enable the design of novel algorithms. Our approach takes advantage of the furthest Voronoi diagrams as well asthe convex hulls of either the data set $P$ (in the \mrfn case) or the query set $Q$ (in the \brfn case). For the \brfn queries, we also extend the analysis to the situation when $Q$ is large in size and becomes disk-resident. Experiments on both synthetic and real data sets confirm the efficiency and scalability of proposed algorithmsover the brute-force search based approach.

#index 1206996
#* Spatial Range Querying for Gaussian-Based Imprecise Query Objects
#@ Yoshiharu Ishikawa;Yuichi Iijima;Jeffrey Xu Yu
#t 2009
#c 17
#! In sensor environments and moving robot applications, the position of an object is often known imprecisely because of measurement error and/or movement of the object. In this paper, we present query processing methods for spatial databases in which the position of the query object is imprecisely specified by a probability density function based on a Gaussian distribution. We define the notion of a probabilistic range query by extending the traditional notion of a spatial range query and present three strategies for query processing. Since the qualification probability evaluation of target objects requires numerical integration by a method such as the Monte Carlo method, reduction of the number of candidate objects that should be evaluated has a large impact on query performance. We compare three strategies and their combinations in terms of the experiments and evaluate their effectiveness.

#index 1206997
#* Keyword Search in Spatial Databases: Towards Searching by Document
#@ Dongxiang Zhang;Yeow Meng Chee;Anirban Mondal;Anthony K.  H. Tung;Masaru Kitsuregawa
#t 2009
#c 17
#! This work addresses a novel spatial keyword query called the m-closest keywords (mCK) query. Given a database of spatial objects, each tuple is associated with some descriptive information represented in the form of keywords. The mCK query aims to find the spatially closest tuples which match m user-specified keywords. Given a set of keywords from a document, mCK query can be very useful in geotagging the document by comparing the keywords to other geotagged documents in a database. To answer mCK queries efficiently, we introduce a new index called the bR*-tree, which is an extension of the R*-tree. Based on bR*-tree, we exploit a priori-based search strategies to effectively reduce the search space. We also propose two monotone constraints, namely the distance mutex and keyword mutex, as our a priori properties to facilitate effective pruning. Our performance study demonstrates that our search strategy is indeed efficient in reducing query response time and demonstrates remarkable scalability in terms of the number of query keywords which is essential for our main application of searching by document.

#index 1206998
#* Parallel Skyline Computation on Multicore Architectures
#@ Sungwoo Park;Taekyung Kim;Jonghyun Park;Jinha Kim;Hyeonseung Im
#t 2009
#c 17
#! With the advent of multicore processors,it has become imperative to write parallel programs if one wishes to exploit the next generation of processors. This paper deals with skyline computation as a case study of parallelizing database operations on multicore architectures. We compare two parallel skyline algorithms: a parallel version of the branch-and-bound algorithm (BBS) and a new parallel algorithm based on skeletal parallel programming. Experimental results show despite its simple design, the new parallel algorithm is comparable to parallel BBS in speed. For sequential skyline computation, the new algorithm far outperforms sequential BBS when the density of skyline tuples is low.

#index 1206999
#* Efficient Table Anonymization for Aggregate Query Answering
#@ Cecilia M. Procopiuc;Divesh Srivastava
#t 2009
#c 17
#! Privacy protection is a major concern when microdata is released for ad hoc analyses. Anonymization schemes have to guarantee privacy goals, as well as preserve sufficient information to support reasonably accurate answers to ad hoc queries. In this paper, we focus on the case when the sensitive attributes are numerical (e.g., salary) for which $(k,e)$-anonymity was shown to be an appropriate privacy goal. We develop efficient algorithms for two optimization criteria for $(k,e)$-anonymity schemes, significantly improving on previous results. We evaluate our methods on a large real dataset, and show that they are scalable and accurate.

#index 1207000
#* Max Regional Aggregate over Sensor Networks
#@ Yongzhen Zhuang;Lei Chen
#t 2009
#c 17
#! Nowadays, wireless sensor networks are widely used in many environmental monitoring applications. However, due to the limitation of the current hardware technology, sensors are often battery powered and it is very difficult to change batteries. Therefore, for applications over wireless sensor networks, it is a critical issue to save the energy of sensors. Many attempts have been made to answer various types of queries energy-efficiently, such as max, top-k, and skylines. However, all of them return the readings of individual sensors that satisfy the query constraint. In practice, query results based on individual sensor readings are unreliable because sensor readings are often noisy. Thus, in this paper, we present a new type of query, max aggregate query over a region (i.e. max regional aggregate), which aims to find a fixed-size region whose regional aggregate is the maximum among all the possible regions with the same size. Compared to traditional max queries, max regional aggregate is more reliable in detecting events. Designing an energy-efficient approach to answer max regional aggregate is non-trivial because a huge number of regions need to be investigated. Thus, in this paper, we propose a novel two-level sampling approach, with region and sensor sampling, to collect the sensor readings intelligently and compute the approximate max regional aggregate based on the collected results. Specifically, region sampling is used to select regions and sensor sampling isused to choose the sensors within a selected region. Our extensive simulation results demonstrate that the proposed two-level sampling approach can answer the max regional aggregate energy-efficiently with a desirable accuracy.

#index 1207001
#* Recommendation Diversification Using Explanations
#@ Cong Yu;Laks V.  S. Lakshmanan;Sihem Amer-Yahia
#t 2009
#c 17
#! We introduce the novel notion of {\em explanation-based diversification} to address the well-known problem of {\em over-specialization} in item recommendations. Over-specialization in recommender systems leads to result sets with items that are too similar to one another, thus reducing the diversity of results and limiting user choices. Traditionally, the problem is addressed through {\em attribute-based diversification}|grouping items in the result set that share many common attributes (e.g., genre for movies) and selecting only a limited number of items from each group. It is, however, not always applicable, especially for social content recommendations. For example, attributes may not be available as in the case of recommending URLs for users of del.icio.us. Explanation-based diversification provides a novel and complementary alternative|it leverages the {\em reason for which a particular item is being recommended} (i.e., explanation)|for diversifying the results, without the need to access the attributes of the items. In this paper, we formally define the problem of {\em explanation-based diversification} and, without going into the details of the actual diversification process, demonstrate its effectiveness on a real world data set, Yahoo!~Movies.

#index 1207002
#* Tree Indexing on Flash Disks
#@ Yinan Li;Bingsheng He;Qiong Luo;Ke Yi
#t 2009
#c 17
#! Large flash disks have become an attractive alternative to magnetic hard disks, due to their high random read performance, low energy consumption and other features. However, writes, especially random writes, on the flash disk are inherently much slower than reads because of the erase-before-write mechanism. To address this asymmetry of read-write speeds in indexing on the flash disk, we propose the FD-tree, a tree index designed with the logarithmic method and fractional cascading techniques. With the logarithmic method, an FD-tree consists of the head tree – a small B+-tree on the top, and a few levels of sorted runs of increasing sizes at the bottom. This design is write-optimized for the flash disk; in particular, an index search will potentially go through more levels or visit more nodes, but random writes are limited to the head tree and are subsequently transformed into sequential ones through merging into the lower runs. With the fractional cascading technique, we store pointers, called fences, in lower level runs to speed up the search. We evaluate the FD-tree in comparison with representative B+-tree variants under a variety of workloads. Our results show that the FD-tree has a similar search performance to the standard B+-tree, and a similar update performance to the write-optimized B+-tree variant. As a result, FD-tree outperforms all these B+-tree index variants on both update- and search-intensive workloads.

#index 1207003
#* A Model for Data Leakage Detection
#@ Panagiotis Papadimitriou;Hector Garcia-Molina
#t 2009
#c 17
#! We study the following problem: A data distributor has given sensitive data to a set of supposedly trusted agents (third parties). Some of the data is leaked and found in an unauthorized place (e.g., on the web or somebody's laptop). The distributor must assess the likelihood that the leaked data came from one or more agents, as opposed to having been independently gathered by other means. We propose data allocation strategies (across the agents) that improve the probability of identifying leakages. These methods do not rely on alterations of the released data (e.g., watermarks). In some cases we can also inject "realistic but fake" data records to further improve our chances of detecting leakage and identifying the guilty party.

#index 1207004
#* Topologically Sorted Skylines for Partially Ordered Domains
#@ Dimitris Sacharidis;Stavros Papadopoulos;Dimitris Papadias
#t 2009
#c 17
#! The vast majority of work on skyline queries considers totally ordered domains, whereas in many applications some attributes are partially ordered, as for instance, domains of set values, hierarchies, intervals and preferences. The only work addressing this issue has limited progressiveness and pruning ability, and it is only applicable to static skylines. This paper overcomes these problems with the following contributions. (i) We introduce a generic framework, termed TSS, for handling partially ordered domains using topological sorting. (ii) We propose a novel dominance check that eliminates false hits/misses, further enhancing progressiveness and pruning ability. (iii) We extend our methodology to dynamic skylines with respect to an input query. In this case, the dominance relationships change according to the query specification, and their computation is rather complex. We perform an extensive experimental evaluation demonstrating that TSS is up to 9 times and up to 2 orders of magnitude faster than existing methods in the static and the dynamic case, respectively.

#index 1207005
#* Routing Questions to the Right Users in Online Communities
#@ Yanhong Zhou;Gao Cong;Bin Cui;Christian S. Jensen;Junjie Yao
#t 2009
#c 17
#! Online forums contain huge amounts of valuable user-generated content. In current forum systems, users have to passively wait for other users to visit the forum systems and read/answer their questions. The user experience for question answering suffers from this arrangement. In this paper, we address the problem of "pushing" the right questions to the right persons, the objective being to obtain quick, high-quality answers, thus improving user satisfaction. We propose a framework for the efficient and effective routing of a given question to the top-k potential experts (users) in a forum, by utilizing both the content and structures of the forum system. First, we compute the expertise of users according to the content of the forum system—-this is to estimate the probability of a user being an expert for a given question based on the previous question answering of the user. Specifically, we design three models for this task, including a profile-based model, a thread-based model, and a cluster-based model. Second, we re-rank the user expertise measured in probability by utilizing the structural relations among users in a forum system. The results of the two steps can be integrated naturally in a probabilistic model that computes a final ranking score for each user. Experimental results show that the proposals are very promising.

#index 1207006
#* On Efficient Recommendations for Online Exchange Markets
#@ Zeinab Abbassi;Laks V.  S. Lakshmanan
#t 2009
#c 17
#! Presently several marketplace applications over online social networks are gaining popularity. An important class of applications is online market exchange of items. Examples include peerflix.com and readitswapit.co.uk. We model this problem as a social network where each user has two associated lists. The item list consists of items the user is willing to give away to other users. The wish list consists of items the user is interested in receiving. A transaction involves a user giving an item to another user. Users are motivated to transact in expectation of realizing their wishes. Wishes may be realized by a pair of users swapping items corresponding to each other's wishes, but more generally by means of users exchanging items through a cycle, where each user gives an item to the next user in the cycle, in accordance with the receiving user's wishes.The problem we consider is how to efficiently generate recommendations for item exchange cycles, for users in a social network. Each cycle has a value which is determined by the number of items exchanged through the cycle. We focus on the problem of generating recommendations under two models. In the deterministic model, the value of a recommendation is the total number of items exchanged through cycles. In the probabilistic model, there is a probability associated with a user transacting with another user and a user being willing to trade an item for another. The value of a recommendation then is the expected number of items exchanged. We show that under both models, the problem of determining an optimal recommendation is NP-complete and develop efficient approximation algorithms for both. We show that our algorithms have guaranteed approximation factors of 2k (for greedy), 2k −1 (for local search), and(2k + 1)/3 (for combination of greedy and local search) where k is the max cycle length. We also develop a so-called maximal algorithm, which does not have an approximation guarantee but is more efficient. We conduct a comprehensive set of experiments. Our experiments show that in practice, the approximation quality achieved by maximal is competitive w.r.t. that of the other algorithms. On the other hand, maximal outperforms all other algorithms on scalability w.r.t. network size.

#index 1207007
#* Querying Communities in Relational Databases
#@ Lu Qin;Jeffrey Xu Yu;Lijun Chang;Yufei Tao
#t 2009
#c 17
#! Keyword search on relational databases provides users with insights that they can not easily observe using the traditional RDBMS techniques. Here, an l-keyword query is specified by a set of l keywords, {k1, k2, · · · , kl}. It finds how the tuples that contain the keywords are connected in a relational database via the possible foreign key references. Conceptually, it is to find some structural information in a database graph, where nodes are tuples and edges are foreign key references. The existing work studied how to find connected trees for an l-keyword query. However, a tree may only show partial information about how those tuples that contain the keywords are connected. In this paper, we focus on finding communities for an l-keyword query. A community is an induced subgraph that contains all the l-keywords within a given distance. We propose new efficient algorithms to find all/top-k communities which consume small memory, for an l-keyword query. For top kl-keyword queries, our algorithm allows users to interactively enlarge k at run time. We conducted extensive performance studies using two large real datasets to confirm the efficiency of our algorithms.

#index 1207008
#* Instant Advertising in Mobile Peer-to-Peer Networks
#@ Zaiben Chen;Heng Tao Shen;Quanqing Xu;Xiaofang Zhou
#t 2009
#c 17
#! To explore the benefit of advertising instant and location-aware commercials that can not be effectively promoted by traditional medium like TV program and Internet, we propose in this paper a solution for disseminating instant advertisements to users within the area of interest through a Mobile Peer-to-Peer Network. This is a new application scenario, and we devise an opportunistic gossiping model for advertisement propagation with spatial and temporal constraints. As bandwidth and computational resources are limited in a wireless environment, two optimization mechanisms utilizing distance and velocity information are provided for reducing redundant advertising messages. User interest is also considered as another critica factor in adjusting the advertisement propagation model, and we adopt the FM algorithm to achieve efficient counting of distinct users' interests. Finally, we study the performance of our solution through simulation in NS-2. Compared with the naive flooding method, our approach achieves high quality delivery rate while reducing the number of messages by nearly an order of magnitude.

#index 1207009
#* Scalability for Virtual Worlds
#@ Nitin Gupta;Alan Demers;Johannes Gehrke;Philipp Unterbrunner;Walker White
#t 2009
#c 17
#! Networked virtual environments (net-VEs) are the next wave of digital entertainment, with Massively Multiplayer Online Games (MMOs) a very popular instance. Current MMO architectures are server-centric in that all game logic is executed at the servers of the company hosting the game. This architecture has lead to severe scalability problems, in particular since MMOs require realistic graphics and game physics – computationally expensive tasks that are currently computed centrally. We propose a distributed action based protocol for net-VEs that pushes most computation to the computers of the players and thereby achieves massive scalability. The key feature of our proposal is a novel distributed consistency model that allows us to explore the tradeoff between scalability, computational complexity at the server, and consistency. We investigate our model both theoretically and through a comprehensive experimental evaluation.

#index 1207010
#* Ef?cient Query Evaluation over Temporally Correlated Probabilistic Streams
#@ Bhargav Kanagal;Amol Deshpande
#t 2009
#c 17
#! Many real world applications such as sensor networks and other monitoring applications naturally generate probabilistic streams that are highly correlated in both time and space. Query processing over such streaming data must be cognizant of these correlations, since they can significantly alter the final query results. Several prior works have suggested approaches to handling correlations in probabilistic databases. However those approaches are either unable to represent the types of correlations that probabilistic streams exhibit, or can not be applied directly because of their complexity. In this paper, we develop a system for managing and querying such streams by exploiting the fact that most real-world probabilistic streams exhibit highly structured Markovian correlations. Our approach is based on the previously proposed framework of viewing probabilistic query evaluation as inference over graphical models; we show how to efficiently construct graphical models for the common stream processing operators, and how to efficiently perform inference over them in an incremental fashion. We also present an algorithm for operator ordering that judiciously rearranges the query operators to make the query evaluation tractable, if possible given the query. Our extensive experimental evaluation illustrates the advantages of exploiting the structured nature of correlations in probabilistic streams.

#index 1207011
#* Temporal Outlier Detection in Vehicle Traffic Data
#@ Xiaolei Li;Zhenhui Li;Jiawei Han;Jae-Gil Lee
#t 2009
#c 17
#! Outlier detection in vehicle traffic data is a practical problem that has gained traction lately due to an increasing capability to track moving vehicles in city roads. In contrast to other applications, this particular domain includes a very dynamic dimension: time. Many existing algorithms have studied the problem of outlier detection at a single instant in time. This study proposes a method for detecting temporal outliers with an emphasis on historical similarity trends between data points. Outliers are calculated from drastic changes in the trends. Experiments with real world traffic data show that this approach is effective and efficient.

#index 1207012
#* CoTS: A Scalable Framework for Parallelizing Frequency Counting over Data Streams
#@ Sudipto Das;Shyam Antony;Divyakant Agrawal;Amr El Abbadi
#t 2009
#c 17
#! Frequency counting, frequent elements and top-k queries form a class of operators that are used for a wide range of stream analysis applications. In spite of the abundance of these algorithms, all known techniques for answering data stream queries are sequential in nature. The imminent ubiquity of Chip Multi-Processor (CMP) architectures requires algorithms that can exploit the parallelism of such architectures. In this paper, we first evaluate different naive techniques for intra-operator parallelism, and summarize the insights obtained from the naive techniques. Our experimental analysis of the naive designs shows that intra-operator parallelism is not straightforward and requires a complete redesign of the system. We then propose an efficient and scalable framework for parallelizing frequency counting, frequent elements and top-k queries over data streams. The proposed CoTS (Co-operative Thread Scheduling) framework is based on the principle of threads co-operating rather than contending. Our experiments on a state-of-the-art quad-core chip multiprocessor architecture and synthetic data sets demonstrate the scalability of the proposed framework, and the efficiency is demonstrated by peak processing throughput of more than 60 million elements per second.

#index 1207013
#* Concept Clustering of Evolving Data
#@ Shixi Chen;Haixun Wang;Shuigeng Zhou
#t 2009
#c 17
#! In Web search, a user refines his search several times before he finds the information he needs. It is very likely that, in the search log, similar sequences of searches appear many times, as many users had searched the Web with the same intent. Precisely interpreting the intent of the user is difficult, even with the help of the search log: there might be numerous instances of such intent scattering in small pieces in the log, but none of them is comprehensive enough to describe the concept precisely. This scenario occurs in many applications. For example, patterns in Web search, Internet traffic, program execution traces, network events, etc., are often non-stationary, yet the same patterns recur over time. In this paper, we argue that visible patterns are generated by hidden intent or hidden concepts, and precisely characterizing such concepts is only possible if we cluster as much data generated by such concepts as possible and learn from the clustered data as a whole, instead of learning from a single episode of such concept. The benefits is obvious as it enables us not only to better understand the underlying system that generates the data, but also to recognize future instance of a concept as soon as it occurs. To achieve this, we introduce a clustering based approach, where we adopt a novel clustering criterion, validation error minimization, to ensure that the found concepts are unique and precise. We propose a two step algorithm, which uses enhanced dynamic programming and EM like methods for clustering. Experiments show that in benchmark datasets, our approach achieves the highest accuracy with lowest cost in comparison with the current best approaches.

#index 1207014
#* On Efficient Query Processing of Stream Counts on the Cell Processor
#@ Dina Thomas;Rajesh Bordawekar;Charu C. Aggarwal;Philip S. Yu
#t 2009
#c 17
#! In recent years, the sketch-based technique has been presented as an effective method for counting stream items on processors with limited storage and processing capabilities, such as the network processors. In this paper, we examine the implementation of a sketch-based counting algorithm on the heterogeneous multi-core Cell processor. Like the network processors, the Cell also contains on-chip special processors with limited local memories. These special processors enable parallel processing of stream items using short-vector data-parallel (SIMD) operations. We demonstrate that the inaccuracies of the estimates computed by straightforward adaptations of current sketch-based counting approaches are exacerbated by increased inaccuracies in approximating counts of low frequency items, and by the inherent space limitations of the Cell processor. To address these concerns, we implement a sketch-based counting algorithm, FCM, that is specifically adapted for the Cell processor architecture. FCM incorporates novel capabilities for improving estimation accuracy using limited space by dynamically identifying low- and high-frequency stream items, and using a variable number of hash functions per item as determined by an item's current frequency phase. We experimentally demonstrate that with similar space consumption, FCM computes better frequency estimates of both the low- and high-frequency items than a naive parallelization of an existing stream counting algorithm. Using FCM as the kernel, our parallel algorithm is able to scale the over all performance linearly as well as improve the estimate accuracy as the number of processors is increased. Thus, this work demonstrates the importance of adapting the algorithm to the specifics of the underlying architecture.

#index 1207015
#* Minimizing Communication Cost in Distributed Multi-query Processing
#@ Jian Li;Amol Deshpande;Samir Khuller
#t 2009
#c 17
#! Increasing prevalence of large-scale distributed monitoring and computing environments such as sensor networks, scientific federations, Grids etc., has led to a renewed interest in the area of distributed query processing and optimization. In this paper we address a general, distributed multi-query processing problem motivated by the need to minimize the communication cost in these environments. Specifically we address the problem of optimally sharing data movement across the communication edges in a distributed communication network given a set of overlapping queries and query plans for them (specifying the operations to be executed). Most of the problem variations of our general problem can be shown to be NP-Hard by a reduction from the Steiner tree problem. However, we show that the problem can be solved optimally if the communication network is a tree, and present a novel algorithm for finding an optimal data movement plan. For general communication networks, we present efficient approximation algorithms for several variations of the problem. Finally, we present an experimental study over synthetic datasets showing both the need for exploiting the sharing of data movement and the effectiveness of our algorithms at finding such plans.

#index 1207016
#* Sequence Pattern Query Processing over Out-of-Order Event Streams
#@ Mo Liu;Ming Li;Denis Golovnya;Elke A. Rundensteiner;Kajal Claypool
#t 2009
#c 17
#! Complex event processing has become increasingly important in modern applications, ranging from RFID tracking for supply chain management to real-time intrusion detection. A key aspect of complex event processing is to extract patterns from event streams to make informed decisions in real-time. However, network latencies and machine failures may cause events to arrive out-of-order at the event processing engine. State-of-the-art event stream processing technology experiences significant challenges when faced with out-of-order data arrival including output blocking, huge system latencies, memory resource overflow, and incorrect result generation. To address these problems, we propose two alternate solutions: aggressive and conservative strategies respectively to process sequence pattern queries on out-of-order event streams. The aggressive strategy produces maximal output under the optimistic assumption that out-of-order event arrival is rare. In contrast, to tackle the unexpected occurrence of an out-of-order event and with it any premature erroneous result generation, appropriate error compensation methods are designed for the aggressive strategy. The conservative method works under the assumption that out-of-order data may be common, and thus produces output only when its correctness can be guaranteed. A partial order guarantee (POG) model is proposed under which such correctness can be guaranteed. For robustness under spiky workloads, both strategies are supplemented with persistent storage support and customized access policies. Our experimental study evaluates the robustness of each method, and compares their respective scope of applicability with state-of-art methods.

#index 1207017
#* Computing Distance Histograms Ef?ciently in Scientific Databases
#@ Yi-Cheng Tu;Shaoping Chen;Sagar Pandit
#t 2009
#c 17
#! Particle simulation has become an important research tool in many scientific and engineering fields. Data generated by such simulations impose great challenges to database storage and query processing. One of the queries against particle simulation data, the spatial distance histogram (SDH) query, is the building block of many high-level analytics, and requires quadratic time to compute using a straightforward algorithm. In this paper, we propose a novel algorithm to compute SDH based on a data structure called density map, which can be easily implemented by augmenting a Quad-tree index. We also show the results of rigorous mathematical analysis of the time complexity of the proposed algorithm: our algorithm runs on O(N^{3/ 2}) for two-dimensional data and O(N^{5/3}) for three-dimensional data, respectively. We also propose an approximate SDH processing algorithm whose running time is unrelated to the input size N. Experimental results confirm our analysis and show that the approximate SDH algorithm achieves very high accuracy.

#index 1207018
#* Dynamic Approaches to In-network Aggregation
#@ Oliver Kennedy;Christoph Koch;Al Demers
#t 2009
#c 17
#! Collaboration between small-scale wireless devices depends on their ability to infer aggregate properties of all nearby nodes. The highly dynamic environment created by mobile devices introduces a silent failure mode that is disruptive to this kind of inference. We address this problem by presenting techniques for extending existing unstructured aggregation protocols to cope with failure modes introduced by mobile environments. The modified protocols allow devices with limited connectivity to maintain estimates of aggregates, despite \textit{unexpected} peer departures and arrivals.

#index 1207019
#* Chameleon: Context-Awareness inside DBMSs
#@ Hicham G. Elmongui;Walid G. Aref;Mohamed F. Mokbel
#t 2009
#c 17
#! Context is any information used to characterize the situation of an entity. Examples of contexts include time, location, identity, and activity of a user. This paper proposes a general context-aware DBMS, named Chameleon, that will eliminate the need for having specialized database engines, e.g., spatial DBMS, temporal DBMS, and Hippocratic DBMS, since space, time, and identity can be treated as contexts in the general context-aware DBMS. In Chameleon, we can combine multiple contexts into more complex ones using the proposed context composition, e.g., a Hippocratic DBMS that also provides spatio-temporal and location contextual services. As a proof of concept, we construct two case studies using the same context-aware DBMS platform within Chameleon. One treats identity as a context to realize a privacy-aware (Hippocratic) database server, while the other treats space as a context to realize a spatial database server using the same proposed constructs and interfaces of Chameleon.

#index 1207020
#* Query Rewrites with Views for XML in DB2
#@ P. Godfrey;J. Gryz;A. Hoppe;W. Ma;C. Zuzarte
#t 2009
#c 17
#! There is much effort to develop comprehensive support for the storage and querying of XML data in database management systems. The major developers have extended their systems to handle XML data natively. These have the advantage over stand-alone XML database systems that relational and XML data can be queried mutually. Indeed, recent SQL standards specify means to query relational and XML data together (called SQL/XML). These systems also now support XQuery, in addition to SQL. It is thus possible to mix the processing of relational and XML data via either query language. While there has been significant progress in efficient native storage systems for XML, there remain numerous challenges to handle efficiently queries over XML. There are efforts to adapt the strong optimization techniques used for relational ("SQL") queries for XML (and mixed) queries as well. One such technique, the materialized view, has been well studied, and well adopted, over the last decade as an effective technique for optimizing relational queries. Our work extends the use of materialized views for SQL/XML, and could be applied to XQuery. Within IBM DB2 9 (Viper), we implement query rewrite rules that enable the use of materialized views in the evaluation of queries over XML. % (We enable views over queries that employ XMLTable.) To accomplish this, it was necessary to extend the existing query matching and compensation framework in DB2 with new functionality. We consider what types of query rewrites based on XMLTable are possible, and which are feasible. We present a linear-time algorithm to determine the locality (self-containment) of XPath expressions within a schema-unaware environment, which we have implemented. We demonstrate the efficacy of our techniques via an experimental evaluation over a representative suite of SQL/XML queries and materialized views, executed over our DB2 prototype.

#index 1207021
#* A Decade of XML Data Management: An Industrial Experience Report from Oracle
#@ Zhen Hua Liu;Ravi Murthy
#t 2009
#c 17
#! XML and its related technologies have now been in use for almost a decade. There has been considerable amount of effort both from research and industry focusing on XML, XQuery/XPath, XSLT and SQL/XML processing in the database. Many research prototypes and industrial products have been built to satisfy the XML use cases. This paper reviews several use cases where XML databases are leveraged to build real-world XML applications. We discuss the lessons learnt in supporting both data-centric and document-centric XMLDB applications within a single database system and the need for the implementation of different XML storage, index and query optimisation techniques for different XML use cases. We show the value of managing XML in databases, the current challenges and improvements that will hopefully promote future research directions. This paper also provides a timely checkpoint of XML data management from industrial perspective with experience of developing and supporting Oracle XML products.

#index 1207022
#* Oracle Streams: A High Performance Implementation for Near Real Time Asynchronous Replication
#@ Lik Wong;Nimar S. Arora;Lei Gao;Thuvan Hoang;Jingwei Wu
#t 2009
#c 17
#! We present the architectural design and recent performance optimizations of a state of the art commercial database replication technology provided in Oracle Streams. The underlying design of Streams replication is a pipeline of components that are responsible for capturing, propagating, and applying logical change records (LCRs) from a source database to a destination database. Each LCR encapsulates a database change. The communication in this pipeline is now latch-free to increase the throughput of LCRs. In addition, the apply component now bypasses SQL whenever possible and uses a new latch-free metadata cache. We outline the algorithms behind these optimizations and quantify the replication performance improvement from each optimization. Finally, we demonstrate that these optimizations improve the replication performance by more than a factor of four and achieve replication throughput of over 20,000 LCRs per second with sub-second latency on commodity hardware.

#index 1207023
#* Letter from the Program Chairs
#@ 
#t 2009
#c 17

#index 1207024
#* Search Computing
#@ Stefano Ceri
#t 2009
#c 17
#! “Who are the strongest European competitors on software ideas? Who is the best doctor to cure insomnia in a nearby hospital? Where can I attend an interesting conference in my field close to a sunny beach?” This information is available on the Web, but no software system can accept such queries nor compute the answer. We hereby propose search computing as a new multi-disciplinary science which will provide the abstractions, foundations, methods, and tools required to answer these and many similar queries. While state-of-art search systems answer generic or domain-specific queries, search computing enables answering questions via a constellation of dynamically selected, cooperating search services. Search computing requires innovation in software principles, languages, interfaces and protocols, as well as contributions from other sciences such as mathematics, operations research, psychology, sociology, economical and legal sciences.

#index 1207025
#* Differencing Provenance in Scientific Workflows
#@ Zhuowei Bao;Sarah Cohen-Boulakia;Susan B. Davidson;Anat Eyal;Sanjeev Khanna
#t 2009
#c 17
#! Scientific workflow management systems are increasingly providing the ability to manage and query the provenance of data products. However, the problem of differencing the provenance of two data products produced by executions of the same specification has not been adequately addressed. Although this problem is NP-hard for general workflow specifications, an analysis of real scientific (and business) workflows shows that their specifications can be captured as series-parallel graphs overlaid with well-nested forking and looping. For this natural restriction, we present efficient, polynomial-time algorithms for differencing executions of the same specification and thereby understanding the difference in the provenance of their data products. We then describe a prototype called PDiffView built around our differencing algorithm. Experimental results demonstrate the scalability of our approach using collected, real workflows and increasingly complex runs.

#index 1207026
#* XOntoRank: Ontology-Aware Search of Electronic Medical Records
#@ Fernando Farfan;Vagelis Hristidis;Anand Ranganathan;Michael Weiner
#t 2009
#c 17
#! As the use of Electronic Medical Records (EMRs) becomes more widespread, so does the need for effective information discovery within them. Recently proposed EMR standards are XML-based. A key characteristic in these standards is the frequent use of ontological references, i.e., ontological concept codes appear as XML elements and are used to associate portions of the EMR document with concepts defined in a domain ontology. A rich corpus of work addresses searching XML documents. Unfortunately, these works do not make use of ontological references to enhance search. In this paper we present the XOntoRank system which addresses the problem of ontology-aware keyword search of XML documents with a particular focus on EMR XML documents. Our current prototypes and experiments use the Health Level Seven (HL7) Clinical Document Architecture (CDA) Release 2.0 standard of EMR representation and the Systematized Nomenclature of Human and Veterinary Medicine (SNOMED) ontology, although the presented techniques and results are applicable to any EMR hierarchical format and any ontology that defines concepts and relationships.

#index 1207027
#* Supporting Database Applications as a Service
#@ Mei Hui;Dawei Jiang;Guoliang Li;Yuan Zhou
#t 2009
#c 17
#! Multi-tenant data management is a form of Software as a Service (SaaS), whereby a third party service provider hosts databases as a service and provides its customers with seamless mechanisms to create, store and access their databases at the host site. One of the main problems in such a system, as we shall discuss in this paper, is scalability, namely the ability to serve an increasing number of tenants without too much query performance degradation. A promising way to handle the scalability issue is to consolidate tuples from different tenants into the same shared tables. However, this approach introduces two problems: 1) The shared tables are too sparse. 2)Indexing on shared tables is not effective. To resolve the problems, we propose a multi-tenant database system called M-Store, which provides storage and indexing services for multi-tenants. To improve the scalability of the system, we develop two techniques in M-Store: Bitmap Interpreted Tuple(BIT) and Multi-Separated Index (MSI). BIT is efficient in that it does not store NULLs from unused attributes in the shared tables and MSI provides flexibility since it only indexes each tenant's own data on frequently accessed attributes. We extended MySQL based on our proposed design and conducted extensive experiments. The experimental results show that our proposed approach is a promising multi-tenancy storage and indexing scheme which can be easily integrated into existing DBMS.

#index 1207028
#* GraphSig: A Scalable Approach to Mining Significant Subgraphs in Large Graph Databases
#@ Sayan Ranu;Ambuj K. Singh
#t 2009
#c 17
#! Graphs are being increasingly used to model a wide range of scientific data. Such widespread usage of graphs has generated considerable interest in mining patterns from graph databases. While an array of techniques exists to mine frequent patterns, we still lack a scalable approach to mine statistically significant patterns, specifically patterns with low p-values, that occur at low frequencies. We propose a highly scalable technique, called GraphSig, to mine significant subgraphs from large graph databases. We convert each graph into a set of feature vectors where each vector represents a region within the graph. Domain knowledge is used to select a meaningful feature set. Prior probabilities of features are computed empirically to evaluate statistical significance of patterns in the feature space. Following analysis in the feature space, only a small portion of the exponential search space is accessed for further analysis. This enables the use of existing frequent subgraph mining techniques to mine significant patterns in a scalable manner even when they are infrequent. Extensive experiments are carried out on the proposed techniques, and empirical results demonstrate that GraphSig is effective and efficient for mining significant patterns. To further demonstrate the power of significant patterns, we develop a classifier using patterns mined by GraphSig. Experimental results show that the proposed classifier achieves superior performance, both in terms of quality and computation cost, over state-of-the-art classifiers.

#index 1207029
#* Context-Aware Object Connection Discovery in Large Graphs
#@ James Cheng;Yiping Ke;Wilfred Ng;Jeffrey Xu Yu
#t 2009
#c 17
#! Given a large graph and a set of objects, the task of object connection discovery is to find a subgraph that retains the best connection between the objects. Object connection discovery is useful to many important applications such as discovering the connection between different terrorist groups for counter-terrorism operations. Existing work considers only the connection between individual objects; however, in many real problems the objects usually have a context (e.g., a terrorist belongs to a terrorist group). We identify the context for the nodes in a large graph. We partition the graph into a set of communities based on the concept of modularity, where each community becomes naturally the context of the nodes within the community. By considering the context we also significantly improve the efficiency of object connection discovery, since we break down the big graph into much smaller communities. We first compute the best intra-community connection by maximizing the amount of information flow in the answer graph. Then, we extend the connection to the inter-community level by utilizing the community hierarchy relation, while the quality of the inter-community connection is also ensured by modularity. Our experiments show that our algorithm is three orders of magnitude faster than the state-of-the-art algorithm, while the quality of the query answer is comparable.

#index 1207030
#* Scale-Up Strategies for Processing High-Rate Data Streams in System S
#@ Henrique Andrade;Bugra Gedik;Kun-Lung Wu;Philip S. Yu
#t 2009
#c 17
#! High performance stream processing is critical in sense-and-respond application domains – from environmental monitoring to algorithmic trading. In this paper, we focus on language and runtime support for improving the performance of sense-and-respond applications in processing data from high rate streams. The central tenet of this work is the definition of a streaming architectural pattern for these application domains and the programming model and the code generation framework to support it. Using IBM Research's System S middleware and the SPADE language, we demonstrate how to scale up a financial trading application.

#index 1207031
#* Design and Evaluation of the iMed Intelligent Medical Search Engine
#@ Gang Luo
#t 2009
#c 17
#! Searching for medical information on the Web is popular and important. However, medical search has its own unique requirements that are poorly handled by existing medical Web search engines. This paper presents iMed, the first intelligent medical Web search engine that extensively uses medical knowledge and questionnaire to facilitate ordinary Internet users to search for medical information. iMed introduces and extends expert system technology into the search engine domain. It uses several key techniques to improve its usability and search result quality. First, since ordinary users often cannot clearly describe their situations due to lack of medical background, iMed uses a questionnaire-based query interface to guide searchers to provide the most important information about their situations. Second, iMed uses medical knowledge to automatically form multiple queries from a searcher' answers to the questions. Using these queries to perform search can significantly improve the quality of search results. Third, iMed structures all the search results into a multi-level hierarchy with explicitly marked medical meanings to facilitate searchers' viewing. Lastly, iMed suggests diversified, related medical phrases at each level of the search result hierarchy. These medical phrases are extracted from the MeSH ontology and can help searchers quickly digest search results and refine their inputs. We evaluated iMed under a wide range of medical scenarios. The results show that iMed is effective and efficient for medical search.

#index 1207032
#* Business Intelligence from Voice of Customer
#@ L. Venkata Subramaniam;Tanveer A. Faruquie;Shajith Ikbal;Shantanu Godbole;Mukesh K. Mohania
#t 2009
#c 17
#! In this paper, we present a first of a kind system, called Business Intelligence from Voice of Customer (BIVoC), that can: 1) combine unstructured information and structured information in an information intensive enterprise and 2) derive richer business insights from the combined data. Unstructured information, in this paper, refers to Voice of Customer (VoC) obtained from interaction of customer with enterprise namely, conversation with call-center agents, email, and sms. Structured database reflect only those business variables that are static over (a longer window of) time such as, educational qualification, age group, and employment details. In contrast, a combination of unstructured and structured data provide access to business variables that reflect upto date dynamic requirements of the customers and more importantly indicate trends that are difficult to derive from a larger population of customers through any other means. For example, some of the variables reflected in unstructured data are problem/interest in a certain product, expression of dissatisfaction with the business provided, and some unexplored category of people showing certain interest/problem. This gives the BIVoC system the ability to derive business insights that are richer, more valuable and crucial to the enterprises than the traditional business intelligence systems which utilize onlystructured information. We demostrate the effectiveness of BIVoC system through one of our real-life engagements where the problem is to determine how to improve agent productivity in a call center scenario. We also highlight major challenges faced while dealing with unstructured information such as handling noise and linking with structured data.

#index 1207033
#* A Static Analysis Framework for Database Applications
#@ Arjun Dasgupta;Vivek Narasayya;Manoj Syamala
#t 2009
#c 17
#! Database developers today use data access APIs such as ADO.NET to execute SQL queries from their application. These applications often have security problems such as SQL injection vulnerabilities and performance problems such as poorly written SQL queries. However today's compilers have little or no understanding of data access APIs or DBMS, and hence the above problems can go undetected until much later in the application lifecycle. We present a framework that adapts traditional program analysis by leveraging understanding of data access APIs in order to identify such problems early on during application development. Our framework can analyze database application binaries that use ADO.NET data access APIs. We show how our framework can be used for a variety of analysis tasks such as SQL injection detection, workload extraction, identifying performance problems, and verifying data integrity constraints in the application.

#index 1207034
#* An Incremental Knowledge Acquisition Method for Improving Duplicate Invoices Detection
#@ Van Hai Ho;Paul Compton;Boualem Benatallah;Julien Vayssière;Lucio Menzel;Hartmut Vogler
#t 2009
#c 17
#! Duplicate records are a major problem and duplicate invoices are a specific example of this. The detection of duplicate invoices is a critical issue for business since duplicate invoices can result in a company paying more than once for goods or services ordered. Past experience has shown that generic duplicate record detection techniques are not very useful when applied to invoices: the rate of false positives can be so high that invoice clerks are discouraged from using the system. This is because such approaches do not take the business context into account, e.g. what types of good were ordered as well as the past relationship with that vendor. In this paper, we discuss applying Ripple Down Rules (RDR), an approach for incremental and end-user-centred knowledge acquisition, to the problem of classifying pairs of potential duplicate invoices. We describe how we built a prototype on top of the SAP ERP product and evaluated it on a real data set that had been previously independently audited for duplicates. The preliminary results have highlighted the significant potential of this approach for assisting invoicing clerks processing potential duplicate invoices. We have observed a drop in the rate of false positives from 92% down to 18.66% when compared to traditional approaches that do not take the business context into account. We suggest that incremental development of domain specific knowledge may have more general application to the problem of handling duplicate records.

#index 1593770
#* Proceedings of the 2011 IEEE 27th International Conference on Data Engineering
#@ 
#t 2011
#c 17

#index 1594556
#* How schema independent are schema free query interfaces?
#@ Arash Termehchy;Marianne Winslett;Yodsawalai Chodpathumwan
#t 2011
#c 17
#! Real-world databases often have extremely complex schemas. With thousands of entity types and relationships, each with a hundred or so attributes, it is extremely difficult for new users to explore the data and formulate queries. Schema free query interfaces (SFQIs) address this problem by allowing users with no knowledge of the schema to submit queries. We postulate that SFQIs should deliver the same answers when given alternative but equivalent schemas for the same underlying information. In this paper, we introduce and formally define design independence, which captures this property for SFQIs. We establish a theoretical framework to measure the amount of design independence provided by an SFQI. We show that most current SFQIs provide a very limited degree of design independence. We also show that SFQIs based on the statistical properties of data can provide design independence when the changes in the schema do not introduce or remove redundancy in the data. We propose a novel XML SFQI called Duplication Aware Coherency Ranking (DA-CR) based on information-theoretic relationships among the data items in the database, and prove that DA-CR is design independent. Our extensive empirical study using three real-world data sets shows that the average case design independence of current SFQIs is considerably lower than that of DA-CR. We also show that the ranking quality of DA-CR is better than or equal to that of current SFQI methods.

#index 1594557
#* XClean: Providing valid spelling suggestions for XML keyword queries
#@ Yifei Lu;Wei Wang;Jianxin Li;Chengfei Liu
#t 2011
#c 17
#! An important facility to aid keyword search on XML data is suggesting alternative queries when user queries contain typographical errors. Query suggestion thus can improve users' search experience by avoiding returning empty result or results of poor qualities. In this paper, we study the problem of effectively and efficiently providing quality query suggestions for keyword queries on an XML document. We illustrate certain biases in previous work and propose a principled and general framework, XClean, based on the state-of-the-art language model. Compared with previous methods, XClean can accommodate different error models and XML keyword query semantics without losing rigor. Algorithms have been developed that compute the top-k suggestions efficiently. We performed an extensive experiment study using two large-scale real datasets. The experiment results demonstrate the effectiveness and efficiency of the proposed methods.

#index 1594558
#* Top-k keyword search over probabilistic XML data
#@ Jianxin Li;Chengfei Liu;Rui Zhou;Wei Wang
#t 2011
#c 17
#! Despite the proliferation of work on XML keyword query, it remains open to support keyword query over probabilistic XML data. Compared with traditional keyword search, it is far more expensive to answer a keyword query over probabilistic XML data due to the consideration of possible world semantics. In this paper, we firstly define the new problem of studying top-k keyword search over probabilistic XML data, which is to retrieve k SLCA results with the k highest probabilities of existence. And then we propose two efficient algorithms. The first algorithm PrStack can find k SLCA results with the k highest probabilities by scanning the relevant keyword nodes only once. To further improve the efficiency, we propose a second algorithm EagerTopK based on a set of pruning properties which can quickly prune unsatisfied SLCA candidates. Finally, we implement the two algorithms and compare their performance with analysis of extensive experimental results.

#index 1594559
#* Selectivity estimation for extraction operators over text data
#@ Daisy Zhe Wang;Long Wei;Yunyao Li;Frederick Reiss;Shivakumar Vaithyanathan
#t 2011
#c 17
#! Recently, there has been increasing interest in extending relational query processing to efficiently support extraction operators, such as dictionaries and regular expressions, over text data. Many text processing queries are sophisticated in that they involve multiple extraction and join operators, resulting in many possible query plans. However, there has been little research on building the selectivity or cost estimation for these extraction operators, which is crucial for an optimizer to pick a good query plan. In this paper, we define the problem of selectivity estimation for dictionaries and regular expressions, and propose to develop document synopses over a text corpus, from which the selectivity can be estimated. We first adapt the language models in the Natural Language Processing literature to form the top-k n-gram synopsis as the baseline document synopsis. Then we develop two classes of novel document synopses: stratified bloom filter synopsis and roll-up synopsis. We also develop techniques to decompose a complicated regular expression into subparts to achieve more effective and accurate estimation. We conduct experiments over the Enron email corpus using both real-world and synthetic workloads to compare the accuracy of the selectivity estimation over different classes and variations of synopses. The results show that, the top-k stratified bloom filter synopsis and the roll-up synopsis is the most accurate in dictionary and regular expression selectivity estimation respectively.

#index 1594560
#* Modern B-tree techniques
#@ Goetz Graefe;Harumi Kuno
#t 2011
#c 17
#! In summary, the core design of B-trees has remained unchanged in 40 years: balanced trees, pages or other units of I/O as nodes, efficient root-to-leaf search, splitting and merging nodes, etc. On the other hand, an enormous amount of research and development has improved every aspect of B-trees including data contents such as multi-dimensional data, access algorithms such as multi-dimensional queries, data organization within each node such as compression and cache optimization, concurrency control such as separation of latching and locking, recovery such as multi-level recovery, etc. Gray and Reuter believed in 1993 that "B-trees are by far the most important access path structure in database and file systems." It seems that this statement remains true today. B-tree indexes are likely to gain new importance in relational databases due to the advent of flash storage. Fast access latencies permit many more random I/O operations than traditional disk storage, thus shifting the break-even point between a full-bandwidth scan and a B-tree index search, even if the scan has the benefit of columnar database storage. We hope that this tutorial of B-tree techniques will stimulate research and development of modern B-tree indexing techniques for future data management systems.

#index 1594561
#* Query optimizer plan diagrams: Production, reduction and applications
#@ Jayant R. Haritsa
#t 2011
#c 17
#! The automated optimization of declarative SQL queries is a classical problem that has been diligently addressed by the database community over several decades. However, due to its inherent complexities and challenges, the topic has largely remained a "black art", and the quality of the query optimizer continues to be a key differentiator between competing database products, with large technical teams involved in their design and implementation.

#index 1594562
#* Schemas for safe and efficient XML processing
#@ Dario Colazzo;Giorgio Ghelli;Carlo Sartiani
#t 2011
#c 17
#! Schemas have always played a crucial role in database management. For traditional relational and object databases, schemas have a relatively simple structure, and this eases their use for optimizing and typechecking queries.

#index 1594563
#* Keyword-based search and exploration on databases
#@ Yi Chen;Wei Wang;Ziyang Liu
#t 2011
#c 17
#! Empowering users to access databases using simple keywords can relieve users from the steep learning curve of mastering a structured query language and understanding complex and possibly fast-evolving data schemas. In this tutorial, we give an overview of the state-of-the-art techniques for supporting keyword-based search and exploration on databases. Several topics will be discussed, including query result definition, ranking functions, result generation and top-k query processing, snippet generation, result clustering, result comparison, query cleaning and suggestion, performance optimization, and search quality evaluation. Various data models will be discussed, including relational data, XML data, graph-structured data, data streams, and workflows. Finally we identify the challenges and opportunities for future research to advance the field.

#index 1594564
#* Join queries on uncertain data: Semantics and efficient processing
#@ Tingjian Ge
#t 2011
#c 17
#! Uncertain data is quite common nowadays in a variety of modern database applications. At the same time, the join operation is one of the most important but expensive operations in SQL. However, join queries on uncertain data have not been adequately addressed thus far. In this paper, we study the SQL join operation on uncertain attributes. We observe and formalize two kinds of join operations on such data, namely v-join and d-join. They are each useful for different applications. Using probability theory, we then devise efficient query processing algorithms for these join operations. Specifically, we use probability bounds that are based on the moments of random variables to either early accept or early reject a candidate v-join result tuple. We also devise an indexing mechanism and an algorithm called Two-End Zigzag Join to further save I/O costs. For d-join, we first observe that it can be reduced to a special form of similarity join in a multidimensional space. We then design an efficient algorithm called condensed d-join and an optimal condensation scheme based on dynamic programming. Finally, we perform a comprehensive empirical study using both real datasets and synthetic datasets.

#index 1594565
#* Interval-based pruning for top-k processing over compressed lists
#@ Kaushik Chakrabarti;Surajit Chaudhuri;Venkatesh Ganti
#t 2011
#c 17
#! Optimizing execution of top-k queries over record-id ordered, compressed lists is challenging. The threshold family of algorithms cannot be effectively used in such cases. Yet, improving execution of such queries is of great value. For example, top-k keyword search in information retrieval (IR) engines represents an important scenario where such optimization can be directly beneficial. In this paper, we develop novel algorithms to improve execution of such queries over state of the art techniques. Our main insights are pruning based on fine-granularity bounds and traversing the lists based on judiciously chosen "intervals" rather than individual records. We formally study the optimality characteristics of the proposed algorithms. Our algorithms require minimal changes and can be easily integrated into IR engines. Our experiments on real-life datasets show that our algorithm outperform the state of the art techniques by a factor of 3 -- 6 in terms of query execution times.

#index 1594566
#* Stochastic skyline operator
#@ Xuemin Lin;Ying Zhang;Wenjie Zhang;Muhammad Aamir Cheema
#t 2011
#c 17
#! In many applications involving the multiple criteria optimal decision making, users may often want to make a personal trade-off among all optimal solutions. As a key feature, the skyline in a multi-dimensional space provides the minimum set of candidates for such purposes by removing all points not preferred by any (monotonic) utility/scoring functions; that is, the skyline removes all objects not preferred by any user no mater how their preferences vary. Driven by many applications with uncertain data, the probabilistic skyline model is proposed to retrieve uncertain objects based on skyline probabilities. Nevertheless, skyline probabilities cannot capture the preferences of monotonic utility functions. Motivated by this, in this paper we propose a novel skyline operator, namely stochastic skyline. In the light of the expected utility principle, stochastic skyline guarantees to provide the minimum set of candidates for the optimal solutions over all possible monotonic multiplicative utility functions. In contrast to the conventional skyline or the probabilistic skyline computation, we show that the problem of stochastic skyline is NP-complete with respect to the dimensionality. Novel and efficient algorithms are developed to efficiently compute stochastic skyline over multi-dimensional uncertain data, which run in polynomial time if the dimensionality is fixed. We also show, by theoretical analysis and experiments, that the size of stochastic skyline is quite similar to that of conventional skyline over certain data. Comprehensive experiments demonstrate that our techniques are efficient and scalable regarding both CPU and IO costs.

#index 1594567
#* Discovery of complex glitch patterns: A novel approach to Quantitative Data Cleaning
#@ Laure Berti-Equille;Tamraparni Dasu;Divesh Srivastava
#t 2011
#c 17
#! Quantitative Data Cleaning (QDC) is the use of statistical and other analytical techniques to detect, quantify, and correct data quality problems (or glitches). Current QDC approaches focus on addressing each category of data glitch individually. However, in real-world data, different types of data glitches co-occur in complex patterns. These patterns and interactions between glitches offer valuable clues for developing effective domain-specific quantitative cleaning strategies. In this paper, we address the shortcomings of the extant QDC methods by proposing a novel framework, the DEC (Detect-Explore-Clean) framework. It is a comprehensive approach for the definition, detection and cleaning of complex, multi-type data glitches. We exploit the distributions and interactions of different types of glitches to develop data-driven cleaning strategies that may offer significant advantages over blind strategies. The DEC framework is a statistically rigorous methodology for evaluating and scoring glitches and selecting the quantitative cleaning strategies that result in cleaned data sets that are statistically proximal to user specifications. We demonstrate the efficacy and scalability of the DEC framework on very large real-world and synthetic data sets.

#index 1594568
#* Towards exploratory hypothesis testing and analysis
#@ Guimei Liu;Mengling Feng;Yue Wang;Limsoon Wong;See-Kiong Ng;Tzia Liang Mah;Edmund Jon Deoon Lee
#t 2011
#c 17
#! Hypothesis testing is a well-established tool for scientific discovery. Conventional hypothesis testing is carried out in a hypothesis-driven manner. A scientist must first formulate a hypothesis based on his/her knowledge and experience, and then devise a variety of experiments to test it. Given the rapid growth of data, it has become virtually impossible for a person to manually inspect all the data to find all the interesting hypotheses for testing. In this paper, we propose and develop a data-driven system for automatic hypothesis testing and analysis. We define a hypothesis as a comparison between two or more sub-populations. We find sub-populations for comparison using frequent pattern mining techniques and then pair them up for statistical testing. We also generate additional information for further analysis of the hypotheses that are deemed significant. We conducted a set of experiments to show the efficiency of the proposed algorithms, and the usefulness of the generated hypotheses. The results show that our system can help users (1) identify significant hypotheses; (2) isolate the reasons behind significant hypotheses; and (3) find confounding factors that form Simpson's Paradoxes with discovered significant hypotheses.

#index 1594569
#* SMM: A data stream management system for knowledge discovery
#@ Hetal Thakkar;Nikolay Laptev;Hamid Mousavi;Barzan Mozafari;Vincenzo Russo;Carlo Zaniolo
#t 2011
#c 17
#! The problem of supporting data mining applications proved to be difficult for database management systems and it is now proving to be very challenging for data stream management systems (DSMSs), where the limitations of SQL are made even more severe by the requirements of continuous queries. The major technical advances that achieved separately on DSMSs and on data stream mining algorithms have failed to converge and produce powerful data stream mining systems. Such systems, however, are essential since the traditional pull-based approach of cache mining is no longer applicable, and the push-based computing mode of data streams and their bursty traffic complicate application development. For instance, to write mining applications with quality of service (QoS) levels approaching those of DSMSs, a mining analyst would have to contend with many arduous tasks, such as support for data buffering, complex storage and retrieval methods, scheduling, fault-tolerance, synopsis-management, load shedding, and query optimization. Our Stream Mill Miner (SMM) system solves these problems by providing a data stream mining workbench that combines the ease of specifying high-level mining tasks, as in Weka, with the performance and QoS guarantees of a DSMS. This is accomplished in three main steps. The first is an open and extensible DSMS architecture where KDD queries can be easily expressed as user-defined aggregates (UDAs) -- our system combines that with the efficiency of synoptic data structures and mining-aware load shedding and optimizations. The second key component of SMM is its integrated library of fast mining algorithms that are light enough to be effective on data streams. The third advanced feature of SMM is a Mining Model Definition Language (MMDL) that allows users to define the flow of mining tasks, integrated with a simple box&arrow GUI, to shield the mining analyst from the complexities of lower-level queries. SMM is the first DSMS capable of online mining and this paper describes its architecture, design, and performance on mining queries.

#index 1594570
#* Knowledge transfer with low-quality data: A feature extraction issue
#@ Brian Quanz;Jun Huan;Meenakshi Mishra
#t 2011
#c 17
#! Effectively utilizing readily available auxiliary data to improve predictive performance on new modeling tasks is a key problem in data mining. In this research the goal is to transfer knowledge between sources of data, particularly when ground truth information for the new modeling task is scarce or is expensive to collect where leveraging any auxiliary sources of data becomes a necessity. Towards seamless knowledge transfer among tasks, effective representation of the data is a critical but yet not fully explored research area for the data engineer and data miner. Here we present a technique based on the idea of sparse coding, which essentially attempts to find an embedding for the data by assigning feature values based on subspace cluster membership. We modify the idea of sparse coding by focusing the identification of shared clusters between data when source and target data may have different distributions. In our paper, we point out cases where a direct application of sparse coding will lead to a failure of knowledge transfer. We then present the details of our extension to sparse coding, by incorporating distribution distance estimates for the embedded data, and show that the proposed algorithm can overcome the shortcomings of the sparse coding algorithm on synthetic data and achieve improved predictive performance on a real world chemical toxicity transfer learning task.

#index 1594571
#* EdiFlow: Data-intensive interactive workflows for visual analytics
#@ Veronique Benzaken;Jean-Daniel Fekete;Pierre-Luc Hemery;Wael Khemiri;Ioana Manolescu
#t 2011
#c 17
#! Visual analytics aims at combining interactive data visualization with data analysis tasks. Given the explosion in volume and complexity of scientific data, e.g., associated to biological or physical processes or social networks, visual analytics is called to play an important role in scientific data management.

#index 1594572
#* A continuous query system for dynamic route planning
#@ Nirmesh Malviya;Samuel Madden;Arnab Bhattacharya
#t 2011
#c 17
#! In this paper, we address the problem of answering continuous route planning queries over a road network, in the presence of updates to the delay (cost) estimates of links. A simple approach to this problem would be to recompute the best path for all queries on arrival of every delay update. However, such a naive approach scales poorly when there are many users who have requested routes in the system. Instead, we propose two new classes of approximate techniques - K-paths and proximity measures to substantially speed up processing of the set of designated routes specified by continuous route planning queries in the face of incoming traffic delay updates. Our techniques work through a combination of pre-computation of likely good paths and by avoiding complete recalculations on every delay update, instead only sending the user new routes when delays change significantly. Based on an experimental evaluation with 7,000 drives from real taxi cabs, we found that the routes delivered by our techniques are within 5% of the best shortest path and have run times an order of magnitude or less compared to a naive approach.

#index 1594573
#* Optimal location queries in road network databases
#@ Xiaokui Xiao;Bin Yao;Feifei Li
#t 2011
#c 17
#! Optimal location (OL) queries are a type of spatial queries particularly useful for the strategic planning of resources. Given a set of existing facilities and a set of clients, an OL query asks for a location to build a new facility that optimizes a certain cost metric (defined based on the distances between the clients and the facilities). Several techniques have been proposed to address OL queries, assuming that all clients and facilities reside in an Lp space. In practice, however, movements between spatial locations are usually confined by the underlying road network, and hence, the actual distance between two locations can differ significantly from their Lp distance. Motivated by the deficiency of the existing techniques, this paper presents the first study on OL queries in road networks. We propose a unified framework that addresses three variants of OL queries that find important applications in practice, and we instantiate the framework with several novel query processing algorithms. We demonstrate the efficiency of our solutions through extensive experiments with real data.

#index 1594574
#* Message from the ICDE 2011 program chairs and general chairs
#@ 
#t 2011
#c 17

#index 1594575
#* Embarrassingly scalable database systems
#@ Anastasia Ailamaki
#t 2011
#c 17
#! Database systems have long optimized for parallel execution; the research community has pursued parallel database machines since the early '80s, and several key ideas from that era underlie the design and success of commercial database engines today. Computer microarchitecture, however, has shifted drastically during the intervening decades. Until the end of the 20th century Moore's Law translated into single-processor performance gains; today's constraints in semiconductor technology cause transistor integration to increase the number of processors per chip roughly every two years. Chip multiprocessor, or multicore, platforms are commodity; harvesting scalable performance from the available raw parallelism, however, is increasingly challenging for conventional database servers running business intelligence and transaction processing workloads.

#index 1594576
#* Ontological queries: Rewriting and optimization
#@ Georg Gottlob;Giorgio Orsi;Andreas Pieris
#t 2011
#c 17
#! Ontological queries are evaluated against an enterprise ontology rather than directly on a database. The evaluation and optimization of such queries is an intriguing new problem for database research. In this paper we discuss two important aspects of this problem: query rewriting and query optimization. Query rewriting consists of the compilation of an ontological query into an equivalent query against the underlying relational database. The focus here is on soundness and completeness. We review previous results and present a new rewriting algorithm for rather general types of ontological constraints (description logics). In particular, we show how a conjunctive query (CQ) against an enterprise ontology can be compiled into a union of conjunctive queries (UCQ) against the underlying database. Ontological query optimization, in this context, attempts to improve this process so to produce possibly small and cost-effective output UCQ. We review existing optimization methods, and propose an effective new method that works for Linear Datalog卤, a description logic that encompasses well-known description logics of the DL-Lite family.

#index 1594577
#* Playing games with databases
#@ Johannes Gehrke
#t 2011
#c 17
#! Scalability is a fundamental problem in the development of computer games and massively multiplayer online games (MMOs). Players always demand more--more polygons, more physics particles, more interesting AI behavior, more monsters, more simultaneous players and interactions, and larger virtual worlds.

#index 1594578
#* Interactive itinerary planning
#@ Senjuti Basu Roy;Gautam Das;Sihem Amer-Yahia;Cong Yu
#t 2011
#c 17
#! Planning an itinerary when traveling to a city involves substantial effort in choosing Points-of-Interest (POIs), deciding in which order to visit them, and accounting for the time it takes to visit each POI and transit between them. Several online services address different aspects of itinerary planning but none of them provides an interactive interface where users give feedbacks and iteratively construct their itineraries based on personal interests and time budget. In this paper, we formalize interactive itinerary planning as an iterative process where, at each step: (1) the user provides feedback on POIs selected by the system, (2) the system recommends the best itineraries based on all feedback so far, and (3) the system further selects a new set of POIs, with optimal utility, to solicit feedback for, at the next step. This iterative process stops when the user is satisfied with the recommended itinerary. We show that computing an itinerary is NP-complete even for simple itinerary scoring functions, and that POI selection is NP-complete. We develop heuristics and optimizations for a specific case where the score of an itinerary is proportional to the number of desired POIs it contains. Our extensive experiments show that our algorithms are efficient and return high quality itineraries.

#index 1594579
#* Spatio-temporal joins on symbolic indoor tracking data
#@ Hua Lu;Bin Yang;Christian S. Jensen
#t 2011
#c 17
#! To facilitate a variety of applications, positioning systems are deployed in indoor settings. For example, Bluetooth and RFID positioning are deployed in airports to support real-time monitoring of delays as well as off-line flow and space usage analyses. Such deployments generate large collections of tracking data. Like in other data management applications, joins are indispensable in this setting. However, joins on indoor tracking data call for novel techniques that take into account the limited capabilities of the positioning systems as well as the specifics of indoor spaces. This paper proposes and studies probabilistic, spatio-temporal joins on historical indoor tracking data. Two meaningful types of join are defined. They return object pairs that satisfy spatial join predicates either at a time point or during a time interval. The predicates considered include "same X," where X is a semantic region such as a room or hallway. Based on an analysis on the uncertainty inherent to indoor tracking data, effective join probabilities are formalized and evaluated for object pairs. Efficient two-phase hash-based algorithms are proposed for the point and interval joins. In a filter-and-refine framework, an R-tree variant is proposed that facilitates the retrieval of join candidates, and pruning rules are supplied that eliminate candidate pairs that do not qualify. An empirical study on both synthetic and real data shows that the proposed techniques are efficient and scalable.

#index 1594580
#* MaxFirst for MaxBRkNN
#@ Zenan Zhou;Wei Wu;Xiaohui Li;Mong Li Lee;Wynne Hsu
#t 2011
#c 17
#! The MaxBRNN problem finds a region such that setting up a new service site within this region would guarantee the maximum number of customers by proximity. This problem assumes that each customer only uses the service provided by his/her nearest service site. However, in reality, a customer tends to go to his/her k nearest service sites. To handle this, MaxBRNN can be extended to the MaxBRkNN problem which finds an optimal region such that setting up a service site in this region guarantees the maximum number of customers who would consider the site as one of their k nearest service locations. We further generalize the MaxBRkNN problem to reflect the real world scenario where customers may have different preferences for different service sites, and at the same time, service sites may have preferred targeted customers. In this paper, we present an efficient solution called MaxFirst to solve this generalized MaxBRkNN problem. The algorithm works by partitioning the space into quadrants and searches only in those quadrants that potentially contain an optimal region. During the space partitioning, we compute the upper and lower bounds of the size of a quadrant's BRkNN, and use these bounds to prune the unpromising quadrants. Experiment results show that MaxFirst can be two to three orders of magnitude faster than the state-of-the-art algorithm.

#index 1594581
#* SQPR: Stream query planning with reuse
#@ Evangelia Kalyvianaki;Wolfram Wiesemann;Quang Hieu Vu;Daniel Kuhn;Peter Pietzuch
#t 2011
#c 17
#! When users submit new queries to a distributed stream processing system (DSPS), a query planner must allocate physical resources, such as CPU cores, memory and network bandwidth, from a set of hosts to queries. Allocation decisions must provide the correct mix of resources required by queries, while achieving an efficient overall allocation to scale in the number of admitted queries. By exploiting overlap between queries and reusing partial results, a query planner can conserve resources but has to carry out more complex planning decisions. In this paper, we describe SQPR, a query planner that targets DSPSs in data centre environments with heterogeneous resources. SQPR models query admission, allocation and reuse as a single constrained optimisation problem and solves an approximate version to achieve scalability. It prevents individual resources from becoming bottlenecks by re-planning past allocation decisions and supports different allocation objectives. As our experimental evaluation in comparison with a state-of-the-art planner shows SQPR makes efficient resource allocation decisions, even with a high utilisation of resources, with acceptable overheads.

#index 1594582
#* Memory-constrained aggregate computation over data streams
#@ K. V. M. Naidu;Rajeev Rastogi;Scott Satkin;Anand Srinivasan
#t 2011
#c 17
#! In this paper, we study the problem of efficiently computing multiple aggregation queries over a data stream. In order to share computation, prior proposals have suggested instantiating certain intermediate aggregates which are then used to generate the final answers for input queries. In this work, we make a number of important contributions aimed at improving the execution and generation of query plans containing intermediate aggregates. These include: (1) a different hashing model, which has low eviction rates, and also allows us to accurately estimate the number of evictions, (2) a comprehensive query execution cost model based on these estimates, (3) an efficient greedy heuristic for constructing good low-cost query plans, (4) provably near-optimal and optimal algorithms for allocating the available memory to aggregates in the query plan when the input data distribution is Zipf-like and Uniform, respectively, and (5) a detailed performance study with real-life IP flow data sets, which show that our multiple aggregates computation techniques consistently outperform the best-known approach.

#index 1594583
#* A new, highly efficient, and easy to implement top-down join enumeration algorithm
#@ Pit Fender;Guido Moerkotte
#t 2011
#c 17
#! Finding an optimal execution order of join operations is a crucial task in every cost-based query optimizer. Since there are many possible join trees for a given query, the overhead of the join (tree) enumeration algorithm per valid join tree should be minimal. In the case of a clique-shaped query graph, the best known top-down algorithm has a complexity of Θ(n2) per join tree, where n is the number of relations. In this paper, we present an algorithm that has an according O(1) complexity in this case. We show experimentally that this more theoretical result has indeed a high impact on the performance in other non-clique settings. This is especially true for cyclic query graphs. Further, we evaluate the performance of our new algorithm and compare it with the best top-down and bottom-up algorithms described in the literature.

#index 1594584
#* CubeLSI: An effective and efficient method for searching resources in social tagging systems
#@ Bin Bi;Sau Dan Lee;Ben Kao;Reynold Cheng
#t 2011
#c 17
#! In a social tagging system, resources (such as photos, video and web pages) are associated with tags. These tags allow the resources to be effectively searched through tag-based keyword matching using traditional IR techniques. We note that in many such systems, tags of a resource are often assigned by a diverse audience of causal users (taggers). This leads to two issues that gravely affect the effectiveness of resource retrieval: (1) Noise: tags are picked from an uncontrolled vocabulary and are assigned by untrained taggers. The tags are thus noisy features in resource retrieval. (2) A multitude of aspects: different taggers focus on different aspects of a resource. Representing a resource using a flattened bag of tags ignores this important diversity of taggers. To improve the effectiveness of resource retrieval in social tagging systems, we propose CubeLSI--a technique that extends traditional LSI to include taggers as another dimension of feature space of resources. We compare CubeLSI against a number of other tag-based retrieval models and show that CubeLSI significantly outperforms the other models in terms of retrieval accuracy. We also prove two interesting theorems that allow CubeLSI to be very efficiently computed despite the much enlarged feature space it employs.

#index 1594585
#* Adding regular expressions to graph reachability and pattern queries
#@ Wenfei Fan;Jianzhong Li;Shuai Ma;Nan Tang;Yinghui Wu
#t 2011
#c 17
#! It is increasingly common to find graphs in which edges bear different types, indicating a variety of relationships. For such graphs we propose a class of reachability queries and a class of graph patterns, in which an edge is specified with a regular expression of a certain form, expressing the connectivity in a data graph via edges of various types. In addition, we define graph pattern matching based on a revised notion of graph simulation. On graphs in emerging applications such as social networks, we show that these queries are capable of finding more sensible information than their traditional counterparts. Better still, their increased expressive power does not come with extra complexity. Indeed, (1) we investigate their containment and minimization problems, and show that these fundamental problems are in quadratic time for reachability queries and are in cubic time for pattern queries. (2) We develop an algorithm for answering reachability queries, in quadratic time as for their traditional counterpart. (3) We provide two cubic-time algorithms for evaluating graph pattern queries based on extended graph simulation, as opposed to the NP-completeness of graph pattern matching via subgraph isomorphism. (4) The effectiveness, efficiency and scalability of these algorithms are experimentally verified using real-life data and synthetic data.

#index 1594586
#* Efficient core decomposition in massive networks
#@ James Cheng;Yiping Ke;Shumo Chu;M. Tamer Ozsu
#t 2011
#c 17
#! The k-core of a graph is the largest subgraph in which every vertex is connected to at least k other vertices within the subgraph. Core decomposition finds the k-core of the graph for every possible k. Past studies have shown important applications of core decomposition such as in the study of the properties of large networks (e.g., sustainability, connectivity, centrality, etc.), for solving NP-hard problems efficiently in real networks (e.g., maximum clique finding, densest subgraph approximation, etc.), and for large-scale network fingerprinting and visualization. The k-core is a well accepted concept partly because there exists a simple and efficient algorithm for core decomposition, by recursively removing the lowest degree vertices and their incident edges. However, this algorithm requires random access to the graph and hence assumes the entire graph can be kept in main memory. Nevertheless, real-world networks such as online social networks have become exceedingly large in recent years and still keep growing at a steady rate. In this paper, we propose the first external-memory algorithm for core decomposition in massive graphs. When the memory is large enough to hold the graph, our algorithm achieves comparable performance as the in-memory algorithm. When the graph is too large to be kept in the memory, our algorithm requires only O(kmax) scans of the graph, where kmax is the largest core number of the graph. We demonstrate the efficiency of our algorithm on real networks with up to 52.9 million vertices and 1.65 billion edges.

#index 1594587
#* T-verifier: Verifying truthfulness of fact statements
#@ Xian Li;Weiyi Meng;Clement Yu
#t 2011
#c 17
#! The Web has become the most popular place for people to acquire information. Unfortunately, it is widely recognized that the Web contains a significant amount of untruthful information. As a result, good tools are needed to help Web users determine the truthfulness of certain information. In this paper, we propose a two-step method that aims to determine whether a given statement is truthful, and if it is not, find out the truthful statement most related to the given statement. In the first step, we try to find a small number of alternative statements of the same topic as the given statement and make sure that one of these statements is truthful. In the second step, we identify the truthful statement from the given statement and the alternative statements. Both steps heavily rely on analysing various features extracted from the search results returned by a popular search engine for appropriate queries. Our experimental results show the best variation of the proposed method can achieve a precision of about 90%.

#index 1594588
#* Distributed cube materialization on holistic measures
#@ Arnab Nandi;Cong Yu;Philip Bohannon;Raghu Ramakrishnan
#t 2011
#c 17
#! Cube computation over massive datasets is critical for many important analyses done in the real world. Unlike commonly studied algebraic measures such as SUM that are amenable to parallel computation, efficient cube computation of holistic measures such as TOP-K is non-trivial and often impossible with current methods. In this paper we detail real-world challenges in cube materialization tasks on Web-scale datasets. Specifically, we identify an important subset of holistic measures and introduce MR-Cube, a MapReduce based framework for efficient cube computation on these measures. We provide extensive experimental analyses over both real and synthetic data. We demonstrate that, unlike existing techniques which cannot scale to the 100 million tuple mark for our datasets, MR-Cube successfully and efficiently computes cubes with holistic measures over billion-tuple datasets.

#index 1594589
#* Transactional In-Page Logging for multiversion read consistency and recovery
#@ Sang-Won Lee;Bongki Moon
#t 2011
#c 17
#! Recently, a new buffer and storage management strategy called In-Page Logging (IPL) has been proposed for database systems based on flash memory. Its main objective is to overcome the limitations of flash memory such as erase-before-write and asymmetric read/write speeds by storing changes made to a data page in a form of log records without overwriting the data page itself. Since it maintains a series of changes made to a data page separately from the original data page until they are merged, the IPL scheme provides unique opportunities to design light-weight transactional support for database systems. In this paper, we propose the transactional IPL (TIPL) scheme that takes advantage of the IPL log records to support multiversion read consistency and light-weight database recovery. Due to the dual use of IPL log records, namely, for snapshot isolation and fast recovery as well as flash-aware write optimization, TIPL achieves transactional support for flash memory database systems that minimizes the space and time overhead during normal database processing and shortens the database recovery time.

#index 1594590
#* Answering approximate string queries on large data sets using external memory
#@ Alexander Behm;Chen Li;Michael J. Carey
#t 2011
#c 17
#! An approximate string query is to find from a collection of strings those that are similar to a given query string. Answering such queries is important in many applications such as data cleaning and record linkage, where errors could occur in queries as well as the data. Many existing algorithms have focused on in-memory indexes. In this paper we investigate how to efficiently answer such queries in a disk-based setting, by systematically studying the effects of storing data and indexes on disk. We devise a novel physical layout for an inverted index to answer queries and we study how to construct it with limited buffer space. To answer queries, we develop a cost-based, adaptive algorithm that balances the I/O costs of retrieving candidate matches and accessing inverted lists. Experiments on large, real datasets verify that simply adapting existing algorithms to a disk-based setting does not work well and that our new techniques answer queries efficiently. Further, our solutions significantly outperform a recent tree-based index, BED-tree.

#index 1594591
#* Discovering popular routes from trajectories
#@ Zaiben Chen;Heng Tao Shen;Xiaofang Zhou
#t 2011
#c 17
#! The booming industry of location-based services has accumulated a huge collection of users' location trajectories of driving, cycling, hiking, etc. In this work, we investigate the problem of discovering the Most Popular Route (MPR) between two locations by observing the traveling behaviors of many previous users. This new query is beneficial to travelers who are asking directions or planning a trip in an unfamiliar city/area, as historical traveling experiences can reveal how people usually choose routes between locations. To achieve this goal, we firstly develop a Coherence Expanding algorithm to retrieve a transfer network from raw trajectories, for indicating all the possible movements between locations. After that, the Absorbing Markov Chain model is applied to derive a reasonable transfer probability for each transfer node in the network, which is subsequently used as the popularity indicator in the search phase. Finally, we propose a Maximum Probability Product algorithm to discover the MPR from a transfer network based on the popularity indicators in a breadth-first manner, and we illustrate the results and performance of the algorithm by extensive experiments.

#index 1594592
#* Spectrum based fraud detection in social networks
#@ Xiaowei Ying;Xintao Wu;Daniel Barbara
#t 2011
#c 17
#! Social networks are vulnerable to various attacks such as spam emails, viral marketing and the such. In this paper we develop a spectrum based detection framework to discover the perpetrators of these attacks. In particular, we focus on Random Link Attacks (RLAs) in which the malicious user creates multiple false identities and interactions among those identities to later proceed to attack the regular members of the network. We show that RLA attackers can be filtered by using their spectral coordinate characteristics, which are hard to hide even after the efforts by the attackers of resembling as much as possible the rest of the network. Experimental results show that our technique is very effective in detecting those attackers and outperforms techniques previously published.

#index 1594593
#* Identity obfuscation in graphs through the information theoretic lens
#@ Francesco Bonchi;Aristides Gionis;Tamir Tassa
#t 2011
#c 17
#! Analyzing the structure of social networks is of interest in a wide range of disciplines, but such activity is limited by the fact that these data represent sensitive information and can not be published in their raw form. One of the approaches to sanitize network data is to randomly add or remove edges from the graph. Recent studies have quantified the level of anonymity that is obtained by random perturbation by means of a-posteriori belief probabilities and, by conducting experiments on small datasets, arrived at the conclusion that random perturbation can not achieve meaningful levels of anonymity without deteriorating the graph features.

#index 1594594
#* Monte Carlo query processing of uncertain multidimensional array data
#@ Tingjian Ge;David Grabiner;Stan Zdonik
#t 2011
#c 17
#! Array database systems are architected for scientific and engineering applications. In these applications, the value of a cell is often imprecise and uncertain. There are at least two reasons that a Monte Carlo query processing algorithm is usually required for such uncertain data. Firstly, a probabilistic graphical model must often be used to model correlation, which requires a Monte Carlo inference algorithm for the operations in our database. Secondly, mathematical operators required by science and engineering domains are much more complex than those of SQL. State-of-the-art query processing uses Monte Carlo approximation. We give an example of using Markov Random Fields combined with an array's chunking or tiling mechanism to model correlated data. We then propose solutions for two of the most challenging problems in this framework, namely the expensive array join operation, and the determination and optimization of stopping conditions of Monte Carlo query processing. Finally, we perform an extensive empirical study on a real world application.

#index 1594595
#* Flexible use of cloud resources through profit maximization and price discrimination
#@ Konstantinos Tsakalozos;Herald Kllapi;Eva Sitaridi;Mema Roussopoulos;Dimitris Paparas;Alex Delis
#t 2011
#c 17
#! Modern frameworks, such as Hadoop, combined with abundance of computing resources from the cloud, offer a significant opportunity to address long standing challenges in distributed processing. Infrastructure-as-a-Service clouds reduce the investment cost of renting a large data center while distributed processing frameworks are capable of efficiently harvesting the rented physical resources. Yet, the performance users get out of these resources varies greatly because the cloud hardware is shared by all users. The value for money cloud consumers achieve renders resource sharing policies a key player in both cloud performance and user satisfaction. In this paper, we employ microeconomics to direct the allotment of cloud resources for consumption in highly scalable master-worker virtual infrastructures. Our approach is developed on two premises: the cloud-consumer always has a budget and cloud physical resources are limited. Using our approach, the cloud administration is able to maximize per-user financial profit. We show that there is an equilibrium point at which our method achieves resource sharing proportional to each user's budget. Ultimately, this approach allows us to answer the question of how many resources a consumer should request from the seemingly endless pool provided by the cloud.

#index 1594596
#* Intelligent management of virtualized resources for database systems in cloud environment
#@ Pengcheng Xiong;Yun Chi;Shenghuo Zhu;Hyun Jin Moon;Calton Pu;Hakan Hacigumus
#t 2011
#c 17
#! In a cloud computing environment, resources are shared among different clients. Intelligently managing and allocating resources among various clients is important for system providers, whose business model relies on managing the infrastructure resources in a cost-effective manner while satisfying the client service level agreements (SLAs). In this paper, we address the issue of how to intelligently manage the resources in a shared cloud database system and present SmartSLA, a cost-aware resource management system. SmartSLA consists of two main components: the system modeling module and the resource allocation decision module. The system modeling module uses machine learning techniques to learn a model that describes the potential profit margins for each client under different resource allocations. Based on the learned model, the resource allocation decision module dynamically adjusts the resource allocations in order to achieve the optimum profits. We evaluate SmartSLA by using the TPC-W benchmark with workload characteristics derived from real-life systems. The performance results indicate that SmartSLA can successfully compute predictive models under different hardware resource allocations, such as CPU and memory, as well as database specific resources, such as the number of replicas in the database systems. The experimental results also show that SmartSLA can provide intelligent service differentiation according to factors such as variable workloads, SLA levels, resource costs, and deliver improved profit margins.

#index 1594597
#* Extensibility and Data Sharing in evolving multi-tenant databases
#@ Stefan Aulbach;Michael Seibold;Dean Jacobs;Alfons Kemper
#t 2011
#c 17
#! Software-as-a-Service applications commonly consolidate multiple businesses into the same database to reduce costs. This practice makes it harder to implement several essential features of enterprise applications. The first is support for master data, which should be shared rather than replicated for each tenant. The second is application modification and extension, which applies both to the database schema and master data it contains. The third is evolution of the schema and master data, which occurs as the application and its extensions are upgraded. These features cannot be easily implemented in a traditional DBMS and, to the extent that they are currently offered at all, they are generally implemented within the application layer. This approach reduces the DBMS to a 'dumb data repository' that only stores data rather than managing it. In addition, it complicates development of the application since many DBMS features have to be re-implemented. Instead, a next-generation multi-tenant DBMS should provide explicit support for Extensibility, Data Sharing and Evolution. As these three features are strongly related, they cannot be implemented independently from each other. Therefore, we propose FLEXSCHEME which captures all three aspects in one integrated model. In this paper, we focus on efficient storage mechanisms for this model and present a novel versioning mechanism, called XOR Delta, which is based on XOR encoding and is optimized for main-memory DBMSs.

#index 1594598
#* Semantic stream query optimization exploiting dynamic metadata
#@ Luping Ding;Karen Works;Elke A. Rundensteiner
#t 2011
#c 17
#! Data stream management systems (DSMS) processing long-running queries over large volumes of stream data must typically deliver time-critical responses. We propose the first semantic query optimization (SQO) approach that utilizes dynamic substream metadata at runtime to find a more efficient query plan than the one selected at compilation time. We identify four SQO techniques guaranteed to result in performance gains. Based on classic satisfiability theory we then design a lightweight query optimization algorithm that efficiently detects SQO opportunities at runtime. At the logical level, our algorithm instantiates multiple concurrent SQO plans, each processing different partially overlapping substreams. Our novel execution paradigm employs multi-modal operators to support the execution of these concurrent SQO logical plans in a single physical plan. This highly agile execution strategy reduces resource utilization while supporting lightweight adaptivity. Our extensive experimental study in the CAPE stream processing system using both synthetic and real data confirms that our optimization techniques significantly reduce query execution times, up to 60%, compared to the traditional approach.

#index 1594599
#* Massively parallel XML twig filtering using dynamic programming on FPGAs
#@ Roger Moussalli;Mariam Salloum;Walid Najjar;Vassilis J. Tsotras
#t 2011
#c 17
#! In recent years, XML-based Publish-Subscribe Systems have become popular due to the increased demand of timely event-notification. Users (or subscribers) pose complex profiles on the structure and content of the published messages. If a profile matches the message, the message is forwarded to the interested subscriber. As the amount of published content continues to grow, current software-based systems will not scale. We thus propose a novel architecture to exploit parallelism of twig matching on FPGAs. This approach yields up to three orders of magnitude higher throughput when compared to conventional approaches bound by the sequential aspect of software computing. This paper, presents a novel method for performing unordered holistic twig matching on FPGAs without any false positives, and whose throughput is independent of the complexity of the user queries or the characteristics of the input XML stream. Furthermore, we present experimental comparison of different granularities of twig matching, namely path-based (root-to-leaf) and pair-based (parent-child or ancestor-descendant).We provide comprehensive experiments that compare the throughput, area utilization and the accuracy of matching (percent of false positives) of our holistic, path-based and pair-based FPGA approaches.

#index 1594600
#* Selectivity estimation of twig queries on cyclic graphs
#@ Yun Peng;Byron Choi;Jianliang Xu
#t 2011
#c 17
#! Recent applications including the Semantic Web, Web ontology and XML have sparked a renewed interest on graph-structured databases. Among others, twig queries have been a popular tool for retrieving subgraphs from graph-structured databases. To optimize twig queries, selectivity estimation has been a crucial and classical step. However, the majority of existing works on selectivity estimation focuses on relational and tree data. In this paper, we investigate selectivity estimation of twig queries on possibly cyclic graph data. To facilitate selectivity estimation on cyclic graphs, we propose a matrix representation of graphs derived from prime labeling--a scheme for reachability queries on directed acyclic graphs. With this representation, we exploit the consecutive ones property (C1P) of matrices. As a consequence, a node is mapped to a point in a two-dimensional space whereas a query is mapped to multiple points. We adopt histograms for scalable selectivity estimation. We perform an extensive experimental evaluation on the proposed technique and show that our technique controls the estimation error under 1.3% on XMARK and DBLP, which is more accurate than previous techniques. On TREEBANK, we produce RMSE and NRMSE 6.8 times smaller than previous techniques.

#index 1594601
#* Efficient XQuery rewriting using multiple views
#@ Ioana Manolescu;Konstantinos Karanasos;Vasilis Vassalos;Spyros Zoupanos
#t 2011
#c 17
#! We consider the problem of rewriting XQuery queries using multiple materialized XQuery views. The XQuery dialect we use to express views and queries corresponds to tree patterns (returning data from several nodes, at different granularities, ranging from node identifiers to full XML subtrees) with value joins. We provide correct and complete algorithms for finding minimal rewritings, in which no view is redundant. Our work extends the state of the art by considering more flexible views than the mostly XPath 1.0 dialects previously considered, and more powerful rewritings. We implemented our algorithms and assess their performance through a set of experiments.

#index 1594602
#* Characteristic sets: Accurate cardinality estimation for RDF queries with multiple joins
#@ Thomas Neumann;Guido Moerkotte
#t 2011
#c 17
#! Accurate cardinality estimates are essential for a successful query optimization. This is not only true for relational DBMSs but also for RDF stores. An RDF database consists of a set of triples and, hence, can be seen as a relational database with a single table with three attributes. This makes RDF rather special in that queries typically contain many self joins. We show that relational DBMSs are not well-prepared to perform cardinality estimation in this context. Further, there are hardly any special cardinality estimation methods for RDF databases. To overcome this lack of appropriate cardinality estimation methods, we introduce characteristic sets together with new cardinality estimation methods based upon them. We then show experimentally that the new methods are-in the RDF context-highly superior to the estimation methods employed by commercial DBMSs and by the open-source RDF store RDF-3X.

#index 1594603
#* PrefJoin: An efficient preference-aware join operator
#@ Mohamed E. Khalefa;Mohamed F. Mokbel;Justin J. Levandoski
#t 2011
#c 17
#! Preference queries are essential to a wide spectrum of applications including multi-criteria decision-making tools and personalized databases. Unfortunately, most of the evaluation techniques for preference queries assume that the set of preferred attributes are stored in only one relation, waiving on a wide set of queries that include preference computations over multiple relations. This paper presents PrefJoin, an efficient preference-aware join query operator, designed specifically to deal with preference queries over multiple relations. PrefJoin consists of four main phases: Local Pruning, Data Preparation, Joining, and Refining that filter out, from each input relation, those tuples that are guaranteed not to be in the final preference set, associate meta data with each non-filtered tuple that will be used to optimize the execution of the next phases, produce a subset of join result that are relevant for the given preference function, and refine these tuples respectively. An interesting characteristic of PrefJoin is that it tightly integrates preference computation with join hence we can early prune those tuples that are guaranteed not to be an answer, and hence it saves significant unnecessary computations cost. PrefJoin supports a variety of preference function including skyline, multi-objective and k-dominance preference queries. We show the correctness of PrefJoin. Experimental evaluation based on a real system implementation inside PostgreSQL shows that PrefJoin consistently achieves from one to three orders of magnitude performance gain over its competitors in various scenarios.

#index 1594604
#* High-performance nested CEP query processing over event streams
#@ Mo Liu;Elke Rundensteiner;Dan Dougherty;Chetan Gupta;Song Wang;Ismail Ari;Abhay Mehta
#t 2011
#c 17
#! Complex event processing (CEP) over event streams has become increasingly important for real-time applications ranging from health care, supply chain management to business intelligence. These monitoring applications submit complex queries to track sequences of events that match a given pattern. As these systems mature the need for increasingly complex nested sequence query support arises, while the state-of-art CEP systems mostly support the execution of flat sequence queries only. To assure real-time responsiveness and scalability for pattern detection even on huge volume high-speed streams, efficient processing techniques must be designed. In this paper, we first analyze the prevailing nested pattern query processing strategy and identify several serious shortcomings. Not only are substantial subsequences first constructed just to be subsequently discarded, but also opportunities for shared execution of nested subexpressions are overlooked. As foundation, we introduce NEEL, a CEP query language for expressing nested CEP pattern queries composed of sequence, negation, AND and OR operators. To overcome deficiencies, we design rewriting rules for pushing negation into inner subexpressions. Next, we devise a normalization procedure that employs these rules for flattening a nested complex event expression. To conserve CPU and memory consumption, we propose several strategies for efficient shared processing of groups of normalized NEEL subexpressions. These strategies include prefix caching, suffix clustering and customized "bit-marking" execution strategies. We design an optimizer to partition the set of all CEP subexpressions in a NEEL normal form into groups, each of which can then be mapped to one of our shared execution operators. Lastly, we evaluate our technologies by conducting a performance study to assess the CPU processing time using real-world stock trades data. Our results confirm that our NEEL execution in many cases performs 100 fold faster than the traditional iterative nested execution strategy for real stock market query workloads.

#index 1594605
#* Continuous monitoring of distance-based outliers over data streams
#@ Maria Kontaki;Anastasios Gounaris;Apostolos N. Papadopoulos;Kostas Tsichlas;Yannis Manolopoulos
#t 2011
#c 17
#! Anomaly detection is considered an important data mining task, aiming at the discovery of elements (also known as outliers) that show significant diversion from the expected case. More specifically, given a set of objects the problem is to return the suspicious objects that deviate significantly from the typical behavior. As in the case of clustering, the application of different criteria lead to different definitions for an outlier. In this work, we focus on distance-based outliers: an object x is an outlier if there are less than k objects lying at distance at most R from x. The problem offers significant challenges when a stream-based environment is considered, where data arrive continuously and outliers must be detected on-the-fly. There are a few research works studying the problem of continuous outlier detection. However, none of these proposals meets the requirements of modern stream-based applications for the following reasons: (i) they demand a significant storage overhead, (ii) their efficiency is limited and (iii) they lack flexibility. In this work, we propose new algorithms for continuous outlier monitoring in data streams, based on sliding windows. Our techniques are able to reduce the required storage overhead, run faster than previously proposed techniques and offer significant flexibility. Experiments performed on real-life as well as synthetic data sets verify our theoretical study.

#index 1594606
#* Algorithms for local sensor synchronization
#@ Lixing Wang;Yin Yang;Xin Miao;Dimitris Papadias;Yunhao Liu
#t 2011
#c 17
#! In a wireless sensor network (WSN), each sensor monitors environmental parameters, and reports its readings to a base station, possibly through other nodes. A sensor works in cycles, in each of which it stays active for a fixed duration, and then sleeps until the next cycle. The frequency of such cycles determines the portion of time that a sensor is active, and is the dominant factor on its battery life. The majority of existing work assumes globally synchronized WSN where all sensors have the same frequency. This leads to waste of battery power for applications that entail different accuracy of measurements, or environments where sensor readings have large variability. To overcome this problem, we propose LS, a query processing framework for locally synchronized WSN. We consider that each sensor ni has a distinct sampling frequency fi, which is determined by the application or environment requirements. The complication of LS is that ni has to wake up with a network frequency Fi≥fi, in order to forward messages of other sensors. Our goal is to minimize the sum of Fi without delaying packet transmissions. Specifically, given a routing tree, we first present a dynamic programming algorithm that computes the optimal network frequency of each sensor; then, we develop a heuristic for finding the best tree topology, if this is not fixed in advance.

#index 1594607
#* Decomposing DAGs into spanning trees: A new way to compress transitive closures
#@ Yangjun Chen;Yibin Chen
#t 2011
#c 17
#! Let G(V, E) be a digraph (directed graph) with n nodes and e edges. Digraph G* = (V, E*) is the reflexive, transitive closure if (v, u) ∈ E* iff there is a path from v to u in G. Efficient storage of G* is important for supporting reachability queries which are not only common on graph databases, but also serve as fundamental operations used in many graph algorithms. A lot of strategies have been suggested based on the graph labeling, by which each node is assigned with certain labels such that the reachability of any two nodes through a path can be determined by their labels. Among them are interval labelling, chain decomposition, and 2-hop labeling. However, due to the very large size of many real world graphs, the computational cost and size of labels using existing methods would prove too expensive to be practical. In this paper, we propose a new approach to decompose a graph into a series of spanning trees which may share common edges, to transform a reachability query over a graph into a set of queries over trees. We demonstrate both analytically and empirically the efficiency and effectiveness of our method.

#index 1594608
#* Preference queries over sets
#@ Xi Zhang;Jan Chomicki
#t 2011
#c 17
#! We propose a "logic + SQL" framework for set preferences. Candidate best sets are represented using profiles consisting of scalar features. This reduces set preferences to tuple preferences over set profiles. We propose two optimization techniques: superpreference and M-relation. Superpreference targets dominated profiles. It reduces the input size by filtering out tuples not belonging to any best k-subset. M-relation targets repeated profiles. It consolidates tuples that are exchangeable with regard to the given set preference, and therefore avoids redundant computation of the same profile. We show the results of an experimental study that demonstrates the efficacy of the optimizations.

#index 1594609
#* A unified approach for computing top-k pairs in multidimensional space
#@ Muhammad Aamir Cheema;Xuemin Lin;Haixun Wang;Jianmin Wang;Wenjie Zhang
#t 2011
#c 17
#! Top-k pairs queries have many real applications. k closest pairs queries, k furthest pairs queries and their bichromatic variants are some of the examples of the top-k pairs queries that rank the pairs on distance functions. While these queries have received significant research attention, there does not exist a unified approach that can efficiently answer all these queries. Moreover, there is no existing work that supports top-k pairs queries based on generic scoring functions. In this paper, we present a unified approach that supports a broad class of top-k pairs queries including the queries mentioned above. Our proposed approach allows the users to define a local scoring function for each attribute involved in the query and a global scoring function that computes the final score of each pair by combining its scores on different attributes. We propose efficient internal and external memory algorithms and our theoretical analysis shows that the expected performance of the algorithms is optimal when two or less attributes are involved. Our approach does not require any pre-built indexes, is easy to implement and has low memory requirement. We conduct extensive experiments to demonstrate the efficiency of our proposed approach.

#index 1594610
#* Bidirectional mining of non-redundant recurrent rules from a sequence database
#@ David Lo;Bolin Ding; Lucia;Jiawei Han
#t 2011
#c 17
#! We are interested in scalable mining of a non-redundant set of significant recurrent rules from a sequence database. Recurrent rules have the form "whenever a series of precedent events occurs, eventually a series of consequent events occurs". They are intuitive and characterize behaviors in many domains. An example is the domain of software specification, in which the rules capture a family of properties beneficial to program verification and bug detection. We enhance a past work on mining recurrent rules by Lo, Khoo, and Liu to perform mining more scalably. We propose a new set of pruning properties embedded in a new mining algorithm. Performance and case studies on benchmark synthetic and real datasets show that our approach is much more efficient and outperforms the state-of-the-art approach in mining recurrent rules by up to two orders of magnitude.

#index 1594611
#* Finding top-k profitable products
#@ Qian Wan;Raymond Chi-Wing Wong;Yu Peng
#t 2011
#c 17
#! The importance of dominance and skyline analysis has been well recognized in multi-criteria decision making applications. Most previous studies focus on how to help customers find a set of "best" possible products from a pool of given products. In this paper, we identify an interesting problem, finding top-k profitable products, which has not been studied before. Given a set of products in the existing market, we want to find a set of k "best" possible products such that these new products are not dominated by the products in the existing market. In this problem, we need to set the prices of these products such that the total profit is maximized. We refer such products as top-k profitable products. A straightforward solution is to enumerate all possible subsets of size k and find the subset which gives the greatest profit. However, there are an exponential number of possible subsets. In this paper, we propose solutions to find the top-k profitable products efficiently. An extensive performance study using both synthetic and real datasets is reported to verify its effectiveness and efficiency.

#index 1594612
#* Partitioning techniques for fine-grained indexing
#@ Eugene Wu;Samuel Madden
#t 2011
#c 17
#! Many data-intensive websites use databases that grow much faster than the rate that users access the data. Such growing datasets lead to ever-increasing space and performance overheads for maintaining and accessing indexes. Furthermore, there is often considerable skew with popular users and recent data accessed much more frequently. These observations led us to design Shinobi, a system which uses horizontal partitioning as a mechanism for improving query performance to cluster the physical data, and increasing insert performance by only indexing data that is frequently accessed. We present database design algorithms that optimally partition tables, drop indexes from partitions that are infrequently queried, and maintain these partitions as workloads change. We show a 60脳 performance improvement over traditionally indexed tables using a real-world query workload derived from a traffic monitoring application

#index 1594613
#* Efficient maintenance of common keys in archives of continuous query results from deep websites
#@ Fajar Ardian;Sourav S. Bhowmick
#t 2011
#c 17
#! In many real-world applications, it is important to create a local archive containing versions of structured results of continuous queries (queries that are evaluated periodically) submitted to autonomous database-driven Web sites (e.g., deep Web). Such history of digital information is a potential gold mine for all kinds of scientific, media and business analysts. An important task in this context is to maintain the set of common keys of the underlying archived results as they play pivotal role in data modeling and analysis, query processing, and entity tracking. A set of attributes in a structured data is a common key iff it is a key for all versions of the data in the archive. Due to the data-driven nature of key discovery from the archive, unlike traditional keys, the common keys are not temporally invariant. That is, keys identified in one version may be different from those in another version. Hence, in this paper, we propose a novel technique to maintain common keys in an archive containing a sequence of versions of evolutionary continuous query results. Given the current common key set of existing versions and a new snapshot, we propose an algorithm called COKE (COmmon KEy maintenancE) which incrementally maintains the common key set without undertaking expensive minimal keys computation from the new snapshot. Furthermore, it exploits certain interesting evolutionary features of real-world data to further reduce the computation cost. Our exhaustive empirical study demonstrates that COKE has excellent performance and is orders of magnitude faster than a baseline approach for maintenance of common keys.

#index 1594614
#* Fast-join: An efficient method for fuzzy token matching based string similarity join
#@ Jiannan Wang;Guoliang Li;Jianhua Fe
#t 2011
#c 17
#! String similarity join that finds similar string pairs between two string sets is an essential operation in many applications, and has attracted significant attention recently in the database community. A significant challenge in similarity join is to implement an effective fuzzy match operation to find all similar string pairs which may not match exactly. In this paper, we propose a new similarity metrics, called "fuzzy token matching based similarity", which extends token-based similarity functions (e.g., Jaccard similarity and Cosine similarity) by allowing fuzzy match between two tokens. We study the problem of similarity join using this new similarity metrics and present a signature-based method to address this problem. We propose new signature schemes and develop effective pruning techniques to improve the performance. Experimental results show that our approach achieves high efficiency and result quality, and significantly outperforms state-of-the-art methods.

#index 1594615
#* Semi-Streamed Index Join for near-real time execution of ETL transformations
#@ Mihaela A. Bornea;Antonios Deligiannakis;Yannis Kotidis;Vasilis Vassalos
#t 2011
#c 17
#! Active data warehouses have emerged as a new business intelligence paradigm where data in the integrated repository is refreshed in near real-time. This shift of practices achieves higher consistency between the stored information and the latest updates, which in turn influences crucially the output of decision making processes. In this paper we focus on the changes required in the implementation of Extract Transform Load (ETL) operations which now need to be executed in an online fashion. In particular, the ETL transformations frequently include the join between an incoming stream of updates and a disk-resident table of historical data or metadata. In this context we propose a novel Semi-Streaming Index Join (SSIJ) algorithm that maximizes the throughput of the join by buffering stream tuples and then judiciously selecting how to best amortize expensive disk seeks for blocks of the stored relation among a large number of stream tuples. The relation blocks required for joining with the stream are loaded from disk based on an optimal plan. In order to maximize the utilization of the available memory space for performing the join, our technique incorporates a simple but effective cache replacement policy for managing the retrieved blocks of the relation. Moreover, SSIJ is able to adapt to changing characteristics of the stream (i.e. arrival rate, data distribution) by dynamically adjusting the allocated memory between the cached relation blocks and the stream. Our experiments with a variety of synthetic and real data sets demonstrate that SSIJ consistently outperforms the state-of-the-art algorithm in terms of the maximum sustainable throughput of the join while being also able to accommodate deadlines on stream tuple processing.

#index 1594616
#* Similarity measures for multidimensional data
#@ Eftychia Baikousi;Georgios Rogkakos;Panos Vassiliadis
#t 2011
#c 17
#! How similar are two data-cubes? In other words, the question under consideration is: given two sets of points in a multidimensional hierarchical space, what is the distance value between them? In this paper we explore various distance functions that can be used over multidimensional hierarchical spaces. We organize the discussed functions with respect to the properties of the dimension hierarchies, levels and values. In order to discover which distance functions are more suitable and meaningful to the users, we conducted two user study analysis. The first user study analysis concerns the most preferred distance function between two values of a dimension. The findings of this user study indicate that the functions that seem to fit better the user needs are characterized by the tendency to consider as closest to a point in a multidimensional space, points with the smallest shortest path with respect to the same dimension hierarchy. The second user study aimed in discovering which distance function between two data cubes, is mostly preferred by users. The two functions that drew the attention of users where (a) the summation of distances between every cell of a cube with the most similar cell of another cube and (b) the Hausdorff distance function. Overall, the former function was preferred by users than the latter; however the individual scores of the tests indicate that this advantage is rather narrow.

#index 1594617
#* HyPer: A hybrid OLTP&OLAP main memory database system based on virtual memory snapshots
#@ Alfons Kemper;Thomas Neumann
#t 2011
#c 17
#! The two areas of online transaction processing (OLTP) and online analytical processing (OLAP) present different challenges for database architectures. Currently, customers with high rates of mission-critical transactions have split their data into two separate systems, one database for OLTP and one so-called data warehouse for OLAP. While allowing for decent transaction rates, this separation has many disadvantages including data freshness issues due to the delay caused by only periodically initiating the Extract Transform Load-data staging and excessive resource consumption due to maintaining two separate information systems. We present an efficient hybrid system, called HyPer, that can handle both OLTP and OLAP simultaneously by using hardware-assisted replication mechanisms to maintain consistent snapshots of the transactional data. HyPer is a main-memory database system that guarantees the ACID properties of OLTP transactions and executes OLAP query sessions (multiple queries) on the same, arbitrarily current and consistent snapshot. The utilization of the processor-inherent support for virtual memory management (address translation, caching, copy on update) yields both at the same time: unprecedentedly high transaction rates as high as 100000 per second and very fast OLAP query response times on a single system executing both workloads in parallel. The performance analysis is based on a combined TPC-C and TPC-H benchmark.

#index 1594618
#* LTS: Discriminative subgraph mining by learning from search history
#@ Ning Jin;Wei Wang
#t 2011
#c 17
#! Discriminative subgraphs can be used to characterize complex graphs, construct graph classifiers and generate graph indices. The search space for discriminative subgraphs is usually prohibitively large. Most measurements of interestingness of discriminative subgraphs are neither monotonic nor antimonotonic with respect to subgraph frequencies. Therefore, branch-and-bound algorithms are unable to mine discriminative subgraphs efficiently. We discover that search history of discriminative subgraph mining is very useful in computing empirical upper-bounds of discrimination scores of subgraphs. We propose a novel discriminative subgraph mining method, LTS (Learning To Search), which begins with a greedy algorithm that first samples the search space through subgraph probing and then explores the search space in a branch and bound fashion leveraging the search history of these samples. Extensive experiments have been performed to analyze the gain in performance by taking into account search history and to demonstrate that LTS can significantly improve performance compared with the state-of-the-art discriminative subgraph mining algorithms.

#index 1594619
#* Efficient SPectrAl Neighborhood blocking for entity resolution
#@ Liangcai Shu;Aiyou Chen;Ming Xiong;Weiyi Meng
#t 2011
#c 17
#! In many telecom and web applications, there is a need to identify whether data objects in the same source or different sources represent the same entity in the real-world. This problem arises for subscribers in multiple services, customers in supply chain management, and users in social networks when there lacks a unique identifier across multiple data sources to represent a real-world entity. Entity resolution is to identify and discover objects in the data sets that refer to the same entity in the real world. We investigate the entity resolution problem for large data sets where efficient and scalable solutions are needed. We propose a novel unsupervised blocking algorithm, namely SPectrAl Neighborhood (SPAN), which constructs a fast bipartition tree for the records based on spectral clustering such that real entities can be identified accurately by neighborhood records in the tree. There are two major novel aspects in our approach: 1)We develop a fast algorithm that performs spectral clustering without computing pairwise similarities explicitly, which dramatically improves the scalability of the standard spectral clustering algorithm; 2) We utilize a stopping criterion specified by Newman-Girvan modularity in the bipartition process. Our experimental results with both synthetic and real-world data demonstrate that SPAN is robust and outperforms other blocking algorithms in terms of accuracy while it is efficient and scalable to deal with large data sets.

#index 1594620
#* Consensus spectral clustering in near-linear time
#@ Dijun Luo;Chris Ding;Heng Huang;Feiping Nie
#t 2011
#c 17
#! This paper addresses the scalability issue in spectral analysis which has been widely used in data management applications. Spectral analysis techniques enjoy powerful clustering capability while suffer from high computational complexity. In most of previous research, the bottleneck of computational complexity of spectral analysis stems from the construction of pairwise similarity matrix among objects, which costs at least O(n2) where n is the number of the data points. In this paper, we propose a novel estimator of the similarity matrix using K-means accumulative consensus matrix which is intrinsically sparse. The computational cost of the accumulative consensus matrix is O(nlogn). We further develop a Non-negative Matrix Factorization approach to derive clustering assignment. The overall complexity of our approach remains O(nlogn). In order to validate our method, we (1) theoretically show the local preserving and convergent property of the similarity estimator, (2) validate it by a large number of real world datasets and compare the results to other state-of-the-art spectral analysis, and (3) apply it to large-scale data clustering problems. Results show that our approach uses much less computational time than other state-of-the-art clustering methods, meanwhile provides comparable clustering qualities. We also successfully apply our approach to a 5-million dataset on a single machine using reasonable time. Our techniques open a new direction for high-quality large-scale data analysis.

#index 1594621
#* On dimensionality reduction of massive graphs for indexing and retrieval
#@ Charu C. Aggarwal;Haixun Wang
#t 2011
#c 17
#! In this paper, we will examine the problem of dimensionality reduction of massive disk-resident data sets. Graph mining has become important in recent years because of its numerous applications in community detection, social networking, and web mining. Many graph data sets are defined on massive node domains in which the number of nodes in the underlying domain is very large. As a result, it is often difficult to store and hold the information necessary in order to retrieve and index the data. Most known methods for dimensionality reduction are effective only for data sets defined on modest domains. Furthermore, while the problem of dimensionality reduction is most relevant to the problem of massive data sets, these algorithms are inherently not designed for the case of disk-resident data in terms of the order in which the data is accessed on disk. This is a serious limitation which restricts the applicability of current dimensionality reduction methods. Furthermore, since dimensionality reduction methods are typically designed for database applications such as indexing, it is important to design the underlying data reduction method, so that it can be effectively used for such applications. In this paper, we will examine the difficult problem of dimensionality reduction of graph data in the difficult case in which the underlying number of nodes are very large and the data set is disk-resident. We will propose an effective sampling algorithm for dimensionality reduction and show how to perform the dimensionality reduction in a limited number of passes on disk. We will also design the technique to be highly interpretable and friendly for indexing applications. We will illustrate the effectiveness and efficiency of the approach on a number of real data sets.

#index 1594622
#* Active learning based frequent itemset mining over the deep web
#@ Tantan Liu;Gagan Agrawal
#t 2011
#c 17
#! In recent years, one mode of data dissemination has become extremely popular, which is the deep web. A key characteristics of deep web data sources is that data can only be accessed through the limited query interface they support. This paper develops a methodology for mining the deep web. Because these data sources cannot be accessed directly, thus, data mining must be performed based on sampling of the datasets. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs. Unlike existing sampling based methods, which are typically applied on relational databases or streaming data, sampling costs, and not the computation or memory costs, are the dominant consideration in designing the algorithm.

#index 1594623
#* SystemML: Declarative machine learning on MapReduce
#@ Amol Ghoting;Rajasekar Krishnamurthy;Edwin Pednault;Berthold Reinwald;Vikas Sindhwani;Shirish Tatikonda;Yuanyuan Tian;Shivakumar Vaithyanathan
#t 2011
#c 17
#! MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.

#index 1594624
#* Mining large graphs: Algorithms, inference, and discoveries
#@ U Kang;Duen Horng Chau;Christos Faloutsos
#t 2011
#c 17
#! How do we find patterns and anomalies, on graphs with billions of nodes and edges, which do not fit in memory? How to use parallelism for such terabyte-scale graphs? In this work, we focus on inference, which often corresponds, intuitively, to "guilt by association" scenarios. For example, if a person is a drug-abuser, probably its friends are so, too; if a node in a social network is of male gender, his dates are probably females. We show how to do inference on such huge graphs through our proposed HAdoop Line graph Fixed Point (Ha-Lfp), an efficient parallel algorithm for sparse billion-scale graphs, using the Hadoop platform. Our contributions include (a) the design of Ha-Lfp, observing that it corresponds to a fixed point on a line graph induced from the original graph; (b) scalability analysis, showing that our algorithm scales up well with the number of edges, as well as with the number of machines; and (c) experimental results on two private, as well as two of the largest publicly available graphs--the Web Graphs from Yahoo! (6.6 billion edges and 0.24 Tera bytes), and the Twitter graph (3.7 billion edges and 0.13 Tera bytes). We evaluated our algorithm using M45, one of the top 50 fastest supercomputers in the world, and we report patterns and anomalies discovered by our algorithm, which would be invisible otherwise.

#index 1594625
#* Accurate latency estimation in a distributed event processing system
#@ Badrish Chandramouli;Jonathan Goldstein;Roger Barga;Mirek Riedewald;Ivo Santos
#t 2011
#c 17
#! A distributed event processing system consists of one or more nodes (machines), and can execute a directed acyclic graph (DAG) of operators called a dataflow (or query), over long-running high-event-rate data sources. An important component of such a system is cost estimation, which predicts or estimates the "goodness" of a given input, i.e., operator graph and/or assignment of individual operators to nodes. Cost estimation is the foundation for solving many problems: optimization (plan selection and distributed operator placement), provisioning, admission control, and user reporting of system misbehavior. Latency is a significant user metric in many commercial real-time applications. Users are usually interested in quantiles of latency, such as worst-case or 99th percentile. However, existing cost estimation techniques for event-based dataflows use metrics that, while they may have the side-effect of being correlated with latency, do not directly or provably estimate latency. In this paper, we propose a new cost estimation technique using a metric called Mace (Maximum cumulative excess). Mace is provably equivalent to maximum system latency in a (potentially complex, multi-node) distributed event-based system. The close relationship to latency makes Mace ideal for addressing the problems described earlier. Experiments with real-world datasets on Microsoft StreamInsight deployed over 1 -- 13 nodes in a data center validate our ability to closely estimate latency (within 4%), and the use of Mace for plan selection and distributed operator placement.

#index 1594626
#* Subscriber assignment for wide-area content-based publish/subscribe
#@ Albert Yu;Pankaj K. Agarwal;Jun Yang
#t 2011
#c 17
#! We study the problem of assigning subscribers to brokers in a wide-area content-based publish/subscribe system. A good assignment should consider both subscriber interests in the event space and subscriber locations in the network space, and balance multiple performance criteria including bandwidth, delay, and load balance. The resulting optimization problem is NP-complete, so systems have turned to heuristics and/or simpler algorithms that ignore some performance criteria. Evaluating these approaches has been challenging because optimal solutions remain elusive for realistic problem sizes. To enable proper evaluation, we develop a Monte Carlo approximation algorithm with good theoretical properties and robustness to workload variations. To make it computationally feasible, we combine the ideas of linear programming, randomized rounding, coreset, and iterative reweighted sampling. We demonstrate how to use this algorithm as a yardstick to evaluate other algorithms, and why it is better than other choices of yardsticks. With its help, we show that a simple greedy algorithm works well for a number of workloads, including one generated from publicly available statistics on Google Groups. We hope that our algorithms are not only useful in their own right, but our principled approach toward evaluation will also be useful in future evaluation of solutions to similar problems in content-based publish/subscribe.

#index 1594627
#* HashFile: An efficient index structure for multimedia data
#@ Dongxiang Zhang;Divyakant Agrawal;Gang Chen;Anthony K.  H. Tung
#t 2011
#c 17
#! Nearest neighbor (NN) search in high dimensional space is an essential query in many multimedia retrieval applications. Due to the curse of dimensionality, existing index structures might perform even worse than a simple sequential scan of data when answering exact NN query. To improve the efficiency of NN search, locality sensitive hashing (LSH) and its variants have been proposed to find approximate NN. They adopt hash functions that can preserve the Euclidean distance so that similar objects have a high probability of colliding in the same bucket. Given a query object, candidate for the query result is obtained by accessing the points that are located in the same bucket. To improve the precision, each hash table is associated with m hash functions to recursively hash the data points into smaller buckets and remove the false positives. On the other hand, multiple hash tables are required to guarantee a high retrieval recall. Thus, tuning a good tradeoff between precision and recall becomes the main challenge for LSH. Recently, locality sensitive B-tree(LSB-tree) has been proposed to ensure both quality and efficiency. However, the index uses random I/O access. When the multimedia database is large, it requires considerable disk I/O cost to obtain an approximate ratio that works in practice. In this paper, we propose a novel index structure, named HashFile, for efficient retrieval of multimedia objects. It combines the advantages of random projection and linear scan. Unlike the LSH family in which each bucket is associated with a concatenation of m hash values, we only recursively partition the dense buckets and organize them as a tree structure. Given a query point q, the search algorithm explores the buckets near the query object in a top-down manner. The candidate buckets in each node are stored sequentially in increasing order of the hash value and can be efficiently loaded into memory for linear scan. HashFile can support both exact and approximate NN queries. Experimental results show that HashFile performs better than existing indexes both in answering both types of NN queries.

#index 1594628
#* CT-index: Fingerprint-based graph indexing combining cycles and trees
#@ Karsten Klein;Nils Kriege;Petra Mutzel
#t 2011
#c 17
#! Efficient subgraph queries in large databases are a time-critical task in many application areas as e.g. biology or chemistry, where biological networks or chemical compounds are modeled as graphs. The NP-completeness of the underlying subgraph isomorphism problem renders an exact subgraph test for each database graph infeasible. Therefore efficient methods have to be found that avoid most of these tests but still allow to identify all graphs containing the query pattern. We propose a new approach based on the filter-verification paradigm, using a new hash-key fingerprint technique with a combination of tree and cycle features for filtering and a new subgraph isomorphism test for verification. Our approach is able to cope with edge and vertex labels and also allows to use wild card patterns for the search. We present an experimental comparison of our approach with state-of-the-art methods using a benchmark set of both real world and generated graph instances that shows its practicability. Our approach is implemented as part of the Scaffold Hunter software, a tool for the visual analysis of chemical compound databases.

#index 1594629
#* Jackpine: A benchmark to evaluate spatial database performance
#@ Suprio Ray;Bogdan Simion;Angela Demke Brown
#t 2011
#c 17
#! The volume of spatial data generated and consumed is rising exponentially and new applications are emerging as the costs of storage, processing power and network bandwidth continue to decline. Database support for spatial operations is fast becoming a necessity rather than a niche feature provided by a few products. However, the spatial functionality offered by current commercial and open-source relational databases differs significantly in terms of available features, true geodetic support, spatial functions and indexing. Benchmarks play a crucial role in evaluating the functionality and performance of a particular database, both for application users and developers, and for the database developers themselves. In contrast to transaction processing, however, there is no standard, widely used benchmark for spatial database operations. In this paper, we present a spatial database benchmark called Jackpine. Our benchmark is portable (it can support any database with a JDBC driver implementation) and includes both micro benchmarks and macro workload scenarios. The micro benchmark component tests basic spatial operations in isolation; it consists of queries based on the Dimensionally Extended 9-intersection model of topological relations and queries based on spatial analysis functions. Each macro workload includes a series of queries that are based on a common spatial data application. These macro scenarios include map search and browsing, geocoding, reverse geocoding, flood risk analysis, land information management and toxic spill analysis. We use Jackpine to evaluate the spatial features in 2 open source databases and 1 commercial offering.

#index 1594630
#* Hyracks: A flexible and extensible foundation for data-intensive computing
#@ Vinayak Borkar;Michael Carey;Raman Grover;Nicola Onose;Rares Vernica
#t 2011
#c 17
#! Hyracks is a new partitioned-parallel software platform designed to run data-intensive computations on large shared-nothing clusters of computers. Hyracks allows users to express a computation as a DAG of data operators and connectors. Operators operate on partitions of input data and produce partitions of output data, while connectors repartition operators' outputs to make the newly produced partitions available at the consuming operators. We describe the Hyracks end user model, for authors of dataflow jobs, and the extension model for users who wish to augment Hyracks' built-in library with new operator and/or connector types. We also describe our initial Hyracks implementation. Since Hyracks is in roughly the same space as the open source Hadoop platform, we compare Hyracks with Hadoop experimentally for several different kinds of use cases. The initial results demonstrate that Hyracks has significant promise as a next-generation platform for data-intensive applications.

#index 1594631
#* Collaborative caching for spatial queries in Mobile P2P Networks
#@ Qijun Zhu;Dik Lun Lee;Wang-Chien Lee
#t 2011
#c 17
#! We propose a novel collaborative caching framework to support spatial query processing in Mobile Peer-to-Peer Networks (MP2PNs). To maximize cache sharing among clients, each client caches not only data objects but also parts of the index structure built on the spatial objects. Thus, we call the proposed method structure-embedded collaborative caching (SECC). By introducing a novel index structure called Signature Augment Tree (SAT), we address two crucial issues in SECC. First, we propose a cost-efficient collaborative query processing method in MP2PNs, including peer selection and result merge from multiple peers. Second, we develop a novel collaborative cache replacement policy which maximizes cache effectiveness by considering not only the peer itself but also its neighbors. We implement two SECC schemes, namely, the periodical and adaptive SAT-based schemes, with different SAT maintenance policies. Simulation results show that our SECC schemes significantly outperform other collaborative caching methods which are based on existing spatial caching schemes in a number of metrics, including traffic volume, query latency and power consumption.

#index 1594632
#* ES2: A cloud data storage system for supporting both OLTP and OLAP
#@ Yu Cao;Chun Chen;Fei Guo;Dawei Jiang;Yuting Lin;Beng Chin Ooi;Hoang Tam Vo;Sai Wu;Quanqing Xu
#t 2011
#c 17
#! Cloud computing represents a paradigm shift driven by the increasing demand of Web based applications for elastic, scalable and efficient system architectures that can efficiently support their ever-growing data volume and large-scale data analysis. A typical data management system has to deal with real-time updates by individual users, and as well as periodical large scale analytical processing, indexing, and data extraction. While such operations may take place in the same domain, the design and development of the systems have somehow evolved independently for transactional and periodical analytical processing. Such a system-level separation has resulted in problems such as data freshness as well as serious data storage redundancy. Ideally, it would be more efficient to apply ad-hoc analytical processing on the same data directly. However, to the best of our knowledge, such an approach has not been adopted in real implementation. Intrigued by such an observation, we have designed and implemented epiC, an elastic power-aware data-itensive Cloud platform for supporting both data intensive analytical operations (ref. as OLAP) and online transactions (ref. as OLTP). In this paper, we present ES2 - the elastic data storage system of epiC, which is designed to support both functionalities within the same storage. We present the system architecture and the functions of each system component, and experimental results which demonstrate the efficiency of the system.

#index 1594633
#* Deriving probabilistic databases with inference ensembles
#@ Julia Stoyanovich;Susan Davidson;Tova Milo;Val Tannen
#t 2011
#c 17
#! Many real-world applications deal with uncertain or missing data, prompting a surge of activity in the area of probabilistic databases. A shortcoming of prior work is the assumption that an appropriate probabilistic model, along with the necessary probability distributions, is given. We address this shortcoming by presenting a framework for learning a set of inference ensembles, termed meta-rule semi-lattices, or MRSL, from the complete portion of the data. We use the MRSL to infer probability distributions for missing data, and demonstrate experimentally that high accuracy is achieved when a single attribute value is missing per tuple. We next propose an inference algorithm based on Gibbs sampling that accurately predicts the probability distribution for multiple missing values. We also develop an optimization that greatly improves performance of multi-attribute inference for collections of tuples, while maintaining high accuracy. Finally, we develop an experimental framework to evaluate the efficiency and accuracy of our approach.

#index 1594634
#* Providing support for full relational algebra in probabilistic databases
#@ Robert Fink;Dan Olteanu;Swaroop Rath
#t 2011
#c 17
#! Extensive work has recently been done on the evaluation of positive queries on probabilistic databases. The case of queries with negation has notoriously been left out, since it raises serious additional challenges to efficient query evaluation.

#index 1594635
#* Creating probabilistic databases from imprecise time-series data
#@ Saket Sathe;Hoyoung Jeung;Karl Aberer
#t 2011
#c 17
#! Although efficient processing of probabilistic databases is a well-established field, a wide range of applications are still unable to benefit from these techniques due to the lack of means for creating probabilistic databases. In fact, it is a challenging problem to associate concrete probability values with given time-series data for forming a probabilistic database, since the probability distributions used for deriving such probability values vary over time. In this paper, we propose a novel approach to create tuple-level probabilistic databases from (imprecise) time-series data. To the best of our knowledge, this is the first work that introduces a generic solution for creating probabilistic databases from arbitrary time series, which can work in online as well as offline fashion. Our approach consists of two key components. First, the dynamic density metrics that infer time-dependent probability distributions for time series, based on various mathematical models. Our main metric, called the GARCH metric, can robustly capture such evolving probability distributions regardless of the presence of erroneous values in a given time series. Second, the Ω-View builder that creates probabilistic databases from the probability distributions inferred by the dynamic density metrics. For efficient processing, we introduce the σ-cache that reuses the information derived from probability values generated at previous times. Extensive experiments over real datasets demonstrate the effectiveness of our approach.

#index 1594636
#* On query result diversification
#@ Marcos R. Vieira;Humberto L. Razente;Maria C.  N. Barioni;Marios Hadjieleftheriou;Divesh Srivastava;Caetano Traina;Vassilis J. Tsotras
#t 2011
#c 17
#! In this paper we describe a general framework for evaluation and optimization of methods for diversifying query results. In these methods, an initial ranking candidate set produced by a query is used to construct a result set, where elements are ranked with respect to relevance and diversity features, i.e., the retrieved elements should be as relevant as possible to the query, and, at the same time, the result set should be as diverse as possible. While addressing relevance is relatively simple and has been heavily studied, diversity is a harder problem to solve. One major contribution of this paper is that, using the above framework, we adapt, implement and evaluate several existing methods for diversifying query results. We also propose two new approaches, namely the Greedy with Marginal Contribution (GMC) and the Greedy Randomized with Neighborhood Expansion (GNE) methods. Another major contribution of this paper is that we present the first thorough experimental evaluation of the various diversification techniques implemented in a common framework. We examine the methods' performance with respect to precision, running time and quality of the result. Our experimental results show that while the proposed methods have higher running times, they achieve precision very close to the optimal, while also providing the best result quality. While GMC is deterministic, the randomized approach (GNE) can achieve better result quality if the user is willing to tradeoff running time.

#index 1594637
#* Generating test data for killing SQL mutants: A constraint-based approach
#@ Shetal Shah;S. Sudarshan;Suhas Kajbaje;Sandeep Patidar;Bhanu Pratap Gupta;Devang Vira
#t 2011
#c 17
#! Complex SQL queries are widely used today, but it is rather difficult to check if a complex query has been written correctly. Formal verification based on comparing a specification with an implementation is not applicable, since SQL queries are essentially a specification without any implementation. Queries are usually checked by running them on sample datasets and checking that the correct result is returned; there is no guarantee that all possible errors are detected. In this paper, we address the problem of test data generation for checking correctness of SQL queries, based on the query mutation approach for modeling errors. Our presentation focuses in particular on a class of join/outer-join mutations, comparison operator mutations, and aggregation operation mutations, which are a common cause of error. To minimize human effort in testing, our techniques generate a test suite containing small and intuitive test datasets. The number of datasets generated, is linear in the size of the query, although the number of mutations in the class we consider is exponential. Under certain assumptions on constraints and query constructs, the test suite we generate is complete for a subclass of mutations that we define, i.e., it kills all non-equivalent mutations in this subclass.

#index 1594638
#* Implementing sentinels in the TARGIT BI suite
#@ Morten Middelfart;Torben Bach Pedersen
#t 2011
#c 17
#! This paper describes the implementation of so-called sentinels in the TARGIT BI Suite. Sentinels are a novel type of rules that can warn a user if one or more measure changes in a multi-dimensional data cube are expected to cause a change to another measure critical to the user. Sentinels notify users based on previous observations, e.g., that revenue might drop within two months if an increase in customer problems combined with a decrease in website traffic is observed. In this paper we show how users, without any prior technical knowledge, can mine and use sentinels in the TARGIT BI Suite. We present in detail how sentinels are mined from data, and how sentinels are scored. We describe in detail how the sentinel mining algorithm is implemented in the TARGIT BI Suite, and show that our implementation is able to discover strong and useful sentinels that could not be found when using sequential pattern mining or correlation techniques. We demonstrate, through extensive experiments, that mining and usage of sentinels is feasible with good performance for the typical users on a real, operational data warehouse.

#index 1594639
#* RCFile: A fast and space-efficient data placement structure in MapReduce-based warehouse systems
#@ Yongqiang He;Rubao Lee;Yin Huai;Zheng Shao;Namit Jain;Xiaodong Zhang;Zhiwei Xu
#t 2011
#c 17
#! MapReduce-based data warehouse systems are playing important roles of supporting big data analytics to understand quickly the dynamics of user behavior trends and their needs in typical Web service providers and social network sites (e.g., Facebook). In such a system, the data placement structure is a critical factor that can affect the warehouse performance in a fundamental way. Based on our observations and analysis of Facebook production systems, we have characterized four requirements for the data placement structure: (1) fast data loading, (2) fast query processing, (3) highly efficient storage space utilization, and (4) strong adaptivity to highly dynamic workload patterns. We have examined three commonly accepted data placement structures in conventional databases, namely row-stores, column-stores, and hybrid-stores in the context of large data analysis using MapReduce. We show that they are not very suitable for big data processing in distributed systems. In this paper, we present a big data placement structure called RCFile (Record Columnar File) and its implementation in the Hadoop system. With intensive experiments, we show the effectiveness of RCFile in satisfying the four requirements. RCFile has been chosen in Facebook data warehouse system as the default option. It has also been adopted by Hive and Pig, the two most widely used data analysis systems developed in Facebook and Yahoo!

#index 1594640
#* Web-scale information extraction with vertex
#@ Pankaj Gulhane;Amit Madaan;Rupesh Mehta;Jeyashankher Ramamirtham;Rajeev Rastogi;Sandeep Satpal;Srinivasan H. Sengamedu;Ashwin Tengli;Charu Tiwari
#t 2011
#c 17
#! Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale.

#index 1594641
#* A novel probabilistic pruning approach to speed up similarity queries in uncertain databases
#@ Thomas Bernecker;Tobias Emrich;Hans-Peter Kriegel;Nikos Mamoulis;Matthias Renz;Andreas Zufle
#t 2011
#c 17
#! In this paper, we propose a novel, effective and efficient probabilistic pruning criterion for probabilistic similarity queries on uncertain data. Our approach supports a general uncertainty model using continuous probabilistic density functions to describe the (possibly correlated) uncertain attributes of objects. In a nutshell, the problem to be solved is to compute the PDF of the random variable denoted by the probabilistic domination count: Given an uncertain database object B, an uncertain reference object R and a set D of uncertain database objects in a multi-dimensional space, the probabilistic domination count denotes the number of uncertain objects in D that are closer to R than B. This domination count can be used to answer a wide range of probabilistic similarity queries. Specifically, we propose a novel geometric pruning filter and introduce an iterative filter-refinement strategy for conservatively and progressively estimating the probabilistic domination count in an efficient way while keeping correctness according to the possible world semantics. In an experimental evaluation, we show that our proposed technique allows to acquire tight probability bounds for the probabilistic domination count quickly, even for large uncertain databases.

#index 1594642
#* Interactive SQL query suggestion: Making databases user-friendly
#@ Ju Fan;Guoliang Li;Lizhu Zhou
#t 2011
#c 17
#! SQL is a classical and powerful tool for querying relational databases. However, it is rather hard for inexperienced users to pose SQL queries, as they are required to be proficient in SQL syntax and have a thorough understanding of the underlying schema. To give users gratification, we propose SQLSUGG, an effective and user-friendly keyword-based method to help various users formulate SQL queries. SQLSUGG suggests SQL queries as users type in keywords, and can save users' typing efforts and help users avoid tedious SQL debugging. To achieve high suggestion effectiveness, we propose queryable templates to model the structures of SQL queries. We propose a template ranking model to suggest templates relevant to query keywords. We generate SQL queries from each suggested template based on the degree of matchings between keywords and attributes. For efficiency, we propose a progressive algorithm to compute top-k templates, and devise an efficient method to generate SQL queries from templates. We have implemented our methods on two real data sets, and the experimental results show that our method achieves high effectiveness and efficiency.

#index 1594643
#* Computing structural statistics by keywords in databases
#@ Lu Qin;Jeffrey Xu Yu;Lijun Chang
#t 2011
#c 17
#! Keyword search in RDBs has been extensively studied in recent years. The existing studies focused on finding all or top-k interconnected tuple-structures that contain keywords. In reality, the number of such interconnected tuple-structures for a keyword query can be large. It becomes very difficult for users to obtain any valuable information more than individual interconnected tuple-structures. Also, it becomes challenging to provide a similar mechanism like group-&-aggregate for those interconnected tuple-structures. In this paper, we study computing structural statistics keyword queries by extending the group-&-aggregate framework. We consider an RDB as a large directed graph where nodes represent tuples, and edges represent the links among tuples. Instead of using tuples as a member in a group to be grouped, we consider rooted subgraphs. Such a rooted subgraph represents an interconnected tuple-structure among tuples and some of the tuples contain keywords. The dimensions of the rooted subgraphs are determined by dimensional-keywords in a data driven fashion. Two rooted subgraphs are grouped into the same group if they are isomorphic based on the dimensions or in other words the dimensional-keywords. The scores of the rooted subgraphs are computed by a user-given score function if the rooted subgraphs contain some of general keywords. Here, the general keywords are used to compute scores rather than determining dimensions. The aggregates are computed using an SQL aggregate function for every group based on the scores computed. We give our motivation using a real dataset. We propose new approaches to compute structural statistics keyword queries, perform extensive performance studies using two large real datasets and a large synthetic dataset, and confirm the effectiveness and efficiency of our approach.

#index 1594644
#* Program transformations for asynchronous query submission
#@ Mahendra Chavan;Ravindra Guravannavar;Karthik Ramachandra;S. Sudarshan
#t 2011
#c 17
#! Synchronous execution of queries or Web service requests forces the calling application to block until the query/request is satisfied. The performance of applications can be significantly improved by asynchronous submission of queries, which allows the application to perform other processing instead of blocking while the query is executed, and to concurrently issue multiple queries. Concurrent submission of multiple queries can allow the query execution engine to better utilize multiple processors and disks, and to reorder disk IO requests to minimize seeks. Concurrent submission also reduces the impact of network round-trip latency and delays at the database, when processing multiple queries. However, manually writing applications to exploit asynchronous query submission is tedious. In this paper we address the issue of automatically transforming a program written assuming synchronous query submission, to one that exploits asynchronous query submission. Our program transformation method is based on dataflow analysis and is framed as a set of transformation rules. Our rules can handle query executions within loops, unlike some of the earlier work in this area. We have built a tool that implements our transformation techniques on Java code that uses JDBC calls; our tool can be extended to handle Web service calls. We have carried out a detailed experimental study on several real-life applications rewritten using our transformation techniques. The experimental study shows the effectiveness of the proposed rewrite techniques, both in terms of their applicability and performance gains achieved.

#index 1594645
#* High performance database logging using storage class memory
#@ Ru Fang;Hui-I Hsiao;Bin He;C. Mohan;Yun Wang
#t 2011
#c 17
#! Storage class memory (SCM), a new generation of memory technology, offers non-volatility, high-speed, and byte-addressability, which combines the best properties of current hard disk drives (HDD) and main memory. With these extraordinary features, current systems and software stacks need to be redesigned to get significantly improved performance by eliminating disk input/output (I/O) barriers; and simpler system designs by avoiding complicated data format transformations. In current DBMSs, logging and recovery are the most important components to enforce the atomicity and durability of a database. Traditionally, database systems rely on disks for logging transaction actions and log records are forced to disks when a transaction commits. Because of the slow disk I/O speed, logging becomes one of the major bottlenecks for a DBMS. Exploiting SCM as a persistent memory for transaction logging can significantly reduce logging overhead. In this paper, we present the detailed design of an SCM-based approach for DBMSs logging, which achieves high performance by simplified system design and better concurrency support. We also discuss solutions to tackle several major issues arising during system recovery, including hole detection, partial write detection, and any-point failure recovery. This new logging approach is used to replace the traditional disk based logging approach in DBMSs. To analyze the performance characteristics of our SCM-based logging approach, we implement the prototype on IBM SolidDB. In common circumstances, our experimental results show that the new SCM-based logging approach provides as much as 7 times throughput improvement over disk-based logging in the Telecommunication Application Transaction Processing (TATP) benchmark.

#index 1594646
#* Dynamic prioritization of database queries
#@ Sivaramakrishnan Narayanan;Florian Waas
#t 2011
#c 17
#! Enterprise database systems handle a variety of diverse query workloads that are of different importance to the business. For example, periodic reporting queries are usually mission critical whereas ad-hoc queries by analysts tend to be less crucial. It is desirable to enable database administrators to express (and modify) the importance of queries at a simple and intuitive level. The mechanism used to enforce these priorities must be robust, adaptive and efficient.

#index 1594647
#* The extensibility framework in Microsoft StreamInsight
#@ Mohamed Ali;Badrish Chandramouli;Jonathan Goldstein;Roman Schindlauer
#t 2011
#c 17
#! Microsoft StreamInsight (StreamInsight, for brevity) is a platform for developing and deploying streaming applications, which need to run continuous queries over high-data-rate streams of input events. StreamInsight leverages a well-defined temporal stream model and operator algebra, as the underlying basis for processing long-running continuous queries over event streams. This allows StreamInsight to handle imperfections in event delivery and to provide correctness guarantees on the generated output. StreamInsight natively supports a diverse range of off-the-shelf streaming operators. In order to cater to a much broader range of customer scenarios and applications, StreamInsight has recently introduced a new extensibility infrastructure. With this infrastructure, StreamInsight enables developers to integrate their domain expertise within the query pipeline in the form of user defined modules (functions, operators, and aggregates). This paper describes the extensibility framework in StreamInsight; an ongoing effort at Microsoft SQL Server to support the integration of user-defined modules in a stream processing system. More specifically, the paper addresses the extensibility problem from three perspectives: the query writer's perspective, the user defined module writer's perspective, and the system's internal perspective. The paper introduces and addresses a range of new and subtle challenges that arise when we try to add extensibility to a streaming system, in a manner that is easy to use, powerful, and practical. We summarize our experience and provide future directions for supporting stream-oriented workloads in different business domains.

#index 1594648
#* Relational databases, virtualization, and the cloud
#@ Maximilian Ahrens;Gustavo Alonso
#t 2011
#c 17
#! Existing relational databases are facing significant challenges as the hardware infrastructure and the underlying platform change from single CPUs to virtualized multicore machines arranged in large clusters. The problems are both technical and related to the licensing models currently in place. In this short abstract we briefly outline the challenges faced by organizations trying to virtualize and bring existing relational databases into the cloud.

#index 1594649
#* Adapting microsoft SQL server for cloud computing
#@ Philip A. Bernstein;Istvan Cseri;Nishant Dani;Nigel Ellis;Ajay Kalhan;Gopal Kakivaya;David B. Lomet;Ramesh Manne;Lev Novik;Tomas Talius
#t 2011
#c 17
#! Cloud SQL Server is a relational database system designed to scale-out to cloud computing workloads. It uses Microsoft SQL Server as its core. To scale out, it uses a partitioned database on a shared-nothing system architecture. Transactions are constrained to execute on one partition, to avoid the need for two-phase commit. The database is replicated for high availability using a custom primary-copy replication scheme. It currently serves as the storage engine for Microsoft's Exchange Hosted Archive and SQL Azure.

#index 1594650
#* Predicting in-memory database performance for automating cluster management tasks
#@ Jan Schaffner;Benjamin Eckart;Dean Jacobs;Christian Schwarz;Hasso Plattner;Alexander Zeier
#t 2011
#c 17
#! In Software-as-a-Service, multiple tenants are typically consolidated into the same database instance to reduce costs. For analytics-as-a-service, in-memory column databases are especially suitable because they offer very short response times. This paper studies the automation of operational tasks in multi-tenant in-memory column database clusters. As a prerequisite, we develop a model for predicting whether the assignment of a particular tenant to a server in the cluster will lead to violations of response time goals. This model is then extended to capture drops in capacity incurred by migrating tenants between servers. We present an algorithm for moving tenants around the cluster to ensure that response time goals are met. In so doing, the number of servers in the cluster may be dynamically increased or decreased. The model is also extended to manage multiple copies of a tenant's data for scalability and availability. We validated the model with an implementation of a multi-tenant clustering framework for SAP's in-memory column database TREX.

#index 1594651
#* Representative skylines using threshold-based preference distributions
#@ Atish Das Sarma;Ashwin Lall;Danupon Nanongkai;Richard J. Lipton;Jim Xu
#t 2011
#c 17
#! The study of skylines and their variants has received considerable attention in recent years. Skylines are essentially sets of most interesting (undominated) tuples in a database. However, since the skyline is often very large, much research effort has been devoted to identifying a smaller subset of (say k) "representative skyline" points. Several different definitions of representative skylines have been considered. Most of these formulations are intuitive in that they try to achieve some kind of clustering "spread" over the entire skyline, with k points. In this work, we take a more principled approach in defining the representative skyline objective. One of our main contributions is to formulate the problem of displaying k representative skyline points such that the probability that a random user would click on one of them is maximized.

#index 1594652
#* Outlier detection in graph streams
#@ Charu C. Aggarwal;Yuchen Zhao;Philip S. Yu
#t 2011
#c 17
#! A number of applications in social networks, telecommunications, and mobile computing create massive streams of graphs. In many such applications, it is useful to detect structural abnormalities which are different from the "typical" behavior of the underlying network. In this paper, we will provide first results on the problem of structural outlier detection in massive network streams. Such problems are inherently challenging, because the problem of outlier detection is specially challenging because of the high volume of the underlying network stream. The stream scenario also increases the computational challenges for the approach. We use a structural connectivity model in order to define outliers in graph streams. In order to handle the sparsity problem of massive networks, we dynamically partition the network in order to construct statistically robust models of the connectivity behavior. We design a reservoir sampling method in order to maintain structural summaries of the underlying network. These structural summaries are designed in order to create robust, dynamic and efficient models for outlier detection in graph streams. We present experimental results illustrating the effectiveness and efficiency of our approach.

#index 1594653
#* Locality Sensitive Outlier Detection: A ranking driven approach
#@ Ye Wang;Srinivasan Parthasarathy;Shirish Tatikonda
#t 2011
#c 17
#! Outlier detection is fundamental to a variety of database and analytic tasks. Recently, distance-based outlier detection has emerged as a viable and scalable alternative to traditional statistical and geometric approaches. In this article we explore the role of ranking for the efficient discovery of distance-based outliers from large high dimensional data sets. Specifically, we develop a light-weight ranking scheme that is powered by locality sensitive hashing, which reorders the database points according to their likelihood of being an outlier. We provide theoretical arguments to justify the rationale for the approach and subsequently conduct an extensive empirical study highlighting the effectiveness of our approach over extant solutions. We show that our ranking scheme improves the efficiency of the distance-based outlier discovery process by up to 5-fold. Furthermore, we find that using our approach the top outliers can often be isolated very quickly, typically by scanning less than 3% of the data set.

#index 1594654
#* Outlier detection on uncertain data: Objects, instances, and inferences
#@ Bin Jiang;Jian Pei
#t 2011
#c 17
#! This paper studies the problem of outlier detection on uncertain data. We start with a comprehensive model considering both uncertain objects and their instances. An uncertain object has some inherent attributes and consists of a set of instances which are modeled by a probability density distribution. We detect outliers at both the instance level and the object level. To detect outlier instances, it is a prerequisite to know normal instances. By assuming that uncertain objects with similar properties tend to have similar instances, we learn the normal instances for each uncertain object using the instances of objects with similar properties. Consequently, outlier instances can be detected by comparing against normal ones. Furthermore, we can detect outlier objects most of whose instances are outliers. Technically, we use a Bayesian inference algorithm to solve the problem, and develop an approximation algorithm and a filtering algorithm to speed up the computation. An extensive empirical study on both real data and synthetic data verifies the effectiveness and efficiency of our algorithms.

#index 1594655
#* Statistical selection of relevant subspace projections for outlier ranking
#@ Emmanuel Muller;Matthias Schiffer;Thomas Seidl
#t 2011
#c 17
#! Outlier mining is an important data analysis task to distinguish exceptional outliers from regular objects. For outlier mining in the full data space, there are well established methods which are successful in measuring the degree of deviation for outlier ranking. However, in recent applications traditional outlier mining approaches miss outliers as they are hidden in subspace projections. Especially, outlier ranking approaches measuring deviation on all available attributes miss outliers deviating from their local neighborhood only in subsets of the attributes. In this work, we propose a novel outlier ranking based on the objects deviation in a statistically selected set of relevant subspace projections. This ensures to find objects deviating in multiple relevant subspaces, while it excludes irrelevant projections showing no clear contrast between outliers and the residual objects. Thus, we tackle the general challenges of detecting outliers hidden in subspaces of the data. We provide a selection of subspaces with high contrast and propose a novel ranking based on an adaptive degree of deviation in arbitrary subspaces. In thorough experiments on real and synthetic data we show that our approach outperforms competing outlier ranking approaches by detecting outliers in arbitrary subspace projections.

#index 1594656
#* ATOM: Automatic target-driven ontology merging
#@ Salvatore Raunich;Erhard Rahm
#t 2011
#c 17
#! The proliferation of ontologies and taxonomies in many domains increasingly demands the integration of multiple such ontologies to provide a unified view on them. We demonstrate a new automatic approach to merge large taxonomies such as product catalogs or web directories. Our approach is based on an equivalence matching between a source and target taxonomy to merge them. It is target-driven, i.e. it preserves the structure of the target taxonomy as much as possible. Further, we show how the approach can utilize additional relationships between source and target concepts to semantically improve the merge result.

#index 1594657
#* Automatic generation of mediated schemas through reasoning over data dependencies
#@ Xiang Li;Christoph Quix;David Kensche;Sandra Geisler;Lisong Guo
#t 2011
#c 17
#! Mediated schemas lie at the center of the well recognized data integration architecture. Classical data integration systems rely on a mediated schema created by human experts through an intensive design process. Automatic generation of mediated schemas is still a goal to be achieved. We generate mediated schemas by merging multiple source schemas interrelated by tuple-generating dependencies (tgds). Schema merging is the process to consolidate multiple schemas into a unified view. The task becomes particularly challenging when the schemas are highly heterogeneous and autonomous. Existing approaches fall short in various aspects, such as restricted expressiveness of input mappings, lacking data level interpretation, the output mapping is not in a logical language (or not given at all), and being confined to binary merging. We present here a novel system which is able to perform native n-ary schema merging using P2P style tgds as input. Suited in the scenario of generating mediated schemas for data integration, the system opts for a minimal schema signature retaining all certain answers of conjunctive queries. Logical output mappings are generated to support the mediated schemas, which enable query answering and, in some cases, query rewriting.

#index 1594658
#* DBridge: A program rewrite tool for set-oriented query execution
#@ Mahendra Chavan;Ravindra Guravannavar;Karthik Ramachandra;S. Sudarshan
#t 2011
#c 17
#! We present DBridge, a novel static analysis and program transformation tool to optimize database access. Traditionally, rewrite of queries and programs are done independently, by the database query optimzier and the language compiler respectively, leaving out many optimization opportunities. Our tool aims to bridge this gap by performing holistic transformations, which include both program and query rewrite.

#index 1594659
#* SmartTrace: Finding similar trajectories in smartphone networks without disclosing the traces
#@ Costandinos Costa;Christos Laoudias;Demetrios Zeinalipour-Yazti;Dimitrios Gunopulos
#t 2011
#c 17
#! In this demonstration paper, we present a powerful distributed framework for finding similar trajectories in a smartphone network, without disclosing the traces of participating users. Our framework, exploits opportunistic and participatory sensing in order to quickly answer queries of the form: "Report objects (i.e., trajectories) that follow a similar spatio-temporal motion to Q, where Q is some query trajectory." SmartTrace, relies on an in-situ data storage model, where geo-location data is recorded locally on smartphones for both performance and privacy reasons. SmartTrace then deploys an efficient top-K query processing algorithm that exploits distributed trajectory similarity measures, resilient to spatial and temporal noise, in order to derive the most relevant answers to Q quickly and efficiently. Our demonstration shows how the SmartTrace algorithmics are ported on a network of Android-based smartphone devices with impressive query response times. To demonstrate the capabilities of SmartTrace during the conference, we will allow the attendees to query local smartphone networks in the following two modes: i) Interactive Mode, where devices will be handed out to participants aiming to identify who is moving similar to the querying node; and ii) Trace-driven Mode, where a large-scale deployment can be launched in order to show how the K most similar trajectories can be identified quickly and efficiently. The conference attendees will be able to appreciate how interesting spatio-temporal search applications can be implemented efficiently (for performance reasons) and without disclosing the complete user traces to the query processor (for privacy reasons)1. For instance, an attendee might be able to determine other attendees that have participated in common sessions, in order to initiate new discussions and collaborations, without knowing their trajectory or revealing his/her own trajectory either.

#index 1594660
#* Real-time pattern matching with FPGAs
#@ Louis Woods;Jens Teubner;Gustavo Alonso
#t 2011
#c 17
#! We demonstrate a hardware implementation of a complex event processor, built on top of field-programmable gate arrays (FPGAs). Compared to CPU-based commodity systems, our solution shows distinctive advantages for stream monitoring tasks, e.g., wire-speed processing and predictable performance. The demonstration is based on a query-to-hardware compiler for complex event patterns that we presented at VLDB 2010 [1]. By example of a click stream monitoring application, we illustrate the inner workings of our compiler and indicate how FPGAs can act as efficient and reliable processors for event streams.

#index 1594661
#* Social networking on top of the WebdamExchange system
#@ Emilien Antoine;Alban Galland;Kristian Lyngbaek;Amelie Marian;Neokis Polyzotis
#t 2011
#c 17
#! The demonstration presents the WebdamExchange system, a distributed knowledge base management system with access rights, localization and provenance. This system is based on the exchange of logical statements that describe documents, collections, access rights, keys and localization information and updates of this data. We illustrate how the model can be used in a social-network context to help users keep control on their data on the web. In particular, we show how users within very different schemes of data-distribution (centralized, dht, unstructured P2P, etc) can still transparently collaborate while keeping a good control over their own data.

#index 1594662
#* A unified model for data and constraint repair
#@ Fei Chiang;Renee J. Miller
#t 2011
#c 17
#! Integrity constraints play an important role in data design. However, in an operational database, they may not be enforced for many reasons. Hence, over time, data may become inconsistent with respect to the constraints. To manage this, several approaches have proposed techniques to repair the data, by finding minimal or lowest cost changes to the data that make it consistent with the constraints. Such techniques are appropriate for the old world where data changes, but schemas and their constraints remain fixed. In many modern applications however, constraints may evolve over time as application or business rules change, as data is integrated with new data sources, or as the underlying semantics of the data evolves. In such settings, when an inconsistency occurs, it is no longer clear if there is an error in the data (and the data should be repaired), or if the constraints have evolved (and the constraints should be repaired). In this work, we present a novel unified cost model that allows data and constraint repairs to be compared on an equal footing. We consider repairs over a database that is inconsistent with respect to a set of rules, modeled as functional dependencies (FDs). FDs are the most common type of constraint, and are known to play an important role in maintaining data quality. We evaluate the quality and scalability of our repair algorithms over synthetic data and present a qualitative case study using a well-known real dataset. The results show that our repair algorithms not only scale well for large datasets, but are able to accurately capture and correct inconsistencies, and accurately decide when a data repair versus a constraint repair is best.

#index 1594663
#* On data dependencies in dataspaces
#@ Shaoxu Song;Lei Chen;Philip S. Yu
#t 2011
#c 17
#! To study data dependencies over heterogeneous data in dataspaces, we define a general dependency form, namely comparable dependencies (CDs), which specifies constraints on comparable attributes. It covers the semantics of a broad class of dependencies in databases, including functional dependencies (FDs), metric functional dependencies (MFDs), and matching dependencies (MDs). As we illustrated, comparable dependencies are useful in real practice of dataspaces, e.g., semantic query optimization. Due to the heterogeneous data in dataspaces, the first question, known as the validation problem, is to determine whether a dependency (almost) holds in a data instance. Unfortunately, as we proved, the validation problem with certain error or confidence guarantee is generally hard. In fact, the confidence validation problem is also NP-hard to approximate to within any constant factor. Nevertheless, we develop several approaches for efficient approximation computation, including greedy and randomized approaches with an approximation bound on the maximum number of violations that an object may introduce. Finally, through an extensive experimental evaluation on real data, we verify the superiority of our methods.

#index 1594664
#* Precisely Serializable Snapshot Isolation (PSSI)
#@ Stephen Revilak;Patrick O'Neil;Elizabeth O'Neil
#t 2011
#c 17
#! Many popular database management systems provide snapshot isolation (SI) for concurrency control, either in addition to or in place of full serializability based on locking. Snapshot isolation was introduced in 1995 [2], with noted anomalies that can lead to serializability violations. Full serializability was provided in 2008 [4] and improved in 2009 [5] by aborting transactions in dangerous structures, which had been shown in 2005 [9] to be precursors to potential SI anomalies. This approach resulted in a runtime environment guaranteeing a serializable form of snapshot isolation (which we call SSI [4] or ESSI [5]) for arbitrary applications. But transactions in a dangerous structure frequently do not cause true anomalies so, as the authors point out, their method is conservative: it can cause unnecessary aborts. In the current paper, we demonstrate our PSSI algorithm to detect cycles in a snapshot isolation dependency graph and abort transactions to break the cycle. This algorithm provides a much more precise criterion to perform aborts. We have implemented our algorithm in an open source production database system (MySQL/InnoDB), and our performance study shows that PSSI throughput improves on ESSI, with significantly fewer aborts.

#index 1594665
#* MobiMix: Protecting location privacy with mix-zones over road networks
#@ Balaji Palanisamy;Ling Liu
#t 2011
#c 17
#! This paper presents MobiMix, a road network based mix-zone framework to protect location privacy of mobile users traveling on road networks. In contrast to spatial cloaking based location privacy protection, the approach in MobiMix is to break the continuity of location exposure by using mix-zones, where no applications can trace user movement. This paper makes two original contributions. First, we provide the formal analysis on the vulnerabilities of directly applying theoretical rectangle mix-zones to road networks in terms of anonymization effectiveness and attack resilience. We argue that effective mix-zones should be constructed and placed by carefully taking into consideration of multiple factors, such as the geometry of the zones, the statistical behavior of the user population, the spatial constraints on movement patterns of the users, and the temporal and spatial resolution of the location exposure. Second, we develop a suite of road network mix-zone construction methods that provide higher level of attack resilience and yield a specified lower-bound on the level of anonymity. We evaluate the MobiMix approach through extensive experiments conducted on traces produced by GTMobiSim on different scales of geographic maps. Our experiments show that MobiMix offers high level of anonymity and high level of resilience to attacks compared to existing mix-zone approaches.

#index 1594666
#* Scientific workflow design 2.0: Demonstrating streaming data collections in Kepler
#@ Lei Dou;Daniel Zinn;Timothy McPhillips;Sven Kohler;Sean Riddle;Shawn Bowers;Bertram Ludascher
#t 2011
#c 17
#! Scientific workflow systems are used to integrate existing software components (actors) into larger analysis pipelines to perform in silico experiments. Current approaches for handling data in nested-collection structures, as required in many scientific domains, lead to many record-management actors (shims) that make the workflow structure overly complex, and as a consequence hard to construct, evolve and maintain. By constructing and executing workflows from bioinformatics and geosciences in the Kepler system, we will demonstrate how COMAD (Collection-Oriented Modeling and Design), an extension of conventional workflow design, addresses these shortcomings. In particular, COMAD provides a hierarchical data stream model (as in XML) and a novel declarative configuration language for actors that functions as a middleware layer between the workflow's data model (streaming nested collections) and the actor's data model (base data and lists thereof). Our approach allows actor developers to focus on the internal actor processing logic oblivious to the workflow structure. Actors can then be re-used in various workflows simply by adapting actor configurations. Due to streaming nested collections and declarative configurations, COMAD workflows can usually be realized as linear data processing pipelines, which often reflect the scientific data analysis intention better than conventional designs. This linear structure not only simplifies actor insertions and deletions (workflow evolution), but also decreases the overall complexity of the workflow, reducing future effort in maintenance.

#index 1594667
#* AMC - A framework for modelling and comparing matching systems as matching processes
#@ Eric Peukert;Julian Eberius;Erhard Rahm
#t 2011
#c 17
#! We present the Auto Mapping Core (AMC), a new framework that supports fast construction and tuning of schema matching approaches for specific domains such as ontology alignment, model matching or database-schema matching. Distinctive features of our framework are new visualisation techniques for modelling matching processes, stepwise tuning of parameters, intermediate result analysis and performance-oriented rewrites. Furthermore, existing matchers can be plugged into the framework to comparatively evaluate them in a common environment. This allows deeper analysis of behaviour and shortcomings in existing complex matching systems.

#index 1594668
#* Using Markov Chain Monte Carlo to play Trivia
#@ Daniel Deutch;Ohad Greenshpan;Boris Kostenko;Tova Milo
#t 2011
#c 17
#! We introduce in this Demonstration a system called Trivia Masster that generates a very large Database of facts in a variety of topics, and uses it for question answering. The facts are collected from human users (the "crowd"); the system motivates users to contribute to the Database by using a Trivia Game, where users gain points based on their contribution. A key challenge here is to provide a suitable Data Cleaning mechanism that allows to identify which of the facts (answers to Trivia questions) submitted by users are indeed correct / reliable, and consequently how many points to grant users, how to answer questions based on the collected data, and which questions to present to the Trivia players, in order to improve the data quality. As no existing single Data Cleaning technique provides a satisfactory solution to this challenge, we propose here a novel approach, based on a declarative framework for defining recursive and probabilistic Data Cleaning rules. Our solution employs an algorithm that is based on Markov Chain Monte Carlo Algorithms.

#index 1594669
#* DiRec: Diversified recommendations for semantic-less Collaborative Filtering
#@ Rubi Boim;Tova Milo;Slava Novgorodov
#t 2011
#c 17
#! In this demo we present DiRec, a plug-in that allows Collaborative Filtering (CF) Recommender systems to diversify the recommendations that they present to users. DiRec estimates items diversity by comparing the rankings that different users gave to the items, thereby enabling diversification even in common scenarios where no semantic information on the items is available. Items are clustered based on a novel notion of priority-medoids that provides a natural balance between the need to present highly ranked items vs. highly diverse ones. We demonstrate the operation of DiRec in the context of a movie recommendation system. We show the advantage of recommendation diversification and its feasibility even in the absence of semantic information.

#index 1594670
#* SkyEngine: Efficient Skyline search engine for Continuous Skyline computations
#@ Yu-Ling Hsueh;Roger Zimmermann;Wei-Shinn Ku;Yifan Jin
#t 2011
#c 17
#! Skyline query processing has become an important feature in multi-dimensional, data-intensive applications. Such computations are especially challenging under dynamic conditions, when either snapshot queries need to be answered with short user response times or when continuous skyline queries need to be maintained efficiently over a set of objects that are frequently updated. To achieve high performance, we have recently designed the ESC algorithm, an Efficient update approach for Skyline Computations. ESC creates a pre-computed candidate skyline set behind the first skyline (a "second line of defense," so to speak) that facilitates an incremental, two-stage skyline update strategy which results in a quicker query response time for the user. Our demonstration presents the two-threaded SkyEngine system that builds upon and extends the base-features of the ESC algorithm with innovative, user-oriented functionalities that are termed SkyAlert and AutoAdjust. These functions enable a data or service provider to be informed about and gain the opportunity of automatically promoting its data records to remain part of the skyline, if so desired. The SkyEngine demonstration includes both a server and a web browser based client. Finally, the SkyEngine system also provides visualizations that reveal its internal performance statistics.

#index 1594671
#* General secure sensor aggregation in the presence of malicious nodes
#@ Keith B. Frikken;Kyle Kauffman;Aaron Steele
#t 2011
#c 17
#! Sensor networks have the potential to allow users to "query the physical world" [1] by querying the sensor nodes for an aggregate result. However, a concern with such aggregation is that a few corrupt nodes in the network may manipulate the results that the querier sees. There has been a substantial amount of work on providing integrity for sensor network computations. However to the best of our knowledge, this prior work has one or more of the following two limitations: i) the methods only work for a specific aggregation function, or ii) the methods do not consider adversaries whose goal is to prevent the base station and the querier from receiving the result. In this paper we present the first scheme that provides general aggregation for sensor networks in the presence of malicious adversaries. The generality of the scheme results from the ability to securely evaluate any algorithm in the streaming model of computation. The main idea of this paper is to convert the commonly used aggregation tree into an aggregation list, and a process for doing this conversion is also presented. This result is an interesting and important first step towards achieving the realization of general secure querying of a sensor network.

#index 1594672
#* Secure and efficient in-network processing of exact SUM queries
#@ Stavros Papadopoulos;Aggelos Kiayias;Dimitris Papadias
#t 2011
#c 17
#! In-network aggregation is a popular methodology adopted in wireless sensor networks, which reduces the energy expenditure in processing aggregate queries (such as SUM, MAX, etc.) over the sensor readings. Recently, research has focused on secure in-network aggregation, motivated (i) by the fact that the sensors are usually deployed in open and unsafe environments, and (ii) by new trends such as outsourcing, where the aggregation process is delegated to an untrustworthy service. This new paradigm necessitates the following key security properties: data confidentiality, integrity, authentication, and freshness. The majority of the existing work on the topic is either unsuitable for large-scale sensor networks, or provides only approximate answers for SUM queries (as well as their derivatives, e.g., COUNT, AVG, etc). Moreover, there is currently no approach offering both confidentiality and integrity at the same time. Towards this end, we propose a novel and efficient scheme called SIES. SIES is the first solution that supports Secure In-network processing of Exact SUM queries, satisfying all security properties. It achieves this goal through a combination of homomorphic encryption and secret sharing. Furthermore, SIES is lightweight (it relies on inexpensive hash operations and modular additions/multiplications), and features a very small bandwidth consumption (in the order of a few bytes). Consequently, SIES constitutes an ideal method for resource-constrained sensors.

#index 1594673
#* Preventing equivalence attacks in updated, anonymized data
#@ Yeye He;Siddharth Barman;Jeffrey F. Naughton
#t 2011
#c 17
#! In comparison to the extensive body of existing work considering publish-once, static anonymization, dynamic anonymization is less well studied. Previous work, most notably m-invariance, has made considerable progress in devising a scheme that attempts to prevent individual records from being associated with too few sensitive values. We show, however, that in the presence of updates, even an m-invariant table can be exploited by a new type of attack we call the "equivalence-attack." To deal with the equivalence attack, we propose a graph-based anonymization algorithm that leverages solutions to the classic "min-cut/max-flow" problem, and demonstrate with experiments that our algorithm is efficient and effective in preventing equivalence attacks.

#index 1594674
#* Efficient continuously moving top-k spatial keyword query processing
#@ Dingming Wu;Man Lung Yiu;Christian S. Jensen;Gao Cong
#t 2011
#c 17
#! Web users and content are increasingly being geo-positioned. This development gives prominence to spatial keyword queries, which involve both the locations and textual descriptions of content.

#index 1594675
#* Large scale Hamming distance query processing
#@ Alex X. Liu;Ke Shen;Eric Torng
#t 2011
#c 17
#! Hamming distance has been widely used in many application domains, such as near-duplicate detection and pattern recognition. We study Hamming distance range query problems, where the goal is to find all strings in a database that are within a Hamming distance bound k from a query string. If k is fixed, we have a static Hamming distance range query problem. If k is part of the input, we have a dynamic Hamming distance range query problem. For the static problem, the prior art uses lots of memory due to its aggressive replication of the database. For the dynamic range query problem, as far as we know, there is no space and time efficient solution for arbitrary databases. In this paper, we first propose a static Hamming distance range query algorithm called HEngines, which addresses the space issue in prior art by dynamically expanding the query on the fly. We then propose a dynamic Hamming distance range query algorithm called HEngined, which addresses the limitation in prior art using a divide-and-conquer strategy. We implemented our algorithms and conducted side-by-side comparisons on large real-world and synthetic datasets. In our experiments, HEngines uses 4.65 times less space and processes queries 16% faster than the prior art, and HEngined processes queries 46 times faster than linear scan while using only 1.7 times more space.

#index 1594676
#* Authentication of moving kNN queries
#@ Man Lung Yiu;Eric Lo;Duncan Yung
#t 2011
#c 17
#! A moving kNN query continuously reports the k nearest neighbors of a moving query point. In addition to the query result, a service provider that evaluates moving queries often returns mobile clients a safe region that bounds the validity of query results to minimize the communication cost between the two parties. However, when a service provider is not trustworthy, it may send inaccurate query results or incorrect safe regions to clients. In this paper, we present a framework and algorithms to authenticate results and safe regions of moving kNN queries. Extensive experiments on both real and synthetic datasets show that our methods are efficient in terms of both computation time and communication costs.

#index 1594677
#* Updating XML schemas and associated documents through exup
#@ Federico Cavalieri;Giovanna Guerrini;Marco Mesiti
#t 2011
#c 17
#! Data on the Web mostly are in XML format and the need often arises to update their structure, commonly described by an XML Schema. When a schema is modified the effects of the modification on documents need to be faced. XSUpdate is a language that allows to easily identify parts of an XML Schema, apply a modification primitive on them and finally define an adaptation for associated documents, while Eχup is the corresponding engine for processing schema modification and document adaptation statements. Purpose of this demonstration is to provide an overview of the facilities of the XSUpdate language and of the Eχup system.

#index 1594678
#* Ruby on semantic web
#@ Vadim Eisenberg;Yaron Kanza
#t 2011
#c 17
#! The impedance mismatch problem that occurs when relational data is being processed by object-oriented (OO) programs, also occurs when OO programs process RDF data, on the Semantic Web. The impedance mismatch problem stems from the inherent differences between RDF and the data model of OO languages. In this paper, we illustrate a solution to this problem. Essentially, we modify an OO language so that RDF individuals become first-class citizens in the language, and objects of the language become first-class citizens in RDF. Three important benefits that follow from this modification are: (1) it becomes natural to use the language as a persistent programming language, (2) the language supports implicit integration of data from multiple data sources, and (3) SPARQL queries and inference can be applied to objects during the run of a program. This demo presents such a modified programming language, namely Ruby on Semantic Web, which is an extension of the Ruby programming language. The demo includes a system, where users can run applications, written in Ruby on Semantic Web, over multiple data sources. In the demo we run code examples. The effects of the execution on the data sources and on the state of the objects in memory are presented visually, in real time.

#index 1594679
#* Preference-based datacube analysis with MYOLAP
#@ Paolo Biondi;Matteo Golfarelli;Stefano Rizzi
#t 2011
#c 17
#! In this demonstration we present MYOLAP, a Java-based tool that allows OLAP analyses to be personalized and enhanced by expressing "soft" query constraints in the form of user preferences. MYOLAP is based on a novel preference algebra and a preference evaluation algorithm specifically devised for the OLAP domain. Preferences are formulated either visually or through an extension of the MDX language, and user interaction with the results is mediated by a visual graph-like structure that shows better-than relationships between different sets of data. The demonstration will show how analysis sessions can benefit from coupling ad-hoc preference constructors with the classical OLAP operators, and in particular how MYOLAP supports users in expressing preference queries, analyzing their results, and navigating datacubes.

#index 1594680
#* Continuous, online monitoring and analysis in large water distribution networks
#@ Xiuli Ma;Hongmei Xiao;Shuiyuan Xie;Qiong Li;Qiong Luo;Chunhua Tian
#t 2011
#c 17
#! Clean drinking water and safe water supply is vital to our life. Recent advances in technologies have made it possible to deploy smart sensor networks in large water distribution networks to monitor and identify the water quality online. In such a large-scale real-time monitoring application, large amounts of data stream out of multiple concurrent sensors continuously. In this paper, we present a system to monitor and analyze the sensor data streams online, find and summarize the spatio-temporal distribution patterns and correlations in co-evolving data, detect contamination events rapidly and facilitate corrective actions or notification. The system consists of an online data mining engine and a GUI providing the user with the current patterns discovered in the network, and an alerter notifying the user if there is anomalous water quality in the network.

#index 1594681
#* Integrating code search into the development session
#@ Mu-Woong Lee;Seung-won Hwang;Sunghun Kim
#t 2011
#c 17
#! To support rapid and efficient software development, we propose to demonstrate our tool, integrating code search into software development process. For example, a developer, right during writing a module, can find a code piece sharing the same syntactic structure from a large code corpus representing the wisdom of other developers in the same team (or in the universe of open-source code). While there exist commercial code search engines on the code universe, they treat software as text (thus oblivious of syntactic structure), and fail at finding semantically related code. Meanwhile, existing tools, searching for syntactic clones, do not focus on efficiency, focusing on "post-mortem" usage scenario of detecting clones "after" the code development is completed. In clear contrast, we focus on optimizing efficiency for syntactic code search and making this search "interactive" for large-scale corpus, to complement the existing two lines of research. From our demonstration, we will show how such interactive search supports rapid software development, as similarly claimed lately in SE and HCI communities [1], [2]. As an enabling technology, we design efficient index building and traversal techniques, optimized for code corpus and code search workload. Our tool can identify relevant code in the corpus of 1.7 million code pieces in a sub-second response time, without compromising any accuracy obtained by a state-of-the-art tool, as we report our extensive evaluation results in [3].

#index 1594682
#* The Proactive Promotion Engine
#@ Karen Works;Elke A. Rundensteiner
#t 2011
#c 17
#! Given the nature of high volume streaming environments, not all tuples can be processed within the required response time. In such instances, it is crucial to dedicate resources to producing the most important results. We will demonstrate the Proactive Promotion Engine (PP) which employs a new preferential resource allocation methodology for priority processing of stream tuples. Our key contributions include: 1) our promotion continuous query language allows the specification of priorities within a query, 2) our promotion query algebra supports proactive promotion query processing, 3) our promotion query optimization locates an optimized PP query plan, and 4) our adaptive promotion control adapts online which subset of tuples are given priority online within a single physical query plan. Our "Portland Home Arrest" demonstration facilitates the capture of in-flight criminals using data generated by the Virginia Tech Network Dynamics and Simulation Science Laboratory via simulation-based modeling techniques.

#index 1594683
#* Influence zone: Efficiently processing reverse k nearest neighbors queries
#@ Muhammad Aamir Cheema;Xuemin Lin;Wenjie Zhang;Ying Zhang
#t 2011
#c 17
#! Given a set of objects and a query q, a point p is called the reverse k nearest neighbor (RkNN) of q if q is one of the k closest objects of p. In this paper, we introduce the concept of influence zone which is the area such that every point inside this area is the RkNN of q and every point outside this area is not the RkNN. The influence zone has several applications in location based services, marketing and decision support systems. It can also be used to efficiently process RkNN queries. First, we present efficient algorithm to compute the influence zone. Then, based on the influence zone, we present efficient algorithms to process RkNN queries that significantly outperform existing best known techniques for both the snapshot and continuous RkNN queries. We also present a detailed theoretical analysis to analyse the area of the influence zone and IO costs of our RkNN processing algorithms. Our experiments demonstrate the accuracy of our theoretical analysis.

#index 1594684
#* RAFTing MapReduce: Fast recovery on the RAFT
#@ Jorge-Arnulfo Quiane-Ruiz;Christoph Pinkel;Jorg Schad;Jens Dittrich
#t 2011
#c 17
#! MapReduce is a computing paradigm that has gained a lot of popularity as it allows non-expert users to easily run complex analytical tasks at very large-scale. At such scale, task and node failures are no longer an exception but rather a characteristic of large-scale systems. This makes fault-tolerance a critical issue for the efficient operation of any application. MapReduce automatically reschedules failed tasks to available nodes, which in turn recompute such tasks from scratch. However, this policy can significantly decrease performance of applications. In this paper, we propose a family of Recovery Algorithms for Fast-Tracking (RAFT) MapReduce. As ease-of-use is a major feature of MapReduce, RAFT focuses on simplicity and also non-intrusiveness, in order to be implementation-independent. To efficiently recover from task failures, RAFT exploits the fact that MapReduce produces and persists intermediate results at several points in time. RAFT piggy-backs checkpoints on the task progress computation. To deal with multiple node failures, we propose query metadata checkpointing. We keep track of the mapping between input key-value pairs and intermediate data for all reduce tasks. Thereby, RAFT does not need to re-execute completed map tasks entirely. Instead RAFT only recomputes intermediate data that were processed for local reduce tasks and hence not shipped to another node for processing. We also introduce a scheduling strategy taking full advantage of these recovery algorithms. We implemented RAFT on top of Hadoop and evaluated it on a 45-node cluster using three common analytical tasks. Overall, our experimental results demonstrate that RAFT outperforms Hadoop runtimes by 23% on average under task and node failures. The results also show that RAFT has negligible runtime overhead.

#index 1594685
#* Processing private queries over untrusted data cloud through privacy homomorphism
#@ Haibo Hu;Jianliang Xu;Chushi Ren;Byron Choi
#t 2011
#c 17
#! Query processing that preserves both the data privacy of the owner and the query privacy of the client is a new research problem. It shows increasing importance as cloud computing drives more businesses to outsource their data and querying services. However, most existing studies, including those on data outsourcing, address the data privacy and query privacy separately and cannot be applied to this problem. In this paper, we propose a holistic and efficient solution that comprises a secure traversal framework and an encryption scheme based on privacy homomorphism. The framework is scalable to large datasets by leveraging an index-based approach. Based on this framework, we devise secure protocols for processing typical queries such as k-nearest-neighbor queries (kNN) on R-tree index. Moreover, several optimization techniques are presented to improve the efficiency of the query processing protocols. Our solution is verified by both theoretical analysis and performance study.

#index 1594686
#* Real-time quantification and classification of consistency anomalies in multi-tier architectures
#@ Kamal Zellag;Bettina Kemme
#t 2011
#c 17
#! While online transaction processing applications heavily rely on the transactional properties provided by the underlying infrastructure, they often choose to not use the highest isolation level, i.e., serializability, because of the potential performance implications of costly strict two-phase locking concurrency control. Instead, modern transaction systems, consisting of an application server tier and a database tier, offer several levels of isolation providing a trade-off between performance and consistency. While it is fairly well known how to identify the anomalies that are possible under a certain level of isolation, it is much more difficult to quantify the amount of anomalies that occur during run-time of a given application. In this paper, we address this issue and present a new approach to detect, in realtime, consistency anomalies for arbitrary multi-tier applications. As the application is running, our tool detect anomalies online indicating exactly the transactions and data items involved. Furthermore, we classify the detected anomalies into patterns showing the business methods involved as well as their occurrence frequency. We use the RUBiS benchmark to show how the introduction of a new transaction type can have a dramatic effect on the number of anomalies for certain isolation levels, and how our tool can quickly detect such problem transactions. Therefore, our system can help designers to either choose an isolation level where the anomalies do not occur or to change the transaction design to avoid the anomalies.

#index 1594687
#* One-copy serializability with snapshot isolation under the hood
#@ Mihaela A. Bornea;Orion Hodson;Sameh Elnikety;Alan Fekete
#t 2011
#c 17
#! This paper presents a method that allows a replicated database system to provide a global isolation level stronger than the isolation level provided on each individual database replica. We propose a new multi-version concurrency control algorithm called, serializable generalized snapshot isolation (SGSI), that targets middleware replicated database systems. Each replica runs snapshot isolation locally and the replication middleware guarantees global one-copy serializability. We introduce novel techniques to provide a stronger global isolation level, namely readset extraction and enhanced certification that prevents read-write and write-write conflicts in a replicated setting. We prove the correctness of the proposed algorithm, and build a prototype replicated database system to evaluate SGSI performance experimentally. Extensive experiments with an 8 replica database system under the TPC-W workload mixes demonstrate the practicality and low overhead of the algorithm.

#index 1594688
#* NORMS: An automatic tool to perform schema label normalization
#@ Serena Sorrentino;Sonia Bergamaschi;Maciej Gawinecki
#t 2011
#c 17
#! Schema matching is the problem of finding relationships among concepts across heterogeneous data sources (heterogeneous in format and structure). Schema matching systems usually exploit lexical and semantic information provided by lexical databases/thesauri to discover intra/inter semantic relationships among schema elements. However, most of them obtain poor performance on real world scenarios due to the significant presence of "non-dictionary words". Non-dictionary words include compound nouns, abbreviations and acronyms. In this paper, we present NORMS (NORMalizer of Schemata), a tool performing schema label normalization to increase the number of comparable labels extracted from schemata1.

#index 1594689
#* GuideMe! The World of sights in your pocket
#@ Sergej Zerr;Kerstin Bischoff;Sergey Chernov
#t 2011
#c 17
#! Web 2.0 applications are a rich source of multimedia resources, that describe sights, events, whether conditions, traffic situations and other relevant objects along the user's route. Compared to static sight descriptions, Web 2.0 resources can provide up-to-date visual information, which has been found important or interesting by the other users. Some algorithms have been suggested recently for the landmark finding problem from photos. Still, if users want related videos or background information about a particular place of interest it is necessary to contact different social platforms or general search engines. In this paper we present GuideMe! - a mobile application that automatically identifies landmark tags from Flickr groups and gathers relevant sightseeing resources from various Web 2.0 social platforms.

#index 1594690
#* CompRec-Trip: A composite recommendation system for travel planning
#@ Min Xie;Laks V. S. Lakshmanan;Peter T. Wood
#t 2011
#c 17
#! Classical recommender systems provide users with a list of recommendations where each recommendation consists of a single item, e.g., a book or a DVD. However, applications such as travel planning can benefit from a system capable of recommending packages of items, under a user-specified budget and in the form of sets or sequences. In this context, there is a need for a system that can recommend top-k packages for the user to choose from. In this paper, we propose a novel system, CompRec-Trip, which can automatically generate composite recommendations for travel planning. The system leverages rating information from underlying recommender systems, allows flexible package configuration and incorporates users' cost budgets on both time and money. Furthermore, the proposed CompRec-Trip system has a rich graphical user interface which allows users to customize the returned composite recommendations and take into account external local information.

#index 1594691
#* Advanced search, visualization and tagging of sensor metadata
#@ Ioannis Paparrizos;Hoyoung Jeung;Karl Aberer
#t 2011
#c 17
#! As sensors continue to proliferate, the capabilities of effectively querying not only sensor data but also its metadata becomes important in a wide range of applications. This paper demonstrates a search system that utilizes various techniques and tools for querying sensor metadata and visualizing the results. Our system provides an easy-to-use query interface, built upon semantic technologies where users can freely store and query their metadata. Going beyond basic keyword search, the system provides a variety of advanced functionalities tailored for sensor metadata search; ordering search results according to our ranking mechanism based on the PageRank algorithm, recommending pages that contain relevant metadata information to given search conditions, presenting search results using various visualization tools, and offering dynamic hypergraphs and tag clouds of metadata. The system has been running as a real application and its effectiveness has been proved by a number of users.

#index 1594692
#* Distributed data management in 2020?
#@ M. Tamer Ozsu;Patrick Valduriez;Serge Abiteboul;Bettina Kemme;Ricardo Jimenez-Peris;Beng Chin Ooi
#t 2011
#c 17
#! Work on distributed data management commenced shortly after the introduction of the relational model in the mid-1970's. 1970's and 1980's were very active periods for the development of distributed relational database technology, and claims were made that in the following ten years centralized databases will be an "antique curiosity" and most organizations will move toward distributed database managers [1]. That prediction has certainly become true, and all commercial DBMSs today are distributed.

#index 1594693
#* Robust query processing
#@ Goetz Graefe
#t 2011
#c 17
#! In the context of data management, robustness is usually associated with resilience against failure, recovery, redundancy, disaster preparedness, etc. Robust query processing, on the other hand, is about robustness of performance and of scalability. It is more than progress reporting or predictability. A system that fails predictably or obviously performs poorly may be better than an unpredictable one, but it is not robust.

#index 1594694
#* Non-metric similarity search problems in very large collections
#@ Benjamin Bustos;Tomas Skopal
#t 2011
#c 17
#! This tutorial surveys domains employing non-metric functions for effective similarity search, and methods for efficient non-metric similarity search in very large collections.

#index 1594695
#* Next generation data integration for Life Sciences
#@ Sarah Cohen-Boulakia;Ulf Leser
#t 2011
#c 17
#! Ever since the advent of high-throughput biology (e.g., the Human Genome Project), integrating the large number of diverse biological data sets has been considered as one of the most important tasks for advancement in the biological sciences. Whereas the early days of research in this area were dominated by virtual integration systems (such as multi-/federated databases), the current predominantly used architecture uses materialization. Systems are built using ad-hoc techniques and a large amount of scripting. However, recent years have seen a shift in the understanding of what a "data integration system" actually should do, revitalizing research in this direction. In this tutorial, we review the past and current state of data integration for the Life Sciences and discuss recent trends in detail, which all pose challenges for the database community.

#index 1732984
#* Proceedings of the Second international conference on Data Engineering Issues in E-Commerce and Services
#@ Juhnyoung Lee;Junho Shim;Sang-goo Lee;Christoph Bussler;Simon Shim
#t 2006
#c 17

#index 1732985
#* An approach to detecting shill-biddable allocations in combinatorial auctions
#@ Tokuro Matsuo;Takayuki Ito;Toramatsu Shintani
#t 2006
#c 17
#% 378898
#% 413867
#% 529496
#% 912341
#% 1273807
#% 1273808
#% 1289297
#% 1289298
#! This paper presents a method for discovering and detecting shill bids in combinatorial auctions. Combinatorial auctions have been studied very widely. The Generalized Vickrey Auction (GVA) is one of the most important combinatorial auctions because it can satisfy the strategy-proof property and Pareto efficiency. As Yokoo et al. pointed out, false-name bids and shill bids pose an emerging problem for auctions, since on the Internet it is easy to establish different e-mail addresses and accounts for auction sites. Yokoo et al. proved that GVA cannot satisfy the false-name-proof property. Moreover, they proved that there is no auction protocol that can satisfy all three of the above major properties. Their approach concentrates on designing new mechanisms. As a new approach against shill-bids, in this paper, we propose a method for finding shill bids with the GVA in order to avoid them. Our algorithm can judge whether there might be a shill bid from the results of the GVA's procedure. However, a straightforward way to detect shill bids requires an exponential amount of computing power because we need to check all possible combinations of bidders. Therefore, in this paper we propose an improved method for finding a shill bidder. The method is based on winning bidders, which can dramatically reduce the computational cost. The results demonstrate that the proposed method successfully reduces the computational cost needed to find shill bids. The contribution of our work is in the integration of the theory and detecting fraud in combinatorial auctions.

#index 1732986
#* Explanation services and request refinement in user friendly semantic-enabled b2c e-marketplaces
#@ Simona Colucci;Tommaso Di Noia;Eugenio Di Sciascio;Francesco M. Donini;Azzurra Ragone;Raffaele Rizzi
#t 2006
#c 17
#% 244095
#% 348132
#% 431557
#% 442977
#% 519428
#% 577334
#% 577335
#% 728317
#% 836321
#% 935898
#% 987501
#% 1279261
#% 1343862
#% 1705177
#! This paper presents an approach aimed at fully exploiting semantics of supply/demand descriptions in B2C and C2C e-marketplaces. Distinguishing aspects include logic-based explanation of request results, semantic ranking of matchmaking results, logic-based request refinement. The user interface has been designed and implemented to be immediate and simple, and it requires no knowledge of any logic principle to be fully used.

#index 1732987
#* Customer future profitability assessment: a data-driven segmentation function approach
#@ Chunhua Tian;Wei Ding;Rongzeng Cao;Michelle Wang
#t 2006
#c 17
#% 252399
#! One of the important tasks in customer relationship management is to find out the future profitability of individual and/or groups of customers. Data mining-based approaches only provide coarse-grained customer segmentation. It is also hard to obtain a high-precision structure model purely by using regression methods. This paper proposes a data-driven segmentation function that provides a precise regression model on top of the segmentation from a data mining approach. For a new customer, a structure model constructed from profit contribution data of current customers is adopted to assess the profitability. For an existing customer, external information such as stock value performance is taken into the regression model as well as historical trend prediction on the profit contribution. In addition, this paper shows how the proposed approach works and how it improves the customer profitability analysis through experiments on the sample data.

#index 1732988
#* Optimization of automatic navigation to hidden web pages by ranking-based browser preloading
#@ Justo Hidalgo;José Losada;Manuel Álvarez;Alberto Pan
#t 2006
#c 17
#% 116303
#% 397605
#% 480479
#% 525667
#% 654469
#% 843166
#% 994016
#! Web applications have become an invaluable source of information for many different vertical solutions, but their complex navigation and semistructured format make their information difficult to retrieve. Web Automation and Extraction systems are able to navigate through web links and to fill web forms automatically in order to get information not directly accessible by a URL. In these systems, the main optimization parameter is the time required to navigate through the intermediate pages which lead to the desired final pages. This paper proposes a series of techniques and algorithms that improves this parameter by basically storing historical information from previous queries, and using it to make the browser manager preload an adequate subset of the whole navigational sequence on a specific browser, before the following query is executed. These techniques also handle which sequences are the most common, thus being the ones which are preloaded more often.

#index 1732989
#* Transforming collaborative business process models into web services choreography specifications
#@ Pablo David Villarreal;Enrique Salomone;Omar Chiotti
#t 2006
#c 17
#% 721513
#% 1671722
#% 1739926
#! Languages for web services choreography are becoming more and more important for B2B integration. However, the development of web services-based systems is complex and time-consuming. Enterprises have to agree on collaborative business processes and then derive their respective web services choreographies in order to implement B2B collaboration. To support it, this paper presents a MDA approach for collaborative processes. We describe the components and techniques of this approach. We show how collaborative process models defined with the UP-ColBPIP language can be used as the main development artifact in order to derive choreography specifications based on WS-CDL. The transformations to be carried out are also discussed. The main advantage of this MDA approach is that it guarantees that the generated web services choreographies fulfill the collaborative processes agreed between the partners in a business level.

#index 1732990
#* Evaluation of IT portfolio options by linking to business services
#@ Vijay Iyengar;David Flaxer;Anil Nigam;John Vergo
#t 2006
#c 17
#% 239338
#% 280413
#% 301185
#% 407822
#% 425136
#% 578481
#% 734060
#% 861838
#% 911976
#! The management of IT portfolios in most enterprises is a complex and challenging ongoing process. IT portfolios typically contain large numbers of inter-related elements. In this paper, we present a model and a method for determining and evaluating IT portfolio options for use as a decision aid in the ongoing management of the portfolio. Business benefits of the portfolio options are articulated by linking IT portfolio elements to componentized business services. Characteristics of the IT portfolio elements are abstracted to a level of granularity suited for portfolio analysis. Our model allows various forms of uncertainty that are utilized in the evaluation. Our evaluation method determines a set of portfolio options with the associated cost/benefits tradeoffs. Business constraints and pruning methods are used to present only the relevant and available options and their tradeoffs to the IT portfolio manager.

#index 1732991
#* Process driven data access component generation
#@ Guanqun Zhang;Xianghua Fu;Shenli Song;Ming Zhu;Ming Zhang
#t 2006
#c 17
#% 230803
#% 409113
#% 654137
#% 775880
#% 778139
#% 789507
#% 807686
#% 813052
#! Process and data are two key perspectives of an SOA solution. They are usually designed relatively independently by different roles with different tools, and then linked together during the implementation phase to produce a runnable solution. It follows the separation of concerns principle to reduce development complexity, but it results in an integration gap for data access in processes, including both functional and non-functional aspects. Currently the gap is manually bridged, so that the development quality and efficiency highly depend on developers' capability. This paper proposes a novel approach to automatically bridge the gap by generating data access components whose granularity and performance are optimized according to process models. Firstly we build a platform independent process data relationship model (PDRM) based on process and data models, and then generate data access components with proper granularity by analyzing the PDRM. Furthermore, indexing technology is applied to optimize performance of data access components.

#index 1732992
#* Using naming tendencies to syntactically link web service messages
#@ Michael F. Nowlan;Daniel R. Kahan;M. Brian Blake
#t 2006
#c 17
#% 445446
#% 728757
#% 732633
#% 796913
#% 806637
#% 813984
#% 831915
#% 831964
#% 838104
#% 864418
#% 1722487
#! Service-oriented computing (SOC) enables organizations and individual users to discover openly-accessible capabilities realized as services over the Internet. An important issue is the management of the messages that flow into and out of these services to ultimately compose higher-level functions. A significant problem occurs when service providers loosely define these messages resulting into many services that in effect cannot be easily integrated. State of the art research explores semantic methods for dealing with this notion of data integration. The assumption is that service providers will define messages in an unpredictable manner. In our work, we investigate the nature of message definitions by analyzing real, fully-operational web services currently available on the Internet (i.e. from the wild). As a result, we have discovered insights into how real web services messages are defined as affected by the tendencies of the web services developers. Using these insights we propose an enhanced syntactical method that can facilitate semantic processing by classifying web services by their message names as a first step.

#index 1732993
#* Maintaining web navigation flows for wrappers
#@ Juan Raposo;Manuel Álvarez;José Losada;Alberto Pan
#t 2006
#c 17
#% 283053
#% 309793
#% 312860
#% 397605
#% 480479
#% 525667
#% 654469
#% 729873
#% 779898
#% 805846
#% 832384
#% 839153
#% 1271981
#! A substantial subset of the web data follows some kind of underlying structure. In order to let software programs gain full benefit from these “semi-structured” web sources, wrapper programs are built to provide a “machine-readable” view over them. A significant problem with wrappers is that, since web sources are autonomous, they may experience changes that invalidate the current wrapper, so automatic maintenance is an important research issue. Web wrappers must perform two kinds of tasks: automatically navigating through websites and automatically extracting structured data from HTML pages. While several previous works have addressed the automatic maintenance of the components performing the data extraction task, the problem of automatically maintaining the required web navigation sequences remains unaddressed to the best of our knowledge. In this paper we propose and expirementally validate a set of novel heuristics and algorithms to fill this gap.

#index 1732994
#* Mobile p2p automatic content sharing by ontology-based and contextualized integrative negotiation
#@ Soe-Tysr Yuan;Mei-Ling Yeh
#t 2006
#c 17
#% 124691
#% 162305
#% 263126
#% 302000
#% 321379
#% 342124
#% 378911
#% 391644
#% 417642
#% 438438
#% 614029
#% 773271
#! The goal of enterprise computing is to achieve efficient enterprise automation and to optimize enterprise value through an understanding of external and internal status for business optimization. With the advancement in wireless technology, mobile communication has become a salient interaction media between people. Particularly, with the ad-hoc mobile networks, each peer is able to attain information and services (from the other peers) of value to the peer anywhere, any time and using a variety of different kinds of devices. However, how to empower peers to share and acquire information/services through an automated negotiation mechanism, to take into account the attributes of the context (physical and non-physical) and then to achieve social welfare is one of the ideals in exploiting a wireless Peer-to-Peer network environment in the era of enterprise computing. Accordingly, this paper presents Contextualized Integrative Negotiation Strategies (CINS) for content sharing in wireless Peer-to-Peer environments. CINS enables peers to engage proper interactions within the environment in time, to understand the needs of the other peers and to employ practices of cooperation negotiation (instead of competition ones). In CINS, peer re-learning negotiation strategies consider content-sharing ontologies employed and the reward attained from the environment for the purpose of useful sharing experience. This not only helps peers embrace a common consensus of collective benefits in content sharing but also guarantees that the negotiation results achieve win-win decisions of allocations.

#index 1732995
#* Integrating XML sources into a data warehouse
#@ Boris Vrdoljak;Marko Banek;Zoran Skočir
#t 2006
#c 17
#% 348637
#% 393641
#% 430753
#% 479956
#% 504154
#% 596210
#% 659924
#% 831111
#% 1698952
#! Since XML has become a standard for data exchange over the Internet, especially in B2B and B2C communication, there is an increasing need of integrating XML data into data warehousing systems. In this paper we propose a methodology for data warehouse design, when data sources are XML Schemas and conforming XML documents. Particular relevance is given to the conceptual and logical multidimensional design. A prototype tool has been developed to verify and support our methodology. Because of the semi-structured nature of XML data, not all the information needed for design can be safely derived from XML Schema. In these situations, XQuery statements are generated by the tool to examine XML documents. The functionality of the tool is explained on a real-life XML Schema that describes purchase orders.

#index 1732996
#* Remote-Specific XML query mobile agents
#@ Myung Sook Kim;Yong Hae Kong;Chang Wan Jeon
#t 2006
#c 17
#% 331774
#% 443376
#% 557050
#% 1699344
#! An efficient XML query method is suggested for a distributed web environment. We develop an algorithm that promptly matches host's ontology and remote site's DTD. Then, a mobile agent system is constructed such that each agent maintains the recently matched ontology. For XML query, each agent generates remote-specific queries based on its matched ontology and returns the search results back to the host. The prompt match, remote-specific query, and autonomous mobile process can greatly reduce the search time and transmission overhead.

#index 1732997
#* A process history capture system for analysis of data dependencies in concurrent process execution
#@ Yang Xiao;Susan D. Urban;Suzanne W. Dietrich
#t 2006
#c 17
#% 32897
#% 122904
#% 122911
#% 384640
#% 464836
#% 565274
#% 578395
#% 778940
#% 822359
#% 829811
#% 836311
#% 843196
#% 1396074
#! This paper presents a Process History Capture System (PHCS) as a logging mechanism for distributed long running business processes executing over Delta-Enabled Grid Services (DEGS). A DEGS is a Grid Service with an enhanced interface to access incremental data changes, known as deltas, associated with service execution in the context of global processes. The PHCS captures process execution context and deltas from distributed DEGSs and constructs a global schedule for multiple executing processes, integrating local schedules that are extracted from deltas at distributed sites. The global schedule forms the basis for analyzing data dependencies among concurrently executing processes. The schedule can be used for rollback and also to identify data dependencies that affect the possible recovery of other concurrent processes. This paper presents the design of the PHCS and the use of the PHCS for process failure recovery. We also outline future directions for specification of user-defined semantic correctness.

#index 1732998
#* Business impact analysis using time correlations
#@ Mehmet Sayal
#t 2006
#c 17
#% 172949
#% 227857
#% 232122
#% 252207
#% 252209
#% 284600
#% 460862
#% 462231
#% 477479
#% 534183
#% 539906
#% 568297
#% 632089
#% 993961
#! A novel method for analyzing time-series data and extracting time-correlations (time-dependent relationships) among multiple time-series data streams is described. The application of time-correlation detection in business impact analysis (BIA) is explained on an example. The method described in this paper is the first one that can efficiently detect and report time-dependent relationships among multiple time-series data streams. Detected time-correlation rules explain how the changes in the values of one set of time-series data streams influence the values in another set of time-series data streams. Those rules can be stored digitally and fed into various data analysis tools, such as simulation, forecasting, impact analysis, etc., for further analysis of the data. Performance experiments showed that the described method is 95% accurate, and has a linear running time with respect to the amount of input data.

#index 1732999
#* A bottom-up workflow mining approach for workflow applications analysis
#@ Walid Gaaloul;Karim Baïna;Claude Godart
#t 2006
#c 17
#% 258498
#% 259602
#% 451429
#% 458207
#% 459021
#% 487251
#% 495976
#% 589166
#% 749035
#% 749039
#% 994005
#% 1698858
#% 1709206
#! Engineering workflow applications are becoming more and more complex, involving numerous interacting business objects within considerable processes. Analysing the interaction structure of those complex applications will enable them to be well understood, controlled, and redesigned. Our contribution to workflow mining is a statistical technique to discover workflow patterns from event-based log. Our approach is characterised by a ”local” workflow patterns discovery that allows to cover partial results through a dynamic programming algorithm. Those local discovered workflow patterns are then composed iteratively until discovering the global workflow model. Our approach has been implemented within our prototype WorkflowMiner.

#index 1733000
#* BestChoice: a decision support system for supplier selection in e-marketplaces
#@ Dongjoo Lee;Taehee Lee;Sue-kyung Lee;Ok-ran Jeong;Hyeonsang Eom;Sang-goo Lee
#t 2006
#c 17
#% 726520
#% 783329
#! A growing number of companies are outsourcing their purchasing processes to independent purchasing agencies. These agencies now have to process an ever increasing number of purchase requests each day. The conventional methods of selecting the right suppliers for the purchase requests incur heavy human and time costs. We have designed and implemented BestChoice, a decision support system for supplier selection. It allows the evaluator to create rules for supplier evaluation based on the Multi Attribute Utility Theory, a theory for evaluating the utility of alternatives. BestChoice provides rule structures that can be saved and reused for similar selection cases. The architecture and selection rules of BestChoice are presented. Performance of BestChoice at one of the largest procurement agencies is analyzed.

#index 1733001
#* Semantic web services enabled b2b integration
#@ Paavo Kotinurmi;Tomas Vitvar;Armin Haller;Ray Richardson;Aidan Boran
#t 2006
#c 17
#% 397392
#% 438313
#% 449214
#% 577524
#% 655354
#% 659963
#% 665425
#% 721519
#% 831891
#% 836354
#% 1092030
#% 1655450
#! The use of Semantic Web Service (SWS) technologies have been suggested to enable more dynamic B2B integration of heterogeneous systems and partners. We present our approach to accomplish dynamic B2B integrations based on the WSMX SWS environment. We particularly show how WSMX can be made to support the RosettaNet e-business framework and how it can add dynamics to B2B interactions by automating mediation of heterogeneous messages. This is illustrated through a purchasing scenario. The benefits of applying SWS technologies include more flexibility in accepting heterogeneity in B2B integrations and easing back-end integrations. This allows for example to introduce more competition into the purchasing process within e-business frameworks.

#index 1733002
#* A novel genetic algorithm for qos-aware web services selection
#@ Chengwen Zhang;Sen Su;Junliang Chen
#t 2006
#c 17
#% 168999
#% 434060
#% 577344
#% 722478
#% 753306
#% 767419
#% 767431
#% 769355
#% 779863
#% 784408
#% 784411
#% 1776414
#! A novel genetic algorithm characterized by improved fitness value is presented for Quality of Service (QoS)-aware web services selection. The genetic algorithm includes a special relation matrix coding scheme of chromosomes, an initial population policy and a mutation policy. The relation matrix coding scheme suits with QoS-aware web service composition more than the one dimension coding scheme. By running only once, the proposed genetic algorithm can construct the composite service plan according with the QoS requirement from many services compositions. Meanwhile, the adoption of the initial population policy and the mutation policy promotes the fitness of genetic algorithm. Experiments on QoS-aware web services selection show that the genetic algorithm with this matrix can get more excellent composite service plan than the genetic algorithm with the one dimension coding scheme, and that the two policies play an important role at the improvement of the fitness of genetic algorithm.

#index 1733003
#* Analysis of web services composition and substitution via CCS
#@ Fangfang Liu;Yuliang Shi;Liang Zhang;Lili Lin;Baile Shi
#t 2006
#c 17
#% 369768
#% 528294
#% 542108
#% 577343
#% 607816
#% 722480
#% 767382
#% 776731
#% 784408
#% 786873
#% 824702
#% 1344122
#% 1561977
#% 1656076
#% 1656079
#! Web services composition is a key issue in web service research area. Substitution of service is closely related with composition and important to robustness of service composition. In this paper, we use process algebra as formalism foundation modeling and specifying web services and reasoning on behavioral features of web services composition. We analyze some cases that have effects on design and implementation of composition. Upon that, and based on definition of composition, we study substitution. As to the problem of how to substitute a component web service, we present a relation. Any new selected web services can substitute old component service independent of context and take part in composition successfully in the case that they satisfy criteria of this relation.

#index 1733004
#* Modified naïve bayes classifier for e-catalog classification
#@ Young-gon Kim;Taehee Lee;Jonghoon Chun;Sang-goo Lee
#t 2006
#c 17
#% 310556
#% 344447
#% 376266
#% 445472
#% 729941
#% 731003
#% 789963
#% 1348329
#! As the wide use of online business transactions, the volume of product information that needs to be managed in a system has become drastically large, and the classification task of such data has become highly complex. The heterogeneity among competing standard classification schemes makes the problem only harder. However, the classification task is an indispensable part for successful e-commerce applications. In this paper, we present an automated approach for e-catalog classification. We extend the Naïve Bayes Classifier to make use of the structural characteristics of e-catalogs. We show how we can improve the accuracy of classification when appropriate characteristics of e-catalogs are utilized. Effectiveness of the proposed methods is validated through experiments.

#index 1733005
#* A sentinel based exception diagnosis in market based multi-agent systems
#@ Nazaraf Shah;Kuo-Ming Chao;Nick Godwin;Anne James;Chun-Lung Huang
#t 2006
#c 17
#% 131565
#% 265798
#% 271040
#% 314942
#% 344115
#% 379037
#% 431508
#% 445083
#% 568487
#% 643161
#% 657509
#% 659837
#% 831352
#! Market based Multi-agent systems (MAS) operate in an open and dynamic environment where agents enter and leave the system unpredictably. The dynamism and unpredictable nature of an open environment gives rise to unpredictable exceptions. It becomes essential to have some exception diagnosis mechanisms in these systems to effectively manage their exception diagnose processes. The defining characteristic of mechanisms is their ability to provide a uniform exception diagnosis capability to independently developed agents. In this paper we present a brief discussion of our proposed sentinel based approach to exception diagnosis in an open MAS and discuss the accuracy and performance overhead of our approach when applying in a market based MAS.

#index 1733006
#* Dynamical e-commerce system for shopping mall site through mobile devices
#@ Sanggil Kang;Wonik Park;Young-Kuk Kim
#t 2006
#c 17
#% 173879
#% 220711
#% 281147
#% 449588
#% 452563
#% 994114
#% 1303527
#! We introduce a novel personalized E-commerce system through mobile devices. By providing mobile clients' preferred service category or items in a shopping mall website, the problem of the limitation of resource of mobile devices can be solved. In this paper, the preferred service items are inferred by analyzing customers' statistical preference transactions and consumption behaviors in the website. In computing the statistical preference transactions, we consider the ratio of the length of each service page and customers' staying time on it. Also, our system dynamically provides the personalized E-commerce service according to the three different cases such as the beginning stage, the positive response, and the negative response. In the experimental section, we demonstrate our personalized E-commerce service system and show how much the resource of mobile devices can be saved.

#index 1733007
#* PROMOD: a modeling tool for product ontology
#@ Kyunghwa Kim;Moonhee Tark;Hyunja Lee;Junho Shim;Junsoo Lee;Seungjin Lee
#t 2006
#c 17
#% 435121
#% 759723
#% 831114
#% 831119
#! Product ontology is often constructed by explicating the domain ontology in a formal ontology language. The OWL Web Ontology Language has been positioned as a standard language. It requires technical expertise to directly represent the domain in OWL. An alternative way is to let a domain expert provide a conceptual representation of the domain, and to mechanically translate it into the corresponding OWL codes. We have developed a modeling tool called PROMOD to achieve this process in the product domain. We employ an Extended Entity-Relationship for conceptual model, enriched with modeling elements specialized for the product domain. We present how each element may be technically represented in OWL. We also provide a modeling scenario to demonstrate the practical feasibility of the tool in the field.

#index 1846657
#* Proceedings of the 2012 IEEE 28th International Conference on Data Engineering
#@ 
#t 2012
#c 17

#index 1846686
#* Searching Uncertain Data Represented by Non-axis Parallel Gaussian Mixture Models
#@ Katrin Haegler;Frank Fiedler;Christian Bohm
#t 2012
#c 17
#! Efficient similarity search in uncertain data is a central problem in many modern applications such as biometric identification, stock market analysis, sensor networks, medical imaging, etc. In such applications, the feature vector of an object is not exactly known but is rather defined by a probability density function like a Gaussian Mixture Model (GMM). Previous work is limited to axis-parallel Gaussian distributions, hence, correlations between different features are not considered in the similarity search. In this paper, we propose a novel, efficient similarity search technique for general GMMs without independence assumption for the attributes, named SUDN, which approximates the actual components of a GMM in a conservative but tight way. A filter-refinement architecture guarantees no false dismissals, due to conservativity, as well as a good filter selectivity, due to the tightness of our approximations. An extensive experimental evaluation of SUDN demonstrates a considerable speed-up of similarity queries on general GMMs and an increase in accuracy compared to existing approaches.

#index 1846687
#* Extending Map-Reduce for Efficient Predicate-Based Sampling
#@ Raman Grover;Michael J. Carey
#t 2012
#c 17
#! In this paper we address the problem of using MapReduce to sample a massive data set in order to produce a fixed-size sample whose contents satisfy a given predicate. While it is simple to express this computation using MapReduce, its default Hadoop execution is dependent on the input size and is wasteful of cluster resources. This is unfortunate, as sampling queries are fairly common (e.g., for exploratory data analysis at Facebook), and the resulting waste can significantly impact the performance of a shared cluster. To address such use cases, we present the design, implementation and evaluation of a Hadoop execution model extension that supports incremental job expansion. Under this model, a job consumes input as required and can dynamically govern its resource consumption while producing the required results. The proposed mechanism is able to support a variety of policies regarding job growth rates as they relate to cluster capacity and current load. We have implemented the mechanism in Hadoop, and we present results from an experimental performance study of different job growth policies under both single- and multi-user workloads.

#index 1846688
#* Branch Code: A Labeling Scheme for Efficient Query Answering on Trees
#@ Yanghua Xiao;Ji Hong;Wanyun Cui;Zhenying He;Wei Wang;Guodong Feng
#t 2012
#c 17
#! Labeling schemes lie at the core of query processing for many tree-structured data such as XML data that is flooding the web. A labeling scheme that can simultaneously and efficiently support various relationship queries on trees (such as parent/children, descendant/ancestor, etc.), computation of lowest common ancestors (LCA) and update of trees, is desired for effective and efficient management of tree-structured data. Although a variety of labeling schemes such as prefix-based labeling, interval-based labeling and prime-based labeling as well as their variants have been available to us for encoding static and dynamic trees, these labeling schemes usually show weakness in one aspect or another. In this paper, we propose an integer-based labeling scheme \emph{branch code} as well as its compressed version as our major solution to simultaneously support efficient query processing on both static and dynamic ordered trees with affordable storage cost. The proposed branch code can answer common queries on ordered trees in constant time, which comes at the cost of consuming $O(N\log N)$ storage. To reduce storage cost to $O(N)$, a compressed branch code is further developed. We also give a relationship determination algorithm purely using compressed branch code, which is of quite low possibility to produce false positive results as verified by experimental results. With the support of splay trees, branch code can also support dynamic trees so that updates and queries can be implemented with $O(\log N)$ amortized cost. All the results above are either theoretically proved or verified by experimental studies.

#index 1846689
#* An Efficient Trie-based Method for Approximate Entity Extraction with Edit-Distance Constraints
#@ Dong Deng;Guoliang Li;Jianhua Feng
#t 2012
#c 17
#! Dictionary-based entity extraction has attracted much attention from the database community recently, which locates sub strings in a document into predefined entities (e.g., person names or locations). To improve extraction recall, a recent trend is to provide approximate matching between sub strings of the document and entities by tolerating minor errors. In this paper we study dictionary-based approximate entity extraction with edit-distance constraints. Existing methods have several limitations. First, they need to tune many parameters to achieve high performance. Second, they are inefficient for large edit-distance thresholds. We propose a trie-based method to address these problems. We first partition each entity into a set of segments, and then use a trie structure to index segments. To extract similar entities, we search segments from the document, and extend the matching segments in both entities and the document to find similar pairs. We develop an extension-based method to efficiently find similar string pairs by extending the matching segments. We optimize our partition scheme and select the best partition strategy to improve the extraction performance. Experimental results show that our method achieves much higher performance compared with state-of-the-art studies.

#index 1846690
#* Energy Efficient Storage Management Cooperated with Large Data Intensive Applications
#@ Norifumi Nishikawa;Miyuki Nakano;Masaru Kitsuregawa
#t 2012
#c 17
#! Power, especially that consumed for storing data, and cooling costs for data centers have increased rapidly. The main applications running at data centers are data intensive applications such as large file servers or database systems. Recently, power management of the data intensive applications has been emphasized in the literature. Such reports discuss the importance of power savings. However, these reports lack research on power management models for the efficient use of data intensive applications' I/O behaviors. This paper proposes a novel energy efficient storage management system that monitors both application- and device-level I/O patterns at run time, and uses not only the device-level I/O pattern but also application level patterns. First, the design of the proposed model combined with such large data intensive applications will be shown. The key features of the model are i) classifying application-level I/O into four patterns using run-time access behaviors such as the length of idle time and read/write frequency, and ii) adopting an appropriate power-saving method-based on these application level I/O patterns. Next, the proposed method is quantitatively evaluated with typical data intensive applications such as file servers, OLTP, and DSS. It is shown that energy efficient storage management is effective in achieving large power savings compared with traditional approaches while an application is running.

#index 1846691
#* Physically Independent Stream Merging
#@ Badrish Chandramouli;David Maier;Jonathan Goldstein
#t 2012
#c 17
#! A facility for merging equivalent data streams can support multiple capabilities in a data stream management system (DSMS), such as query-plan switching and high availability. One can logically view a data stream as a temporal table of events, each associated with a lifetime (time interval) over which the event contributes to output. In many applications, the "same" logical stream may present itself physically in multiple physical forms, for example, due to disorder arising in transmission or from combining multiple sources, and modifications of earlier events. Merging such streams correctly is challenging when the streams may differ physically in timing, order, and composition. This paper introduces a new stream operator called Logical Merge (LMerge) that takes multiple logically consistent streams as input and outputs a single stream that is compatible with all of them. LMerge can handle the dynamic attachment and detachment of input streams. We present a range of algorithms for LMerge that can exploit compile-time stream properties for efficiency. Experiments with Stream Insight, a commercial DSMS, show that LMerge is sometimes orders-of-magnitude more efficient than enforcing determinism on inputs, and that there is benefit to using specialized algorithms when stream variability is limited. We also show that LMerge and its extensions can provide performance benefits in several real-world applications.

#index 1846692
#* A General Method for Estimating Correlated Aggregates over a Data Stream
#@ Srikanta Tirthapura;David P. Woodruff
#t 2012
#c 17
#! On a stream of two dimensional data items $(x,y)$ where $x$ is an item identifier, and $y$ is a numerical attribute, a correlated aggregate query requires us to first apply a selection predicate along the second ($y$) dimension, followed by an aggregation along the first ($x$) dimension. For selection predicates of the form $(y , c)$, where parameter $c$ is provided at query time, we present new streaming algorithms and lower bounds for estimating statistics of the resulting sub stream of elements that satisfy the predicate. We provide the first sub linear space algorithms for a large family of statistics in this model, including frequency moments. We experimentally validate our algorithms, showing that their memory requirements are significantly smaller than existing linear storage schemes for large datasets, while simultaneously achieving fast per-record processing time. We also study the problem when the items have weights. Allowing negative weights allows for analyzing values which occur in the symmetric difference of two datasets. We give a strong space lower bound which holds even if the algorithm is allowed up to a logarithmic number of passes over the data(before the query is presented). We complement this with a small space algorithm which uses a logarithmic number of passes.

#index 1846693
#* Accuracy-Aware Uncertain Stream Databases
#@ Tingjian Ge;Fujun Liu
#t 2012
#c 17
#! Previous work has introduced probability distributions as first-class components in uncertain stream database systems. A lacking element is the fact of how accurate these probability distributions are. This indeed has a profound impact on the accuracy of query results presented to end users. While there is some previous work that studies unreliable intermediate query results in the tuple uncertainty model, to the best of our know-ledge, we are the first to consider an uncertain stream database in which accuracy is taken into consideration all the way from the learned distributions based on raw data samples to the query results. We perform an initial study of various components in an accuracy-aware uncertain stream database system, including the representation of accuracy information and how to obtain query results' accuracy. In addition, we propose novel predicates based on hypothesis testing for decision-making using data with limited accuracy. We augment our study with a comprehensive set of experimental evaluations.

#index 1846694
#* An Efficient Graph Indexing Method
#@ Xiaoli Wang;Xiaofeng Ding;Anthony K. H. Tung;Shanshan Ying;Hai Jin
#t 2012
#c 17
#! Graphs are popular models for representing complex structure data and similarity search for graphs has become a fundamental research problem. Many techniques have been proposed to support similarity search based on the graph edit distance. However, they all suffer from certain drawbacks: high computational complexity, poor scalability in terms of database size, or not taking full advantage of indexes. To address these problems, in this paper, we propose SEGOS, an indexing and query processing framework for graph similarity search. First, an effective two-level index is constructed off-line based on sub-unit decomposition of graphs. Then, a novel search strategy based on the index is proposed. Two algorithms adapted from TA and CA methods are seamlessly integrated into the proposed strategy to enhance graph search. More specially, the proposed framework is easy to be pipelined to support continuous graph pruning. Extensive experiments are conducted on two real datasets to evaluate the effectiveness and scalability of our approaches.

#index 1846695
#* HiCS: High Contrast Subspaces for Density-Based Outlier Ranking
#@ Fabian Keller;Emmanuel Muller;Klemens Bohm
#t 2012
#c 17
#! Outlier mining is a major task in data analysis. Outliers are objects that highly deviate from regular objects in their local neighborhood. Density-based outlier ranking methods score each object based on its degree of deviation. In many applications, these ranking methods degenerate to random listings due to low contrast between outliers and regular objects. Outliers do not show up in the scattered full space, they are hidden in multiple high contrast subspace projections of the data. Measuring the contrast of such subspaces for outlier rankings is an open research challenge. In this work, we propose a novel subspace search method that selects high contrast subspaces for density-based outlier ranking. It is designed as pre-processing step to outlier ranking algorithms. It searches for high contrast subspaces with a significant amount of conditional dependence among the subspace dimensions. With our approach, we propose a first measure for the contrast of subspaces. Thus, we enhance the quality of traditional outlier rankings by computing outlier scores in high contrast projections only. The evaluation on real and synthetic data shows that our approach outperforms traditional dimensionality reduction techniques, naive random projections as well as state-of-the-art subspace search techniques and provides enhanced quality for outlier ranking.

#index 1846696
#* Extracting Analyzing and Visualizing Triangle K-Core Motifs within Networks
#@ Yang Zhang;Srinivasan Parthasarathy
#t 2012
#c 17
#! Cliques are topological structures that usually provide important information for understanding the structure of a graph or network. However, detecting and extracting cliques efficiently is known to be very hard. In this paper, we define and introduce the notion of a Triangle K-Core, a simpler topological structure and one that is more tractable and can moreover be used as a proxy for extracting clique-like structure from large graphs. Based on this definition we first develop a localized algorithm for extracting Triangle K-Cores from large graphs. Subsequently we extend the simple algorithm to accommodate dynamic graphs (where edges can be dynamically added and deleted). Finally, we extend the basic definition to support various template pattern cliques with applications to network visualization and event detection on graphs and networks. Our empirical results reveal the efficiency and efficacy of the proposed methods on many real world datasets.

#index 1846697
#* Horizontal Reduction: Instance-Level Dimensionality Reduction for Similarity Search in Large Document Databases
#@ Min Soo Kim;Kyu-Young Whang;Yang-Sae Moon
#t 2012
#c 17
#! Dimensionality reduction is essential in text mining since the dimensionality of text documents could easily reach several tens of thousands. Most recent efforts on dimensionality reduction, however, are not adequate to large document databases due to lack of scalability. We hence propose a new type of simple but effective dimensionality reduction, called horizontal (dimensionality) reduction, for large document databases. Horizontal reduction converts each text document to a few bitmap vectors and provides tight lower bounds of inter-document distances using those bitmap vectors. Bitmap representation is very simple and extremely fast, and its instance-based nature makes it suitable for large and dynamic document databases. Using the proposed horizontal reduction, we develop an efficient k-nearest neighbor (k-NN) search algorithm for text mining such as classification and clustering, and we formally prove its correctness. The proposed algorithm decreases I/O and CPU overheads simultaneously since horizontal reduction (1) reduces the number of accesses to documents significantly by exploiting the bitmap-based lower bounds in filtering dissimilar documents at an early stage, and accordingly, (2) decreases the number of CPU-intensive computations for obtaining a real distance between high-dimensional document vectors. Extensive experimental results show that horizontal reduction improves the performance of the reduction (preprocessing) process by one to two orders of magnitude compared with existing reduction techniques, and our k-NN search algorithm significantly outperforms the existing ones by one to three orders of magnitude.

#index 1846698
#* Adaptive Windows for Duplicate Detection
#@ Uwe Draisbach;Felix Naumann;Sascha Szott;Oliver Wonneberg
#t 2012
#c 17
#! Duplicate detection is the task of identifying all groups of records within a data set that represent the same real-world entity, respectively. This task is difficult, because (i) representations might differ slightly, so some similarity measure must be defined to compare pairs of records and (ii) data sets might have a high volume making a pair-wise comparison of all records infeasible. To tackle the second problem, many algorithms have been suggested that partition the data set and compare all record pairs only within each partition. One well-known such approach is the Sorted Neighborhood Method (SNM), which sorts the data according to some key and then advances a window over the data comparing only records that appear within the same window. We propose with the Duplicate Count Strategy (DCS) a variation of SNM that uses a varying window size. It is based on the intuition that there might be regions of high similarity suggesting a larger window size and regions of lower similarity suggesting a smaller window size. Next to the basic variant of DCS, we also propose and thoroughly evaluate a variant called DCS++ which is provably better than the original SNM in terms of efficiency (same results with fewer comparisons).

#index 1846699
#* Evaluating Probabilistic Queries over Uncertain Matching
#@ Reynold Cheng;Jian Gong;David W. Cheung;Jiefeng Cheng
#t 2012
#c 17
#! A matching between two database schemas, generated by machine learning techniques (e.g., COMA++), is often uncertain. Handling the uncertainty of schema matching has recently raised a lot of research interest, because the quality of applications rely on the matching result. We study query evaluation over an inexact schema matching, which is represented as a set of ``possible mappings'', as well as the probabilities that they are correct. Since the number of possible mappings can be large, evaluating queries through these mappings can be expensive. By observing the fact that the possible mappings between two schemas often exhibit a high degree of overlap, we develop two efficient solutions. We also present a fast algorithm to compute answers with the k highest probabilities. An extensive evaluation on real schemas shows that our approaches improve the query performance by almost an order of magnitude.

#index 1846700
#* On Discovery of Traveling Companions from Streaming Trajectories
#@ Lu-An Tang;Yu Zheng;Jing Yuan;Jiawei Han;Alice Leung;Chih-Chieh Hung;Wen-Chih Peng
#t 2012
#c 17
#! The advance of object tracking technologies leads to huge volumes of spatio-temporal data collected in the form of trajectory data stream. In this study, we investigate the problem of discovering object groups that travel together (i.e., traveling companions) from trajectory stream. Such technique has broad applications in the areas of scientific study, transportation management and military surveillance. To discover traveling companions, the monitoring system should cluster the objects of each snapshot and intersect the clustering results to retrieve moving-together objects. Since both clustering and intersection steps involve high computational overhead, the key issue of companion discovery is to improve the algorithm's efficiency. We propose the models of closed companion candidates and smart intersection to accelerate data processing. A new data structure termed traveling buddy is designed to facilitate scalable and flexible companion discovery on trajectory stream. The traveling buddies are micro-groups of objects that are tightly bound together. By only storing the object relationships rather than their spatial coordinates, the buddies can be dynamically maintained along trajectory stream with low cost. Based on traveling buddies, the system can discover companions without accessing the object details. The proposed methods are evaluated with extensive experiments on both real and synthetic datasets. The buddy-based method is an order of magnitude faster than existing methods. It also outperforms other competitors with higher precision and recall in companion discovery.

#index 1846701
#* Iterative Graph Feature Mining for Graph Indexing
#@ Dayu Yuan;Prasenjit Mitra;Huiwen Yu;C. Lee Giles
#t 2012
#c 17
#! Sub graph search is a popular query scenario on graph databases. Given a query graph q, the sub graph search algorithm returns all database graphs having q as a sub graph. To efficiently implement a subgraph search, subgraph features are mined in order to index the graph database. Many subgraph feature mining approaches have been proposed. They are all "mine-at-once" algorithms in which the whole feature set is mined in one run before building a stable graph index. However, due to the change of environments (such as an update of the graph database and the increase of available memory), the index needs to be updated to accommodate such changes. Most of the "mine-at-once" algorithms involve frequent subgraph or subtree mining over the whole graph database. Also, constructing and deploying a new index involves an expensive disk operation such that it is inefficient to re-mine the features and rebuild the index from scratch. We observe that, under most cases, it is sufficient to update a small part of the graph index. Here we propose an "iterative subgraph mining" algorithm which iteratively finds one feature to insert into (or remove from) the index. Since the majority of indexing features and the index structure are not changed, the algorithm can be frequently invoked. We define an objective function that guides the feature mining. Next, we propose a basic branch and bound algorithm to mine the features. Finally, we design an advanced search algorithm, which quickly finds a near-optimum subgraph feature and reduces the search space. Experiments show that our feature mining algorithm is 5 times faster than the popular graph indexing algorithm gIndex, and that features mined by our iterative algorithm have a better filtering rate for the subgraph search problem.

#index 1846702
#* PRAGUE: Towards Blending Practical Visual Subgraph Query Formulation and Query Processing
#@ Changjiu Jin;Sourav S. Bhowmick;Byron Choi;Shuigeng Zhou
#t 2012
#c 17
#! In a previous paper, we laid out the vision of a novel graph query processing paradigm where instead of processing a visual query graph after its construction, it interleaves visual query formulation and processing by exploiting the latency offered by the GUI to filter irrelevant matches and prefetch partial query results [8]. Our first attempt at implementing this vision, called GBLENDER [8], shows significant improvement in system response time (SRT) for sub graph containment queries. However, GBLENDER suffers from two key drawbacks, namely inability to handle visual sub graph similarity queries and inefficient support for visual query modification, limiting its usage in practical environment. In this paper, we propose a novel algorithm called PRAGUE (Practical visu Al Graph QUery Blender), that addresses these limitations by exploiting a novel data structure called spindle-shaped graphs (SPIG). A SPIG succinctly records various information related to the set of super graphs of a newly added edge in the visual query fragment. Specifically, PRAGUE realizes a unified visual framework to support SPIG-based processing of modification-efficient sub graph containment and similarity queries. Extensive experiments on real-world and synthetic datasets demonstrate effectiveness of PRAGUE.

#index 1846703
#* Ego-centric Graph Pattern Census
#@ Walaa Eldin Moustafa;Amol Deshpande;Lise Getoor
#t 2012
#c 17
#! There is increasing interest in analyzing networks of all types including social, biological, sensor, computer, and transportation networks. Broadly speaking, we may be interested in global network-wide analysis (e.g., centrality analysis, community detection) where the properties of the entire network are of interest, or local ego-centric analysis where the focus is on studying the properties of nodes (egos) by analyzing their neighborhood sub graphs. In this paper we propose and study ego-centric pattern census queries, a new type of graph analysis query, where a given structural pattern is searched for in every node's neighborhood and the counts are reported or used in further analysis. This kind of analysis is useful in many domains in social network analysis including opinion leader identification, node classification, link prediction, and role identification. We propose an SQL-based declarative language to support this class of queries, and develop a series of efficient query evaluation algorithms for it. We evaluate our algorithms on a variety of synthetically generated graphs. We also show an application of our language in a real-world scenario for predicting future collaborations from DBLP data.

#index 1846704
#* Efficient Dual-Resolution Layer Indexing for Top-k Queries
#@ Jongwuk Lee;Hyunsouk Cho;Seung-won Hwang
#t 2012
#c 17
#! Top-k queries have gained considerable attention as an effective means for narrowing down the overwhelming amount of data. This paper studies the problem of constructing an indexing structure that efficiently supports top-k queries for varying scoring functions and retrieval sizes. The existing work can be categorized into three classes: list-, layer-, and view-based approaches. This paper focuses on the layer-based approach, pre-materializing tuples into consecutive multiple layers. The layer-based index enables us to return top-k answers efficiently by restricting access to tuples in the k layers. However, we observe that the number of tuples accessed in each layer can be reduced further. For this purpose, we propose a dual-resolution layer structure. Specifically, we iteratively build coarse-level layers using skylines, and divide each coarse-level layer into fine-level sub layers using convex skylines. The dual-resolution layer is able to leverage not only the dominance relationship between coarse-level layers, named for all-dominance, but also a relaxed dominance relationship between fine-level sub layers, named exists-dominance. Our extensive evaluation results demonstrate that our proposed method significantly reduces the number of tuples accessed than the state-of-the-art methods.

#index 1846705
#* Detecting Outliers in Sensor Networks Using the Geometric Approach
#@ Sabbas Burdakis;Antonios Deligiannakis
#t 2012
#c 17
#! The topic of outlier detection in sensor networks has received significant attention in recent years. Detecting when the measurements of a node become ``abnormal'' is interesting, because this event may help detect either a malfunctioning node, or a node that starts observing a local interesting phenomenon (i.e., a fire). In this paper we present a new algorithm for detecting outliers in sensor networks, based on the geometric approach. Unlike prior work. our algorithms perform a distributed monitoring of outlier readings, exhibit 100\% accuracy in their monitoring (assuming no message losses), and require the transmission of messages only at a fraction of the epochs, thus allowing nodes to safely refrain from transmitting in many epochs. Our approach is based on transforming common similarity metrics in a way that admits the application of the recently proposed geometric approach. We then propose a general framework and suggest multiple modes of operation, which allow each sensor node to accurately monitor its similarity to other nodes. Our experiments demonstrate that our algorithms can accurately detect outliers at a fraction of the communication cost that a centralized approach would require (even in the case where the central node lies just one hop away from all sensor nodes). Moreover, we demonstrate that these bandwidth savings become even larger as we incorporate further optimizations in our proposed modes of operation.

#index 1846706
#* Efficient Threshold Monitoring for Distributed Probabilistic Data
#@ Mingwang Tang;Feifei Li;Jeff M. Phillips;Jeffrey Jestes
#t 2012
#c 17
#! In distributed data management, a primary concern is monitoring the distributed data and generating an alarm when a user specified constraint is violated. A particular useful instance is the threshold based constraint, which is commonly known as the distributed threshold monitoring problem. This work extends this useful and fundamental study to distributed probabilistic data that emerge in a lot of applications, where uncertainty naturally exists when massive amounts of data are produced at multiple sources in distributed, networked locations. Examples include distributed observing stations, large sensor fields, geographically separate scientific institutes/units and many more. When dealing with probabilistic data, there are two thresholds involved, the score and the probability thresholds. One must monitor both simultaneously, as such, techniques developed for deterministic data are no longer directly applicable. This work presents a comprehensive study to this problem. Our algorithms have significantly outperformed the baseline method in terms of both the communication cost (number of messages and bytes) and the running time, as shown by an extensive experimental evaluation using several, real large datasets.

#index 1846707
#* Incorporating Duration Information for Trajectory Classification
#@ Dhaval Patel;Chang Sheng;Wynne Hsu;Mong Li Lee
#t 2012
#c 17
#! Trajectory classification has many useful applications. Existing works on trajectory classification do not consider the duration information of trajectory. In this paper, we extract duration-aware features from trajectories to build a classifier. Our method utilizes information theory to obtain regions where the trajectories have similar speeds and directions. Further, trajectories are summarized into a network based on the MDL principle that takes into account the duration difference among trajectories of different classes. A graph traversal is performed on this trajectory network to obtain the top-k covering path rules for each trajectory. Based on the discovered regions and top-k path rules, we build a classifier to predict the class labels of new trajectories. Experiment results on real-world datasets show that the proposed duration-aware classifier can obtain higher classification accuracy than the state-of-the-art trajectory classifier.

#index 1846708
#* Reducing Uncertainty of Low-Sampling-Rate Trajectories
#@ Kai Zheng;Yu Zheng;Xing Xie;Xiaofang Zhou
#t 2012
#c 17
#! The increasing availability of GPS-embedded mobile devices has given rise to a new spectrum of location-based services, which have accumulated a huge collection of location trajectories. In practice, a large portion of these trajectories are of low-sampling-rate. For instance, the time interval between consecutive GPS points of some trajectories can be several minutes or even hours. With such a low sampling rate, most details of their movement are lost, which makes them difficult to process effectively. In this work, we investigate how to reduce the uncertainty in such kind of trajectories. Specifically, given a low-sampling-rate trajectory, we aim to infer its possible routes. The methodology adopted in our work is to take full advantage of the rich information extracted from the historical trajectories. We propose a systematic solution, History based Route Inference System (HRIS), which covers a series of novel algorithms that can derive the travel pattern from historical data and incorporate it into the route inference process. To validate the effectiveness of the system, we apply our solution to the map-matching problem which is an important application scenario of this work, and conduct extensive experiments on a real taxi trajectory dataset. The experiment results demonstrate that HRIS can achieve higher accuracy than the existing map-matching algorithms for low-sampling-rate trajectories.

#index 1846709
#* Aggregate Query Answering on Possibilistic Data with Cardinality Constraints
#@ Graham Cormode;Divesh Srivastava;Entong Shen;Ting Yu
#t 2012
#c 17
#! Uncertainties in data can arise for a number of reasons: when data is incomplete, contains conflicting information or has been deliberately perturbed or coarsened to remove sensitive details. An important case which arises in many real applications is when the data describes a set of possibilities, but with cardinality constraints. These constraints represent correlations between tuples encoding, e.g. that at most two possible records are correct, or that there is an (unknown) one-to-one mapping between a set of tuples and attribute values. Although there has been much effort to handle uncertain data, current systems are not equipped to handle such correlations, beyond simple mutual exclusion and co-existence constraints. Vitally, they have little support for efficiently handling aggregate queries on such data. In this paper, we aim to address some of these deficiencies, by introducing LICM (Linear Integer Constraint Model), which can succinctly represent many types of tuple correlations, particularly a class of cardinality constraints. We motivate and explain the model with examples from data cleaning and masking sensitive data, to show that it enables modeling and querying such data, which was not previously possible. We develop an efficient strategy to answer conjunctive and aggregate queries on possibilistic data by describing how to implement relational operators over data in the model. LICM compactly integrates the encoding of correlations, query answering and lineage recording. In combination with off-the-shelf linear integer programming solvers, our approach provides exact bounds for aggregate queries. Our prototype implementation demonstrates that query answering with LICM can be effective and scalable.

#index 1846710
#* Discovering Threshold-based Frequent Closed Itemsets over Probabilistic Data
#@ Yongxin Tong;Lei Chen;Bolin Ding
#t 2012
#c 17
#! In recent years, many new applications, such as sensor network monitoring and moving object search, show a growing amount of importance of uncertain data management and mining. In this paper, we study the problem of discovering threshold-based frequent closed item sets over probabilistic data. Frequent item set mining over probabilistic database has attracted much attention recently. However, existing solutions may lead an exponential number of results due to the downward closure property over probabilistic data. Moreover, it is hard to directly extend the successful experiences from mining exact data to a probabilistic environment due to the inherent uncertainty of data. Thus, in order to obtain a reasonable result set with small size, we study discovering frequent closed item sets over probabilistic data. We prove that even a sub-problem of this problem, computing the frequent closed probability of an item set, is #P-Hard. Therefore, we develop an efficient mining algorithm based on depth-first search strategy to obtain all probabilistic frequent closed item sets. To reduce the search space and avoid redundant computation, we further design several probabilistic pruning and bounding techniques. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments.

#index 1846711
#* Ranking Query Answers in Probabilistic Databases: Complexity and Efficient Algorithms
#@ Dan Olteanu;Hongkai Wen
#t 2012
#c 17
#! In many applications of probabilistic databases, the probabilities are mere degrees of uncertainty in the data and are not otherwise meaningful to the user. Often, users care only about the ranking of answers in decreasing order of their probabilities or about a few most likely answers. In this paper, we investigate the problem of ranking query answers in probabilistic databases. We give a dichotomy for ranking in case of conjunctive queries without repeating relation symbols: it is either in polynomial time or \#P-hard. Surprisingly, our syntactic characterisation of tractable queries is not the same as for probability computation. The key observation is that there are queries for which probability computation is \#P-hard, yet ranking can be computed in polynomial time. This is possible whenever probability computation for distinct answers has a common factor that is hard to compute but irrelevant for ranking. We complement this tractability analysis with an effective ranking technique for conjunctive queries. Given a query, we construct a share plan, which exposes sub queries whose probability computation can be shared or ignored across query answers. Our technique combines share plans with incremental approximate probability computation of sub queries. We implemented our technique in the SPROUT query engine and report on performance gains of orders of magnitude over Monte Carlo simulation using FPRAS and exact probability computation based on knowledge compilation.

#index 1846712
#* Joint Entity Resolution
#@ Steven Euijong Whang;Hector Garcia-Molina
#t 2012
#c 17
#! Entity resolution (ER) is the problem of identifying which records in a database represent the same entity. Often, records of different types are involved (e.g., authors, publications, institutions, venues), and resolving records of one type can impact the resolution of other types of records. In this paper we propose a flexible, modular resolution framework where existing ER algorithms developed for a given record type can be plugged in and used in concert with other ER algorithms. Our approach also makes it possible to run ER on subsets of similar records at a time, important when the full data is too large to resolve together. We study the scheduling and coordination of the individual ER algorithms in order to resolve the full data set. We then evaluate our joint ER techniques on synthetic and real data and show the scalability of our approach.

#index 1846713
#* Efficient Similarity Search over Encrypted Data
#@ Mehmet Kuzu;Mohammad Saiful Islam;Murat Kantarcioglu
#t 2012
#c 17
#! In recent years, due to the appealing features of cloud computing, large amount of data have been stored in the cloud. Although cloud based services offer many advantages, privacy and security of the sensitive data is a big concern. To mitigate the concerns, it is desirable to outsource sensitive data in encrypted form. Encrypted storage protects the data against illegal access, but it complicates some basic, yet important functionality such as the search on the data. To achieve search over encrypted data without compromising the privacy, considerable amount of searchable encryption schemes have been proposed in the literature. However, almost all of them handle exact query matching but not similarity matching, a crucial requirement for real world applications. Although some sophisticated secure multi-party computation based cryptographic techniques are available for similarity tests, they are computationally intensive and do not scale for large data sources. In this paper, we propose an efficient scheme for similarity search over encrypted data. To do so, we utilize a state-of-the-art algorithm for fast near neighbor search in high dimensional spaces called locality sensitive hashing. To ensure the confidentiality of the sensitive data, we provide a rigorous security definition and prove the security of the proposed scheme under the provided definition. In addition, we provide a real world application of the proposed scheme and verify the theoretical results with empirical observations on a real dataset.

#index 1846714
#* Obfuscating the Topical Intention in Enterprise Text Search
#@ Hwee Hwa Pang;Xiaokui Xiao;Jialie Shen
#t 2012
#c 17
#! The text search queries in an enterprise can reveal the users' topic of interest, and in turn confidential staff or business information. To safeguard the enterprise from consequences arising from a disclosure of the query traces, it is desirable to obfuscate the true user intention from the search engine, without requiring it to be re-engineered. In this paper, we advocate a unique approach to profile the topics that are relevant to the user intention. Based on this approach, we introduce an $(\epsilon_1, \epsilon_2)$-privacy model that allows a user to stipulate that topics relevant to her intention at $\epsilon_1$ level should appear to any adversary to be innocuous at $\epsilon_2$ level. We then present a Top Priv algorithm to achieve the customized $(\epsilon_1, \epsilon_2)$-privacy requirement of individual users through injecting automatically formulated fake queries. The advantages of Top Priv over existing techniques are confirmed through benchmark queries on a real corpus, with experiment settings fashioned after an enterprise search application.

#index 1846715
#* Correlation Support for Risk Evaluation in Databases
#@ Katrin Eisenreich;Jochen Adamek;Philipp Rosch;Volker Markl;Gregor Hackenbroich
#t 2012
#c 17
#! Investigating potential dependencies in data and their effect on future business developments can help experts to prevent misestimations of risks and chances. This makes correlation a highly important factor in risk analysis tasks. Previous research on correlation in uncertain data management addressed foremost the handling of dependencies between discrete rather than continuous distributions. Also, none of the existing approaches provides a clear method for extracting correlation structures from data and introducing assumptions about correlation to independently represented data. To enable risk analysis under correlation assumptions, we use an approximation technique based on copula functions. This technique enables analysts to introduce arbitrary correlation structures between arbitrary distributions and calculate relevant measures over thus correlated data. The correlation information can either be extracted at runtime from historic data or be accessed from a parametrically precomputed structure. We discuss the construction, application and querying of approximate correlation representations for different analysis tasks. Our experiments demonstrate the efficiency and accuracy of the proposed approach, and point out several possibilities for optimization.

#index 1846716
#* A Game-Theoretic Approach for High-Assurance of Data Trustworthiness in Sensor Networks
#@ Hyo-Sang Lim;Gabriel Ghinita;Elisa Bertino;Murat Kantarcioglu
#t 2012
#c 17
#! Sensor networks are being increasingly deployed in many application domains ranging from environment monitoring to supervising critical infrastructure systems (e.g., the power grid). Due to their ability to continuously collect large amounts of data, sensor networks represent a key component in decision-making, enabling timely situation assessment and response. However, sensors deployed in hostile environments may be subject to attacks by adversaries who intend to inject false data into the system. In this context, {\em data trustworthiness} is an important concern, as false readings may result in wrong decisions with serious consequences (e.g., large-scale power outages). To defend against this threat, it is important to establish trust levels for sensor nodes and adjust node trustworthiness scores to account for malicious interferences. In this paper, we develop a game-theoretic defense strategy to protect sensor nodes from attacks and to guarantee a high level of trustworthiness for sensed data. We use a discrete time model, and we consider that there is a limited attack budget that bounds the capability of the attacker in each round. The defense strategy objective is to ensure that sufficient sensor nodes are protected in each round such that the discrepancy between the value accepted and the truthful sensed value is below a certain threshold. We model the attack-defense interaction as a Stackel berg game, and we derive the Nash equilibrium condition that is sufficient to ensure that the sensed data are truthful within a nominal error bound. We implement a prototype of the proposed strategy and we show through extensive experiments that our solution provides an effective and efficient way of protecting sensor networks from attacks.

#index 1846717
#* Data Management Issues on the Semantic Web
#@ Oktie Hassanzadeh;Anastasios Kementsietsidis;Yannis Velegrakis
#t 2012
#c 17
#! We provide an overview of the current data management research issues in the context of the Semantic Web. The objective is to introduce the audience into the area of the Semantic Web, and to highlight the fact that the area provides many interesting research opportunities for the data management community. A new model, the Resource Description Framework (RDF), coupled with a new query language, called SPARQL, lead us to revisit some classical data management problems, including efficient storage, query optimization, and data integration. These are problems that the Semantic Web community has only recently started to explore, and therefore the experience and long tradition of the database community can prove valuable. We target both experienced and novice researchers that are looking for a thorough presentation of the area and its key research topics.

#index 1846718
#* A Self-Configuring Schema Matching System
#@ Eric Peukert;Julian Eberius;Erhard Rahm
#t 2012
#c 17
#! Mapping complex metadata structures is crucial in a number of domains such as data integration, ontology alignment or model management. To speed up the generation of such mappings, automatic matching systems were developed to compute mapping suggestions that can be corrected by a user. However, constructing and tuning match strategies still requires a high manual effort by matching experts as well as correct mappings to evaluate generated mappings. We therefore propose a self-configuring schema matching system that is able to automatically adapt to the given mapping problem at hand. Our approach is based on analyzing the input schemas as well as intermediate matching results. A variety of matching rules use the analysis results to automatically construct and adapt an underlying matching process for a given match task. We comprehensively evaluate our approach on different mapping problems from the schema, ontology and model management domains. The evaluation shows that our system is able to robustly return good quality mappings across different mapping problems and domains.

#index 1846719
#* Incremental Detection of Inconsistencies in Distributed Data
#@ Wenfei Fan;Jianzhong Li;Nan Tang;Wenyuan Yu
#t 2012
#c 17
#! This paper investigates the problem of incremental detection of errors in distributed data. Given a distributed database D, a set \Sigma of conditional functional dependencies (CFDs), the set V of violations of the CFDs in D, and updates \Delta D to D, it is to find, with minimum data shipment, changes \Delta V to V in response to \Delta D. The need for the study is evident since real-life data is often dirty, distributed and is frequently updated. It is often prohibitively expensive to recompute the entire set of violations when D is updated. We show that the incremental detection problem is NP-complete for D partitioned either vertically or horizontally, even when \Sigma and D are fixed. Nevertheless, we show that it is bounded and better still, actually optimal: there exist algorithms to detect errors such that their computational cost and data shipment are both linear in the size of \Delta D and \Delta V, independent of the size of the database D. We provide such incremental algorithms for vertically partitioned data, and show that the algorithms are optimal. We further propose optimization techniques for the incremental algorithm over vertical partitions to reduce data shipment. We verify experimentally, using real-life data on Amazon Elastic Compute Cloud (EC2), that our algorithms substantially outperform their batch counterparts even when \Delta V is reasonably large.

#index 1846720
#* Recomputing Materialized Instances after Changes to Mappings and Data
#@ Todd J. Green;Zachary G. Ives
#t 2012
#c 17
#! A major challenge faced by today's information systems is that of evolution as data usage evolves or new data resources become available. Modern organizations sometimes exchange data with one another via declarative mappings among their databases, as in data exchange and collaborative data sharing systems. Such mappings are frequently revised and refined as new data becomes available, new cross-reference tables are created, and corrections are made. A fundamental question is how to handle changes to these mapping definitions, when the organizations each materialize the results of applying the mappings to the available data. We consider how to incrementally recompute these database instances in this setting, reusing (if possible) previously computed instances to speed up computation. We develop a principled solution that performs cost-based exploration of recomputation versus reuse, and simultaneously handles updates to source data and mapping definitions through a single, unified mechanism. Our solution also takes advantage of provenance information, when present, to speed up computation even further. We present an implementation that takes advantage of an off-the-shelf DBMS's query processing system, and we show experimentally that our approach provides substantial performance benefits.

#index 1846721
#* SWST: A Disk Based Index for Sliding Window Spatio-Temporal Data
#@ Manish Singh;Qiang Zhu;H. V. Jagadish
#t 2012
#c 17
#! Numerous applications such as wireless communication and telematics need to keep track of evolution of spatio-temporal data for a limited past. Limited retention may even be required by regulations. In general, each data entry can have its own user specified lifetime. It is desired that expired entries are automatically removed by the system through some garbage collection mechanism. This kind of limited retention can be achieved by using a sliding window semantics similar to that from stream data processing. However, due to the large volume and relatively long lifetime of data in the aforementioned applications (in contrast to the real-time transient streaming data), the sliding window here needs to be maintained for data on disk rather than in memory. It is a new challenge to provide fast access to the information from the recent past and, at the same time, facilitate efficient deletion of the expired entries. In this paper, we propose a disk based, two-layered, sliding window indexing scheme for discretely moving spatio-temporal data. Our index can support efficient processing of standard time slice and interval queries and delete expired entries with almost no overhead. In existing historical spatio-temporal indexing techniques, deletion is either infeasible or very inefficient. Our sliding window based processing model can support both current and past entries, while many existing historical spatio-temporal indexing techniques cannot keep these two types of data together in the same index. Our experimental comparison with the best known historical index (i.e., the MV3R tree) for discretely moving spatio-temporal data shows that our index is about five times faster in terms of insertion time and comparable in terms of search performance. MV3R follows a partial persistency model, whereas our index can support very efficient deletion and update.

#index 1846722
#* Discovering Multiple Clustering Solutions: Grouping Objects in Different Views of the Data
#@ Emmanuel Muller;Stephan Gunnemann;Ines Farber;Thomas Seidl
#t 2012
#c 17
#! Traditional clustering algorithms identify just a single clustering of the data. Today's complex data, however, allow multiple interpretations leading to several valid groupings hidden in different views of the database. Each of these multiple clustering solutions is valuable and interesting as different perspectives on the same data and several meaningful groupings for each object are given. Especially for high dimensional data, where each object is described by multiple attributes, alternative clusters in different attribute subsets are of major interest. In this tutorial, we describe several real world application scenarios for multiple clustering solutions. We abstract from these scenarios and provide the general challenges in this emerging research area. We describe state-of-the-art paradigms, we highlight specific techniques, and we give an overview of this topic by providing a taxonomy of the existing clustering methods. By focusing on open challenges, we try to attract young researchers for participating in this emerging research field.

#index 1846723
#* Detecting Clones, Copying and Reuse on the Web
#@ Xin Luna Dong;Divesh Srivastava
#t 2012
#c 17
#! The Web has enabled the availability of a vast amount of useful information in recent years. However, the web technologies that have enabled sources to share their information have also made it easy for sources to copy from each other and often publish without proper attribution. Understanding the copying relationships between sources has many benefits, including helping data providers protect their own rights, improving various aspects of data integration, and facilitating in-depth analysis of information flow. The importance of copy detection has led to a substantial amount of research in many disciplines of Computer Science, based on the type of information considered, such as text, images, videos, software code, and structured data. This seminar explores the similarities and differences between the techniques proposed for copy detection across the different types of information. We also examine the computational challenges associated with large-scale copy detection, indicating how they could be detected efficiently, and identify a range of open problems for the community.

#index 1846724
#* Mining Knowledge from Data: An Information Network Analysis Approach
#@ Jiawei Han;Yizhou Sun;Xifeng Yan;Philip S. Yu
#t 2012
#c 17
#! Most objects and data in the real world are interconnected, forming complex, heterogeneous but often semistructured information networks. However, many database researchers consider a database merely as a data repository that supports storage and retrieval rather than an information-rich, inter-related and multi-typed information network that supports comprehensive data analysis, whereas many network researchers focus on homogeneous networks. Departing from both, we view interconnected, semi-structured datasets as heterogeneous, information-rich networks and study how to uncover hidden knowledge in such networks. For example, a university database can be viewed as a heterogeneous information network, where objects of multiple types, such as students, professors, courses, departments, and multiple typed relationships, such as teach and advise are intertwined together, providing abundant information. In this tutorial, we present an organized picture on mining heterogeneous information networks and introduce a set of interesting, effective and scalable network mining methods. The topics to be covered include (i) database as an information network, (ii) mining information networks: clustering, classification, ranking, similarity search, and meta path-guided analysis, (iii) construction of quality, informative networks by data mining, (iv) trend and evolution analysis in heterogeneous information networks, and (v) research frontiers. We show that heterogeneous information networks are informative, and link analysis on such networks is powerful at uncovering critical knowledge hidden in large semi-structured datasets. Finally, we also present a few promising research directions.

#index 1846725
#* Emerging Graph Queries in Linked Data
#@ Arijit Khan;Yinghui Wu;Xifeng Yan
#t 2012
#c 17
#! In a wide array of disciplines, data can be modeled as an interconnected network of entities, where various attributes could be associated with both the entities and the relations among them. Knowledge is often hidden in the complex structure and attributes inside these networks. While querying and mining these linked datasets are essential for various applications, traditional graph queries may not be able to capture the rich semantics in these networks. With the advent of complex information networks, new graph queries are emerging, including graph pattern matching and mining, similarity search, ranking and expert finding, graph aggregation and OLAP. These queries require both the topology and content information of the network data, and hence, different from classical graph algorithms such as shortest path, reach ability and minimum cut, which depend only on the structure of the network. In this tutorial, we shall give an introduction of the emerging graph queries, their indexing and resolution techniques, the current challenges and the future research directions.

#index 1846726
#* Boolean Matrix Decomposition Problem: Theory, Variations and Applications to Data Engineering
#@ Jaideep Vaidya
#t 2012
#c 17
#! With the ubiquitous nature and sheer scale of data collection, the problem of data summarization is most critical for effective data management. Classical matrix decomposition techniques have often been used for this purpose, and have been the subject of much study. In recent years, several other forms of decomposition, including Boolean Matrix Decomposition have become of significant practical interest. Since much of the data collected is categorical in nature, it can be viewed in terms of a Boolean matrix. Boolean matrix decomposition (BMD), wherein a boolean matrix is expressed as a product of two Boolean matrices, can be used to provide concise and interpretable representations of Boolean data sets. The decomposed matrices give the set of meaningful concepts and their combination which can be used to reconstruct the original data. Such decompositions are useful in a number of application domains including role engineering, text mining as well as knowledge discovery from databases. In this seminar, we look at the theory underlying the BMD problem, study some of its variants and solutions, and examine different practical applications.

#index 1846727
#* Querying Uncertain Spatio-Temporal Data
#@ Tobias Emrich;Hans-Peter Kriegel;Nikos Mamoulis;Matthias Renz;Andreas Zufle
#t 2012
#c 17
#! The problem of modeling and managing uncertain data has received a great deal of interest, due to its manifold applications in spatial, temporal, multimedia and sensor databases. There exists a wide range of work covering spatial uncertainty in the static (snapshot) case, where only one point of time is considered. In contrast, the problem of modeling and querying uncertain spatio-temporal data has only been treated as a simple extension of the spatial case, disregarding time dependencies between consecutive timestamps. In this work, we present a framework for efficiently modeling and querying uncertain spatio-temporal data. The key idea of our approach is to model possible object trajectories by stochastic processes. This approach has three major advantages over previous work. First it allows answering queries in accordance with the possible worlds model. Second, dependencies between object locations at consecutive points in time are taken into account. And third it is possible to reduce all queries on this model to simple matrix multiplications. Based on these concepts we propose efficient solutions for different probabilistic spatio-temporal queries. In an experimental evaluation we show that our approaches are several order of magnitudes faster than state-of-the-art competitors.

#index 1846728
#* The Min-dist Location Selection Query
#@ Jianzhong Qi;Rui Zhang;Lars Kulik;Dan Lin;Yuan Xue
#t 2012
#c 17
#! We propose and study a new type of location optimization problem: given a set of clients and a set of existing facilities, we select a location from a given set of potential locations for establishing a new facility so that the average distance between a client and her nearest facility is minimized. We call this problem the min-dist location selection problem, which has a wide range of applications in urban development simulation, massively multiplayer online games, and decision support systems. We explore two common approaches to location optimization problems and propose methods based on those approaches for solving this new problem. However, those methods either need to maintain an extra index or fall short in efficiency. To address their drawbacks, we propose a novel method (named MND), which has very close performance to the fastest method but does not need an extra index. We provide a detailed comparative cost analysis on the various algorithms. We also perform extensive experiments to evaluate their empirical performance and validate the efficiency of the MND method.

#index 1846729
#* Bi-level Locality Sensitive Hashing for k-Nearest Neighbor Computation
#@ Jia Pan;Dinesh Manocha
#t 2012
#c 17
#! We present a new Bi-level LSH algorithm to perform approximate $k$-nearest neighbor search in high dimensional spaces. Our formulation is based on a two-level scheme. In the first level, we use a RP-tree that divides the dataset into sub-groups with bounded aspect ratios and is used to distinguish well-separated clusters. During the second level, we compute a single LSH hash table for each sub-group along with a hierarchical structure based on space-filling curves. Given a query, we first determine the sub-group that it belongs to and perform $k$-nearest neighbor search within the suitable buckets in the LSH hash table corresponding to the sub-group. Our algorithm also maps well to current GPU architectures and can improve the quality of approximate KNN queries as compared to prior LSH-based algorithms. We highlight its performance on two large, high-dimensional image datasets. Given a runtime budget, Bi-level LSH can provide better accuracy in terms of recall or error ration. Moreover, our formulation reduces the variation in runtime cost or the quality of results.

#index 1846730
#* Learning-based Query Performance Modeling and Prediction
#@ Mert Akdere;Ugur Çetintemel;Matteo Riondato;Eli Upfal;Stanley B. Zdonik
#t 2012
#c 17
#! Accurate query performance prediction (QPP) is central to effective resource management, query optimization and query scheduling. Analytical cost models, used in current generation of query optimizers, have been successful in comparing the costs of alternative query plans, but they are poor predictors of execution latency. As a more promising approach to QPP, this paper studies the practicality and utility of sophisticated learning-based models, which have recently been applied to a variety of predictive tasks with great success, in both static (i.e., fixed) and dynamic query workloads. We propose and evaluate predictive modeling techniques that learn query execution behavior at different granularities, ranging from coarse-grained plan-level models to fine-grained operator-level models. We demonstrate that these two extremes offer a tradeoff between high accuracy for static workload queries and generality to unforeseen queries in dynamic workloads, respectively, and introduce a hybrid approach that combines their respective strengths by selectively composing them in the process of QPP. We discuss how we can use a training workload to (i) pre-build and materialize such models offline, so that they are readily available for future predictions, and (ii) build new models online as new predictions are needed. All prediction models are built using only static features (available prior to query execution) and the performance values obtained from the offline execution of the training workload. We fully implemented all these techniques and extensions on top of Postgre SQL and evaluated them experimentally by quantifying their effectiveness over analytical workloads, represented by well-established TPC-H data and queries. The results provide quantitative evidence that learning-based modeling for QPP is both feasible and effective for both static and dynamic workload scenarios.

#index 1846731
#* SMIX Live -- A Self-Managing Index Infrastructure for Dynamic Workloads
#@ Thomas Kissinger;Hannes Voigt;Wolfgang Lehner
#t 2012
#c 17
#! As databases accumulate growing amounts of data at an increasing rate, adaptive indexing becomes more and more important. At the same time, applications and their use get more agile and flexible, resulting in less steady and less predictable workload characteristics. Being inert and coarse-grained, state-of-the-art index tuning techniques become less useful in such environments. Especially the full-column indexing paradigm results in lot of indexed but never queried data and prohibitively high memory and maintenance costs. In our demonstration, we present Self-Managing Indexes, a novel, adaptive, fine-grained, autonomous indexing infrastructure. In its core, our approach builds on a novel access path that automatically collects useful index information, discards useless index information, and competes with its kind for resources to host its index information. Compared to existing technologies for adaptive indexing, we are able to dynamically grow and shrink our indexes, instead of incrementally enhancing the index granularity. In the demonstration, we visualize performance and system measures for different scenarios and allow the user to interactively change several system parameters.

#index 1846732
#* Multi-query Stream Processing on FPGAs
#@ Mohammad Sadoghi;Rija Javed;Naif Tarafdar;Harsh Singh;Rohan Palaniappan;Hans-Arno Jacobsen
#t 2012
#c 17
#! We present an efficient multi-query event stream platform to support query processing over high-frequency event streams. Our platform is built over reconfigurable hardware -- FPGAs -- to achieve line-rate multi-query processing by exploiting unprecedented degrees of parallelism and potential for pipelining, only available through custom-built, application-specific and low-level logic design. Moreover, a multi-query event stream processing engine is at the core of a wide range of applications including real-time data analytics, algorithmic trading, targeted advertisement, and (complex) event processing.

#index 1846733
#* EUDEMON: A System for Online Video Frame Copy Detection by Earth Mover's Distance
#@ Jia Xu;Qiushi Bai;Yu Gu;Anthony K. H. Tung;Guoren Wang;Ge Yu;Zhenjie Zhang
#t 2012
#c 17
#! The Earth Mover's Distance, or EMD for short, has been proven to be effective for content-based image retrieval. However, due to the cubic complexity of EMD computation, it remains difficult to use EMD in applications with stringent requirement for efficiency. In this paper, we present our new system, called EUDEMON, which utilizes new techniques to support fast Online Video Frame Copy Detection based on the EMD. Given a group of registered frames as queries and a set of targeted detection videos, EUDEMON is capable of identifying relevant frames from the video stream in real time. The significant improvement on efficiency mainly relies on the primal-dual theory in linear programming and well-designed B+ tree filters for adaptive candidate pruning. Generally speaking, our system includes a variety of new features crucial to the deployment of EUDEMON in real applications. First, EUDEMON achieves high throughput even when a large number of queries are registered in the system. Second, EUDEMON contains self-optimization component to automatically enhance the effectiveness of the filters based on the recent content of the video stream. Finally, EUDEMON provides a user-friendly visualization interface, named EMD Flow Chart, to help the users to better understand the alarm with the perspective of the EMD.

#index 1846734
#* A Dataset Search Engine for the Research Document Corpus
#@ Meiyu Lu;Srinivas Bangalore;Graham Cormode;Marios Hadjieleftheriou;Divesh Srivastava
#t 2012
#c 17
#! A key step in validating a proposed idea or system is to evaluate over a suitable dataset. However, to this date there have been no useful tools for researchers to understand which datasets have been used for what purpose, or in what prior work. Instead, they have to manually browse through papers to find the suitable datasets and their corresponding URLs, which is laborious and inefficient. To better aid the dataset discovery process, and provide a better understanding of how and where datasets have been used, we propose a framework to effectively identify datasets within the scientific corpus. The key technical challenges are identification of datasets, and discovery of the association between a dataset and the URLs where they can be accessed. Based on this, we have built a user friendly web-based search interface for users to conveniently explore the dataset-paper relationships, and find relevant datasets and their properties.

#index 1846735
#* AskFuzzy: Attractive Visual Fuzzy Query Builder
#@ Keivan Kianmehr;Negar Koochakzadeh;Reda Alhajj
#t 2012
#c 17
#! The user-centric query interface is very common application that allows expressing both the input and the output using fuzzy terms. This is becoming a need in the evolving internet-based era where web-based applications are very common and the number of users accessing structured databases is increasing rapidly. Restricting the user group to only experts in query coding must be avoided. The Ask Fuzzy system has been developed to address this vital issue which has social and industrial impact. It is an attractive and friendly visual user interface that facilitates expressing queries using both fuzziness and traditional methods. The fuzziness is not expressed explicitly inside the database, it is rather absorbed and effectively handled by an intermediate layer which is cleverly incorporated between the front-end visual user-interface and the back-end database.

#index 1846736
#* F2DB: The Flash-Forward Database System
#@ Ulrike Fischer;Frank Rosenthal;Wolfgang Lehner
#t 2012
#c 17
#! Forecasts are important to decision-making and risk assessment in many domains. Since current database systems do not provide integrated support for forecasting, it is usually done outside the database system by specially trained experts using forecast models. However, integrating model-based forecasting as a first-class citizen inside a DBMS speeds up the forecasting process by avoiding exporting the data and by applying database-related optimizations like reusing created forecast models. It especially allows subsequent processing of forecast results inside the database. In this demo, we present our prototype F2DB based on PostgreSQL, which allows for transparent processing of forecast queries. Our system automatically takes care of model maintenance when the underlying dataset changes. In addition, we offer optimizations to save maintenance costs and increase accuracy by using derivation schemes for multidimensional data. Our approach reduces the required expert knowledge by enabling arbitrary users to apply forecasting in a declarative way.

#index 1846737
#* Parametric Plan Caching Using Density-Based Clustering
#@ Gunes Aluç;David E. DeHaan;Ivan T. Bowman
#t 2012
#c 17
#! Query plan caching eliminates the need for repeated query optimization, hence, it has strong practical implications for relational database management systems (RDBMSs). Unfortunately, existing approaches consider only the query plan generated at the expected values of parameters that characterize the query, data and the current state of the system, while these parameters may take different values during the lifetime of a cached plan. A better alternative is to harvest the optimizer's plan choice for different parameter values, populate the cache with promising query plans, and select a cached plan based upon current parameter values. To address this challenge, we propose a parametric plan caching (PPC) framework that uses an online plan space clustering algorithm. The clustering algorithm is density-based, and it exploits locality-sensitive hashing as a pre-processing step so that clusters in the plan spaces can be efficiently stored in database histograms and queried in constant time. We experimentally validate that our approach is precise, efficient in space-and-time and adaptive, requiring no eager exploration of the plan spaces of the optimizer.

#index 1846738
#* Effective and Robust Pruning for Top-Down Join Enumeration Algorithms
#@ Pit Fender;Guido Moerkotte;Thomas Neumann;Viktor Leis
#t 2012
#c 17
#! Finding the optimal execution order of join operations is a crucial task of today's cost-based query optimizers. There are two approaches to identify the best plan: bottom-up and top-down join enumeration. For both optimization strategies efficient algorithms have been published. However, only the top-down approach allows for branch-and-bound pruning. Two pruning techniques can be found in the literature. We add six new ones. Combined, they improve performance roughly by an average factor of 2 - 5. Even more important, our techniques improve the worst case by two orders of magnitude. Additionally, we introduce a new, very efficient, and easy to implement top-down join enumeration algorithm. This algorithm, together with our improved pruning techniques, yields a performance which is by an average factor of 6 - 9 higher than the performance of the original top-down enumeration algorithm with the original pruning methods.

#index 1846739
#* Towards Preference-aware Relational Databases
#@ Anastasios Arvanitis;Georgia Koutrika
#t 2012
#c 17
#! In implementing preference-aware query processing, a straightforward option is to build a plug-in on top of the database engine. However, treating the DBMS as a black box affects both the expressivity and performance of queries with preferences. In this paper, we argue that preference-aware query processing needs to be pushed closer to the DBMS. We present a preference-aware relational data model that extends database tuples with preferences and an extended algebra that captures the essence of processing queries with preferences. A key novelty of our preference model itself is that it defines a preference in three dimensions showing the tuples affected, their preference scores and the credibility of the preference. Our query processing strategies push preference evaluation inside the query plan and leverage its algebraic properties for finer-grained query optimization. We experimentally evaluate the proposed strategies. Finally, we compare our framework to a pure plug-in implementation and we show its feasibility and advantages.

#index 1846740
#* A Foundation for Efficient Indoor Distance-Aware Query Processing
#@ Hua Lu;Xin Cao;Christian S. Jensen
#t 2012
#c 17
#! Indoor spaces accommodate large numbers of spatial objects, e.g., points of interest (POIs), and moving populations. A variety of services, e.g., location-based services and security control, are relevant to indoor spaces. Such services can be improved substantially if they are capable of utilizing indoor distances. However, existing indoor space models do not account well for indoor distances. To address this shortcoming, we propose a data management infrastructure that captures indoor distance and facilitates distance-aware query processing. In particular, we propose a distance-aware indoor space model that integrates indoor distance seamlessly. To enable the use of the model as a foundation for query processing, we develop accompanying, efficient algorithms that compute indoor distances for different indoor entities like doors as well as locations. We also propose an indexing framework that accommodates indoor distances that are pre-computed using the proposed algorithms. On top of this foundation, we develop efficient algorithms for typical indoor, distance-aware queries. The results of an extensive experimental evaluation demonstrate the efficacy of the proposals.

#index 1846741
#* Provenance-Based Debugging and Drill-Down in Data-Oriented Workflows
#@ Robert Ikeda;Junsang Cho;Charlie Fang;Semih Salihoglu;Satoshi Torikai;Jennifer Widom
#t 2012
#c 17
#! Panda (for Provenance and Data) is a system that supports the creation and execution of data-oriented workflows, with automatic provenance generation and built-in provenance tracing operations. Workflows in Panda are arbitrary a cyclic graphs containing both relational (SQL) processing nodes and opaque processing nodes programmed in Python. For both types of nodes, Panda generates logical provenance -- provenance information stored at the processing-node level -- and uses the generated provenance to support record-level backward tracing and forward tracing operations. In our demonstration we use Panda to integrate, process, and analyze actual education data from multiple sources. We specifically demonstrate how Panda's provenance generation and tracing capabilities can be very useful for workflow debugging, and for drilling down on specific results of interest.

#index 1846742
#* M3: Stream Processing on Main-Memory MapReduce
#@ Ahmed M. Aly;Asmaa Sallam;Bala M. Gnanasekaran;Long-Van Nguyen-Dinh;Walid G. Aref;Mourad Ouzzani;Arif Ghafoor
#t 2012
#c 17
#! The continuous growth of social web applications along with the development of sensor capabilities in electronic devices is creating countless opportunities to analyze the enormous amounts of data that is continuously steaming from these applications and devices. To process large scale data on large scale computing clusters, MapReduce has been introduced as a framework for parallel computing. However, most of the current implementations of the MapReduce framework support only the execution of fixed-input jobs. Such restriction makes these implementations inapplicable for most streaming applications, in which queries are continuous in nature, and input data streams are continuously received at high arrival rates. In this demonstration, we showcase M$^3$, a prototype implementation of the MapReduce framework in which continuous queries over streams of data can be efficiently answered. M$^3$ extends Hadoop, the open source implementation of MapReduce, bypassing the Hadoop Distributed File System (HDFS) to support main-memory-only processing. Moreover, M$^3$ supports continuous execution of the Map and Reduce phases where individual Mappers and Reducers never terminate.

#index 1846743
#* A Deep Embedding of Queries into Ruby
#@ Torsten Grust;Manuel Mayr
#t 2012
#c 17
#! We demonstrate SWITCH, a deep embedding of relational queries into Ruby and Ruby on Rails. With SWITCH, there is no syntactic or stylistic difference between Ruby programs that operate over in-memory array objects or database-resident tables, even if these programs rely on array order or nesting. SWITCH's built-in compiler and SQL code generator guarantee to emit few queries, addressing long-standing performance problems that trace back to Rails' Active Record database binding. "Looks likes Ruby, but performs like handcrafted SQL, " is the ideal that drives the research and development effort behind SWITCH.

#index 1846744
#* Asking the Right Questions in Crowd Data Sourcing
#@ Rubi Boim;Ohad Greenshpan;Tova Milo;Slava Novgorodov;Neoklis Polyzotis;Wang-Chiew Tan
#t 2012
#c 17
#! Crowd-based data sourcing is a new and powerful data procurement paradigm that engages Web users to collectively contribute information. In this work, we target the problem of gathering data from the crowd in an economical and principled fashion. We present Ask It!, a system that allows interactive data sourcing applications to effectively determine which questions should be directed to which users for reducing the uncertainty about the collected data. Ask It! uses a set of novel algorithms for minimizing the number of probing (questions) required from the different users. We demonstrate the challenge and our solution in the context of a multiple-choice question game played by the ICDE'12 attendees, targeted to gather information on the conference's publications, authors and colleagues.

#index 1846745
#* LotusX: A Position-Aware XML Graphical Search System with Auto-Completion
#@ Chunbin Lin;Jiaheng Lu;Tok Wang Ling;Bogdan Cautis
#t 2012
#c 17
#! The existing query languages for XML (e.g., XQuery) require professional programming skills to be formulated, however, such complex query languages burden the query processing. In addition, when issuing an XML query, users are required to be familiar with the content (including the structural and textual information) of the hierarchical XML, which is diffcult for common users. The need for designing user friendly interfaces to reduce the burden of query formulation is fundamental to the spreading of XML community. We present a twig-based XML graphical search system, called LotusX, that provides a graphical interface to simplify the query processing without the need of learning query language and data schemas and the knowledge of the content of the XML document. The basic idea is that LotusX proposes "position-aware" and "auto-completion" features to help users to create tree-modeled queries (twig pattern) by providing the possible candidates on-the-fly. In addition, complex twig queries (including order sensitive queries) are supported in LotusX. Furthermore, a new ranking strategy and a query rewriting solution are implemented to rank and rewrite the query effectively. We provide an online demo for LotusX system: http://datasearch.ruc.edu.cn:8080/LotusX.

#index 1846746
#* Efficient Top-k Keyword Search in Graphs with Polynomial Delay
#@ Mehdi Kargar;Aijun An
#t 2012
#c 17
#! A system for efficient keyword search in graphs is demonstrated. The system has two components, a search through only the nodes containing the input keywords for a set of nodes that are close to each other and together cover the input keywords and an exploration for finding how these nodes are related to each other. The system generates all or top-$k$ answers in polynomial delay. Answers are presented to the user according to a ranking criterion so that the answers with nodes closer to each other are presented before the ones with nodes farther away from each other. In addition, the set of answers produced by our system is duplication free. The system uses two methods for presenting the final answer to the user. The presentation methods reveal relationships among the nodes in an answer through a tree or a multi-center graph. We will show that each method has its own advantages and disadvantages. The system is demonstrated using two challenging datasets, very large DBLP and highly cyclic Mondial. Challenges and difficulties in implementing an efficient keyword search system are also demonstrated.

#index 1846747
#* LARS: A Location-Aware Recommender System
#@ Justin J. Levandoski;Mohamed Sarwat;Ahmed Eldawy;Mohamed F. Mokbel
#t 2012
#c 17
#! This paper proposes LARS, a location-aware recommender system that uses location-based ratings to produce recommendations. Traditional recommender systems do not consider spatial properties of users nor items, LARS, on the other hand, supports a taxonomy of three novel classes of location-based ratings, namely, spatial ratings for non-spatial items, non-spatial ratings for spatial items, and spatial ratings for spatial items. LARS exploits user rating locations through user partitioning, a technique that influences recommendations with ratings spatially close to querying users in a manner that maximizes system scalability while not sacrificing recommendation quality. LARS exploits item locations using travel penalty, a technique that favors recommendation candidates closer in travel distance to querying users in a way that avoids exhaustive access to all spatial items. LARS can apply these techniques separately, or in concert, depending on the type of location-based rating available. Experimental evidence using large-scale real-world data from both the Foursquare location-based social network and the Movie Lens movie recommendation system reveals that LARS is efficient, scalable, and capable of producing recommendations twice as accurate compared to existing recommendation approaches.

#index 1846748
#* Approximate Shortest Distance Computing: A Query-Dependent Local Landmark Scheme
#@ Miao Qiao;Hong Cheng;Lijun Chang;Jeffrey Xu Yu
#t 2012
#c 17
#! Shortest distance query between two nodes is a fundamental operation in large-scale networks. Most existing methods in the literature take a landmark embedding approach, which selects a set of graph nodes as landmarks and computes the shortest distances from each landmark to all nodes as an embedding. To handle a shortest distance query between two nodes, the precomputed distances from the landmarks to the query nodes are used to compute an approximate shortest distance based on the triangle inequality. In this paper, we analyze the factors that affect the accuracy of the distance estimation in the landmark embedding approach. In particular we find that a globally selected, query-independent landmark set plus the triangulation based distance estimation introduces a large relative error, especially for nearby query nodes. To address this issue, we propose a query-dependent local landmark scheme, which identifies a local landmark close to the specific query nodes and provides a more accurate distance estimation than the traditional global landmark approach. Specifically, a local landmark is defined as the least common ancestor of the two query nodes in the shortest path tree rooted at a global landmark. We propose efficient local landmark indexing and retrieval techniques, which are crucial to achieve low offline indexing complexity and online query complexity. Two optimization techniques on graph compression and graph online search are also proposed, with the goal to further reduce index size and improve query accuracy. Our experimental results on large-scale social networks and road networks demonstrate that the local landmark scheme reduces the shortest distance estimation error significantly when compared with global landmark embedding.

#index 1846749
#* DESKS: Direction-Aware Spatial Keyword Search
#@ Guoliang Li;Jianhua Feng;Jing Xu
#t 2012
#c 17
#! Location-based services (LBS) have been widely accepted by mobile users. Many LBS users have direction-aware search requirement that answers must be in the search direction. However to the best of our knowledge there is not yet any research available that investigates direction-aware search. A straightforward method first finds candidates without considering the direction constraint, and then generates the answers by pruning those candidates which invalidate the direction constraint. However this method is rather expensive as it involves a lot of useless computation on many unnecessary directions. To address this problem, we propose a direction-aware spatial keyword search method which inherently supports direction-aware search. We devise novel direction-aware indexing structures to prune unnecessary directions. We develop effective pruning techniques and search algorithms to efficiently answer a direction-aware query. As users may dynamically change their search directions, we propose to incrementally answer a query. Experimental results on real datasets show that our method achieves high performance and outperforms existing methods significantly.

#index 1846750
#* Fuzzy Joins Using MapReduce
#@ Foto N. Afrati;Anish Das Sarma;David Menestrina;Aditya Parameswaran;Jeffrey D. Ullman
#t 2012
#c 17
#! Fuzzy/similarity joins have been widely studied in the research community and extensively used in real-world applications. This paper proposes and evaluates several algorithms for finding all pairs of elements from an input set that meet a similarity threshold. The computation model is a single MapReduce job. Because we allow only one MapReduce round, the Reduce function must be designed so a given output pair is produced by only one task, for many algorithms, satisfying this condition is one of the biggest challenges. We break the cost of an algorithm into three components: the execution cost of the mappers, the execution cost of the reducers, and the communication cost from the mappers to reducers. The algorithms are presented first in terms of Hamming distance, but extensions to edit distance and Jaccard distance are shown as well. We find that there are many different approaches to the similarity-join problem using MapReduce, and none dominates the others when both communication and reducer costs are considered. Our cost analyses enable applications to pick the optimal algorithm based on their communication, memory, and cluster requirements.

#index 1846751
#* TEDAS: A Twitter-based Event Detection and Analysis System
#@ Rui Li;Kin Hou Lei;Ravi Khadiwala;Kevin Chen-Chuan Chang
#t 2012
#c 17
#! Witnessing the emergence of Twitter, we propose a Twitter-based Event Detection and Analysis System (TEDAS), which helps to (1) detect new events, to (2) analyze the spatial and temporal pattern of an event, and to (3) identify importance of events. In this demonstration, we show the overall system architecture, explain in detail the implementation of the components that crawl, classify, and rank tweets and extract location from tweets, and present some interesting results of our system.

#index 1846752
#* AutoDict: Automated Dictionary Discovery
#@ Fei Chiang;Periklis Andritsos;Erkang Zhu;Renee J. Miller
#t 2012
#c 17
#! An attribute dictionary is a set of attributes together with a set of common values of each attribute. Such dictionaries are valuable in understanding unstructured or loosely structured textual descriptions of entity collections, such as product catalogs. Dictionaries provide the supervised data for learning product or entity descriptions. In this demonstration, we will present Auto Dict, a system that analyzes input data records, and discovers high quality dictionaries using information theoretic techniques. To the best of our knowledge, Auto Dict is the first end-to-end system for building attribute dictionaries. Our demonstration will showcase the different information analysis and extraction features within Auto Dict, and highlight the process of generating high quality attribute dictionaries.

#index 1846753
#* Trust and Share: Trusted Information Sharing in Online Social Networks
#@ Barbara Carminati;Elena Ferrari;Jacopo Girardi
#t 2012
#c 17
#! Trust & Share (T&S) aims at providing relationship-based access control in the Facebook realm. T&S is a third-party Facebook application, designed to support a flexible and controlled sharing of user data. It makes users able to upload resources (i.e., any file) and specify for each of them which users have to be authorized by T&S to access them. To enforce this controlled information sharing, T&S relies on the OSN access control model proposed in \cite{tissec}, where social network relationships have an enhanced semantics than the contacts in Facebook. According to \cite{tissec}, OSN users associate with each of their contacts a {\em type}, representing the nature of the relationship (e.g., friends, colleagues, parents). Moreover, the creator of the relationship can assign to it also a trust level to represent the strength of the connection. This graph enables users to specify more expressive rules for the controlled information sharing. Indeed, on top of this enhanced social graph, T\&S users can specify access constraints on the type, trust level and depth of the relationship it must exist with a given Facebook contact in order to access a certain resource.

#index 1846754
#* Evaluation of Clusterings -- Metrics and Visual Support
#@ Elke Achtert;Sascha Goldhofer;Hans-Peter Kriegel;Erich Schubert;Arthur Zimek
#t 2012
#c 17
#! When comparing clustering results, any evaluation metric breaks down the available information to a single number. However, a lot of evaluation metrics are around, that are not always concordant nor easily interpretable in judging the agreement of a pair of clusterings. Here, we provide a tool to visually support the assessment of clustering results in comparing multiple clusterings. Along the way, the suitability of a couple of clustering comparison measures can be judged in different scenarios.

#index 1846755
#* Horton: Online Query Execution Engine for Large Distributed Graphs
#@ Mohamed Sarwat;Sameh Elnikety;Yuxiong He;Gabriel Kliot
#t 2012
#c 17
#! Graphs are used in many large-scale applications, such as social networking. The management of these graphs poses new challenges as such graphs are too large for a single server to manage efficiently. Current distributed techniques such as map-reduce and Pregel are not well-suited to processing interactive ad-hoc queries against large graphs. In this paper we demonstrate Horton, a distributed interactive query execution engine for large graphs. Horton defines a query language that allows the expression of regular language reach ability queries and provides a query execution engine with a query optimizer that allows interactive execution of queries on large distributed graphs in parallel. In the demo, we show the functionality of Horton managing a large graph for a social networking application called Codebook, whose graph represents data on software components, developers, development artifacts such as bug reports, and their interactions in large software projects.

#index 1846756
#* MXQuery with Hardware Acceleration
#@ Peter M. Fischer;Jens Teubner
#t 2012
#c 17
#! We demonstrate MXQuery/H, a modified version of MXQuery that uses hardware acceleration to speed up XML processing. The main goal of this demonstration is to give an interactive example of hardware/software co-design and show how system performance and energy efficiency can be improved by off-loading tasks to FPGA hardware. To this end, we equipped MXQuery/H with various hooks to inspect the different parts of the system. Besides that, our system can finally really leverage the idea of XML projection. Though the idea of projection had been around for a while, its effectiveness remained always limited because of the unavoidable and high parsing overhead. By performing the task in hardware, we relieve the software part from this overhead and achieve processing speed-ups of several factors.

#index 1846757
#* Data3 -- A Kinect Interface for OLAP Using Complex Event Processing
#@ Steffen Hirte;Andreas Seifert;Stephan Baumann;Daniel Klan;Kai-Uwe Sattler
#t 2012
#c 17
#! Motion sensing input devices like Microsoft's Kinect offer an alternative to traditional computer input devices like keyboards and mouses. Daily new applications using this interface appear. Most of them implement their own gesture detection. In our demonstration we show a new approach using the data stream engine Andu IN. The gesture detection is done based on Andu IN's complex event processing functionality. This way we build a system that allows to define new and complex gestures on the basis of a declarative programming interface. On this basis our demonstration data^3 provides a basic natural interaction OLAP interface for a sample star schema database using Microsoft's Kinect.

#index 1846758
#* Parallel Top-K Similarity Join Algorithms Using MapReduce
#@ Younghoon Kim;Kyuseok Shim
#t 2012
#c 17
#! There is a wide range of applications that require finding the top-k most similar pairs of records in a given database. However, computing such top-k similarity joins is a challenging problem today, as there is an increasing trend of applications that expect to deal with vast amounts of data. For such data-intensive applications, parallel executions of programs on a large cluster of commodity machines using the MapReduce paradigm have recently received a lot of attention. In this paper, we investigate how the top-k similarity join algorithms can get benefits from the popular MapReduce framework. We first develop the divide-and-conquer and branch-and-bound algorithms. We next propose the all pair partitioning and essential pair partitioning methods to minimize the amount of data transfers between map and reduce functions. We finally perform the experiments with not only synthetic but also real-life data sets. Our performance study confirms the effectiveness and scalability of our MapReduce algorithms.

#index 1846759
#* Load Balancing in MapReduce Based on Scalable Cardinality Estimates
#@ Benjamin Gufler;Nikolaus Augsten;Angelika Reiser;Alfons Kemper
#t 2012
#c 17
#! MapReduce has emerged as a popular tool for distributed and scalable processing of massive data sets and is being used increasingly in e-science applications. Unfortunately, the performance of MapReduce systems strongly depends on an even data distribution while scientific data sets are often highly skewed. The resulting load imbalance, which raises the processing time, is even amplified by high runtime complexity of the reducer tasks. An adaptive load balancing strategy is required for appropriate skew handling. In this paper, we address the problem of estimating the cost of the tasks that are distributed to the reducers based on a given cost model. An accurate cost estimation is the basis for adaptive load balancing algorithms and requires to gather statistics from the mappers. This is challenging: (a) Since the statistics from all mappers must be integrated, the mapper statistics must be small. (b) Although each mapper sees only a small fraction of the data, the integrated statistics must capture the global data distribution. (c) The mappers terminate after sending the statistics to the controller, and no second round is possible. Our solution to these challenges consists of two components. First, a monitoring component executed on every mapper captures the local data distribution and identifies its most relevant subset for cost estimation. Second, an integration component aggregates these subsets approximating the global data distribution.

#index 1846760
#* Community Detection with Edge Content in Social Media Networks
#@ Guo-Jun Qi;Charu C. Aggarwal;Thomas Huang
#t 2012
#c 17
#! The problem of community detection in social media has been widely studied in the social networking community in the context of the structure of the underlying graphs. Most community detection algorithms use the links between the nodes in order to determine the dense regions in the graph. These dense regions are the communities of social media in the graph. Such methods are typically based purely on the linkage structure of the underlying social media network. However, in many recent applications, edge content is available in order to provide better supervision to the community detection process. Many natural representations of edges in social interactions such as shared images and videos, user tags and comments are naturally associated with content on the edges. While some work has been done on utilizing node content for community detection, the presence of edge content presents unprecedented opportunities and flexibility for the community detection process. We will show that such edge content can be leveraged in order to greatly improve the effectiveness of the community detection process in social media networks. We present experimental results illustrating the effectiveness of our approach.

#index 1846761
#* Cross Domain Search by Exploiting Wikipedia
#@ Chen Liu;Sai Wu;Shouxu Jiang;Anthony K. H. Tung
#t 2012
#c 17
#! The abundance of Web 2.0 resources in various media formats calls for better resource integration to enrich user experience. This naturally leads to a new cross-modal resource search requirement, in which a query is a resource in one modal and the results are closely related resources in other modalities. With cross-modal search, we can better exploit existing resources. Tags associated with Web 2.0 resources are intuitive medium to link resources with different modality together. However, tagging is by nature an ad hoc activity. They often contain noises and are affected by the subjective inclination of the tagger. Consequently, linking resources simply by tags will not be reliable. In this paper, we propose an approach for linking tagged resources to concepts extracted from Wikipedia, which has become a fairly reliable reference over the last few years. Compared to the tags, the concepts are therefore of higher quality. We develop effective methods for cross-modal search based on the concepts associated with resources. Extensive experiments were conducted, and the results show that our solution achieves good performance.

#index 1846762
#* Learning Stochastic Models of Information Flow
#@ Luke Dickens;Ian Molloy;Jorge Lobo;Pau-Chen Cheng;Alessandra Russo
#t 2012
#c 17
#! An understanding of information flow has many applications, including for maximizing marketing impact on social media, limiting malware propagation, and managing undesired disclosure of sensitive information. This paper presents scalable methods for both learning models of information flow in networks from data, based on the Independent Cascade Model, and predicting probabilities of unseen flow from these models. Our approach is based on a principled probabilistic construction and results compare favourably with existing methods in terms of accuracy of prediction and scalable evaluation, with the addition that we are able to evaluate a broader range of queries than previously shown, including probability of joint and/or conditional flow, as well as reflecting model uncertainty. Exact evaluation of flow probabilities is exponential in the number of edges and naive sampling can also be expensive, so we propose sampling in an efficient Markov-Chain Monte-Carlo fashion using the Metropolis-Hastings algorithm--details described in the paper. We identify two types of data, those where the paths of past flows are known--attributed data, and those where only the endpoints are known--unattributed data. Both data types are addressed in this paper, including training methods, example real world data sets, and experimental evaluation. In particular, we investigate flow data from the Twitter microblogging service, exploring the flow of messages through retweets (tweet forwards) for the attributed case, and the propagation of hash tags (metadata tags) and urls for the unattributed case.

#index 1846763
#* Analyzing Query Optimization Process: Portraits of Join Enumeration Algorithms
#@ Anisoara Nica;Ian Charlesworth;Maysum Panju
#t 2012
#c 17
#! Search spaces generated by query optimizers during the optimization process encapsulate characteristics of the join enumeration algorithms, the cost models, as well as critical decisions made for pruning and choosing the best plan. We demonstrate the Join Enumeration Viewer which is a tool designed for visualizing, mining, and comparing plan search spaces generated by different join enumeration algorithms when optimizing same SQL statement. We have enhanced Sybase SQL Anywhere relational database management system to log, in a very compact format, its search space during an optimization process. Such optimization log can then be analyzed by the Join Enumeration Viewer which internally builds the logical and physical plan graphs representing complete and partial plans considered during the optimization process. The optimization logs also contain statistics of the resource consumption during the query optimization such as optimization time breakdown, for example, for logical join enumeration versus costing physical plans, and memory allocation for different optimization structures. The SQL Anywhere Optimizer implements a highly adaptable, self-managing, search space generation algorithm by having several join enumeration algorithms to choose from, each enhanced with different ordering and pruning techniques. The emphasis of the demonstration will be on comparing and contrasting these join enumeration algorithms by analyzing their optimization logs. The demonstration scenarios will include optimizing SQL statements under various conditions which will exercise different algorithms, pruning and ordering techniques. These search spaces will then be visualized and compared using the Join Enumeration Viewer.

#index 1846764
#* DPCube: Releasing Differentially Private Data Cubes for Health Information
#@ Yonghui Xiao;James Gardner;Li Xiong
#t 2012
#c 17
#! We propose to demonstrate DPCube, a component in our Health Information DE-identification (HIDE) framework, for releasing differentially private data cubes (or multidimensional histograms) for sensitive data. HIDE is a framework we developed for integrating heterogenous structured and unstructured health information and provides methods for privacy preserving data publishing. The DPCube component provides the differentially private multidimensional data cube release. The DPCube algorithm uses the differentially private access mechanisms as provided by HIDE and guarantees differential privacy for the released data. It utilizes an innovative two-step multidimensional partitioning technique to publish a generalized data cube or multi-dimensional histogram that achieve good utility while satisfying the privacy requirement. We demonstrate that the released data cubes can serve as a sanitized synopsis of the raw database and, together with an optional synthesized dataset based on the data cubes, can support various Online Analytical Processing (OLAP) queries and learning tasks.

#index 1846765
#* NYAYA: A System Supporting the Uniform Management of Large Sets of Semantic Data
#@ Roberto De Virgilio;Giorgio Orsi;Letizia Tanca;Riccardo Torlone
#t 2012
#c 17
#! We present Nyaya, a flexible system for the management of large-scale semantic data which couples a general-purpose storage mechanism with efficient ontological query answering. Nyaya rapidly imports semantic data expressed in different formalisms into semantic data kiosks. Each kiosk exposes the native ontological constraints in a uniform fashion using data log+-, a very general rule-based language for the representation of ontological constraints. A group of kiosks forms a semantic data market where the data in each kiosk can be uniformly accessed using conjunctive queries and where users can specify user-defined constraints over the data. Nyaya is easily extensible and robust to updates of both data and meta-data in the kiosk and can readily adapt to different logical organizations of the persistent storage. In the demonstration, we will show the capabilities of Nyaya over real-world case studies and demonstrate its efficiency over well-known benchmarks.

#index 1846766
#* R2DB: A System for Querying and Visualizing Weighted RDF Graphs
#@ Songling Liu;Juan P. Cedeno;K. Selçuk Candan;Maria Luisa Sapino;Shengyu Huang;Xinsheng Li
#t 2012
#c 17
#! Existing RDF query languages and RDF stores fail to support a large class of knowledge applications which associate utilities or costs on the available knowledge statements. A recent proposal includes (a) a ranked RDF (R2DF) specification to enhance RDF triples with an application specific weights and (b) a SPA Rank QL query language specification, which provides novel primitives on top of the SPARQL language to express top-k queries using traditional query patterns as well as novel flexible path predicates. We introduce and demonstrate R2DB, a database system for querying weighted RDF graphs. R2DB relies on the AR2Q query processing engine, which leverages novel index structures to support efficient ranked path search and includes query optimization strategies based on proximity and sub-result inter-arrival times. In addition to being the first data management system for the R2DF data model, R2DB also provides an innovative features-of-interest (FoI) based method for visualizing large sets of query results (i.e., sub graphs of the data graph).

#index 1846767
#* Project Daytona: Data Analytics as a Cloud Service
#@ Roger S. Barga;Jaliya Ekanayake;Wei Lu
#t 2012
#c 17
#! Spreadsheets are established data collection and analysis tools in business, technical computing and academic research. Excel, for example, offers an attractive user interface, provides an easy to use data entry model, and offers substantial interactivity for what-if analysis. However, spreadsheets and other common client applications do not offer scalable computation for large scale data analytics and exploration. Increasingly researchers in domains ranging from the social sciences to environmental sciences are faced with a deluge of data, often sitting in spreadsheets such as Excel or other client applications, and they lack a convenient way to explore the data, to find related data sets, or to invoke scalable analytical models over the data. To address these limitations, we have developed a cloud data analytics service based on Daytona, which is an iterative MapReduce runtime optimized for data analytics. In our model, Excel and other existing client applications provide the data entry and user interaction surfaces, Daytona provides a scalable runtime on the cloud for data analytics, and our service seamlessly bridges the gap between the client and cloud. Any analyst can use our data analytics service to discover and import data from the cloud, invoke cloud scale data analytics algorithms to extract information from large datasets, invoke data visualization, and then store the data back to the cloud all through a spreadsheet or other client application they are already familiar with.

#index 1846768
#* Provenance-based Indexing Support in Micro-blog Platforms
#@ Junjie Yao;Bin Cui;Zijun Xue;Qingyun Liu
#t 2012
#c 17
#! Recently, lots of micro-blog message sharing applications have emerged on the web. Users can publish short messages freely and get notified by the subscriptions instantly. Prominent examples include Twitter, Facebook's statuses, and Sina Weibo in China. The Micro-blog platform becomes a useful service for real time information creation and propagation. However, these messages' short length and dynamic characters have posed great challenges for effective content understanding. Additionally, the noise and fragments make it difficult to discover the temporal propagation trail to explore development of micro-blog messages. In this paper, we propose a provenance model to capture connections between micro-blog messages. Provenance refers to data origin identification and transformation logging, demonstrating of great value in recent database and workflow systems. To cope with the real time micro-message deluge, we utilize a novel message grouping approach to encode and maintain the provenance information. Furthermore, we adopt a summary index and several adaptive pruning strategies to implement efficient provenance updating. Based on the index, our provenance solution can support rich query retrieval and intuitive message tracking for effective message organization. Experiments conducted on a real dataset verify the effectiveness and efficiency of our approach. Provenance refers to data origin identification and transformation monitoring, which has been demonstrated of great value in database and workflow systems. In this paper, we propose a provenance model in micro-blog platforms, and design an indexing scheme to support provenance-based message discovery and maintenance, which can capture the interactions of messages for effective message organization. To cope with the real time micro-message tornadoes, we introduce a novel virtual annotation grouping approach to encode and maintain the provenance information. Furthermore, we design a summary index and adaptive pruning strategies to facilitate efficient message update. Based on this provenance index, our approach can support query and message tracking in micro-blog systems. Experiments conducted on real datasets verify the effectiveness and efficiency of our approach.

#index 1846769
#* BestPeer++: A Peer-to-Peer Based Large-Scale Data Processing Platform
#@ Gang Chen;Tianlei Hu;Dawei Jiang;Peng Lu;Kian-Lee Tan;Hoang Tam Vo;Sai Wu
#t 2012
#c 17
#! The corporate network is often used for sharing information among the participating companies and facilitating collaboration in a certain industry sector where companies share a common interest. It can effectively help the companies to reduce their operational costs and increase the revenues. However, the inter-company data sharing and processing poses unique challenges to such a data management system including scalability, performance, throughput, and security. In this paper, we present Best Peer++, a system which delivers elastic data sharing services for corporate network applications in the cloud based on Best Peer--a peer-to-peer (P2P) based data management platform. By integrating cloud computing, database, and P2P technologies into one system, Best Peer++ provides an economical, flexible and scalable platform for corporate network applications and delivers data sharing services to participants based on the widely accepted pay-as-you-go business model. We evaluate Best Peer++ on Amazon EC2 Cloud platform. The benchmarking results show that Best Peer++ outperforms Hadoop DB, a recently proposed large-scale data processing system, in performance when both systems are employed to handle typical corporate network workloads. The benchmarking results also demonstrate that Best Peer++ achieves near linear scalability for throughput with respect to the number of peer nodes.

#index 1846770
#* Effective Data Density Estimation in Ring-Based P2P Networks
#@ Minqi Zhou;Heng Tao Shen;Xiaofang Zhou;Weining Qian;Aoying Zhou
#t 2012
#c 17
#! Estimating the global data distribution in Peer-to-Peer (P2P) networks is an important issue and has yet to be well addressed. It can benefit many P2P applications, such as load balancing analysis, query processing, and data mining. Inspired by the inversion method for random variate generation, in this paper we present a novel model named distribution-free data density estimation for dynamic ring-based P2P networks to achieve high estimation accuracy with low estimation cost regardless of distribution models of the underlying data. It generates random samples for any arbitrary distribution by sampling the global cumulative distribution function and is free from sampling bias. In P2P networks, the key idea for distribution-free estimation is to sample a small subset of peers for estimating the global data distribution over the data domain. Algorithms on computing and sampling the global cumulative distribution function based on which global data distribution is estimated are introduced with detailed theoretical analysis. Our extensive performance study confirms the effectiveness and efficiency of our methods in ring-based P2P networks.

#index 1846771
#* Processing of Rank Joins in Highly Distributed Systems
#@ Christos Doulkeridis;Akrivi Vlachou;Kjetil Nørvåg;Yannis Kotidis;Neoklis Polyzotis
#t 2012
#c 17
#! In this paper, we study efficient processing of rank joins in highly distributed systems, where servers store fragments of relations in an autonomous manner. Existing rank-join algorithms exhibit poor performance in this setting due to excessive communication costs or high latency. We propose a novel distributed rank-join framework that employs data statistics, maintained as histograms, to determine the subset of each relational fragment that needs to be fetched to generate the top-k join results. At the heart of our framework lies a distributed score bound estimation algorithm that produces sufficient score bounds for each relation, that guarantee the correctness of the rank-join result set, when the histograms are accurate. Furthermore, we propose a generalization of our framework that supports approximate statistics, in the case that the exact statistical information is not available. An extensive experimental study validates the efficiency of our framework and demonstrates its advantages over existing methods.

#index 1846772
#* Interactive User Feedback in Ontology Matching Using Signature Vectors
#@ Isabel F. Cruz;Cosmin Stroe;Matteo Palmonari
#t 2012
#c 17
#! When compared to a gold standard, the set of mappings that are generated by an automatic ontology matching process is neither complete nor are the individual mappings always correct. However, given the explosion in the number, size, and complexity of available ontologies, domain experts no longer have the capability to create ontology mappings without considerable effort. We present a solution to this problem that consists of making the ontology matching process interactive so as to incorporate user feedback in the loop. Our approach clusters mappings to identify where user feedback will be most beneficial in reducing the number of user interactions and system iterations. This feedback process has been implemented in the Agreement Maker system and is supported by visual analytic techniques that help users to better understand the matching process. Experimental results using the OAEI benchmarks show the effectiveness of our approach. We will demonstrate how users can interact with the ontology matching process through the Agreement Maker user interface to match real-world ontologies.

#index 1846773
#* DObjects+: Enabling Privacy-Preserving Data Federation Services
#@ Pawel Jurczyk;Li Xiong;Slawomir Goryczka
#t 2012
#c 17
#! The emergence of cloud computing implies and facilitates managing large collections of highly distributed, autonomous, and possibly private databases. While there is an increasing need for services that allow integration and sharing of various data repositories, it remains a challenge to ensure the privacy, interoperability, and scalability for such services. In this paper we demonstrate a scalable and extensible framework that is aimed to enable privacy preserving data federations. The framework is built on top of a distributed mediator-wrapper architecture where nodes can form collaborative groups for secure anonymization and secure query processing when private data need to be accessed. New anonymization models and protocols will be demonstrated that counter potential attacks in the distributed setting.

#index 1846774
#* DRAGOON: An Information Accountability System for High-Performance Databases
#@ Kyriacos E. Pavlou;Richard T. Snodgrass
#t 2012
#c 17
#! Regulations and societal expectations have recently emphasized the need to mediate access to valuable databases, even access by insiders. Fraud occurs when a person, often an insider, tries to hide illegal activity. Companies would like to be assured that such tampering has not occurred, or if it does, that it will be quickly discovered and used to identify the perpetrator. At one end of the compliance spectrum lies the approach of restricting access to information and on the other that of information accountability. We focus on effecting information accountability of data stored in high-performance databases. The demonstrated work ensures appropriate use and thus end-to-end accountability of database information via a continuous assurance technology based on cryptographic hashing techniques. A prototype tamper detection and forensic analysis system named DRAGOON was designed and implemented to determine when tampering(s) occurred and what data were tampered with. DRAGOON is scalable, customizable, and intuitive. This work will show that information accountability is a viable alternative to information restriction for ensuring the correct storage, use, and maintenance of databases on extant DBMSes.

#index 1846775
#* Intuitive Interaction with Encrypted Query Execution in DataStorm
#@ Ken Smith;Ameet Kini;William Wang;Chris Wolf;M. David Allen;Andrew Sillers
#t 2012
#c 17
#! The encrypted execution of database queries promises powerful security protections, however users are currently unlikely to benefit without significant expertise. In this demonstration, we illustrate a simple workflow enabling users to design secure executions of their queries. The Data Storm system demonstrated simplifies both the design and execution of encrypted execution plans, and represents progress toward the challenge of developing a general planner for encrypted query execution.

#index 1846776
#* Exploiting Common Subexpressions for Cloud Query Processing
#@ Yasin N. Silva;Paul-Ake Larson;Jingren Zhou
#t 2012
#c 17
#! Many companies now routinely run massive data analysis jobs--expressed in some scripting language--on large clusters of low-end servers. Many analysis scripts are complex and contain common sub expressions, that is, intermediate results that are subsequently joined and aggregated in multiple different ways. Applying conventional optimization techniques to such scripts will produce plans that execute a common sub expression multiple times, once for each consumer, which is clearly wasteful. Moreover, different consumers may have different physical requirements on the result: one consumer may want it partitioned on a column A and another one partitioned on column B. To find a truly optimal plan, the optimizer must trade off such conflicting requirements in a cost-based manner. In this paper we show how to extend a Cascade-style optimizer to correctly optimize scripts containing common sub expression. The approach has been prototyped in SCOPE, Microsoft's system for massive data analysis. Experimental analysis of both simple and large real-world scripts shows that the extended optimizer produces plans with 21 to 57% lower estimated costs.

#index 1846777
#* Vectorwise: A Vectorized Analytical DBMS
#@ Marcin Zukowski;Mark van de Wiel;Peter Boncz
#t 2012
#c 17
#! Vector wise is a new entrant in the analytical database marketplace whose technology comes straight from innovations in the database research community in the past years. The product has since made waves due to its excellent performance in analytical customer workloads as well as benchmarks. We describe the history of Vector wise, as well as its basic architecture and the experiences in turning a technology developed in an academic context into a commercial-grade product. Finally, we turn our attention to recent performance results, most notably on the TPC-H benchmark at various sizes.

#index 1846778
#* Load Balancing for MapReduce-based Entity Resolution
#@ Lars Kolb;Andreas Thor;Erhard Rahm
#t 2012
#c 17
#! The effectiveness and scalability of MapReduce-based implementations of complex data-intensive tasks depend on an even redistribution of data between map and reduce tasks. In the presence of skewed data, sophisticated redistribution approaches thus become necessary to achieve load balancing among all reduce tasks to be executed in parallel. For the complex problem of entity resolution, we propose and evaluate two approaches for such skew handling and load balancing. The approaches support blocking techniques to reduce the search space of entity resolution, utilize a preprocessing MapReduce job to analyze the data distribution, and distribute the entities of large blocks among multiple reduce tasks. The evaluation on a real cloud infrastructure shows the value and effectiveness of the proposed load balancing approaches.

#index 1846779
#* Mapping XML to a Wide Sparse Table
#@ Liang Jeff Chen;Philip A. Bernstein;Peter Carlin;Dimitrije Filipovic;Michael Rys;Nikita Shamgunov;James F. Terwilliger;Milos Todic;Sasa Tomasevic;Dragan Tomic
#t 2012
#c 17
#! XML is commonly supported by SQL database systems. However, existing mappings of XML to tables can only deliver satisfactory query performance for limited use cases. In this paper, we propose a novel mapping of XML data into one wide table whose columns are sparsely populated. This mapping provides good performance for document types and queries that are observed in enterprise applications but are not supported efficiently by existing work. XML queries are evaluated by translating them into SQL queries over the wide sparsely-populated table. We show how to translate full XPath 1.0 into SQL. Based on the characteristics of the new mapping, we present rewriting optimizations that minimize the number of joins. Experiments demonstrate that query evaluation over the new mapping delivers considerable improvements over existing techniques for the target use cases.

#index 1846780
#* Querying XML Data: As You Shape It
#@ Curtis E. Dyreson;Sourav S. Bhowmick
#t 2012
#c 17
#! A limitation of XQuery is that a programmer has to be familiar with the shape of the data to query it effectively. And if that shape changes, or if the shape is other than what the programmer expects, the query may fail. One way to avoid this limitation is to transform the data into a desired shape. A data transformation is a rearrangement of data into a new shape. In this paper, we present the semantics and implementation of XMorph 2.0, a shape-polymorphic data transformation language for XML. An XMorph program can act as a query guard. The guard both transforms data to the shape needed by the query and determines whether and how the transformation potentially loses information, a transformation that loses information may lead to a query yielding an inaccurate result. This paper describes how to use XMorph as a query guard, gives a formal semantics for shape-to-shape transformations, documents how XMorph determines how a transformation potentially loses information, and describes the XMorph implementation.

#index 1846781
#* Scalable Multi-query Optimization for SPARQL
#@ Wangchao Le;Anastasios Kementsietsidis;Songyun Duan;Feifei Li
#t 2012
#c 17
#! This paper revisits the classical problem of multi-query optimization in the context of RDF/SPARQL. We show that the techniques developed for relational and semi-structured data/query languages are hard, if not impossible, to be extended to account for RDF data model and graph query patterns expressed in SPARQL. In light of the NP-hardness of the multi-query optimization for SPARQL, we propose heuristic algorithms that partition the input batch of queries into groups such that each group of queries can be optimized together. An essential component of the optimization incorporates an efficient algorithm to discover the common sub-structures of multiple SPARQL queries and an effective cost model to compare candidate execution plans. Since our optimization techniques do not make any assumption about the underlying SPARQL query engine, they have the advantage of being portable across different RDF stores. The extensive experimental studies, performed on three popular RDF stores, show that the proposed techniques are effective, efficient and scalable.

#index 1846782
#* Discovering Conservation Rules
#@ Lukasz Golab;Howard Karloff;Flip Korn;Barna Saha;Divesh Srivastava
#t 2012
#c 17
#! Many applications process data in which there exists a ``conservation law'' between related quantities. For example, in traffic monitoring, every incoming event, such as a packet's entering a router or a car's entering an intersection, should ideally have an immediate outgoing counterpart. We propose a new class of constraints -- Conservation Rules -- that express the semantics and characterize the data quality of such applications. We give confidence metrics that quantify how strongly a conservation rule holds and present approximation algorithms (with error guarantees) for the problem of discovering a concise summary of subsets of the data that satisfy a given conservation rule. Using real data, we demonstrate the utility of conservation rules and we show order-of-magnitude performance improvements of our discovery algorithms over naive approaches.

#index 1846783
#* Scalable and Numerically Stable Descriptive Statistics in SystemML
#@ Yuanyuan Tian;Shirish Tatikonda;Berthold Reinwald
#t 2012
#c 17
#! With the exponential growth in the amount of data that is being generated in recent years, there is a pressing need for applying machine learning algorithms to large data sets. SystemML is a framework that employs a declarative approach for large scale data analytics. In SystemML, machine learning algorithms are expressed as scripts in a high-level language, called DML, which is syntactically similar to R. DML scripts are compiled, optimized, and executed in the SystemML runtime that is built on top of MapReduce. As the basis of virtually every quantitative analysis, descriptive statistics provide powerful tools to explore data in SystemML. In this paper, we describe our experience in implementing descriptive statistics in SystemML. In particular, we elaborate on how to overcome the two major challenges: (1) achieving numerical stability while operating on large data sets in a distributed setting of MapReduce, and (2) designing scalable algorithms to compute order statistics in MapReduce. By empirically comparing to algorithms commonly used in existing tools and systems, we demonstrate the numerical accuracy achieved by SystemML. We also highlight the valuable lessons we have learned in this exercise.

#index 1846784
#* Earlybird: Real-Time Search at Twitter
#@ Michael Busch;Krishna Gade;Brian Larson;Patrick Lok;Samuel Luckenbill;Jimmy Lin
#t 2012
#c 17
#! The web today is increasingly characterized by social and real-time signals, which we believe represent two frontiers in information retrieval. In this paper, we present Early bird, the core retrieval engine that powers Twitter's real-time search service. Although Early bird builds and maintains inverted indexes like nearly all modern retrieval engines, its index structures differ from those built to support traditional web search. We describe these differences and present the rationale behind our design. A key requirement of real-time search is the ability to ingest content rapidly and make it searchable immediately, while concurrently supporting low-latency, high-throughput query evaluation. These demands are met with a single-writer, multiple-reader concurrency model and the targeted use of memory barriers. Early bird represents a point in the design space of real-time search engines that has worked well for Twitter's needs. By sharing our experiences, we hope to spur additional interest and innovation in this exciting space.

#index 1846785
#* Data Infrastructure at LinkedIn
#@ Aditya Auradkar;Chavdar Botev;Shirshanka Das;Dave De Maagd;Alex Feinberg;Phanindra Ganti;Lei Gao;Bhaskar Ghosh;Kishore Gopalakrishna;Brendan Harris;Joel Koshy;Kevin Krawez;Jay Kreps;Shi Lu;Sunil Nagaraj;Neha Narkhede;Sasha Pachev;Igor Perisic;Lin Qiao;Tom Quiggle;Jun Rao;Bob Schulman;Abraham Sebastian;Oliver Seeliger;Adam Silberstein;BBoris Shkolnik;Chinmay Soman;Roshan Sumbaly;Kapil Surlaker;Sajid Topiwala;Cuong Tran;Balaji Varadarajan;Jemiah Westerman;Zach White;David Zhang;Jason Zhang
#t 2012
#c 17
#! Linked In is among the largest social networking sites in the world. As the company has grown, our core data sets and request processing requirements have grown as well. In this paper, we describe a few selected data infrastructure projects at Linked In that have helped us accommodate this increasing scale. Most of those projects build on existing open source projects and are themselves available as open source. The projects covered in this paper include: (1) Voldemort: a scalable and fault tolerant key-value store, (2) Data bus: a framework for delivering database changes to downstream applications, (3) Espresso: a distributed data store that supports flexible schemas and secondary indexing, (4) Kafka: a scalable and efficient messaging system for collecting various user activity events and log data.

#index 1846786
#* The Credit Suisse Meta-data Warehouse
#@ Claudio Jossen;Lukas Blunschi;Magdalini Mori;Donald Kossmann;Kurt Stockinger
#t 2012
#c 17
#! This paper describes the meta-data warehouse of Credit Suisse that is productive since 2009. Like most other large organizations, Credit Suisse has a complex application landscape and several data warehouses in order to meet the information needs of its users. The problem addressed by the meta-data warehouse is to increase the agility and flexibility of the organization with regards to changes such as the development of a new business process, a new business analytics report, or the implementation of a new regulatory requirement. The meta-data warehouse supports these changes by providing services to search for information items in the data warehouses and to extract the lineage of information items. One difficulty in the design of such a meta-data warehouse is that there is no standard or well-known meta-data model that can be used to support such search services. Instead, the meta-data structures need to be flexible themselves and evolve with the changing IT landscape. This paper describes the current data structures and implementation of the Credit Suisse meta-data warehouse and shows how its services help to increase the flexibility of the whole organization. A series of example meta-data structures, use cases, and screenshots are given in order to illustrate the concepts used and the lessons learned based on feedback of real business and IT users within Credit Suisse.

#index 1846787
#* Efficient Support of XQuery Update Facility in XML Enabled RDBMS
#@ Zhen Hua Liu;Hui J. Chang;Balasubramanyam Sthanikam
#t 2012
#c 17
#! XQuery Update Facility (XQUF), which provides a declarative way of updating XML, has become recommendation by W3C. The SQL/XML standard, on the other hand, defines XMLType as a column data type in RDBMS environment and defines the standard SQL/XML operator, such as XML Query() to embed XQuery to query XMLType column in RDBMS. Based on this SQL/XML standard, XML enabled RDBMS becomes industrial strength platforms to host XML applications in a standard compliance way by providing XML store and query capability. However, updating XML capability support remains to be proprietary in RDBMS until XQUF becomes the recommendation. XQUF is agnostic of how XML is stored so that propagation of actual update to any persistent XML store is beyond the scope of XQUF. In this paper, we show how XQUF can be incorporated into XML Query() to effectively update XML stored in XMLType column in the environment of XML enabled RDBMS, such as Oracle XMLDB. We present various compile time and run time optimisation techniques to show how XQUF can be efficiently implemented to declaratively update XML stored in RDBMS. We present how our approaches of optimising XQUF for common physical XML storage models: native binary XML storage model and relational decomposition of XML storage model. Although our study is done using Oracle XMLDB, all of the presented optimisation techniques are generic to XML stores that need to support update of persistent XML store and not specific to Oracle XMLDB implementation.

#index 1846788
#* GSLPI: A Cost-Based Query Progress Indicator
#@ Jiexing Li;Rimma V. Nehme;Jeffrey Naughton
#t 2012
#c 17
#! Progress indicators for SQL queries were first published in 2004 with the simultaneous and independent proposals from Chaudhuri et al. and Luo et al. In this paper, we implement both progress indicators in the same commercial RDBMS to investigate their performance. We summarize common cases in which they are both accurate and cases in which they fail to provide reliable estimates. Although there are differences in their performance, much more striking is the similarity in the errors they make due to a common simplifying uniform future speed assumption. While the developers of these progress indicators were aware that this assumption could cause errors, they neither explored how large the errors might be nor did they investigate the feasibility of removing the assumption. To rectify this we propose a new query progress indicator, similar to these early progress indicators but without the uniform speed assumption. Experiments show that on the TPC-H benchmark, on queries for which the original progress indicators have errors up to 30X the query running time, the new progress indicator is accurate to within 10 percent. We also discuss the sources of the errors that still remain and shed some light on what would need to be done to eliminate them.

#index 1846789
#* Micro-Specialization in DBMSes
#@ Rui Zhang;Richard T. Snodgrass;Saumya Debray
#t 2012
#c 17
#! Relational database management systems are general in the sense that they can handle arbitrary schemas, queries, and modifications, this generality is implemented using runtime metadata lookups and tests that ensure that control is channelled to the appropriate code in all cases. Unfortunately, these lookups and tests are carried out even when information is available that renders some of these operations superfluous, leading to unnecessary runtime overheads. This paper introduces micro-specialization, an approach that uses relation- and query-specific information to specialize the DBMS code at runtime and thereby eliminate some of these overheads. We develop a taxonomy of approaches and specialization times and propose a general architecture that isolates most of the creation and execution of the specialized code sequences in a separate DBMS-independent module. Through three illustrative types of micro-specializations applied to Postgre SQL, we show that this approach requires minimal changes to a DBMS and can improve the performance simultaneously across a wide range of queries, modifications, and bulk-loading, in terms of storage, CPU usage, and I/O time of the TPC-H and TPC-C benchmarks.

#index 1846790
#* Towards Multi-tenant Performance SLOs
#@ Willis Lang;Srinath Shankar;Jignesh M. Patel;Ajay Kalhan
#t 2012
#c 17
#! As traditional and mission-critical relational database workloads migrate to the cloud in the form of Database-as-a-Service (DaaS), there is an increasing motivation to provide performance goals in Service Level Objectives (SLOs). Providing such performance goals is challenging for DaaS providers as they must balance the performance that they can deliver to tenants and the data center's operating costs. In general, aggressively aggregating tenants on each server reduces the operating costs but degrades performance for the tenants, and vice versa. In this paper, we present a framework that takes as input the tenant workloads, their performance SLOs, and the server hardware that is available to the DaaS provider, and outputs a cost-effective recipe that specifies how much hardware to provision and how to schedule the tenants on each hardware resource. We evaluate our method and show that it produces effective solutions that can reduce the costs for the DaaS provider while meeting performance goals.

#index 1846791
#* Multi-version Concurrency via Timestamp Range Conflict Management
#@ David Lomet;Alan Fekete;Rui Wang;Peter Ward
#t 2012
#c 17
#! A database supporting multiple versions of records may use the versions to support queries of the past or to increase concurrency by enabling reads and writes to be concurrent. We introduce a new concurrency control approach that enables all SQL isolation levels including serializability to utilize multiple versions to increase concurrency while also supporting transaction time database functionality. The key insight is to manage a range of possible timestamps for each transaction that captures the impact of conflicts that have occurred. Using these ranges as constraints often permits concurrent access where lock based concurrency control would block. This can also allow blocking instead of some aborts that are common in earlier multi-version concurrency techniques. Also, timestamp ranges can be used to conservatively find deadlocks without graph based cycle detection. Thus, our multi-version support can enhance performance of current time data access via improved concurrency, while supporting transaction time functionality.

#index 1846792
#* Automatic Extraction of Structured Web Data with Domain Knowledge
#@ Nora Derouiche;Bogdan Cautis;Talel Abdessalem
#t 2012
#c 17
#! We present in this paper a novel approach for extracting structured data from the Web, whose goal is to harvest real-world items from template-based HTML pages (the structured Web). It illustrates a two-phase querying of the Web, in which an intentional description of the data that is targeted is first provided, in a flexible and widely applicable manner. The extraction process leverages then both the input description and the source structure. Our approach is domain-independent, in the sense that it applies to any relation, either flat or nested, describing real-world items. Extensive experiments on five different domains and comparison with the main state of the art extraction systems from literature illustrate its flexibility and precision. We advocate via our technique that automatic extraction and integration of complex structured data can be done fast and effectively, when the redundancy of the Web meets knowledge over the to-be-extracted data.

#index 1846793
#* Making Unstructured Data SPARQL Using Semantic Indexing in Oracle Database
#@ Souripriya Das;Seema Sundara;Matthew Perry;Jagannathan Srinivasan;Jayanta Banerjee;Aravind Yalamanchi
#t 2012
#c 17
#! This paper describes the Semantic Indexing feature introduced in Oracle Database for indexing unstructured text (document) columns. This capability enables searching for concepts (such as people, places, organizations, and events), in addition to words or phrases, with further options for sense disambiguation and term expansion by consulting knowledge captured in OWL/RDF ontologies. The distinguishing aspects of our approach are: 1) Indexing: Instead of building a traditional inverted index of (annotated) token and/or named entity occurrences, we extract the entities, associations, and events present in a text column data and store them as RDF named graphs in the Oracle Database Semantic Store. This base content can be further augmented with knowledge bases and inferred triples (obtained by applying domain-specific ontologies and rule bases). 2) Querying: Instead of relying on proprietary extensions for specifying a search, we allow users to specify a complete SPARQL query pattern that can capture arbitrarily complex relationships between query terms. We have implemented this feature by introducing a sem_contains SQL operator and the associated sem_indextype indexing scheme. The indexing scheme employs an extensible architecture that supports indexing of unstructured text using native as well as third party text extraction tools. The paper presents a model for the semantic index and querying, describes the feature, and outlines its implementation leveraging Oracle's native support for RDF/OWL storage, inferencing, and querying. We also report a study involving use of this feature on a TREC collection of over 130,000 news articles.

#index 1846794
#* A Meta-language for MDX Queries in eLog Business Solution
#@ Sonia Bergamaschi;Matteo Interlandidi;Mario Longo;Laura Po;Maurizio Vincini
#t 2012
#c 17
#! The adoption of business intelligence technology in industries is growing rapidly. Business managers are not satisfied with ad hoc and static reports and they ask for more flexible and easy to use data analysis tools. Recently, application interfaces that expand the range of operations available to the user, hiding the underlying complexity, have been developed. The paper presents eLog, a business intelligence solution designed and developed in collaboration between the database group of the University of Modena and Reggio Emilia and eBilling, an Italian SME supplier of solutions for the design, production and automation of documentary processes for top Italian companies. eLog enables business managers to define OLAP reports by means of a web interface and to customize analysis indicators adopting a simple meta-language. The framework translates the user's reports into MDX queries and is able to automatically select the data cube suitable for each query. Over 140 medium and large companies have exploited the technological services of eBilling S.p.A. to manage their documents flows. In particular, eLog services have been used by the major media and telecommunications Italian companies and their foreign annex, such as Sky, Media set, H3G, Tim Brazil etc. The largest customer can provide up to 30 millions mail pieces within 6 months (about 200 GB of data in the relational DBMS). In a period of 18 months, eLog could reach 150 millions mail pieces (1 TB of data) to handle.

#index 1846795
#* Answering Why-not Questions on Top-k Queries
#@ Zhian He;Eric Lo
#t 2012
#c 17
#! After decades of effort working on database performance, the quality and the usability of database systems have received more attention in recent years. In particular, the feature of explaining missing tuples in a query result, or the so-called "why-not" questions, has recently become an active topic. In this paper, we study the problem of answering why-not questions on top-k queries. Our motivation is that we know many users love to use top-k queries when they are making multi-criteria decisions. However, they often feel frustrated when they are asked to quantify their feeling as a set of numeric weightings, and feel even more frustrated after they see the query results do not include their expected answers. In this paper, we use the query-refinement method to approach the problem. Given as inputs the original top-k query and a set of missing tuples, our algorithm returns to the user a refined top-k query that includes the missing tuples. A case study and experimental results show that our approach returns high quality explanations to users efficiently.

#index 1846796
#* On Top-k Structural Similarity Search
#@ Pei Lee;Laks V. S. Lakshmanan;Jeffrey Xu Yu
#t 2012
#c 17
#! Search for objects similar to a given query object in a network has numerous applications including web search and collaborative filtering. We use the notion of structural similarity to capture the commonality of two objects in a network, e.g., if two nodes are referenced by the same node, they may be similar. Meeting-based methods including SimRank and P-Rank capture structural similarity very well. Deriving inspiration from PageRank, SimRank has gained popularity by a natural intuition and domain independence. Since it's computationally expensive, subsequent work has focused on optimizing and approximating the computation of SimRank. In this paper, we approach SimRank from a top-k querying perspective where given a query node v, we are interested in finding the top-k nodes that have the highest SimRank score w.r.t. v. The only known approaches for answering such queries are either a naive algorithm of computing the similarity matrix for all node pairs or computing the similarity vector by comparing the query node v with each other node independently, and then picking the top-k. None of these approaches can handle top-k structural similarity search efficiently by scaling to very large graphs consisting of millions of nodes. We propose an algorithmic framework called TopSim based on transforming the top-k SimRank problem on a graph G to one of finding the top-k nodes with highest authority on the product graph G G. We further accelerate Top Sim by merging similarity paths and develop a more efficient algorithm called Top Sim-SM. Two heuristic algorithms, Trun-Top Sim-SM and Prio-Top Sim-SM, are also proposed to approximate Top Sim-SM on scale-free graphs to trade accuracy for speed, based on truncated random walk and prioritizing propagation respectively. We analyze the accuracy and performance of Top Sim family algorithms and report the results of a detailed experimental study.

#index 1846797
#* Relevance Matters: Capitalizing on Less (Top-k Matching in Publish/Subscribe)
#@ Mohammad Sadoghi;Hans-Arno Jacobsen
#t 2012
#c 17
#! The efficient processing of large collections of Boolean expressions plays a central role in major data intensive applications ranging from user-centric processing and personalization to real-time data analysis. Emerging applications such as computational advertising and selective information dissemination demand determining and presenting to an end-user only the most relevant content that is both user-consumable and suitable for limited screen real estate of target devices. To retrieve the most relevant content, we present BE*-Tree, a novel indexing data structure designed for effective hierarchical top-k pattern matching, which as its by-product also reduces the operational cost of processing millions of patterns. To further reduce processing cost, BE*-Tree employs an adaptive and non-rigid space-cutting technique designed to efficiently index Boolean expressions over a high-dimensional continuous space. At the core of BE*-Tree lie two innovative ideas: (1) a bi-directional tree expansion build as a top-down (data and space clustering) and a bottom-up growths (space clustering), which together enable indexing only non-empty continuous sub-spaces, and (2) an overlap-free splitting strategy. Finally, the performance of BE*-Tree is proven through a comprehensive experimental comparison against state-of-the-art index structures for matching Boolean expressions.

#index 1846798
#* Efficient Exact Similarity Searches Using Multiple Token Orderings
#@ Jongik Kim;Hongrae Lee
#t 2012
#c 17
#! Similarity searches are essential in many applications including data cleaning and near duplicate detection. Many similarity search algorithms first generate candidate records, and then identify true matches among them. A major focus of those algorithms has been on how to reduce the number of candidate records in the early stage of similarity query processing. One of the most commonly used techniques to reduce the candidate size is the prefix filtering principle, which exploits the document frequency ordering of tokens. In this paper, we propose a novel partitioning technique that considers multiple token orderings based on token co-occurrence statistics. Experimental results show that the proposed technique is effective in reducing the number of candidate records and as a result improves the performance of existing algorithms significantly.

#index 1846799
#* Efficiently Monitoring Top-k Pairs over Sliding Windows
#@ Zhitao Shen;Muhammad Aamir Cheema;Xuemin Lin;Wenjie Zhang;Haixun Wang
#t 2012
#c 17
#! Top-k pairs queries have received significant attention by the research community. k-closest pairs queries, k-furthest pairs queries and their variants are among the most well studied special cases of the top-k pairs queries. In this paper, we present the first approach to answer a broad class of top-k pairs queries over sliding windows. Our framework handles multiple top-k pairs queries and each query is allowed to use a different scoring function, a different value of k and a different size of the sliding window. Although the number of possible pairs in the sliding window is quadratic to the number of objects N in the sliding window, we efficiently answer the top-k pairs query by maintaining a small subset of pairs called K-sky band which is expected to consist of O(K log(N/K)) pairs. For all the queries that use the same scoring function, we need to maintain only one K-sky band. We present efficient techniques for the K-sky band maintenance and query answering. We conduct a detailed complexity analysis and show that the expected cost of our approach is reasonably close to the lower bound cost. We experimentally verify this by comparing our approach with a specially designed supreme algorithm that assumes the existence of an oracle and meets the lower bound cost.

#index 1846800
#* Processing and Notifying Range Top-k Subscriptions
#@ Albert Yu;Pankaj K. Agarwal;Jun Yang
#t 2012
#c 17
#! We consider how to support a large number of users over a wide-area network whose interests are characterised by range top-k continuous queries. Given an object update, we need to notify users whose top-k results are affected. Simple solutions include using a content-driven network to notify all users whose interest ranges contain the update (ignoring top-k), or using a server to compute only the affected queries and notifying them individually. The former solution generates too much network traffic, while the latter overwhelms the server. We present a geometric framework for the problem that allows us to describe the set of affected queries succinctly with messages that can be efficiently disseminated using content-driven networks. We give fast algorithms to reformulate each update into a set of messages whose number is provably optimal, with or without knowing all user interests. We also present extensions to our solution, including an approximate algorithm that trades off between the cost of server-side reformulation and that of user-side post-processing, as well as efficient techniques for batch updates.

#index 1846801
#* Efficient Graph Similarity Joins with Edit Distance Constraints
#@ Xiang Zhao;Chuan Xiao;Xuemin Lin;Wei Wang
#t 2012
#c 17
#! Graphs are widely used to model complicated data semantics in many applications in bioinformatics, chemistry, social networks, pattern recognition, etc. A recent trend is to tolerate noise arising from various sources, such as erroneous data entry, and find similarity matches. In this paper, we study the graph similarity join problem that returns pairs of graphs such that their edit distances are no larger than a threshold. Inspired by the q-gram idea for string similarity problem, our solution extracts paths from graphs as features for indexing. We establish a lower bound of common features to generate candidates. An efficient algorithm is proposed to exploit both matching and mismatching features to improve the filtering and verification on candidates. We demonstrate the proposed algorithm significantly outperforms existing approaches with extensive experiments on publicly available datasets.

#index 1846802
#* Parameter-Free Determination of Distance Thresholds for Metric Distance Constraints
#@ Shaoxu Song;Lei Chen;Hong Cheng
#t 2012
#c 17
#! The importance of introducing distance constraints to data dependencies, such as differential dependencies (DDs) [28], has recently been recognized. The metric distance constraints are tolerant to small variations, which enable them apply to wide data quality checking applications, such as detecting data violations. However, the determination of distance thresholds for the metric distance constraints is non-trivial. It often relies on a truth data instance which embeds the distance constraints. To find useful distance threshold patterns from data, there are several guidelines of statistical measures to specify, e.g., support, confidence and dependent quality. Unfortunately, given a data instance, users might not have any knowledge about the data distribution, thus it is very challenging to set the right parameters. In this paper, we study the determination of distance thresholds for metric distance constraints, in a parameter-free style. Specifically, we compute an expected utility based on the statistical measures from the data. According to our analysis as well as experimental verification, distance threshold patterns with higher expected utility could offer better usage in real applications, such as violation detection. We then develop efficient algorithms to determine the distance thresholds having the maximum expected utility. Finally, our extensive experimental evaluation demonstrates the effectiveness and efficiency of the proposed methods.

#index 1846803
#* Random Error Reduction in Similarity Search on Time Series: A Statistical Approach
#@ Wush Chi-Hsuan Wu;Mi-Yen Yeh;Jian Pei
#t 2012
#c 17
#! Errors in measurement can be categorized into two types: systematic errors that are predictable, and random errors that are inherently unpredictable and have null expected value. Random error is always present in a measurement. More often than not, readings in time series may contain inherent random errors due to causes like dynamic error, drift, noise, hysteresis, digitalization error and limited sampling frequency. Random errors may affect the quality of time series analysis substantially. Unfortunately, most of the existing time series mining and analysis methods, such as similarity search, clustering, and classification tasks, do not address random errors, possibly because random error in a time series, which can be modeled as a random variable of unknown distribution, is hard to handle. In this paper, we tackle this challenging problem. Taking similarity search as an example, which is an essential task in time series analysis, we develop MISQ, a statistical approach for random error reduction in time series analysis. The major intuition in our method is to use only the readings at different time instants in a time series to reduce random errors. We achieve a highly desirable property in MISQ: it can ensure that the recall is above a user-specified threshold. An extensive empirical study on 20 benchmark real data sets clearly shows that our method can lead to better performance than the baseline method without random error reduction in real applications such as classification. Moreover, MISQ achieves good quality in similarity search.

#index 1846804
#* Message from the Program Chairs and the General Chair
#@ 
#t 2012
#c 17

#index 1846805
#* Viewing the Web as a Distributed Knowledge Base
#@ Serge Abiteboul;Émilien Antoine;Julia Stoyanovich
#t 2012
#c 17
#! This paper addresses the challenges faced by everyday Web users, who interact with inherently heterogeneous and distributed information. Managing such data is currently beyond the skills of casual users. We describe ongoing work that has as its goal the development of foundations for declarative distributed data management. In this approach, we see the Web as a knowledge base consisting of distributed logical facts and rules. Our objective is to enable automated reasoning over this knowledge base, ultimately improving the quality of service and of data. For this, we use Webdamlog, a Datalogstyle language with rule delegation. We outline ongoing efforts on the Web dam Exchange platform that combines Webdamlog evaluation with communication and security protocols.

#index 1846806
#* How Different is Big Data?
#@ Surajit Chaudhuri
#t 2012
#c 17
#! One buzzword that has been popular in the last couple of years is Big Data. In simplest terms, Big Data symbolizes the aspiration to build platforms and tools to ingest, store and analyze data that can be voluminous, diverse, and possibly fast changing. In this talk, I will try to reflect on a few of the technical problems presented by the exploration of Big Data. Some of these challenges in data analytics have been addressed by our community in the past in a more traditional relational database context but only with mixed results. I will review these quests and study some of the key lessons learned. At the same time, significant developments such as the emergence of cloud infrastructure and availability of data rich web services hold the potential for transforming our industry. I will discuss the unique opportunities they present for Big Data Analytics.

#index 1846807
#* Accountability and Trust in Cooperative Information Systems
#@ Peter Druschel
#t 2012
#c 17
#! Cooperation and trust play an increasingly important role in today's information systems. For instance, peer-to-peer systems like BitTorrent, Sopcast and Skype are powered by resource contributions from participating users, federated systems like the Internet have to respect the interests, policies and laws of participating organizations and countries; in the Cloud, users entrust their data and computation to third-part infrastructure. In this talk, we consider accountability as a way to facilitate transparency and trust in cooperative systems. We look at practical techniques to account for the integrity of distributed, cooperative computations, and look at some of the difficulties and open problems in accountability.

#index 1846808
#* The Future of Scientific Data Bases
#@ Michael Stonebraker;Anastasia Ailamaki;Jeremy Kepner;Alex Szalay
#t 2012
#c 17
#! For many decades, users in scientific fields (domain scientists) have resorted to either home-grown tools or legacy software for the management of their data. Technological advancements nowadays necessitate many of the properties such as data independence, scalability, and functionality found in the roadmap of DBMS technology, DBMS products, however, are not yet ready to address scientific application and user needs. Recent efforts toward building a science DBMS indicate that there is a long way ahead of us, paved by a research agenda that is rich in interesting and challenging problems.

#index 1846809
#* Privacy in Social Networks: How Risky is Your Social Graph?
#@ Cuneyt Akcora;Barbara Carminati;Elena Ferrari
#t 2012
#c 17
#! Several efforts have been made for more privacy aware Online Social Networks (OSNs) to protect personal data against various privacy threats. However, despite the relevance of these proposals, we believe there is still the lack of a conceptual model on top of which privacy tools have to be designed. Central to this model should be the concept of risk. Therefore, in this paper, we propose a risk measure for OSNs. The aim is to associate a risk level with social network users in order to provide other users with a measure of how much it might be risky, in terms of disclosure of private information, to have interactions with them. We compute risk levels based on similarity and benefit measures, by also taking into account the user risk attitudes. In particular, we adopt an active learning approach for risk estimation, where user risk attitude is learned from few required user interactions. The risk estimation process discussed in this paper has been developed into a Facebook application and tested on real data. The experiments show the effectiveness of our proposal.

#index 1846810
#* Privacy-Preserving and Content-Protecting Location Based Queries
#@ Russell Paulet;Md. Golam Koasar;Xun Yi;Elisa Bertino
#t 2012
#c 17
#! In this paper we present a solution to one of the location-based query problems. This problem is defined as follows: (i) a user wants to query a database of location data, known as Points Of Interest (POI), and does not want to reveal his/her location to the server due to privacy concerns, (ii) the owner of the location data, that is, the location server, does not want to simply distribute its data to all users. The location server desires to have some control over its data, since the data is its asset. Previous solutions have used a trusted anonymiser to address privacy, but introduced the impracticality of trusting a third party. More recent solutions have used homomorphic encryption to remove this weakness. Briefly, the user submits his/her encrypted coordinates to the server and the server would determine the user's location homomorphically, and then the user would acquire the corresponding record using Private Information Retrieval techniques. We propose a major enhancement upon this result by introducing a similar two stage approach, where the homomorphic comparison step is replaced with Oblivious Transfer to achieve a more secure solution for both parties. The solution we present is efficient and practical in many scenarios. We also include the results of a working prototype to illustrate the efficiency of our protocol.

#index 1846811
#* Optimizing Statistical Information Extraction Programs over Evolving Text
#@ Fei Chen;Xixuan Feng;Christopher Re;Min Wang
#t 2012
#c 17
#! Statistical information extraction (IE) programs are increasingly used to build real-world IE systems such as Alibaba, Cite Seer, Kylin, and YAGO. Current statistical IE approaches consider the text corpora underlying the extraction program to be static. However, many real-world text corpora are dynamic (documents are inserted, modified, and removed). As the corpus evolves, and IE programs must be applied repeatedly to consecutive corpus snapshots to keep extracted information up to date. Applying IE {\em from scratch\/} to each snapshot may be inefficient: a pair of consecutive snapshots may change very little, but unaware of this, the program must run again from scratch. In this paper, we present \crflex, a system that efficiently executes such repeated statistical IE, by recycling previous IE results to enable incremental update. We focus on statistical IE programs which use a leading statistical model, Conditional Random Fields (CRFs). We show how to model properties of the CRF inference algorithms for incremental update and how to exploit them to correctly recycle previous inference results. Then we show how to efficiently capture and store intermediate results of IE programs for subsequent recycling. We find that there is a tradeoff between the I/O cost spent on reading and writing intermediate results, and CPU cost we can save from recycling those intermediate results. Therefore we present a cost-based solution to determine the most efficient recycling approach for any given CRF-based IE program and an evolving corpus. We present extensive experiments with CRF-based IE programs for 3 IE tasks over a real-world data set to demonstrate the utility of our approach.

#index 1846812
#* Approximate String Membership Checking: A Multiple Filter, Optimization-Based Approach
#@ Chong Sun;Jeffrey F. Naughton;Siddharth Barman
#t 2012
#c 17
#! We consider the approximate string membership checking (ASMC) problem of extracting all the strings or sub strings in a document that approximately match some string in a given dictionary. To solve this problem, the current state-of-art approach involves first applying an approximate, fast filter, then applying a more expensive exact verification algorithm to the strings that pass the filter. Correspondingly, many string filters have been proposed. We note that different filters are good at eliminating different strings, depending on the characteristics of the strings in both the documents and the dictionary. We suspect that no single filter will dominate all other filters everywhere. Given an ASMC problem instance and a set of string filters, we need to select the optimal filter to maximize the performance. Furthermore, in our experiments we found that in some cases a sequence of filters dominates any of the filters of the sequence in isolation, and that the best set of filters and their ordering depend upon the specific problem instance encountered. Accordingly, we propose that the approximate match problem be viewed as an optimization problem, and evaluate a number of techniques for solving this optimization problem.

#index 1846813
#* On Text Clustering with Side Information
#@ Charu C. Aggarwal;Yuchen Zhao;Philip S. Yu
#t 2012
#c 17
#! Text clustering has become an increasingly important problem in recent years because of the tremendous amount of unstructured data which is available in various forms in online forums such as the web, social networks, and other information networks. In most cases, the data is not purely available in text form. A lot of side-information is available along with the text documents. Such side-information may be of different kinds, such as the links in the document, user-access behavior from web logs, or other non-textual attributes which are embedded into the text document. Such attributes may contain a tremendous amount of information for clustering purposes. However, the relative importance of this side-information may be difficult to estimate, especially when some of the information is noisy. In such cases, it can be risky to incorporate side-information into the clustering process, because it can either improve the quality of the representation for clustering, or can add noise to the process. Therefore, we need a principled way to perform the clustering process, so as to maximize the advantages from using this side information. In this paper, we design an algorithm which combines classical partitioning algorithms with probabilistic models in order to create an effective clustering approach. We present experimental results on a number of real data sets in order to illustrate the advantages of using such an approach.

#index 1846814
#* Fast SLCA and ELCA Computation for XML Keyword Queries Based on Set Intersection
#@ Junfeng Zhou;Zhifeng Bao;Wei Wang;Tok Wang Ling;Ziyang Chen;Xudong Lin;Jingfeng Guo
#t 2012
#c 17
#! In this paper, we focus on efficient keyword query processing for XML data based on the SLCA and ELCA semantics. We propose a novel form of inverted lists for keywords which include IDs of nodes that directly or indirectly contain a given keyword. We propose a family of efficient algorithms that are based on the set intersection operation for both semantics. We show that the problem of SLCA/ELCA computation becomes finding a set of nodes that appear in all involved inverted lists and satisfy certain conditions. We also propose several optimization techniques to further improve the query processing performance. We have conducted extensive experiments with many alternative methods. The results demonstrate that our proposed methods outperform previous methods by up to two orders of magnitude in many cases.

#index 1846815
#* Optimization of Massive Pattern Queries by Dynamic Configuration Morphing
#@ Nikolay Laptev;Carlo Zaniolo
#t 2012
#c 17
#! Complex pattern queries play a critical role in many applications that must efficiently search databases and data streams. Current techniques support the search for multiple patterns using deterministic or non-deterministic automata. In practice however, the static pattern representation does not fully utilize available system resources, subsequently suffering from poor performance. Therefore a low overhead auto-reconfigurable automaton is needed that optimizes pattern matching performance. In this paper, we propose a dynamic system that entails the efficient and reliable evaluation of a very large number of pattern queries on a resource constrained system under changing stress-load. Our system prototype, Morpheus, pre-computes several query pattern representations, named templates, which are then morphed into a required form during run-time. Morpheus uses templates to speed up dynamic automaton reconfiguration. Results from empirical studies confirm the benefits of our approach, with three orders of magnitude improvement achieved in the overall pattern matching performance with the help of dynamic reconfiguration. This is accomplished only with a modest increase in amortized memory usage.

#index 1846816
#* Differentially Private Spatial Decompositions
#@ Graham Cormode;Cecilia Procopiuc;Divesh Srivastava;Entong Shen;Ting Yu
#t 2012
#c 17
#! Differential privacy has recently emerged as the de facto standard for private data release. This makes it possible to provide strong theoretical guarantees on the privacy and utility of released data. While it is well-understood how to release data based on counts and simple functions under this guarantee, it remains to provide general purpose techniques to release data that is useful for a variety of queries. In this paper, we focus on spatial data such as locations and more generally any multi-dimensional data that can be indexed by a tree structure. Directly applying existing differential privacy methods to this type of data simply generates noise. We propose instead the class of ``private spatial decompositions'': these adapt standard spatial indexing methods such as quad trees and kd-trees to provide a private description of the data distribution. Equipping such structures with differential privacy requires several steps to ensure that they provide meaningful privacy guarantees. Various basic steps, such as choosing splitting points and describing the distribution of points within a region, must be done privately, and the guarantees of the different building blocks composed to provide an overall guarantee. Consequently, we expose the design space for private spatial decompositions, and analyze some key examples. A major contribution of our work is to provide new techniques for parameter setting and post-processing the output to improve the accuracy of query answers. Our experimental study demonstrates that it is possible to build such decompositions efficiently, and use them to answer a variety of queries privately with high accuracy.

#index 1846817
#* Differentially Private Histogram Publication
#@ Jia Xu;Zhenjie Zhang;Xiaokui Xiao;Yin Yang;Ge Yu
#t 2012
#c 17
#! Differential privacy (DP) is a promising scheme for releasing the results of statistical queries on sensitive data, with strong privacy guarantees against adversaries with arbitrary background knowledge. Existing studies on DP mostly focus on simple aggregations such as counts. This paper investigates the publication of DP-compliant histograms, which is an important analytical tool for showing the distribution of a random variable, e.g., hospital bill size for certain patients. Compared to simple aggregations whose results are purely numerical, a histogram query is inherently more complex, since it must also determine its structure, i.e., the ranges of the bins. As we demonstrate in the paper, a DP-compliant histogram with finer bins may actually lead to significantly lower accuracy than a coarser one, since the former requires stronger perturbations in order to satisfy DP. Moreover, the histogram structure itself may reveal sensitive information, which further complicates the problem. Motivated by this, we propose two novel algorithms, namely Noise First and Structure First, for computing DP-compliant histograms. Their main difference lies in the relative order of the noise injection and the histogram structure computation steps. Noise First has the additional benefit that it can improve the accuracy of an already published DP-complaint histogram computed using a naiive method. Going one step further, we extend both solutions to answer arbitrary range queries. Extensive experiments, using several real data sets, confirm that the proposed methods output highly accurate query answers, and consistently outperform existing competitors.

#index 1846818
#* GeoFeed: A Location Aware News Feed System
#@ Jie Bao;Mohamed F. Mokbel;Chi-Yin Chow
#t 2012
#c 17
#! This paper presents the Geo Feed system, a location-aware news feed system that provides a new platform for its users to get spatially related message updates from either their friends or favorite news sources. Geo Feed distinguishes itself from all existing news feed systems in that it takes into account the spatial extents of messages and user locations when deciding upon the selected news feed. Geo Feed is equipped with three different approaches for delivering the news feed to its users, namely, spatial pull, spatial push, and shared push. Then, the main challenge of Geo Feed is to decide on when to use each of these three approaches to which users. Geo Feed is equipped with a smart decision model that decides about using these approaches in a way that: (a) minimizes the system overhead for delivering the location-aware news feed, and (b) guarantees a certain response time for each user to obtain the requested location-aware news feed. Experimental results, based on real and synthetic data, show that Geo Feed outperforms existing news feed systems in terms of response time and maintenance cost.

#index 1846819
#* Entity Search Strategies for Mashup Applications
#@ Stefan Endrullis;Andreas Thor;Erhard Rahm
#t 2012
#c 17
#! Programmatic data integration approaches such as mashups have become a viable approach to dynamically integrate web data at runtime. Key data sources for mashups include entity search engines and hidden databases that need to be queried via source-specific search interfaces or web forms. Current mashups are typically restricted to simple query approaches such as using keyword search. Such approaches may need a high number of queries if many objects have to be found. Furthermore, the effectiveness of the queries may be limited, i.e., they may miss relevant results. We therefore propose more advanced search strategies that aim at finding a set of entities with high efficiency and high effectiveness. Our strategies use different kinds of queries that are determined by source-specific query generators. Furthermore, the queries are selected based on the characteristics of input entities. We introduce a flexible model for entity search strategies that includes a ranking of candidate queries determined by different query generators. We describe different query generators and outline their use within four entity search strategies. These strategies apply different query ranking and selection approaches to optimize efficiency and effectiveness. We evaluate our search strategies in detail for two domains: product search and publication search. The comparison with a standard keyword search shows that the proposed search strategies provide significant improvements in both domains.

#index 1846820
#* Three-Level Processing of Multiple Aggregate Continuous Queries
#@ Shenoda Guirguis;Mohamed A. Sharaf;Panos K. Chrysanthis;Alexandros Labrinidis
#t 2012
#c 17
#! Aggregate Continuous Queries (ACQs) are both a very popular class of Continuous Queries (CQs) and also have a potentially high execution cost. As such, optimizing the processing of ACQs is imperative for Data Stream Management Systems (DSMSs) to reach their full potential in supporting (critical) monitoring applications. For multiple ACQs that vary in window specifications and pre-aggregation filters, existing multiple ACQs optimization schemes assume a processing model where each ACQ is computed as a final-aggregation of a sub-aggregation. In this paper, we propose a novel processing model for ACQs, called Tri Ops, with the goal of minimizing the repetition of operator execution at the sub-aggregation level. We also propose Tri Weave, a Tri Ops-aware multi-query optimizer. We analytically and experimentally demonstrate the performance gains of our proposed schemes which shows their superiority over alternative schemes. Finally, we generalize Tri Weave to incorporate the classical subsumption-based multi-query optimization techniques.

#index 1846821
#* Accelerating Range Queries for Brain Simulations
#@ Farhan Tauheed;Laurynas Biveinis;Thomas Heinis;Felix Schurmann;Henry Markram;Anastasia Ailamaki
#t 2012
#c 17
#! Neuroscientists increasingly use computational tools in building and simulating models of the brain. The amounts of data involved in these simulations are immense and efficiently managing this data is key. One particular problem in analyzing this data is the scalable execution of range queries on spatial models of the brain. Known indexing approaches do not perform well even on today's small models which represent a small fraction of the brain, containing only few millions of densely packed spatial elements. The problem of current approaches is that with the increasing level of detail in the models, also the overlap in the tree structure increases, ultimately slowing down query execution. The neuroscientists' need to work with bigger and more detailed (denser) models thus motivates us to develop a new indexing approach. To this end we develop FLAT, a scalable indexing approach for dense data sets. We base the development of FLAT on the key observation that current approaches suffer from overlap in case of dense data sets. We hence design FLAT as an approach with two phases, each independent of density. In the first phase it uses a traditional spatial index to retrieve an initial object efficiently. In the second phase it traverses the initial object's neighborhood to retrieve the remaining query result. Our experimental results show that FLAT not only outperforms R-Tree variants from a factor of two up to eight but that it also achieves independence from data set size and density.

#index 1846822
#* Keyword Query Reformulation on Structured Data
#@ Junjie Yao;Bin Cui;Liansheng Hua;Yuxin Huang
#t 2012
#c 17
#! Textual web pages dominate web search engines nowadays. However, there is also a striking increase of structured data on the web. Efficient keyword query processing on structured data has attracted enough attention, but effective query understanding has yet to be investigated. In this paper, we focus on the problem of keyword query reformulation in the structured data scenario. These reformulated queries provide alternative descriptions of original input. They could better capture users' information need and guide users to explore related items in the target structured data. We propose an automatic keyword query reformulation approach by exploiting structural semantics in the underlying structured data sources. The reformulation solution is decomposed into two stages, i.e., offline term relation extraction and online query generation. We first utilize a heterogenous graph to model the words and items in structured data, and design an enhanced Random Walk approach to extract relevant terms from the graph context. In the online query reformulation stage, we introduce an efficient probabilistic generation module to suggest substitutable reformulated queries. Extensive experiments are conducted on a real-life data set, and our approach yields promising results.

#index 1846823
#* Predicting Approximate Protein-DNA Binding Cores Using Association Rule Mining
#@ Po-Yuen Wong;Tak-Ming Chan;Man-Hon Wong;Kwong-Sak Leung
#t 2012
#c 17
#! The studies of protein-DNA bindings between transcription factors (TFs) and transcription factor binding sites (TFBSs) are important bioinformatics topics. High-resolution (length490) are shown promising in identifying accurate binding cores without using any 3D structures. While the current association rule mining method on this problem addresses exact sequences only, the most recent ad hoc method for approximation does not establish any formal model and is limited by experimentally known patterns. As biological mutations are common, it is desirable to formally extend the exact model into an approximate one. In this paper, we formalize the problem of mining approximate protein-DNA association rules from sequence data and propose a novel efficient algorithm to predict protein-DNA binding cores. Our two-phase algorithm first constructs two compact intermediate structures called frequent sequence tree (FS-Tree) and frequent sequence class tree (FSCTree). Approximate association rules are efficiently generated from the structures and bioinformatics concepts (position weight matrix and information content) are further employed to prune meaningless rules. Experimental results on real data show the performance and applicability of the proposed algorithm.

#index 1846824
#* Integrating Frequent Pattern Mining from Multiple Data Domains for Classification
#@ Dhaval Patel;Wynne Hsu;Mong Li Lee
#t 2012
#c 17
#! Many frequent pattern mining algorithms have been developed for categorical, numerical, time series, or interval data. However, little attention has been given to integrate these algorithms so as to mine frequent patterns from multiple domain datasets for classification. In this paper, we introduce the notion of a heterogenous pattern to capture the associations among different kinds of data. We propose a unified framework for mining multiple domain datasets and design an iterative algorithm called HTMiner. HTMiner discovers essential heterogenous patterns for classification and performs instance elimination. This instance elimination step reduces the problem size progressively by removing training instances which are correctly covered by the discovered essential heterogenous pattern. Experiments on two real world datasets show that the HTMiner is efficient and can significantly improve the classification accuracy.

#index 1846825
#* CI-Rank: Ranking Keyword Search Results Based on Collective Importance
#@ Xiaohui Yu;Huxia Shi
#t 2012
#c 17
#! Keyword search over databases, popularized by keyword search in WWW, allows ordinary users to access database information without the knowledge of structured query languages and database schemas. Most of the previous studies in this area use IR-style ranking, which fail to consider the importance of the query answers. In this paper, we propose \cirank, a new approach for keyword search in databases, which considers the importance of individual nodes in a query answer and the cohesiveness of the result structure in a balanced way. \cirank\ is built upon a carefully designed model call Random Walk with Message Passing that helps capture the relationships between different nodes in the query answer. We develop a branch and bound algorithm to support the efficient generation of top-$k$ query answers. Indexing methods are also introduced to further speed up the run-time processing of queries. Extensive experiments conducted on two real data sets with a real user query log confirm the effectiveness and efficiency of \cirank.

#index 1846826
#* Temporal Analytics on Big Data for Web Advertising
#@ Badrish Chandramouli;Jonathan Goldstein;Songyun Duan
#t 2012
#c 17
#! "Big Data" in map-reduce (M-R) clusters is often fundamentally temporal in nature, as are many analytics tasks over such data. For instance, display advertising uses Behavioral Targeting (BT) to select ads for users based on prior searches, page views, etc. Previous work on BT has focused on techniques that scale well for offline data using M-R. However, this approach has limitations for BT-style applications that deal with temporal data: (1) many queries are temporal and not easily expressible in M-R, and moreover, the set-oriented nature of M-R front-ends such as SCOPE is not suitable for temporal processing, (2) as commercial systems mature, they may need to also directly analyze and react to real-time data feeds since a high turnaround time can result in missed opportunities, but it is difficult for current solutions to naturally also operate over real-time streams. Our contributions are twofold. First, we propose a novel framework called TiMR (pronounced timer), that combines a time-oriented data processing system with a M-R framework. Users write and submit analysis algorithms as temporal queries - these queries are succinct, scale-out-agnostic, and easy to write. They scale well on large-scale offline data using TiMR, and can work unmodified over real-time streams. We also propose new cost-based query fragmentation and temporal partitioning schemes for improving efficiency with TiMR. Second, we show the feasibility of this approach for BT, with new temporal algorithms that exploit new targeting opportunities. Experiments using real data from a commercial ad platform show that TiMR is very efficient and incurs orders-of-magnitude lower development effort. Our BT solution is easy and succinct, and performs up to several times better than current schemes in terms of memory, learning time, and click-through-rate/coverage.

#index 1846827
#* Lookup Tables: Fine-Grained Partitioning for Distributed Databases
#@ Aubrey L. Tatarowicz;Carlo Curino;Evan P. C. Jones;Sam Madden
#t 2012
#c 17
#! The standard way to get linear scaling in a distributed OLTP DBMS is to horizontally partition data across several nodes. Ideally, this partitioning will result in each query being executed at just one node, to avoid the overheads of distributed transactions and allow nodes to be added without increasing the amount of required coordination. For some applications, simple strategies, such as hashing on primary key, provide this property. Unfortunately, for many applications, including social networking and order-fulfillment, many-to-many relationships cause simple strategies to result in a large fraction of distributed queries. Instead, what is needed is a fine-grained partitioning, where related individual tuples (e.g., cliques of friends) are co-located together in the same partition. Maintaining such a fine-grained partitioning requires the database to store a large amount of metadata about which partition each tuple resides in. We call such metadata a lookup table, and present the design of a data distribution layer that efficiently stores these tables and maintains them in the presence of inserts, deletes, and updates. We show that such tables can provide scalability for several difficult to partition database workloads, including Wikipedia, Twitter, and TPC-E. Our implementation provides 40% to 300% better performance on these workloads than either simple range or hash partitioning and shows greater potential for further scale-out.

#index 1846828
#* Temporal Support for Persistent Stored Modules
#@ Richard T. Snodgrass;Dengfeng Gao;Rui Zhang;Stephen W. Thomas
#t 2012
#c 17
#! We show how to extend temporal support of SQL to the Turing-complete portion of SQL, that of persistent stored modules (PSM). Our approach requires minor new syntax beyond that already in SQL/Temporal to define and to invoke PSM routines, thereby extending the current, sequenced, and non-sequenced semantics of queries to PSM routines. Temporal upward compatibility (existing applications work as before when one or more tables are rendered temporal) is ensured. We provide a transformation that converts Temporal SQL/PSM to conventional SQL/PSM. To support sequenced evaluation of PSM routines, we define two different slicing approaches, maximal slicing and per-statement slicing. We compare these approaches empirically using a comprehensive benchmark and provide a heuristic for choosing between them.

#index 1846829
#* ISOBAR Preconditioner for Effective and High-throughput Lossless Data Compression
#@ Eric R. Schendel;Ye Jin;Neil Shah;Jackie Chen;C. S. Chang;Seung-Hoe Ku;Stephane Ethier;Scott Klasky;Robert Latham;Robert Ross;Nagiza F. Samatova
#t 2012
#c 17
#! Efficient handling of large volumes of data is a necessity for exascale scientific applications and database systems. To address the growing imbalance between the amount of available storage and the amount of data being produced by high speed (FLOPS) processors on the system, data must be compressed to reduce the total amount of data placed on the file systems. General-purpose loss less compression frameworks, such as zlib and bzlib2, are commonly used on datasets requiring loss less compression. Quite often, however, many scientific data sets compress poorly, referred to as hard-to-compress datasets, due to the negative impact of highly entropic content represented within the data. An important problem in better loss less data compression is to identify the hard-to-compress information and subsequently optimize the compression techniques at the byte-level. To address this challenge, we introduce the In-Situ Orthogonal Byte Aggregate Reduction Compression (ISOBAR-compress) methodology as a preconditioner of loss less compression to identify and optimize the compression efficiency and throughput of hard-to-compress datasets.

#index 1846830
#* Upgrading Uncompetitive Products Economically
#@ Hua Lu;Christian S. Jensen
#t 2012
#c 17
#! The skyline of a multidimensional point set consists of the points that are not dominated by other points. In a scenario where product features are represented by multidimensional points, the skyline points may be viewed as representing competitive products. A product provider may wish to upgrade uncompetitive products to become competitive, but wants to take into account the upgrading cost. We study the top-k product upgrading problem. Given a set P of competitor products, a set T of products that are candidates for upgrade, and an upgrading cost function f that applies to T, the problem is to return the k products in T that can be upgraded to not be dominated by any products in P at the lowest cost. This problem is non-trivial due to not only the large data set sizes, but also to the many possibilities for upgrading a product. We identify and provide solutions for the different options for upgrading an uncompetitive product, and combine the solutions into a single solution. We also propose a spatial join-based solution that assumes P and T are indexed by an R-tree. Given a set of products in the same R-tree node, we derive three lower bounds on their upgrading costs. These bounds are employed by the join approach to prune upgrade candidates with uncompetitive upgrade costs. Empirical studies with synthetic and real data show that the join approach is efficient and scalable.

#index 1846831
#* Attribute-Based Subsequence Matching and Mining
#@ Yu Peng;Raymond Chi-Wing Wong;Liangliang Ye;Philip S. Yu
#t 2012
#c 17
#! Sequence analysis is very important in our daily life. Typically, each sequence is associated with an ordered list of elements. For example, in a movie rental application, a customer's movie rental record containing an ordered list of movies is a sequence example. Most studies about sequence analysis focus on subsequence matching which finds all sequences stored in the database such that a given query sequence is a subsequence of each of these sequences. In many applications, elements are associated with properties or attributes. For example, each movie is associated with some attributes like "Director" and "Actors". Unfortunately, to the best of our knowledge, all existing studies about sequence analysis do not consider the attributes of elements. In this paper, we propose two problems. The first problem is: given a query sequence and a set of sequences, considering the attributes of elements, we want to find all sequences which are matched by this query sequence. This problem is called attribute-based subsequence matching (ASM). All existing applications for the traditional subsequence matching problem can also be applied to our new problem provided that we are given the attributes of elements. We propose an efficient algorithm for problem ASM. The key idea to the efficiency of this algorithm is to compress each whole sequence with potentially many associated attributes into just a triplet of numbers. By dealing with these very compressed representations, we greatly speed up the attribute-based subsequence matching. The second problem is to find all frequent attribute-based subsequence. We also adapt an existing efficient algorithm for this second problem to show we can use the algorithm developed for the first problem. Empirical studies show that our algorithms are scalable in large datasets. In particular, our algorithms run at least an order of magnitude faster than a straightforward method in most cases. This work can stimulate a number of existing data mining problems which are fundamentally based on subsequence matching such as sequence classification, frequent sequence mining, motif detection and sequence matching in bioinformatics.

#index 1846832
#* Efficient Versioning for Scientific Array Databases
#@ Adam Seering;Philippe Cudre-Mauroux;Samuel Madden;Michael Stonebraker
#t 2012
#c 17
#! In this paper, we describe a versioned database storage manager we are developing for the SciDB scientific database. The system is designed to efficiently store and retrieve array-oriented data, exposing a ``no-overwrite'' storage model in which each update creates a new ``version'' of an array. This makes it possible to perform comparisons of versions produced at different times or by different algorithms, and to create complex chains and trees of versions. We present algorithms to efficiently encode these versions, minimizing storage space while still providing efficient access to the data. Additionally, we present an optimal algorithm that, given a long sequence of versions, determines which versions to encode in terms of each other (using delta compression) to minimize total storage space or query execution cost. We compare the performance of these algorithms on real world data sets from the National Oceanic and Atmospheric Administration (NOAA), Open Street Maps, and several other sources. We show that our algorithms provide better performance than existing version control systems not optimized for array data, both in terms of storage size and access time, and that our delta-compression algorithms are able to substantially reduce the total storage space when versions exist with a high degree of similarity.

#index 1846833
#* Multidimensional Analysis of Atypical Events in Cyber-Physical Data
#@ Lu-An Tang;Xiao Yu;Sangkyum Kim;Jiawei Han;Wen-Chih Peng;Yizhou Sun;Hector Gonzalez;Sebastian Seith
#t 2012
#c 17
#! A Cyber-Physical System (CPS) integrates physical devices (e.g., sensors, cameras) with cyber (or informational) components to form a situation-integrated analytical system that may respond intelligently to dynamic changes of the real-world situations. CPS claims many promising applications, such as traffic observation, battlefield surveillance and sensor-network based monitoring. One important research topic in CPS is about the atypical event analysis, i.e., retrieving the events from large amount of data and analyzing them with spatial, temporal and other multi-dimensional information. Many traditional approaches are not feasible for such analysis since they use numeric measures and cannot describe the complex atypical events. In this study, we propose a new model of atypical cluster to effectively represent those events and efficiently retrieve them from massive data. The micro-cluster is designed to summarize individual events, and the macro-cluster is used to integrate the information from multiple event. To facilitate scalable, flexible and online analysis, the concept of significant cluster is defined and a guided clustering algorithm is proposed to retrieve significant clusters in an efficient manner. We conduct experiments on real datasets with the size of more than 50 GB, the results show that the proposed method can provide more accurate information with only 15% to 20% time cost of the baselines.

#index 2009992
#* Proceedings of the 2013 IEEE International Conference on Data Engineering (ICDE 2013)
#@ 
#t 2013
#c 17

#index 2010304
#* SODIT: An innovative system for outlier detection using multiple localized thresholding and interactive feedback
#@ Ji Zhang;Xiaohui Tao;Lili Sun;Hua Wang
#t 2013
#c 17
#! Outlier detection is an important long-standing research problem in data mining and has enjoyed applications in a wide range of applications in business, engineering, biology and security, etc. However, the traditional outlier detection methods inevitably need to use different parameters for detection such as those used to specify the distance or density cutoff for distinguish outliers from normal data points. Using the trial and error approach, the traditional outlier detection methods are rather tedious in parameter tuning. In this demo proposal, we introduce an innovative outlier detection system, called SODIT, that uses localized thresholding to assist the value specification of the thresholds that reflect closely the local data distribution. In addition, easy-to-use user feedback are employed to further facilitate the determination of optimal parameter values. SODIT is able to make outlier detection much easier to operate and produce more accurate, intuitive and informative results than before.

#index 2010305
#* T-share: A large-scale dynamic taxi ridesharing service
#@ Ouri Wolfson;Yu Zheng;Shuo Ma
#t 2013
#c 17
#! Taxi ridesharing can be of significant social and environmental benefit, e.g. by saving energy consumption and satisfying people's commute needs. Despite the great potential, taxi ridesharing, especially with dynamic queries, is not well studied. In this paper, we formally define the dynamic ridesharing problem and propose a large-scale taxi ridesharing service. It efficiently serves real-time requests sent by taxi users and generates ridesharing schedules that reduce the total travel distance significantly. In our method, we first propose a taxi searching algorithm using a spatio-temporal index to quickly retrieve candidate taxis that are likely to satisfy a user query. A scheduling algorithm is then proposed. It checks each candidate taxi and inserts the query's trip into the schedule of the taxi which satisfies the query with minimum additional incurred travel distance. To tackle the heavy computational load, a lazy shortest path calculation strategy is devised to speed up the scheduling algorithm. We evaluated our service using a GPS trajectory dataset generated by over 33,000 taxis during a period of 3 months. By learning the spatio-temporal distributions of real user queries from this dataset, we built an experimental platform that simulates user real behaviours in taking a taxi. Tested on this platform with extensive experiments, our approach demonstrated its efficiency, effectiveness, and scalability. For example, our proposed service serves 25% additional taxi users while saving 13% travel distance compared with no-ridesharing (when the ratio of the number of queries to that of taxis is 6).

#index 2010306
#* Towards efficient SimRank computation on large networks
#@ Xuemin Lin;Weiren Yu;Wenjie Zhang
#t 2013
#c 17
#! SimRank has been a powerful model for assessing the similarity of pairs of vertices in a graph. It is based on the concept that two vertices are similar if they are referenced by similar vertices. Due to its self-referentiality, fast SimRank computation on large graphs poses significant challenges. The state-of-the-art work [17] exploits partial sums memorization for computing SimRank in O(Kmn) time on a graph with n vertices and m edges, where K is the number of iterations. Partial sums memorizing can reduce repeated calculations by caching part of similarity summations for later reuse. However, we observe that computations among different partial sums may have duplicate redundancy. Besides, for a desired accuracy ∊, the existing SimRank model requires K = [logC ∊] iterations [17], where C is a damping factor. Nevertheless, such a geometric rate of convergence is slow in practice if a high accuracy is desirable. In this paper, we address these gaps. (1) We propose an adaptive clustering strategy to eliminate partial sums redundancy (i.e., duplicate computations occurring in partial sums), and devise an efficient algorithm for speeding up the computation of SimRank to 0(Kd'n2) time, where d' is typically much smaller than the average in-degree of a graph. (2) We also present a new notion of SimRank that is based on a differential equation and can be represented as an exponential sum of transition matrices, as opposed to the geometric sum of the conventional counterpart. This leads to a further speedup in the convergence rate of SimRank iterations. (3) Using real and synthetic data, we empirically verify that our approach of partial sums sharing outperforms the best known algorithm by up to one order of magnitude, and that our revised notion of SimRank further achieves a 5X speedup on large graphs while also fairly preserving the relative order of original SimRank scores.

#index 2010307
#* Panel: Big data for the public
#@ 
#t 2013
#c 17

#index 2010308
#* Message from the ICDE 2013 program committee and general chairs
#@ 
#t 2013
#c 17

#index 2010309
#* Efficient tracking and querying for coordinated uncertain mobile objects
#@ Ambuj Singh;Nicholas D. Larusso
#t 2013
#c 17
#! Accurately estimating the current positions of moving objects is a challenging task due to the various forms of data uncertainty (e.g. limited sensor precision, periodic updates from continuously moving objects). However, in many cases, groups of objects tend to exhibit similarities in their movement behavior. For example, vehicles in a convoy or animals in a herd both exhibit tightly coupled movement behavior within the group. While such statistical dependencies often increase the computational complexity necessary for capturing this additional structure, they also provide useful information which can be utilized to provide more accurate location estimates. In this paper, we propose a novel model for accurately tracking coordinated groups of mobile uncertain objects. We introduce an exact and more efficient approximate inference algorithm for updating the current location of each object upon the arrival of new (uncertain) location observations. Additionally, we derive probability bounds over the groups in order to process probabilistic threshold range queries more efficiently. Our experimental evaluation shows that our proposed model can provide 4X improvements in tracking accuracy over competing models which do not consider group behavior. We also show that our bounds enable us to prune up to 50% of the database, resulting in more efficient processing over a linear scan.

#index 2010310
#* Interval reverse nearest neighbor queries on uncertain data with Markov correlations
#@ Ge Yu;Yu Gu;Jianzhong Qiao;Lei Chen;Chuanfei Xu
#t 2013
#c 17
#! Nowadays, many applications return to the user a set of results that take the query as their nearest neighbor, which are commonly expressed through reverse nearest neighbor (RNN) queries. When considering moving objects, users would like to find objects that appear in the RNN result set for a period of time in some real-world applications such as collaboration recommendation and anti-tracking. In this work, we formally define the problem of interval reverse nearest neighbor (IRNN) queries over moving objects, which return the objects that maintain nearest neighboring relations to the moving query objects for the longest time in the given interval. Location uncertainty of moving data objects and moving query objects is inherent in various domains, and we investigate objects that exhibit Markov correlations, that is, each object's location is only correlated with its own location at previous timestamp while being independent of other objects. There exists the efficiency challenge for answering IRNN queries on uncertain moving objects with Markov correlations since we have to retrieve not only all the possible locations of each object at current time but also its historically possible locations. To speed up the query processing, we present a general framework for answering IRNN queries on uncertain moving objects with Markov correlations in two phases. In the first phase, we apply space pruning and probability pruning techniques, which reduce the search space significantly. In the second phase, we verify whether each unpruned object is an IRNN of the query object. During this phase, we propose an approach termed Probability Decomposition Verification (PDV) algorithm which avoid computing the probability of any object being an RNN of the query object exactly and thus improve the efficiency of verification. The performance of the proposed algorithm is demonstrated by extensive experiments on synthetic and real datasets, and the experimental results show that our algorithm is more efficient than the Monte-Carlo based approximate algorithm.

#index 2010311
#* Tajo: A distributed data warehouse system on large clusters
#@ Hyunsik Choi;Yon Dohn Chung;Jihoon Son;Haemi Yang;Byungnam Lim;Soohyung Kim;Hyoseok Ryu
#t 2013
#c 17
#! The increasing volumes of relational data let us find an alternative to cope with them. Recently, several hybrid approaches (e.g., HadoopDB and Hive) between parallel databases and Hadoop have been introduced to the database community. Although these hybrid approaches have gained wide popularity, they cannot avoid the choice of suboptimal execution strategies. We believe that this problem is caused by the inherent limits of their architectures. In this demo, we present Tajo, a relational, distributed data warehouse system on shared-nothing clusters. It uses Hadoop Distributed File System (HDFS) as the storage layer and has its own query execution engine that we have developed instead of the MapReduce framework. A Tajo cluster consists of one master node and a number of workers across cluster nodes. The master is mainly responsible for query planning and the coordinator for workers. The master divides a query into small tasks and disseminates them to workers. Each worker has a local query engine that executes a directed acyclic graph of physical operators. A DAG of operators can take two or more input sources and be pipelined within the local query engine. In addition, Tajo can control distributed data flow more flexible than that of MapReduce and supports indexing techniques. By combining these features, Tajo can employ more optimized and efficient query processing, including the existing methods that have been studied in the traditional database research areas. To give a deep understanding of the Tajo architecture and behavior during query processing, the demonstration will allow users to submit TPC-H queries to 32 Tajo cluster nodes. The web-based user interface will show (1) how the submitted queries are planned, (2) how the query are distributed across nodes, (3) the cluster and node status, and (4) the detail of relations and their physical information. Also, we provide the performance evaluation of Tajo compared with Hive.

#index 2010312
#* Identifying hot and cold data in main-memory databases
#@ Radu Stoica;Justin J. Levandoski;Per-Ake Larson
#t 2013
#c 17
#! Main memories are becoming sufficiently large that most OLTP databases can be stored entirely in main memory, but this may not be the best solution. OLTP workloads typically exhibit skewed access patterns where some records are hot (frequently accessed) but many records are cold (infrequently or never accessed). It is more economical to store the coldest records on secondary storage such as flash. As a first step towards managing cold data in databases optimized for mainmemory we investigate how to efficiently identify hot and cold data. We propose to log record accesses — possibly only a sample to reduce overhead — and perform offline analysis to estimate record access frequencies. We present four estimation algorithms based on exponential smoothing and experimentally evaluate their efficiency and accuracy. We find that exponential smoothing produces very accurate estimates, leading to higher hit rates than the best caching techniques. Our most efficient algorithm is able to analyze a log of 1B accesses in sub-second time on a workstation-class machine.

#index 2010313
#* Top-k query processing in probabilistic databases with non-materialized views
#@ Maximilian Dylla;Martin Theobald;Iris Miliaraki
#t 2013
#c 17
#! We investigate a novel approach of computing confidence bounds for top-k ranking queries in probabilistic databases with non-materialized views. Unlike related approaches, we present an exact pruning algorithm for finding the top-ranked query answers according to their marginal probabilities without the need to first materialize all answer candidates via the views. Specifically, we consider conjunctive queries over multiple levels of select-project-join views, the latter of which are cast into Datalog rules which we ground in a top-down fashion directly at query processing time. To our knowledge, this work is the first to address integrated data and confidence computations for intensional query evaluations in the context of probabilistic databases by considering confidence bounds over first-order lineage formulas. We extend our query processing techniques by a tool-suite of scheduling strategies based on selectivity estimation and the expected impact on confidence bounds. Further extensions to our query processing strategies include improved top-k bounds in the case when sorted relations are available as input, as well as the consideration of recursive rules. Experiments with large datasets demonstrate significant runtime improvements of our approach compared to both exact and sampling-based top-k methods over probabilistic data.

#index 2010314
#* Time travel in a scientific array database
#@ Emad Soroush;Magdalena Balazinska
#t 2013
#c 17
#! In this paper, we present TimeArr, a new storage manager for an array database. TimeArr supports the creation of a sequence of versions of each stored array and their exploration through two types of time travel operations: selection of a specific version of a (sub)-array and a more general extraction of a (sub)-array history, in the form of a series of (sub)-array versions. TimeArr contributes a combination of array-specific storage techniques to efficiently support these operations. To speed-up array exploration, TimeArr further introduces two additional techniques. The first is the notion of approximate time travel with two types of operations: approximate version selection and approximate history. For these operations, users can tune the degree of approximation tolerable and thus trade-off accuracy and performance in a principled manner. The second is to lazily create short connections, called skip links, between the same (sub)-arrays at different versions with similar data patterns to speed up the selection of a specific version. We implement TimeArr within the SciDB array processing engine and demonstrate its performance through experiments on two real datasets from the astronomy and earth sciences domains.

#index 2010315
#* Time travel in column stores
#@ Martin Kaufmann;Stefan Hildenbrand;Amin A. Manjili;Andreas Tonder;Donald Kossmann
#t 2013
#c 17
#! Recent studies have shown that column stores can outperform row stores significantly. This paper explores alternative approaches to extend column stores with versioning, i.e., time travel queries and the maintenance of historic data. On the one hand, adding versioning can actually simplify the design of a column store because it provides a solution for the implementation of updates, traditionally a weak point in the design of column stores. On the other hand, implementing a versioned column store is challenging because it imposes a two dimensional clustering problem: should the data be clustered by row or by version? This paper devises the details of three memory layouts: clustering by row, clustering by version, and hybrid clustering. Performance experiments demonstrate that all three approaches outperform a (traditional) versioned row store. The efficiency of these three memory layouts depends on the query and update workload. Furthermore, the performance experiments analyze the time-space tradeoff that can be made in the implementation of versioned column stores.

#index 2010316
#* SUSIE: Search using services and information extraction
#@ Wenjun Yuan;Fabian Suchanek;Nicoleta Preda;Gerhard Weikum
#t 2013
#c 17
#! The API of a Web service restricts the types of queries that the service can answer. For example, a Web service might provide a method that returns the songs of a given singer, but it might not provide a method that returns the singers of a given song. If the user asks for the singer of some specific song, then the Web service cannot be called — even though the underlying database might have the desired piece of information. This asymmetry is particularly problematic if the service is used in a Web service orchestration system. In this paper, we propose to use on-the-fly information extraction to collect values that can be used as parameter bindings for the Web service. We show how this idea can be integrated into a Web service orchestration system. Our approach is fully implemented in a prototype called SUSIE. We present experiments with real-life data and services to demonstrate the practical viability and good performance of our approach.

#index 2010317
#* On discovery of gathering patterns from trajectories
#@ Yu Zheng;Nicholas Jing Yuan;Kai Zheng;Shuo Shang
#t 2013
#c 17
#! The increasing pervasiveness of location-acquisition technologies has enabled collection of huge amount of trajectories for almost any kind of moving objects. Discovering useful patterns from their movement behaviours can convey valuable knowledge to a variety of critical applications. In this light, we propose a novel concept, called gathering, which is a trajectory pattern modelling various group incidents such as celebrations, parades, protests, traffic jams and so on. A key observation is that these incidents typically involve large congregations of individuals, which form durable and stable areas with high density. Since the process of discovering gathering patterns over large-scale trajectory databases can be quite lengthy, we further develop a set of well thought out techniques to improve the performance. These techniques, including effective indexing structures, fast pattern detection algorithms implemented with bit vectors, and incremental algorithms for handling new trajectory arrivals, collectively constitute an efficient solution for this challenging task. Finally, the effectiveness of the proposed concepts and the efficiency of the approaches are validated by extensive experiments based on a real taxicab trajectory dataset.

#index 2010318
#* SAP HANA distributed in-memory database system: Transaction, session, and metadata management
#@ Yong Sik Kwon;Joo Yeon Lee;Juchang Lee;Chulwon Lee;Christian Bensberg;Michael Muehle;Franz Farber;Wolfgang Lehner;Arthur H. Lee
#t 2013
#c 17
#! One of the core principles of the SAP HANA database system is the comprehensive support of distributed query facility. Supporting scale-out scenarios was one of the major design principles of the system from the very beginning. Within this paper, we first give an overview of the overall functionality with respect to data allocation, metadata caching and query routing. We then dive into some level of detail for specific topics and explain features and methods not common in traditional disk-based database systems. In summary, the paper provides a comprehensive overview of distributed query processing in SAP HANA database to achieve scalability to handle large databases and heterogeneous types of workloads.

#index 2010319
#* SASH: Enabling continuous incremental analytic workflows on Hadoop
#@ Sriram Raghavan;Manish Sethi;Narendran Sachindran
#t 2013
#c 17
#! There is an emerging class of enterprise applications in areas such as log data analysis, information discovery, and social media marketing that involve analytics over large volumes of unstructured and semi-structured data. These applications are leveraging new analytics platforms based on the MapReduce framework and its open source Hadoop implementation. While this trend has engendered work on high-level data analysis languages, NoSQL data stores, workflow engines etc., there has been very little attention to the challenges of deploying analytic workflows into production for continuous operation. In this paper, we argue that an essential platform component for enabling continuous production analytic workflows is an analytics store. We highlight five key requirements that impact the design of such a store: (i) efficient incremental operations, (ii) flexible storage model for hierarchical data, (iii) snapshot support (iv) object-level incremental updates, and (v) support for handling change sets. We describe the design of SASH, a scalable analytics store that we have developed on top of HBase to address these requirements. Using the workload from a production workflow that powers search within IBM's intranet and extranet, we demonstrate orders of magnitude improvement in IO performance using SASH.

#index 2010320
#* Machine learning on Big Data
#@ Paul Mineiro;Neoklis Polyzotis;Tyson Condie;Markus Weimer
#t 2013
#c 17
#! Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.

#index 2010321
#* Focused matrix factorization for audience selection in display advertising
#@ Jeff Yuan;Bhargav Kanagal;Vanja Josifovski;Lluis Garcia-Pueyo;Amr Ahmed;Sandeep Pandey
#t 2013
#c 17
#! Audience selection is a key problem in display advertising systems in which we need to select a list of users who are interested (i.e., most likely to buy) in an advertising campaign. The users' past feedback on this campaign can be leveraged to construct such a list using collaborative filtering techniques such as matrix factorization. However, the user-campaign interaction is typically extremely sparse, hence the conventional matrix factorization does not perform well. Moreover, simply combining the users feedback from all campaigns does not address this since it dilutes the focus on target campaign in consideration. To resolve these issues, we propose a novel focused matrix factorization model (FMF) which learns users' preferences towards the specific campaign products, while also exploiting the information about related products. We exploit the product taxonomy to discover related campaigns, and design models to discriminate between the users' interest towards campaign products and non-campaign products. We develop a parallel multi-core implementation of the FMF model and evaluate its performance over a real-world advertising dataset spanning more than a million products. Our experiments demonstrate the benefits of using our models over existing approaches.

#index 2010322
#* Efficient notification of meeting points for moving groups via independent safe regions
#@ Man Lung Yiu;Jing Li;Nikos Mamoulis
#t 2013
#c 17
#! In applications like social networking services and online games, multiple moving users form a group and wish to be continuously notified with the best meeting point from their locations. To reduce the communication frequency of the application server, a promising technique is to apply safe regions, which capture the validity of query results with respect to the users' locations. Unfortunately, the safe regions in our problem exhibit characteristics such as irregular shapes and dependency among multiple safe regions. These unique characteristics render existing safe region methods that focus on a single safe region inapplicable to our problem. To tackle these challenges, we first examine the shapes of safe regions in our problem context and propose feasible approximations for them. We design efficient algorithms for computing these safe regions, as well as develop compression techniques for representing safe regions in a compact manner. Experiments with both real and synthetic data demonstrate the efficiency of our proposal in terms of computation and communication costs.

#index 2010323
#* LSII: An indexing structure for exact real-time search on microblogs
#@ Xiaokui Xiao;Yabo Xu;Lingkun Wu;Wenqing Lin
#t 2013
#c 17
#! Indexing microblogs for real-time search is challenging given the efficiency issue caused by the tremendous speed at which new microblogs are created by users. Existing approaches address this efficiency issue at the cost of query accuracy, as they either (i) exclude a significant portion of microblogs from the index to reduce update cost or (ii) rank microblogs mostly by their timestamps (without sufficient consideration of their relevance to the queries) to enable append-only index insertion. As a consequence, the search results returned by the existing approaches do not satisfy the users who demand timely and high-quality search results. To remedy this deficiency, we propose the Log-Structured Inverted Indices (LSII), a structure for exact real-time search on microblogs. The core of LSII is a sequence of inverted indices with exponentially increasing sizes, such that new microblogs are (i) first inserted into the smallest index and (ii) later moved into the larger indices in a batch manner. The batch insertion mechanism leads to a small amortize update cost for each new microblog, without significantly degrading query performance. We present a comprehensive study on LSII, exploring various design options to strike a good balance between query and update performance. In addition, we propose extensions of LSII to support personalized search and to exploit multi-threading for performance improvement. Extensive experiments demonstrate the efficiency of LSII with experiments on real data.

#index 2010324
#* Presenting diverse location views with real-time near-duplicate photo elimination
#@ Yueguo Chen;Heng Tao Shen;Hong Cheng;Jiajun Liu;Zi Huang;Yanchun Zhang
#t 2013
#c 17
#! Supported by the technical advances and the commercial success of GPS-enabled mobile devices, geo-tagged photos have drawn plenteous attention in research community. The explosive growth of geo-tagged photos enables many large-scale applications, such as location-based photo browsing, landmark recognition, etc. Meanwhile, as the number of geo-tagged photos continues to climb, new challenges are brought to various applications. The existence of massive near-duplicate geo-tagged photos jeopardizes the effective presentation for the above applications. A new dimension in the search and presentation of geo-tagged photos is urgently demanded. In this paper, we devise a location visualization framework to efficiently retrieve and present diverse views captured within a local proximity. Novel photos, in terms of capture locations and visual content, are identified and returned in response to a query location for diverse visualization. For real-time response and good scalability, a new Hybrid Index structure which integrates R-tree and Geographic Grid is proposed to quickly identify the Maximal Near-duplicate Photo Groups (MNPG) in the query proximity. The most novel photos from different groups are then returned to generate diverse views on the location. Extensive experiments on synthetic and real-life photo datasets prove the novelty and efficiency of our methods.

#index 2010325
#* Efficient search algorithm for SimRank
#@ Makoto Onizuka;Yasuhiro Fujiwara;Hiroaki Shiokawa;Makoto Nakatsuji
#t 2013
#c 17
#! Graphs are a fundamental data structure and have been employed to model objects as well as their relationships. The similarity of objects on the web (e.g., webpages, photos, music, micro-blogs, and social networking service users) is the key to identifying relevant objects in many recent applications. SimRank, proposed by Jeh and Widom, provides a good similarity score and has been successfully used in many applications such as web spam detection, collaborative tagging analysis, link prediction, and so on. SimRank computes similarities iteratively, and it needs O(N4T) time and O(N2) space for similarity computation where N and T are the number of nodes and iterations, respectively. Unfortunately, this iterative approach is computationally expensive. The goal of this work is to process top-k search and range search efficiently for a given node. Our solution, SimMat, is based on two ideas: (1) It computes the approximate similarity of a selected node pair efficiently in non-iterative style based on the Sylvester equation, and (2) It prunes unnecessary approximate similarity computations when searching for the high similarity nodes by exploiting estimations based on the Cauchy-Schwarz inequality. These two ideas reduce the time and space complexities of the proposed approach to O(Nn) where n is the target rank of the low-rank approximation (n ≪ N in practice). Our experiments show that our approach is much faster, by several orders of magnitude, than previous approaches in finding the high similarity nodes.

#index 2010326
#* ExpFinder: Finding experts by graph pattern matching
#@ Yinghui Wu;Wen fei Fan;Xin Wang
#t 2013
#c 17
#! We present ExpFinder, a system for finding experts in social networks based on graph pattern matching. We demonstrate (1) how ExpFinder identifies top-K experts in a social network by supporting bounded simulation of graph patterns, and by ranking the matches based on a metric for social impact; (2) how it copes with the sheer size of real-life social graphs by supporting incremental query evaluation and query preserving graph compression, and (3) how the GUI of ExpFinder interacts with users to help them construct queries and inspect matches.

#index 2010327
#* Πgora: An Integration System for Probabilistic Data
#@ Sebastiaan J. van Schaik;Lampros Papageorgiou;Dan Olteanu
#t 2013
#c 17
#! Πgora is an integration system for probabilistic data modelled using different formalisms such as pc-tables, Bayesian networks, and stochastic automata. User queries are expressed over a global relational layer and are evaluated by Πgora using a range of strategies, including data conversion into one probabilistic formalism followed by evaluation using a formalism-specific engine, and hybrid plans, where subqueries are evaluated using engines for different formalisms. This demonstration allows users to experience Πgora on real-world heterogeneous data sources from the medical domain.

#index 2010328
#* Complex pattern matching in complex structures: The XSeq approach
#@ Carlo Zaniolo;Kai Zeng;Barzan Mozafari;Mohan Yang
#t 2013
#c 17
#! There is much current interest in applications of complex event processing over data streams and of complex pattern matching over stored sequences. While some applications use streams of flat records, XML and various semi-structured information formats are preferred by many others-in particular, applications that deal with domain science, social networks, RSS feeds, and finance. XSeq and its system improve complex pattern matching technology significantly, both in terms of expressive power and efficient implementation. XSeq achieves higher expressiveness through an extension of XPath based on Kleene-∗ pattern constructs, and achieves very efficient execution, on both stored and streaming data, using Visibly Pushdown Automata (VPA). In our demo, we will (i) show examples of XSeq in different application domains, (ii) explain its compilation/query optimization techniques and show the speed-ups they deliver, and (iii) demonstrate how powerful and efficient application-specific languages were implemented by superimposing simple ‘skins’ on XSeq and its system.

#index 2010329
#* Destination prediction by sub-trajectory synthesis and privacy protection against such prediction
#@ Andy Yuan Xue;Rui Zhang;Yu Zheng;Xing Xie;Jin Huang;Zhenghua Xu
#t 2013
#c 17
#! Destination prediction is an essential task for many emerging location based applications such as recommending sightseeing places and targeted advertising based on destination. A common approach to destination prediction is to derive the probability of a location being the destination based on historical trajectories. However, existing techniques using this approach suffer from the “data sparsity problem”, i.e., the available historical trajectories is far from being able to cover all possible trajectories. This problem considerably limits the number of query trajectories that can obtain predicted destinations. We propose a novel method named Sub-Trajectory Synthesis (SubSyn) algorithm to address the data sparsity problem. SubSyn algorithm first decomposes historical trajectories into sub-trajectories comprising two neighbouring locations, and then connects the sub-trajectories into “synthesised” trajectories. The number of query trajectories that can have predicted destinations is exponentially increased by this means. Experiments based on real datasets show that SubSyn algorithm can predict destinations for up to ten times more query trajectories than a baseline algorithm while the SubSyn prediction algorithm runs over two orders of magnitude faster than the baseline algorithm. In this paper, we also consider the privacy protection issue in case an adversary uses SubSyn algorithm to derive sensitive location information of users. We propose an efficient algorithm to select a minimum number of locations a user has to hide on her trajectory in order to avoid privacy leak. Experiments also validate the high efficiency of the privacy protection algorithm.

#index 2010330
#* Pipe failure prediction: A data mining method
#@ Rui Wang;Xin Yao;Weishan Dong;Ke Tang;Yu Wang
#t 2013
#c 17
#! Pipe breaks in urban water distribution network lead to significant economical and social costs, putting the service quality as well as the profit of water utilities at risk. To cope with such a situation, scheduled preventive maintenance is desired, which aims to predict and fix potential break pipes proactively. Physical models developed for understanding and predicting the failure of pipes are usually expensive, thus can only be used on a limited number of trunk pipes. As an alternative, statistical models that try to predict pipe breaks based on historical data are far less expensive, and therefore have attracted a lot of interests from water utilities recently. In this paper, we report a novel data mining prediction system that has been built for a water utility in a big Chinese city. Various aspects of how to build such a system are described, including problem formulation, data cleaning, model construction, as well as evaluating the importance of attributes according to the requirements of end users in water utilities. Satisfactory results have been achieved by our prediction system. For example, with the system trained on the available dataset at the end of 2010, the water utility would avoid 50% of pipe breaks in 2011 by examining only 6.98% of its pipes in advance. During the construction of the system, we find that the extremely skew distribution of break and non-break pipes, interestingly, is not an obstacle. This lesson could serve as a practical reference for both academical studies on imbalanced learning as well as future explorations on pipe failure prediction problems.

#index 2010331
#* A unified model for stable and temporal topic detection from social media data
#@ Hongzhi Yin;Bin Cui;Yuxin Huang;Hua Lu;Junjie Yao
#t 2013
#c 17
#! Web 2.0 users generate and spread huge amounts of messages in online social media. Such user-generated contents are mixture of temporal topics (e.g., breaking events) and stable topics (e.g., user interests). Due to their different natures, it is important and useful to distinguish temporal topics from stable topics in social media. However, such a discrimination is very challenging because the user-generated texts in social media are very short in length and thus lack useful linguistic features for precise analysis using traditional approaches. In this paper, we propose a novel solution to detect both stable and temporal topics simultaneously from social media data. Specifically, a unified user-temporal mixture model is proposed to distinguish temporal topics from stable topics. To improve this model's performance, we design a regularization framework that exploits prior spatial information in a social network, as well as a burst-weighted smoothing scheme that exploits temporal prior information in the time dimension. We conduct extensive experiments to evaluate our proposal on two real data sets obtained from Del.icio.us and Twitter. The experimental results verify that our mixture model is able to distinguish temporal topics from stable topics in a single detection process. Our mixture model enhanced with the spatial regularization and the burst-weighted smoothing scheme significantly outperforms competitor approaches, in terms of topic detection accuracy and discrimination in stable and temporal topics.

#index 2010332
#* Crowdsourced enumeration queries
#@ Michael J. Franklin;Beth Trushkowsky;Purnamrita Sarkar;Tim Kraska
#t 2013
#c 17
#! Hybrid human/computer database systems promise to greatly expand the usefulness of query processing by incorporating the crowd for data gathering and other tasks. Such systems raise many implementation questions. Perhaps the most fundamental question is that the closed world assumption underlying relational query semantics does not hold in such systems. As a consequence the meaning of even simple queries can be called into question. Furthermore, query progress monitoring becomes difficult due to non-uniformities in the arrival of crowdsourced data and peculiarities of how people work in crowdsourcing systems. To address these issues, we develop statistical tools that enable users and systems developers to reason about query completeness. These tools can also help drive query execution and crowdsourcing strategies. We evaluate our techniques using experiments on a popular crowdsourcing platform.

#index 2010333
#* On incentive-based tagging
#@ Xuan S. Yang;David W. Cheung;Luyi Mo;Reynold Cheng;Ben Kao
#t 2013
#c 17
#! A social tagging system, such as del.icio.us and Flickr, allows users to annotate resources (e.g., web pages and photos) with text descriptions called tags. Tags have proven to be invaluable information for searching, mining, and recommending resources. In practice, however, not all resources receive the same attention from users. As a result, while some highly-popular resources are over-tagged, most of the resources are under-tagged. Incomplete tagging on resources severely affects the effectiveness of all tag-based techniques and applications. We address an interesting question: if users are paid to tag specific resources, how can we allocate incentives to resources in a crowd-sourcing environment so as to maximize the tagging quality of resources? We address this question by observing that the tagging quality of a resource becomes stable after it has been tagged a sufficient number of times. We formalize the concepts of tagging quality (TQ) and tagging stability (TS) in measuring the quality of a resource's tag description. We propose a theoretically optimal algorithm given a fixed “budget” (i.e., the amount of money paid for tagging resources). This solution decides the amount of rewards that should be invested on each resource in order to maximize tagging stability. We further propose a few simple, practical, and efficient incentive allocation strategies. On a dataset from del.icio.us, our best strategy provides resources with a close-to-optimal gain in tagging stability.

#index 2010334
#* Hardware killed the software star
#@ Gustavo Alonso
#t 2013
#c 17
#! Until relatively recently, the development of data processing applications took place largely ignoring the underlying hardware. Only in niche applications (supercomputing, embedded systems) or in special software (operating systems, database internals, language runtimes) did (some) programmers had to pay attention to the actual hardware where the software would run. In most cases, working atop the abstractions provided by either the operating system or by system libraries was good enough. The constant improvements in processor speed did the rest. The new millennium has radically changed the picture. Driven by multiple needs — e.g., scale, physical constraints, energy limitations, virtualization, business models- hardware architectures are changing at a speed and in ways that current development practices for data processing cannot accommodate. From now on, software will have to be developed paying close attention to the underlying hardware and following strict performance engineering principles. In this paper, several aspects of the ongoing hardware revolution and its impact on data processing are analysed, pointing to the need for new strategies to tackle the challenges ahead.

#index 2010335
#* Trustworthy data from untrusted databases
#@ Rohit Jain;Sunil Prabhakar
#t 2013
#c 17
#! Ensuring the trustworthiness of data retrieved from a database is of utmost importance to users. The correctness of data stored in a database is defined by the faithful execution of only valid (authorized) transactions. In this paper we address the question of whether it is necessary to trust a database server in order to trust the data retrieved from it. The lack of trust arises naturally if the database server is owned by a third party, as in the case of cloud computing. It also arises if the server may have been compromised, or there is a malicious insider. In particular, we reduce the level of trust necessary in order to establish the authenticity and integrity of data at an untrusted server. Earlier work on this problem is limited to situations where there are no updates to the database, or all updates are authorized and vetted by a central trusted entity. This is an unreasonable assumption for a truly dynamic database, as would be expected in many business applications, where multiple clients can update data without having to check with a central server that approves of their changes. We identify the problem of ensuring trustworthiness of data at an untrusted server in the presence of transactional updates that run directly on the database, and develop the first solutions to this problem. Our solutions also provide indemnity for an honest server and assured provenance for all updates to the data. We implement our solution in a prototype system built on top of Oracle with no modifications to the database internals. We also provide an empirical evaluation of the proposed solutions and establish their feasibility.

#index 2010336
#* C-Cube: Elastic continuous clustering in the cloud
#@ Zhenjie Zhang;Hua Lu;Hu Shu;Yin Yang;Zhihong Chong
#t 2013
#c 17
#! Continuous clustering analysis over a data stream reports clustering results incrementally as updates arrive. Such analysis has a wide spectrum of applications, including traffic monitoring and topic discovery on microblogs. A common characteristic of streaming applications is that the amount of workload fluctuates, often in an unpredictable manner. On the other hand, most existing solutions for continuous clustering assume either a central server, or a distributed setting with a fixed number of dedicated servers. In other words, they are not ELASTIC, meaning that they cannot dynamically adapt to the amount of computational resources to the fluctuating workload. Consequently, they incur considerable waste of resources, as the servers are under-utilized when the amount of workload is low. This paper proposes C-Cube, the first elastic approach to continuous streaming clustering. Similar to popular cloud-based paradigms such as MapReduce, C-Cube routes each new record to a processing unit, e.g., a virtual machine, based on its hash value. Each processing unit performs the required computations, and sends its results to a lightweight aggregator. This design enables dynamic adding/removing processing units, as well as replacing faulty ones and re-running their tasks. In addition to elasticity, C-Cube is also effective (in that it provides quality guarantees on the clustering results), efficient (it minimizes the computational workload at all times), and generally applicable to a large class of clustering criteria. We implemented C-Cube in a real system based on Twitter Storm, and evaluated it using real and synthetic datasets. Extensive experimental results confirm our performance claims.

#index 2010337
#* Recent progress towards an ecosystem of structured data on the Web
#@ Hongrae Lee;Jayant Madhavan;Heidi Lam;Alon Y. Halevy;Boulos Harb;Nitin Gupta;Cong Yu;Fei Wu
#t 2013
#c 17
#! Google Fusion Tables aims to support an ecosystem of structured data on the Web by providing a tool for managing and visualizing data on the one hand, and for searching and exploring for data on the other. This paper describes a few recent developments in our efforts to further the ecosystem.

#index 2010338
#* Sorting in Space: Multidimensional, spatial, and metric data structures for applications in spatial databases, geographic information systems (GIS), and location-based services
#@ Hanan Samet
#t 2013
#c 17
#! Techniques for representing multidimensional, spatial, and metric data for applications in spatial databases, geographic information systems (GIS), and location-based services are reviewed. This includes both geometric and textual representations of spatial data.

#index 2010339
#* Query time scaling of attribute values in interval timestamped databases
#@ Anton Dignos;Johann Gamper;Michael Bohlen
#t 2013
#c 17
#! In valid-time databases with interval timestamping each tuple is associated with a time interval over which the recorded fact is true in the modeled reality. The adjustment of these intervals is an essential part of processing interval timestamped data. Some attribute values remain valid if the associated interval changes, whereas others have to be scaled along with the time interval. For example, attributes that record total (cumulative) quantities over time, such as project budgets, total sales or total costs, often must be scaled if the timestamp is adjusted. The goal of this demo is to show how to support the scaling of attribute values in SQL at query time.

#index 2010340
#* Inverted linear quadtree: Efficient top k spatial keyword search
#@ Chengyuan Zhang;Ying Zhang;Wenjie Zhang;Xuemin Lin
#t 2013
#c 17
#! With advances in geo-positioning technologies and geo-location services, there are a rapidly growing amount of spatio-textual objects collected in many applications such as location based services and social networks, in which an object is described by its spatial location and a set of keywords (terms). Consequently, the study of spatial keyword search which explores both location and textual description of the objects has attracted great attention from the commercial organizations and research communities. In the paper, we study the problem of top k spatial keyword search (TOPK-SK), which is fundamental in the spatial keyword queries. Given a set of spatio-textual objects, a query location and a set of query keywords, the top k spatial keyword search retrieves the closest k objects each of which contains all keywords in the query. Based on the inverted index and the linear quadtree, we propose a novel index structure, called inverted linear quadtree (IL-Quadtree), which is carefully designed to exploit both spatial and keyword based pruning techniques to effectively reduce the search space. An efficient algorithm is then developed to tackle top k spatial keyword search. In addition, we show that the IL-Quadtree technique can also be applied to improve the performance of other spatial keyword queries such as the direction-aware top k spatial keyword search and the spatio-textual ranking query. Comprehensive experiments on real and synthetic data clearly demonstrate the efficiency of our methods.

#index 2010341
#* Similarity query processing for probabilistic sets
#@ Ming Gao;Cheqing Jin;Wei Wang;Aoying Zhou;Xuemin Lin
#t 2013
#c 17
#! Evaluating similarity between sets is a fundamental task in computer science. However, there are many applications in which elements in a set may be uncertain due to various reasons. Existing work on modeling such probabilistic sets and computing their similarities suffers from huge model sizes or significant similarity evaluation cost, and hence is only applicable to small probabilistic sets. In this paper, we propose a simple yet expressive model that supports many applications where one probabilistic set may have thousands of elements. We define two types of similarities between two probabilistic sets using the possible world semantics; they complement each other in capturing the similarity distributions in the cross product of possible worlds. We design efficient dynamic programming-based algorithms to calculate both types of similarities. Novel individual and batch pruning techniques based on upper bounding the similarity values are also proposed. To accommodate extremely large probabilistic sets, we also design sampling-based approximate query processing methods with strong probabilistic guarantees. We have conducted extensive experiments using both synthetic and real datasets, and demonstrated the effectiveness and efficiency of our proposed methods.

#index 2010342
#* Ficklebase: Looking into the future to erase the past
#@ Sumeet Bajaj;Radu Sion
#t 2013
#c 17
#! It has become apparent that in the digital world data once stored is never truly deleted even when such an expunction is desired either as a normal system function or for regulatory compliance purposes. Forensic Analysis techniques on systems are often successful at recovering information said to have been deleted in the past. Efforts aimed at thwarting such forensic analysis of systems have either focused on (i) identifying the system components where deleted data lingers and performing a secure delete operation over these remnants, or (ii) designing history independent data structures that hide information about past operations which result in the current system state. Yet, new data is constantly derived by processing existing (input) data which makes it increasingly difficult to remove all traces of this existing data, i.e., for regulatory compliance purposes. Even after deletion, significant information can linger in and be recoverable from the side effects the deleted data records left on the currently available state. In this paper we address this aspect in the context of a relational database, such that when combined with (i) & (ii), complete erasure of data and its effects can be achieved (““un-traceable deletion”). We introduce Ficklebase — a relational database wherein once a tuple has been “expired” — any and all its side-effects are removed, thereby eliminating all its traces, rendering it unrecoverable, and also guaranteeing that the deletion itself is undetectable. We present the design and evaluation of Ficklebase, and then discuss several of the fundamental functional implications of un-traceable deletion.

#index 2010343
#* The adaptive radix tree: ARTful indexing for main-memory databases
#@ Viktor Leis;Alfons Kemper;Thomas Neumann
#t 2013
#c 17
#! Main memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck. Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches. Hash tables, also often used for main-memory indexes, are fast but only support point queries. To overcome these shortcomings, we present ART, an adaptive radix tree (trie) for efficient indexing in main memory. Its lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well. At the same time, ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes. Even though ART's performance is comparable to hash tables, it maintains the data in sorted order, which enables additional operations like range scan and prefix lookup.

#index 2010344
#* Workload management for Big Data analytics
#@ Ashraf Aboulnaga;Shivnath Babu
#t 2013
#c 17
#! Parallel database systems and MapReduce systems (most notably Hadoop) are essential components of today's infrastructure for Big Data analytics. These systems process multiple concurrent workloads consisting of complex user requests, where each request is associated with an (explicit or implicit) service level objective. For example, the workload of a particular user or application may have a higher priority than other workloads. Or a particular workload may have strict deadlines for the completion of its requests.

#index 2010345
#* Triples in the clouds
#@ Ioana Manolescu;Zoi Kaoudi
#t 2013
#c 17
#! The W3C's Resource Description Framework (or RDF, in short) is a promising candidate which may deliver many of the original semi-structured data promises: flexible structure, optional schema, and rich, flexible URIs as a basis for information sharing. Moreover, RDF is uniquely positioned to benefit from the efforts of scientific communities studying databases, knowledge representation, and Web technologies. Many RDF data collections are being published, going from scientific data to general-purpose ontologies to open government data, in particular in the Linked Data movement. Managing such large volumes of RDF data is challenging, due to the sheer size, the heterogeneity, and the further complexity brought by RDF reasoning. To tackle the size challenge, distributed storage architectures are required. Cloud computing is an emerging paradigm massively adopted in many applications for the scalability, fault-tolerance and elasticity features it provides. This tutorial discusses the problems involved in efficiently handling massive amounts of RDF data in a cloud environment. We provide the necessary background, analyze and classify existing solutions, and discuss open problems and perspectives.

#index 2010346
#* Inferring data currency and consistency for conflict resolution
#@ Wenfei Fan;Nan Tang;Floris Geerts;Wenyuan Yu
#t 2013
#c 17
#! This paper introduces a new approach for conflict resolution: given a set of tuples pertaining to the same entity, it is to identify a single tuple in which each attribute has the latest and consistent value in the set. This problem is important in data integration, data cleaning and query answering. It is, however, challenging since in practice, reliable timestamps are often absent, among other things. We propose a model for conflict resolution, by specifying data currency in terms of partial currency orders and currency constraints, and by enforcing data consistency with constant conditional functional dependencies. We show that identifying data currency orders helps us repair inconsistent data, and vice versa. We investigate a number of fundamental problems associated with conflict resolution, and establish their complexity. In addition, we introduce a framework and develop algorithms for conflict resolution, by integrating data currency and consistency inferences into a single process, and by interacting with users. We experimentally verify the accuracy and efficiency of our methods using real-life and synthetic data.

#index 2010347
#* Twitter+: Build personalized newspaper for Twitter
#@ Chen Liu;Anthony K.  H. Tung
#t 2013
#c 17
#! Nowadays, microblogging services, e.g., Twitter, have played important roles in people's everyday lives. It enables users to publish and read text-based posts, known as “tweets” and interact with each other through re-tweeting or commenting. In the literature, many efforts have been devoted on exploiting the social property of Twitter. However, except the social component, Twitter itself has become an indispensable source for users to acquire useful information. To maximize its value, we expect to pay more attention on the media property of Twitter. To be good media, the first requirement is that it should provide an effective presentation of its news so that users are facilitated of reading. Currently, all tweets from followings are presented to the users and usually organized by their published timelines or coming sources. However, too few dimensions of presenting tweets hinder users from finding their interested information conveniently. In this demo, we presents “Twitter+”, which aims to enrich user's reading experiences in Twitter by providing multiple ways for them to explore tweets, such as keyword presentation, topic finding. It presents users an alternative interface to browse tweets more effectively.

#index 2010348
#* Aeolus: An optimizer for distributed intra-node-parallel streaming systems
#@ Meichun Hsu;Matthias J. Sax;Qiming Chen;Malu Castellanos
#t 2013
#c 17
#! Aeolus is a prototype implementation of a topology optimizer on top of the distributed streaming system Storm. Aeolus extends Storm with a batching layer which can increase the topology's throughput by more than one order of magnitude. Furthermore, Aeolus implements an optimization algorithm that computes the optimal batch size and degree of parallelism for each node in the topology automatically. Even if Aeolus is built on top of Storm, the developed concepts are not limited to Storm and can be applied to any distributed intra-node-parallel streaming system. We propose to demo Aeolus using an interactive Web UI. One part of the Web UI is a topology builder allowing the user to interact with the system. Topologies can be created from scratch and their structure and/or parameters can be modified. Furthermore, the user is able to observe the impact of the changes on the optimization decisions and runtime behavior. Additionally, the Web UI gives a deep insight in the optimization process by visualizing it. The user can interactively step through the optimization process while the UI shows the optimizer's state, computations, and decisions. The Web UI is also able to monitor the execution of a non-optimized and optimized topology simultaneously showing the advantage of using Aeolus.

#index 2010349
#* Predicting query execution time: Are optimizer cost models really unusable?
#@ Hakan Hacigumus;Yun Chi;Wentao Wu;Shenghuo Zhu;Junichi Tatemura;Jeffrey F. Naughton
#t 2013
#c 17
#! Predicting query execution time is useful in many database management issues including admission control, query scheduling, progress monitoring, and system sizing. Recently the research community has been exploring the use of statistical machine learning approaches to build predictive models for this task. An implicit assumption behind this work is that the cost models used by query optimizers are insufficient for query execution time prediction. In this paper we challenge this assumption and show while the simple approach of scaling the optimizer's estimated cost indeed fails, a properly calibrated optimizer cost model is surprisingly effective. However, even a well-tuned optimizer cost model will fail in the presence of errors in cardinality estimates. Accordingly we investigate the novel idea of spending extra resources to refine estimates for the query plan after it has been chosen by the optimizer but before execution. In our experiments we find that a well calibrated query optimizer model along with cardinality estimation refinement provides a low overhead way to provide estimates that are always competitive and often much better than the best reported numbers from the machine learning approaches.

#index 2010350
#* Data services for E-tailers leveraging web search engine assets
#@ Surajit Chaudhuri;Manoj Syamala;Tao Cheng;Vivek Narasayya;Kaushik Chakrabarti
#t 2013
#c 17
#! Retail is increasingly moving online. There are only a few big e-tailers but there is a long tail of small-sized e-tailers. The big e-tailers are able to collect significant data on user activities at their websites. They use these assets to derive insights about their products and to provide superior experiences for their users. On the other hand, small e-tailers do not possess such user data and hence cannot match the rich user experiences offered by big e-tailers. Our key insight is that web search engines possess significant data on user behaviors that can be used to help smaller e-tailers mine the same signals that big e-tailers derive from their proprietary user data assets. These signals can be exposed as data services in the cloud; e-tailers can leverage them to enable similar user experiences as the big e-tailers. We present three such data services in the paper: entity synonym data service, query-to-entity data service and entity tagging data service. The entity synonym service is an in-production data service that is currently available while the other two are data services currently in development at Microsoft. Our experiments on product datasets show (i) these data services have high quality and (ii) they have significant impact on user experiences on e-tailer websites. To the best of our knowledge, this is the first paper to explore the potential of using search engine data assets for e-tailers.

#index 2010351
#* Very fast estimation for result and accuracy of big data analytics: The EARL system
#@ Carlo Zaniolo;Kai Zeng;Nikolay Laptev
#t 2013
#c 17
#! Approximate results based on samples often provide the only way in which advanced analytical applications on very massive data sets (a.k.a. ‘big data’) can satisfy their time and resource constraints. Unfortunately, methods and tools for the computation of accurate early results are currently not supported in big data systems (e.g., Hadoop). Therefore, we propose a nonparametric accuracy estimation method and system to speedup big data analytics. Our framework is called EARL (Early Accurate Result Library) and it works by predicting the learning curve and choosing the appropriate sample size for achieving the desired error bound specified by the user. The error estimates are based on a technique called bootstrapping that has been widely used and validated by statisticians, and can be applied to arbitrary functions and data distributions. Therefore, this demo will elucidate (a) the functionality of EARL and its intuitive GUI interface whereby first-time users can appreciate the accuracy obtainable from increasing sample sizes by simply viewing the learning curve displayed by EARL, (b) the usability of EARL, whereby conference participants can interact with the system to quickly estimate the sample sizes needed to obtain the desired accuracies or response times, and then compare them against the accuracies and response times obtained in the actual computations.

#index 2010352
#* Extracting interesting related context-dependent concepts from social media streams using temporal distributions
#@ Meichun Hsu;Craig P. Sayers
#t 2013
#c 17
#! To enable the interactive exploration of large social media datasets we exploit the temporal distributions of word n-grams within the message stream to discover “interesting” concepts, determine “relatedness” between concepts, and find representative examples for display. We present a new algorithm for context-dependent “interestingness” using the coefficient of variation of the temporal distribution, apply the well-known technique of Pearson's Correlation to tweets using equi-height histogramming to determine correlation, and employ an asymmetric variant for computing “relatedness” to encourage exploration. We further introduce techniques using interestingness, correlation, and relatedness to automatically discover concepts and select preferred word N-grams for display. These techniques are demonstrated on an 800,000 tweet dataset from the Academy Awards.

#index 2010353
#* KORS: Keyword-aware Optimal Route Search System
#@ Gao Cong;Lisi Chen;Xiaokui Xiao;Xin Cao;Nhan-Tue Phan;Jihong Guan
#t 2013
#c 17
#! We present the Keyword-aware Optimal Route Search System (KORS), which efficiently answers the KOR queries. A KOR query is to find a route such that it covers a set of user-specified keywords, a specified budget constraint is satisfied, and an objective score of the route is optimized. Consider a tourist who wants to spend a day exploring a city. The user may issue the following KOR query: “find the most popular route such that it passes by shopping mall, restaurant, and pub, and the travel time to and from her hotel is within 4 hours.” KORS provides browser-based interfaces for desktop and laptop computers and provides a client application for mobile devices as well. The interfaces and the client enable users to formulate queries and view the query results on a map. Queries are then sent to the server for processing by the HTTP post operation. Since answering a KOR query is NP-hard, we devise two approximation algorithms with provable performance bounds and one greedy algorithm to process the KOR queries in our KORS prototype. We use two real-world datasets to demonstrate the functionality and performance of this system.

#index 2010354
#* EAGRE: Towards scalable I/O efficient SPARQL query evaluation on the cloud
#@ Xiaofei Zhang;Lei Chen;Yongxin Tong;Min Wang
#t 2013
#c 17
#! To benefit from the Cloud platform's unlimited resources, managing and evaluating huge volume of RDF data in a scalable manner has attracted intensive research efforts recently. Progresses have been made on evaluating SPARQL queries with either high-level declarative programming languages, like Pig [1], or a sequence of sophisticated designed MapReduce jobs, both of which tend to answer the query with multiple join operations. However, due to the simplicity of Cloud storage and the coarse organization of RDF data in existing solutions, multiple join operations easily bring significant I/O and network traffic which can severely degrade the system performance. In this work, we first propose EAGRE, an Entity-Aware Graph compREssion technique to form a new representation of RDF data on Cloud platforms, based on which we propose an I/O efficient strategy to evaluate SPARQL queries as quickly as possible, especially queries with specified solution sequence modifiers, e.g., PROJECTION, ORDER BY, etc. We implement a prototype system and conduct extensive experiments over both real and synthetic datasets on an in-house cluster. The experimental results show that our solution can achieve over an order of magnitude of time saving for the SPARQL query evaluation compared to the state-of-art MapReduce-based solutions.

#index 2010355
#* Ontology-based subgraph querying
#@ Yinghui Wu;Xifeng Yan;Shengqi Yang
#t 2013
#c 17
#! Subgraph querying has been applied in a variety of emerging applications. Traditional subgraph querying based on subgraph isomorphism requires identical label matching, which is often too restrictive to capture the matches that are semantically close to the query graphs. This paper extends subgraph querying to identify semantically related matches by leveraging ontology information. (1) We introduce the ontology-based subgraph querying, which revises subgraph isomorphism by mapping a query to semantically related subgraphs in terms of a given ontology graph. We introduce a metric to measure the similarity of the matches. Based on the metric, we introduce an optimization problem to find top K best matches. (2) We provide a filtering-and-verification framework to identify (top-K) matches for ontology-based subgraph queries. The framework efficiently extracts a small subgraph of the data graph from an ontology index, and further computes the matches by only accessing the extracted subgraph. (3) In addition, we show that the ontology index can be efficiently updated upon the changes to the data graphs, enabling the framework to cope with dynamic data graphs. (4) We experimentally verify the effectiveness and efficiency of our framework using both synthetic and real life graphs, comparing with traditional subgraph querying methods.

#index 2010356
#* ASVTDECTOR: A practical near duplicate video retrieval system
#@ Lei Chen;Xiangmin Zhou
#t 2013
#c 17
#! In this paper, we present a system, named ASVT-DECTOR, to retrieve the near duplicate videos with large variations based on an 3D structure tensor model, named ASVT series, over the local descriptors of video segments. Different from the traditional global feature-based video detection systems that incur severe information loss, ASVT model is built over the local descriptor set of each video segment, keeping the robustness of local descriptors. Meanwhile, unlike the traditional local feature-based methods that suffer from the high cost of pair-wise descriptor comparison, ASVT model describes a video segment as an 3D structure tensor that is actually a 3脳3 matrix, obtaining high retrieval efficiency. In this demonstration, we show that, given a clip, our ASVTDETECTOR system can effectively find the near-duplicates with large variations from a large collection in real time.

#index 2010357
#* Logical provenance in data-oriented workflows?
#@ Robert Ikeda;Jennifer Widom;Akash Das Sarma
#t 2013
#c 17
#! We consider the problem of defining, generating, and tracing provenance in data-oriented workflows, in which input data sets are processed by a graph of transformations to produce output results. We first give a new general definition of provenance for general transformations, introducing the notions of correctness, precision, and minimality. We then determine when properties such as correctness and minimality carry over from the individual transformations' provenance to the workflow provenance. We describe a simple logical-provenance specification language consisting of attribute mappings and filters. We provide an algorithm for provenance tracing in workflows where logical provenance for each transformation is specified using our language. We consider logical provenance in the relational setting, observing that for a class of Select-Project-Join (SPJ) transformations, logical provenance specifications encode minimal provenance. We have built a prototype system supporting the features and algorithms presented in the paper, and we report a few preliminary experimental results.

#index 2010358
#* Top-k string similarity search with edit-distance constraints
#@ Guoliang Li;Dong Deng;Jianhua Feng;Wen-Syan Li
#t 2013
#c 17
#! String similarity search is a fundamental operation in many areas, such as data cleaning, information retrieval, and bioinformatics. In this paper we study the problem of top-k string similarity search with edit-distance constraints, which, given a collection of strings and a query string, returns the top-k strings with the smallest edit distances to the query string. Existing methods usually try different edit-distance thresholds and select an appropriate threshold to find top-k answers. However it is rather expensive to select an appropriate threshold. To address this problem, we propose a progressive framework by improving the traditional dynamic-programming algorithm to compute edit distance. We prune unnecessary entries in the dynamic-programming matrix and only compute those pivotal entries. We extend our techniques to support top-k similarity search. We develop a range-based method by grouping the pivotal entries to avoid duplicated computations. Experimental results show that our method achieves high performance, and significantly outperforms state-of-the-art approaches on real-world datasets.

#index 2010359
#* YumiInt — A deep Web integration system for local search engines for Geo-referenced objects
#@ Bhaskar DasGupta;A. Neyestani;Eduard Dragut;B. Atassi;Meng Weiyi;B. P. Beirne;Clement Yu
#t 2013
#c 17
#! We present YumiInt a deep Web integration system for local search engines for Geo-referenced objects. YumiInt consists of two systems: YumiDev and YumiMeta. YumiDev is an off-line integration system that builds the key components (e.g., query translation and entity resolution) of YumiMeta. YumiMeta is the Web application to which users post queries. It translates queries to multiple sources and gets back aggregated lists of results. We present the two systems in this paper.

#index 2010360
#* Fast peak-to-peak behavior with SSD buffer pool
#@ David J. DeWitt;Jaeyoung Do;Jignesh M. Patel;Donghui Zhang
#t 2013
#c 17
#! A promising use of flash SSDs in a DBMS is to extend the main memory buffer pool by caching selected pages that have been evicted from the buffer pool. Such a use has been shown to produce significant gains in the steady state performance of the DBMS. One strategy for using the SSD buffer pool is to throw away the data in the SSD when the system is restarted (either when recovering from a crash or restarting after a shutdown), and consequently a long “ramp-up” period to regain peak performance is needed. One approach to eliminate this limitation is to use a memory-mapped file to store the SSD buffer table in order to be able to restore its contents on restart. However, this design can result in lower sustained performance, because every update to the SSD buffer table may incur an I/O operation to the memory-mapped file. In this paper we propose two new alternative designs. One design reconstructs the SSD buffer table using transactional logs. The other design asynchronously flushes the SSD buffer table, and upon restart, lazily verifies the integrity of the data cached in the SSD buffer pool. We have implemented these three designs in SQL Server 2012. For each design, both the write-through and write-back SSD caching policies were implemented. Using two OLTP benchmarks (TPC-C and TPC-E), our experimental results show that our designs produce up to 3.8X speedup on the interval between peak-to-peak performance, with negligible performance loss; in contrast, the previous approach has a similar speedup but up to 54% performance loss.

#index 2010361
#* Coupled clustering ensemble: Incorporating coupling relationships both between base clusterings and objects
#@ Can Wang;Longbing Cao;Zhong She
#t 2013
#c 17
#! Clustering ensemble is a powerful approach for improving the accuracy and stability of individual (base) clustering algorithms. Most of the existing clustering ensemble methods obtain the final solutions by assuming that base clusterings perform independently with one another and all objects are independent too. However, in real-world data sources, objects are more or less associated in terms of certain coupling relationships. Base clusterings trained on the source data are complementary to one another since each of them may only capture some specific rather than full picture of the data. In this paper, we discuss the problem of explicating the dependency between base clusterings and between objects in clustering ensembles, and propose a framework for coupled clustering ensembles (CCE). CCE not only considers but also integrates the coupling relationships between base clusterings and between objects. Specifically, we involve both the intra-coupling within one base clustering (i.e., cluster label frequency distribution) and the inter-coupling between different base clusterings (i.e., cluster label co-occurrence dependency). Furthermore, we engage both the intra-coupling between two objects in terms of the base clustering aggregation and the inter-coupling among other objects in terms of neighborhood relationship. This is the first work which explicitly addresses the dependency between base clusterings and between objects, verified by the application of such couplings in three types of consensus functions: clustering-based, object-based and cluster-based. Substantial experiments on synthetic and UCI data sets demonstrate that the CCE framework can effectively capture the interactions embedded in base clusterings and objects with higher clustering accuracy and stability compared to several state-of-the-art techniques, which is also supported by statistical analysis.

#index 2010362
#* Efficient direct search on compressed genomic data
#@ Xiaohui Xie;Xiaochun Yang;Jiaying Wang;Bin Wang;Chen Li
#t 2013
#c 17
#! The explosive growth in the amount of data produced by next-generation sequencing poses significant computational challenges on how to store, transmit and query these data, efficiently and accurately. A unique characteristic of the genomic sequence data is that many of them can be highly similar to each other, which has motivated the idea of compressing sequence data by storing only their differences to a reference sequence, thereby drastically cutting the storage cost. However, an unresolved question in this area is whether it is possible to perform search directly on the compressed data, and if so, how. Here we show that directly querying compressed genomic sequence data is possible and can be done efficiently. We describe a set of novel index structures and algorithms for this purpose, and present several optimization techniques to reduce the space requirement and query response time. We demonstrate the advantage of our method and compare it against existing ones through a thorough experimental study on real genomic data.

#index 2010363
#* Re-thinking the performance of information processing systems
#@ Vishal Sikka
#t 2013
#c 17
#! Recent advances in hardware and software technologies have enabled us to re-think how we architect databases to meet the demands of today's information systems. However, this makes existing performance evaluation metrics obsolete. In this paper, I describe SAP HANA a novel, powerful database platform that leverages the availability of large main memory and massively parallel processors. Based on this, I propose a new, multi-dimensional performance metric that better reflects the value expected from today's complex information systems.

#index 2010364
#* HFMS: Managing the lifecycle and complexity of hybrid analytic data flows
#@ Meichun Hsu;Alkis Simitsis;Umeshwar Dayal;Kevin Wilkinson
#t 2013
#c 17
#! To remain competitive, enterprises are evolving their business intelligence systems to provide dynamic, near realtime views of business activities. To enable this, they deploy complex workflows of analytic data flows that access multiple storage repositories and execution engines and that span the enterprise and even outside the enterprise. We call these multi-engine flows hybrid flows. Designing and optimizing hybrid flows is a challenging task. Managing a workload of hybrid flows is even more challenging since their execution engines are likely under different administrative domains and there is no single point of control. To address these needs, we present a Hybrid Flow Management System (HFMS). It is an independent software layer over a number of independent execution engines and storage repositories. It simplifies the design of analytic data flows and includes optimization and executor modules to produce optimized executable flows that can run across multiple execution engines. HFMS dispatches flows for execution and monitors their progress. To meet service level objectives for a workload, it may dynamically change a flow's execution plan to avoid processing bottlenecks in the computing infrastructure. We present the architecture of HFMS and describe its components. To demonstrate its potential benefit, we describe performance results for running sample batch workloads with and without HFMS. The ability to monitor multiple execution engines and to dynamically adjust plans enables HFMS to provide better service guarantees and better system utilization.

#index 2010365
#* Maximum visibility queries in spatial databases
#@ Mohammed Eunus Ali;Farhana Murtaza Choudhury;Sarana Nutanong;Sarah Masud
#t 2013
#c 17
#! Many real-world problems, such as placement of surveillance cameras and pricing of hotel rooms with a view, require the ability to determine the visibility of a given target object from different locations. Advances in large-scale 3D modeling (e.g., 3D virtual cities) provide us with data that can be used to solve these problems with high accuracy. In this paper, we investigate the problem of finding the location which provides the best view of a target object with visual obstacles in 2D or 3D space, for example, finding the location that provides the best view of fireworks in a city with tall buildings. To solve this problem, we first define the quality measure of a view (i.e., visibility measure) as the visible angular size of the target object. Then, we propose a new query type called the k-Maximum Visibility (kMV) query, which finds k locations from a set of locations that maximize the visibility of the target object. Our objective in this paper is to design a query solution which is capable of handling large-scale city models. This objective precludes the use of approaches that rely on constructing a visibility graph of the entire data space. As a result, we propose three approaches that incrementally consider relevant obstacles in order to determine the visibility of a target object from a given set of locations. These approaches differ in the order of obstacle retrieval, namely: query centric distance based, query centric visible region based, and target centric distance based approaches. We have conducted an extensive experimental study on real 2D and 3D datasets to demonstrate the efficiency and effectiveness of our solutions.

#index 2010366
#* Finding connected components in map-reduce in logarithmic rounds
#@ Laukik Chitnis;Anish Das Sarma;Ashwin Machanavajjhala;Vibhor Rastogi
#t 2013
#c 17
#! Given a large graph G = (V, E) with millions of nodes and edges, how do we compute its connected components efficiently? Recent work addresses this problem in map-reduce, where a fundamental trade-off exists between the number of map-reduce rounds and the communication of each round. Denoting d the diameter of the graph, and n the number of nodes in the largest component, all prior techniques for map-reduce either require a linear, Θ(d), number of rounds, or a quadratic, Θ (n|V| + |E|), communication per round. We propose here two efficient map-reduce algorithms: (i) Hash-Greater-to-Min, which is a randomized algorithm based on PRAM techniques, requiring O(log n) rounds and O(|V | + |E|) communication per round, and (ii) Hash-to-Min, which is a novel algorithm, provably finishing in O(log n) iterations for path graphs. The proof technique used for Hash-to-Min is novel, but not tight, and it is actually faster than Hash-Greater-to-Min in practice. We conjecture that it requires 2 log d rounds and 3(|V| + |E|) communication per round, as demonstrated in our experiments. Using secondary sorting, a standard map-reduce feature, we scale Hash-to-Min to graphs with very large connected components. Our techniques for connected components can be applied to clustering as well. We propose a novel algorithm for agglomerative single linkage clustering in map-reduce. This is the first map-reduce algorithm for clustering in at most O(log n) rounds, where n is the size of the largest cluster. We show the effectiveness of all our algorithms through detailed experiments on large synthetic as well as real-world datasets.

#index 2010367
#* Engineering Generalized Shortest Path queries
#@ Michael N. Rice;Vassilis J. Tsotras
#t 2013
#c 17
#! Generalized Shortest Path (GSP) queries represent a variant of constrained shortest path queries in which a solution path of minimum total cost must visit at least one location from each of a set of specified location categories (e.g., gas stations, grocery stores) in a specified order. This problem type has many practical applications in logistics and personalized location-based services, and is closely related to the NP-hard Generalized Traveling Salesman Path Problem (GTSPP). In this work, we present a new dynamic programming formulation to highlight the structure of this problem. Using this formulation as our foundation, we progressively engineer a fast and scalable GSP query algorithm for use on large, real-world road networks. Our approach incorporates concepts from Contraction Hierarchies, a well-known graph indexing technique for static shortest path queries. To demonstrate the practicality of our algorithm we experimented on the North American road network (with over 50 million edges) where we achieved up to several orders of magnitude speed improvements over the previous-best algorithm, depending on the relative sizes of the location categories.

#index 2010368
#* gIceberg: Towards iceberg analysis in large graphs
#@ Jian Wu;Jiawei Han;Lijie Ren;Ziyu Guan;Nan Li;Xifeng Yan
#t 2013
#c 17
#! Traditional multi-dimensional data analysis techniques such as iceberg cube cannot be directly applied to graphs for finding interesting or anomalous vertices due to the lack of dimensionality in graphs. In this paper, we introduce the concept of graph icebergs that refer to vertices for which the concentration (aggregation) of an attribute in their vicinities is abnormally high. Intuitively, these vertices shall be “close” to the attribute of interest in the graph space. Based on this intuition, we propose a novel framework, called gIceberg, which performs aggregation using random walks, rather than traditional SUM and AVG aggregate functions. This proposed framework scores vertices by their different levels of interestingness and finds important vertices that meet a user-specified threshold. To improve scalability, two aggregation strategies, forward and backward aggregation, are proposed with corresponding optimization techniques and bounds. Experiments on both real-world and synthetic large graphs demonstrate that gIceberg is effective and scalable.

#index 2010369
#* Top-k graph pattern matching over large graphs
#@ Jeffrey Xu Yu;Xianggang Zeng;Jiefeng Cheng
#t 2013
#c 17
#! There exist many graph-based applications including bioinformatics, social science, link analysis, citation analysis, and collaborative work. All need to deal with a large data graph. Given a large data graph, in this paper, we study finding top-k answers for a graph pattern query (kGPM), and in particular, we focus on top-k cyclic graph queries where a graph query is cyclic and can be complex. The capability of supporting kGPM provides much more flexibility for a user to search graphs. And the problem itself is challenging. In this paper, we propose a new framework of processing kGPM with on-the-fly ranked lists based on spanning trees of the cyclic graph query. We observe a multidimensional representation for using multiple ranked lists to answer a given kGPM query. Under this representation, we propose a cost model to estimate the least number of tree answers to be consumed in each ranked list for a given kGPM query. This leads to a query optimization approach for kGPM processing, and a top-k algorithm to process kGPM with the optimal query plan. We conducted extensive performance studies using a synthetic dataset and a real dataset, and we confirm the efficiency of our proposed approach.

#index 2010370
#* Query optimization for differentially private data management systems
#@ Yong Yu;Zhenjie Zhang;Yin Yang;Marianne Winslett;Shangfu Peng
#t 2013
#c 17
#! Differential privacy (DP) enables publishing statistical query results over sensitive data, with rigorous privacy guarantees, and very conservative assumptions about the adversary's background knowledge. This paper focuses on the interactive DP framework, which processes incoming queries on the fly, each of which consumes a portion of the user-specified privacy budget. Existing systems process each query independently, which often leads to considerable privacy budget waste. Motivated by this, we propose Pioneer, a query optimizer for an interactive, DP-compliant DBMS. For each new query, Pioneer creates an execution plan that combines past query results and new results from the underlying data. When a query has multiple semantically equivalent plans, Pioneer automatically selects one with minimal privacy budget consumption. Extensive experiments confirm that Pioneer achieves significant savings of the privacy budget, and can answer many more queries than existing systems for a fixed total budget, with comparable result accuracy.

#index 2010371
#* Materialization strategies in the Vertica analytic database: Lessons learned
#@ Vivek Bharathan;Lakshmikant Shrinivas;Sreenath Bodagala;Ramakrishna Varadarajan;Chuck Bear;Ariel Cary
#t 2013
#c 17
#! Column store databases allow for various tuple reconstruction strategies (also called materialization strategies). Early materialization is easy to implement but generally performs worse than late materialization. Late materialization is more complex to implement, and usually performs much better than early materialization, although there are situations where it is worse. We identify these situations, which essentially revolve around joins where neither input fits in memory (also called spilling joins). Sideways information passing techniques provide a viable solution to get the best of both worlds. We demonstrate how early materialization combined with sideways information passing allows us to get the benefits of late materialization, without the bookkeeping complexity or worse performance for spilling joins. It also provides some other benefits to query processing in Vertica due to positive interaction with compression and sort orders of the data. In this paper, we report our experiences with late and early materialization, highlight their strengths and weaknesses, and present the details of our sideways information passing implementation. We show experimental results of comparing these materialization strategies, which highlight the significant performance improvements provided by our implementation of sideways information passing (up to 72% on some TPC-H queries).

#index 2010372
#* HANDS: A heuristically arranged non-backup in-line deduplication system
#@ Ohad Rodeh;Avani Wildani;Ethan L. Miller
#t 2013
#c 17
#! Deduplicating in-line data on primary storage is hampered by the disk bottleneck problem, an issue which results from the need to keep an index mapping portions of data to hash values in memory in order to detect duplicate data without paying the performance penalty of disk paging. The index size is proportional to the volume of unique data, so placing the entire index into RAM is not cost effective with a deduplication ratio below 45%. HANDS reduces the amount of in-memory index storage required by up to 99% while still achieving between 30% and 90% of the deduplication a full memory-resident index provides, making primary deduplication cost effective in workloads with deduplication rates as low as 8%. HANDS is a framework that dynamically pre-fetches fingerprints from disk into memory cache according to working sets statistically derived from access patterns. We use a simple neighborhood grouping as our statistical technique to demonstrate the effectiveness of our approach. HANDS is modular and requires only spatio-temporal data, making it suitable for a wide range of storage systems without the need to modify host file systems.

#index 2010373
#* FERRARI: Flexible and efficient reachability range assignment for graph indexing
#@ Avishek Anand;Stephan Seufert;Srikanta Bedathur;Gerhard Weikum
#t 2013
#c 17
#! In this paper, we propose a scalable and highly efficient index structure for the reachability problem over graphs. We build on the well-known node interval labeling scheme where the set of vertices reachable from a particular node is compactly encoded as a collection of node identifier ranges. We impose an explicit bound on the size of the index and flexibly assign approximate reachability ranges to nodes of the graph such that the number of index probes to answer a query is minimized. The resulting tunable index structure generates a better range labeling if the space budget is increased, thus providing a direct control over the trade off between index size and the query processing performance. By using a fast recursive querying method in conjunction with our index structure, we show that, in practice, reachability queries can be answered in the order of microseconds on an off-the-shelf computer — even for the case of massive-scale real world graphs. Our claims are supported by an extensive set of experimental results using a multitude of benchmark and real-world web-scale graph datasets.

#index 2010374
#* SELECT triggers for data auditing
#@ Raghav Kaushik;Ravi Ramamurthy;Daniel Fabbri
#t 2013
#c 17
#! Auditing is a key part of the security infrastructure in a database system. While commercial database systems provide mechanisms such as triggers that can be used to track and log any changes made to “sensitive” data using UPDATE queries, they are not useful for tracking accesses to sensitive data using complex SQL queries, which is important for many applications given recent laws such as HIPAA. In this paper, we propose the notion of SELECT triggers that extends triggers to work for SELECT queries in order to facilitate data auditing. We discuss the challenges in integrating SELECT triggers in a database system including specification, semantics as well as efficient implementation techniques. We have prototyped our framework in a commercial database system and present an experimental evaluation of our framework using the TPC-H benchmark.

#index 2010375
#* Attribute extraction and scoring: A probabilistic approach
#@ Taesung Lee;Zhongyuan Wang;Haixun Wang;Seung-won Hwang
#t 2013
#c 17
#! Knowledge bases, which consist of concepts, entities, attributes and relations, are increasingly important in a wide range of applications. We argue that knowledge about attributes (of concepts or entities) plays a critical role in inferencing. In this paper, we propose methods to derive attributes for millions of concepts and we quantify the typicality of the attributes with regard to their corresponding concepts. We employ multiple data sources such as web documents, search logs, and existing knowledge bases, and we derive typicality scores for attributes by aggregating different distributions derived from different sources using different methods. To the best of our knowledge, ours is the first approach to integrate concept- and instance-based patterns into probabilistic typicality scores that scale to broad concept space. We have conducted extensive experiments to show the effectiveness of our approach.

#index 2010376
#* KuaFu: Closing the parallelism gap in database replication
#@ Mao Yang;Dong Zhou;Carbo Kuo;Chuntao Hong;Lintao Zhang;Lidong Zhou
#t 2013
#c 17
#! Database systems are nowadays increasingly deployed on multi-core commodity servers, with replication to guard against failures. Database engine is best designed to scale with the number of cores to offer a high degree of parallelism on a modern multi-core architecture. On the other hand, replication traditionally resorts to a certain form of serialization for data consistency among replicas. In the widely used primary/backup replication with log shipping, concurrent executions on the primary and the serialized log replay on a backup creates a serious parallelism gap. Our experiment on MySQL with a 16-core configuration shows that the serial replay of a backup can sustain only less than one third of the throughput achievable on the primary under an OLTP workload. This paper proposes KuaFu to close the parallelism gap on replicated database systems by enabling concurrent replay of transactions on a backup. KuaFu maintains write consistency on backups by tracking transaction dependencies. Concurrent replay on a backup does introduce read inconsistency between the primary and backups. KuaFu further leverages multi-version concurrency control to produce snapshots in order to restore the consistency semantics. We have implemented KuaFu on MySQL; our evaluations show that KuaFu allows a backup to keep up with the primary while preserving replication consistency.

#index 2010377
#* Secure and efficient range queries on outsourced databases using Rp-trees
#@ Peng Wang;Chinya V. Ravishankar
#t 2013
#c 17
#! We show how to execute range queries securely and efficiently on encrypted databases in the cloud. Current methods provide either security or efficiency, but not both. Many schemes even reveal the ordering of encrypted tuples, which, as we show, allows adversaries to estimate plaintext values accurately. We present the Rp-tree, a hierarchical encrypted index that may be securely placed in the cloud, and searched efficiently. It is based on a mechanism we design for encrypted halfspace range queries in Rd, using Asymmetric Scalar-product Preserving Encryption. Data owners can tune the Rp-tree parameters to achieve desired security-efficiency tradeoffs. We also present extensive experiments to evaluate Rp-tree performance. Our results show that Rp-tree queries are efficient on encrypted databases, and reveal far less information than competing methods.

#index 2010378
#* Shallow Information Extraction for the knowledge Web
#@ Cong Yu;Denilson Barbosa;Haixun Wang
#t 2013
#c 17
#! A new breed of Information Extraction tools has become popular and shown to be very effective in building massive-scale knowledge bases that fuel applications such as question answering and semantic search. These approaches rely on Web-scale probabilistic models populated through shallow language processing of the text, pre-existing knowledge, and structured data already on the Web. This tutorial provides an introduction to these techniques, starting from the foundations of information extraction, and covering some of its key applications.

#index 2010379
#* Holistic data cleaning: Putting violations into context
#@ Paolo Papotti;Xu Chu;Ihab F. Ilyas
#t 2013
#c 17
#! Data cleaning is an important problem and data quality rules are the most promising way to face it with a declarative approach. Previous work has focused on specific formalisms, such as functional dependencies (FDs), conditional functional dependencies (CFDs), and matching dependencies (MDs), and those have always been studied in isolation. Moreover, such techniques are usually applied in a pipeline or interleaved. In this work we tackle the problem in a novel, unified framework. First, we let users specify quality rules using denial constraints with ad-hoc predicates. This language subsumes existing formalisms and can express rules involving numerical values, with predicates such as “greater than” and “less than”. More importantly, we exploit the interaction of the heterogeneous constraints by encoding them in a conflict hypergraph. Such holistic view of the conflicts is the starting point for a novel definition of repair context which allows us to compute automatically repairs of better quality w.r.t. previous approaches in the literature. Experimental results on real datasets show that the holistic approach outperforms previous algorithms in terms of quality and efficiency of the repair.

#index 2010380
#* Crowd-answering system via microblogging
#@ Sai Wu;Xianke Zhou;Bingbing Zhang;Ke Chen
#t 2013
#c 17
#! Most crowdsourcing systems leverage the public platforms, such as Amazon Mechanical Turk (AMT), to publish their jobs and collect the results. They are charged for using the platform's service and they are also required to pay the workers for each successful job. Although the average wage of the online human worker is not high, for a 24脳7 running service, the crowdsourcing system becomes very expensive to maintain. We observe that there are, in fact, many sources that can provide free online human volunteers. Microblogging system is one of the most promising human resources. In this paper, we present our CrowdAnswer system, which is built on top of Weibo, the largest microblogging system in China. CrowdAnswer is a question-answering system, which distributes various questions to different groups of microblogging users adaptively. The answers are then collected from those users' tweets and visualized for the question originator. CrowdAnswer maintains a virtual credit system. The users need credits to publish questions and they can gain credits by answering the questions. A novel algorithm is proposed to route the questions to the interested users, which tries to maximize the probability of successfully answering a question.

#index 2010381
#* Road network mix-zones for anonymous location based services
#@ Sindhuja Ravichandran;Calton Pu;Binh Han;Kisung Lee;Balaji Palanisamy;Ling Liu
#t 2013
#c 17
#! We present MobiMix, a road network based mix-zone framework to protect location privacy of mobile users traveling on road networks. An alternative and complementary approach to spatial cloaking based location privacy protection is to break the continuity of location exposure by introducing techniques, such as mix-zones, where no applications can trace user movements. However, existing mixzone proposals fail to provide effective mix-zone construction and placement algorithms that are resilient to timing and transition attacks. In MobiMix, mix-zones are constructed and placed by carefully taking into consideration of multiple factors, such as the geometry of the zones, the statistical behavior of the user population, the spatial constraints on movement patterns of the users, and the temporal and spatial resolution of the location exposure. In this demonstration, we first introduce a visualization of the location privacy risks of mobile users traveling on road networks and show how mixzone based anonymization breaks the continuity of location exposure to protect user location privacy. We demonstrate a suite of road network mix-zone construction and placement methods that provide higher level of resilience to timing and transition attacks on road networks. We show the effectiveness of the MobiMix approach through detailed visualization using traces produced by GTMobiSim on different scales of geographic maps.

#index 2010382
#* VERDICT: Privacy-preserving authentication of range queries in location-based services
#@ Jianliang Xu;Qian Chen;Haibo Hu
#t 2013
#c 17
#! We demonstrate VERDICT, a location-based range query service featuring the privacy-preserving authentication capability. VERDICT adopts the common data-as-a-service (DaaS) model, which consists of the data owner (a location registry or a mobile operator) who provides the querying data, the service provider who executes the query, and the querying users. The system features a privacy-preserving query authentication module that enables the user to verify the correctness of results while still protecting the data privacy. This feature is crucial in many location-based services where the querying data are user locations. To achieve this, VERDICT employs an MR-tree based privacy-preserving authentication scheme proposed in our earlier work [3]. The use case study shows that VERDICT provides efficient and smooth user experience for authenticating location-based range queries.

#index 2010383
#* Stratification driven placement of complex data: A framework for distributed data analytics
#@ P. Sadayappan;Ye Wang;Srinivasan Parthasarathy
#t 2013
#c 17
#! With the increasing popularity of XML data stores, social networks and Web 2.0 and 3.0 applications, complex data formats, such as trees and graphs, are becoming ubiquitous. Managing and processing such large and complex data stores, on modern computational eco-systems, to realize actionable information efficiently, is an important challenge. A critical element at the heart of this challenge relates to the placement, storage and access of such tera- and peta- scale data. In this work we develop a novel distributed framework to ease the burden on the programmer and propose an agile and intelligent placement service layer as a flexible yet unified means to address this challenge. Central to our framework is the notion of stratification which seeks to initially group structurally (or semantically) similar entities into strata. Subsequently strata are partitioned within this ecosystem according to the needs of the application to maximize locality, balance load, or minimize data skew. Results on several real-world applications validate the efficacy and efficiency of our approach.

#index 2010384
#* Automating pattern discovery for rule based data standardization systems
#@ Tanveer A. Faruquie;L Venkata Subramaniam;Snigdha Chaturvedi;Bhupesh S. Chawda;Raghu Krishnapuram;K Hima Prasad
#t 2013
#c 17
#! Data quality is a perennial problem for many enterprise data assets. To improve data quality, businesses often employ rule based data standardization systems in which domain experts code rules for handling important and prevalent patterns. Finding these patterns is laborious and time consuming, particularly for noisy or highly specialized data sets. It is also subjective to the persons determining these patterns. In this paper we present a tool to automatically mine patterns that can help in improving the efficiency and effectiveness of these data standardization systems. The automatically extracted patterns are used by the domain and knowledge experts for rule writing. We use a greedy algorithm to extract patterns that result in a maximal coverage of data. We further group the extracted patterns such that each group represents patterns that capture similar domain knowledge. We propose a similarity measure that uses input pattern semantics to group these patterns. We demonstrate the effectiveness of our method for standardization tasks on three real world datasets.

#index 2010385
#* Big data integration
#@ Divesh Srivastava;Xin Luna Dong
#t 2013
#c 17
#! The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.

#index 2010386
#* Secure and privacy-preserving database services in the cloud
#@ Amr El Abbadi;Shiyuan Wang;Divyakant Agrawal
#t 2013
#c 17
#! Cloud computing becomes a very successful paradigm for data computing and storage. Increasing concerns about data security and privacy in the cloud, however, have arisen. Ensuring security and privacy for data management and query processing in the cloud is critical for better and broader uses of the cloud. This tutorial covers recent research on cloud security and privacy, while focusing on the works that protect data confidentiality and query access privacy for sensitive data being stored and queried in the cloud. We provide a comprehensive study of state-of-the-art schemes and techniques for protecting data confidentiality and access privacy, and explain their tradeoffs in security, privacy, functionality and performance.

#index 2010387
#* With a little help from my friends
#@ John C. Shafer;Stelios Paparizos;Rakesh Agrawal;Arnab Nandi
#t 2013
#c 17
#! A typical person has numerous online friends that, according to studies, the person often consults for opinions and advice. However, public broadcasting a question to all friends risks social capital when repeated too often, is not tolerant to topic sensitivity, and can result in no response, as the message is lost in a myriad of status updates. Direct messaging is more personal and avoids these pitfalls, but requires manual selection of friends to contact, which can be time consuming and challenging. A user may have difficulty guessing which of their numerous online friends can provide a high quality and timely response. We demonstrate a working system that addresses these issues by returning an ordered subset of friends predicting (a) near-term availability, (b) willingness to respond and (c) topical knowledge, given a query. The combination of these three aspects are unique to our solution, and all are critical to the problem of obtaining timely and relevant responses. Our system acts as a decision aid — we give insight into why each friend was recommended and let the user decide whom to contact.

#index 2010388
#* T-Music: A melody composer based on frequent pattern mining
#@ Raymond Ka Wai Sze;Raymond Chi-Wing Wong;Cheng Long
#t 2013
#c 17
#! There are a bulk of studies on proposing algorithms for composing the melody of a song automatically with algorithms, which is known as algorithmic composition. To the best of our knowledge, none of them took the lyric into consideration for melody composition. However, according to some recent studies, within a song, there usually exists a certain extent of correlation between its melody and its lyric. In this demonstration, we propose to utilize this type of correlation information for melody composition. Based on this idea, we design a new melody composition algorithm and develop a melody composer called T-Music which employs this composition algorithm.

#index 2010389
#* A real-time abnormality detection system for intensive care management
#@ Guangyan Huang;Michael Steyn;Zhi Qiao;Kersi Taraporewalla;Jing He;Jie Cao
#t 2013
#c 17
#! Detecting abnormalities from multiple correlated time series is valuable to those applications where a credible realtime event prediction system will minimize economic losses (e.g. stock market crash) and save lives (e.g. medical surveillance in the operating theatre). For example, in an intensive care scenario, anesthetists perform a vital role in monitoring the patient and adjusting the flow and type of anesthetics to the patient during an operation. An early awareness of possible complications is vital for an anesthetist to correctly react to a given situation. In this demonstration, we provide a comprehensive medical surveillance system to effectively detect abnormalities from multiple physiological data streams for assisting online intensive care management. Particularly, a novel online support vector regression (OSVR) algorithm is developed to approach the problem of discovering the abnormalities from multiple correlated time series for accuracy and real-time efficiency. We also utilize historical data streams to optimize the precision of the OSVR algorithm. Moreover, this system comprises a friendly user interface by integrating multiple physiological data streams and visualizing alarms of abnormalities.

#index 2010390
#* Towards efficient search for activity trajectories
#@ Kai Zheng;Yi Yang;Shuo Shang;Nicholas Jing Yuan
#t 2013
#c 17
#! The advances in location positioning and wireless communication technologies have led to a myriad of spatial trajectories representing the mobility of a variety of moving objects. While processing trajectory data with the focus of spatio-temporal features has been widely studied in the last decade, recent proliferation in location-based web applications (e.g., Foursquare, Facebook) has given rise to large amounts of trajectories associated with activity information, called activity trajectory. In this paper, we study the problem of efficient similarity search on activity trajectory database. Given a sequence of query locations, each associated with a set of desired activities, an activity trajectory similarity query (ATSQ) returns k trajectories that cover the query activities and yield the shortest minimum match distance. An order-sensitive activity trajectory similarity query (OATSQ) is also proposed to take into account the order of the query locations. To process the queries efficiently, we firstly develop a novel hybrid grid index, GAT, to organize the trajectory segments and activities hierarchically, which enables us to prune the search space by location proximity and activity containment simultaneously. In addition, we propose algorithms for efficient computation of the minimum match distance and minimum order-sensitive match distance, respectively. The results of our extensive empirical studies based on real online check-in datasets demonstrate that our proposed index and methods are capable of achieving superior performance and good scalability.

#index 2010391
#* Recycling in pipelined query evaluation
#@ Fabian Nagel;Peter Boncz;Stratis D. Viglas
#t 2013
#c 17
#! Database systems typically execute queries in isolation. Sharing recurring intermediate and final results between successive query invocations is ignored or only exploited by caching final query results. The DBA is kept in the loop to make explicit sharing decisions by identifying and/or defining materialized views. Thus decisions are made only after a long time and sharing opportunities may be missed. Recycling intermediate results has been proposed as a method to make database query engines profit from opportunities to reuse fine-grained partial query results, that is fully autonomous and is able to continuously adapt to changes in the workload. The technique was recently revisited in the context of MonetDB, a system that by default materializes all intermediate results. Materializing intermediate results can consume significant system resources, therefore most other database systems avoid this where possible, following a pipelined query architecture instead. The novelty of this paper is to show how recycling can successfully be applied in pipelined query executors, by tracking the benefit of materializing possible intermediate results and then choosing the ones making best use of a limited intermediate result cache. We present ways to maximize the potential of recycling by leveraging subsumption and proactive query rewriting. We have implemented our approach in the Vectorwise database engine and have experimentally evaluated its potential using both synthetic and real-world datasets. Our results show that intermediate result recycling significantly improves performance.

#index 2010392
#* On answering why-not questions in reverse skyline queries
#@ Rui Zhou;Chengfei Liu;Md. Saiful Islam
#t 2013
#c 17
#! This paper aims at answering the so called why-not questions in reverse skyline queries. A reverse skyline query retrieves all data points whose dynamic skylines contain the query point. We outline the benefit and the semantics of answering why-not questions in reverse skyline queries. In connection with this, we show how to modify the why-not point and the query point to include the why-not point in the reverse skyline of the query point. We then show, how a query point can be positioned safely anywhere within a region (i.e., called safe region) without losing any of the existing reverse skyline points. We also show how to answer why-not questions considering the safe region of the query point. Our approach efficiently combines both query point and data point modification techniques to produce meaningful answers. Experimental results also demonstrate that our approach can produce high quality explanations for why-not questions in reverse skyline queries.

#index 2010393
#* RoadAlarm: A spatial alarm system on road networks
#@ Ling Liu;Balaji Palanisamy;Calton Pu;Emre Yigitoglu;Kisung Lee;Binh Han
#t 2013
#c 17
#! Spatial alarms are one of the fundamental functionalities for many LBSs. We argue that spatial alarms should be road network aware as mobile objects travel on spatially constrained road networks or walk paths. In this software system demonstration, we will present the first prototype system of ROADALARM — a spatial alarm processing system for moving objects on road networks. The demonstration system of ROAD-ALARM focuses on the three unique features of ROADALARM system design. First, we will show that the road network distance-based spatial alarm is best modeled using road network distance such as segment length-based and travel time-based distance. Thus, a road network spatial alarm is a star-like subgraph centered at the alarm target. Second, we will show the suite of ROADALARM optimization techniques to scale spatial alarm processing by taking into account spatial constraints on road networks and mobility patterns of mobile subscribers. Third, we will show that, by equipping the ROADALARM system with an activity monitoring-based control panel, we are able to enable the system administrator and the end users to visualize road network-based spatial alarms, mobility traces of moving objects and dynamically make selection or customization of the ROADALARM techniques for spatial alarm processing through graphical user interface. We show that the ROADALARM system provides both the general system architecture and the essential building blocks for location-based advertisements and location-based reminders.

#index 2010394
#* Interval indexing and querying on key-value cloud stores
#@ Ioannis Patlakas;George Sfakianakis;Peter Triantafillou;Nikos Ntarmos
#t 2013
#c 17
#! Cloud key-value stores are becoming increasingly more important. Challenging applications, requiring efficient and scalable access to massive data, arise every day. We focus on supporting interval queries (which are prevalent in several data intensive applications, such as temporal querying for temporal analytics), an efficient solution for which is lacking. We contribute a compound interval index structure, comprised of two tiers: (i) the MRSegmentTree (MRST), a key-value representation of the Segment Tree, and (ii) the Endpoints Index (EPI), a column family index that stores information for interval endpoints. In addition to the above, our contributions include: (i) algorithms for efficiently constructing and populating our indices using MapReduce jobs, (ii) techniques for efficient and scalable index maintenance, and (iii) algorithms for processing interval queries. We have implemented all algorithms using HBase and Hadoop, and conducted a detailed performance evaluation. We quantify the costs associated with the construction of the indices, and evaluate our query processing algorithms using queries on real data sets. We compare the performance of our approach to two alternatives: the native support for interval queries provided in HBase, and the execution of such queries using the Hive query execution tool. Our results show a significant speedup, far outperforming the state of the art.

#index 2010395
#* Robust distributed stream processing
#@ Elke A. Rundensteiner;Chuan Lei;Joshua D. Guttman
#t 2013
#c 17
#! Distributed stream processing systems must function efficiently for data streams that fluctuate in their arrival rates and data distributions. Yet repeated and prohibitively expensive load re-allocation across machines may make these systems ineffective, potentially resulting in data loss or even system failure. To overcome this problem, we instead propose a load distribution (RLD) strategy that is robust to data fluctuations. RLD provides ∊-optimal query performance under load fluctuations without suffering from the performance penalty caused by load migration. RLD is based on three key strategies. First, we model robust distributed stream processing as a parametric query optimization problem. The notions of robust logical and robust physical plans then are overlays of this parameter space. Second, our Early-terminated Robust Partitioning (ERP) finds a set of robust logical plans, covering the parameter space, while minimizing the number of prohibitively expensive optimizer calls with a probabilistic bound on the space coverage. Third, our OptPrune algorithm maps the space-covering logical solution to a single robust physical plan tolerant to deviations in data statistics that maximizes the parameter space coverage at runtime. Our experimental study using stock market and sensor networks streams demonstrates that our RLD methodology consistently outperforms state-of-the-art solutions in terms of efficiency and effectiveness in highly fluctuating data stream environments.

#index 2010396
#* SubZero: A fine-grained lineage system for scientific databases
#@ Eugene Wu;Samuel Madden;Michael Stonebraker
#t 2013
#c 17
#! Data lineage is a key component of provenance that helps scientists track and query relationships between input and output data. While current systems readily support lineage relationships at the file or data array level, finer-grained support at an array-cell level is impractical due to the lack of support for user defined operators and the high runtime and storage overhead to store such lineage. We interviewed scientists in several domains to identify a set of common semantics that can be leveraged to efficiently store fine-grained lineage. We use the insights to define lineage representations that efficiently capture common locality properties in the lineage data, and a set of APIs so operator developers can easily export lineage information from user defined operators. Finally, we introduce two benchmarks derived from astronomy and genomics, and show that our techniques can reduce lineage query costs by up to 10x while incuring substantially less impact on workflow runtime and storage.

#index 2010397
#* The Bw-Tree: A B-tree for new hardware platforms
#@ David B. Lomet;Sudipta Sengupta;Justin J. Levandoski
#t 2013
#c 17
#! The emergence of new hardware and platforms has led to reconsideration of how data management systems are designed. However, certain basic functions such as key indexed access to records remain essential. While we exploit the common architectural layering of prior systems, we make radically new design decisions about each layer. Our new form of B-tree, called the Bw-tree achieves its very high performance via a latch-free approach that effectively exploits the processor caches of modern multi-core chips. Our storage manager uses a unique form of log structuring that blurs the distinction between a page and a record store and works well with flash storage. This paper describes the architecture and algorithms for the Bw-tree, focusing on the main memory aspects. The paper includes results of our experiments that demonstrate that this fresh approach produces outstanding performance.

#index 2010398
#* Scalable and parallelizable processing of influence maximization for large-scale social networks?
#@ Hwanjo Yu;Seung-Keol Kim;Jinha Kim
#t 2013
#c 17
#! As social network services connect people across the world, influence maximization, i.e., finding the most influential nodes (or individuals) in the network, is being actively researched with applications to viral marketing. One crucial challenge in scalable influence maximization processing is evaluating influence, which is #P-hard and thus hard to solve in polynomial time. We propose a scalable influence approximation algorithm, Independent Path Algorithm (IPA) for Independent Cascade (IC) diffusion model. IPA efficiently approximates influence by considering an independent influence path as an influence evaluation unit. IPA are also easily parallelized by simply adding a few lines of OpenMP meta-programming expressions. Also, overhead of maintaining influence paths in memory is relieved by safely throwing away insignificant influence paths. Extensive experiments conducted on large-scale real social networks show that IPA is an order of magnitude faster and uses less memory than the state of the art algorithms. Our experimental results also show that parallel versions of IPA speeds up further as the number of CPU cores increases, and more speed-up is achieved for larger datasets. The algorithms have been implemented in our demo application for influence maximization (available at http://dm.postech.ac.kr/ipa demo), which efficiently finds the most influential nodes in a social network.

#index 2010399
#* [PDF Reader FAQ and support]
#@ 
#t 2013
#c 17
#! Provides instructions on viewing the proceedings articles in PDF format and support information for CD users.

#index 2010400
#* ICDE 2012 [Abstracts book]
#@ 
#t 2013
#c 17
#! Presents abstracts for the articles comprising the conference proceedings.

#index 2010401
#* [USB label]
#@ 
#t 2013
#c 17
#! Presents the USB label art of the proceedings record.

#index 2010402
#* ICDE 2013 Conference [cover]
#@ 
#t 2013
#c 17
#! Presents the start page/splash page of the 2013 IEEE 29th International Conference on Data Engineering (ICDE) proceedings record.

#index 2010403
#* Proceedings [editors]
#@ 
#t 2013
#c 17
#! The editors of the IEEE 29th International Conference on Data Engineering (ICDE) are: Christian S. Jensen, Chris Jermaine, Jiaheng Lu, Egemen Tanin, and Xiaofang Zhou.

#index 2010404
#* Hub page
#@ 
#t 2013
#c 17
#! Presents the proceedings page that links various sections of the overall electronic record.

#index 2010405
#* Session list
#@ 
#t 2013
#c 17
#! Provides a listing of sessions.

#index 2010406
#* Breaking the top-k barrier of hidden web databases?
#@ Saravanan Thirumuruganathan;Gautam Das;Nan Zhang
#t 2013
#c 17
#! A large number of web databases are only accessible through proprietary form-like interfaces which require users to query the system by entering desired values for a few attributes. A key restriction enforced by such an interface is the top-k output constraint — i.e., when there are a large number of matching tuples, only a few (top-k) of them are preferentially selected and returned by the website, often according to a proprietary ranking function. Since most web database owners set k to be a small value, the top-k output constraint prevents many interesting third-party (e.g., mashup) services from being developed over real-world web databases. In this paper we consider the novel problem of “digging deeper” into such web databases. Our main contribution is the meta-algorithm GetNext that can retrieve the next ranked tuple from the hidden web database using only the restrictive interface of a web database without any prior knowledge of its ranking function. This algorithm can then be called iteratively to retrieve as many top ranked tuples as necessary. We develop principled and efficient algorithms that are based on generating and executing multiple reformulated queries and inferring the next ranked tuple from their returned results. We provide theoretical analysis of our algorithms, as well as extensive experimental results over synthetic and real-world databases that illustrate the effectiveness of our techniques.

#index 2010407
#* Top down plan generation: From theory to practice
#@ Pit Fender;Guido Moerkotte
#t 2013
#c 17
#! Finding the optimal execution order of join operations is a crucial task of today's cost-based query optimizers. There are two approaches to identify the best plan: bottom-up and top-down join enumeration. But only the top-down approach allows for branch-and-bound pruning, which can improve compile time by several orders of magnitude while still preserving optimality. For both optimization strategies, efficient enumeration algorithms have been published. However, there are two severe limitations for the top-down approach: The published algorithms can handle only (1) simple (binary) join predicates and (2) inner joins. Since real queries may contain complex join predicates involving more than two relations, and outer joins as well as other non-inner joins, efficient top-down join enumeration cannot be used in practice yet. We develop a novel top-down join enumeration algorithm that overcomes these two limitations. Furthermore, we show that our new algorithm is competitive when compared with the state of the art in bottom-up processing even without playing out its advantage by making use of its branch-and-bound pruning capabilities.

#index 2010408
#* Efficient many-core query execution in main memory column-stores
#@ Jonathan Dees;Peter Sanders
#t 2013
#c 17
#! We use the full query set of the TPC-H Benchmark as a case study for the efficient implementation of decision support queries on main memory column-store databases. Instead of splitting a query into separate independent operators, we consider the query as a whole and translate the execution plan into a single function performing the query. This allows highly efficient CPU utilization, minimal materialization, and execution in a single pass over the data for most queries. The single pass is performed in parallel and scales near-linearly with the number of cores. The resulting query plans for most of the 22 queries are remarkably simple and are suited for automatic generation and fast compilation. Using a data-parallel, NUMA-aware many-core implementation with block summaries, inverted index data structures, and efficient aggregation algorithms, we achieve one to two orders of magnitude better performance than the current record holders of the TPC-H Benchmark.

#index 2010409
#* A generic database benchmarking service
#@ Martin Kaufmann;Peter M. Fischer;Norman May;Donald Kossmann
#t 2013
#c 17
#! Benchmarks are widely applied for the development and optimization of database systems. Standard benchmarks such as TPC-C and TPC-H provide a way of comparing the performance of different systems. In addition, micro benchmarks can be exploited to test a specific behavior of a system. Yet, despite all the benefits that can be derived from benchmark results, the effort of implementing and executing benchmarks remains prohibitive: Database systems need to be set up, a large number of artifacts such as data generators and queries need to be managed and complex, time-consuming operations have to be orchestrated. In this demo, we introduce a generic benchmarking service that combines a rich meta model, low marginal cost and ease of use, which drastically reduces the time and cost to define, adapt and run a benchmark.

#index 2010410
#* Optimizing approximations of DNF query lineage in probabilistic XML
#@ Pierre Senellart;Asma Souihli
#t 2013
#c 17
#! Probabilistic XML is a probabilistic model for uncertain tree-structured data, with applications to data integration, information extraction, or uncertain version control. We explore in this work efficient algorithms for evaluating tree-pattern queries with joins over probabilistic XML or, more specifically, for listing the answers to a query along with their computed or approximated probability. The approach relies on, first, producing the lineage query by evaluating it over the probabilistic XML document, and, second, looking for an optimal strategy to compute the probability of the lineage formula. This latter part relies on a query-optimizer — like approach: exploring different evaluation plans for different parts of the formula and estimating the cost of each plan, using a cost model for the various evaluation algorithms. We demonstrate the efficiency of this approach on datasets used in previous research on probabilistic XML querying, as well as on synthetic data. We also compare the performance of our query engine with EvalDP [1], Trio [2], and MayBMS/SPROUT [3].

#index 2010411
#* Differentially private grids for geospatial data
#@ Ninghui Li;Weining Yang;Wahbeh Qardaji
#t 2013
#c 17
#! In this paper, we tackle the problem of constructing a differentially private synopsis for two-dimensional datasets such as geospatial datasets. The current state-of-the-art methods work by performing recursive binary partitioning of the data domains, and constructing a hierarchy of partitions. We show that the key challenge in partition-based synopsis methods lies in choosing the right partition granularity to balance the noise error and the non-uniformity error. We study the uniform-grid approach, which applies an equi-width grid of a certain size over the data domain and then issues independent count queries on the grid cells. This method has received no attention in the literature, probably due to the fact that no good method for choosing a grid size was known. Based on an analysis of the two kinds of errors, we propose a method for choosing the grid size. Experimental results validate our method, and show that this approach performs as well as, and often times better than, the state-of-the-art methods. We further introduce a novel adaptive-grid method. The adaptive grid method lays a coarse-grained grid over the dataset, and then further partitions each cell according to its noisy count. Both levels of partitions are then used in answering queries over the dataset. This method exploits the need to have finer granularity partitioning over dense regions and, at the same time, coarse partitioning over sparse regions. Through extensive experiments on real-world datasets, we show that this approach consistently and significantly outperforms the uniform-grid method and other state-of-the-art methods.

#index 2010412
#* Voronoi-based nearest neighbor search for multi-dimensional uncertain databases
#@ Matthias Renz;Nikos Mamoulis;Tobias Emrich;Yu Tang;Reynold Cheng;Andreas Zufle;Peiwu Zhang
#t 2013
#c 17
#! In Voronoi-based nearest neighbor search, the Voronoi cell of every point p in a database can be used to check whether p is the closest to some query point q. We extend the notion of Voronoi cells to support uncertain objects, whose attribute values are inexact. Particularly, we propose the Possible Voronoi cell (or PV-cell). A PV-cell of a multi-dimensional uncertain object o is a region R, such that for any point p∊R, o may be the nearest neighbor of p. If the PV-cells of all objects in a database S are known, they can be used to identify objects that have a chance to be the nearest neighbor of q. However, there is no efficient algorithm for computing an exact PV-cell. We hence study how to derive an axis-parallel hyper-rectangle (called the Uncertain Bounding Rectangle, or UBR) that tightly contains a PV-cell. We further develop the PV-index, a structure that stores UBRs, to evaluate probabilistic nearest neighbor queries over uncertain data. An advantage of the PV-index is that upon updates on S, it can be incrementally updated. Extensive experiments on both synthetic and real datasets are carried out to validate the performance of the PV-index.

#index 2010413
#* TBF: A memory-efficient replacement policy for flash-based caches
#@ Biplob Debnath;Cristian Ungureanu;Akshat Aranya;Stephen Rago
#t 2013
#c 17
#! The performance and capacity characteristics of flash storage make it attractive to use as a cache. Recency-based cache replacement policies rely on an in-memory full index, typically a B-tree or a hash table, that maps each object to its recency information. Even though the recency information itself may take very little space, the full index for a cache holding N keys requires at least log N bits per key. This metadata overhead is undesirably high when used for very large flash-based caches, such as key-value stores with billions of objects. To solve this problem, we propose a new RAM-frugal cache replacement policy that approximates the least-recently-used (LRU) policy. It uses two in-memory Bloom sub-filters (TBF) for maintaining the recency information and leverages an on-flash key-value store to cache objects. TBF requires only one byte of RAM per cached object, making it suitable for implementing very large flash-based caches. We evaluate TBF through simulation on traces from several block stores and key-value stores, as well as evaluate it using the Yahoo! Cloud Serving Benchmark in a real system implementation. Evaluation results show that TBF achieves cache hit rate and operations per second comparable to those of LRU in spite of its much smaller memory requirements.

#index 2010414
#* Knowledge harvesting from text and Web sources
#@ Gerhard Weikum;Fabian Suchanek
#t 2013
#c 17
#! The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources has enabled the automatic construction of very large knowledge bases. Recent endeavors of this kind include academic research projects such as DBpedia, KnowItAll, Probase, ReadTheWeb, and YAGO, as well as industrial ones such as Freebase and Trueknowledge. These projects provide automatically constructed knowledge bases of facts about named entities, their semantic classes, and their mutual relationships. Such world knowledge in turn enables cognitive applications and knowledge-centric services like disambiguating natural-language text, deep question answering, and semantic search for entities and relations in Web and enterprise data. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Graph and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, recent advances, research opportunities, and open challenges along this avenue of knowledge harvesting and its applications.

#index 2010415
#* Querying encrypted data
#@ Ravi Ramamurthy;Raghav Kaushik;Arvind Arasu;Ken Eguro
#t 2013
#c 17
#! Data security is a serious concern when we migrate data to a cloud DBMS. Database encryption, where sensitive columns are encrypted before they are stored in the cloud, has been proposed as a mechanism to address such data security concerns. The intuitive expectation is that an adversary cannot “learn” anything about the encrypted columns, since she does not have access to the encryption key. However, query processing becomes a challenge since it needs to “look inside” the data. This tutorial explores the space of designs studied in prior work on processing queries over encrypted data. We cover approaches based on both classic client-server and involving the use of a trusted hardware module where data can be securely decrypted. We discuss the privacy challenges that arise in both approaches and how they may be addressed. Briefly, supporting the full complexity of a modern DBMS including complex queries, transactions and stored procedures leads to significant challenges that we survey.

#index 2010416
#* Sampling node pairs over large graphs
#@ John C.  S. Lui;Pinghui Wang;Don Towsley;Xiaohong Guan;Junzhou Zhao
#t 2013
#c 17
#! Characterizing user pair relationships is important for applications such as friend recommendation and interest targeting in online social networks (OSNs). Due to the large scale nature of such networks, it is infeasible to enumerate all user pairs and so sampling is used. In this paper, we show that it is a great challenge even for OSN service providers to characterize user pair relationships even when they possess the complete graph topology. The reason is that when sampling techniques (i.e., uniform vertex sampling (UVS) and random walk (RW)) are naively applied, they can introduce large biases, in particular, for estimating similarity distribution of user pairs with constraints such as existence of mutual neighbors, which is important for applications such as identifying network homophily. Estimating statistics of user pairs is more challenging in the absence of the complete topology information, since an unbiased sampling technique such as UVS is usually not allowed, and exploring the OSN graph topology is expensive. To address these challenges, we present asymptotically unbiased sampling methods to characterize user pair properties based on UVS and RW techniques respectively. We carry out an evaluation of our methods to show their accuracy and efficiency. Finally, we apply our methods to two Chinese OSNs, Doudan and Xiami, and discover significant homophily is present in these two networks.

#index 2010417
#* COLA: A cloud-based system for online aggregation
#@ Yantao Gan;Xiaofeng Meng;Yingjie Shi
#t 2013
#c 17
#! Online aggregation is a promising solution to achieving fast early responses for interactive ad-hoc queries that compute aggregates on massive data. To process large datasets on large-scale computing clusters, MapReduce has been introduced as a popular paradigm into many data analysis applications. However, typical MapReduce implementations are not well-suited to analytic tasks, since they are geared towards batch processing. With the increasing popularity of ad-hoc analytic query processing over enormous datasets, processing aggregate queries using MapReduce in an online fashion is therefore an emerging important application need. We present a MapReduce-based online aggregation system called COLA, which provides progressive approximate aggregate answers for both single table and multiple joined tables. COLA provides an online aggregation execution engine with novel sampling techniques to support incremental and continuous computing of aggregation, and minimize the waiting time before an acceptably precise estimate is available. In addition, user-friendly SQL queries are supported in COLA. Furthermore, COLA can implicitly convert non-OLA jobs into online version so that users don't have to write any special-purpose code to make estimates.

#index 2010418
#* AFFINITY: Efficiently querying statistical measures on time-series data
#@ Saket Sathe;Karl Aberer
#t 2013
#c 17
#! Computing statistical measures for large databases of time series is a fundamental primitive for querying and mining time-series data [1]–[6]. This primitive is gaining importance with the increasing number and rapid growth of time series databases. In this paper, we introduce a framework for efficient computation of statistical measures by exploiting the concept of affine relationships. Affine relationships can be used to infer statistical measures for time series, from other related time series, instead of computing them directly; thus, reducing the overall computational cost significantly. The resulting methods exhibit at least one order of magnitude improvement over the best known methods. To the best of our knowledge, this is the first work that presents an unified approach for computing and querying several statistical measures at once. Our approach exploits affine relationships using three key components. First, the AFCLST algorithm clusters the time-series data, such that high-quality affine relationships could be easily found. Second, the SYMEX algorithm uses the clustered time series and efficiently computes the desired affine relationships. Third, the SCAPE index structure produces a many-fold improvement in the performance of processing several statistical queries by seamlessly indexing the affine relationships. Finally, we establish the effectiveness of our approaches by performing comprehensive experimental evaluation on real datasets.

#index 2010419
#* Revision provenance in text documents of asynchronous collaboration
#@ Jing Zhang;H. V. Jagadish
#t 2013
#c 17
#! Many text documents today are collaboratively edited, often with multiple small changes. The problem we consider in this paper is how to find provenance for a specific part of interest in the document. A full revision history, represented as a version tree, can tell us about all updates made to the document, but most of these updates may apply to other parts of the document, and hence not be relevant to answer the provenance question at hand. In this paper, we propose the notion of a revision unit as a flexible unit to capture the necessary provenance. We demonstrate through experiments the capability of the revision units in keeping only relevant updates in the provenance representation and the flexibility of the revision units in adjusting to updates reflected in the version tree.

#index 2010420
#* CPU and cache efficient management of memory-resident databases
#@ Alfons Kemper;Florian Funke;Holger Pirk;Stefan Manegold;Ulf Leser;Martin Grund;Thomas Neumann;Martin Kersten
#t 2013
#c 17
#! Memory-Resident Database Management Systems (MRDBMS) have to be optimized for two resources: CPU cycles and memory bandwidth. To optimize for bandwidth in mixed OLTP/OLAP scenarios, the hybrid or Partially Decomposed Storage Model (PDSM) has been proposed. However, in current implementations, bandwidth savings achieved by partial decomposition come at increased CPU costs. To achieve the aspired bandwidth savings without sacrificing CPU efficiency, we combine partially decomposed storage with Just-in-Time (JiT) compilation of queries, thus eliminating CPU inefficient function calls. Since existing cost based optimization components are not designed for JiT-compiled query execution, we also develop a novel approach to cost modeling and subsequent storage layout optimization. Our evaluation shows that the JiT-based processor maintains the bandwidth savings of previously presented hybrid query processors but outperforms them by two orders of magnitude due to increased CPU efficiency.

#index 2010421
#* Utilizing users' tipping points in E-commerce Recommender systems
#@ Wynne Hsu;Mong Li Lee;Kailun Hu
#t 2013
#c 17
#! Existing recommendation algorithms assume that users make their purchase decisions solely based on individual preferences, without regard to the type of users nor the products' maturity stages. Yet, extensive studies have shown that there are two types of users: innovators and imitators. Innovators tend to make purchase decisions based solely on their own preferences; whereas imitators' purchase decisions are often influenced by a product's stage of maturity. In this paper, we propose a framework that seamlessly incorporates the type of user and product maturity into existing recommendation algorithms. We apply Bass model to classify each user as either an innovator or imitator according to his/her previous purchase behavior. In addition, we introduce the concept of tipping point of a user. This tipping point refers to the point on the product maturity curve beyond which the user is likely to be more receptive to purchasing the product. We refine two widely-adopted recommendation algorithms to incorporate the effect of product maturity in relation to the user type. Experiment results on a real-world dataset obtained from an E-commerce website show that the proposed approach outperforms existing algorithms.

#index 2010422
#* RoundTripRank: Graph-based proximity with importance and specificity?
#@ Yuan Fang;Kevin Chen-Chuan Chang;Hady W. Lauw
#t 2013
#c 17
#! Graph-based proximity has many applications with different ranking needs. However, most previous works only stress the sense of importance by finding “popular” results for a query. Often times important results are overly general without being well-tailored to the query, lacking a sense of specificity — which only emerges recently. Even then, the two senses are treated independently, and only combined empirically. In this paper, we generalize the well-studied importance-based random walk into a round trip and develop RoundTripRank, seamlessly integrating specificity and importance in one coherent process. We also recognize the need for a flexible trade-off between the two senses, and further develop RoundTripRank+ based on a scheme of hybrid random surfers. For efficient computation, we start with a basic model that decomposes RoundTripRank into smaller units. For each unit, we apply a novel two-stage bounds updating framework, enabling an online top-K algorithm 2SBound. Finally, our experiments show that RoundTripRank and RoundTripRank+ are robust over various ranking tasks, and 2SBound enables scalable online processing.

#index 2010423
#* SHARE: Secure information sharing framework for emergency management
#@ Elena Ferrari;Barbara Carminati;Michele Guglielmi
#t 2013
#c 17
#! 9/11, Katrina, Fukushima and other recent emergencies demonstrate the need for effective information sharing across government agencies as well as non-governmental and private organizations to assess emergency situations, and generate proper response plans. In this demo, we present a system to enforce timely and controlled information sharing in emergency situations. The framework is able to detect emergencies, enforce temporary access control policies and obligations to be activated during emergencies, simulate emergency situations for demonstrational purposes and show statistical results related to emergency activation/deactivation and consequent access control policies triggering.

#index 2010424
#* RECODS: Replica consistency-on-demand store
#@ Jianmin Wang;Yuqing Zhu;Philip S. Yu
#t 2013
#c 17
#! Replication is critical to the scalability, availability and reliability of large-scale systems. The trade-off of replica consistency vs. response latency has been widely understood for large-scale stores with replication. The weak consistency guaranteed by existing large-scale stores complicates application development, while the strong consistency hurts application performance. It is desirable that the best consistency be guaranteed for a tolerable response latency, but none of existing large-scale stores supports maximized replica consistency within a given latency constraint. In this demonstration, we showcase RECODS (REplica Consistency-On-Demand Store), a NoSQL store implementation that can finely control the trade-off on an operation basis and thus facilitate application development with on-demand replica consistency. With RECODS, developers can specify the tolerable latency for each read/write operation. Within the specified latency constraint, a response will be returned and the replica consistency be maximized. RECODS implementation is based on Cassandra, an open source NoSQL store, but with a different operation execution process, replication process and in-memory storage hierarchy.

#index 2010425
#* Learning to rank from distant supervision: Exploiting noisy redundancy for relational entity search
#@ Kevin Chen-Chuan Change;Hongning Wang;Mianwei Zhou
#t 2013
#c 17
#! In this paper, we study the task of relational entity search which aims at automatically learning an entity ranking function for a desired relation. To rank entities, we exploit the redundancy abound in their snippets; however, such redundancy is noisy as not all the snippets represent information relevant to the desired relation. To explore useful information from such noisy redundancy, we abstract the task as a distantly supervised ranking problem — based on coarse entity-level annotations, deriving a relation-specific ranking function for the purpose of online searching. As the key challenge, without detailed snippet-level annotations, we have to learn an entity ranking function that can effectively filter noise; furthermore, the ranking function should also be online executable. We develop Pattern-based Filter Network (PFNet), a novel probabilistic graphical model, as our solution. To balance the accuracy and efficiency requirements, PFNet selects a limited size of indicative patterns to filter noisy snippets, and inverted indexes are utilized to retrieve required features. Experiments on the large scale CuleWeb09 data set for six different relations confirm the effectiveness of the proposed PFNet model, which outperforms five state-of-the-art relational entity ranking methods.

#index 2010426
#* SociaLite: Datalog extensions for efficient social network analysis
#@ Monica S. Lam;Stephen Guo;Jiwon Seo
#t 2013
#c 17
#! With the rise of social networks, large-scale graph analysis becomes increasingly important. Because SQL lacks the expressiveness and performance needed for graph algorithms, lower-level, general-purpose languages are often used instead. For greater ease of use and efficiency, we propose SociaLite, a high-level graph query language based on Datalog. As a logic programming language, Datalog allows many graph algorithms to be expressed succinctly. However, its performance has not been competitive when compared to low-level languages. With SociaLite, users can provide high-level hints on the data layout and evaluation order; they can also define recursive aggregate functions which, as long as they are meet operations, can be evaluated incrementally and efficiently. We evaluated SociaLite by running eight graph algorithms (shortest paths, PageRank, hubs and authorities, mutual neighbors, connected components, triangles, clustering coefficients, and betweenness centrality) on two real-life social graphs, Live-Journal and Last.fm. The optimizations proposed in this paper speed up almost all the algorithms by 3 to 22 times. SociaLite even outperforms typical Java implementations by an average of 50% for the graph algorithms tested. When compared to highly optimized Java implementations, SociaLite programs are an order of magnitude more succinct and easier to write. Its performance is competitive, giving up only 16% for the largest benchmark. Most importantly, being a query language, SociaLite enables many more users who are not proficient in software engineering to make social network queries easily and efficiently.

#index 2010427
#* LinkProbe: Probabilistic inference on large-scale social networks
#@ Liang Tang;Haiquan Chen;Haixun Wang;Min-Te Sun;Wei-Shinn Ku
#t 2013
#c 17
#! As one of the most important Semantic Web applications, social network analysis has attracted more and more interest from researchers due to the rapidly increasing availability of massive social network data. A desired solution for social network analysis should address the following issues. First, in many real world applications, inference rules are partially correct. An ideal solution should be able to handle partially correct rules. Second, applications in practice often involve large amounts of data. The inference mechanism should scale up towards large-scale data. Third, inference methods should take into account probabilistic evidence data because these are domains abounding with uncertainty. Various solutions for social network analysis have existed for quite a few years; however, none of them support all the aforementioned features. In this paper, we design and implement LinkProbe, a prototype to quantitatively predict the existence of links among nodes in large-scale social networks, which are empowered by Markov Logic Networks (MLNs). MLN has been proved to be an effective inference model which can handle complex dependencies and partially correct rules. More importantly, although MLN has shown acceptable performance in prior works, it is also reported as impractical in handling large-scale data due to its highly demanding nature in terms of inference time and memory consumption. In order to overcome these limitations, LinkProbe retrieves the ⁁-backbone graphs and conducts the MLN inference on both the most globally influencing nodes and most locally related nodes. Our extensive experiments show that LinkProbe manages to provide a tunable balance between MLN inference accuracy and inference efficiency.

#index 2010428
#* An efficient and compact indexing scheme for large-scale data store
#@ Kian-Lee Tan;Sai Wu;Lidan Shou;Peng Lu
#t 2013
#c 17
#! The amount of data managed in today's Cloud systems has reached an unprecedented scale. In order to speed up query processing, an effective mechanism is to build indexes on attributes that are used in query predicates. However, conventional indexing schemes fail to provide a scalable service: as the size of these indexes are proportional to the data size, it is not space efficient to build many indexes. As such, it becomes more crucial to develop effective index to provide scalable database services in the Cloud. In this paper, we propose a compact bitmap indexing scheme for a large-scale data store. The bitmap indexing scheme combines state-of-the-art bitmap compression techniques, such as WAH encoding and bit-sliced encoding. To further reduce the index cost, a novel and query efficient partial indexing technique is adopted, which dynamically refreshes the index to handle updates and process queries. The intuition of our indexing approach is to maximize the number of indexed attributes, so that a wider range of queries, including range and join queries, can be efficiently supported. Our indexing scheme is light-weight and its creation can be seamlessly grafted onto the MapReduce processing engine without incurring significant running cost. Moreover, the compactness allows us to maintain the bitmap indexes in memory so that performance overhead of index access is minimal. We implement our indexing scheme on top of the underlying Distributed File System (DFS) and evaluate its performance on an in-house cluster. We compare our index-based query processing with HadoopDB to show its superior performance. Our experimental results confirm the effectiveness, efficiency and scalability of the indexing scheme.

#index 2010429
#* Memory-efficient algorithms for spatial network queries
#@ Hanan Samet;Sarana Nutanong
#t 2013
#c 17
#! Incrementally finding the k nearest neighbors (kNN) in a spatial network is an important problem in location-based services. One method (INE) simply applies Dijkstra's algorithm. Another method (IER) computes the k nearest neighbors using Euclidean distance followed by computing their corresponding network distances, and then incrementally finds the next nearest neighbors in order of increasing Euclidean distance until finding one whose Euclidean distance is greater than the current k nearest neighbor in terms of network distance. The LBC method improves on INE by avoiding the visit of nodes that cannot possibly lead to the k nearest neighbors by using a Euclidean heuristic estimator, and on IER by avoiding the repeated visits to nodes in the spatial network that appear on the shortest paths to different members of the k nearest neighbors by performing multiple instances of heuristic search using a Euclidean heuristic estimator on candidate objects around the query point. LBC's drawback is that the maintenance of multiple instances of heuristic search (called wavefronts) requires k priority queues and the queue operations required to maintain them incur a high in-memory processing cost. A method (SWH) is proposed that utilizes a novel heuristic function which considers objects surrounding the query point together as a single unit, instead of as one destination at a time as in LBC, thereby eliminating the need for multiple wavefronts and needs just one priority queue. These results in a significant reduction in the in-memory processing cost components while having the same reduced cost of the access to the spatial network as LBC. SWH is also extended to support the incremental distance semi-join (IDSJ) query, which is a multiple query point generalization of the kNN query. In addition, SWH is shown to support landmark-based heuristic functions, thereby enabling it to be applied to non-spatial networks/graphs such as social networks. Comparisons of experiments on SWH for kNN queries with INE, the best single-wavefront method, show that SWH is 2.5 times faster, and with LBC, the best existing heuristic search method, show that SWH is 3.5 times faster. For IDSJ queries, SWH-IDSJ is 5 times faster than INE-IDSJ, and 4 times faster than LBC-IDSJ.

#index 2010430
#* Layered processing of skyline-window-join (SWJ) queries using iteration-fabric
#@ Mithila Nagendra;K. Selcuk Candan
#t 2013
#c 17
#! The problem of finding interesting tuples in a data set, more commonly known as the skyline problem, has been extensively studied in scenarios where the data is static. More recently, skyline research has moved towards data streaming environments, where tuples arrive/expire in a continuous manner. Several algorithms have been developed to track skyline changes over sliding windows; however, existing methods focus on skyline analysis in which all required skyline attributes belong to a single incoming data stream. This constraint renders current algorithms unsuitable for applications that require a real-time “join” operation to be carried out between multiple incoming data streams, arriving from different sources, before the skyline query can be answered. Based on this motivation, in this paper, we address the problem of computing skyline-window-join (SWJ) queries over pairs of data streams, considering sliding windows that take into account only the most recent tuples. In particular, we propose a Layered Skyline-window-Join (LSJ) operator that (a) partitions the overall process into processing layers and (b) maintains skyline-join results in an incremental manner by continuously monitoring the changes in all layers of the process. We combine the advantages of existing skyline methods (including those that efficiently maintain skyline results over a single stream, and those that compute the skyline of pairs of static data sets) to develop a novel iteration-fabric skyline-window-join processing structure. Using the iteration-fabric, LSJ eliminates redundant work across consecutive windows by leveraging shared data across all iteration layers of the windowed skyline-join processing. To the best of our knowledge, this is the first paper that addresses join-based skyline queries over sliding windows. Extensive experimental evaluations over real and simulated data show that LSJ provides large gains over naive extensions of existing schemes which are not designed to eliminate redundant work across multiple processing layers.

#index 2010431
#* Efficient snapshot retrieval over historical graph data
#@ Udayan Khurana;Amol Deshpande
#t 2013
#c 17
#! We present a distributed graph database system to manage historical data for large evolving information networks, with the goal to enable temporal and evolutionary queries and analysis. The cornerstone of our system is a novel, user-extensible, highly tunable, and distributed hierarchical index structure called DeltaGraph, that enables compact recording of the historical network information, and that supports efficient retrieval of historical graph snapshots for single-site or parallel processing. Our system exposes a general programmatic API to process and analyze the retrieved snapshots. Along with the original graph data, DeltaGraph can also maintain and index auxiliary information; this functionality can be used to extend the structure to efficiently execute queries like subgraph pattern matching over historical data. We develop analytical models for both the storage space needed and the snapshot retrieval times to aid in choosing the right construction parameters for a specific scenario. We also present an in-memory graph data structure called GraphPool that can maintain hundreds of historical graph instances in main memory in a non-redundant manner. We present a comprehensive experimental evaluation that illustrates the effectiveness of our proposed techniques at managing historical graph information.

#index 2010432
#* Scalable maximum clique computation using MapReduce
#@ Ashraf Aboulnaga;Jingen Xiang;Cong Guo
#t 2013
#c 17
#! We present a scalable and fault-tolerant solution for the maximum clique problem based on the MapReduce framework. The key contribution that enables us to effectively use MapReduce is a recursive partitioning method that partitions the graph into several subgraphs of similar size. After partitioning, the maximum cliques of the different partitions can be computed independently, and the computation is sped up using a branch and bound method. Our experiments show that our approach leads to good scalability, which is unachievable by other partitioning methods since they result in partitions of different sizes and hence lead to load imbalance. Our method is more scalable than an MPI algorithm, and is simpler and more fault tolerant.

#index 2010433
#* Graph stream classification using labeled and unlabeled graphs
#@ Xingquan Zhu;Chengqi Zhang;Shirui Pan;Philip S. Yu
#t 2013
#c 17
#! Graph classification is becoming increasingly popular due to the rapidly rising applications involving data with structural dependency. The wide spread of the graph applications and the inherent complex relationships between graph objects have made the labels of the graph data expensive and/or difficult to obtain, especially for applications involving dynamic changing graph records. While labeled graphs are limited, the copious amounts of unlabeled graphs are often easy to obtain with trivial efforts. In this paper, we propose a framework to build a stream based graph classification model by combining both labeled and unlabeled graphs. Our method, called gSLU, employs an ensemble based framework to partition graph streams into a number of graph chunks each containing some labeled and unlabeled graphs. For each individual chunk, we propose a minimum-redundancy subgraph feature selection module to select a set of informative subgraph features to build a classifier. To tackle the concept drifting in graph streams, an instance level weighting mechanism is used to dynamically adjust the instance weight, through which the subgraph feature selection can emphasize on difficult graph samples. The classifiers built from different graph chunks form an ensemble for graph stream classification. Experiments on real-world graph streams demonstrate clear benefits of using minimum-redundancy subgraph features to build accurate classifiers. By employing instance level weighting, our graph ensemble model can effectively adapt to the concept drifting in the graph stream for classification.

#index 2010434
#* Efficient distance-aware query evaluation on indoor moving objects
#@ Torben Bach Pedersen;Xike Xie;Hua Lu
#t 2013
#c 17
#! Indoor spaces accommodate large parts of people's life. The increasing availability of indoor positioning, driven by technologies like Wi-Fi, RFID, and Bluetooth, enables a variety of indoor location-based services (LBSs). Efficient indoor distance-aware queries on indoor moving objects play an important role in supporting and boosting such LBSs. However, the distance-aware query evaluation on indoor moving objects is challenging because: (1) indoor spaces are characterized by many special entities and thus render distance calculation very complex; (2) the limitations of indoor positioning technologies create inherent uncertainties in indoor moving objects data. In this paper, we propose a complete set of techniques for efficient distance-aware queries on indoor moving objects. We define and categorize the indoor distances in relation to indoor uncertain objects, and derive different distance bounds that can facilitate query evaluation. Existing works often assume indoor floor plans are static, and require extensive pre-computation on indoor topologies. In contrast, we design a composite index scheme that integrates indoor geometries, indoor topologies, as well as indoor uncertain objects, and thus supports indoor distance-aware queries efficiently without time-consuming and volatile distance computation. We design algorithms for range query and k nearest neighbor query on indoor moving objects. The results of extensive experimental studies demonstrate that our proposals are efficient and scalable in evaluating distance-aware queries over indoor moving objects.

#index 2010435
#* Cleaning uncertain data for top-k queries
#@ David W. Cheung;Reynold Cheng;Luyi Mo;Xiang Li;Xuan S. Yang
#t 2013
#c 17
#! The information managed in emerging applications, such as sensor networks, location-based services, and data integration, is inherently imprecise. To handle data uncertainty, probabilistic databases have been recently developed. In this paper, we study how to quantify the ambiguity of answers returned by a probabilistic top-k query. We develop efficient algorithms to compute the quality of this query under the possible world semantics. We further address the cleaning of a probabilistic database, in order to improve top-k query quality. Cleaning involves the reduction of ambiguity associated with the database entities. For example, the uncertainty of a temperature value acquired from a sensor can be reduced, or cleaned, by requesting its newest value from the sensor. While this “cleaning operation” may produce a better query result, it may involve a cost and fail. We investigate the problem of selecting entities to be cleaned under a limited budget. Particularly, we propose an optimal solution and several heuristics. Experiments show that the greedy algorithm is efficient and close to optimal.

#index 2010436
#* A demonstration of the G∗ graph database system
#@ Sean R. Spillane;Daniel Bokser;Daniel Kemp;Jeong-Hyon Hwang;Jeremy Birnbaum;Alan Labouseur;Paul W. Olsen;Jayadevan Vijayan;Jun-Weon Yoon
#t 2013
#c 17
#! The world is full of evolving networks, many of which can be represented by a series of large graphs. Neither the current graph processing systems nor database systems can efficiently store and query these graphs due to their lack of support for managing multiple graphs and lack of essential graph querying capabilities. We propose to demonstrate our system, G∗, that meets the new challenges of managing multiple graphs and supporting fundamental graph querying capabilities. G∗ can store graphs on a large number of servers while compressing these graphs based on their commonalities. G∗ also allows users to easily express queries on graphs and efficiently executes these queries by sharing computations across graphs. During our demonstrations, conference attendees will run various analytic queries on large, practical data sets. These demonstrations will highlight the convenience and performance benefits of G∗ over existing database and graph processing systems, the effectiveness of sharing in graph data storage and processing, as well as G∗'s scalability.

#index 2010437
#* Automatic extraction of top-k lists from the web
#@ Kenny Q. Zhu;Hongsong Li;Zhixian Zhang;Haixun Wang
#t 2013
#c 17
#! This paper is concerned with information extraction from top-k web pages, which are web pages that describe top k instances of a topic which is of general interest. Examples include “the 10 tallest buildings in the world”, “the 50 hits of 2010 you don't want to miss”, etc. Compared to other structured information on the web (including web tables), information in top-k lists is larger and richer, of higher quality, and generally more interesting. Therefore top-k lists are highly valuable. For example, it can help enrich open-domain knowledge bases (to support applications such as search or fact answering). In this paper, we present an efficient method that extracts top-k lists from web pages with high performance. Specifically, we extract more than 1.7 million top-k lists from a web corpus of 1.6 billion pages with 92.0% precision and 72.3% recall.

#index 2010438
#* Finding interesting correlations with conditional heavy hitters
#@ Graham Cormode;Katsiaryna Mirylenka;Themis Palpanas;Divesh Srivastava
#t 2013
#c 17
#! The notion of heavy hitters-items that make up a large fraction of the population — has been successfully used in a variety of applications across sensor and RFID monitoring, network data analysis, event mining, and more. Yet this notion often fails to capture the semantics we desire when we observe data in the form of correlated pairs. Here, we are interested in items that are conditionally frequent: when a particular item is frequent within the context of its parent item. In this work, we introduce and formalize the notion of Conditional Heavy Hitters to identify such items, with applications in network monitoring, and Markov chain modeling. We introduce several streaming algorithms that allow us to find conditional heavy hitters efficiently, and provide analytical results. Different algorithms are successful for different input characteristics. We perform experimental evaluations to demonstrate the efficacy of our methods, and to study which algorithms are most suited for different types of data.

#index 2010439
#* TYPifier: Inferring the type semantics of structured data
#@ Yongtao Ma;Thanh Tran;Veli Bicer
#t 2013
#c 17
#! Structured data representing entity descriptions often lacks precise type information. That is, it is not known to which type an entity belongs to, or the type is too general to be useful. In this work, we propose to deal with this novel problem of inferring the type semantics of structured data, called typification. We formulate it as a clustering problem and discuss the features needed to obtain several solutions based on existing clustering solutions. Because schema features perform best, but are not abundantly available, we propose an approach to automatically derive them from data. Optimized for the use of schema features, we present TYPifier, a novel clustering algorithm that in experiments, yields better typification results than the baseline clustering solutions.

#index 2010440
#* Main-memory hash joins on multi-core CPUs: Tuning to the underlying hardware
#@ Jens Teubner;Gustavo Alonso;Cagri Balkesen;M. Tamer Ozsu
#t 2013
#c 17
#! The architectural changes introduced with multi-core CPUs have triggered a redesign of main-memory join algorithms. In the last few years, two diverging views have appeared. One approach advocates careful tailoring of the algorithm to the architectural parameters (cache sizes, TLB, and memory bandwidth). The other approach argues that modern hardware is good enough at hiding cache and TLB miss latencies and, consequently, the careful tailoring can be omitted without sacrificing performance. In this paper we demonstrate through experimental analysis of different algorithms and architectures that hardware still matters. Join algorithms that are hardware conscious perform better than hardware-oblivious approaches. The analysis and comparisons in the paper show that many of the claims regarding the behavior of join algorithms that have appeared in literature are due to selection effects (relative table sizes, tuple sizes, the underlying architecture, using sorted data, etc.) and are not supported by experiments run under different parameters settings. Through the analysis, we shed light on how modern hardware affects the implementation of data operators and provide the fastest implementation of radix join to date, reaching close to 200 million tuples per second.

#index 2010441
#* On the relative trust between inconsistent data and inaccurate constraints
#@ Lukasz Golab;Ihab F. Ilyas;George Beskales;Artur Galiullin
#t 2013
#c 17
#! Functional dependencies (FDs) specify the intended data semantics while violations of FDs indicate deviation from these semantics. In this paper, we study a data cleaning problem in which the FDs may not be completely correct, e.g., due to data evolution or incomplete knowledge of the data semantics. We argue that the notion of relative trust is a crucial aspect of this problem: if the FDs are outdated, we should modify them to fit the data, but if we suspect that there are problems with the data, we should modify the data to fit the FDs. In practice, it is usually unclear how much to trust the data versus the FDs. To address this problem, we propose an algorithm for generating non-redundant solutions (i.e., simultaneous modifications of the data and the FDs) corresponding to various levels of relative trust. This can help users determine the best way to modify their data and/or FDs to achieve consistency.

#index 2010442
#* Catch the Wind: Graph workload balancing on cloud
#@ Zechao Shang;Jeffrey Xu Yu
#t 2013
#c 17
#! Graph partitioning is a key issue in graph database processing systems for achieving high efficiency on Cloud. However, the balanced graph partitioning itself is difficult because it is known to be NP-complete. In addition a static graph partitioning cannot keep all graph algorithms efficient for a long time in parallel on Cloud because the workload balancing in different iterations for different graph algorithms are all possible different. In this paper, we investigate graph behaviors by exploring the working window (we call it wind) changes, where a working window is a set of active vertices that a graph algorithm really needs to access in parallel computing. We investigated nine classic graph algorithms using real datasets, and propose simple yet effective policies that can achieve both high graph workload balancing and efficient partition on Cloud.

#index 2010443
#* Top-K oracle: A new way to present top-k tuples for uncertain data
#@ Tingjian Ge;Zheng Li;Chunyao Song
#t 2013
#c 17
#! Managing noisy and uncertain data is needed in a great number of modern applications. A major difficulty in managing such data is the sheer number of query result tuples with diverse probabilities. In many cases, users have a preference over the tuples in a deterministic world, determined by a scoring function. Yet it has been a challenging problem to return top-k for uncertain data. Various semantics have been proposed, and they have been shown to give wildly different tuple rankings. In this paper, we propose a completely different approach. Instead of returning users fc tuples, which are merely one point in the complex distribution of top-k tuple vectors, we provide a so-called top-k oracle and users can arbitrarily query it. Intuitively, an oracle is a black box that, whenever given an SQL query, returns its result. Any information we give is based on faithful, best-effort estimates of the ground-truth top-k tuples. This is especially critical in emergency response applications and in monitoring top-k applications. Furthermore, we are the first to provide the nested query capability with the uncertain top-k result being a subquery. We devise various query processing algorithms for top-k oracles, and verify their efficiency and accuracy through a systematic evaluation over real-world and synthetic datasets.

#index 2010444
#* Publicly verifiable grouped aggregation queries on outsourced data streams
#@ Suman Nath;Ramarathnam Venkatesan
#t 2013
#c 17
#! Outsourcing data streams and desired computations to a third party such as the cloud is a desirable option to many companies. However, data outsourcing and remote computations intrinsically raise issues of trust, making it crucial to verify results returned by third parties. In this context, we propose a novel solution to verify outsourced grouped aggregation queries (e.g., histogram or SQL Group-by queries) that are common in many business applications. We consider a setting where a data owner employs an untrusted remote server to run continuous grouped aggregation queries on a data stream it forwards to the server. Untrusted clients then query the server for results and efficiently verify correctness of the results by using a small and easy-to-compute signature provided by the data owner. Our work complements previous works on authenticating remote computation of selection and aggregation queries. The most important aspect of our solution is that it is publicly verifiable — unlike most prior works, we support untrusted clients (who can collude with other clients or with the server). Experimental results on real and synthetic data show that our solution is practical and efficient.

#index 2010445
#* Secure nearest neighbor revisited
#@ Xiaokui Xiao;Feifei Li;Bin Yao
#t 2013
#c 17
#! In this paper, we investigate the secure nearest neighbor (SNN) problem, in which a client issues an encrypted query point E(q) to a cloud service provider and asks for an encrypted data point in E(D) (the encrypted database) that is closest to the query point, without allowing the server to learn the plaintexts of the data or the query (and its result). We show that efficient attacks exist for existing SNN methods [21], [15], even though they were claimed to be secure in standard security models (such as indistinguishability under chosen plaintext or ciphertext attacks). We also establish a relationship between the SNN problem and the order-preserving encryption (OPE) problem from the cryptography field [6], [5], and we show that SNN is at least as hard as OPE. Since it is impossible to construct secure OPE schemes in standard security models [6], [5], our results imply that one cannot expect to find the exact (encrypted) nearest neighbor based on only E(q) and E(D). Given this hardness result, we design new SNN methods by asking the server, given only E(q) and E(D), to return a relevant (encrypted) partition E(G) from E(D) (i.e., G ⊆ D), such that that E(G) is guaranteed to contain the answer for the SNN query. Our methods provide customizable tradeoff between efficiency and communication cost, and they are as secure as the encryption scheme E used to encrypt the query and the database, where E can be any well-established encryption schemes.

#index 2010446
#* CrowdPlanr: Planning made easy with crowd
#@ Tova Milo;Ilia Lotosh;Slava Novgorodov
#t 2013
#c 17
#! Recent research has shown that crowd sourcing can be used effectively to solve problems that are difficult for computers, e.g., optical character recognition and identification of the structural configuration of natural proteins [1]. In this demo we propose to use the power of the crowd to address yet another difficult problem that frequently occurs in a daily life-planning a sequence of actions, when the goal is hard to formalize. For example, planning the sequence of places/attractions to visit in the course of a vacation, where the goal is to enjoy the resulting vacation the most, or planning the sequence of courses to take in an academic schedule planning, where the goal is to obtain solid knowledge of a given subject domain. Such goals may be easily understandable by humans, but hard or even impossible to formalize for a computer. We present a novel algorithm for efficiently harnessing the crowd to assist in solving such planning problems. The algorithm builds the desired plans incrementally, optimally choosing at each step the ‘best’ questions so that the overall number of questions that need to be asked is minimized. We demonstrate the effectiveness of our solution in CrowdPlanr, a system for vacation travel planning. Given a destination, dates, preferred activities and other constraints CrowdPlanr employs the crowd to build a vacation plan (sequence of places to visit) that is expected to maximize the “enjoyment” of the vacation.

#index 2010447
#* Finding distance-preserving subgraphs in large road networks
#@ Wilfred Ng;James Cheng;Steven Liu;Da Yan
#t 2013
#c 17
#! Given two sets of points, S and T, in a road network, G, a distance-preserving subgraph (DPS) query returns a subgraph of G that preserves the shortest path from any point in S to any point in T. DPS queries are important in many real world applications, such as route recommendation systems, logistics planning, and all kinds of shortest-path-related applications that run on resource-limited mobile devices. In this paper, we study efficient algorithms for processing DPS queries in large road networks. Four algorithms are proposed with different tradeoffs in terms of DPS quality and query processing time, and the best one is a graph-partitioning based index, called RoadPart, that finds a high quality DPS with short response time. Extensive experiments on large road networks demonstrate the merits of our algorithms, and verify the efficiency of RoadPart for finding a high-quality DPS.

#index 2010448
#* Accurate and efficient private release of datacubes and contingency tables
#@ Grigory Yaroslavtsev;Cecilia M. Procopiuc;Graham Cormode;Divesh Srivastava
#t 2013
#c 17
#! A central problem in releasing aggregate information about sensitive data is to do so accurately while providing a privacy guarantee on the output. Recent work focuses on the class of linear queries, which include basic counting queries, data cubes, and contingency tables. The goal is to maximize the utility of their output, while giving a rigorous privacy guarantee. Most results follow a common template: pick a “strategy” set of linear queries to apply to the data, then use the noisy answers to these queries to reconstruct the queries of interest. This entails either picking a strategy set that is hoped to be good for the queries, or performing a costly search over the space of all possible strategies. In this paper, we propose a new approach that balances accuracy and efficiency: we show how to improve the accuracy of a given query set by answering some strategy queries more accurately than others. This leads to an efficient optimal noise allocation for many popular strategies, including wavelets, hierarchies, Fourier coefficients and more. For the important case of marginal queries we show that this strictly improves on previous methods, both analytically and empirically. Our results also extend to ensuring that the returned query answers are consistent with an (unknown) data set at minimal extra cost in terms of time and noise.

#index 2010449
#* Link prediction across networks by biased cross-network sampling
#@ Charu C. Aggarwal;Thomas Huang;Guo-Jun Qi
#t 2013
#c 17
#! The problem of link inference has been widely studied in a variety of social networking scenarios. In this problem, we wish to predict future links in a growing network with the use of the existing network structure. However, most of the existing methods work well only if a significant number of links are already available in the network for the inference process. In many scenarios, the existing network may be too sparse, and may have too few links to enable meaningful learning mechanisms. This paucity of linkage information can be challenging for the link inference problem. However, in many cases, other (more densely linked) networks may be available which show similar linkage structure in terms of underlying attribute information in the nodes. The linkage information in the existing networks can be used in conjunction with the node attribute information in both networks in order to make meaningful link recommendations. Thus, this paper introduces the use of transfer learning methods for performing cross-network link inference. We present experimental results illustrating the effectiveness of the approach.

#index 2010450
#* Forecasting the data cube: A model configuration advisor for multi-dimensional data sets
#@ Ulrike Fischer;Christopher Schildt;Wolfgang Lehner;Claudio Hartmann
#t 2013
#c 17
#! Forecasting time series data is crucial in a number of domains such as supply chain management and display advertisement. In these areas, the time series data to forecast is typically organized along multiple dimensions leading to a high number of time series that need to be forecasted. Most current approaches focus only on selection and optimizing a forecast model for a single time series. In this paper, we explore how we can utilize time series at different dimensions to increase forecast accuracy and, optionally, reduce model maintenance overhead. Solving this problem is challenging due to the large space of possibilities and possible high model creation costs. We propose a model configuration advisor that automatically determines the best set of models, a model configuration, for a given multi-dimensional data set. Our approach is based on a general process that iteratively examines more and more models and simultaneously controls the search space depending on the data set, model type and available hardware. The final model configuration is integrated into F2DB, an extension of PostgreSQL, that processes forecast queries and maintains the configuration as new data arrives. We comprehensively evaluated our approach on real and synthetic data sets. The evaluation shows that our approach significantly increases forecast query accuracy while ensuring low model costs.

#index 2010451
#* Peeking into the optimization of data flow programs with MapReduce-style UDFs
#@ Kostas Tzoumas;Johann-Christoph Freytag;Volker Markl;Fabian Hueske;Mathias Peters;Matthias Ringwald;Aljoscha Krettek
#t 2013
#c 17
#! Data flows are a popular abstraction to define dataintensive processing tasks. In order to support a wide range of use cases, many data processing systems feature MapReduce-style user-defined functions (UDFs). In contrast to UDFs as known from relational DBMS, MapReduce-style UDFs have less strict templates. These templates do not alone provide all the information needed to decide whether they can be reordered with relational operators and other UDFs. However, it is well-known that reordering operators such as filters, joins, and aggregations can yield runtime improvements by orders of magnitude. We demonstrate an optimizer for data flows that is able to reorder operators with MapReduce-style UDFs written in an imperative language. Our approach leverages static code analysis to extract information from UDFs which is used to reason about the reorderbility of UDF operators. This information is sufficient to enumerate a large fraction of the search space covered by conventional RDBMS optimizers including filter and aggregation push-down, bushy join orders, and choice of physical execution strategies based on interesting properties. We demonstrate our optimizer and a job submission client that allows users to peek step-by-step into each phase of the optimization process: the static code analysis of UDFs, the enumeration of reordered candidate data flows, the generation of physical execution plans, and their parallel execution. For the demonstration, we provide a selection of relational and nonrelational data flow programs which highlight the salient features of our approach.

#index 2010452
#* Faster random walks by rewiring online social networks on-the-fly
#@ Gautam Das;Zhiguo Gong;Zhuojie Zhou;Nan Zhang
#t 2013
#c 17
#! Many online social networks feature restrictive web interfaces which only allow the query of a user's local neighborhood through the interface. To enable analytics over such an online social network through its restrictive web interface, many recent efforts reuse the existing Markov Chain Monte Carlo methods such as random walks to sample the social network and support analytics based on the samples. The problem with such an approach, however, is the large amount of queries often required (i.e., a long “mixing time”) for a random walk to reach a desired (stationary) sampling distribution. In this paper, we consider a novel problem of enabling a faster random walk over online social networks by “rewiring” the social network on-the-fly. Specifically, we develop Modified TOpology (MTO)-Sampler which, by using only information exposed by the restrictive web interface, constructs a “virtual” overlay topology of the social network while performing a random walk, and ensures that the random walk follows the modified overlay topology rather than the original one. We show that MTO-Sampler not only provably enhances the efficiency of sampling, but also achieves significant savings on query cost over real-world online social networks such as Google Plus, Epinion etc.

#index 2010453
#* Enumerating subgraph instances using map-reduce
#@ Foto N. Afrati;Dimitris Fotakis;Jeffrey D. Ullman
#t 2013
#c 17
#! The theme of this paper is how to find all instances of a given “sample” graph in a larger “data graph,” using a single round of map-reduce. For the simplest sample graph, the triangle, we improve upon the best known such algorithm. We then examine the general case, considering both the communication cost between mappers and reducers and the total computation cost at the reducers. To minimize communication cost, we exploit the techniques of [1] for computing multiway joins (evaluating conjunctive queries) in a single map-reduce round. Several methods are shown for translating sample graphs into a union of conjunctive queries with as few queries as possible. We also address the matter of optimizing computation cost. Many serial algorithms are shown to be “convertible,” in the sense that it is possible to partition the data graph, explore each partition in a separate reducer, and have the total computation cost at the reducers be of the same order as the computation cost of the serial algorithm.

