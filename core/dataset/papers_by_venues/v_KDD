#index 280387
#* Data mining (Invited talk. Abstract only): crossing the Chasm
#@ Rakesh Agrawal
#t 1999
#c 0
#! Data mining has attracted tremendous interest in the research community as well as commercial market place. The last few years have witnessed a flurry of technical innovations and introduction of comercial products. The next major challenge facing data mining is to make a transition from a niche technology to a main stream technology. I will present key technical and environmental issues that must be addressed for a successful transition.

#index 280400
#* Farming the Web for systematic business intelligence (Invited talk. Abstract only)
#@ Richard Hackathorn
#t 1999
#c 0
#! The technologies of data warehousing, data mining, hypertext analysis, information visualization, and web information resources are rapidly converging. The challenge is to architect these technologies into a system for systematic business intelligence for a corporation. We need to move from an information refining process that is often haphazard and narrow to one that is reliable and continuous. Web farming is a new area that suggests a methodology and architecture for accomplishing this.

#index 280401
#* 2001 (Invited talk. Abstract only): a statistical odyssey
#@ Daryl Pregibon
#t 1999
#c 0
#! This talk is an interim report on the 5 year plan launched in 1996 to provide a theoretical and computational foundation of Statistics for massive data sets. The plan coincided with the formation of AT&T Labs and the proposed research agenda of the InfoLab, which is both a physical laboratory and an interdisciplinary collection of information researchers in CS, mathematics, and Statistics. At the halfway point of this odyssey we can identify some success stories but more importantly it is an opportune time to re-calibrate the challenges and the milestones.

#index 280402
#* Squashing flat files flatter
#@ William DuMouchel;Chris Volinsky;Theodore Johnson;Corinna Cortes;Daryl Pregibon
#t 1999
#c 0
#% 420057

#index 280403
#* Classification and regression: money *can* grow on trees
#@ Johannes Gehrke;Wie-Yin Loh;Raghu Ramakrishnan
#t 1999
#c 0
#% 4868
#% 90661
#% 111021
#% 129980
#% 136350
#% 156181
#% 179767
#% 182686
#% 191910
#% 246747
#% 273900
#% 314784
#% 419919
#% 420065
#% 444931
#% 449529
#% 459008
#% 479640
#% 479643
#% 479787
#% 481945
#% 481949
#% 1290031
#% 1499571
#! With over 800 million pages covering most areas of human endeavor, the World-wide Web is a fertile ground for data mining research to make a difference to the effectiveness of information search. Today, Web surfers access the Web through two dominant interfaces clicking on hyperlinks and searching via keyword queries This process is often tentative and unsatisfactory Better support is needed for expressing one's information need and dealing with a search result in more structured ways than available now. Data mining and machine learning have significant roles to play towards this end.In this paper we will survey recent advances in learning and mining problems related to hypertext in general and the Web in particular. We will review the continuum of supervised to semi-supervised to unsupervised learning problems, highlight the specific challenges which distinguish data mining in the hypertext domain from data mining in the context of data warehouses, and summarize the key areas of recent and ongoing research.

#index 280404
#* Fast and effective text mining using linear-time document clustering
#@ Bjornar Larsen;Chinatsu Aone
#t 1999
#c 0
#% 67565
#% 118771
#% 144023
#% 214711
#% 218992
#% 227794
#% 232655
#% 249155
#% 262042

#index 280405
#* Scalable algorithms for mining large databases
#@ Rajeev Rastogi;Kyuseok Shim
#t 1999
#c 0
#% 36672
#% 42994
#% 61792
#% 86950
#% 129301
#% 129987
#% 152934
#% 169940
#% 172949
#% 191581
#% 191910
#% 196977
#% 201893
#% 201894
#% 210160
#% 210162
#% 210173
#% 213977
#% 227857
#% 227917
#% 227919
#% 227922
#% 228351
#% 230138
#% 232102
#% 232136
#% 248784
#% 248785
#% 248786
#% 248790
#% 248792
#% 257637
#% 273890
#% 273891
#% 273900
#% 273919
#% 280434
#% 340291
#% 369236
#% 369349
#% 437405
#% 449588
#% 452821
#% 459008
#% 459025
#% 460862
#% 461909
#% 462070
#% 462231
#% 462234
#% 463883
#% 463903
#% 464204
#% 464839
#% 479482
#% 479484
#% 479627
#% 479640
#% 479658
#% 479659
#% 479787
#% 479791
#% 479799
#% 480940
#% 480964
#% 481281
#% 481290
#% 481588
#% 481609
#% 481611
#% 481754
#% 481758
#% 481779
#% 481945
#% 481949
#% 534183
#% 631971
#% 631984
#% 631985
#% 1813697

#index 280406
#* Efficient progressive sampling
#@ Foster Provost;David Jensen;Tim Oates
#t 1999
#c 0
#% 697
#% 2194
#% 136350
#% 222442
#% 420091
#% 465753
#% 644560
#% 1272328

#index 280407
#* Clustering techniques for large data sets—from the past to the future
#@ Daniel A. Keim;Alexander Hinneburg
#t 1999
#c 0
#% 80995
#% 86950
#% 102772
#% 185079
#% 210173
#% 211794
#% 248792
#% 248796
#% 248831
#% 252360
#% 443082
#% 459012
#% 462243
#% 463414
#% 479799
#% 481281
#% 481956
#% 527022
#% 566128
#% 571100
#% 937189
#% 1650729

#index 280408
#* Event detection from time series data
#@ Valery Guralnik;Jaideep Srivastava
#t 1999
#c 0
#% 420063
#% 461903
#% 481758

#index 280409
#* Efficient mining of emerging patterns: discovering trends and differences
#@ Guozhu Dong;Jinyan Li
#t 1999
#c 0
#% 232147
#% 243706
#% 248791
#% 420062
#% 420063
#% 463903
#% 464839
#% 481290
#% 501204
#% 546047

#index 280410
#* Mining unstructured data
#@ Ronen Feldman
#t 1999
#c 0

#index 280412
#* Combining estimators to improve performance
#@ John Elder;Greg Ridgeway
#t 1999
#c 0
#% 209021

#index 280413
#* Activity monitoring: noticing interesting changes in behavior
#@ Tom Fawcett;Foster Provost
#t 1999
#c 0
#% 260148
#% 262043
#% 272632
#% 420060
#% 420064
#% 466086

#index 280414
#* Data mining by business users: integrating data mining in business processes
#@ Marcel Holsheimer
#t 1999
#c 0

#index 280416
#* Trajectory clustering with mixtures of regression models
#@ Scott Gaffney;Padhraic Smyth
#t 1999
#c 0
#% 169358

#index 280417
#* Entropy-based subspace clustering for mining numerical data
#@ Chun-Hung Cheng;Ada Waichee Fu;Yi Zhang
#t 1999
#c 0
#% 10181
#% 115608
#% 136350
#% 210162
#% 210173
#% 213977
#% 227919
#% 248790
#% 248792
#% 252399
#% 375388
#% 408638
#% 420052
#% 426376
#% 449588
#% 462243
#% 479658
#% 481281
#% 481290
#% 481949
#% 937189

#index 280419
#* CACTUS—clustering categorical data using summaries
#@ Venkatesh Ganti;Johannes Gehrke;Raghu Ramakrishnan
#t 1999
#c 0
#% 14738
#% 232117
#% 232136
#% 238413
#% 248792
#% 280419
#% 408396
#% 479659
#% 631985

#index 280422
#* Statistics and data mining techniques for lifetime value modeling
#@ D. R. Mani;James Drew;Andrew Betz;Piew Datta
#t 1999
#c 0
#% 61477
#% 376589
#% 466065
#% 572780

#index 280425
#* Mining GPS data to augment road models
#@ Seth Rogers;Pat Langley;Christopher Wilson
#t 1999
#c 0
#% 150385
#% 161241
#% 266254
#% 375388
#% 445275
#% 634112

#index 280429
#* Mining in a data-flow environment: experience in network intrusion detection
#@ Wenke Lee;Salvatore J. Stolfo;Kui W. Mok
#t 1999
#c 0
#% 152934
#% 216500
#% 420063
#% 420064
#% 1272369

#index 280433
#* Pruning and summarizing the discovered associations
#@ Bing Liu;Wynne Hsu;Yiming Ma
#t 1999
#c 0
#% 136350
#% 152934
#% 172386
#% 210160
#% 227919
#% 248785
#% 280433
#% 420112
#% 443092
#% 459008
#% 461909
#% 462238
#% 479484
#% 481290
#% 481588
#% 1499588

#index 280434
#* Mining optimized gain rules for numeric attributes
#@ Sergey Brin;Rajeev Rastogi;Kyuseok Shim
#t 1999
#c 0
#% 136704
#% 152934
#% 201894
#% 210160
#% 210162
#% 213977
#% 280434
#% 282658
#% 462234
#% 481290
#% 481588
#% 481754
#% 481758

#index 280436
#* Mining the most interesting rules
#@ Roberto J. Bayardo, Jr.;Rakesh Agrawal
#t 1999
#c 0
#% 99396
#% 152934
#% 210162
#% 227917
#% 232136
#% 248791
#% 280436
#% 376266
#% 452822
#% 462234
#% 479643
#% 631970
#% 1272179

#index 280437
#* MetaCost: a general method for making classifiers cost-sensitive
#@ Pedro Domingos
#t 1999
#c 0
#% 92533
#% 136350
#% 209021
#% 211820
#% 232106
#% 246831
#% 458372
#% 465755
#% 465912
#% 466086
#% 565245

#index 280439
#* Extending naïve Bayes classifiers using long itemsets
#@ Dimitris Meretakis;Beat Wüthrich
#t 1999
#c 0
#% 44876
#% 99400
#% 107414
#% 136350
#% 246247
#% 246831
#% 246832
#% 481290
#% 486328
#% 1650767

#index 280441
#* A classification-based methodology for planning audit strategies in fraud detection
#@ F. Bonchi;F. Giannotti;G. Mainetto;D. Pedreschi
#t 1999
#c 0
#% 42994
#% 115680
#% 136350
#% 153021
#% 198701
#% 232146
#% 234756
#% 252533
#% 266280
#% 393792
#% 417542
#% 420064
#% 461208
#% 558720

#index 280442
#* Estimating campaign benefits and modeling lift
#@ Gregory Piatetsky-Shapiro;Brij Masand
#t 1999
#c 0
#% 191910
#% 232108
#% 497805

#index 280445
#* Generalized additive neural networks
#@ William J. E. Potts
#t 1999
#c 0
#% 33917
#% 197956
#% 211820
#% 279014
#% 380342

#index 280447
#* Horting hatches an egg: a new graph-theoretic approach to collaborative filtering
#@ Charu C. Aggarwal;Joel L. Wolf;Kun-Lung Wu;Philip S. Yu
#t 1999
#c 0
#% 1921
#% 70370
#% 202011
#% 220706
#% 220711
#% 273891
#% 564279
#% 565631

#index 280448
#* Compressed data cubes for OLAP aggregate query approximation on continuous dimensions
#@ Jayavel Shanmugasundaram;Usama Fayyad;P. S. Bradley
#t 1999
#c 0
#% 80995
#% 210173
#% 210182
#% 211820
#% 227869
#% 227880
#% 248805
#% 248806
#% 248807
#% 259995
#% 273916
#% 420053
#% 451052
#% 462204
#% 481948
#% 481951
#% 482092

#index 280449
#* Discovering roll-up dependencies
#@ Jef Wijsen;Raymond T. Ng;Toon Calders
#t 1999
#c 0
#% 152934
#% 210160
#% 210182
#% 225003
#% 279170
#% 420053
#% 420062
#% 420080
#% 452747

#index 280452
#* Density-based indexing for approximate nearest-neighbor queries
#@ Kristin P. Bennett;Usama Fayyad;Dan Geiger
#t 1999
#c 0
#% 86950
#% 201893
#% 227939
#% 248796
#% 248797
#% 249321
#% 411694
#% 462070
#% 462239
#% 464195
#% 479973
#% 481956

#index 280454
#* Using a knowledge cache for interactive discovery of association rules
#@ Biswadeep Nag;Prasad M. Deshpande;David J. DeWitt
#t 1999
#c 0
#% 172939
#% 201894
#% 210182
#% 227917
#% 246016
#% 248785
#% 248806
#% 273898
#% 280454
#% 462238
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779

#index 280456
#* Using association rules for product assortment decisions: a case study
#@ Tom Brijs;Gilbert Swinnen;Koen Vanhoof;Geert Wets
#t 1999
#c 0
#% 152934
#% 201894
#% 227917
#% 227919
#% 232136
#% 420082
#% 464712
#% 481290
#% 481918

#index 280458
#* A statistical theory for quantitative association rules
#@ Yonatan Aumann;Yehuda Lindell
#t 1999
#c 0
#% 152934
#% 210160
#% 210162
#% 227919
#% 481290
#% 481779

#index 280460
#* A study of support vectors on model independent example selection
#@ Nadeem Ahmed Syed;Huan Liu;Kah Kay Sung
#t 1999
#c 0
#% 92148
#% 92533
#% 117075
#% 136350
#% 190581
#% 197394
#% 232106
#% 243727
#% 420077

#index 280463
#* Accelerating exact k-means algorithms with geometric reasoning
#@ Dan Pelleg;Andrew Moore
#t 1999
#c 0
#% 114667
#% 210173
#% 211820
#% 304932
#% 319601
#% 481281
#% 1272326
#% 1290057

#index 280467
#* An efficient algorithm to update large itemsets with early pruning
#@ Necip Fazil Ayan;Abdullah Uz Tansel;Erol Arkun
#t 1999
#c 0
#% 152934
#% 201894
#% 248785
#% 464204
#% 481290
#% 481754
#% 481779
#% 489397
#% 511333
#% 589303

#index 280471
#* Applying general Bayesian techniques to improve TAN induction
#@ Jesús Cerquides
#t 1999
#c 0
#% 246832

#index 280473
#* Breaking the barrier of transactions: mining inter-transaction association rules
#@ Anthony K.H. Tung;Hongjun Lu;Jiawei Han;Ling Feng
#t 1999
#c 0
#% 152934
#% 172386
#% 201894
#% 210160
#% 248785
#% 273899
#% 420063
#% 463903
#% 481290
#% 481758
#% 481779
#% 481954

#index 280477
#* Detecting change in categorical data: mining contrast sets
#@ Stephen D. Bay;Michael J. Pazzani
#t 1999
#c 0
#% 152934
#% 248791
#% 479785
#% 481290

#index 280478
#* Evaluating a class of distance-mapping algorithms for data mining and clustering
#@ Jason Tsong-Li Wang;Xiong Wang;King-Ip Lin;Dennis Shasha;Bruce A. Shapiro;Kaizhong Zhang
#t 1999
#c 0
#% 80995
#% 201893
#% 210173
#% 248790
#% 388776
#% 546436

#index 280480
#* Fast density estimation using CF-kernel for very large databases
#@ Tian Zhang;Raghu Ramakrishnan;Miron Livny
#t 1999
#c 0
#% 22982
#% 210173

#index 280481
#* Handling concept drifts in incremental learning with support vector machines
#@ Nadeem Ahmed Syed;Huan Liu;Kah Kay Sung
#t 1999
#c 0
#% 92533
#% 190581
#% 232106
#% 451036
#% 677159

#index 280482
#* Identifying distinctive subsequences in multivariate time series by clustering
#@ Tim Oates
#t 1999
#c 0
#% 70370
#% 252472
#% 481609

#index 280483
#* Information mining platforms: an infrastructure for KDD rapid deployment
#@ Corinna Cortes;Daryl Pregibon
#t 1999
#c 0
#% 18528
#% 248819
#% 273943
#% 420064

#index 280485
#* Interestingness via what is not interesting
#@ Sigal Sahar
#t 1999
#c 0
#% 172386
#% 232106
#% 232136
#% 443092

#index 280487
#* Mining association rules with multiple minimum supports
#@ Bing Liu;Wynne Hsu;Yiming Ma
#t 1999
#c 0
#% 152934
#% 201894
#% 227917
#% 248785
#% 280433
#% 280487
#% 462234
#% 462238
#% 481290
#% 481588
#% 481758

#index 280488
#* Mining features for sequence classification
#@ Neal Lesh;Mohammed J. Zaki;Mitsunori Ogihara
#t 1999
#c 0
#% 124589
#% 179770
#% 259993
#% 266284
#% 451055
#% 729437

#index 280490
#* Mining lesion-deficit associations in a brain image database
#@ Vasileios Megalooikonomou;Christos Davatzikos;Edward H. Herskovits
#t 1999
#c 0
#% 6199
#% 57628
#% 129987
#% 227611

#index 280492
#* On the merits of building categorization systems by supervised clustering
#@ Charu C. Aggarwal;Stephen C. Gates;Philip S. Yu
#t 1999
#c 0
#% 118771
#% 232655
#% 262050
#% 300131
#% 465747
#% 571073

#index 280494
#* Prediction with local patterns using cross-entropy
#@ Heikki Mannila;Dmitry Pavlov;Padhraic Smyth
#t 1999
#c 0
#% 36683
#% 152934
#% 232136
#% 238413

#index 280496
#* The application of AdaBoost for distributed, scalable and on-line learning
#@ Wei Fan;Salvatore J. Stolfo;Junxin Zhang
#t 1999
#c 0
#% 235377
#% 252009
#% 565528

#index 280498
#* The impact of changing populations on classifier performance
#@ Mark G. Kelly;David J. Hand;Niall M. Adams
#t 1999
#c 0
#% 204531

#index 280499
#* Towards automated synthesis of data mining programs
#@ Wray Buntine;Bernd Fischer;Thomas Pressburger
#t 1999
#c 0
#% 178066
#% 232110
#% 277396
#% 445209
#% 497633
#% 585688
#% 748465

#index 280500
#* User profiling in personalization applications through rule discovery and validation
#@ Gediminas Adomavicius;Alexander Tuzhilin
#t 1999
#c 0
#% 172386
#% 227917
#% 232136
#% 443092
#% 462238

#index 280501
#* Using approximations to scale exploratory data analysis in datacubes
#@ Daniel Barbará;Xintao Wu
#t 1999
#c 0
#% 36672
#% 236410
#% 459025
#% 464215
#% 479450

#index 280502
#* Bayesian networks for lossless dataset compression
#@ Scott Davies;Andrew Moore
#t 1999
#c 0
#% 44876
#% 193743
#% 261205
#% 280502
#% 656044
#% 687769
#% 1272326

#index 280511
#* Visual classification: an interactive approach to decision tree construction
#@ Mihael Ankerst;Christian Elsen;Martin Ester;Hans-Peter Kriegel
#t 1999
#c 0
#% 191910
#% 273890
#% 376266
#% 481945
#% 619523

#index 280512
#* Text mining: finding nuggets in mountains of textual data
#@ Jochen Dörre;Peter Gerstl;Roland Seiffert
#t 1999
#c 0
#% 742425

#index 280513
#* Monitoring a newsfeed for hot topics
#@ Mark Shewhart;Mark Wasson
#t 1999
#c 0
#% 252533

#index 280514
#* Origami: a new data visualization tool
#@ Jen Que Louie;Tom Kraay
#t 1999
#c 0

#index 280515
#* Discovery of fraud rules for telecommunications—challenges and solutions
#@ Saharon Rosset;Uzi Murad;Einat Neumann;Yizhak Idan;Gadi Pinkas
#t 1999
#c 0
#% 136350
#% 420064
#% 481758
#% 493574
#% 586813

#index 280517
#* Optimization of collection efforts in automobile financing—a KDD supported environment
#@ H. Kauderer;G. Nakhaeizadeh;F. Artiles;H. Jeromin
#t 1999
#c 0
#% 136350

#index 280519
#* WAPS, a data mining support environment for the planning of warranty and goodwill costs in the automobile industry
#@ E. Hotz;G. Nakhaeizadeh;B. Petzsche;H. Spiegelberger
#t 1999
#c 0

#index 280520
#* The PanQ tool and EMF SQL for complex data management
#@ Damianos Chatziantoniou
#t 1999
#c 0
#% 238650
#% 248813
#% 287213
#% 464215
#% 482082
#% 632007

#index 280521
#* NonStop SQL/MX primitives for knowledge discovery
#@ John Clear;Debbie Dunn;Brad Harvey;Michael Heytens;Peter Lohman;Abhay Mehta;Mark Melton;Lars Rohrberg;Ashok Savasere;Robert Wehrmeister;Melody Xu
#t 1999
#c 0

#index 280522
#* Mining interesting knowledge using DM-II
#@ Bing Liu;Wynne Hsu;Yiming Ma;Shu Chen
#t 1999
#c 0
#% 136350
#% 172386
#% 280433
#% 280487
#% 443092
#% 481290
#% 637522
#% 1271849
#% 1499588

#index 285711
#* Adaptive query processing for time-series data
#@ Yun-Wu Huang;Philip S. Yu
#t 1999
#c 0
#% 58593
#% 86950
#% 172892
#% 172949
#% 227857
#% 289010
#% 427199
#% 452821
#% 460862
#% 461885
#% 481609
#% 481611
#% 545956

#index 310485
#* On certain rigorous approaches to data mining (invited talk) (abstract only)
#@ Christos H. Papadimitriou
#t 2000
#c 0

#index 310486
#* Informed knowledge discovery (invited talk) (abstract only): using prior knowledge in discovery programs
#@ Bruce Buchanan
#t 2000
#c 0

#index 310487
#* Among those dark electronic mills (invited talk) (abstract only): privacy and data mining
#@ Jason Catlett
#t 2000
#c 0

#index 310488
#* Hancock: a language for extracting signatures from data streams
#@ Corinna Cortes;Kathleen Fisher;Daryl Pregibon;Anne Rogers
#t 2000
#c 0
#% 18528
#% 280483
#% 297183
#% 420064

#index 310489
#* Decision support in the booming e-world (invited talk) (abstract only)
#@ James Goodnight
#t 2000
#c 0

#index 310490
#* E-metrics: tomorrow's business metrics today (invited talk) (abstract only)
#@ Matt Cutler
#t 2000
#c 0

#index 310491
#* After the gold rush (invited talk) (abstract only): data mining in the new economy
#@ David Stodder
#t 2000
#c 0

#index 310492
#* Mining IC test data to optimize VLSI testing
#@ Tony Fountain;Thomas Dietterich;Bill Sudyka
#t 2000
#c 0
#% 708018

#index 310493
#* An empirical analysis of techniques for constructing and searching k-dimensional trees
#@ Douglas A. Talbert;Doug Fisher
#t 2000
#c 0
#% 129212
#% 304932
#% 317313
#% 321455
#% 404817
#% 466264
#% 1290057

#index 310494
#* Generating non-redundant association rules
#@ Mohammed J. Zaki
#t 2000
#c 0
#% 169705
#% 172386
#% 227917
#% 232136
#% 248785
#% 248791
#% 279120
#% 280433
#% 280436
#% 384416
#% 459020
#% 481754

#index 310495
#* Ongoing management and application of discovered knowledge in a large regulatory organization: a case study of the use and impact of NASD Regulation's Advanced Detection System (RADS)
#@ Ted E. Senator
#t 2000
#c 0
#% 232102
#% 232175
#% 280413

#index 310496
#* Small is beautiful: discovering the minimal set of unexpected patterns
#@ Balaji Padmanabhan;Alexander Tuzhilin
#t 2000
#c 0
#% 210160
#% 232136
#% 280433
#% 280436
#% 304319
#% 443092
#% 479785
#% 631970
#% 709484
#% 1499588

#index 310498
#* Data selection for support vector machine classifiers
#@ Glenn Fung;Olvi L. Mangasarian
#t 2000
#c 0
#% 26125
#% 190581
#% 271281
#% 376266
#% 384950
#% 420077
#% 466084
#% 572922

#index 310500
#* Mining high-speed data streams
#@ Pedro Domingos;Geoff Hulten
#t 2000
#c 0
#% 136350
#% 191257
#% 273900
#% 280406
#% 449529
#% 451055
#% 459008
#% 479658
#% 481779
#% 481945
#% 963888
#% 1272179
#% 1290030
#% 1499581

#index 310502
#* Deformable Markov model templates for time-series pattern matching
#@ Xianping Ge;Padhraic Smyth
#t 2000
#c 0
#% 172949
#% 285711
#% 460862
#% 461885
#% 462231
#% 481609
#% 617843
#% 631923

#index 310503
#* Active learning using adaptive resampling
#@ Vijay S. Iyengar;Chidanand Apte;Tong Zhang
#t 2000
#c 0
#% 96688
#% 116165
#% 165110
#% 169717
#% 170649
#% 236729
#% 271060
#% 280406
#% 312727
#% 393792
#% 424997
#% 445319
#% 450951
#% 451056
#% 465754
#% 466095
#% 476744
#% 565531
#% 1272282
#% 1478821

#index 310505
#* Efficient search for association rules
#@ Geoffrey I. Webb
#t 2000
#c 0
#% 152934
#% 179770
#% 201894
#% 232136
#% 280436
#% 420112
#% 449566
#% 481754
#% 481779
#% 708179
#% 1272179

#index 310507
#* Depth first generation of long patterns
#@ Ramesh C. Agarwal;Charu C. Aggarwal;V. V. V. Prasad
#t 2000
#c 0
#% 152934
#% 172386
#% 210160
#% 210162
#% 213977
#% 227917
#% 227919
#% 248012
#% 248791
#% 273898
#% 273899
#% 280436
#% 280454
#% 300120
#% 329598
#% 459020
#% 462234
#% 462238
#% 464714
#% 481290
#% 481754
#% 481758
#% 631986

#index 310509
#* The IGrid index: reversing the dimensionality curse for similarity indexing in high dimensional space
#@ Charu C. Aggarwal;Philip S. Yu
#t 2000
#c 0
#% 68091
#% 86950
#% 201876
#% 217292
#% 227939
#% 248796
#% 273891
#% 273920
#% 280419
#% 280452
#% 300131
#% 406493
#% 427199
#% 435141
#% 464195
#% 479649
#% 479973
#% 480093
#% 480132
#% 481455
#% 481956
#% 632043

#index 310511
#* The generalized Bayesian committee machine
#@ Volker Tresp
#t 2000
#c 0
#% 73372
#% 190581
#% 198701
#% 235377
#% 266408
#% 268069
#% 269212
#% 360691
#% 857421

#index 310512
#* A general probabilistic framework for clustering individuals and objects
#@ Igor V. Cadez;Scott Gaffney;Padhraic Smyth
#t 2000
#c 0
#% 36672
#% 280416
#% 310543
#% 466243

#index 310514
#* Efficient identification of Web communities
#@ Gary William Flake;Steve Lawrence;C. Lee Giles
#t 2000
#c 0
#% 8919
#% 70370
#% 122671
#% 249110
#% 281251
#% 282226
#% 282905
#% 288780
#% 408396
#% 443723
#% 584932

#index 310515
#* Global partial orders from sequential data
#@ Heikki Mannila;Christopher Meek
#t 2000
#c 0
#% 172949
#% 420063
#% 459006
#% 463903

#index 310516
#* Efficient clustering of high-dimensional data sets with application to reference matching
#@ Andrew McCallum;Kamal Nigam;Lyle H. Ungar
#t 2000
#c 0
#% 46803
#% 201889
#% 210173
#% 249143
#% 304932
#% 317313
#% 420495

#index 310517
#* Towards an effective cooperation of the user and the computer for classification
#@ Mihael Ankerst;Martin Ester;Hans-Peter Kriegel
#t 2000
#c 0
#% 191910
#% 280511
#% 376266
#% 420093
#% 420102
#% 438134
#% 459008

#index 310518
#* A framework for specifying explicit bias for revision of approximate information extraction rules
#@ Ronen Feldman;Yair Liberzon;Binyamin Rosenfeld;Jonathan Schler;Jonathan Stoppi
#t 2000
#c 0
#% 147068
#% 165115
#% 264706
#% 266215
#% 278109
#% 449588
#% 465919
#% 476568
#% 477642
#% 477653
#% 477817
#% 477971
#% 496886
#% 815335

#index 310519
#* Explicitly representing expected cost: an alternative to ROC representation
#@ Chris Drummond;Robert C. Holte
#t 2000
#c 0
#% 260149
#% 266280
#% 280437
#% 420064
#% 466086

#index 310520
#* Multi-level organization and summarization of the discovered rules
#@ Bing Liu;Minqing Hu;Wynne Hsu
#t 2000
#c 0
#% 78791
#% 136350
#% 172386
#% 248785
#% 280433
#% 280500
#% 443092
#% 481290
#% 481588
#% 501204
#% 529648
#% 1499588

#index 310521
#* Visualization and the process of modeling: a cognitive-theoretic view
#@ Andrew W. Crapo;Laurie B. Waisel;William A. Wallace;Thomas R. Willemain
#t 2000
#c 0
#% 1925
#% 3018
#% 5185
#% 28144
#% 52577
#% 68659
#% 75896
#% 180947
#% 207824
#% 208154
#% 257684
#% 270633
#% 314985
#% 332530
#% 363751
#% 405949
#% 422684
#% 573331
#% 707859
#% 835098
#% 835105

#index 310525
#* Visualizing association rules with interactive mosaic plots
#@ Heike Hofmann;Arno P. J. M. Siebes;Adalbert F. X. Wilhelm
#t 2000
#c 0
#% 152934
#% 280433
#% 280436
#% 280456

#index 310526
#* Interactive exploration of very large relational datasets through 3D dynamic projections
#@ Li Yang
#t 2000
#c 0
#% 1211
#% 17144
#% 76703
#% 86286
#% 115659
#% 321455
#% 411694
#% 434477
#% 619521
#% 641108

#index 310531
#* RuleViz: a model for visualizing knowledge discovery process
#@ Jianchao Han;Nick Cercone
#t 2000
#c 0
#% 136350
#% 210162
#% 221380
#% 224592
#% 227953
#% 232106
#% 232108
#% 238650
#% 259754
#% 267537
#% 280511
#% 376266
#% 385563
#% 436116
#% 443086
#% 533968
#% 726032

#index 310533
#* Hardening soft information sources
#@ William W. Cohen;Henry Kautz;David McAllester
#t 2000
#c 0
#% 194290
#% 201889
#% 248801
#% 408396
#% 442830
#% 465919

#index 310537
#* Using the fractal dimension to cluster datasets
#@ Daniel Barbará;Ping Chen
#t 2000
#c 0
#% 36672
#% 182913
#% 479799
#% 566128
#% 937189

#index 310539
#* Growing decision trees on support-less association rules
#@ Ke Wang;Senqiang Zhou;Yu He
#t 2000
#c 0
#% 136350
#% 152934
#% 246832
#% 280439
#% 479817
#% 481290

#index 310541
#* Efficient mining of weighted association rules (WAR)
#@ Wei Wang;Jiong Yang;Philip S. Yu
#t 2000
#c 0
#% 210160
#% 248792
#% 481290

#index 310542
#* Mining asynchronous periodic patterns in time series data
#@ Jiong Yang;Wei Wang;Philip S. Yu
#t 2000
#c 0
#% 464839
#% 479627
#% 631926

#index 310543
#* Visualization of navigation patterns on a Web site using model-based clustering
#@ Igor Cadez;David Heckerman;Christopher Meek;Padhraic Smyth;Steven White
#t 2000
#c 0

#index 310545
#* Scaling up dynamic time warping for datamining applications
#@ Eamonn J. Keogh;Michael J. Pazzani
#t 2000
#c 0
#% 137711
#% 172949
#% 461885
#% 462231
#% 466260
#% 481609
#% 617843
#% 701402
#% 705649

#index 310546
#* IntelliClean: a knowledge-based intelligent data cleaner
#@ Mong Li Lee;Tok Wang Ling;Wai Lup Low
#t 2000
#c 0
#% 201889
#% 211348
#% 394745

#index 310547
#* Towards scalable support vector machines using squashing
#@ Dmitry Pavlov;Darya Chudova;Padhraic Smyth
#t 2000
#c 0
#% 190581
#% 197394
#% 269207
#% 269218
#% 496419
#% 592108

#index 310548
#* A data mining framework for optimal product selection in retail supermarket data: the generalized PROFSET model
#@ Tom Brijs;Bart Goethals;Gilbert Swinnen;Koen Vanhoof;Geert Wets
#t 2000
#c 0
#% 152934
#% 172386
#% 243449
#% 280433
#% 280456
#% 304319
#% 383498
#% 420073
#% 420082
#% 477786
#% 998772
#% 1499588

#index 310549
#* Application of neural networks to biological data mining: a case study in protein sequence classification
#@ Jason T. L. Wang;Qicheng Ma;Dennis Shasha;Cathy H. Wu
#t 2000
#c 0
#% 132676
#% 172892
#% 388776
#% 412174
#% 471430

#index 310550
#* Exploring constraints to efficiently mine emerging patterns from large high-dimensional datasets
#@ Xiuzhen Zhang;Guozu Dong;Ramamohanarao Kotagiri
#t 2000
#c 0
#% 248785
#% 248791
#% 273899
#% 280409
#% 481290
#% 1272179

#index 310551
#* Multivariate discretization of continuous variables for set mining
#@ Stephen D. Bay
#t 2000
#c 0
#% 90848
#% 136350
#% 152934
#% 197057
#% 210160
#% 227953
#% 248791
#% 280477
#% 1272179

#index 310552
#* On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms
#@ Kenji Yamanishi;Jun-Ichi Takeuchi;Graham Williams;Peter Milne
#t 2000
#c 0
#% 280413
#% 280429
#% 280441
#% 280515
#% 479791

#index 310553
#* Unsupervised Bayesian visualization of high-dimensional data
#@ Petri Kontkanen;Jussi Lahtinen;Petri Myllymäki;Henry Tirri
#t 2000
#c 0
#% 44876
#% 67866
#% 234978
#% 246832
#% 380725
#% 424807
#% 490612
#% 1650303
#% 1650722

#index 310554
#* A sequential sampling algorithm for a general class of utility criteria
#@ Tobias Scheffer;Stefan Wrobel
#t 2000
#c 0
#% 152934
#% 180945
#% 190581
#% 211583
#% 232126
#% 477497
#% 481779
#% 1780573

#index 310555
#* Efficient algorithms for constructing decision trees with constraints
#@ Minos Garofalakis;Dongjoon Hyun;Rajeev Rastogi;Kyuseok Shim
#t 2000
#c 0
#% 61792
#% 136350
#% 170378
#% 207781
#% 273900
#% 479640
#% 481945

#index 310556
#* A classifier for semi-structured documents
#@ Jeonghee Yi;Neel Sundaresan
#t 2000
#c 0
#% 90847
#% 136350
#% 248810
#% 318412
#% 375017
#% 406493
#% 464720
#% 465747
#% 482113
#% 571073

#index 310557
#* Alpha seeding for support vector machines
#@ Dennis DeCoste;Kiri Wagstaff
#t 2000
#c 0
#% 190581
#% 269207
#% 269217
#% 269218
#% 420077
#% 496085

#index 310558
#* Can we push more constraints into frequent pattern mining?
#@ Jian Pei;Jiawei Han
#t 2000
#c 0
#% 248785
#% 273899
#% 300120
#% 481290
#% 632028

#index 310559
#* FreeSpan: frequent pattern-projected sequential pattern mining
#@ Jiawei Han;Jian Pei;Behzad Mortazavi-Asl;Qiming Chen;Umeshwar Dayal;Mei-Chun Hsu
#t 2000
#c 0
#% 210160
#% 300120
#% 329598
#% 420063
#% 459006
#% 463903
#% 481290

#index 310560
#* Visualization and interactive feature selection for unsupervised data
#@ Jennifer G. Dy;Carla E. Brodley
#t 2000
#c 0
#% 51647
#% 80995
#% 443086
#% 466414

#index 310561
#* Feature selection in unsupervised learning via evolutionary search
#@ YeongSeog Kim;W. Nick Street;Filippo Menczer
#t 2000
#c 0
#% 26125
#% 224794
#% 248792
#% 572922
#% 846507
#% 846512

#index 310562
#* Classification and visualization for high-dimensional data
#@ Alfred Inselberg;Tova Avidan
#t 2000
#c 0
#% 136350
#% 191910
#% 376266

#index 310563
#* Data mining solves tough semiconductor manufacturing problems
#@ Mike Gardner;Jack Bieker
#t 2000
#c 0
#% 136350
#% 234978
#% 376266

#index 310564
#* Genome scale prediction of protein functional class from sequence using data mining
#@ Ross D. King;Andreas Karwath;Amanda Clare;Luc Dephaspe
#t 2000
#c 0
#% 92533
#% 92776
#% 136350
#% 286668
#% 376266
#% 412588

#index 310565
#* Data mining to detect abnormal behavior in aerospace data
#@ José M. Peña;Fazel Famili;Sylvain Létourneau
#t 2000
#c 0
#% 136350
#% 280408
#% 445343
#% 637586

#index 310566
#* Predictive modeling in automotive direct marketing: tools, experiences and open issues
#@ Wendy Gersten;Rüdiger Wirth;Dirk Arndt
#t 2000
#c 0
#% 280442
#% 393792

#index 310567
#* Agglomerative clustering of a search engine query log
#@ Doug Beeferman;Adam Berger
#t 2000
#c 0
#% 36672
#% 46809
#% 55490
#% 70370
#% 152934
#% 204673
#% 220711
#% 227919
#% 253188
#% 255137
#% 262045
#% 280852
#% 282905
#% 304321

#index 310568
#* Textual data mining of service center call records
#@ Pang-Ning Tan;Hannah Blau;Steve Harp;Robert Goldman
#t 2000
#c 0
#% 46803
#% 136350
#% 152934
#% 239588
#% 278500
#% 304423
#% 406493
#% 452821
#% 477984
#% 717527
#% 742083

#index 310570
#* Automating exploratory data analysis for efficient data mining
#@ Jonathan D. Becher;Pavel Berkhin;Edmund Freeman
#t 2000
#c 0
#% 57948
#% 91872
#% 115608
#% 132779
#% 136350
#% 211707
#% 243728
#% 356892
#% 376266
#% 465741
#% 465756
#% 465905

#index 310571
#* Exploration mining in diabetic patients databases: findings and conclusions
#@ Wynne Hsu;Mong Li Lee;Bing Liu;Tok Wang Ling
#t 2000
#c 0
#% 152934
#% 172386
#% 201889
#% 280433
#% 310520
#% 412479
#% 443092
#% 1499588

#index 310572
#* Cross-sell: a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities
#@ Brendan Kitts;David Freed;Martin Vrieze
#t 2000
#c 0
#% 17144
#% 481290
#% 1650569

#index 310574
#* Identifying prospective customers
#@ Paul B. Chou;Edna Grossman;Dimitrios Gunopulos;Pasumarti Kamesam
#t 2000
#c 0
#% 136350
#% 210173
#% 248792
#% 459008
#% 481281
#% 481945

#index 310575
#* Targeting the right students using data mining
#@ Yiming Ma;Bing Liu;Ching Kian Wong;Philip S. Yu;Shuik Ming Lee
#t 2000
#c 0
#% 136350
#% 152934
#% 210162
#% 227917
#% 248785
#% 273900
#% 280436
#% 280439
#% 280442
#% 280473
#% 280487
#% 280494
#% 462234
#% 481290
#% 481588
#% 481779
#% 481945
#% 1272365

#index 310577
#* Evolutionary algorithms in data mining: multi-objective performance modeling for direct marketing
#@ Siddhartha Bhattacharyya
#t 2000
#c 0
#% 81982
#% 82760
#% 124073
#% 160830
#% 166235
#% 167520
#% 207535
#% 216128
#% 369236
#% 465696
#% 466219
#% 466374
#% 478671
#% 573224
#% 1022816

#index 310578
#* Hybrid Poisson process
#@ Ayman Farahat
#t 2000
#c 0
#% 184495
#% 280408
#% 280413
#% 304917
#% 406493
#% 740900

#index 310579
#* Data mining techniques for optimizing inventories for electronic commerce
#@ Anjali Dhond;Amar Gupta;Sanjeev Vadhavkar
#t 2000
#c 0
#% 92135
#% 365240
#% 420075
#% 446058
#% 508320

#index 310580
#* Mining the stock market (extended abstract): which measure is best?
#@ Martin Gavrilov;Dragomir Anguelov;Piotr Indyk;Rajeev Motwani
#t 2000
#c 0
#% 115462
#% 172949
#% 227857
#% 236692
#% 280404
#% 285711
#% 460862
#% 477479
#% 481609

#index 310583
#* Discovering similar patterns in time series
#@ Juan P. Caraça-Valente;Ignacio López-Chavarrías
#t 2000
#c 0
#% 86950
#% 232102
#% 232108
#% 232122
#% 300411
#% 367346
#% 460862
#% 463903
#% 481290
#% 514736
#% 710509

#index 310584
#* Defection detection: using activity profiles to predict ISP customer vulnerability
#@ Nandini Raghavan;Robert M. Bell;Matthias Schonlau
#t 2000
#c 0

#index 310585
#* Incremental quantile estimation for massive tracking
#@ Fei Chen;Diane Lambert;José C. Pinheiro
#t 2000
#c 0
#% 248820

#index 310586
#* Discovery of multi-level rules and exceptions from a distributed database
#@ Rónán Páircéir;Sally McClean;Bryan Scotney
#t 2000
#c 0
#% 223781
#% 237198
#% 340291
#% 403617
#% 459025
#% 464215
#% 477805
#% 641013

#index 310874
#* Data mining for hypertext (tutorial session) (title only)
#@ Soumen Chakrabarti
#t 2000
#c 0

#index 310875
#* Multidimensional visualization for high dimensional datasets and multivariate relations (tutorial session) (title only)
#@ Alfred Inselberg
#t 2000
#c 0

#index 310876
#* Knowledge discobery in biological domains (tutorial session) (title only)
#@ I. Jurisica;I. Rigoutsos;A. Floratos
#t 2000
#c 0

#index 310877
#* Successful customer relationship management in financial applications
#@ Gregory Piatesky-Shapiro;Steve Gallant;Dorian Pyle
#t 2000
#c 0

#index 310878
#* Time series similarity measures (tutorial session) (title only)
#@ Gautam Das;Dimitrios Gunopulos
#t 2000
#c 0

#index 310879
#* High performance data mining (tutorial session) (title only)
#@ Vipin Kumar;Mohammed Zaki
#t 2000
#c 0

#index 310880
#* Web mining for e-commerce (workshop session) (title only)
#@ Ronny Kohavi;Myra Spilopoulou;Jaideep Srivastava
#t 2000
#c 0

#index 310881
#* Distributed and parallel knowledge discovery (workshop session) (title only)
#@ Hillol Kargupta;Philip Chan;Vipin Kumar;Zoran Obradovic
#t 2000
#c 0

#index 310882
#* Multimedia data mining (workshop session) (title only)
#@ Simeon J. Simoff;Osmar R. Zaïane
#t 2000
#c 0

#index 310883
#* Text mining (workshop session) (title only)
#@ Marko Grobelnik;Dunja Miadenik;Natasa Milic-Frayling
#t 2000
#c 0

#index 310884
#* Post-processing in machine learning and data mining (workshop session) (title only)
#@ A. Famili;Ivan Bruha
#t 2000
#c 0

#index 310885
#* KDD process standards (panel session) (title only)
#@ Brad Husick;Rudieger Wirth;Robert Grossman;Usama Fayyad;Erik Thomsen;Ismail Parsa
#t 2000
#c 0

#index 310886
#* Personalization and data mining (panel session) (title only): exploring the synergies
#@ Paul Hagen;Rony Kahavi;Bonnie J. Lowell;John Riedl;Alex Tuzhilin
#t 2000
#c 0

#index 310887
#* Privacy (panel session) (title only)
#@ Dan Jaye;Lyle Ungar;Jane Swift;Jonathan Smith
#t 2000
#c 0

#index 312043
#* Hypertext data mining (tutorial AM-1)
#@ Soumen Chakrabarti
#t 2000
#c 0

#index 312048
#* Visualizing high dimensional datasets and multivariate relations (tutorial AM-2)
#@ Alfred Inselberg
#t 2000
#c 0
#% 1211
#% 27058
#% 28144
#% 63662
#% 68659
#% 76703
#% 80227
#% 80330
#% 162710
#% 162711
#% 172811
#% 234401
#% 234733
#% 240337
#% 405949
#% 434411
#% 434558
#% 434559
#% 435912
#% 435913
#% 435914
#% 435915
#% 533699
#% 619521
#% 619859
#% 641084
#% 674748
#% 702875
#% 726016
#% 726017
#% 726032
#% 726066
#% 727085

#index 312050
#* Knowledge discovery in biological domains (tutorial AM-3)
#@ A. Floratos;I. Jurisica;I. Rigoutsos
#t 2000
#c 0

#index 312052
#* Successful customer relationship management in financial applications (tutorial PM-1)
#@ Steve Gallant;Gregory Piatesky-Shapiro;Dorian Pyle
#t 2000
#c 0

#index 312054
#* Time series similarity measures (tutorial PM-2)
#@ Dimitrios Gunopulos;Gautam Das
#t 2000
#c 0
#% 86950
#% 172949
#% 191581
#% 201893
#% 227857
#% 227924
#% 227937
#% 232122
#% 236692
#% 240182
#% 248796
#% 248798
#% 249321
#% 260008
#% 273704
#% 280408
#% 280478
#% 280846
#% 289020
#% 294634
#% 303021
#% 310502
#% 310545
#% 359751
#% 427199
#% 435141
#% 458297
#% 460862
#% 462070
#% 462231
#% 477479
#% 477825
#% 477968
#% 479973
#% 480093
#% 480146
#% 480156
#% 480307
#% 481609
#% 481611
#% 503713
#% 534183
#% 546285
#% 562803
#% 566132
#% 616530
#% 617843
#% 631920
#% 631926
#% 661026

#index 312055
#* High performance data mining (tutorial PM-3)
#@ Vipin Kumar;Mohammed Zaki
#t 2000
#c 0
#% 68518
#% 189880
#% 199538
#% 201075
#% 204528
#% 227922
#% 232136
#% 232641
#% 248786
#% 250046
#% 259993
#% 296825
#% 328271
#% 340290
#% 340291
#% 382327
#% 390532
#% 420063
#% 420066
#% 420067
#% 420091
#% 420095
#% 420096
#% 420097
#% 420098
#% 434348
#% 434349
#% 443085
#% 443091
#% 444138
#% 458318
#% 459006
#% 463903
#% 468927
#% 470693
#% 479952
#% 481945
#% 501216
#% 631969
#% 633175
#% 660537
#% 1478807

#index 342583
#* Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Doheon Lee;Mario Schkolnick;Foster Provost;Ramakrishnan Srikant
#t 2001
#c 0
#! The end of the 20th century saw an explosion of investment in connecting computer systems--within organizations, between organizations, and between organizations and individuals--and a corresponding explosion in the on-line collection of data. Now that we have entered the 21st century we face the problem of extracting useful knowledge from these data, which is becoming increasingly difficult as volume and complexity push traditional analysis methods beyond their limits. Knowledge Discovery and Data Mining (KDD) techniques address this problem. The annual ACM SIGKDD International Conference on Knowledge Discovery and Data Mining brings together researchers and practitioners focusing on new developments and challenges in KDD. KDD-2001, the seventh conference in the series, was held in San Francisco, on August 26-29, 2001.We received 203 research-paper submissions from twenty-three countries. Each submitted research paper was reviewed by at least three members of the program committee. This period of independent review was followed by discussion among the reviewers, and when necessary we requested additional reviews from other experts. Twenty papers were selected to appear in the program as full papers (10%), and another thirty-two were selected to appear in the program as poster papers (16%). The Industry Track received thirty-four submissions, from which eleven (32%) were selected. In addition, the Program Committee referred three research papers to the Industry Track, of which one was selected. For the Industry Track, papers were selected because they presented useful knowledge for practitioners, or because they bridged a gap between industry and research.The program for KDD-2001 also included three keynote lectures, five invited talks by well-known practitioners (as part of the Industry Track), and three panel discussions on topics of current interest. There were six tutorials, geared both for novices and for experts, plus six specialized workshops on cutting-edge research issues. The 2001 KDDCUP competition focused on problems of bioinformatics and drug design. And, finally, the program included dozens of exhibits of products from vendors and from research projects.

#index 342584
#* Challenges for knowledge discovery in biology
#@ Russ Altman
#t 2001
#c 0
#! Bioinformatics is the study of information flow in biology. Interest in the field has exploded in the last 10 years with the emergence of techniques for large scale experimental data collection-including genome sequencing, gene expression analysis, protein interaction detection, high-throughput structure determination and others. These techniques, in the context of a large online published literature, have created relatively large data sets (at least by biological standards) that are not possible to analyze manually. There is therefore a critical need for methods to analyze these data and reduce them to new knowledge. The principle challenges to the field include the great diversity of data types and questions that are asked of the data, and the communication difficulties that can exist between experts in biology and experts in machine learning. In this talk, I will provide an introduction to the major biological questions that are being addressed, why they are important, and how the field is trying to address them with technical approaches.

#index 342585
#* Extracting targeted data from the web
#@ Tom Mitchell
#t 2001
#c 0
#! Tom M. Mitchell is author of the textbook "Machine Learning" (McGraw Hill, 1997), President of the American Association for Artificial Intelligence and a member of the National Research Council's Computer Science and Telecommunications Board. He is Vice President and Chief Scientist at WhizBang Labs and is currently on a two-year leave of absence from Carnegie Mellon University where he is the Fredkin Professor of Learning and AI in the School of Computer Science and founding Director of CMU's Center for Automated Learning and Discovery. Mitchell's research interests span many areas of Machine Learning theory and practice. His current work at WhizBang Labs involves developing machine learning methods for extracting information from text. For example, WhizBang has developed the world's largest database of job openings by training its software to automatically locate and extract detailed information from job postings on corporate web sites (see www.flipdog.com).

#index 342586
#* Mass collaboration and data mining
#@ Raghu Ramakrishnan
#t 2001
#c 0
#! Mass Collaboration is a new "P2P"-style approach to large-scale knowledge sharing, with applications in customer support, focused community development, and capturing knowledge distributed within large organizations. Effectively supporting this paradigm raises many technical challenges, and offers intriguing opportunities for mining massive amounts of data captured continually from user interactions. Data mining offers the promise of increased business intelligence, and also improved user experiences, leading to increased participation and greater quality in the knowledge that is captured, both of which are central objectives in Mass Collaboration. In this talk, I will introduce Mass Collaboration and discuss some important data mining related issues.

#index 342587
#* Applications of generalized support vector machines to predictive modeling
#@ Nitin Agrarwal
#t 2001
#c 0
#! The work of the Russian mathematician Vladimir Vapnik (AT&T Labs) enables us to go back to the roots of theoretical statistics, leaving behind Fisher's parameters in favor of the general approaches started in the 1930s by Glivenko-Cantelli-Kolmogorov. Nowadays, it has become possible to model millions of events described by thousands of variables, within a reasonable time for a specific application. The SRM approach works with a family of models and calibrates the family of models to a point which is the best compromise between accuracy and robustness. It also measures the complexity of the model using VC dimension which is not plagued by number of parameters. Hence models for large events described by several parameters can be generalized. This opens up great prospects in numerous fields like Customer Relationship Management, Network Optimization, Risk Management, Manufacturing Yield Management, and a number of other data-rich problems.

#index 342588
#* Data mining: are we there yet?
#@ Herb Edelstein
#t 2001
#c 0
#! Data mining started its move out of the statistics and machine learning ghettos and into the mainstream almost 10 years ago. With great fanfare and a large influx of venture capital, data mining was going to change the very nature of business. Yet data mining products have had relatively modest success in the marketplace. The reasons include limitations and misplaced emphasis in the products' features and functions, unrealistic expectations set by messages from the data mining community, and a lack of readiness by many prospective users. This session will look at where vendors have succeeded and failed with their products, what expectations users should have, and suggestions for achieving the potential of this exciting and valuable technology.

#index 342589
#* Mining e-commerce data: the good, the bad, and the ugly
#@ Ron Kohavi
#t 2001
#c 0
#% 186340
#% 332648
#% 338609
#% 420116
#% 711323
#! Organizations conducting Electronic Commerce (e-commerce) can greatly benefit from the insight that data mining of transactional and clickstream data provides. Such insight helps not only to improve the electronic channel (e.g., a web site), but it is also a learning vehicle for the bigger organization conducting business at brick-and-mortar stores. The e-commerce site serves as an early alert system for emerging patterns and a laboratory for experimentation. For successful data mining, several ingredients are needed and e-commerce provides all the right ones (the Good). Web server logs, which are commonly used as the source of data for mining e-commerce data, were designed to debug web servers, and the data they provide is insufficient, requiring the use of heuristics to reconstruct events. Moreover, many events are never logged in web server logs, limiting the source of data for mining (the Bad). Many of the problems of dealing with web server log data can be resolved by properly architecting the e-commerce sites to generate data needed for mining. Even with a good architecture, however, there are challenging problems that remain hard to solve (the Ugly). Lessons and metrics based on mining real e-commerce data are presented.

#index 342590
#* Data mining platform for database developers
#@ Amir Netz
#t 2001
#c 0

#index 342591
#* Recommender systems in commerce and community
#@ John Riedl
#t 2001
#c 0
#! Recommender systems have been revolutionizing the way shoppers and information seekers find what they want. We will study some of the tremendous successes and spectacular failures of recommenders in E-commerce to understand the causes of the success or failure. We will leverage that understanding into a set of principles for successfully applying recommenders to business problems. Finally, we will study the economic and social forces that are shaping the evolution of recommenders, and peer into the crystal ball to glimpse the directions the technology will be going in the future.

#index 342592
#* The "DGX" distribution for mining massive, skewed data
#@ Zhiqiang Bi;Christos Faloutsos;Flip Korn
#t 2001
#c 0
#% 176500
#% 340296
#% 480948
#% 642534
#! Skewed distributions appear very often in practice. Unfortunately, the traditional Zipf distribution often fails to model them well. In this paper, we propose a new probability distribution, the Discrete Gaussian Exponential (DGX), to achieve excellent fits in a wide variety of settings; our new distribution includes the Zipf distribution as a special case. We present a statistically sound method for estimating the DGX parameters based on maximum likelihood estimation (MLE). We applied DGX to a wide variety of real world data sets, such as sales data from a large retailer chain, us-age data from AT&T, and Internet clickstream data; in all cases, DGX fits these distributions very well, with almost a 99% correlation coefficient in quantile-quantile plots. Our algorithm also scales very well because it requires only a single pass over the data. Finally, we illustrate the power of DGX as a new tool for data mining tasks, such as outlier detection.

#index 342593
#* Data mining criteria for tree-based regression and classification
#@ Andreas Buja;Yung-Seop Lee
#t 2001
#c 0
#% 136350
#% 208180
#% 283138
#! This paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees. To this end, we propose new splitting criteria for growing trees. Conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit between the left and the right side. By contrast, we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data, as opposed to modeling all of the data equally well. The new criteria do not split based on a compromise between the left and the right bucket; they effectively pick the more interesting bucket and ignore the other.As expected, the result is often a simpler characterization of interesting subsets of the data. Less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions. Surprisingly, it is a "flaw" that works to their advantage: The new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges. This so-called "end-cut problem" leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees.

#index 342594
#* Probabilistic modeling of transaction data with applications to profiling, visualization, and prediction
#@ Igor V. Cadez;Padhraic Smyth;Heikki Mannila
#t 2001
#c 0
#% 32361
#% 152934
#% 280819
#% 310548
#% 420117
#% 722754
#% 1650569
#! Transaction data is ubiquitous in data mining applications. Examples include market basket data in retail commerce, telephone call records in telecommunications, and Web logs of individual page-requests at Web sites. Profiling consists of using historical transaction data on individuals to construct a model of each individual's behavior. Simple profiling techniques such as histograms do not generalize well from sparse transaction data. In this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data. In effect, the mixture model represents each individual's behavior as a linear combination of "basis transactions." We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques, as well as being relatively scalable, interpretable, and flexible. In addition we point to applications in outlier detection, customer ranking, interactive visualization, and so forth. The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules.

#index 342595
#* GESS: a scalable similarity-join algorithm for mining large data sets in high dimensional spaces
#@ Jens-Peter Dittrich;Bernhard Seeger
#t 2001
#c 0
#% 13041
#% 68091
#% 201968
#% 210186
#% 210187
#% 227932
#% 237187
#% 251459
#% 252608
#% 300160
#% 301163
#% 314740
#% 316536
#% 443326
#% 462070
#% 462236
#% 465000
#% 479649
#% 480825
#% 526847
#% 632105
#! The similarity join is an important operation for mining high-dimensional feature spaces. Given two data sets, the similarity join computes all tuples (x, y) that are within a distance &egr;.One of the most efficient algorithms for processing similarity-joins is the Multidimensional-Spatial Join (MSJ) by Koudas and Sevcik. In our previous work --- pursued for the two-dimensional case --- we found however that MSJ has several performance shortcomings in terms of CPU and I/O cost as well as memory-requirements. Therefore, MSJ is not generally applicable to high-dimensional data.In this paper, we propose a new algorithm named Generic External Space Sweep (GESS). GESS introduces a modest rate of data replication to reduce the number of expensive distance computations. We present a new cost-model for replication, an I/O model, and an inexpensive method for duplicate removal. The principal component of our algorithm is a highly flexible replication engine.Our analytical model predicts a tremendous reduction of the number of expensive distance computations by several orders of magnitude in comparison to MSJ (factor 107). In addition, the memory requirements of GESS are shown to be lower by several orders of magnitude. Furthermore, the I/O cost of our algorithm is by factor 2 better (independent from the fact whether replication occurs or not). Our analytical results are confirmed by a large series of simulations and experiments with synthetic and real high-dimensional data sets.

#index 342596
#* Mining the network value of customers
#@ Pedro Domingos;Matt Richardson
#t 2001
#c 0
#% 146494
#% 173879
#% 220707
#% 220708
#% 220710
#% 246831
#% 248810
#% 262317
#% 268079
#% 280422
#% 280442
#% 280852
#% 282905
#% 310514
#% 420059
#% 420110
#% 445369
#% 479969
#% 496116
#% 528168
#% 529654
#% 722754
#% 1650569
#! One of the major applications of data mining is in helping companies determine which potential customers to market to. If the expected profit from a customer is greater than the cost of marketing to her, the marketing action for that customer is executed. So far, work in this area has considered only the intrinsic value of the customer (i.e, the expected profit from sales to her). We propose to model also the customer's network value: the expected profit from sales to other customers she may influence to buy, the customers those may influence, and so on recursively. Instead of viewing a market as a set of independent entities, we view it as a social network and model it as a Markov random field. We show the advantages of this approach using a social network mined from a collaborative filtering database. Marketing that exploits the network value of customers---also known as viral marketing---can be extremely effective, but is still a black art. Our work can be viewed as a step towards providing a more solid foundation for it, taking advantage of the availability of large relevant databases.

#index 342597
#* Empirical bayes screening for multi-item associations
#@ William DuMouchel;Daryl Pregibon
#t 2001
#c 0
#% 152934
#% 227917
#% 248012
#% 280402
#% 420073
#% 481290
#! This paper considers the framework of the so-called "market basket problem", in which a database of transactions is mined for the occurrence of unusually frequent item sets. In our case, "unusually frequent" involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently. The focus is on obtaining reliable estimates of this measure of interestingness for all item sets, even item sets with relatively low frequencies. For example, in a medical database of patient histories, unusual item sets including the item "patient death" (or other serious adverse event) might hopefully be flagged with as few as 5 or 10 occurrences of the item set, it being unacceptable to require that item sets occur in as many as 0.1% of millions of patient reports before the data mining algorithm detects a signal. Similar considerations apply in fraud detection applications. Thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support, and adopt a criterion based on the results of fitting an empirical Bayes model to the item set counts. The model allows us to define a 95% Bayesian lower confidence limit for the "interestingness" measure of every item set, whereupon the item sets can be ranked according to their empirical Bayes confidence limits. For item sets of size J 2, we also distinguish between multi-item associations that can be explained by the observed J(J-1)/2 pairwise associations, and item sets that are significantly more frequent than their pairwise associations would suggest. Such item sets can uncover complex or synergistic mechanisms generating multi-item associations. This methodology has been applied within the U.S. Food and Drug Administration (FDA) to databases of adverse drug reaction reports and within AT&T to customer international calling histories. We also present graphical techniques for exploring and understanding the modeling results.

#index 342598
#* Proximal support vector machine classifiers
#@ Glenn Fung;Olvi L. Mangasarian
#t 2001
#c 0
#% 31429
#% 190581
#% 224113
#% 269217
#% 269218
#% 290137
#% 292664
#% 310498
#% 376266
#% 378173
#% 384950
#% 390723
#% 420077
#% 722758
#% 1860545
#! Instead of a standard support vector machine (SVM) that classifies points by assigning them to one of two disjoint half-spaces, points are classified by assigning them to the closest of two parallel planes (in input or feature space) that are pushed apart as far as possible. This formulation, which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks [8, 9], leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations. In contrast, standard SVMs solve a quadratic or a linear program that require considerably longer computational time. Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard SVM classifiers, but with considerably faster computational time that can be an order of magnitude faster. The linear proximal SVM can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds. All computational results are based on 6 lines of MATLAB code.

#index 342599
#* Data mining with sparse grids using simplicial basis functions
#@ Jochen Garcke;Michael Griebel
#t 2001
#c 0
#% 185955
#% 188197
#% 190581
#% 213119
#% 259314
#% 266882
#% 267537
#% 267542
#% 272995
#% 294977
#% 328856
#% 378173
#% 384950
#% 420065
#% 544389
#% 837668
#! Recently we presented a new approach [18] to the classification problem arising in data mining. It is based on the regularization network approach but, in contrast to other methods which employ ansatz functions associated to data points, we use a grid in the usually high-dimensional feature space for the minimization process. To cope with the curse of dimensionality, we employ sparse grids [49]. Thus, only O(hn-1nd-1) instead of O(hn-d) grid points and unknowns are involved. Here d denotes the dimension of the feature space and hn = 2-n gives the mesh size. We use the sparse grid combination technique [28] where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension. The sparse grid solution is then obtained by linear combination. In contrast to our former work, where d-linear functions were used, we now apply linear basis functions based on a simplicial discretization. This allows to handle more dimensions and the algorithm needs less operations per data point.We describe the sparse grid combination technique for the classification problem, give implementational details and discuss the complexity of the algorithm. It turns out that the method scales linearly with the number of given data points. Finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions. It turns out that our new method achieves correctness rates which are competitive to that of the best existing methods.

#index 342600
#* Mining time-changing data streams
#@ Geoff Hulten;Laurie Spencer;Pedro Domingos
#t 2001
#c 0
#% 136350
#% 204531
#% 273900
#% 280413
#% 280467
#% 280498
#% 302394
#% 310500
#% 425001
#% 459008
#% 464204
#% 479785
#% 481945
#% 589303
#% 632036
#% 963888
#! Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.

#index 342601
#* Visualizing multi-dimensional clusters, trends, and outliers using star coordinates
#@ Eser Kandogan
#t 2001
#c 0
#% 28144
#% 86300
#% 172812
#% 216500
#% 333582
#% 435913
#% 436116
#% 441058
#% 641072
#% 641103
#% 726071
#% 726258
#! Interactive visualizations are effective tools in mining scientific, engineering, and business data to support decision-making activities. Star Coordinates is proposed as a new multi-dimensional visualization technique, which supports various interactions to stimulate visual thinking in early stages of knowledge discovery process. In Star Coordinates, coordinate axes are arranged on a two-dimensional surface, where each axis shares the same origin point. Each multi-dimensional data element is represented by a point, where each attribute of the data contributes to its location through uniform encoding. Interaction features of Star Coordinates provide users the ability to apply various transformations dynamically, integrate and separate dimensions, analyze correlations of multiple dimensions, view clusters, trends, and outliers in the distribution of data, and query points based on data ranges. Our experience with Star Coordinates shows that it is particularly useful for the discovery of hierarchical clusters, and analysis of multiple factors providing insight in various real datasets including telecommunications churn.

#index 342602
#* Ensemble-index: a new approach to indexing large databases
#@ Eamonn Keogh;Selina Chu;Michael Pazzani
#t 2001
#c 0
#% 144076
#% 172949
#% 214595
#% 227924
#% 237204
#% 248798
#% 273704
#% 316526
#% 316559
#% 316560
#% 333941
#% 427199
#% 460862
#% 464851
#% 480146
#% 480307
#% 592279
#% 617886
#% 631920
#% 631923
#! The problem of similarity search (query-by-content) has attracted much research interest. It is a difficult problem because of the inherently high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier Transform (DFT), the Discrete Wavelet Transform (DWT) and Piecewise Polynomial Approximation. In this work, we introduce a novel framework for using ensembles of two or more representations for more efficient indexing. The basic idea is that instead of committing to a single representation for an entire dataset, different representations are chosen for indexing different parts of the database. The representations are chosen based upon a local view of the database. For example, sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets, but highly spectral sections of the data are indexed using the Fourier transform. At query time, it is necessary to search several small heterogeneous indices, rather than one large homogeneous index. As we will theoretically and empirically demonstrate this results in much faster query response times.

#index 342603
#* Robust space transformations for distance-based operations
#@ Edwin M. Knorr;Raymond T. Ng;Ruben H. Zamar
#t 2001
#c 0
#% 34077
#% 86950
#% 169940
#% 227937
#% 248792
#% 280452
#% 300136
#% 300183
#% 406493
#% 427199
#% 479791
#% 481281
#% 482109
#% 566128
#! For many KDD operations, such as nearest neighbor search, distance-based clustering, and outlier detection, there is an underlying &kgr;-D data space in which each tuple/object is represented as a point in the space. In the presence of differing scales, variability, correlation, and/or outliers, we may get unintuitive results if an inappropriate space is used.The fundamental question that this paper addresses is: "What then is an appropriate space?" We propose using a robust space transformation called the Donoho-Stahel estimator. In the first half of the paper, we show the key properties of the estimator. Of particular importance to KDD applications involving databases is the stability property, which says that in spite of frequent updates, the estimator does not: (a) change much, (b) lose its usefulness, or (c) require re-computation. In the second half, we focus on the computation of the estimator for high-dimensional databases. We develop randomized algorithms and evaluate how well they perform empirically. The novel algorithm we develop called the Hybrid-random algorithm is, in most cases, at least an order of magnitude faster than the Fixed-angle and Subsampling algorithms.

#index 342604
#* Molecular feature mining in HIV data
#@ Stefan Kramer;Luc De Raedt;Christoph Helma
#t 2001
#c 0
#% 109952
#% 152934
#% 178515
#% 216508
#% 248791
#% 307109
#% 420062
#% 420076
#% 420087
#% 420088
#% 438134
#% 464289
#% 464714
#% 496247
#% 1289265
#! We present the application of Feature Mining techniques to the Developmental Therapeutics Program's AIDS antiviral screen database. The database consists of 43576 compounds, which were measured for their capability to protect human cells from HIV-1 infection. According to these measurements, the compounds were classified as either active, moderately active or inactive. The distribution of classes is extremely skewed: Only 1.3 % of the molecules is known to be active, and 2.7 % is known to be moderately active.Given this database, we were interested in molecular substructures (i.e., features) that are frequent in the active molecules, and infrequent in the inactives. In data mining terms, we focused on features with a minimum support in active compounds and a maximum support in inactive compounds. We analyzed the database using the levelwise version space algorithm that forms the basis of the inductive query and database system MOLFEA (Molecular Feature Miner). Within this framework, it is possible to declaratively specify the features of interest, such as the frequency of features on (possibly different) datasets as well as on the generality and syntax of them. Assuming that the detected substructures are causally related to biochemical mechanisms, it should be possible to facilitate the development of new pharmaceuticals with improved activities.

#index 342605
#* Discovering unexpected information from your competitors' web sites
#@ Bing Liu;Yiming Ma;Philip S. Yu
#t 2001
#c 0
#% 46803
#% 244103
#% 248791
#% 248801
#% 249110
#% 261741
#% 268068
#% 268079
#% 281209
#% 281218
#% 281251
#% 282905
#% 303113
#% 310496
#% 310518
#% 312861
#% 387427
#% 406493
#% 443092
#% 481290
#% 1499588
#! Ever since the beginning of the Web, finding useful information from the Web has been an important problem. Existing approaches include keyword-based search, wrapper-based information extraction, Web query and user preferences. These approaches essentially find information that matches the user's explicit specifications. This paper argues that this is insufficient. There is another type of information that is also of great interest, i.e., unexpected information, which is unanticipated by the user. Finding unexpected information is useful in many applications. For example, it is useful for a company to find unexpected information bout its competitors, e.g., unexpected services and products that its competitors offer. With this information, the company can learn from its competitors and/or design counter measures to improve its competitiveness. Since the number of pages of a typical commercial site is very large and there are also many relevant sites (competitors), it is very difficult for a human user to view each page to discover the unexpected information. Automated assistance is needed. In this paper, we propose a number of methods to help the user find various types of unexpected information from his/her competitors' Web sites. Experiment results show that these techniques are very useful in practice and also efficient.

#index 342606
#* Personalization from incomplete data: what you don't know can hurt
#@ Balaji Padmanabhan;Zhiqiang Zheng;Steven O. Kimbrough
#t 2001
#c 0
#% 246087
#% 266283
#% 268184
#% 268197
#% 280500
#% 310490
#% 314938
#% 355470
#% 462238
#% 1273676
#! Clickstream data collected at any web site (site-centric data) is inherently incomplete, since it does not capture users' browsing behavior across sites (user-centric data). Hence, models learned from such data may be subject to limitations, the nature of which has not been well studied. Understanding the limitations is particularly important since most current personalization techniques are based on site-centric data only. In this paper, we empirically examine the implications of learning from incomplete data in the context of two specific problems: (a) predicting if the remainder of any given session will result in a purchase and (b) predicting if a given user will make a purchase at any future session. For each of these problems we present new algorithms for fast and accurate data preprocessing of clickstream data. Based on a comprehensive experiment on user-level clickstream data gathered from 20,000 users' browsing behavior, we demonstrate that models built on user-centric data outperform models built on site-centric data for both prediction tasks.

#index 342607
#* Probabilistic query models for transaction data
#@ Dmitry Pavlov;Padhraic Smyth
#t 2001
#c 0
#% 43163
#% 44876
#% 82346
#% 152934
#% 211044
#% 248812
#% 248822
#% 269187
#% 280448
#% 280494
#% 480306
#% 481290
#% 482092
#% 528023
#% 1272326
#% 1476311
#% 1650705
#! We investigate the application of Bayesian networks, Markov random fields, and mixture models to the problem of query answering for transaction data sets. We formulate two versions of the querying problem: the query selectivity estimation (i.e., finding exact counts for tuples in a data set) and the query generalization problem (i.e., computing the probability that a tuple will occur in new data). We show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an ADTree data structure. In an extension of our earlier work on this topic we propose several new schemes for query answering based on the compressed representation that avoid direct scans of the data at query time. Experimental results on real-world transaction data sets provide insights into various tradeoffs involving the offline time for model-building, the online time for query-answering, the memory footprint of the compressed data, and the accuracy of the estimate provided to the query.

#index 342608
#* Extracting collective probabilistic forecasts from web games
#@ David M. Pennock;Steve Lawrence;Finn Årup Nielsen;C. Lee Giles
#t 2001
#c 0
#% 220706
#% 268722
#% 310514
#% 310533
#% 310546
#% 310567
#% 420091
#% 739019
#! Game sites on the World Wide Web draw people from around the world with specialized interests, skills, and knowledge. Data from the games often reflects the players' expertise and will to win. We extract probabilistic forecasts from data obtained from three online games: the Hollywood Stock Exchange (HSX), the Foresight Exchange (FX), and the Formula One Pick Six (F1P6) competition. We find that all three yield accurate forecasts of uncertain future events. In particular, prices of so-called "movie stocks" on HSX are good indicators of actual box office returns. Prices of HSX securities in Oscar, Emmy, and Grammy awards correlate well with observed frequencies of winning. FX prices are reliable indicators of future developments in science and technology. Collective predictions from players in the F1 competition serve as good forecasts of true race outcomes. In some cases, forecasts induced from game data are more reliable than expert opinions. We argue that web games naturally attract well-informed and well-motivated players, and thus offer a valuable and oft-overlooked source of high-quality data with significant predictive value.

#index 342609
#* Tri-plots: scalable tools for multidimensional data mining
#@ Agma Traina;Caetano Traina;Spiros Papadimitriou;Christos Faloutsos
#t 2001
#c 0
#% 210173
#% 237187
#% 248796
#% 300160
#% 310537
#% 438133
#% 438134
#% 443082
#% 443083
#% 479649
#% 479799
#% 481281
#% 481620
#% 527160
#% 631970
#% 632043
#! We focus on the problem of finding patterns across two large, multidimensional datasets. For example, given feature vectors of healthy and of non-healthy patients, we want to answer the following questions: Are the two clouds of points separable? What is the smallest/largest pair-wise distance across the two datasets? Which of the two clouds does a new point (feature vector) come from?We propose a new tool, the tri-plot, and its generalization, the pq-plot, which help us answer the above questions. We provide a set of rules on how to interpret a tri-plot, and we apply these rules on synthetic and real datasets. We also show how to use our tool for classification, when traditional methods (nearest neighbor, classification trees) may fail.

#index 342610
#* Efficient discovery of error-tolerant frequent itemsets in high dimensions
#@ Cheng Yang;Usama Fayyad;Paul S. Bradley
#t 2001
#c 0
#% 78695
#% 152934
#% 173879
#% 210160
#% 210173
#% 220706
#% 232117
#% 232136
#% 248790
#% 248791
#% 248792
#% 280419
#% 280448
#% 280452
#% 464714
#% 479659
#% 481281
#% 481290
#% 631985
#% 979690
#! We present a generalization of frequent itemsets allowing for the notion of errors in the itemset definition. We motivate the problem and present an efficient algorithm that identifies error-tolerant frequent clusters of items in transactional data (customer-purchase data, web browsing data, text, etc.). The algorithm exploits sparseness of the underlying data to find large groups of items that are correlated over database records (rows). The notion of transaction coverage allows us to extend the algorithm and view it as a fast clustering algorithm for discovering segments of similar transactions in binary sparse data. We evaluate the new algorithm on three real-world applications: clustering high-dimensional data, query selectivity estimation and collaborative filtering. Results show that the algorithm consistently uncovers structure in large sparse databases that other traditional clustering algorithms fail to find.

#index 342611
#* Learning and making decisions when costs and probabilities are both unknown
#@ Bianca Zadrozny;Charles Elkan
#t 2001
#c 0
#% 136350
#% 209021
#% 227510
#% 280437
#% 331909
#% 424997
#% 458250
#% 458361
#% 464280
#% 466568
#% 1289281
#! In many data mining domains, misclassification costs are different for different examples, in the same way that class membership probabilities are example-dependent. In these domains, both costs and probabilities are unknown for test examples, so both cost estimators and probability estimators must be learned. After discussing how to make optimal decisions given cost and probability estimates, we present decision tree and naive Bayesian learning methods for obtaining well-calibrated probability estimates. We then explain how to obtain unbiased estimators for example-dependent costs, taking into account the difficulty that in general, probabilities and costs are not independent random variables, and the training examples for which costs are known are not representative of all examples. The latter problem is called sample selection bias in econometrics. Our solution to it is based on Nobel prize-winning work due to the economist James Heckman. We show that the methods we propose perform better than MetaCost and all other known methods, in a comprehensive experimental comparison that uses the well-known, large, and challenging dataset from the KDD'98 data mining contest.

#index 342612
#* Data mining case study: modeling the behavior of offenders who commit serious sexual assaults
#@ Richard Adderley;Peter B. Musgrove
#t 2001
#c 0
#% 60576
#% 252533
#! This paper looks at the use of a Self Organizing Map (SOM), to link of records of crimes of serious sexual attacks. Once linked a profile can be derived of the offender(s) responsible.The data was drawn from the major crimes database at the National Crime Faculty of the National Police Staff College Bramshill UK. The data was encoded from text by a small team of specialists working to a well-defined protocol. The encoded data was analyzed using SOMs. Two exercises were conducted. These resulted in the linking of several offences in to clusters each of which were sufficiently similar to have possibly been committed by the same offender(s). A number of clusters were used to form profiles of offenders. Some of these profiles were confirmed by independent analysts as either belonging to known offenders or appeared sufficiently interesting to warrant further investigation.The prototype was developed over 10 weeks. This contrasts with an in-house study using a conventional approach, which took 2 years to reach similar results. As a consequence of this study the NCF intends to pursue an in-depth follow up study.

#index 342613
#* A human-computer cooperative system for effective high dimensional clustering
#@ Charu C. Aggarwal
#t 2001
#c 0
#% 210160
#% 248792
#% 273891
#% 300131
#% 332094
#% 436509
#! High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Therefore, techniques have recently been proposed to find clusters in hidden subspaces of the data. However, since the behavior of the data may vary considerably in different subspaces, it is often difficult to define the notion of a cluster with the use of simple mathematical formalizations. In fact, the meaningfulness and definition of a cluster is best characterized with the use of human intuition. In this paper, we propose a system which performs high dimensional clustering by effective cooperation between the human and the computer. The complex task of cluster creation is accomplished by a combination of human intuition and the computational support provided by the computer. The result is a system which leverages the best abilities of both the human and the computer in order to create very meaningful sets of clusters in high dimensionality.

#index 342614
#* Mining massively incomplete data sets by conceptual reconstruction
#@ Charu C. Aggarwal;Srinivasan Parthasarathy
#t 2001
#c 0
#% 17144
#% 136350
#% 248027
#% 248798
#% 300184
#% 668895
#! Incomplete data sets have become almost ubiquitous in a wide variety of application domains. Common examples can be found in climate and image data sets, sensor data sets and medical data sets. The incompleteness in these data sets may arise from a number of factors: in some cases it may simply be a reflection of certain measurements not being available at the time; in others the information may be lost due to partial system failure; or it may simply be a result of users being unwilling to specify attributes due to privacy concerns. When a significant fraction of the entries are missing in all of the attributes, it becomes very difficult to perform any kind of reasonable extrapolation on the original data. For such cases, we introduce the novel idea of conceptual reconstruction, in which we create effective conceptual representations on which the data mining algorithms can be directly applied. The attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in terms of concepts rather the original dimensions. As a result, the reconstruction procedure estimates only those conceptual aspects of the data which can be mined from the incomplete data set, rather than force errors created by extrapolation. We demonstrate the effectiveness of the approach on a variety of real data sets.

#index 342615
#* Evaluating the novelty of text-mined rules using lexical knowledge
#@ Sugato Basu;Raymond J. Mooney;Krupakar V. Pasupuleti;Joydeep Ghosh
#t 2001
#c 0
#% 152968
#% 172386
#% 279755
#% 280436
#% 280485
#% 316709
#% 443092
#% 443313
#% 481588
#% 529678
#% 786497
#% 1275285
#% 1289282
#! In this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet, a lexical knowledge-base of English words. We assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. The novelty of rules extracted by the DiscoTEX text-mining system on Amazon.com book descriptions were evaluated by both human subjects and by our algorithm. By computing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another. @Text mining

#index 342616
#* Fast ordering of large categorical datasets for better visualization
#@ Alina Beygelzimer;Chang-Shing Perng;Sheng Ma
#t 2001
#c 0
#% 183248
#% 201893
#% 274623
#% 282481
#% 310516
#% 529288
#% 641057
#% 727085
#! An important issue in visualizing categorical data is how to order categorical values. The focus of this paper is on constructing such orderings efficiently without compromising their visual quality.

#index 342617
#* Random projection in dimensionality reduction: applications to image and text data
#@ Ella Bingham;Heikki Mannila
#t 2001
#c 0
#% 41374
#% 85279
#% 232764
#% 248027
#% 249321
#% 272536
#% 273920
#% 333881
#% 406493
#% 435222
#% 460862
#% 527853
#% 593839
#% 593926
#% 593928
#% 594011
#! Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.

#index 342618
#* Gaining insights into support vector machine pattern classifiers using projection-based tour methods
#@ Doina Caragea;Dianne Cook;Vasant G. Honavar
#t 2001
#c 0
#% 1211
#% 190581
#% 197394
#% 445243
#! This paper discusses visual methods that can be used to understand and interpret the results of classification using support vector machines (SVM) on data with continuous real-valued variables. SVM induction algorithms build pattern classifiers by identifying a maximal margin separating hyperplane from training examples in high dimensional pattern spaces or spaces induced by suitable nonlinear kernel transformations over pattern spaces. SVM have been demonstrated to be quite effective in a number of practical pattern classification tasks. Since the separating hyperplane is defined in terms of more than two variables it is necessary to use visual techniques that can navigate the viewer through high-dimensional spaces. We demonstrate the use of projection-based tour methods to gain useful insights into SVM classifiers with linear kernels on 8-dimensional data.

#index 342619
#* PVA: a self-adaptive personal view agent system
#@ Chien Chin Chen;Meng Chang Chen;Yeali Sun
#t 2001
#c 0
#% 67565
#% 234793
#% 252753
#% 252836
#% 262090
#% 271082
#% 290149
#% 309123
#% 376266
#% 443506
#% 637576
#! In this paper, we present PVA, an adaptive personal view information agent system to track, learn and manage, user's interests in Internet documents. When user's interests change, PVA, in not only the contents, but also in the structure of user profile, is modified to adapt to the changes. Experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems.

#index 342620
#* A robust and scalable clustering algorithm for mixed type attributes in large database environment
#@ Tom Chiu;DongPing Fang;John Chen;Yao Wang;Christopher Jeris
#t 2001
#c 0
#% 280419
#% 420057
#% 420081
#! Clustering is a widely used technique in data mining applications to discover patterns in the underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either continuous or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining problems. In this paper, we propose a distance measure that enables clustering data with both continuous and categorical attributes. This distance measure is derived from a probabilistic model that the distance between two clusters is equivalent to the decrease in log-likelihood function as a result of merging. Calculation of this measure is memory efficient as it depends only on the merging cluster pair and not on all the other clusters. Zhang et al [8] proposed a clustering method named BIRCH that is especially suitable for very large datasets. We develop a clustering algorithm using our distance measure based on the framework of BIRCH. Similar to BIRCH, our algorithm first performs a pre-clustering step by scanning the entire dataset and storing the dense regions of data records in terms of summary statistics. A hierarchical clustering algorithm is then applied to cluster the dense regions. Apart from the ability of handling mixed type of attributes, our algorithm differs from BIRCH in that we add a procedure that enables the algorithm to automatically determine the appropriate number of clusters and a new strategy of assigning cluster membership to noisy data. For data with mixed type of attributes, our experimental results confirm that the algorithm not only generates better quality clusters than the traditional k-means algorithms, but also exhibits good scalability properties and is able to identify the underlying number of clusters in the data correctly. The algorithm is implemented in the commercial data mining tool Clementine 6.0 which supports the PMML standard of data mining model deployment.

#index 342621
#* Co-clustering documents and words using bipartite spectral graph partitioning
#@ Inderjit S. Dhillon
#t 2001
#c 0
#% 11646
#% 54433
#% 74120
#% 118771
#% 232655
#% 234978
#% 262059
#% 304423
#% 313959
#% 329562
#% 375017
#% 406493
#% 408396
#% 729437
#! Both document clustering and word clustering are well studied problems. Most existing algorithms cluster documents and words separately but not simultaneously. In this paper we present the novel idea of modeling the document collection as a bipartite graph between documents and words, using which the simultaneous clustering problem can be posed as a bipartite graph partitioning problem. To solve the partitioning problem, we use a new spectral co-clustering algorithm that uses the second left and right singular vectors of an appropriately scaled word-document matrix to yield good bipartitionings. The spectral algorithm enjoys some optimality properties; it can be shown that the singular vectors solve a real relaxation to the NP-complete graph bipartitioning problem. We present experimental results to verify that the resulting co-clustering algorithm works well in practice.

#index 342622
#* A spectral method to separate disconnected and nearly-disconnected web graph components
#@ Chris H. Q. Ding;Xiaofeng He;Hongyuan Zha
#t 2001
#c 0
#% 27237
#% 70370
#% 74120
#% 109452
#% 162955
#% 214673
#% 252478
#% 262061
#% 290830
#% 309749
#% 310514
#% 313959
#% 433672
#! Separation of connected components from a graph with disconnected graph components mostly use breadth-first search (BFS) or depth-first search (DFS) graph algorithms. Here we propose a new algebraic method to separate disconnected and nearly-disconnected components. This method is based on spectral graph partitioning, following a key observation that disconnected components will show up, after properly sorted, as step-function like curve in the lowest eigenvectors of the Laplacian matrix of the graph. Following an perturbative analysis framework, we systematically analyzed the graph structures, first on the disconnected subgraph case, and second on the effects of adding edges sparsely connecting different subgraphs as a perturbation. Several new results are derived, providing insights to spectral methods and related clustering objective function. Examples are given illustrating the concepts and results our methods. Comparing to the standard graph algorithms, this method has the same O(&Verbar;E &Verbar; + &Verbar;V&Verbar;log(&Verbar;V&Verbar;)) complexity, but is easier to implement (using readily available eigensolvers). Further more the method can easily identify articulation points and bridges on nearly-disconnected graphs. Segmentation of a real example of Web graph for query amazon is given. We found that each disconnected or nearly-disconnected components forms a cluster on a clear topic.

#index 342623
#* Clustering spatial data using random walks
#@ David Harel;Yehuda Koren
#t 2001
#c 0
#% 36672
#% 212873
#% 438137
#% 683389
#! Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task. A common approach to this problem is to use cluster analysis. We propose a novel approach to clustering, based on the deterministic analysis of random walks on a weighted graph generated from the data. Our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities, overcoming noise and outliers that may blur the natural decomposition of the data. The method requires only O(n log n) time, and one of its variants needs only constant space.

#index 342624
#* Solving regression problems with rule-based ensemble classifiers
#@ Nitin Indurkhya;Sholom M. Weiss
#t 2001
#c 0
#% 209021
#% 283138
#% 424997
#% 465746
#% 466744
#% 1272177
#! We describe a lightweight learning method that induces an ensemble of decision-rule solutions for regression problems. Instead of direct prediction of a continuous output variable, the method discretizes the variable by k-means clustering and solves the resultant classification problem. Predictions on new examples are made by averaging the mean values of classes with votes that are close in number to the most likely class. We provide experimental evidence that this indirect approach can often yield strong results for many applications, generally outperforming direct approaches such as regression trees and rivaling bagged regression trees.

#index 342625
#* Mining top-n local outliers in large databases
#@ Wen Jin;Anthony K. H. Tung;Jiawei Han
#t 2001
#c 0
#% 210173
#% 248790
#% 300136
#% 300183
#% 479791
#% 481281
#% 481956
#! Outlier detection is an important task in data mining with numerous applications, including credit card fraud detection, video surveillance, etc. A recent work on outlier detection has introduced a novel notion of local outlier in which the degree to which an object is outlying is dependent on the density of its local neighborhood, and each object can be assigned a Local Outlier Factor (LOF) which represents the likelihood of that object being an outlier. Although the concept of local outliers is a useful one, the computation of LOF values for every data objects requires a large number of &kgr;-nearest neighbors searches and can be computationally expensive. Since most objects are usually not outliers, it is useful to provide users with the option of finding only n most outstanding local outliers, i.e., the top-n data objects which are most likely to be local outliers according to their LOFs. However, if the pruning is not done carefully, finding top-n outliers could result in the same amount of computation as finding LOF for all objects. In this paper, we propose a novel method to efficiently find the top-n local outliers in large databases. The concept of "micro-cluster" is introduced to compress the data. An efficient micro-cluster-based local outlier mining algorithm is designed based on this concept. As our algorithm can be adversely affected by the overlapping in the micro-clusters, we proposed a meaningful cut-plane solution for overlapping data. The formal analysis and experiments show that this method can achieve good performance in finding the most outstanding local outliers.

#index 342626
#* Generalized clustering, supervised learning, and data assignment
#@ Annaka Kalton;Pat Langley;Kiri Wagstaff;Jungsoon Yoo
#t 2001
#c 0
#% 156186
#% 156189
#% 191680
#% 316481
#! Clustering algorithms have become increasingly important in handling and analyzing data. Considerable work has been done in devising effective but increasingly specific clustering algorithms. In contrast, we have developed a generalized framework that accommodates diverse clustering algorithms in a systematic way. This framework views clustering as a general process of iterative optimization that includes modules for supervised learning and instance assignment. The framework has also suggested several novel clustering methods. In this paper, we investigate experimentally the efficacy of these algorithms and test some hypotheses about the relation between such unsupervised techniques and the supervised methods embedded in them.

#index 342627
#* Mining a stream of transactions for customer patterns
#@ Diane Lambert;José C. Pinheiro
#t 2001
#c 0
#% 310585
#% 482123
#! Transaction data can arrive at a ferocious rate in the order that transactions are completed. The data contain an enormous amount of information about customers, not just transactions, but extracting up-to-date customer information from an ever changing stream of data and mining it in real-time is a challenge. This paper describes a statistically principled approach to designing short, accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions. A signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately, as an empirical study of the calling patterns of 96,000 wireless customers who made about 18 million wireless calls over a three month period shows.

#index 342628
#* The distributed boosting algorithm
#@ Aleksandar Lazarevic;Zoran Obradovic
#t 2001
#c 0
#% 90212
#% 280496
#% 708179
#! In this paper, we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that cannot be merged at a single location. Our distributed boosting algorithm can also be used as a parallel classification technique, where a massive database that cannot fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis. In the proposed method, at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites. Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set. The ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites. In experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time. In addition, the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases.

#index 342629
#* Induction of semantic classes from natural language text
#@ Dekang Lin;Patrick Pantel
#t 2001
#c 0
#% 118738
#% 185288
#% 217255
#% 248792
#% 310516
#% 363038
#% 448776
#% 747738
#% 755834
#% 756821
#% 757422
#! Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.

#index 342630
#* DIRT @SBT@discovery of inference rules from text
#@ Dekang Lin;Patrick Pantel
#t 2001
#c 0
#% 199036
#% 240015
#% 262090
#% 266218
#% 276155
#% 280404
#% 280840
#% 310516
#% 477642
#% 748457
#% 748465
#% 748691
#% 756585
#% 756964
#% 786518
#% 786578
#! In this paper, we propose an unsupervised method for discovering inference rules from text, such as "X is author of Y &ap; X wrote Y", "X solved Y &ap; X found a solution to Y", and "X caused Y &ap; Y is triggered by X". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.

#index 342631
#* Identifying non-actionable association rules
#@ Bing Liu;Wynne Hsu;Yiming Ma
#t 2001
#c 0
#% 136350
#% 172386
#% 280433
#% 280436
#% 280485
#% 280500
#% 310494
#% 310496
#% 443092
#% 462238
#% 481290
#% 481588
#% 1499588
#! Building predictive models and finding useful rules are two important tasks of data mining. While building predictive models has been well studied, finding useful rules for action still presents a major problem. A main obstacle is that many data mining algorithms often produce too many rules. Existing research has shown that most of the discovered rules are actually redundant or insignificant. Pruning techniques have been developed to remove those spurious and/or insignificant rules. In this paper, we argue that being a significant rule (or a non-redundant rule), however, does not mean that it is a potentially useful rule for action. Many significant rules (unpruned rules) are in fact not actionable. This paper studies this issue and presents an efficient algorithm to identify these non-actionable rules. Experiment results on many real-life datasets show that the number of non-actionable rules is typically quite large. The proposed technique thus enables the user to focus on fewer rules and to be assured that the remaining rules are non-redundant and potentially useful for action.

#index 342632
#* Discovering the set of fundamental rule changes
#@ Bing Liu;Wynne Hsu;Yiming Ma
#t 2001
#c 0
#% 204531
#% 273693
#% 280409
#% 280433
#% 310496
#% 443092
#% 464204
#% 481290
#% 481588
#! The world around us changes constantly. Knowing what has changed is an important part of our lives. For businesses, recognizing changes is also crucial. It allows businesses to adapt themselves to the changing market needs. In this paper, we study changes of association rules from one time period to another. One approach is to compare the supports and/or confidences of each rule in the two time periods and report the differences. This technique, however, is too simplistic as it tends to report a huge number of rule changes, and many of them are, in fact, simply the snowball effect of a small subset of fundamental changes. Here, we present a technique to highlight the small subset of fundamental changes. A change is fundamental if it cannot be explained by some other changes. The proposed technique has been applied to a number of real-life datasets. Experiments results show that the number of rules whose changes are unexplainable is quite small (about 20% of the total number of changes discovered), and many of these unexplainable changes reflect some fundamental shifts in the application domain.

#index 342633
#* Finding simple intensity descriptions from event sequence data
#@ Heikki Mannila;Marko Salmenkivi
#t 2001
#c 0
#% 280408
#! Sequences of events are an important type of data arising in various applications, including telecommunications, bio-statistics, web access analysis, etc. A basic approach to modeling such sequences is to find the underlying intensity functions describing the expected number of events per time unit. Typically, the intensity functions are assumed to be piecewise constant. We therefore consider different ways of fitting intensity models to event sequence data. We start by considering a Bayesian approach using Markov chain Monte Carlo (MCMC) methods with varying number of pieces. These methods can be used to produce posterior distributions on the intensity functions and they can also accomodate covariates. The drawback is that they are computationally intensive and thus are not very suitable for data mining applications in which large numbers of intensity functions have to be estimated. We consider dynamic programming approaches to finding the change points in the intensity functions. These methods can find the maximum likelihood intensity function in O(n2k) time for a sequence of n events and k different pieces of intensity. We show that simple heuristics can be used to prune the number of potential change points, yielding speedups of several orders of magnitude. The results of the improved dynamic programming method correspond very closely with the posterior averages produced by the MCMC methods.

#index 342634
#* Data filtering for automatic classification of rocks from reflectance spectra
#@ Jonathan Moody;Ricardo Silva;Joseph Vanderwaart
#t 2001
#c 0
#% 136733
#% 188076
#% 229972
#% 232118
#% 269634
#% 369236
#% 424759
#% 445218
#% 466205
#! The ability to identify the mineral composition of rocks and soils is an important tool for the exploration of geological sites. For instance, NASA intends to design robots that are sufficiently autonomous to perform this task on planetary missions. Spectrometer readings provide one important source of data for identifying sites with minerals of interest. Reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths. Spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals. For some mineral classes, carbonates for example, specific short spectral intervals are known to carry a distinctive signature. Finding similar distinctive spectral ranges for other mineral classes is not an easy problem. We propose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals. In one set of studies, we partition the whole interval of wavelengths available in our data into sub-intervals, or bins, and use a genetic algorithm to evaluate a candidate selection of subintervals. As alternatives to this computationally expensive search technique, we present an entropy-based heuristic that gives higher scores for wavelengths more likely to distinguish between classes, as well as other greedy search procedures. Results are presented for four different classes, showing reasonable improvements in identifying some, but not all, of the mineral classes tested.

#index 342635
#* Mining frequent neighboring class sets in spatial databases
#@ Yasuhiko Morimoto
#t 2001
#c 0
#% 444
#% 227996
#% 235114
#% 435137
#% 443083
#% 479658
#% 481281
#% 481290
#% 527021
#% 527022
#% 566128
#! We consider the problem of finding neighboring class sets. Objects of each instance of a neighboring class set are grouped using their Euclidean distances from each other. Recently, location-based services are growing along with mobile computing infrastructure such as cellular phones and PDAs. Therefore, we expect to see the development of spatial databases that contains very large number of access records including location information. The most typical type would be a database of point objects. Records of the objects may consist of "requested service name," "number of packet transmitted" in addition to x and y coordinate values indicating where the request came from. The algorithm presented here efficiently finds sets of "service names" that were frequently close to each other in the spatial database. For example, it may find a frequent neighboring class set, where "ticket" and "timetable" are frequently requested close to each other. By recognizing this, location-based service providers can promote a "ticket" service for customers who access the "timetable."

#index 342636
#* Experimental comparisons of online and batch versions of bagging and boosting
#@ Nikunj C. Oza;Stuart Russell
#t 2001
#c 0
#% 209021
#% 235377
#% 246747
#! Bagging and boosting are well-known ensemble learning methods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, i.e., they require multiple passes through the training data. In previous work, we presented online bagging and boosting algorithms that only require one pass through the training data and presented experimental results on some relatively small datasets. Through additional experiments on a variety of larger synthetic and real datasets, this paper demonstrates that our online versions perform comparably to their batch counterparts in terms of classification accuracy. We also demonstrate the substantial reduction in running time we obtain with our online algorithms because they require fewer passes through the training data.

#index 342637
#* TreeDT: gene mapping by tree disequilibrium test
#@ Petteri Sevon;Hannu T. T. Toivonen;Vesa Ollikainen
#t 2001
#c 0
#% 589349
#! We introduce and evaluate TreeDT, a novel gene mapping method which is based on discovering and assessing tree-like patterns in genetic marker data. Gene mapping aims at discovering a statistical connection from a particular disease or trait to a narrow region in the genome. In a typical case-control setting, data consists of genetic markers typed for a set of disease-associated chromosomes and a set of control chromosomes. A computer scientist would view this data as a set of strings.TreeDT extracts, essentially in the form of substrings and prefix trees, information about the historical recombinations in the population. This information is used to locate fragments potentially inherited from a common diseased founder, and to map the disease gene into the most likely such fragment. The method measures for each chromosomal location the disequilibrium of the prefix tree of marker strings starting from the location, to assess the distribution of disease-associated chromosomes.We evaluate experimentally the performance of TreeDT on realistic, simulated data sets, and comparisons to state of the art methods (TDT, HPM) show that TreeDT is very competitive.

#index 342638
#* Detecting graph-based spatial outliers: algorithms and applications (a summary of results)
#@ Shashi Shekhar;Chang-Tien Lu;Pusheng Zhang
#t 2001
#c 0
#% 2115
#% 51647
#% 230138
#% 273890
#% 300183
#% 415957
#% 443105
#% 462617
#% 477821
#% 479791
#! Identification of outliers can lead to the discovery of unexpected, interesting, and useful knowledge. Existing methods are designed for detecting spatial outliers in multidimensional geometric data sets, where a distance metric is available. In this paper, we focus on detecting spatial outliers in graph structured data sets. We define statistical tests, analyze the statistical foundation underlying our approach, design several fast algorithms to detect spatial outliers, and provide a cost model for outlier detection procedures. In addition, we provide experimental results from the application of our algorithms on a Minneapolis-St.Paul(Twin Cities) traffic dataset to show their effectiveness and usefulness.

#index 342639
#* A streaming ensemble algorithm (SEA) for large-scale classification
#@ W. Nick Street;YongSeog Kim
#t 2001
#c 0
#% 73372
#% 136350
#% 191854
#% 204531
#% 209021
#% 225872
#% 232102
#% 273900
#% 283145
#% 310498
#% 310500
#% 424997
#% 1650569
#! Ensemble methods have recently garnered a great deal of attention in the machine learning community. Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data, making them inappropriate in a data mining context. The methods presented in this paper take advantage of plentiful data, building separate classifiers on sequential chunks of training points. These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy. The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data, requires approximately constant memory, and adjusts quickly to concept drift.

#index 342640
#* Discovering associations with numeric variables
#@ Geoffrey I. Webb
#t 2001
#c 0
#% 152934
#% 172386
#% 179770
#% 210160
#% 280436
#% 280458
#% 310505
#% 481290
#% 1272179
#! This paper further develops Aumann and Lindell's [3] proposal for a variant of association rules for which the consequent is a numeric variable. It is argued that these rules can discover useful interactions with numeric data that cannot be discovered directly using traditional association rules with discretization. Alternative measures for identifying interesting rules are proposed. Efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of Auman and Lindell's algorithm is infeasible.

#index 342641
#* Discovering outlier filtering rules from unlabeled data: combining a supervised learner with an unsupervised learner
#@ Kenji Yamanishi;Jun-ichi Takeuchi
#t 2001
#c 0
#% 131686
#% 280441
#% 280515
#% 287206
#% 289519
#% 310552
#% 420064
#% 449559
#% 477809
#% 479791
#% 479986
#% 1808676
#% 1809407
#! This paper is concerned with the problem of detecting outliers from unlabeled data. In prior work we have developed SmartSifter, which is an on-line outlier detection algorithm based on unsupervised learning from data. On the basis of SmartSifter this paper yields a new framework for outlier filtering using both supervised and unsupervised learning techniques iteratively in order to make the detection process more effective and more understandable. The outline of the framework is as follows: In the first round, for an initial dataset, we run SmartSifter to give each data a score, with a high score indicating a high possibility of being an outlier. Next, giving positive labels to a number of higher scored data and negative labels to a number of lower scored data, we create labeled examples. Then we construct an outlier filtering rule by supervised learning from them. Here the rule is generated based on the principle of minimizing extended stochastic complexity. In the second round, for a new dataset, we filter the data using the constructed rule, then among the filtered data, we run SmartSifter again to evaluate the data in order to update the filtering rule. Applying of our framework to the network intrusion detection, we demonstrate that 1) it can significantly improve the accuracy of SmartSifter, and 2) outlier filtering rules can help the user to discover a general pattern of an outlier group.

#index 342642
#* Infominer: mining surprising periodic patterns
#@ Jiong Yang;Wei Wang;Philip S. Yu
#t 2001
#c 0
#% 21992
#% 172386
#% 227919
#% 280487
#% 280494
#% 310494
#% 310496
#% 310542
#% 443092
#% 464986
#% 480154
#% 496269
#% 631926
#! In this paper, we focus on mining surprising periodic patterns in a sequence of events. In many applications, e.g., computational biology, an infrequent pattern is still considered very significant if its actual occurrence frequency exceeds the prior expectation by a large margin. The traditional metric, such as support, is not necessarily the ideal model to measure this kind of surprising patterns because it treats all patterns equally in the sense that every occurrence carries the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence. A more suitable measurement, information, is introduced to naturally value the degree of surprise of each occurrence of a pattern as a continuous and monotonically decreasing function of its probability of occurrence. This would allow patterns with vastly different occurrence probabilities to be handled seamlessly. As the accumulated degree of surprise of all repetitions of a pattern, the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence. The bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem. Empirical tests demonstrate the efficiency and the usefulness of the proposed model.

#index 342643
#* Real world performance of association rule algorithms
#@ Zijian Zheng;Ron Kohavi;Llew Mason
#t 2001
#c 0
#% 191910
#% 232136
#% 280433
#% 300120
#% 310494
#% 310505
#% 338609
#% 1272179
#! This study compares five well-known association rule algorithms using three real-world datasets and an artificial dataset. The experimental results confirm the performance improvements previously claimed by the authors on the artificial data, but some of these gains do not carry over to the real datasets, indicating overfitting of the algorithms to the IBM artificial dataset. More importantly, we found that the choice of algorithm only matters at support levels that generate more rules than would be useful in practice. For support levels that generate less than 1,000,000 rules, which is much more than humans can handle and is sufficient for prediction purposes where data is loaded into RAM, Apriori finishes processing in less than 10 minutes. On our datasets, we observed super-exponential growth in the number of rules. On one of our datasets, a 0.02% change in the support increased the number of rules from less than a million to over a billion, implying that outside a very narrow range of support values, the choice of algorithm is irrelevant.

#index 342644
#* Segmentation-based modeling for advanced targeted marketing
#@ C. Apte;E. Bibelnieks;R. Natarajan;E. Pednault;F. Tipu;D. Campbell;B. Nelson
#t 2001
#c 0
#% 136350
#% 252398
#% 348323
#% 445342
#% 452819
#! Fingerhut Business Intelligence (BI) has a long and successful history of building statistical models to predict consumer behavior. The models constructed are typically segmentation-based models in which the target audience is split into subpopulations (i.e., customer segments) and individually tailored statistical models are then developed for each segment. Such models are commonly employed in the direct-mail industry; however, segmentation is often performed on an ad-hoc basis without directly considering how segmentation affects the accuracy of the resulting segment models. Fingerhut BI approached IBM Research with the problem of how to build segmentation-based models more effectively so as to maximize predictive accuracy. The IBM Advanced Targeted Marketing-Single EventsTM (IBM ATM-SETM) solution is the result of IBM Research and Fingerhut BI directing their efforts jointly towards solving this problem. This paper presents an evaluation of ATM-SE's modeling capabilities using data from Fingerhut's catalog mailings.

#index 342645
#* Interactive path analysis of web site traffic
#@ Pavel Berkhin;Jonathan D. Beche;Dee Jay Randall
#t 2001
#c 0
#% 232136
#% 247316
#% 297554
#% 310543
#% 369849
#% 406493
#% 424280
#% 424283
#% 459006
#% 463903
#% 963898
#! The goal of Path Analysis is to understand visitors' navigation of a Web site. The fundamental analysis component is a path. A path is a finite sequence of elements, typically representing URLs or groups of URLs. A full path is an abstraction of a visit or a session, which can contain attributes described below. Subpaths represent interesting subsequences of the full paths.Path Analysis provides user-configurable extraction, filtering, preprocessing, noise reduction, descriptive statistics and detailed analysis of three basic specific objects: elements, (sub)paths, and couples of elements. In each case, lists of frequent objects --- subject to particular filtering and sorting --- are available. We call the corresponding interactive tools Element, Path, and Couple Analyzers.We also allow in-depth exploration of individual elements, paths, and couples: Element Explorer investigates composition and convergence of traffic through an element and allows conditioning based on the number of preceding/succeeding steps. Path Explorer visualizes in and out flows of a path and attrition rate along the path. Couple Explorer presents distinct paths connecting couple elements, along with measures of their association and some additional statistics.

#index 342646
#* Estimating business targets
#@ Piew Datta;James H. Drew;Andrew Betz;D. R. Mani;Jeffery Howard
#t 2001
#c 0
#% 5182
#% 92533
#% 280422
#% 280452
#% 729437
#% 1272304
#! Determining and setting maximal revenue expectations or other business performance targets---whether it is for regional company divisions or individual customers---can have profound financial implications. Operational techniques are changed, staffing levels are altered and management attention is re-focused---all in the name of expectations. In practice these expectations are often derived in an ad hoc manner. To address this unsupervised task, we combine nearest neighbor methods and classical statistical methods and derive a new solution to the classical econometric task of frontier analysis. We apply our methodology to two real world business problems in Verizon, a major telecommunications provider in the United States, more specifically in the print yellow page division Verizon Information Services: (1) identifying under marketed customers for targeted upselling campaigns and focused sales attention, and (2) benchmarking regional directory divisions to incent performance improvements. Our analysis uncovers some commercially useful aspects of these domains and by conservative estimates can increase revenue by several million dollars in each domain.

#index 342647
#* Magical thinking in data mining: lessons from CoIL challenge 2000
#@ Charles Elkan
#t 2001
#c 0
#% 272995
#% 320942
#% 342611
#% 464280
#! CoIL challenge 2000 was a supervised learning contest that attracted 43 entries. The authors of 29 entries later wrote explanations of their work. This paper discusses these reports and reaches three main conclusions. First, naive Bayesian classifiers remain competitive in practice: they were used by both the winning entry and the next best entry. Second, identifying feature interactions correctly is important for maximizing predictive accuracy: this was the difference between the winning classifier and all others. Third and most important, too many researchers and practitioners in data mining do not appreciate properly the issue of statistical significance and the danger of overfitting. Given a dataset such as the one for the CoIL contest, it is pointless to apply a very complicated learning algorithm, or to perform a very time-consuming model search. In either ease, one is likely to overfit the training data and to fool oneself in estimating predictive accuracy and in discovering useful correlations.

#index 342648
#* REVI-MINER, a KDD-environment for deviation detection and analysis of warranty and goodwill cost statements in automotive industry
#@ E. Hotz;U. Grimmer;W. Heuser;G. Nakhaeizadeh;M. Wieczorek
#t 2001
#c 0
#% 280519
#! REVI-MINER is a KDD-environment which supports the detection and analysis of deviations in warranty and goodwill cost statements. The system was developed within the framework of a cooperation between DaimlerChrysler Research & Technology and Global Service and Parts (GSP) and is based upon the CRISP-DM methodology as a widely accepted process model for the solution of Data Mining problems. Also, we have implemented different approaches based on Machine learning and statistics which can be utilized for data cleaning in the preprocessing phase. The Data Mining models applied have been developed by using a statistical deviation detection approach. The tool supports controllers in their task of auditing the authorized repair shops. In this paper we describe the development phases which have led to REVI-MINER.

#index 342649
#* Data mining techniques to improve forecast accuracy in airline business
#@ Christoph Hueglin;Francesco Vannotti
#t 2001
#c 0
#% 380342
#! Predictive models developed by applying Data Mining techniques are used to improve forecasting accuracy in the airline business. In order to maximize the revenue on a flight, the number of seats available for sale is typically higher than the physical seat capacity (overbooking). To optimize the overbooking rate, an accurate estimation of the number of no-show passengers (passengers who hold a valid booking but do not appear at the gate to board for the flight) is essential. Currently, no-shows on future flights are estimated from the number of no-shows on historical flights averaged on booking class level. In this work, classification trees and logistic regression models are applied to estimate the probability that an individual passenger turns out to be a no-show. Passenger information stored in the reservation system of the airline is either directly used as explanatory variable or used to create attributes that have an impact on the probability of a passenger to be a no-show. The total number of no-shows in each booking class or on the total flight is then obtained by accumulating the individual no-show probabilities over the entity of interest. We show that this forecasting approach is more accurate than the currently used method. In addition, the selected models lead to a deepened insight into passenger behavior.

#index 342650
#* Mining from open answers in questionnaire data
#@ Hang Li;Kenji Yamanishi
#t 2001
#c 0
#% 131686
#% 280512
#% 280513
#% 287196
#% 287206
#% 786497
#% 853847
#% 1808676
#% 1809407
#! Surveys are an important part of marketing and customer relationship management, and open answers (i.e., answers to open questions) in particular may contain valuable information and provide an important basis for making business decisions. We have developed a text mining system that provides a new way for analyzing open answers in questionnaire data. The product is able to perform the following two functions: (A) accurate extraction of characteristics for individual analysis targets, (B) accurate extraction of the relationships among characteristics of analysis targets. In this paper, we describe the working of our text mining system. It employs two statistical learning techniques: rule analysis and Correspondence Analysis for performing the two functions. Our text mining system has already been put into use by a number of large corporations in Japan in the performance of text mining on various types of survey data, including open answers about brand images, open answers about company images, complaints about products, comments written on home pages, business reports, and help desk records. In this it has been found to be useful in forming a basis for effective business decisions.

#index 342651
#* Funnel report mining for the MSN network
#@ Teresa Mah;Hank Hoek;Ying Li
#t 2001
#c 0
#% 257945
#% 459006
#% 630984
#! Data mining research has long concentrated on the five main areas: clustering, association discovery, classification, forecasting and sequential patterns. Web data mining projects are concerned mainly with text mining, user segmentation, forecasting web usage and analyzing users' clickstream patterns. We present a new type of web usage mining called funnel analysis or funnel report mining. A funnel report is a study of the retention behavior among a series of pages or sites. For example, of all hits on the home page of www.msn.com, what percentages of those are followed by hits to moneycentral.msn.com? What percentage of www.msn.com hits are followed by moneycentral.msn.com, and then www.msnbc.com? What are the most interesting funnels starting with www.msn.com? Where does the greatest drop off rate occur after a user has hit MSNBC? Funnel reports are extremely useful in e-business because they give product planners an idea of how usable and well-structured their site is. From our experience performing web usage mining for the MSN network of sites, funnel reports are requested even more than user segmentation analyses, site affiliation studies and classification exercises. In this paper, we define a framework for funnel analysis and provide a tree-based solution we have been using successfully to extract all relevant funnels using only one scan of the data file.

#index 342652
#* Evaluation of prediction models for marketing campaigns
#@ Saharon Rosset;Einat Neumann;Uri Eick;Nurit Vatnik;Izhak Idan
#t 2001
#c 0
#! We consider prediction-model evaluation in the context of marketing-campaign planning. In order to evaluate and compare models with specific campaign objectives in mind, we need to concentrate our attention on the appropriate evaluation-criteria. These should portray the model's ability to score accurately and to identify the relevant target population. In this paper we discuss some applicable model-evaluation and selection criteria, their relevance for campaign planning, their robustness under changing population distributions, and their employment when constructing confidence intervals. We illustrate our results with a case study based on our experience from several projects.

#index 342653
#* Knowledge base maintenance using knowledge gap analysis
#@ Scott Spangler;Jeffrey Kreulen
#t 2001
#c 0
#% 46803
#% 115478
#% 232108
#% 375388
#% 406493
#! As the web and e-business have proliferated, the practice of using customer facing knowledge bases to augment customer service and support operations has increased. This can be a very efficient, scalable and cost effective way to share knowledge. The effectiveness and cost savings are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge. To address this issue, we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required.In this paper, we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem tickets to determine which categories and subcategories are not well addressed within the knowledge base. We utilize text clustering on problem ticket text to determine a set of problem categories. We then compare each knowledge base solution document to each problem category centroid using a cosine distance metric. The distance between the "closest" solution document and the corresponding centroid becomes the basis of that problem category's "knowledge gap". Our claim is that this gap metric serves as a useful method for quickly and automatically determining which problem categories have no relevant solutions in a knowledge base. We have implemented our approach, and we present the results of performing a knowledge gap analysis on a set of support center problem tickets.

#index 342654
#* Mining user session data to facilitate user interaction with a customer service knowledge base in RightNow Web
#@ Doug Warner;J. Neal Richter;Stephen D. Durbin;Bikramjit Banerjee
#t 2001
#c 0
#% 179800
#% 210173
#% 756253
#% 1650569
#! RightNow Web is an integrated software package for web-based customer service that has, at its core, a database of answers to frequently asked questions (FAQs). One major design goal is to facilitate end-user interaction with this dynamic document collection, i.e. make it as easy and efficient as possible for users to browse the collection and locate desired information. To this end, we perform several types of analysis on the session tracking database that records user navigation histories. First, using both explicit and implicit measures of user satisfaction, we infer a "solved count" representing the average utility of an FAQ. Second, using the user navigation patterns we construct a link matrix representing connections between FAQs. The technique of building up the link matrix and using it to advise users on related information amounts to a form of the "swarm intelligence" method of finding optimal paths. Both solved count and the link matrix are continuously updated as users interact with the site; furthermore, they are periodically "aged" to emphasize recent activity. The synergistic combination of these techniques allows users to learn from the database in a more effective manner, as evidenced by usage statistics.

#index 342655
#* Mining web logs for prediction models in WWW caching and prefetching
#@ Qiang Yang;Haining Henry Zhang;Tianyi Li
#t 2001
#c 0
#% 201696
#% 268184
#% 443262
#% 463903
#% 665553
#% 963897
#% 963898
#% 978378
#% 979370
#! Web caching and prefetching are well known strategies for improving the performance of Internet systems. When combined with web log mining, these strategies can decide to cache and prefetch web documents with higher accuracy. In this paper, we present an application of web log mining to obtain web-document access patterns and use these patterns to extend the well-known GDSF caching policies and prefetching policies. Using real web logs, we show that this application of data mining can achieve dramatic improvement to web-access performance.

#index 342812
#* Tutorial notes of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Tom Fawcett
#t 2001
#c 0
#! Tutorials have become an integral part of the KDD conference. This is partly because of the interdisciplinary nature of data mining, but also because of the amount and speed of progress in the past decade. Tutorials are an effective way for conference attendees to educate themselves in specific topics and to familiarize themselves with emerging subfields. Traditionally, KDD conferences have offered high-quality tutorials on many aspects of data mining.This year KDD-2001 continues this tradition with six three-hour tutorials. This tutorial set was chosen to serve a broad range of interests, from theoretical to applied, from academic to commercial, and from traditional to innovative. These tutorials also cover a range of depths, some treating an individual topic in detail and others surveying a broad area.&bul; E-Business Enterprise Data Mining (Usama Fayyad, Neal Rothleder and Paul Bradley)&bul; Data Mining for Outliers with Robust Statistics (R. Douglas Martin)&bul; Advances in Decision Tree Construction (Johannes Gehrke and Wei-Yin Loh)&bul; Data Mining "To Go": Ubiquitous KDD for Mobile and Distributed Environments (Hillol Kargupta and Anupam Joshi)&bul; Scalable Frequent-Pattern Mining Methods: An Overview (Jiawei Han, Laks V. S. Lakshrnanan and Jian Pei)&bul; Value-based Data Mining and Web Mining for CRM (Steve Gallant, Gregory Piatetsky-Shapiro and Ming Tan)

#index 342813
#* E-business enterprise data mining
#@ Usama Fayyad;Neal Rothleder;Paul Bradley
#t 2001
#c 0
#% 91872
#% 92135
#% 136350
#% 190581
#% 232136
#% 269634
#% 300120
#% 420057
#% 420077
#% 420109
#% 481290

#index 342814
#* Data mining for outliers with robust statistics
#@ R. Douglas Martin
#t 2001
#c 0

#index 342815
#* Advances in decision tree construction
#@ Johannes Gehrke;Wei-Yin Loh
#t 2001
#c 0

#index 342816
#* Data mining "to go": ubiquitous KDD for mobile and distributed environments
#@ Hillol Kargupta;Anupam Joshi
#t 2001
#c 0

#index 342817
#* Scalable frequent-pattern mining methods: an overview
#@ Jiawei Han;Laks V. S. Lakshmanan;Jian Pei
#t 2001
#c 0
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 227917
#% 227919
#% 227922
#% 227953
#% 248784
#% 248785
#% 248791
#% 248813
#% 259993
#% 273899
#% 273916
#% 280473
#% 300120
#% 310494
#% 310558
#% 310559
#% 318994
#% 329598
#% 333925
#% 420063
#% 420067
#% 438134
#% 459006
#% 461909
#% 463903
#% 464204
#% 464822
#% 464839
#% 464989
#% 464996
#% 479482
#% 479484
#% 479627
#% 479795
#% 479971
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 481954
#% 584891
#% 631926
#% 632028
#% 632037

#index 342818
#* Value-based data mining and web mining for CRM
#@ Steve Gallant;Gregory Piatetsky-Shapiro;Ming Tan
#t 2001
#c 0

#index 577210
#* Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Osmar R. Zaïane;Randy Goebel;David Hand;Daniel Keim;Raymond Ng
#t 2002
#c 0
#! The KDD 2002 conference, held from 23rd to 26th July 2002, was the eighth in the series. It represented a return to the country in which the series was launched: the first was held in Montreal, Canada, and this, the eighth, was held in Edmonton, Canada. In the years between the first conference in the series and this present one, data mining has be, come a well-established discipline. It has continued to strengthen its links to other data analytic disciplines, including statistics, machine learning, pattern recognition, visualization, and database technology, but has now clearly carved out a niche of its own. Over the period in which this series has been running, hardware technology has continued to advance in great leaps, with the result that large databases have continued to grow in both number and size. The implication is that the challenge of data mining is even more important, that the problems requiring data mining solutions are ever more ubiquitous, and that new tools and methods for tackling are even more necessary.KDD 2002 received a record number of submitted papers - 307 in total, 37 of which were considered for the industral/applicafion track. Among the 270 research submissions, 32 were selected (12%) for full papers; and among the 37 industrial/application submissions, 12 (32%) were selected for full papers. An additional 44 submissions were chosen to be presented as posters, a vast majority of which were research submissions. This low rate of acceptance reflects a conscious effort to maintain the very high standards of quality and relevance, which have been achieved by previous conferences in the series. It means that the papers and posters in the proceedings represent the cutting edge of data mining problemsl solutions, and technology. On the other hand, this policy inevitably meant that many excellent contributions did not make it to the final program. The choice had to be informed by balance as well as quality - KDD 2002 had to showcase research in data mining across the entire frontier of the discipline. This breadth was reflected in the choice of invited speakers, both well known in the data mining; community, but from different backgrounds: Daryl Pregibon and Geoff Hinton. The program also includes 6 workshops in such diverse areas as 'Data Mining in Bioinformatics', 'Web Mining', 'Multimedia Data Mining', 'Multi-Relational Data Mining', 'Temporal Data Mining', and 'Fractals in Data Mining' as well as 6 tutorials on 'Text Mining for Bioinformatics', 'Querying and Mining Data Streams', 'Link Analysis', 'Multivariate Density Estimation', 'Common Reasons Data Mining Projects Fail', and 'Visual Data Mining'.

#index 577211
#* Bayesian analysis of massive datasets via particle filters
#@ Greg Ridgeway;David Madigan
#t 2002
#c 0
#% 16251
#% 232111
#% 391410
#% 413458
#% 420052
#! Markov Chain Monte Carlo (MCMC) techniques revolutionized statistical practice in the 1990s by providing an essential toolkit for making the rigor and flexibility of Bayesian analysis computationally practical. At the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need to produce statistically sound methods that scale to these large problems. Except for the most trivial examples, current MCMC methods require a complete scan of the dataset for each iteration eliminating their candidacy as feasible data mining techniques.In this article we present a method for making Bayesian analysis of massive datasets computationally feasible. The algorithm simulates from a posterior distribution that conditions on a smaller, more manageable portion of the dataset. The remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling. Computation of the importance weights requires a single scan of the remaining observations. While importance sampling increases efficiency in data access, it comes at the expense of estimation efficiency. A simple modification, based on the "rejuvenation" step used in particle filters for dynamic systems models, sidesteps the loss of efficiency with only a slight increase in the number of data accesses.To show proof-of-concept, we demonstrate the method on a mixture of transition models that has been used to model web traffic and robotics. For this example we show that estimation efficiency is not affected while offering a 95% reduction in data accesses.

#index 577212
#* Scalable robust covariance and correlation estimates for data mining
#@ Fatemah A. Alqallaf;Kjell P. Konis;R. Douglas Martin;Ruben H. Zamar
#t 2002
#c 0
#% 34077
#% 124536
#% 273907
#% 296697
#! Covariance and correlation estimates have important applications in data mining. In the presence of outliers, classical estimates of covariance and correlation matrices are not reliable. A small fraction of outliers, in some cases even a single outlier, can distort the classical covariance and correlation estimates making them virtually useless. That is, correlations for the vast majority of the data can be very erroneously reported; principal components transformations can be misleading; and multidimensional outlier detection via Mahalanobis distances can fail to detect outliers. There is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases. All such estimators have unacceptable exponential complexity in the number of variables and quadratic complexity in the number of observations. In this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complexity in the number of variables and linear complexity in the number of observations. These estimators are based on several forms of pairwise robust covariance and correlation estimates. The estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by [14]. We show that the estimators have attractive robustness properties, and give an example that uses one of the estimators in the new Insightful Miner data mining product.

#index 577213
#* MARK: a boosting algorithm for heterogeneous kernel models
#@ Kristin P. Bennett;Michinari Momma;Mark J. Embrechts
#t 2002
#c 0
#% 138308
#% 190581
#% 235377
#% 292664
#% 309208
#% 342598
#% 390723
#% 425040
#% 425063
#% 466081
#% 562953
#! Support Vector Machines and other kernel methods have proven to be very effective for nonlinear inference. Practical issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadratically with the data. Inspired by ensemble and boosting methods like MART, we propose the Multiple Additive Regression Kernels (MARK) algorithm to address these issues. MARK considers a large (potentially infinite) library of kernel matrices formed by different kernel functions and parameters. Using gradient boosting/column generation, MARK constructs columns of the heterogeneous kernel matrix (the base hypotheses) on the fly and then adds them into the kernel ensemble. Regularization methods such as used in SVM, kernel ridge regression, and MART, are used to prevent overfitting. We investigate how MARK is applied to heterogeneous kernel ridge regression. The resulting algorithm is simple to implement and efficient. Kernel parameter selection is handled within MARK. Sampling and "weak" kernels are used to further enhance the computational efficiency of the resulting additive algorithm. The user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernels. MARK compares very favorably with SVM and kernel ridge regression on several benchmark datasets.

#index 577214
#* Selecting the right interestingness measure for association patterns
#@ Pang-Ning Tan;Vipin Kumar;Jaideep Srivastava
#t 2002
#c 0
#% 152934
#% 172386
#% 248012
#% 280433
#% 341700
#% 408159
#% 412588
#% 420073
#% 443092
#% 515387
#% 1290032
#! Many techniques for association rule mining and feature selection require a suitable metric to capture the dependencies among variables in a data set. For example, metrics such as support, confidence, lift, correlation, and collective strength are often used to determine the interestingness of association patterns. However, many such measures provide conflicting information about the interestingness of a pattern, and the best metric to use for a given application domain is rarely known. In this paper, we present an overview of various measures proposed in the statistics, machine learning and data mining literature. We describe several key properties one should examine in order to select the right measure for a given application domain. A comparative study of these properties is made using twenty one of the existing measures. We show that each measure has different properties which make them useful for some application domains, but not for others. We also present two scenarios in which most of the existing measures agree with each other, namely, support-based pruning and table standardization. Finally, we present an algorithm to select a small set of tables such that an expert can select a desirable measure by looking at just this small set of tables.

#index 577215
#* DualMiner: a dual-pruning algorithm for itemsets with constraints
#@ Cristian Bucila;Johannes Gehrke;Daniel Kifer;Walker White
#t 2002
#c 0
#% 152934
#% 232136
#% 237200
#% 248783
#% 248785
#% 248791
#% 274146
#% 310558
#% 420112
#% 464989
#% 465003
#% 481290
#% 1289265
#! Constraint-based mining of itemsets for questions such as "find all frequent itemsets where the total price is at least $50" has received much attention recently. Two classes of constraints, monotone and antimonotone, have been identified as very useful. There are algorithms that efficiently take advantage of either one of these two classes, but no previous algorithms can efficiently handle both types of constraints simultaneously. In this paper, we present the first algorithm (called DualMiner) that uses both monotone and antimonotone constraints to prune its search space. We complement a theoretical analysis and proof of correctness of DualMiner with an experimental study that shows the efficacy of DualMiner compared to previous work.

#index 577216
#* Querying multiple sets of discovered rules
#@ Alexander Tuzhilin;Bing Liu
#t 2002
#c 0
#! Rule mining is an important data mining task that has been applied to numerous real-world applications. Often a rule mining system generates a large number of rules and only a small subset of them is really useful in applications. Although there exist some systems allowing the user to query the discovered rules, they are less suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rules. In this paper, we propose a new powerful rule query language Rule-QL for querying multiple rulebases that is modeled after SQL and has rigorous theoretical foundations of a rule-based calculus. In particular, we first propose a rule-based calculus RC based on the first-order logic, and then present the language Rule-QL that is at least as expressive as the safe fragment of RC. We also propose a number of efficient query evaluation techniques for Rule-QL and test them experimentally on some representative queries to demonstrate the feasibility of Rule-QL.

#index 577217
#* Mining knowledge-sharing sites for viral marketing
#@ Matthew Richardson;Pedro Domingos
#t 2002
#c 0
#% 146494
#% 220708
#% 246831
#% 268079
#% 280422
#% 280442
#% 282905
#% 342596
#% 479969
#% 528168
#% 529654
#! Viral marketing takes advantage of networks of influence among customers to inexpensively achieve large changes in behavior. Our research seeks to put it on a firmer footing by mining these networks from data, building probabilistic models of them, and using these models to choose the best viral marketing plan. Knowledge-sharing sites, where customers review products and advise each other, are a fertile source for this type of data mining. In this paper we extend our previous techniques, achieving a large reduction in computational cost, and apply them to data from a knowledge-sharing site. We optimize the amount of marketing funds spent on each customer, rather than just making a binary decision on whether to market to him. We take into account the fact that knowledge of the network is partial, and that gathering that knowledge can itself have a cost. Our results show the robustness and utility of our approach.

#index 577218
#* Efficiently mining frequent trees in a forest
#@ Mohammed J. Zaki
#t 2002
#c 0
#% 184048
#% 187659
#% 232136
#% 237192
#% 262071
#% 282470
#% 300033
#% 325384
#% 333981
#% 462235
#% 463903
#% 465018
#% 466644
#% 480489
#% 552188
#% 631914
#% 661023
#! Mining frequent trees is very useful in domains like bioinformatics, web mining, mining semistructured data, and so on. We formulate the problem of mining (embedded) subtrees in a forest of rooted, labeled, and ordered trees. We present TREEMINER, a novel algorithm to discover all frequent subtrees in a forest, using a new data structure called scope-list. We contrast TREEMINER with a pattern matching tree mining algorithm (PATTERNMATCHER). We conduct detailed experiments to test the performance and scalability of these methods. We find that TREEMINER outperforms the pattern matching approach by a factor of 4 to 20, and has good scaleup properties. We also present an application of tree mining to analyze real web logs for usage patterns.

#index 577219
#* ANF: a fast and scalable tool for data mining in massive graphs
#@ Christopher R. Palmer;Phillip B. Gibbons;Christos Faloutsos
#t 2002
#c 0
#% 2833
#% 77940
#% 137889
#% 243166
#% 268079
#% 283833
#% 299941
#% 309749
#% 342596
#% 406493
#% 445369
#! Graphs are an increasingly important data source, with such important graphs as the Internet and the Web. Other familiar graphs include CAD circuits, phone records, gene sequences, city streets, social networks and academic citations. Any kind of relationship, such as actors appearing in movies, can be represented as a graph. This work presents a data mining tool, called ANF, that can quickly answer a number of interesting questions on graph-represented data, such as the following. How robust is the Internet to failures? What are the most influential database papers? Are there gender differences in movie appearance patterns? At its core, ANF is based on a fast and memory-efficient approach for approximating the complete "neighbourhood function" for a graph. For the Internet graph (268K nodes), ANF's highly-accurate approximation is more than 700 times faster than the exact computation. This reduces the running time from nearly a day to a matter of a minute or two, allowing users to perform ad hoc drill-down tasks and to repeatedly answer questions about changing data sources. To enable this drill-down, ANF employs new techniques for approximating neighbourhood-type functions for graphs with distinguished nodes and/or edges. When compared to the best existing approximation, ANF's approach is both faster and more accurate, given the same resources. Additionally, unlike previous approaches, ANF scales gracefully to handle disk resident graphs. Finally, we present some of our results from mining large graphs using ANF.

#index 577220
#* Bursty and hierarchical structure in streams
#@ Jon Kleinberg
#t 2002
#c 0
#% 9197
#% 159108
#% 177468
#% 193325
#% 214751
#% 223630
#% 230532
#% 248049
#% 252755
#% 259633
#% 262042
#% 262043
#% 271083
#% 272793
#% 278106
#% 280408
#% 287196
#% 292235
#% 308923
#% 309096
#% 309100
#% 341700
#% 342633
#% 463903
#% 466564
#% 641137
#% 641150
#% 647336
#% 1781027
#! A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise --- that the appearance of a topic in a document stream is signaled by a "burst of activity," with certain features rising sharply in frequency as the topic emerges.The goal of the present work is to develop a formal approach for modeling such "bursts," in such a way that they can be robustly and efficiently identified, and can provide an organizational framework for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions; in some ways, it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them.

#index 577221
#* On the need for time series data mining benchmarks: a survey and empirical demonstration
#@ Eamonn Keogh;Shruti Kasetty
#t 2002
#c 0
#% 172949
#% 227924
#% 232122
#% 240182
#% 260014
#% 260016
#% 264633
#% 273704
#% 280408
#% 285711
#% 310502
#% 310580
#% 310583
#% 316538
#% 316559
#% 316560
#% 330932
#% 333941
#% 342690
#% 460862
#% 461885
#% 462231
#% 464851
#% 465014
#% 466507
#% 477479
#% 477482
#% 477825
#% 480146
#% 480156
#% 480302
#% 481609
#% 481611
#% 534183
#% 586837
#% 616530
#% 617886
#% 617888
#% 631920
#% 631923
#% 632042
#% 632088
#% 659936
#% 659944
#! In the last decade there has been an explosion of interest in mining time series data. Literally hundreds of papers have introduced new algorithms to index, classify, cluster and segment time series. In this work we make the following claim. Much of this work has very little utility because the contribution made (speed in the case of indexing, accuracy in the case of classification and clustering, model accuracy in the case of segmentation) offer an amount of "improvement" that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets, or the variance that would have been observed by changing minor (unstated) implementation details.To illustrate our point, we have undertaken the most exhaustive set of time series experiments ever attempted, re-implementing the contribution of more than two dozen papers, and testing them on 50 real world, highly diverse datasets. Our empirical results strongly support our assertion, and suggest the need for a set of time series benchmarks and more careful empirical evaluation in the data mining community.

#index 577222
#* Query, analysis, and visualization of hierarchically structured data using Polaris
#@ Chris Stolte;Diane Tang;Pat Hanrahan
#t 2002
#c 0
#% 149109
#% 172757
#% 227927
#% 239243
#% 309425
#% 340759
#% 428505
#% 434617
#% 436116
#% 464215
#% 480123
#% 529290
#% 641093
#% 641123
#% 727085
#! In the last several years, large OLAP databases have become common in a variety of applications such as corporate data warehouses and scientific computing. To support interactive analysis, many of these databases are augmented with hierarchical structures that provide meaningful levels of abstraction that can be leveraged by both the computer and analyst. This hierarchical structure generates many challenges and opportunities in the design of systems for the query, analysis, and visualization of these databases.In this paper, we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouses with rich hierarchical structure, such as might be stored in data cubes. We base this tool on Polaris, a system for rapidly constructing table-based graphical displays of multidimensional databases. Polaris builds visualizations using an algebraic formalism derived from the interface and interpreted as a set of queries to a database. We extend the user interface, algebraic formalism, and generation of data queries in Polaris to expose and take advantage of hierarchical structure. In the resulting system, analysts can navigate through the hierarchical projections of a database, rapidly and incrementally generating visualizations for each projection.

#index 577223
#* On interactive visualization of high-dimensional data using the hyperbolic plane
#@ Jörg A. Walter;Helge Ritter
#t 2002
#c 0
#% 36160
#% 46803
#% 173425
#% 202036
#% 322953
#% 325209
#% 506179
#% 564886
#% 641101
#! We propose a novel projection based visualization method for high-dimensional datasets by combining concepts from MDS and the geometry of the hyperbolic spaces. Our approach Hyperbolic Multi-Dimensional Scaling (H-MDS) extends earlier work [7] using hyperbolic spaces for visualization of tree structures data ( "hyperbolic tree browser" ).By borrowing concepts from multi-dimensional scaling we map proximity data directly into the 2-dimensional hyperbolic space (H2). This removes the restriction to "quasihierarchical", graph-based data -- limiting previous work. Since a suitable distance function can convert all kinds of data to proximity (or distance-based) data this type of data can be considered the most general.We used the circular Poincaré model of the H2 which allows effective human-computer interaction: by moving the "focus" via mouse the user can navigate in the data without loosing the "context". In H2 the "fish-eye" behavior originates not simply by a non-linear view transformation but rather by extraordinary, non-Euclidean properties of the H2. Especially, the exponential growth of length and area of the underlying space makes the H2 a prime target for mapping hierarchical and (now also) high-dimensional data.We present several high-dimensional mapping examples including synthetic and real world data and a successful application for unstructured text. By analyzing and integrating multiple film critiques from news:rec.art.movies.reviews and the internet movie database, each movie becomes placed within the H2. Here the idea is, that related films share more words in their reviews than unrelated. Their semantic proximity leads to a closer arrangement. The result is a kind of high-level content structured display allowing the user to explore the "space of movies".

#index 577224
#* Optimizing search engines using clickthrough data
#@ Thorsten Joachims
#t 2002
#c 0
#% 46803
#% 57484
#% 116149
#% 169774
#% 186989
#% 187763
#% 190581
#% 197394
#% 269217
#% 310567
#% 387427
#% 402289
#% 564279
#% 1272396
#% 1275346
#! This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.

#index 577225
#* Relational Markov models and their application to adaptive web navigation
#@ Corin R. Anderson;Pedro Domingos;Daniel S. Weld
#t 2002
#c 0
#% 75936
#% 152934
#% 225837
#% 228812
#% 246836
#% 271129
#% 292169
#% 309777
#% 310543
#% 312861
#% 312874
#% 338308
#% 338609
#% 464434
#% 466077
#% 466078
#% 496116
#% 580510
#% 1273676
#% 1275346
#% 1289268
#% 1499473
#! Relational Markov models (RMMs) are a generalization of Markov models where states can be of different types, with each type described by a different set of variables. The domain of each variable can be hierarchically structured, and shrinkage is carried out over the cross product of these hierarchies. RMMs make effective learning possible in domains with very large and heterogeneous state spaces, given only sparse data. We apply them to modeling the behavior of web site users, improving prediction in our PROTEUS architecture for personalizing web sites. We present experiments on an e-commerce and an academic web site showing that RMMs are substantially more accurate than alternative methods, and make good predictions even when applied to previously-unvisited parts of the site.

#index 577226
#* Pattern discovery in sequences under a Markov assumption
#@ Darya Chudova;Padhraic Smyth
#t 2002
#c 0
#% 190581
#% 196811
#% 328321
#% 380342
#% 469571
#% 616537
#! In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences. An important real-world problem of this nature is motif discovery in DNA sequences. We investigate the fundamental aspects of this data mining problem that can make discovery "easy" or "hard." We present a general framework for characterizing learning in this context by deriving the Bayes error rate for this problem under a Markov assumption. The Bayes error framework demonstrates why certain patterns are much harder to discover than others. It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery. We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms, providing a lower bound on achievable performance. We discuss a number of fundamental issues that characterize sequential pattern discovery in this context, present a variety of empirical results to complement and verify the theoretical analysis, and apply our methodology to real-world motif-discovery problems in computational biology.

#index 577227
#* On effective classification of strings with wavelets
#@ Charu C. Aggarwal
#t 2002
#c 0
#% 4868
#% 136350
#% 235941
#% 273900
#% 280408
#% 280482
#% 300181
#% 333941
#% 463903
#% 466668
#% 479787
#% 481290
#% 481609
#% 566132
#% 631926
#% 632089
#! In recent years, the technological advances in mapping genes have made it increasingly easy to store and use a wide variety of biological data. Such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task. For example, a typical DNA string may be millions of characters long, and there may be thousands of such strings in a database. In many cases, the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which cannot be easily determined apriori. Another problem which complicates the classification task is that in some cases the classification behavior is reflected in global behavior of the string, whereas in others it is reflected in local patterns. Given the enormous variation in the behavior of the strings over different data sets, it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification. For this purpose, we will exploit the multi-resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristics at different levels of granularity. The resulting scheme turns out to be very effective in practice on a wide range of problems.

#index 577228
#* Shrinkage estimator generalizations of Proximal Support Vector Machines
#@ Deepak K. Agarwal
#t 2002
#c 0
#% 117563
#% 209021
#% 342597
#% 342598
#% 342599
#% 378173
#% 722818
#! We give a statistical interpretation of Proximal Support Vector Machines (PSVM) proposed at KDD2001 as linear approximaters to (nonlinear) Support Vector Machines (SVM). We prove that PSVM using a linear kernel is identical to ridge regression, a biased-regression method known in the statistical community for more than thirty years. Techniques from the statistical literature to estimate the tuning constant that appears in the SVM and PSVM framework are discussed. Better shrinkage strategies that incorporate more than one tuning constant are suggested. For nonlinear kernels, the minimization problem posed in the PSVM framework is equivalent to finding the posterior mode of a Bayesian model defined through a Gaussian process on the predictor space. Apart from providing new insights, these interpretations help us attach an estimate of uncertainty to our predictions and enable us to build richer classes of models. In particular, we propose a new algorithm called PSVMMIX which is a combination of ridge regression and a Gaussian process model. Extension to the case of continuous response is straightforward and illustrated with example datasets.

#index 577229
#* Hierarchical model-based clustering of large datasets through fractionation and refractionation
#@ Jeremy Tantrum;Alejandro Murua;Werner Stuetzle
#t 2002
#c 0
#% 118771
#% 200694
#% 273890
#% 1860128
#! The goal of clustering is to identify distinct groups in a dataset. Compared to non-parametric clustering methods like complete linkage, hierarchical model-based clustering has the advantage of offering a way to estimate the number of groups present in the data. However, its computational cost is quadratic in the number of items to be clustered, and it is therefore not applicable to large problems. We review an idea called Fractionation, originally conceived by Cutting, Karger, Pedersen and Tukey for non-parametric hierarchical clustering of large datasets, and describe an adaptation of Fractionation to model-based clustering. A further extension, called Refractionation, leads to a procedure that can be successful even in the difficult situation where there are large numbers of small groups.

#index 577230
#* Enhanced word clustering for hierarchical text classification
#@ Inderjit S. Dhillon;Subramanyam Mallela;Rahul Kumar
#t 2002
#c 0
#% 91091
#% 115608
#% 190581
#% 246831
#% 262059
#% 280819
#% 340905
#% 376266
#% 406493
#% 420054
#% 425010
#% 458379
#% 465747
#% 465754
#% 482113
#% 729437
#% 748465
#% 1809518
#! In this paper we propose a new information-theoretic divisive algorithm for word clustering applied to text classification. In previous work, such "distributional clustering" of features has been found to achieve improvements over feature selection in terms of classification accuracy, especially at lower number of features [2, 28]. However the existing clustering techniques are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost. In order to explicitly capture the optimality of word clusters in an information theoretic framework, we first derive a global criterion for feature clustering. We then present a fast, divisive algorithm that monotonically decreases this objective function value, thus converging to a local minimum. We show that our algorithm minimizes the "within-cluster Jensen-Shannon divergence" while simultaneously maximizing the "between-cluster Jensen-Shannon divergence". In comparison to the previously proposed agglomerative strategies our divisive algorithm achieves higher classification accuracy especially at lower number of features. We further show that feature clustering is an effective technique for building smaller class models in hierarchical classification. We present detailed experimental results using Naive Bayes and Support Vector Machines on the 20 Newsgroups data set and a 3-level hierarchy of HTML documents collected from Dmoz Open Directory.

#index 577231
#* A parallel learning algorithm for text classification
#@ Canasai Kruengkrai;Chuleerat Jaruskulchai
#t 2002
#c 0
#% 237949
#% 295986
#% 311027
#% 348555
#% 376266
#% 387427
#% 443091
#% 465895
#% 660537
#! Text classification is the process of classifying documents into predefined categories based on their content. Existing supervised learning algorithms to automatically classify text need sufficient labeled documents to learn accurately. Applying the Expectation-Maximization (EM) algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled documents to augment the available labeled documents. Unfortunately, the time needed to learn with these large unlabeled documents is too high. This paper introduces a novel parallel learning algorithm for text classification task. The parallel algorithm is based on the combination of the EM algorithm and the naive Bayes classifier. Our goal is to improve the computational time in learning and classifying process. We studied the performance of our parallel algorithm on a large Linux PC cluster called PIRUN Cluster. We report both timing and accuracy results. These results indicate that the proposed parallel algorithm is capable of handling large document collections.

#index 577232
#* A refinement approach to handling model misfit in text categorization
#@ Haoran Wu;Tong Heng Phang;Bing Liu;Xiaoli Li
#t 2002
#c 0
#% 132938
#% 169717
#% 169806
#% 190581
#% 197394
#% 209021
#% 218961
#% 219050
#% 219051
#% 262085
#% 266215
#% 280817
#% 302391
#% 309122
#% 310503
#% 311034
#% 318412
#% 340940
#% 458379
#% 465746
#% 465895
#% 465919
#% 466572
#% 682681
#% 1271840
#% 1499573
#! Text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents. This problem has been studied in information retrieval, machine learning and data mining. So far, many effective techniques have been proposed. However, most techniques are based on some underlying models and/or assumptions. When the data fits the model well, the classification accuracy will be high. However, when the data does not fit the model well, the classification accuracy can be very low. In this paper, we propose a refinement approach to dealing with this problem of model misfit. We show that we do not need to change the classification technique itself (or its underlying model) to make it more flexible. Instead, we propose to use successive refinements of classification on the training data to correct the model misfit. We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers, the Rocchio classifier and the naïve Bayesian classifier. These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier. Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically. In particular, our refined model is able to improve the naïve Bayesian or Rocchio classifier's prediction performance by 45% on average.

#index 577233
#* Privacy preserving mining of association rules
#@ Alexandre Evfimievski;Ramakrishnan Srikant;Rakesh Agrawal;Johannes Gehrke
#t 2002
#c 0
#% 67453
#% 152934
#% 232136
#% 248791
#% 264246
#% 280387
#% 300184
#% 333876
#% 376266
#% 449588
#% 482049
#% 577289
#% 601649
#! We present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions. While it is feasible to recover association rules and preserve privacy using a straightforward "uniform" randomization, the discovered rules can unfortunately be exploited to find privacy breaches. We analyze the nature of privacy breaches and propose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches. We derive formulae for an unbiased support estimator and its variance, which allow us to recover itemset supports from randomized datasets, and show how to incorporate these formulae into mining algorithms. Finally, we present experimental results that validate the algorithm by applying it on real datasets.

#index 577234
#* Mining frequent item sets by opportunistic projection
#@ Junqiang Liu;Yunhe Pan;Ke Wang;Jiawei Han
#t 2002
#c 0
#% 152934
#% 201894
#% 227917
#% 248791
#% 300120
#% 310507
#% 329598
#% 342643
#% 459020
#% 465003
#% 466490
#% 481290
#% 481588
#% 481754
#% 481779
#! In this paper, we present a novel algorithm Opportune Project for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree. Our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures, array-based or tree-based, to represent projected transaction subsets, and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets. More importantly, we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets, which makes our algorithm both CPU time efficient and memory saving. Basically, the algorithm grows the frequent item set tree by depth first search, whereas breadth first search is used to build the upper portion of the tree if necessary. We test our algorithm versus several other algorithms on real world datasets, such as BMS-POS, and on IBM artificial datasets. The empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold, but also highly scalable to very large databases.

#index 577235
#* PEBL: positive example based learning for Web page classification using SVM
#@ Hwanjo Yu;Jiawei Han;Kevin Chen-Chuan Chang
#t 2002
#c 0
#% 197394
#% 266215
#% 280817
#% 309141
#% 309142
#% 310556
#% 311027
#% 316533
#% 458379
#% 466263
#% 615723
#% 722756
#% 722811
#% 722812
#! Web page classification is one of the essential techniques for Web mining. Specifically, classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web. However, constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples. For instance, in order to construct a "homepage" classifier, one needs to collect a sample of homepages (positive examples) and a sample of non-homepages (negative examples). In particular, collecting negative training examples requires arduous work and special caution to avoid biasing them. We introduce in this paper the Positive Example Based Learning (PEBL) framework for Web page classification which eliminates the need for manually collecting negative training examples in pre-processing. We present an algorithm called Mapping-Convergence (M-C) that achieves classification accuracy (with positive and unlabeled data) as high as that of traditional SVM (with positive and negative data). Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM, the M-C algorithm performs as well as traditional SVM.

#index 577236
#* Web site mining: a new way to spot competitors, customers and suppliers in the world wide web
#@ Martin Ester;Hans-Peter Kriegel;Matthias Schubert
#t 2002
#c 0
#% 136350
#% 248810
#% 280488
#% 280817
#% 290482
#% 312861
#% 425006
#% 458379
#% 501994
#! When automatically extracting information from the world wide web, most established methods focus on spotting single HTML-documents. However, the problem of spotting complete web sites is not handled adequately yet, in spite of its importance for various applications. Therefore, this paper discusses the classification of complete web sites. First, we point out the main differences to page classification by discussing a very intuitive approach and its weaknesses. This approach treats a web site as one large HTML-document and applies the well-known methods for page classification. Next, we show how accuracy can be improved by employing a preprocessing step which assigns an occurring web page to its most likely topic. The determined topics now represent the information the web site contains and can be used to classify it more accurately. We accomplish this by following two directions. First, we apply well established classification algorithms to a feature space of occurring topics. The second direction treats a site as a tree of occurring topics and uses a Markov tree model for further classification. To improve the efficiency of this approach, we additionally introduce a powerful pruning method reducing the number of considered web pages. Our experiments show the superiority of the Markov tree approach regarding classification accuracy. In particular, we demonstrate that the use of our pruning method not only reduces the processing time, but also improves the classification accuracy.

#index 577237
#* Sequential cost-sensitive decision making with reinforcement learning
#@ Edwin Pednault;Naoki Abe;Bianca Zadrozny
#t 2002
#c 0
#% 124691
#% 280437
#% 342611
#% 342644
#% 384911
#% 393786
#% 466268
#% 466568
#% 1272286
#% 1289281
#! Recently, there has been increasing interest in the issues of cost-sensitive learning and decision making in a variety of applications of data mining. A number of approaches have been developed that are effective at optimizing cost-sensitive decisions when each decision is considered in isolation. However, the issue of sequential decision making, with the goal of maximizing total benefits accrued over a period of time instead of immediate benefits, has rarely been addressed. In the present paper, we propose a novel approach to sequential decision making based on the reinforcement learning framework. Our approach attempts to learn decision rules that optimize a sequence of cost-sensitive decisions so as to maximize the total benefits accrued over time. We use the domain of targeted' marketing as a testbed for empirical evaluation of the proposed method. We conducted experiments using approximately two years of monthly promotion data derived from the well-known KDD Cup 1998 donation data set. The experimental results show that the proposed method for optimizing total accrued benefits out performs the usual targeted-marketing methodology of optimizing each promotion in isolation. We also analyze the behavior of the targeting rules that were obtained and discuss their appropriateness to the application domain.

#index 577238
#* Interactive deduplication using active learning
#@ Sunita Sarawagi;Anuradha Bhamidipaty
#t 2002
#c 0
#% 116165
#% 136350
#% 169806
#% 170649
#% 236729
#% 310503
#% 310516
#% 333679
#% 333943
#% 342611
#% 376266
#% 420072
#% 420077
#% 438103
#% 466419
#% 466580
#% 480496
#% 480499
#% 480654
#% 565531
#% 637522
#% 659966
#% 722757
#% 722797
#% 1478821
#! Deduplication is a key operation in integrating data from multiple sources. The main challenge in this task is designing a function that can resolve when a pair of records refer to the same entity in spite of various data inconsistencies. Most existing systems use hand-coded functions. One way to overcome the tedium of hand-coding is to train a classifier to distinguish between duplicates and non-duplicates. The success of this method critically hinges on being able to provide a covering and challenging set of training pairs that bring out the subtlety of deduplication function. This is non-trivial because it requires manually searching for various data inconsistencies between any two records spread apart in large lists.We present our design of a learning-based deduplication system that uses a novel method of interactively discovering challenging training pairs using active learning. Our experiments on real-life datasets show that active learning significantly reduces the number of instances needed to achieve high accuracy. We investigate various design issues that arise in building a system to provide interactive response, fast convergence, and interpretable output.

#index 577239
#* Transforming data to satisfy privacy constraints
#@ Vijay S. Iyengar
#t 2002
#c 0
#% 81961
#% 114994
#% 300184
#% 369236
#% 443158
#% 443463
#% 449588
#% 466459
#% 488324
#! Data on individuals and entities are being collected widely. These data can contain information that explicitly identifies the individual (e.g., social security number). Data can also contain other kinds of personal information (e.g., date of birth, zip code, gender) that are potentially identifying when linked with other available data sets. Data are often shared for business or legal reasons. This paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process. We explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data. We extend earlier works in this area along various dimensions. First, satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated. This allows us to optimize the process of preserving privacy for the specified usage. In particular, we investigate the privacy transformation in the context of data mining applications like building classification and regression models. Second, our work improves on previous approaches by allowing more flexible generalizations for the data. Lastly, this is combined with a more thorough exploration of the solution space using the genetic algorithm framework. These extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints.

#index 577240
#* Exploiting unlabeled data in ensemble methods
#@ Kristin P. Bennett;Ayhan Demiriz;Richard Maclin
#t 2002
#c 0
#% 190581
#% 252011
#% 266255
#% 304876
#% 311027
#% 342639
#% 424997
#! An adaptive semi-supervised ensemble method, ASSEMBLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data. ASSEMBLE alternates between assigning "pseudo-classes" to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data. Mathematically, this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data. Unlike alternative approaches, ASSEMBLE does not require a semi-supervised learning method for the base classifier. ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems. ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition. In addition, strong results on several benchmark datasets using both decision trees and neural networks support the proposed method.

#index 577241
#* Predicting rare classes: can boosting make any weak learner strong?
#@ Mahesh V. Joshi;Ramesh C. Agarwal;Vipin Kumar
#t 2002
#c 0
#% 169777
#% 180945
#% 283138
#% 302391
#% 333934
#% 375017
#% 466268
#% 466561
#% 466654
#! Boosting is a strong ensemble-based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner, as long as it satisfies the condition of yielding weighted accuracy 0.5. In this paper, we analyze boosting with respect to this basic condition on the base learner, to see if boosting ensures prediction of rarely occurring events with high recall and precision. First we show that a base learner can satisfy the required condition even for poor recall or precision levels, especially for very rare classes. Furthermore, we show that the intelligent weight updating mechanism in boosting, even in its strong cost-sensitive form, does not prevent cases where the base learner always achieves high precision but poor recall or high recall but poor precision, when mapped to the original distribution. In either of these cases, we show that the voting mechanism of boosting falls to achieve good overall recall and precision for the ensemble. In effect, our analysis indicates that one cannot be blind to the base learner performance, and just rely on the boosting mechanism to take care of its weakness. We validate our arguments empirically on variety of real and synthetic rare class problems. In particular, using AdaCost as the boosting algorithm, and variations of PNrule and RIPPER as the base learners, we show that if algorithm A achieves better recall-precision balance than algorithm B, then using A as the base learner in AdaCost yields significantly better performance than using B as the base learner.

#index 577242
#* Efficient handling of high-dimensional feature spaces by randomized classifier ensembles
#@ Aleksander Kołcz;Xiaomei Sun;Jugal Kalita
#t 2002
#c 0
#% 165110
#% 190581
#% 209021
#% 256439
#% 280817
#% 311034
#% 312727
#% 400847
#% 465754
#! Handling massive datasets is a difficult problem not only due to prohibitively large numbers of entries but in some cases also due to the very high dimensionality of the data. Often, severe feature selection is performed to limit the number of attributes to a manageable size, which unfortunately can lead to a loss of useful information. Feature space reduction may well be necessary for many stand-alone classifiers, but recent advances in the area of ensemble classifier techniques indicate that overall accurate classifier aggregates can be learned even if each individual classifier operates on incomplete "feature view" training data, i.e., such where certain input attributes are excluded. In fact, by using only small random subsets of features to build individual component classifiers, surprisingly accurate and robust models can be created. In this work we demonstrate how these types of architectures effectively reduce the feature space for submodels and groups of sub-models, which lends itself to efficient sequential and/or parallel implementations. Experiments with a randomized version of Adaboost are used to support our arguments, using the text classification task as an example.

#index 577243
#* From run-time behavior to usage scenarios: an interaction-pattern mining approach
#@ Mohammad El-Ramly;Eleni Stroulia;Paul Sorenson
#t 2002
#c 0
#% 89780
#% 150994
#% 231963
#% 254163
#% 299698
#% 302184
#% 399965
#% 420063
#% 440653
#% 463903
#% 608533
#% 611276
#% 622340
#% 622376
#% 622425
#% 640555
#% 708368
#! A key challenge facing IT organizations today is their evolution towards adopting e-business practices that gives rise to the need for reengineering their underlying software systems. Any reengineering effort has to be aware of the functional requirements of the subject system, in order not to violate the integrity of its intended uses. However, as software systems get regularly maintained throughout their lifecycle, the documentation of their requirements often become obsolete or get lost. To address this problem of "software requirements loss", we have developed an interaction-pattern mining method for the recovery of functional requirements as usage scenarios. Our method analyzes traces of the run-time system-user interaction to discover frequently recurring patterns; these patterns correspond to the functionality currently exercised by the system users, represented as usage scenarios. The discovered scenarios provide the basis for reengineering the software system into web-accessible components, each one supporting one of the discovered scenarios. In this paper, we describe IPM2, our interaction-pattern discovery algorithm, we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed.

#index 577244
#* Exploiting response models: optimizing cross-sell and up-sell opportunities in banking
#@ Andrew Storey;Marc-david Cohen
#t 2002
#c 0
#% 36698
#% 348323
#! The banking industry regularly mounts campaigns to improve customer value by offering new products to existing customers. In recent years this approach has gained significant momentum because of the increasing availability of customer data and the improved analysis capabilities in data mining. Typically, response models based on historical data are used to estimate the probability of a customer purchasing an additional product and the expected return from that additional purchase. Even with these computational improvements and accurate models of customer behavior, the problem of efficiently using marketing resources to maximize the return on marketing investment is a challenge. This problem is compounded because of the capability to launch multiple campaigns through several distribution channels over multiple time periods. The combination of alternatives creates a complicated array of possible actions. This paper presents a solution that answers the question of what products, if any, to offer to each customer in a way that maximizes the marketing return on investment. The solution is an improvement over the usual approach of picking the customers that have the largest expected value for a particular product because it is a global maximization from the viewpoint of the bank and allows for the effective implementation of business constraints across customers and business units. The approach accounts for limited resources, multiple sequential campaigns, and other business constraints. Furthermore, the solution provides insight into the cost of these constraints, in terms of decreased profits, and thus is an effective tool for both tactical campaign execution and strategic planning.

#index 577245
#* Customer lifetime value modeling and its use for customer retention planning
#@ Saharon Rosset;Einat Neumann;Uri Eick;Nurit Vatnik;Yizhak Idan
#t 2002
#c 0
#% 280422
#% 280515
#% 338614
#% 338624
#% 342652
#% 420054
#% 477809
#% 630998
#! We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry. We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ. We then describe how we build on this approach to estimate the effects of retention on Lifetime Value. Our solution has been successfully implemented in Amdocs' Business Insight (BI) platform, and we illustrate its usefulness in real-world scenarios.

#index 577246
#* Mining product reputations on the Web
#@ Satoshi Morinaga;Kenji Yamanishi;Kenji Tateishi;Toshikazu Fukushima
#t 2002
#c 0
#% 131686
#% 136350
#% 240955
#% 244103
#% 248808
#% 261741
#% 309124
#% 309127
#% 312861
#% 342650
#% 438201
#% 450037
#% 481290
#% 755834
#% 817611
#% 1808676
#% 1809407
#! Knowing the reputations of your own and/or competitors' products is important for marketing and customer relationship management. It is, however, very costly to collect and analyze survey data manually. This paper presents a new framework for mining product reputations on the Internet. It automatically collects people's opinions about target products from Web pages, and it uses text mining techniques to obtain the reputations of those products.On the basis of human-test samples, we generate in advance syntactic and linguistic rules to determine whether any given statement is an opinion or not, as well as whether such any opinion is positive or negative in nature. We first collect statements regarding target products using a general search engine, and then, using the rules, extract opinions from among them and attach three labels to each opinion, labels indicating the positive/negative determination, the product name itself, and an numerical value expressing the degree of system confidence that the statement is, in fact, an opinion. The labeled opinions are then input into an opinion database.The mining of reputations, i.e., the finding of statistically meaningful information included in the database, is then conducted. We specify target categories using label values (such as positive opinions of product A) and perform four types of text mining: extraction of 1) characteristic words, 2) co-occurrence words, 3) typical sentences, for individual target categories, and 4) correspondence analysis among multiple target categories.Actual marketing data is used to demonstrate the validity and effectiveness of the framework, which offers a drastic reduction in the overall cost of reputation analysis over that of conventional survey approaches and supports the discovery of knowledge from the pool of opinions on the web.

#index 577247
#* Learning domain-independent string transformation weights for high accuracy object identification
#@ Sheila Tejada;Craig A. Knoblock;Steven Minton
#t 2002
#c 0
#% 115462
#% 131061
#% 152980
#% 248801
#% 266292
#% 271128
#% 287222
#% 310516
#% 350103
#% 420072
#% 466095
#% 481607
#% 668675
#! The task of object identification occurs when integrating information from multiple websites. The same data objects can exist in inconsistent text formats across sites, making it difficult to identify matching objects using exact text match. Previous methods of object identification have required manual construction of domain-specific string transformations or manual setting of general transformation parameter weights for recognizing format inconsistencies. This manual process can be time consuming and error-prone. We have developed an object identification system called Active Atlas [18], which applies a set of domain-independent string transformations to compare the objects' shared attributes in order to identify matching objects. In this paper, we discuss extensions to the Active Atlas system, which allow it to learn to tailor the weights of a set of general transformations to a specific application domain through limited user input. The experimental results demonstrate that this approach achieves higher accuracy and requires less user involvement than previous methods across various application domains.

#index 577248
#* A system for real-time competitive market intelligence
#@ Sholom M. Weiss;Naval K. Verma
#t 2002
#c 0
#% 342605
#% 344447
#% 445319
#% 466744
#! A method is described for real-time market intelligence and competitive analysis. News stories are collected online for a designated group of companies. The goal is to detect critical differences in the text written about a company versus the text for its competitors. A solution is found by mapping the task into a non-stationary text categorization model. The overall design consists of the following components: (a) a real-time crawler that monitors newswires for stories about the competitors (b) a conditional document retriever that selects only those documents that meet the indicated conditions (c) text analysis techniques that convert the documents to a numerical format (d) rule induction methods for finding patterns in data (e) presentation techniques for displaying results. The method is extended to combine text with numerical measures, such as those based on stock prices and market capitalizations, that allow for more objective evaluations and projections.

#index 577249
#* Mining intrusion detection alarms for actionable knowledge
#@ Klaus Julisch;Marc Dacier
#t 2002
#c 0
#% 131422
#% 156752
#% 183319
#% 232147
#% 280419
#% 296738
#% 302479
#% 310840
#% 314054
#% 321551
#% 324276
#% 340031
#% 402489
#% 420063
#% 420064
#% 451052
#% 451058
#% 452747
#% 479191
#% 479192
#% 653940
#! In response to attacks against enterprise networks, administrators increasingly deploy intrusion detection systems. These systems monitor hosts, networks, and other resources for signs of security violations. The use of intrusion detection has given rise to another difficult problem, namely the handling of a generally large number of alarms. In this paper, we mine historical alarms to learn how future alarms can be handled more efficiently. First, we investigate episode rules with respect to their suitability in this approach. We report the difficulties encountered and the unexpected insights gained. In addition, we introduce a new conceptual clustering technique, and use it in extensive experiments with real-world data to show that intrusion detection alarms can be handled efficiently by using previously mined knowledge.

#index 577250
#* Learning nonstationary models of normal network traffic for detecting novel attacks
#@ Matthew V. Mahoney;Philip K. Chan
#t 2002
#c 0
#% 67454
#% 188026
#% 321553
#% 344059
#% 536877
#% 536883
#% 664547
#% 664711
#% 790040
#% 978633
#! Traditional intrusion detection systems (IDS) detect attacks by comparing current behavior to signatures of known attacks. One main drawback is the inability of detecting new attacks which do not have known signatures. In this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic. Behavior that deviates from the learned normal model signals possible novel attacks. Our IDS is unique in two respects. First, it is nonstationary, modeling probabilities based on the time since the last event rather than on average rate. This prevents alarm floods. Second, the IDS learns protocol vocabularies (at the data link through application layers) in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software. On the 1999 DARPA IDS evaluation data set [9], we detect 70 of 180 attacks (with 100 false alarms), about evenly divided between user behavioral anomalies (IP addresses and ports, as modeled by most other systems) and protocol anomalies. Because our methods are unconventional there is a significant non-overlap of our IDS with the original DARPA participants, which implies that they could be combined to increase coverage.

#index 577251
#* ADMIT: anomaly-based data mining for intrusions
#@ Karlton Sequeira;Mohammed Zaki
#t 2002
#c 0
#% 18528
#% 70370
#% 92533
#% 289519
#% 428402
#% 583711
#% 713956
#% 978636
#% 1001828
#! Security of computer systems is essential to their acceptance and utility. Computer security analysts use intrusion detection systems to assist them in maintaining computer system security. This paper deals with the problem of differentiating between masqueraders and the true user of a computer terminal. Prior efficient solutions are less suited to real time application, often requiring all training data to be labeled, and do not inherently provide an intuitive idea of what the data model means. Our system, called ADMIT, relaxes these constraints, by creating user profiles using semi-incremental techniques. It is a real-time intrusion detection system with host-based data collection and processing. Our method also suggests ideas for dealing with concept drift and affords a detection rate as high as 80.3% and a false positive rate as low as 15.3%.

#index 577252
#* Handling very large numbers of association rules in the analysis of microarray data
#@ Alexander Tuzhilin;Gediminas Adomavicius
#t 2002
#c 0
#% 152934
#% 172386
#% 232136
#% 232146
#% 280433
#% 297490
#% 310558
#% 328301
#% 420076
#% 420101
#% 420118
#% 461909
#% 577216
#% 631970
#% 729437
#% 1499588
#! The problem of analyzing microarray data became one of important topics in bioinformatics over the past several years, and different data mining techniques have been proposed for the analysis of such data. In this paper, we propose to use association rule discovery methods for determining associations among expression levels of different genes. One of the main problems related to the discovery of these associations is the scalability issue. Microarrays usually contain very large numbers of genes that are sometimes measured in 10,000s. Therefore, analysis of such data can generate a very large number of associations that can often be measured in millions. The paper addresses this problem by presenting a method that enables biologists to evaluate these very large numbers of discovered association rules during the post-analysis stage of the data mining process. This is achieved by providing several rule evaluation operators, including rule grouping, filtering, browsing, and data inspection operators, that allow biologists to validate multiple individual gane regulation patterns at a time. By iteratively applying these operators, biologists can explore a significant part of all the initially generated rules in an acceptable period of time and thus answer biological questions that are of a particular interest to him or her. To validate our method, we tested our system on the microarray data pertaining to the studies of environmental hazards and their influence of gane expression processes. As a result, we managed to answer several questions that were of interest to the biologists that had collected this data.

#index 577253
#* On the potential of domain literature for clustering and Bayesian network learning
#@ Peter Antal;Patrick Glenisson;Geert Fannes
#t 2002
#c 0
#% 36672
#% 44876
#% 115470
#% 129987
#% 197387
#% 234793
#% 328374
#% 387427
#% 469421
#% 590188
#% 654675
#! Thanks to its increasing availability, electronic literature can now be a major source of information when developing complex statistical models where data is scarce or contains much noise. This raises the question of how to integrate information from domain literature with statistical data. Because quantifying similarities or dependencies between variables is a basic building block in knowledge discovery, we consider here the following question. Which vector representations of text and which statistical scores of similarity or dependency support best the use of literature in statistical models? For the text source, we assume to have annotations for the domain variables as short free-text descriptions and optionally to have a large literature repository from which we can further expand the annotations. For evaluation, we contrast the variables similarities or dependencies obtained from text using different annotation sources and vector representations with those obtained from measurement data or expert assessments. Specifically, we consider two learning problems: clustering and Bayesian network learning. Firstly, we report performance (against an expert reference) for clustering yeast genes from textual annotations. Secondly, we assess the agreement between text-based and data-based scores of variable dependencies when learning Bayesian network substructures for the task of modeling the joint distribution of clinical measurements of ovarian tumors.

#index 577254
#* Mining heterogeneous gene expression data with time lagged recurrent neural networks
#@ Yulan Liang;Arpad Kelemen
#t 2002
#c 0
#% 361100
#% 1860336
#% 1862634
#! Heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment, disease development and drug effect at the molecular level. In this paper for both exploring and prediction purposes a Time Lagged Recurrent Neural Network with trajectory learning is proposed for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments. The proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time. Also, the trajectory learning with Back-propagation through time algorithm can recognize gene expression patterns vary over time. This may reveal much more information about the regulatory network underlying gene expressions. The analyzed data were extracted from spotted DNA microarrays in the budding yeast expression measurements, produced by Eisen et al. The gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions. The number of recognized gene patterns in our study ranged from two to ten and were divided into three cases. Optimal network architectures with different memory structures were selected based on Akaike and Bayesian information statistical criteria using two-way factorial design. The optimal model performance was compared to other popular gene classification algorithms such as Nearest Neighbor, Support Vector Machine, and Self-Organized Map. The reliability of the performance was verified with multiple iterated runs.

#index 577255
#* Collaborative crawling: mining user experiences for topical resource discovery
#@ Charu C. Aggarwal
#t 2002
#c 0
#% 281251
#% 330599
#% 577255
#! The rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent years. In this problem, it is desired to find web pages which satisfy a predicate specified by the user. Such a predicate could be a keyword query, a topical query, or some arbitrary contraint. Several techniques such as focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery. All these crawlers are linkage based, since they use the hyperlink behavior in order to perform resource discovery. Recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process. In this paper, we will approach the problem of resource discovery from an entirely different perspective; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate. This user behavior can be mined from the freely available traces of large public domain proxies on the world wide web. We refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources. Such a strategy is extremely effective because the topical consistency in world wide web browsing patterns turns out to very reliable. In addition, the user-centered crawling system can be combined with linkage based systems to create an overall system which works more effectively than a system based purely on either user behavior or hyperlinks.

#index 577256
#* Sequential PAttern mining using a bitmap representation
#@ Jay Ayres;Jason Flannick;Johannes Gehrke;Tomi Yiu
#t 2002
#c 0
#% 248791
#% 425006
#% 459006
#% 463903
#% 464996
#% 465003
#% 479971
#% 481290
#% 631926
#! We introduce a new algorithm for mining sequential patterns. Our algorithm is especially efficient when the sequential patterns in the database are very long. We introduce a novel depth-first search strategy that integrates a depth-first traversal of the search space with effective pruning mechanisms.Our implementation of the search strategy combines a vertical bitmap representation of the database with efficient support counting. A salient feature of our algorithm is that it incrementally outputs new frequent itemsets in an online fashion.In a thorough experimental evaluation of our algorithm on standard benchmark data from the literature, our algorithm outperforms previous work up to an order of magnitude.

#index 577257
#* Frequent term-based text clustering
#@ Florian Beil;Martin Ester;Xiaowei Xu
#t 2002
#c 0
#% 118771
#% 252836
#% 262045
#% 280404
#% 320944
#% 396747
#% 481290
#% 630983
#! Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates flat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the- art text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets.

#index 577258
#* A theoretical framework for learning from a pool of disparate data sources
#@ Shai Ben-David;Johannes Gehrke;Reba Schuller
#t 2002
#c 0
#% 66937
#% 180945
#% 236497
#% 267027
#% 333990
#% 479465
#% 479783
#% 637829
#% 641044
#% 1271814
#! Many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task. For example, aiming towards the design of an automated diagnostic tool for some disease, one may wish to integrate data gathered in many different hospitals. A major obstacle to such endeavors is that different data sources may vary considerably in the way they choose to represent related data. In practice, the problem is usually solved by a manual construction of semantic mappings and translations between the different sources. Recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations.In this work we propose a theoretical framework for making classification predictions from a collection of different data sources, without creating explicit translations between them. Our framework allows a precise mathematical analysis of the complexity of such tasks, and it provides a tool for the development and comparison of different learning algorithms. Our main objective, at this stage, is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework.

#index 577259
#* Topics in 0--1 data
#@ Ella Bingham;Heikki Mannila;Jouni K. Seppänen
#t 2002
#c 0
#% 152934
#% 176172
#% 211044
#% 226495
#% 232136
#% 248027
#% 280819
#% 342594
#% 342607
#% 528023
#% 857458
#! Large 0--1 datasets arise in various applications, such as market basket analysis and information retrieval. We concentrate on the study of topic models, aiming at results which indicate why certain methods succeed or fail. We describe simple algorithms for finding topic models from 0--1 data. We give theoretical results showing that the algorithms can discover the epsilon-separable topic models of Papadimitriou et al. We present empirical results showing that the algorithms find natural topics in real-world data sets. We also briefly discuss the connections to matrix approaches, including nonnegative matrix factorization and independent component analysis.

#index 577260
#* Extracting decision trees from trained neural networks
#@ Olcay Boz
#t 2002
#c 0
#% 92542
#% 96695
#% 129979
#% 703975
#% 711806
#! Neural Networks are successful in acquiring hidden knowledge in datasets. Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans. Researchers tried to address this problem by extracting rules from trained Neural Networks. Most of the proposed rule extraction methods required specialized type of Neural Networks; some required binary inputs and some were computationally expensive. Craven proposed extracting MofN type Decision Trees from Neural Networks. We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex. In this paper, we introduced a new method for extracting regular C4.5 like Decision Trees from trained Neural Networks. We showed that the new method (DecText) is effective in extracting high fidelity trees from trained networks. We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity.

#index 577261
#* A new two-phase sampling based algorithm for discovering association rules
#@ Bin Chen;Peter Haas;Peter Scheuermann
#t 2002
#c 0
#% 152934
#% 164368
#% 227917
#% 248791
#% 273898
#% 300120
#% 300195
#% 310507
#% 465162
#% 480805
#% 481290
#% 481779
#% 504021
#% 678172
#! This paper introduces FAST, a novel two-phase sampling-based algorithm for discovering association rules in large databases. In Phase I a large initial sample of transactions is collected and used to quickly and accurately estimate the support of each individual item in the database. In Phase II these estimated supports are used to either trim "outlier" transactions or select "representative" transactions from the initial sample, thereby forming a small final sample that more accurately reflects the statistical characteristics (i.e., itemset supports) of the entire database. The expensive operation of discovering association rules is then performed on the final sample. In an empirical study, FAST was able to achieve 90--95% accuracy using a final sample having a size of only 15--33% of that of a comparable random sample. This efficiency gain resulted in a speedup by roughly a factor of 10 over previous algorithms that require expensive processing of the entire database --- even efficient algorithms that exploit sampling. Our new sampling technique can be used in conjunction with almost any standard association-rule algorithm, and can potentially render scalable other algorithms that mine "count" data.

#index 577262
#* CVS: a Correlation-Verification based Smoothing technique on information retrieval and term clustering
#@ Christina Yip Chung;Bin Chen
#t 2002
#c 0
#% 109200
#% 169729
#% 218978
#% 240212
#% 262084
#% 262114
#% 280847
#% 280849
#% 340948
#% 340951
#% 387427
#% 853784
#! As information volume in enterprise systems and in the Web grows rapidly, how to accurately retrieve information is an important research area. Several corpus based smoothing techniques have been proposed to address the data sparsity and synonym problems faced by information retrieval systems. Such smoothing techniques are often unable to discover and utilize the correlations among terms.We propose CVS, a Correlation-Verification based Smoothing method, that considers co-occurrence information in smoothing. Strongly correlated terms in a document are identified by their co-occurrence frequencies in the document. To avoid missing correlated terms with low co-occurrence frequencies but specific to the theme of the document, the joint distributions of terms in the document are compared with those in the corpus for statistical significance.A common approach to apply corpus based smoothing techniques to information retrieval is by refining the vector representations of documents. This paper investigates the effects of corpus based smoothing on information retrieval by query expansion using term clusters generated from a term clustering process. The results can also be viewed in light of the effects of smoothing on clustering.Empirical studies show that our approach outperforms previous corpus based smoothing techniques. It improves retrieval effectiveness by 14.6%. The results demonstrate that corpus based smoothing can be used for query expansion by term clustering.

#index 577263
#* Learning to match and cluster large high-dimensional data sets for data integration
#@ William W. Cohen;Jacob Richman
#t 2002
#c 0
#% 55490
#% 201889
#% 266215
#% 301169
#% 310516
#% 312870
#% 314740
#% 420495
#% 431501
#% 438103
#% 1272396
#! Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems, and is nearly always competitive with the best baseline system.

#index 577264
#* SECRET: a scalable linear regression tree algorithm
#@ Alin Dobra;Johannes Gehrke
#t 2002
#c 0
#% 80995
#% 131402
#% 136350
#% 420084
#% 458382
#% 465897
#% 479787
#! Developing regression models for large datasets that are both accurate and easy to interpret is a very important data mining problem. Regression trees with linear models in the leaves satisfy both these requirements, but thus far, no truly scalable regression tree algorithm is known. This paper proposes a novel regression tree construction algorithm (SECRET) that produces trees of high quality and scales to very large datasets. At every node, SECRET uses the EM algorithm for Gaussian mixtures to find two clusters in the data and to locally transform the regression problem into a classification problem based on closeness to these clusters. Goodness of split measures, like the gini gain, can then be used to determine the split variable and the split point much like in classification tree construction. Scalability of the algorithm can be achieved by employing scalable versions of the EM and classification tree construction algorithms. An experimental evaluation on real and artificial data shows that SECRET has accuracy comparable to other linear regression tree algorithms but takes orders of magnitude less computation time for large datasets.

#index 577265
#* Statistical modeling of large-scale simulation data
#@ Tina Eliassi-Rad;Terence Critchlow;Ghaleb Abdulla
#t 2002
#c 0
#% 15245
#% 274152
#% 296822
#% 308500
#% 337432
#% 480306
#% 566128
#! With the advent of fast computer systems, scientists are now able to generate terabytes of simulation data. Unfortunately, the sheer size of these data sets has made efficient exploration of them impossible. To aid scientists in gleaning insight from their simulation data, we have developed an ad-hoc query infrastructure. Our system, called AQSim (short for Ad-hoc Queries for Simulation) reduces the data storage requirements and query access times in two stages. First, it creates and stores mathematical and statistical models of the data at multiple resolutions. Second, it evaluates queries on the models of the data instead of on the entire data set. In this paper, we present two simple but effective statistical modeling techniques for simulation data. Our first modeling technique computes the "true" (unbiased) mean of systematic partitions of the data. It makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model. Our second statistical modeling technique uses the Andersen-Darling goodness-of-fit method on systematic partitions of the data. This method evaluates a model by how well it passes the normality test on the data. Both of our statistical models effectively answer range queries. At each resolution of the data, we compute the precision of our answer to the user's query by scaling the one-sided Chebyshev Inequalities with the original mesh's topology. We combine precisions at different resolutions by calculating their weighted average. Our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets.

#index 577266
#* Tumor cell identification using features rules
#@ Bin Fang;Wynne Hsu;Mong Li Lee
#t 2002
#c 0
#% 80995
#% 136350
#% 251145
#% 301174
#% 625759
#% 1784199
#! Advances in imaging techniques have led to large repositories of images. There is an increasing demand for automated systems that can analyze complex medical images and extract meaningful information for mining patterns. Here, we describe a real-life image mining application to the problem of tumour cell counting. The quantitative analysis of tumour cells is fundamental to characterizing the activity of tumour cells. Existing approaches are mostly manual, time-consuming and subjective. Efforts to automate the process of cell counting have largely focused on using image processing techniques only. Our studies indicate that image processing alone is unable to give accurate results. In this paper, we examine the use of extracted features rules to aid in the process of tumor cell counting. We propose a robust local adaptive thresholding and dynamic water immersion algorithms to segment regions of interesting from background. Meaningful features are then extracted from the segmented regions. A number of base classifiers are built to generate features rules to help identify the tumor cell. Two voting strategies are implemented to combine the base classifiers into a meta-classifier. Experiment results indicate that this process of using extracted features rules to help identify tumor cell leads to better accuracy than pure image processing techniques alone.

#index 577267
#* Integrating feature and instance selection for text classification
#@ Dimitris Fragoudis;Dimitris Meretakis;Spiros Likothanassis
#t 2002
#c 0
#% 169717
#% 194267
#% 243727
#% 246247
#% 246832
#% 260001
#% 316474
#% 458379
#% 465754
#% 465760
#% 466419
#% 817843
#! Instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data. Feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances. So far, these two methods have mostly been considered in isolation. In this paper, we present a new algorithm, which we call FIS (Feature and Instance Selection) that targets both problems simultaneously in the context of text classificationOur experiments on the Reuters and 20-Newsgroups datasets show that FIS considerably reduces both the number of features and the number of instances. The accuracy of a range of classifiers including Naïve Bayes, TAN and LB considerably improves when using the FIS preprocessed datasets, matching and exceeding that of Support Vector Machines, which is currently considered to be one of the best text classification methods. In all cases the results are much better compared to Mutual Information based feature selection. The training and classification speed of all classifiers is also greatly improved.

#index 577268
#* SyMP: an efficient clustering approach to identify clusters of arbitrary shapes in large data sets
#@ Hichem Frigui
#t 2002
#c 0
#% 210173
#% 248790
#% 248792
#% 280407
#% 320942
#% 324274
#% 369349
#% 462243
#% 466667
#% 479799
#% 481281
#% 566128
#% 1788919
#! We propose a new clustering algorithm, called SyMP, which is based on synchronization of pulse-coupled oscillators. SyMP represents each data point by an Integrate-and-Fire oscillator and uses the relative similarity between the points to model the interaction between the oscillators. SyMP is robust to noise and outliers, determines the number of clusters in an unsupervised manner, identifies clusters of arbitrary shapes, and can handle very large data sets. The robustness of SyMP is an intrinsic property of the synchronization mechanism. To determine the optimum number of clusters, SyMP uses a dynamic resolution parameter. To identify clusters of various shapes, SyMP models each cluster by multiple Gaussian components. The number of components is automatically determined using a dynamic intra-cluster resolution parameter. Clusters with simple shapes would be modeled by few components while clusters with more complex shapes would require a larger number of components. The scalable version of SyMP uses an efficient incremental approach that requires a simple pass through the data set. The proposed clustering approach is empirically evaluated with several synthetic and real data sets, and its performance is compared with CURE.

#index 577269
#* Scaling multi-class support vector machines using inter-class confusion
#@ Shantanu Godbole;Sunita Sarawagi;Soumen Chakrabarti
#t 2002
#c 0
#% 190581
#% 260001
#% 269221
#% 340903
#% 465747
#% 466737
#% 466762
#% 571073
#% 1272365
#! Support vector machines (SVMs) excel at two-class discriminative learning problems. They often outperform generative classifiers, especially those that use inaccurate generative models, such as the naïve Bayes (NB) classifier. On the other hand, generative classifiers have no trouble in handling an arbitrary number of classes efficiently, and NB classifiers train much faster than SVMs owing to their extreme simplicity. In contrast, SVMs handle multi-class problems by learning redundant yes/no (one-vs-others) classifiers for each class, further worsening the performance gap. We propose a new technique for multi-way classification which exploits the accuracy of SVMs and the speed of NB classifiers. We first use a NB classifier to quickly compute a confusion matrix, which is used to reduce the number and complexity of the two-class SVMs that are built in the second stage. During testing, we first get the prediction of a NB classifier and use that to selectively apply only a subset of the two-class SVMs. On standard benchmarks, our algorithm is 3 to 6 times faster than SVMs and yet matches or even exceeds their accuracy.

#index 577270
#* Visualization support for a user-centered KDD process
#@ TuBao Ho;TrongDung Nguyen;DungDuc Nguyen
#t 2002
#c 0
#% 96296
#% 270633
#% 310531
#% 340739
#% 464712
#% 661263
#! Viewing knowledge discovery as a user-centered process that requires an effective collaboration between the user and the discovery system, our work aims to support an active role of the user in that process by developing synergistic visualization tools integrated in our discovery system D2MS. These tools provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing, selecting mining algorithms and parameters, evaluating and comparing discovered models, and taking control of the whole discover process. Our case-studies with two medical datasets on meningitis and stomach cancer show that, with visualization tools in D2MS, the user gains better insight in each step of the knowledge discovery process as well the relationship between data and discovered knowledge.

#index 577271
#* Mining complex models from arbitrarily large databases in constant time
#@ Geoff Hulten;Pedro Domingos
#t 2002
#c 0
#% 170649
#% 197387
#% 211583
#% 280406
#% 310500
#% 338609
#% 342600
#% 420137
#% 464294
#% 722802
#% 1650289
#% 1650785
#! In this paper we propose a scaling-up method that is applicable to essentially any induction algorithm based on discrete search. The result of applying the method to an algorithm is that its running time becomes independent of the size of the database, while the decisions made are essentially identical to those that would be made given infinite data. The method works within pre-specified memory limits and, as long as the data is iid, only requires accessing it sequentially. It gives anytime results, and can be used to produce batch, stream, time-changing and active-learning versions of an algorithm. We apply the method to learning Bayesian networks, developing an algorithm that is faster than previous ones by orders of magnitude, while achieving essentially the same predictive performance. We observe these gains on a series of large databases "generated from benchmark networks, on the KDD Cup 2000 e-commerce data, and on a Web log containing 100 million requests.

#index 577272
#* A model for discovering customer value for E-content
#@ Srinivasan Jagannathan;Jayanth Nayak;Kevin Almeroth;Markus Hofmann
#t 2002
#c 0
#% 191906
#% 253310
#% 283497
#% 316270
#% 434677
#% 476050
#% 495782
#% 629267
#% 1789118
#! There exists a huge demand for multimedia goods and services in the Internet. Currently available bandwidth speeds can support sale of downloadable content like CDs, e-books, etc. as well as services like video-on-demand. In the future, such services will be prevalent in the Internet. Since costs are typically fixed, maximizing revenue can maximize profits. A primary determinant of revenue in such e-content markets is how much value the customers associate with the content. Though marketing surveys are useful, they cannot adapt to the dynamic nature of the Internet market. In this work, we examine how to learn customer valuations in close to real-time. Our contributions in this paper are threefold: (1) we develop a probabilistic model to describe customer behavior, (2) we develop a framework for pricing e-content based on basic economic principles, and (3) we propose a price discovering algorithm that learns customer behavior parameters and suggests prices to an e-content provider. We validate our algorithm using simulations. Our simulations indicate that our algorithm generates revenue close to the maximum expectation. Further, they also indicate that the algorithm is robust to transient customer behavior.

#index 577273
#* SimRank: a measure of structural-context similarity
#@ Glen Jeh;Jennifer Widom
#t 2002
#c 0
#% 124010
#% 135571
#% 202011
#% 220711
#% 282905
#% 387427
#! The problem of measuring "similarity" of objects arises in many applications, and many domain-specific measures have been developed, e.g., matching text across documents or computing overlap among item-sets. We propose a complementary approach, applicable in any domain with object-to-object relationships, that measures similarity of the structural context in which objects occur, based on their relationships with other objects. Effectively, we compute a measure that says "two objects are similar if they are related to similar objects:" This general similarity measure, called SimRank, is based on a simple and intuitive graph-theoretic model. For a given domain, SimRank can be combined with other domain-specific similarity measures. We suggest techniques for efficient computation of SimRank scores, and provide experimental results on two application domains showing the computational feasibility and effectiveness of our approach.

#index 577274
#* Similarity measure based on partial information of time series
#@ Xiaoming Jin;Yuchang Lu;Chunyi Shi
#t 2002
#c 0
#% 80995
#% 169805
#% 191581
#% 227857
#% 310580
#% 310583
#% 316709
#% 461885
#% 481609
#% 631923
#! Similarity measure of time series is an important subroutine in many KDD applications. Previous similarity models mainly focus on the prominent series behaviors by considering the whole information of time series. In this paper, we address the problem: which portion of information is more suitable for similarity measure for the data collected from a certain field. We propose a model for the retrieval and representation of the partial information in time series data, and a methodology for evaluating the similarity measurements based on partial information. The methodology is to retrieve various portions of information from the raw data and represent it in a concise form, then cluster the time series using the partial information and evaluate the similarity measurements through comparing the results with a standard classification. Experiments on data set from stock market give some interesting observations and justify the usefulness of our approach.

#index 577275
#* Finding surprising patterns in a time series database in linear time and space
#@ Eamonn Keogh;Stefano Lonardi;Bill 'Yuan-chi' Chiu
#t 2002
#c 0
#% 172949
#% 235941
#% 280413
#% 285711
#% 289010
#% 310502
#% 333941
#% 397629
#% 479785
#% 515979
#% 566132
#% 593764
#% 617888
#% 632088
#% 641125
#% 715230
#! The problem of finding a specified pattern in a time series database (i.e. query by content) has received much attention and is now a relatively mature field. In contrast, the important problem of enumerating all surprising or interesting patterns has received far less attention. This problem requires a meaningful definition of "surprise", and an efficient search technique. All previous attempts at finding surprising patterns in time series use a very limited notion of surprise, and/or do not scale to massive datasets. To overcome these limitations we introduce a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance, given some previously seen data.

#index 577276
#* Clustering seasonality patterns in the presence of errors
#@ Mahesh Kumar;Nitin R. Patel;Jonathan Woo
#t 2002
#c 0
#% 36672
#% 280416
#% 296738
#% 481609
#% 573121
#% 573132
#! Clustering is a very well studied problem that attempts to group similar data points. Most traditional clustering algorithms assume that the data is provided without measurement error. Often, however, real world data sets have such errors and one can obtain estimates of these errors. We present a clustering method that incorporates information contained in these error estimates. We present a new distance function that is based on the distribution of errors in data. Using a Gaussian model for errors, the distance function follows a Chi-Square distribution and is easy to compute. This distance function is used in hierarchical clustering to discover meaningful clusters. The distance function is scale-invariant so that clustering results are independent of units of measuring data. In the special case when the error distribution is the same for each attribute of data points, the rank order of pair-wise distances is the same for our distance function and the Euclidean distance function. The clustering method is applied to the seasonality estimation problem and experimental results are presented for the retail industry data as well as for simulated data, where it outperforms classical clustering methods.

#index 577277
#* Construct robust rule sets for classification
#@ Jiuyong Li;Rodney Topor;Hong Shen
#t 2002
#c 0
#% 99396
#% 136350
#% 152934
#% 208268
#% 209021
#% 235377
#% 280439
#% 280487
#% 376266
#% 449568
#% 466483
#% 481290
#! We study the problem of computing classification rule sets from relational databases so that accurate predictions can be made on test data with missing attribute values. Traditional classifiers perform badly when test data are not as complete as the training data because they tailor a training database too much. We introduce the concept of one rule set being more robust than another, that is, able to make more accurate predictions on test data with missing attribute values. We show that the optimal class association rule set is as robust as the complete class association rule set. We then introduce the k-optimal rule set, which provides predictions exactly the same as the optimal class association rule set on test data with up to k missing attribute values. This leads to a hierarchy of k-optimal rule sets in which decreasing size corresponds to decreasing robustness, and they all more robust than a traditional classification rule set. We introduce two methods to find k-optimal rule sets, i.e. an optimal association rule mining approach and a heuristic approximate approach. We show experimentally that a k-optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule sets perform significantly better than a typical classification rule set (C4.5Rules) on incomplete test data.

#index 577278
#* Instability of decision tree classification algorithms
#@ Ruey-Hsia Li;Geneva G. Belford
#t 2002
#c 0
#% 209021
#% 225872
#% 273900
#% 449588
#% 481949
#% 652807
#! The instability problem of decision tree classification algorithms is that small changes in input training samples may cause dramatically large changes in output classification rules. Different rules generated from almost the same training samples are against human intuition and complicate the process of decision making. In this paper, we present fundamental theorems for the instability problem of decision tree classifiers. The first theorem gives the relationship between a data change and the resulting tree structure change (i.e. split change). The second theorem, Instability Theorem, provides the cause of the instability problem. Based on the two theorems, algorithmic improvements can be made to lessen the instability problem. Empirical results illustrate the theorem statements. The trees constructed by the proposed algorithm are more stable, noise-tolerant, informative, expressive, and concise. Our proposed sensitivity measure can be used as a metric to evaluate the stability of splitting predicates. The tree sensitivity is an indicator of the confidence level in rules and the effective lifetime of rules.

#index 577279
#* Distributed data mining in a chain store database of short transactions
#@ Cheng-Ru Lin;Chang-Hung Lee;Ming-Syan Chen;Philip S. Yu
#t 2002
#c 0
#% 152934
#% 273899
#% 280487
#% 300120
#% 342605
#% 342610
#% 342689
#% 443082
#% 443164
#% 443194
#% 463903
#% 464996
#% 466650
#% 479817
#% 481758
#! In this paper, we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database. Specifically, the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events, and is designed with the capability of mining non-sequential, inter-transaction information. Hence, the causality rule mining provides a very general framework for rule derivation. Note, however, that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate sets and a distributed database, and in our opinion, cannot be dealt with by direct extensions from existing rule mining methods. Consequently, we devise in this paper a series of level matching algorithms, including Level Matching (abbreviatedly as LM), Level Matching with Selective Scan (abbreviatedly as LMS), and Distributed Level Matching (abbreviatedly as Distibuted LM), to minimize the computing cost needed for the distributed data mining of causality rules. In addition, the phenomena of time window constraints are also taken into consideration for the development of our algorithms. As a result of properly employing the technologies of level matching and selective scan, the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules. Scale-up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions.Index Terms: knowledge discovery, distributed data mining causality rules, triggering events, consequential events

#index 577280
#* A robust and efficient clustering algorithm based on cohesion self-merging
#@ Cheng-Ru Lin;Ming-Syan Chen
#t 2002
#c 0
#% 33306
#% 91872
#% 210173
#% 248790
#% 296738
#% 300132
#% 443082
#% 481281
#% 483675
#% 631985
#! Data clustering has attracted a lot of research attention in the field of computational statistics and data mining. In most related studies, the dissimilarity between two clusters is defined as the distance between their centroids, or the distance between two closest (or farthest) data points. However, all of these measurements are vulnerable to outliers, and removing the outliers precisely is yet another difficult task. In view of this, we propose a new similarity measurement referred to as cohesion, to measure the inter-cluster distances. By using this new measurement of cohesion, we design a two-phase clustering algorithm, called cohesion-based self-merging (abbreviated as CSM), which runs in linear time to the size of input data set. Combining the features of partitional and hierarchical clustering methods, algorithm CSM partitions the input data set into several small subclusters in the first phase, and then continuously merges the subclusters based on cohesion in a hierarchical manner in the second phase. As shown by our performance studies, the cohesion-based clustering is very robust and possesses the excellent tolerance to outliers in various workloads. More importantly, algorithm CSM is shown to be able to cluster the data sets of arbitrary shapes very efficiently, and provide better clustering results than those by prior methods.Index Terms: Data mining, data clustering, hierarchical clustering, partitional clustering

#index 577281
#* Discovering informative content blocks from Web documents
#@ Shian-Hua Lin;Jan-Ming Ho
#t 2002
#c 0
#% 21992
#% 67565
#% 84654
#% 115462
#% 210985
#% 232650
#% 268079
#% 275915
#% 290830
#% 320930
#% 330676
#% 443349
#% 705442
#% 709066
#! In this paper, we propose a new approach to discover informative contents from a set of tabular documents (or Web pages) of a Web site. Our system, InfoDiscoverer, first partitions a page into several content blocks according to HTML tag in a Web page. Based on the occurrence of the features (terms) in the set of pages, it calculates entropy value of each feature. According to the entropy value of each feature in a content block, the entropy value of the block is defined. By analyzing the information measure, we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant. Informative content blocks are distinguished parts of the page, whereas redundant content blocks are common parts. Based on the answer set generated from 13 manually tagged news Web sites with a total of 26,518 Web pages, experiments show that both recall and precision rates are greater than 0.956. That is, using the approach, informative blocks (news articles) of these sites can be automatically separated from semantically redundant contents such as advertisements, banners, navigation panels, news categories, etc. By adopting InfoDiscoverer as the preprocessor of information retrieval and extraction applications, the retrieval and extracting precision will be increased, and the indexing size and extracting complexity will also be reduced.

#index 577282
#* Collusion in the U.S. crop insurance program: applied data mining
#@ Bertis B. Little;Walter L. Johnston;Ashley C. Lovell;Roderick M. Rejesus;Steve A. Steed
#t 2002
#c 0
#% 243449
#% 252533
#% 290482
#% 420126
#% 486316
#! This paper quantitatively analyzes indicators of Agent (policy seller), Adjuster (indemnity claim adjuster), Producer (policy purchaser/holder) indemnity behavior suggestive of collusion in the United States Department of Agriculture (USDA) Risk Management Agency (RMA) national crop insurance program. According to guidance from the federal law and using six indicator variables of indemnity behavior, those entities equal to or exceeding 150% of the county mean (computed using a simple jackknife procedure) on all entity-relevant indicators were flagged as "anomalous." Log linear analysis was used to test (I) hierarchical node-node arrangements and (2) a non-recursive model of node information sharing. Chi-square distributed deviance statistic identified the optimal log linear model. The results of the applied data mining technique used here suggest that the non-recursive triplet and Agent-producer doublet collusion probabilistically accounts for the greatest proportion of waste, fraud, and abuse in the federal crop insurance program. Triplet and Agent-producer doublets need detailed investigation for possible collusion. Hence, this data mining technique provided a high level of confidence when 24 million records were quantitatively analyzed for possible fraud, waste, or other abuse of the crop insurance program administered by the USDA RMA, and suspect entities reported to USDA. This data mining technique can be applied where vast amounts of data are available to detect patterns of collusion or conspiracy as may be of interest to the criminal justice or intelligence agencies.

#index 577283
#* Incremental context mining for adaptive document classification
#@ Rey-Long Liu;Yun-Ling Lu
#t 2002
#c 0
#% 46809
#% 165110
#% 165111
#% 165115
#% 194289
#% 219051
#% 219053
#% 232653
#% 262050
#% 262085
#% 280817
#% 309141
#% 433674
#% 445382
#% 465747
#% 465754
#% 466078
#! Automatic document classification (DC) is essential for the management of information and knowledge. This paper explores two practical issues in DC: (1) each document has its context of discussion, and (2) both the content and vocabulary of the document database is intrinsically evolving. The issues call for adaptive document classification (ADC) that adapts a DC system to the evolving contextual requirement of each document category, so that input documents may be classified based on their contexts of discussion. We present an incremental context mining technique to tackle the challenges of ADC. Theoretical analyses and empirical results show that, given a text hierarchy, the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category. Based on the contextual requirements mined by the system, higher-precision DC may be achieved with better efficiency.

#index 577284
#* Evaluating classifiers' performance in a constrained environment
#@ Anna Olecka
#t 2002
#c 0
#% 68151
#% 166586
#% 266280
#% 331909
#! In this paper, we focus on methodology of finding a classifier with a minimal cost in presence of additional performance constraints. ROCCH analysis, where accuracy and cost are intertwined in the solution space, was a revolutionary tool for two-class problems. We propose an alternative formulation, as an optimization problem, commonly used in Operations Research. This approach extends the ROCCH analysis to allow for locating optimal solutions while outside constraints are present. Similarly to the ROCCH analysis, we combine cost and class distribution while defining the objective function. Rather than focusing on slopes of the edges in the convex hull of the solution space, however, we treat cost as an objective function to be minimized over the solution space, by selecting the best performing classifier(s) (one or more vertex in the solution space). The Linear Programming framework provides a theoretical and computational methodology for finding the vertex (classifier) which minimizes the objective function.

#index 577285
#* Discovering word senses from text
#@ Patrick Pantel;Dekang Lin
#t 2002
#c 0
#% 118771
#% 226099
#% 279755
#% 296738
#% 340954
#% 342629
#% 406493
#% 438137
#% 631985
#% 747738
#% 748691
#! Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses.

#index 577286
#* Combining clustering and co-training to enhance text classification using unlabelled data
#@ Bhavani Raskutti;Herman Ferrá;Adam Kowalczyk
#t 2002
#c 0
#% 190581
#% 252011
#% 309208
#% 311027
#% 342670
#% 393059
#% 464780
#% 466263
#% 466888
#% 486933
#! In this paper, we present a new co-training strategy that makes use of unlabelled data. It trains two predictors in parallel, with each predictor labelling the unlabelled data for training the other predictor in the next round. Both predictors are support vector machines, one trained using data from the original feature space, the other trained with new features that are derived by clustering both the labelled and unlabelled data. Hence, unlike standard co-training methods, our method does not require a priori the existence of two redundant views either of which can be used for classification, nor is it dependent on the availability of two different supervised learning algorithms that complement each other.We evaluated our method with two classifiers and three text benchmarks: WebKB, Reuters newswire articles and 20 NewsGroups. Our evaluation shows that our co-training technique improves text classification accuracy especially when the number of labelled examples are very few.

#index 577287
#* Single-shot detection of multiple categories of text using parametric mixture models
#@ Naonori Ueda;Kazumi Saito
#t 2002
#c 0
#% 55490
#% 190581
#% 211820
#% 260001
#% 279755
#% 280819
#% 311027
#% 458379
#% 465754
#% 466229
#! In this paper, we address the problem of detecting multiple topics or categories of text where each text is not assumed to belong to one of a number of mutually exclusive categories. Conventionally, the binary classification approach has been employed, in which whether or not text belongs to a category is judged by the binary classifier for every category. In this paper, we propose a more sophisticated approach to simultaneously detect multiple categories of text using parametric mixture models (PMMs), newly presented in this paper. PMMs are probabilistic generative models for text that has multiple categories. Our PMMs are essentially different from the conventional mixture of multinomial distributions in the sense that in the former several basis multinomial parameters are mixed in the parameter space, while in the latter several multinomial components are mixed. We derive efficient learning algorithms for PMMs within the framework of the maximum a posteriori estimate. We also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of World Wide Web pages, focusing on those from the "yahoo.com" domain.

#index 577288
#* What's the code?: automatic classification of source code archives
#@ Secil Ugurel;Robert Krovetz;C. Lee Giles
#t 2002
#c 0
#% 116575
#% 191046
#% 211244
#% 260001
#% 312876
#% 338577
#% 437795
#% 458379
#% 465754
#% 615723
#% 1558464
#! There are various source code archives on the World Wide Web. These archives are usually organized by application categories and programming languages. However, manually organizing source code repositories is not a trivial task since they grow rapidly and are very large (on the order of terabytes). We demonstrate machine learning methods for automatic classification of archived source code into eleven application topics and ten programming languages. For topical classification, we concentrate on C and C++ programs from the Ibiblio and the Sourceforge archives. Support vector machine (SVM) classifiers are trained on examples of a given programming language or programs in a specified category. We show that source code can be accurately and automatically classified into topical categories and can be identified to be in a specific programming language class.

#index 577289
#* Privacy preserving association rule mining in vertically partitioned data
#@ Jaideep Vaidya;Chris Clifton
#t 2002
#c 0
#% 23638
#% 152934
#% 225872
#% 300184
#% 333876
#% 346201
#% 443085
#% 466345
#% 481290
#% 637062
#% 703747
#! Privacy considerations often constrain data mining projects. This paper addresses the problem of association rule mining where transactions are distributed across sources. Each site holds some attributes of each transaction, and the sites wish to collaborate to identify globally valid association rules. However, the sites must not reveal individual transaction data. We present a two-party algorithm for efficiently discovering frequent itemsets with minimum support levels, without either site revealing individual transaction values.

#index 577290
#* Non-linear dimensionality reduction techniques for classification and visualization
#@ Michail Vlachos;Carlotta Domeniconi;Dimitrios Gunopulos;George Kollios;Nick Koudas
#t 2002
#c 0
#% 25351
#% 86950
#% 136350
#% 172949
#% 190429
#% 201893
#% 209623
#% 333941
#% 376589
#% 460862
#% 481349
#% 631923
#% 641079
#! In this paper we address the issue of using local embeddings for data visualization in two and three dimensions, and for classification. We advocate their use on the basis that they provide an efficient mapping procedure from the original dimension of the data, to a lower intrinsic dimension. We depict how they can accurately capture the user's perception of similarity in high-dimensional data for visualization purposes. Moreover, we exploit the low-dimensional mapping provided by these embeddings, to develop new classification techniques, and we show experimentally that the classification accuracy is comparable (albeit using fewer dimensions) to a number of other classification procedures.

#index 577291
#* Item selection by "hub-authority" profit ranking
#@ Ke Wang;Ming-Yen Thomas Su
#t 2002
#c 0
#% 152934
#% 280456
#% 282905
#% 310548
#% 481290
#! A fundamental problem in business and other applications is ranking items with respect to some notion of profit based on historical transactions. The difficulty is that the profit of one item not only comes from its own sales, but also from its influence on the sales of other items, i.e., the "cross-selling effect". In this paper, we draw an analogy between this influence and the mutual reinforcement of hub/authority web pages. Based on this analogy, we present a novel approach to the item ranking problem.We apply this ranking approach to solve two selection problems. In size-constrained selection, the maximum number of items that can be selected is fixed. In cost-constrained selection, there is no maximum number of items to be selected, but there is some cost associated with the selection of each item. In both cases, the question is what items should be selected to maximize the profit. Empirically, we show that this method finds profitable items in the presence of cross-selling effect.

#index 577292
#* Discovery net: towards a grid of knowledge discovery
#@ V. Ćurčin;M. Ghanem;Y. Guo;M. Köhler;A. Rowe;J. Syed;P. Wendel
#t 2002
#c 0
#% 232102
#% 266629
#% 468927
#% 550397
#% 822359
#! This paper provides a blueprint for constructing collaborative and distributed knowledge discovery systems within Grid-based computing environments. The need for such systems is driven by the quest for sharing knowledge, information and computing resources within the boundaries of single large distributed organisations or within complex Virtual Organisations (VO) created to tackle specific projects. The proposed architecture is built on top of a resource federation management layer and is composed of a set of different resources. We show how this architecture will behave during a typical KDD process design and deployment, how it enables the execution of complex and distributed data mining tasks with high performance and how it provides a community of e-scientists with means to collaborate, retrieve and reuse both KDD algorithms, discovery processes and knowledge in a visual analytical environment.

#index 577293
#* Making every bit count: fast nonlinear axis scaling
#@ Leejay Wu;Christos Faloutsos
#t 2002
#c 0
#% 201893
#% 265099
#% 266426
#% 333954
#% 342603
#% 476569
#% 480307
#% 481620
#! Existing axis scaling and dimensionality methods focus on preserving structure, usually determined via the Euclidean distance. In other words, they inherently assume that the Euclidean distance is already correct. We instead propose a novel nonlinear approach driven by an information-theoretic viewpoint, which we show is also strongly linked to intrinsic dimensionality, or degrees of freedom; and uniformity. Nonlinear transformations based on common probability distributions, combined with information-driven selection, simultaneously reduce the number of dimensions required and increase the value of those we retain. Experiments on real data confirm that this approach reveals correlations, finds novel attributes, and scales well.

#index 577294
#* B-EM: a classifier incorporating bootstrap with EM approach for data mining
#@ Xintao Wu;Jianping Fan;Kalpathi R. Subramanian
#t 2002
#c 0
#% 4868
#% 192878
#% 232117
#% 273900
#% 311027
#% 369236
#% 428155
#% 449588
#% 459008
#% 481945
#% 1808946
#! This paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy. This is significant for many applications such as image classification where obtaining classification labels is expensive, while large unlabeled examples are easily available. We investigate an Expectation Maximization (EM) algorithm for learning from labeled and unlabeled data. The reason why unlabeled data boosts learning accuracy is because it provides the information about the joint probability distribution. A theoretical argument shows that the more unlabeled examples are combined in learning, the more accurate the result. We then introduce B-EM algorithm, based on the combination of EM with bootstrap method, to exploit the large unlabeled data while avoiding prohibitive I/O cost. Experimental results over both synthetic and real data sets that the proposed approach has a satisfactory performance.

#index 577295
#* A unifying framework for detecting outliers and change points from non-stationary time series data
#@ Kenji Yamanishi;Jun-ichi Takeuchi
#t 2002
#c 0
#% 269195
#% 280408
#% 280413
#% 310552
#% 342641
#% 477809
#% 479791
#% 1808676
#! We are concerned with the issues of outlier detection and change point detection from a data stream. In the area of data mining, there have been increased interest in these issues since the former is related to fraud detection, rare event discovery, etc., while the latter is related to event/trend by change detection, activity monitoring, etc. Specifically, it is important to consider the situation where the data source is non-stationary, since the nature of data source may change over time in real applications. Although in most previous work outlier detection and change point detection have not been related explicitly, this paper presents a unifying framework for dealing with both of them on the basis of the theory of on-line learning of non-stationary time series. In this framework a probabilistic model of the data source is incrementally learned using an on-line discounting learning algorithm, which can track the changing data source adaptively by forgetting the effect of past data gradually. Then the score for any given data is calculated to measure its deviation from the learned model, with a higher score indicating a high possibility of being an outlier. Further change points in a data stream are detected by applying this scoring method into a time series of moving averaged losses for prediction using the learned model. Specifically we develop an efficient algorithms for on-line discounting learning of auto-regression models from time series data, and demonstrate the validity of our framework through simulation and experimental applications to stock market data analysis.

#index 577296
#* CLOPE: a fast and effective clustering algorithm for transactional data
#@ Yiling Yang;Xudong Guan;Jinyuan You
#t 2002
#c 0
#% 152934
#% 210173
#% 248792
#% 280419
#% 287285
#% 300120
#% 479659
#% 481281
#! This paper studies the problem of categorical data clustering, especially for transactional data characterized by high dimensionality and large volume. Starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram, we develop a novel algorithm -- CLOPE, which is very fast and scalable, while being quite effective. We demonstrate the performance of our algorithm on two real world datasets, and compare CLOPE with the state-of-art algorithms.

#index 577297
#* Topic-conditioned novelty detection
#@ Yiming Yang;Jian Zhang;Jaime Carbonell;Chun Jin
#t 2002
#c 0
#% 262042
#% 262085
#% 316546
#% 318412
#% 465754
#% 466572
#% 742424
#! Automated detection of the first document reporting each new event in temporally-sequenced streams of documents is an open challenge. In this paper we propose a new approach which addresses this problem in two stages: 1) using a supervised learning algorithm to classify the on-line document stream into pre-defined broad topic categories, and 2) performing topic-conditioned novelty detection for documents in each topic. We also focus on exploiting named-entities for event-level novelty detection and using feature-based heuristics derived from the topic histories. Evaluating these methods using a set of broadcast news stories, our results show substantial performance gains over the traditional one-level approach to the novelty detection problem.

#index 577298
#* Transforming classifier scores into accurate multiclass probability estimates
#@ Bianca Zadrozny;Charles Elkan
#t 2002
#c 0
#% 272518
#% 342611
#% 342647
#% 464280
#% 466737
#% 1272365
#! Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.

#index 645895
#* Kdd-2001: Proceedings of the Seventh ACM Sigkdd International Conference on Knowledge Discovery and Data Mining : August 26-29, 2001 San Francisco, CA, USA
#@ 
#t 2002
#c 0

#index 729906
#* Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Lise Getoor;Ted Senator;Pedro Domingos;Christos Faloutsos
#t 2003
#c 0
#! KDD-2003, the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, was held in Washington, DC, U.S.A., on August 24-27, 2003. KDD is the leading international forum for the exchange of research results and practical experience in field of knowledge discovery and data mining. As the mountains of data available to organizations and individuals continue to grow without limit, and the need to extract useful knowledge from them becomes ever more intense, scientists, government workers and business people turn to the KDD community for solutions. The volume you have in your hands (or on your screen) contains a snapshot of a year of developments in this field; we hope you find it useful and rewarding.The KDD-2003 technical program featured two parallel research tracks and an industrial/government track. The latter was the result of expanding the scope of KDD's industrial track to reflect the increased importance of knowledge discovery and data mining in government and vice-versa. The program also featured three keynote speakers, nine workshops, seven tutorials and two panels. The 2003 KDD Cup competition focused on mining citation networks and data cleaning in large bibliographic repositories. Dozens of exhibits from vendors and other organizations added to the ferment, and underscored the conference's dual role as an industry and academic event.Continuing its tradition of collocation with related conferences, this year KDD was collocated with ICML-2003, the Twentieth International Conference on Machine Learning. The two conferences held a joint session on the first day of KDD (last of ICML), which featured a selection of papers from the two conferences and a joint keynote speaker. COLT-2003, the Sixteenth Annual Conference on Computational Learning Theory, was also collocated with KDD and ICML.We received a large number of submissions, and the selection process was extremely competitive. Each paper was independently reviewed by three members of the program committee for originality, significance, technical quality, and clarity of presentation. This was followed by discussion among the reviewers and final decisions. Of the 258 research track submissions received, 34 were accepted as full papers for oral presentation, and 36 were accepted for poster presentation (13% and 14% of submissions, respectively). The industrial/government track received 40 submissions, of which 12 were accepted for oral presentation and 10 were accepted for poster presentation (30% and 25%). Additionally, ne research submission was reassigned to the industrial/government track, and accepted for oral presentation there.The resulting program was notable for its diversity and vitality. Alongside traditional KDD topics like classification, clustering, frequent sets, scalability, and temporal data, we also saw papers in rapidly growing areas like graph and relational mining, data streams, semi-structured data, and privacy. Application areas included the Web, bioinformatics, health care, marketing, crime-fighting, and many others.

#index 729907
#* On-line science: the world-wide telescope as a prototype for the new computational science
#@ Jim Gray
#t 2003
#c 0

#index 729908
#* Statistical learning from relational data
#@ Daphne Koller
#t 2003
#c 0

#index 729909
#* Analyzing customer behavior at Amazon.com
#@ Andreas S. Weigend
#t 2003
#c 0

#index 729910
#* Towards systematic design of distance functions for data mining applications
#@ Charu C. Aggarwal
#t 2003
#c 0
#% 4868
#% 218982
#% 310509
#% 332094
#% 334059
#% 406493
#% 436509
#% 465017
#% 477968
#% 480132
#% 480302
#% 481609
#% 482109
#! Distance function computation is a key subtask in many data mining algorithms and applications. The most effective form of the distance function can only be expressed in the context of a particular data domain. It is also often a challenging and non-trivial task to find the most effective form of the distance function. For example, in the text domain, distance function design has been considered such an important and complex issue that it has been the focus of intensive research over three decades. The final design of distance functions in this domain has been reached only by detailed empirical testing and consensus over the quality of results provided by the different variations. With the increasing ability to collect data in an automated way, the number of new kinds of data continues to increase rapidly. This makes it increasingly difficult to undertake such efforts for each and every new data type. The most important aspect of distance function design is that since a human is the end-user for any application, the design must satisfy the user requirements with regard to effectiveness. This creates the need for a systematic framework to design distance functions which are sensitive to the particular characteristics of the data domain. In this paper, we discuss such a framework. The goal is to create distance functions in an automated waywhile minimizing the work required from the user. We will show that this framework creates distance functions which are significantly more effective than popularly used functions such as the Euclidean metric.

#index 729911
#* Generative model-based clustering of directional data
#@ Arindam Banerjee;Inderjit Dhillon;Joydeep Ghosh;Suvrit Sra
#t 2003
#c 0
#% 36672
#% 115608
#% 269201
#% 277483
#% 304917
#% 311027
#% 330687
#% 332094
#% 425010
#% 450876
#% 593926
#% 593960
#% 629666
#! High dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data. A natural model for multi-variate directional data is provided by the von Mises-Fisher (vMF) distribution on the unit hypersphere that is analogous to the multi-variate Gaussian distribution in Rd. In this paper, we propose modeling complex directional data as a mixture of vMF distributions. We derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the parameters of this mixture. We also propose two clustering algorithms corresponding to these variants. An interesting aspect of our methodology is that the spherical kmeans algorithm (kmeans with cosine similarity) can be shown to be a special case of both our algorithms. Thus, modeling text data by vMF distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community. As part of experimental validation, we present results on modeling high-dimensional text and gene-expression data as a mixture of vMF distributions. The results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces.

#index 729912
#* Mining distance-based outliers in near linear time with randomization and a simple pruning rule
#@ Stephen D. Bay;Mark Schwabacher
#t 2003
#c 0
#% 289519
#% 300136
#% 300183
#% 321455
#% 333929
#% 425014
#% 427199
#% 478624
#% 479986
#% 481956
#% 570886
#! Defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set. Recently, much work has been conducted with the goal of finding fast algorithms for this task. We show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used. We test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude. Our average case analysis suggests that much of the efficiency is because the time to process non-outliers, which are the majority of examples, does not depend on the size of the data set.

#index 729913
#* Adaptive duplicate detection using learnable string similarity measures
#@ Mikhail Bilenko;Raymond J. Mooney
#t 2003
#c 0
#% 190581
#% 201889
#% 235941
#% 251405
#% 269217
#% 290482
#% 310516
#% 310533
#% 387427
#% 464280
#% 466240
#% 466263
#% 577238
#% 577247
#% 577263
#! The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques.

#index 729914
#* An iterative hypothesis-testing strategy for pattern discovery
#@ Richard J. Bolton;Niall M. Adams
#t 2003
#c 0
#% 152934
#% 341700
#% 342597
#% 417589
#% 443092
#% 443313
#% 466494
#% 546703
#% 546821
#% 577226
#% 1562061
#! Pattern discovery has emerged as a direct result of increased data storage and analytic capabilities available to the data analyst. Without a massive amount of data, we do not have the evidence to support the discovery of the local deterministic structures that we call patterns. As such, pattern discovery is one of the few areas of data mining that cannot be considered simply as a 'scaling-up' of current statistical methodology to analyze large data sets. However, the philosophies of hypothesis testing and modeling in traditional statistics do lend themselves to forming a framework for pattern discovery, and we can also draw from ideas relating to outlier discovery and residual analysis to discover patterns. We illustrate an iterative strategy in a statistical framework by way of its application to one simulated and two real data sets.

#index 729915
#* Efficient data reduction with EASE
#@ Hervé Brönnimann;Bin Chen;Manoranjan Dash;Peter Haas;Peter Scheuermann
#t 2003
#c 0
#% 697
#% 152934
#% 205612
#% 300120
#% 310916
#% 345611
#% 464215
#% 481290
#% 481779
#% 577261
#% 678172
#% 729915
#% 993960
#! A variety of mining and analysis problems --- ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes --- involve the extraction of knowledge from a set of categorical count data. Such data can be viewed as a collection of "transactions," where a transaction is a fixed-length vector of counts. Classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow. One effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data. We present a new data-reduction algorithm, called EASE, for producing such a sample. Like the FAST algorithm introduced by Chen et al., EASE is especially designed for count data applications. Both EASE and FAST take a relatively large initial random sample and then deterministically produce a subsample whose "distance" --- appropriately defined --- from the complete database is minimal. Unlike FAST, which obtains the final subsample by quasi-greedy descent, EASE uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving. Experiments both in the context of association rule mining and classical χ2 contingency-table analysis show that EASE outperforms both FAST and simple random sampling, sometimes dramatically.

#index 729916
#* Extracting semantics from data cubes using cube transversals and closures
#@ Alain Casali;Rosine Cicchetti;Lotfi Lakhal
#t 2003
#c 0
#% 125557
#% 197754
#% 209020
#% 237200
#% 310494
#% 316709
#% 333925
#% 376266
#% 384416
#% 399793
#% 411354
#% 420053
#% 420062
#% 429873
#% 452845
#% 458365
#% 480143
#% 480630
#% 660006
#% 993996
#! In this paper we propose a lattice-based approach intended for extracting semantics from datacubes: borders of version spaces for supervised classification, closed cube lattice to summarize the semantics of datacubes w.r.t. COUNT, SUM, and covering graph of the quotient cube as a visualization tool of minimal multidimensional associations. With this intention, we introduce two novel concepts: the cube transversals and the cube closures over the cube lattice of a categorical database relation. We propose a levelwise merging algorithm for mining minimal cube transversals with a single database scan. We introduce the cube connection, show that it is a Galois connection and derive a closure operator over the cube lattice. Using cube transversals and closures, we define a new characterization of boundary sets which provide a condensed representation of version spaces used to enhance supervised classification. The algorithm designed for computing such borders improves the complexity of previous proposals. We also introduce the concept of closed cube lattice and show that it is isomorph to on one hand the Galois lattice and on the other hand the quotient cube w.r.t. COUNT, SUM. Proposed in [16], the quotient cube is a succinct summary of a datacube preserving the Rollup/Drilldown semantics. We show that the quotient cube w.r.t. COUNT, SUM and the closed cube lattice have a similar expression power but the latter has the smallest possible size. Finally we focus on the multidimensional association issue and introduce the covering graph of the quotient cube which provides the user with a visualization tool of minimal multidimensional associations.

#index 729917
#* Translation-invariant mixture models for curve clustering
#@ Darya Chudova;Scott Gaffney;Eric Mjolsness;Padhraic Smyth
#t 2003
#c 0
#% 280416
#% 360691
#% 388024
#% 397631
#% 424809
#% 444047
#% 732538
#% 1051442
#% 1227446
#% 1335352
#% 1672993
#% 1855031
#! In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid. Our approach uses the Expectation-Maximization (EM) algorithm to recover both the mean curve shapes for each cluster, and the most likely shifts, offsets, and cluster memberships for each curve. We demonstrate how Bayesian estimation methods can improve the results for small sample sizes by enforcing smoothness in the cluster mean curves. We evaluate the methodology on two real-world data sets, time-course gene expression data and storm trajectory data. Experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets. The proposed approach provides a non-parametric, computationally efficient, and robust methodology for clustering broad classes of curve data.

#index 729918
#* Information-theoretic co-clustering
#@ Inderjit S. Dhillon;Subramanyam Mallela;Dharmendra S. Modha
#t 2003
#c 0
#% 36672
#% 115608
#% 280819
#% 309128
#% 342621
#% 425010
#% 469422
#% 495929
#% 528174
#% 722934
#! Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text, web-log and market-basket data analysis. A basic problem in contingency table analysis is co-clustering: simultaneous clustering of the rows and columns. A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in information theory---the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters. We present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages. Using the practical example of simultaneous word-document clustering, we demonstrate that our algorithm works well in practice, especially in the presence of sparsity and high-dimensionality.

#index 729919
#* SEWeP: using site semantics and a taxonomy to enhance the Web personalization process
#@ M. Eirinaki;M. Vazirgiannis;I. Varlamis
#t 2003
#c 0
#% 46803
#% 209662
#% 266283
#% 268073
#% 268079
#% 281155
#% 308766
#% 308767
#% 308769
#% 308770
#% 348165
#% 438136
#% 453320
#% 479658
#% 481758
#% 482113
#% 496107
#% 552194
#% 571039
#% 584891
#% 586843
#% 630984
#% 727326
#% 728756
#% 748600
#% 1273676
#! Web personalization is the process of customizing a Web site to the needs of each specific user or set of users, taking advantage of the knowledge acquired through the analysis of the user's navigational behavior. Integrating usage data with content, structure or user profile data enhances the results of the personalization process. In this paper, we present SEWeP, a system that makes use of both the usage logs and the semantics of a Web site's content in order to personalize it. Web content is semantically annotated using a conceptual hierarchy (taxonomy). We introduce C-logs, an extended form of Web usage logs that encapsulates knowledge derived from the link semantics. C-logs are used as input to the Web usage mining process, resulting in a broader yet semantically focused set of recommendations.

#index 729920
#* Inverted matrix: efficient discovery of frequent items in large datasets in the context of interactive mining
#@ Mohammad El-Hajj;Osmar R. Zaïane
#t 2003
#c 0
#% 152934
#% 201894
#% 227917
#% 300120
#% 316709
#% 320944
#% 434348
#% 443348
#% 481290
#% 577234
#% 629642
#% 629681
#% 632037
#! Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets. One major problem is the high memory dependency: either the gigantic data structure built is assumed to fit in main memory, or the recursive mining process is too voracious in memory resources. Another major impediment is the repetitive and interactive nature of any knowledge discovery process. To tune parameters, many runs of the same algorithms are necessary leading to the building of these huge data structures time and again. This paper proposes a new disk-based association rule mining algorithm called Inverted Matrix, which achieves its efficiency by applying three new ideas. First, transactional data is converted into a new database layout called Inverted Matrix that prevents multiple scanning of the database during the mining phase, in which finding frequent patterns could be achieved in less than a full scan with random access. Second, for each frequent item, a relatively small independent tree is built summarizing co-occurrences. Finally, a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed. Experimental studies reveal that our Inverted Matrix approach outperform FP-Tree especially in mining very large transactional databases with a very large number of unique items. Our random access disk-based approach is particularly advantageous in a repetitive and interactive setting.

#index 729921
#* To buy or not to buy: mining airfare data to minimize ticket purchase price
#@ Oren Etzioni;Rattapoom Tuchinda;Craig A. Knoblock;Alexander Yates
#t 2003
#c 0
#% 132938
#% 165663
#% 203604
#% 209021
#% 232122
#% 240955
#% 280437
#% 384911
#% 461618
#% 463903
#% 578783
#% 630974
#% 720479
#% 1272397
#% 1860830
#! As product prices become increasingly available on the World Wide Web, consumers attempt to understand how corporations vary these prices over time. However, corporations change prices based on proprietary algorithms and hidden variables (e.g., the number of unsold seats on a flight). Is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions?This paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period. When trained on this data, Hamlet --- our multi-strategy data mining algorithm --- generated a predictive model that saved 341 simulated passengers $198,074 by advising them when to buy and when to postpone ticket purchases. Remarkably, a clairvoyant algorithm with complete knowledge of future prices could save at most $320,572 in our simulation, thus HAMLET's savings were 61.8% of optimal. The algorithm's savings of $198,074 represents an average savings of 23.8% for the 341 passengers for whom savings are possible. Overall, HAMLET saved 4.4% of the ticket price averaged over the entire set of 4,488 simulated passengers. Our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum.

#index 729922
#* Fragments of order
#@ Aristides Gionis;Teija Kujala;Heikki Mannila
#t 2003
#c 0
#% 152934
#% 268762
#% 310515
#% 316709
#% 390132
#% 417366
#% 420063
#% 463903
#% 584932
#% 599926
#! High-dimensional collections of 0--1 data occur in many applications. The attributes in such data sets are typically considered to be unordered. However, in many cases there is a natural total or partial order &pr; underlying the variables of the data set. Examples of variables for which such orders exist include terms in documents, courses in enrollment data, and paleontological sites in fossil data collections. The observations in such applications are flat, unordered sets; however, the data sets respect the underlying ordering of the variables. By this we mean that if A &pr; B &pr; C are three variables respecting the underlying ordering &pr;, and both of variables A and C appear in an observation, then, up to noise levels, variable B also appears in this observation. Similarly, if A1 &pr; A2 &pr; … &pr; Al-1 &pr; Ai is a longer sequence of variables, we do not expect to see many observations for which there are indices i j k such that Ai and Ak occur in the observation but Aj does not.In this paper we study the problem of discovering fragments of orders of variables implicit in collections of unordered observations. We define measures that capture how well a given order agrees with the observed data. We describe a simple and efficient algorithm for finding all the fragments that satisfy certain conditions. We also discuss the sometimes necessary postprocessing for selecting only the best fragments of order. Also, we relate our method with a sequencing approach that uses a spectral algorithm, and with the consecutive ones problem. We present experimental results on some real data sets (author lists of database papers, exam results data, and paleontological data).

#index 729923
#* Maximizing the spread of influence through a social network
#@ David Kempe;Jon Kleinberg;Éva Tardos
#t 2003
#c 0
#% 36698
#% 210182
#% 342596
#% 348527
#% 577217
#! Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of "word of mouth" in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.

#index 729924
#* PROXIMUS: a framework for analyzing very high dimensional discrete-attributed datasets
#@ Mehmet Koyutürk;Ananth Grama
#t 2003
#c 0
#% 200694
#% 262217
#% 274612
#% 314054
#% 319234
#% 415792
#% 420083
#% 420091
#% 478767
#% 479659
#% 481290
#% 481779
#% 656201
#% 678172
#! This paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets. Such datasets, which frequently arise in a wide variety of applications, pose some of the most significant challenges in data analysis. Subsampling and compression are two key technologies for analyzing these datasets. PROXIMUS provides a technique for reducing large datasets into a much smaller set of representative patterns, on which traditional (expensive) analysis algorithms can be applied with minimal loss of accuracy. We show desirable properties of PROXIMUS in terms of runtime, scalability to large datasets, and performance in terms of capability to represent data in a compact form. We also demonstrate applications of PROXIMUS in association rule mining. In doing so, we establish PROXIMUS as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns. Our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values (near 100%) across a range of support thresholds while reducing the time required for association rule mining drastically.

#index 729925
#* Visualizing changes in the structure of data for exploratory feature selection
#@ Elias Pampalk;Werner Goebl;Gerhard Widmer
#t 2003
#c 0
#% 216500
#% 234978
#% 257039
#% 273693
#% 342601
#% 342613
#% 345858
#% 386001
#% 451656
#% 493275
#% 543942
#% 577223
#% 633773
#% 659943
#% 849888
#% 930290
#% 937051
#% 1860651
#% 1860652
#! Using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays. Several visualization techniques have been developed to study the cluster structure of data, i.e., the existence of distinctive groups in the data and how these clusters are related to each other. However, only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed. Understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase.In this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure. We demonstrate the application of our approach in two music related data mining projects.

#index 729926
#* Aggregation-based feature invention and relational concept classes
#@ Claudia Perlich;Foster Provost
#t 2003
#c 0
#% 136350
#% 136733
#% 252221
#% 266230
#% 307109
#% 398841
#% 398845
#% 398847
#% 412460
#% 458257
#% 464449
#% 550390
#% 550551
#% 713332
#% 1378224
#! Model induction from relational data requires aggregation of the values of attributes of related entities. This paper makes three contributions to the study of relational learning. (1) It presents a hierarchy of relational concepts of increasing complexity, using relational schema characteristics such as cardinality, and derives classes of aggregation operators that are needed to learn these concepts. (2) Expanding one level of the hierarchy, it introduces new aggregation operators that model the distributions of the values to be aggregated and (for classification problems) the differences in these distributions by class. (3) It demonstrates empirically on a noisy business domain that more-complex aggregation methods can increase generalization performance. Constructing features using target-dependent aggregations can transform relational prediction tasks so that well-understood feature-vector-based modeling algorithms can be applied successfully.

#index 729927
#* Cross-training: learning probabilistic mappings between topics
#@ Sunita Sarawagi;Soumen Chakrabarti;Shantanu Godbole
#t 2003
#c 0
#% 209690
#% 236497
#% 252011
#% 260001
#% 269217
#% 280817
#% 281187
#% 311027
#% 330767
#% 348187
#% 464641
#% 577235
#% 1271814
#! Classification is a well-established operation in text mining. Given a set of labels A and a set DA of training documents tagged with these labels, a classifier learns to assign labels to unlabeled test documents. Suppose we also had available a different set of labels B, together with a set of documents DB marked with labels from B. If A and B have some semantic overlap, can the availability of DB help us build a better classifier for A, and vice versa? We answer this question in the affirmative by proposing cross-training: a new approach to semi-supervised learning in presence of multiple label sets. We give distributional and discriminative algorithms for cross-training and show, through extensive experiments, that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification.

#index 729928
#* Generating English summaries of time series data using the Gricean maxims
#@ Somayajulu G. Sripada;Ehud Reiter;Jim Hunter;Jin Yu
#t 2003
#c 0
#% 183418
#% 297158
#% 404133
#% 449752
#% 460919
#% 466506
#% 577275
#% 641125
#% 641175
#% 811303
#% 850471
#% 853890
#% 855136
#% 1271989
#! We are developing technology for generating English textual summaries of time-series data, in three domains: weather forecasts, gas-turbine sensor readings, and hospital intensive care data. Our weather-forecast generator is currently operational and being used daily by a meteorological company. We generate summaries in three steps: (a) selecting the most important trends and patterns to communicate; (b) mapping these patterns onto words and phrases; and (c) generating actual texts based on these words and phrases. In this paper we focus on the first step, (a), selecting the information to communicate, and describe how we perform this using modified versions of standard data analysis algorithms such as segmentation. The modifications arose out of empirical work with users and domain experts, and in fact can all be regarded as applications of the Gricean maxims of Quality, Quantity, Relevance, and Manner, which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text. The Gricean maxims are perhaps a key element of adapting data analysis algorithms for effective communication of information to human users, and should be considered by other researchers interested in communicating data to human users.

#index 729929
#* Assessment and pruning of hierarchical model based clustering
#@ Jeremy Tantrum;Alejandro Murua;Werner Stuetzle
#t 2003
#c 0
#% 464292
#% 577229
#! The goal of clustering is to identify distinct groups in a dataset. The basic idea of model-based clustering is to approximate the data density by a mixture model, typically a mixture of Gaussians, and to estimate the parameters of the component densities, the mixing fractions, and the number of components from the data. The number of distinct groups in the data is then taken to be the number of mixture components, and the observations are partitioned into clusters (estimates of the groups) using Bayes' rule. If the groups are well separated and look Gaussian, then the resulting clusters will indeed tend to be "distinct" in the most common sense of the word - contiguous, densely populated areas of feature space, separated by contiguous, relatively empty regions. If the groups are not Gaussian, however, this correspondence may break down; an isolated group with a non-elliptical distribution, for example, may be modeled by not one, but several mixture components, and the corresponding clusters will no longer be well separated. We present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters. We also propose a new clustering method that can be regarded as a hybrid between model-based and nonparametric clustering. The hybrid clustering algorithm prunes the cluster tree generated by hierarchical model-based clustering. Starting with the tree corresponding to the mixture model chosen by the Bayesian Information Criterion, it progressively merges clusters that do not appear to correspond to different modes of the data density.

#index 729930
#* Privacy-preserving k-means clustering over vertically partitioned data
#@ Jaideep Vaidya;Chris Clifton
#t 2003
#c 0
#% 23638
#% 80995
#% 259504
#% 300184
#% 333876
#% 342603
#% 346201
#% 379340
#% 466083
#% 577233
#% 577289
#% 635219
#% 653942
#% 809530
#% 993988
#% 1386180
#! Privacy and security concerns can prevent sharing of data, derailing data mining projects. Distributed knowledge discovery, if done correctly, can alleviate this problem. The key is to obtain valid results, while providing guarantees on the (non)disclosure of data. We present a method for k-means clustering when different sites contain different attributes for a common set of entities. Each site learns the cluster of each entity, but learns nothing about the attributes at other sites.

#index 729931
#* Indexing multi-dimensional time-series with support for multiple distance measures
#@ Michail Vlachos;Marios Hadjieleftheriou;Dimitrios Gunopulos;Eamonn Keogh
#t 2003
#c 0
#% 318129
#% 397631
#% 398427
#% 458857
#% 462231
#% 477479
#% 504158
#% 564263
#% 577221
#% 632042
#% 632088
#% 659971
#% 993965
#! Although most time-series data mining research has concentrated on providing solutions for a single distance function, in this work we motivate the need for a single index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of trajectory similarities. Trajectory datasets are very common in environmental applications, mobility experiments, video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the Longest Common Subsequence (LCSS) model, that offers enhanced robustness, particularly for noisy data, which are encountered very often in real world applications. However, our index is able to accommodate other distance measures as well, including the ubiquitous Euclidean distance, and the increasingly popular Dynamic Time Warping (DTW). While other researchers have advocated one or other of these similarity measures, a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW.

#index 729932
#* Mining concept-drifting data streams using ensemble classifiers
#@ Haixun Wang;Wei Fan;Philip S. Yu;Jiawei Han
#t 2003
#c 0
#% 132583
#% 136350
#% 273900
#% 310500
#% 333931
#% 342600
#% 342639
#% 378388
#% 397380
#% 424997
#% 428155
#% 449529
#% 466401
#% 481945
#% 578678
#% 594012
#% 629625
#% 993958
#% 1279299
#! Recently, mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection, target marketing, network intrusion detection, etc. Conventional knowledge discovery tools are facing two challenges, the overwhelming volume of the streaming data, and the concept drifts. In this paper, we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers. We train an ensemble of classification models, such as C4.5, RIPPER, naive Beyesian, etc., from sequential chunks of the data stream. The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment. Thus, the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification. Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy, and the ensemble framework is effective for a variety of classification models.

#index 729933
#* CLOSET+: searching for the best strategies for mining frequent closed itemsets
#@ Jianyong Wang;Jiawei Han;Jian Pei
#t 2003
#c 0
#% 152934
#% 201894
#% 227917
#% 248791
#% 300120
#% 310494
#% 342643
#% 443348
#% 464714
#% 465003
#% 466490
#% 481290
#% 481779
#% 577234
#% 629644
#! Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, top-down vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask "what are the pros and cons of the strategies?" and "what and how can we pick and integrate the best strategies to achieve higher performance in general cases?"In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability.

#index 729934
#* Mining unexpected rules by pushing user dynamics
#@ Ke Wang;Yuelong Jiang;Laks V. S. Lakshmanan
#t 2003
#c 0
#% 172386
#% 227919
#% 248785
#% 273916
#% 280436
#% 280485
#% 310494
#% 310496
#% 420112
#% 443092
#% 481290
#% 577214
#% 578394
#% 1499588
#! Unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest. In this paper, we study three important issues that have been previously ignored in mining unexpected rules. First, the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario, in addition to the knowledge itself. Second, the prior knowledge should be considered right from the start to focus the search on unexpected rules. Third, the unexpectedness of a rule depends on what other rules the user has seen so far. Thus, only rules that remain unexpected given what the user has seen should be considered interesting. We develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules.

#index 729935
#* On detecting differences between groups
#@ Geoffrey I. Webb;Shane Butler;Douglas Newlands
#t 2003
#c 0
#% 136350
#% 152934
#% 179770
#% 248791
#% 280409
#% 310505
#% 420126
#% 1272179
#! Understanding the differences between contrasting groups is a fundamental task in data analysis. This realization has led to the development of a new special purpose data mining technique, contrast-set mining. We undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques. To our surprise we observed that straightforward application of an existing commercial rule-discovery system, Magnum Opus, could successfully perform the contrast-set-mining task. This led to the realization that contrast-set mining is a special case of the more general rule-discovery task. We present the results of our study together with a proof of this conclusion.

#index 729936
#* Algorithms for estimating relative importance in networks
#@ Scott White;Padhraic Smyth
#t 2003
#c 0
#% 268079
#% 290830
#% 309779
#% 330707
#% 348173
#% 438103
#% 453464
#% 466891
#! Large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis---examples include social networks, Web graphs, telecommunication networks, and biological networks. In interactive analysis of such data a natural query is "which entities are most important in the network relative to a particular individual or set of individuals?" We investigate the problem of answering such queries in this paper, focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes. We define a general framework and a number of different algorithms, building on ideas from social networks, graph theory, Markov models, and Web graph analysis. We experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among September 11th terrorists, a network of collaborative research in biotechnology among companies and universities, and a network of co-authorship relationships among computer science researchers.

#index 729937
#* Screening and interpreting multi-item associations based on log-linear modeling
#@ Xintao Wu;Daniel Barbará;Yong Ye
#t 2003
#c 0
#% 152934
#% 210182
#% 300120
#% 333946
#% 342597
#% 347881
#% 420073
#% 459025
#% 481290
#% 487853
#% 528023
#! Association rules have received a lot of attention in the data mining community since their introduction. The classical approach to find rules whose items enjoy high support (appear in a lot of the transactions in the data set) is, however, filled with shortcomings. It has been shown that support can be misleading as an indicator of how interesting the rule is. Alternative measures, such as lift, have been proposed. More recently, a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that cannot be explained by pairwise associations between the items involved. This approach, however, has its limitations, since it stops short of considering higher order interactions (other than pairwise) among the items. In this paper, we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items. Since fitting loglinear models for large data sets can be computationally prohibitive, we apply graph-theoretical results to divide the original set of items into components (sets of items) that are statistically independent from each other. We then apply loglinear modeling to each of the components and find the interesting associations among items in them. The technique is experimentally evaluated with a real data set (insurance data) and a series of synthetic data sets. The results show that the technique is effective in finding interesting associations among the items involved.

#index 729938
#* CloseGraph: mining closed frequent graph patterns
#@ Xifeng Yan;Jiawei Han
#t 2003
#c 0
#% 300120
#% 410276
#% 465003
#% 466644
#% 481290
#% 577218
#% 629603
#% 629630
#% 629646
#% 629708
#! Recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees, lattices, and graphs. As a general data structure, graph can model complicated relations among data with wide applications in bioinformatics, Web exploration, and etc. However, mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs. Instead of mining all the subgraphs, we propose to mine closed frequent graph patterns. A graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. A closed graph pattern mining algorithm, CloseGraph, is developed by exploring several interesting pruning methods. Our performance study shows that CloseGraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining, especially in the presence of large graph patterns.

#index 729939
#* Eliminating noisy information in Web pages for data mining
#@ Lan Yi;Bing Liu;Xiaoli Li
#t 2003
#c 0
#% 169717
#% 255137
#% 271060
#% 278106
#% 282905
#% 310546
#% 348180
#% 413617
#% 438676
#% 451536
#% 465754
#% 577281
#% 746910
#! A commercial Web page typically contains many information blocks. Apart from the main content blocks, it usually has such blocks as navigation panels, copyright and privacy notices, and advertisements (for business purposes and for easy user access). We call these blocks that are not the main content blocks of the page the noisy blocks. We show that the information contained in these noisy blocks can seriously harm Web data mining. Eliminating these noises is thus of great importance. In this paper, we propose a noise elimination technique based on the following observation: In a given Web site, noisy blocks usually share some common contents and presentation styles, while the main content blocks of the pages are often diverse in their actual contents and/or presentation styles. Based on this observation, we propose a tree structure, called Style Tree, to capture the common presentation styles and the actual contents of the pages in a given Web site. By sampling the pages of the site, a Style Tree can be built for the site, which we call the Site Style Tree (SST). We then introduce an information based measure to determine which parts of the SST represent noises and which parts represent the main contents of the site. The SST is employed to detect and eliminate noises in any Web page of the site by mapping this page to the SST. The proposed technique is evaluated with two data mining tasks, Web page clustering and classification. Experimental results show that our noise elimination technique is able to improve the mining results significantly.

#index 729940
#* Classifying large data sets using SVMs with hierarchical clusters
#@ Hwanjo Yu;Jiong Yang;Jiawei Han
#t 2003
#c 0
#% 210173
#% 248790
#% 269217
#% 269218
#% 342598
#% 342625
#% 420077
#% 438137
#% 458379
#% 466419
#% 466887
#% 543892
#% 577228
#% 577235
#% 722757
#% 855583

#index 729941
#* XRules: an effective structural classifier for XML data
#@ Mohammed J. Zaki;Charu C. Aggarwal
#t 2003
#c 0
#% 4868
#% 136350
#% 262071
#% 273900
#% 280437
#% 311027
#% 466483
#% 479640
#% 481290
#% 552188
#% 577218
#% 577227
#% 629656
#! XML documents have recently become ubiquitous because of their varied applicability in a number of applications. Classification is an important problem in the data mining domain, but current classification methods for XML documents use IR-based methods in which each document is treated as a bag of words. Such techniques ignore a significant amount of information hidden inside the documents. In this paper we discuss the problem of rule based classification of XML data by using frequent discriminatory substructures within XML documents. Such a technique is more capable of finding the classification characteristics of documents. In addition, the technique can also be extended to cost sensitive classification. We show the effectiveness of the method with respect to other classifiers. We note that the methodology discussed in this paper is applicable to any kind of semi-structured data.

#index 729942
#* Fast vertical mining using diffsets
#@ Mohammed J. Zaki;Karam Gouda
#t 2003
#c 0
#% 201894
#% 227917
#% 232136
#% 248791
#% 248813
#% 300120
#% 300124
#% 310494
#% 310507
#% 316709
#% 443350
#% 459020
#% 462219
#% 464714
#% 465003
#% 466664
#% 481754
#% 481945
#% 577256
#% 631986
#% 678196
#! A number of vertical mining algorithms have been proposed recently for association mining, which have shown to be very effective and usually outperform horizontal approaches. The main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids (tids) and automatic pruning of irrelevant data. The main problem with these approaches is when intermediate results of vertical tid lists become too large for memory, thus affecting the algorithm scalability.In this paper we present a novel vertical data representation called Diffset, that only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns. We show that diffsets drastically cut down the size of memory required to store intermediate results. We show how diffsets, when incorporated into previous vertical mining methods, increase the performance significantly.

#index 729943
#* Efficient elastic burst detection in data streams
#@ Yunyue Zhu;Dennis Shasha
#t 2003
#c 0
#% 248822
#% 273902
#% 318051
#% 333926
#% 342600
#% 378388
#% 379445
#% 460862
#% 480306
#% 480628
#% 566132
#% 577220
#% 577275
#% 617888
#% 631923
#% 659965
#% 993949
#% 993961
#! Burst detection is the activity of finding abnormal aggregates in data streams. Such aggregates are based on sliding windows over data streams. In some applications, we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods. We will present a general data structure for detecting interesting aggregates over such elastic windows in near linear time. We present applications of the algorithm for detecting Gamma Ray Bursts in large-scale astrophysical data. Detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time Trade and Quote (TAQ) data from the New York Stock Exchange (NYSE). Our algorithm beats the direct computation approach by several orders of magnitude.

#index 729944
#* Golden Path Analyzer: using divide-and-conquer to cluster Web clickstreams
#@ Kamal Ali;Steven P. Ketchpel
#t 2003
#c 0
#% 310543
#% 341700
#% 443194
#% 449588
#% 463903
#% 552194
#% 614610
#% 1809531
#! This paper describes a novel algorithm and deployed system Golden Path Analyzer (GPA) that analyzes clickstreams of people trying to complete the same task on a website. It finds the shortest, successful paths taken by users - 'golden paths' - and uses these as seeds for clickstream clusters. Other users are assigned to a cluster if their clickstream is a supersequence of the golden path. The advantages of this approach are that the resulting clusters are easily comprehended, they are few in number, correspond to semantically different strategies used by the users, and jointly partition all the clickstreams. GPA's key contribution over prior work in process funnels is that by not excluding users that make diversions from the golden path, GPA is able to assign more users to fewer clusters. Another key contribution is to use actual full clickstreams as cluster seeds to which supersequences of other users are added. Golden paths correspond to complete clickstreams that are based on actual user page transitions. GPA is particularly useful for site designers to improve processes such as shopping, returns and registration. Its analyses identify which web pages cause many users to deviate from a golden path, which links distract users and the percentage of users taking each golden path. GPA has demonstrated value on more than twenty client projects in diverse industries.

#index 729945
#* Empirical Bayesian data mining for discovering patterns in post-marketing drug safety
#@ David M. Fram;June S. Almenoff;William DuMouchel
#t 2003
#c 0
#% 280402
#% 342597
#% 481290
#! Because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing, manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports ("pharmacovigilance").The resulting databases, while rich in real-world information, are notoriously difficult to analyze using traditional techniques. Each report may involve multiple medicines, symptoms, and demographic factors, and there is no easily linked information on drug exposure in the reporting population. KDD techniques, such as association finding, are well-matched to the problem, but are difficult for medical staff to apply and interpret.To deploy KDD effectively for pharmacovigilance, Lincoln Technologies and GlaxoSmithKline collaborated to create a webbased safety data mining web environment. The analytical core is a high-performance implementation of the MGPS (Multi-Item Gamma Poisson Shrinker) algorithm described previously by DuMouchel and Pregibon, with several significant extensions and enhancements. The environment offers an interface for specifying data mining runs, a batch execution facility, tabular and graphical methods for exploring associations, and drilldown to case details. Substantial work was involved in preparing the raw adverse event data for mining, including harmonization of drug names and removal of duplicate reports.The environment can be used to explore both drug-event and multi-way associations (interactions, syndromes). It has been used to study age/gender effects, to predict the safety profiles of proposed combination drugs, and to separate contributions of individual drugs to safety problems in polytherapy situations.

#index 729946
#* Mining hepatitis data with temporal abstraction
#@ Tu Bao Ho;Trong Dung Nguyen;Saori Kawasaki;Si Quang Le;Dung Duc Nguyen;Hideto Yokoi;Katsuhiko Takabayashi
#t 2003
#c 0
#% 136350
#% 224476
#% 316709
#% 556804
#% 577270
#% 629650
#% 1843690
#! The hepatitis temporal database collected at Chiba university hospital between 1982--2001 was recently given to challenge the KDD research. The database is large where each patient corresponds to 983 tests represented as sequences of irregular timestamp points with different lengths. This paper presents a temporal abstraction approach to mining knowledge from this hepatitis database. Exploiting hepatitis background knowledge and data analysis, we introduce new notions and methods for abstracting short-term changed and long-term changed tests. The abstracted data allow us to apply different machine learning methods for finding knowledge part of which is considered as new and interesting by medical doctors.

#index 729947
#* Information awareness: a prospective technical assessment
#@ David Jensen;Matthew Rattigan;Hannah Blau
#t 2003
#c 0
#% 248810
#% 331909
#% 420064
#% 464449
#% 466086
#% 579593
#% 1650403
#! Recent proposals to apply data mining systems to problems in law enforcement, national security, and fraud detection have attracted both media attention and technical critiques of their expected accuracy and impact on privacy. Unfortunately, the majority of technical critiques have been based on simplistic assumptions about data, classifiers, inference procedures, and the overall architecture of such systems. We consider these critiques in detail, and we construct a simulation model that more closely matches realistic systems. We show how both the accuracy and privacy impact of a hypothetical system could be substantially improved, and we discuss the necessary and sufficient conditions for this improvement to be achieved. This analysis is neither a defense nor a critique of any particular system concept. Rather, our model suggests alternative technical designs that could mitigate some concerns, but also raises more specific conditions that must be met for such systems to be both accurate and socially desirable.

#index 729948
#* The data mining approach to automated software testing
#@ Mark Last;Menahem Friedman;Abraham Kandel
#t 2003
#c 0
#% 3873
#% 112310
#% 115608
#% 136350
#% 169657
#% 244720
#% 248107
#% 279132
#% 311929
#% 311953
#% 311996
#% 335012
#% 352244
#% 368206
#% 387146
#% 391425
#% 445826
#% 577243
#% 733620
#% 998631
#% 1290037
#! In today's industry, the design of software tests is mostly based on the testers' expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. The costs of the so-called "catastrophic" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs when testing new, potentially flawed releases of the system. To study the feasibility of the proposed approach, we have applied a novel data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a general-purpose code for solving partial differential equations. After being trained on a relatively small number of randomly generated input-output examples, the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program.

#index 729949
#* Passenger-based predictive modeling of airline no-show rates
#@ Richard D. Lawrence;Se June Hong;Jacques Cherrier
#t 2003
#c 0
#% 136350
#% 342649
#% 376266
#% 464280
#% 573166
#% 629713
#% 1306054
#! Airlines routinely overbook flights based on the expectation that some fraction of booked passengers will not show for each flight. Accurate forecasts of the expected number of no-shows for each flight can increase airline revenue by reducing the number of spoiled seats (empty seats that might otherwise have been sold) and the number of involuntary denied boardings at the departure gate. Conventional no-show forecasting methods typically average the no-show rates of historically similar flights, without the use of passenger-specific information.We develop two classes of models to predict cabin-level no-show rates using specific information on the individual passengers booked on each flight. The first of these models computes the no-show probability for each passenger, using both the cabin-level historical forecast and the extracted passenger features as explanatory variables. This passenger-level model is implemented using three different predictive methods: a C4.5 decision-tree, a segmented Naive Bayes algorithm, and a new aggregation method for an ensemble of probabilistic models. The second cabin-level model is formulated using the desired cabin-level no-show rate as the response variable. Inputs to this model include the predicted cabin-level no-show rates derived from the various passenger-level models, as well as simple statistics of the features of the cabin passenger population. The cabin-level model is implemented using either linear regression, or as a direct probability model with explicit incorporation of the cabin-level no-show rates derived from the passenger-level model outputs.The new passenger-based models are compared to a conventional historical model, using train and evaluation data sets taken from over 1 million passenger name records. Standard metrics such as lift curves and mean-square cabin-level errors establish the improved accuracy of the passenger-based models over the historical model. All models are also evaluated using a simple revenue model, and it is shown that the cabin-level passenger-based model can produce between 0.4% and 3.2% revenue gain over the conventional model, depending on the revenue-model parameters.

#index 729950
#* Capturing best practice for microarray gene expression data analysis
#@ Gregory Piatetsky-Shapiro;Tom Khabaza;Sridhar Ramaswamy
#t 2003
#c 0
#% 243728
#! Analyzing gene expression data from microarray devices has many important application in medicine and biology, but presents significant challenges to data mining. Microarray data typically has many attributes (genes) and few examples (samples), making the process of correctly analyzing such data difficult to formulate and prone to common mistakes. For this reason it is unusually important to capture and record good practices for this form of data mining. This paper presents a process for analyzing microarray data, including pre-processing, gene selection, randomization testing, classification and clustering; this process is captured with "Clementine Application Templates". The paper describes the process in detail and includes three case studies, showing how the process is applied to 2-class classification, multi-class classification and clustering analyses for publicly available microarray datasets.

#index 729951
#* Clinical and financial outcomes analysis with existing hospital patient records
#@ R. Bharat Rao;Sathyakama Sandilya;Radu Stefan Niculescu;Colin Germond;Harsha Rao
#t 2003
#c 0
#% 266292
#% 279755
#% 376266
#% 380725
#% 420077
#% 466892
#% 529678
#! Existing patient records are a valuable resource for automated outcomes analysis and knowledge discovery. However, key clinical data in these records is typically recorded in unstructured form as free text and images, and most structured clinical information is poorly organized. Time-consuming interpretation and analysis is required to convert these records into structured clinical data. Thus, only a tiny fraction of this resource is utilized. We present REMIND, a Bayesian Framework for Reliable Extraction and Meaningful Inference from Nonstructured Data. REMIND integrates and blends the structured and unstructured clinical data in patient records to automatically created high-quality structured clinical data. This structuring allows existing patient records to be mined for quality assurance, regulatory compliance, and to relate financial and clinical factors. We demonstrate REMIND on two medical applications: (a) Extract "recurrence", the key outcome for measuring treatment effectiveness, for colon cancer patients (ii) Extract key diagnoses and complications for acute myocardial infarction (heart attack) patients, and demonstrate the impact of these clinical factors on financial outcomes.

#index 729952
#* Critical event prediction for proactive management in large-scale computer clusters
#@ R. K. Sahoo;A. J. Oliner;I. Rish;M. Gupta;J. E. Moreira;S. Ma;R. Vilalta;A. Sivasubramaniam
#t 2003
#c 0
#% 1267
#% 44876
#% 136350
#% 266674
#% 380725
#% 465762
#% 470258
#% 478766
#% 569905
#% 594202
#% 629640
#% 656841
#% 1306056
#% 1650580
#! As the complexity of distributed computing systems increases, systems management tasks require significantly higher levels of automation; examples include diagnosis and prediction based on real-time streams of computer events, setting alarms, and performing continuous monitoring. The core of autonomic computing, a recently proposed initiative towards next-generation IT-systems capable of 'self-healing', is the ability to analyze data in real-time and to predict potential problems. The goal is to avoid catastrophic failures through prompt execution of remedial actions.This paper describes an attempt to build a proactive prediction and control system for large clusters. We collected event logs containing various system reliability, availability and serviceability (RAS) events, and system activity reports (SARs) from a 350-node cluster system for a period of one year. The 'raw' system health measurements contain a great deal of redundant event data, which is either repetitive in nature or misaligned with respect to time. We applied a filtering technique and modeled the data into a set of primary and derived variables. These variables used probabilistic networks for establishing event correlations through prediction algorithms. We also evaluated the role of time-series methods, rule-based classification algorithms and Bayesian network models in event prediction.Based on historical data, our results suggest that it is feasible to predict system performance parameters (SARs) with a high degree of accuracy using time-series models. Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70% accuracy.

#index 729953
#* Frequent-subsequence-based prediction of outer membrane proteins
#@ Rong She;Fei Chen;Ke Wang;Martin Ester;Jennifer L. Gardy;Fiona S. L. Brinkman
#t 2003
#c 0
#% 58593
#% 136350
#% 172892
#% 190581
#% 280488
#% 310539
#% 402289
#% 457905
#% 501994
#% 545956
#! A number of medically important disease-causing bacteria (collectively called Gram-negative bacteria) are noted for the extra "outer" membrane that surrounds their cell. Proteins resident in this membrane (outer membrane proteins, or OMPs) are of primary research interest for antibiotic and vaccine drug design as they are on the surface of the bacteria and so are the most accessible targets to develop new drugs against. With the development of genome sequencing technology and bioinformatics, biologists can now deduce all the proteins that are likely produced in a given bacteria and have attempted to classify where proteins are located in a bacterial cell. However such protein localization programs are currently least accurate when predicting OMPs, and so there is a current need for the development of a better OMP classifier. Data mining research suggests that the use of frequent patterns has good performance in aiding the development of accurate and efficient classification algorithms. In this paper, we present two methods to identify OMPs based on frequent subsequences and test them on all Gram-negative bacterial proteins whose localizations have been determined by biological experiments. One classifier follows an association rule approach, while the other is based on support vector machines (SVMs). We compare the proposed methods with the state-of-the-art methods in the biological domain. The results demonstrate that our methods are better both in terms of accurately identifying OMPs and providing biological insights that increase our understanding of the structures and functions of these important proteins.

#index 729954
#* Discovery of climate indices using clustering
#@ Michael Steinbach;Pang-Ning Tan;Vipin Kumar;Steven Klooster;Christopher Potter
#t 2003
#c 0
#% 36672
#% 238376
#! To analyze the effect of the oceans and atmosphere on land climate, Earth Scientists have developed climate indices, which are time series that summarize the behavior of selected regions of the Earth's oceans and atmosphere. In the past, Earth scientists have used observation and, more recently, eigenvalue analysis techniques, such as principal components analysis (PCA) and singular value decomposition (SVD), to discover climate indices. However, eigenvalue techniques are only useful for finding a few of the strongest signals. Furthermore, they impose a condition that all discovered signals must be orthogonal to each other, making it difficult to attach a physical interpretation to them. This paper presents an alternative clustering-based methodology for the discovery of climate indices that overcomes these limitiations and is based on clusters that represent regions with relatively homogeneous behavior. The centroids of these clusters are time series that summarize the behavior of the ocean or atmosphere in those regions. Some of these centroids correspond to known climate indices and provide a validation of our methodology; other centroids are variants of known indices that may provide better predictive power for some land areas; and still other indices may represent potentially new Earth science phenomena. Finally, we show that cluster based indices generally outperform SVD derived indices, both in terms of area weighted correlation and direct correlation with the known indices.

#index 729955
#* Knowledge-based data mining
#@ Sholom M. Weiss;Stephen J. Buckley;Shubir Kapoor;Søren Damgaard
#t 2003
#c 0
#% 38669
#% 168280
#% 209021
#% 444931
#% 496419
#! We describe techniques for combining two types of knowledge systems: expert and machine learning. Both the expert system and the learning system represent information by logical decision rules or trees. Unlike the classical views of knowledge-base evaluation or refinement, our view accepts the contents of the knowledge base as completely correct. The knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules. An expert system called SEAS was built to discover sales leads for computer products and solutions. The system interviews executives by asking questions, and based on the responses, recommends products that may improve a business' operations. Leveraging this expert system, we record the results of the interviews and the program's recommendations. The very same data stored by the expert system is used to find new predictive rules. Among the potential advantages of this approach are (a) the capability to spot new sales trends and (b) the substitution of less expensive probabilistic rules that use database data instead of interviews.

#index 729956
#* The anatomy of a multimodal information filter
#@ Yi-Leh Wu;King-Shy Goh;Beitao Li;Huaxing You;Edward Y. Chang
#t 2003
#c 0
#% 296533
#% 341269
#% 420077
#% 458379
#% 465746
#% 468102
#% 837668
#% 1272365
#% 1811362
#% 1858012
#! The proliferation of objectionable information on the Internet has reached a level of serious concern. To empower end-users with the choice of blocking undesirable and offensive websites, we propose a multimodal information filter, named MORF. In this paper, we present MORF's core components: its confidence-based classifier, a Cross-bagging ensemble scheme, and multimodal classification algorithm. Empirical studies and initial statistics collected from the MORF filters deployed at sites in the U.S. and Asia show that MORF is both efficient and effective, due to our classification methods.

#index 729957
#* Style mining of electronic messages for multiple authorship discrimination: first results
#@ Shlomo Argamon;Marin Šarić;Sterling S. Stein
#t 2003
#c 0
#% 27842
#% 227736
#% 309208
#% 344447
#% 740416
#% 744557
#! This paper considers the use of computational stylistics for performing authorship attribution of electronic messages, addressing categorization problems with as many as 20 different classes (authors). Effective stylistic characterization of text is potentially useful for a variety of tasks, as language style contains cues regarding the authorship, purpose, and mood of the text, all of which would be useful adjuncts to information retrieval or knowledge-management tasks. We focus here on the problem of determining the author of an anonymous message, based only on the message text. Several multiclass variants of the Winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating different authors. We present results comparing the classification accuracy of the different approaches. The results show that stylistic models can be accurately learned to determine an author's identity.

#index 729958
#* Mining high dimensional data for classifier knowledge
#@ Raj Bhatnagar;Goutham Kurra;Wen Niu
#t 2003
#c 0
#! We present in this paper the problem of discovering sets of attribute-value pairs in high dimensional data sets that are of interest not because of co-occurrence alone, but due to their value in serving as cores for potential classifiers of clusters. We present our algorithm in the context of a gene-expression dataset. Gene expression data, in most situations, is insufficient for clustering algorithms and any statistical inference because for 6000+ genes, typically only 10s and at most 100s of data points become available. It is difficult to use statistical techniques to design a classifier for such immensely under-specified data. The observed data, though statistically, insufficient contains some information about the domain. Our goal is to discover as much information about all potential classifiers as possible from the data and then summarize this knowledge. This summarization provides insights into the composition of potential classifiers. We present here algorithms and methods for mining a high dimensional data set, exemplified by a gene expression data set, for mining such information.

#index 729959
#* Finding recent frequent itemsets adaptively over online data streams
#@ Joong Hyuk Chang;Won Suk Lee
#t 2003
#c 0
#% 227917
#% 273898
#% 310507
#% 342689
#% 481290
#% 492912
#% 632090
#% 993960
#! A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate. Consequently, the knowledge embedded in a data stream is more likely to be changed as time goes by. Identifying the recent change of a data stream, specially for an online data stream, can provide valuable information for the analysis of the data stream. In addition, monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge. However, most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present. This paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream. The effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by. Furthermore, several optimization techniques are devised to minimize processing time as well as main memory usage. Finally, the proposed method is analyzed by a series of experiments.

#index 729960
#* Probabilistic discovery of time series motifs
#@ Bill Chiu;Eamonn Keogh;Stefano Lonardi
#t 2003
#c 0
#% 196811
#% 232767
#% 310502
#% 310583
#% 328321
#% 397629
#% 462231
#% 469571
#% 479973
#% 480146
#% 480156
#% 481609
#% 529189
#% 577221
#% 631923
#% 631926
#% 659971
#! Several important time series data mining problems reduce to the core task of finding approximately repeated subsequences in a longer time series. In an earlier work, we formalized the idea of approximately repeated subsequences by introducing the notion of time series motifs. Two limitations of this work were the poor scalability of the motif discovery algorithm, and the inability to discover motifs in the presence of noise.Here we address these limitations by introducing a novel algorithm inspired by recent advances in the problem of pattern discovery in biosequences. Our algorithm is probabilistic in nature, but as we show empirically and theoretically, it can find time series motifs with very high probability even in the presence of noise or "don't care" symbols. Not only is the algorithm fast, but it is an anytime algorithm, producing likely candidate motifs almost immediately, and gradually improving the quality of results over time.

#index 729961
#* Understanding captions in biomedical publications
#@ William W. Cohen;Richard Wang;Robert F. Murphy
#t 2003
#c 0
#% 283138
#% 402289
#% 469402
#% 471758
#% 531458
#% 589404
#! From the standpoint of the automated extraction of scientific knowledge, an important but little-studied part of scientific publications are the figures and accompanying captions. Captions are dense in information, but also contain many extra-grammatical constructs, making them awkward to process with standard information extraction methods. We propose a scheme for "understanding" captions in biomedical publications by extracting and classifying "image pointers" (references to the accompanying image). We evaluate a number of automated methods for this task, including hand-coded methods, methods based on existing learning techniques, and methods based on novel learning techniques. The best of these methods leads to a usefully accurate tool for caption-understanding, with both recall and precision in excess of 94% on the most important single class in a combined extraction/classification task.

#index 729962
#* Using randomized response techniques for privacy-preserving data mining
#@ Wenliang Du;Zhijun Zhan
#t 2003
#c 0
#% 300184
#% 316709
#% 577233
#% 577289
#% 635215
#! Privacy is an important issue in data mining and knowledge discovery. In this paper, we propose to use the randomized response techniques to conduct the data mining computation. Specially, we present a method to build decision tree classifiers from the disguised data. We conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data. Our results show that although the data are disguised, our method can still achieve fairly high accuracy. We also show how the parameter used in the randomized response techniques affects the accuracy of the results.

#index 729963
#* Applications of sampling and fractional factorial designs to model-free data squashing
#@ William DuMouchel;Deepak K. Agarwal
#t 2003
#c 0
#% 280402
#% 420139
#% 580221
#! The concept of "data squashing" was introduced by DuMouchel et al [4] as a method of summarizing massive data sets that preserves statistical relationships among variables. The idea is to create a smaller data set that allows statistical modeling to take place using in-memory algorithms, and to preserve the modeling results more accurately than would a same-size random sample from the massive data set. This research attempts to avoid several limitations of previous approaches to data squashing. Our method avoids the curse of dimensionality by a double use of principal components transformations that makes computing time linear in the number of cases and quadratic in the number of variables. Categorical and continuous variables are smoothly integrated. Because the binning is based on principal components, which are uncorrelated, we can use fractional factorial designs that sample less than one point per bin. We also investigate various weighting schemes for the squashed sample to see whether matching moments or matching subregion data counts is more effective. Finally, previous work required the specification of a statistical model, either to perform the squashing algorithm or to compare the worth of different squashing methods. Our approach to evaluation is model free and does not even require the specification of variables as responses or predictors. Instead, we develop a chi-squared like measure of accuracy to compare the closeness of various discrete densities (the squashed data sets) to the discrete massive data set.

#index 729964
#* Experiments with random projections for machine learning
#@ Dmitriy Fradkin;David Madigan
#t 2003
#c 0
#% 136350
#% 249321
#% 269217
#% 333881
#% 342617
#% 379455
#% 397385
#% 527853
#% 593926
#! Dimensionality reduction via Random Projections has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning. In particular, we compare Random Projections and PCA on a number of different datasets and using different machine learning methods. While we find that the random projection approach predictively underperforms PCA, its computational advantages may make it attractive for certain applications.

#index 729965
#* Accurate decision trees for mining high-speed data streams
#@ João Gama;Ricardo Rocha;Pedro Medas
#t 2003
#c 0
#% 136350
#% 218963
#% 246747
#% 246831
#% 310500
#% 342600
#% 464453
#% 466401
#% 1499581
#! In this paper we study the problem of constructing accurate decision tree models from data streams. Data streams are incremental tasks that require incremental, online, and any-time learning algorithms. One of the most successful algorithms for mining data streams is VFDT. In this paper we extend the VFDT system in two directions: the ability to deal with continuous data and the use of more powerful classification techniques at tree leaves. The proposed system, VFDTc, can incorporate and classify new information online, with a single scan of the data, in time constant per example. The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets. This is relevant due to the any-time property. We study the behaviour of VFDTc in different problems and demonstrate its utility in large and medium data sets. Under a bias-variance analysis we observe that VFDTc in comparison to C4.5 is able to reduce the variance component.

#index 729966
#* Correlating synchronous and asynchronous data streams
#@ Sudipto Guha;D. Gunopulos;Nick Koudas
#t 2003
#c 0
#% 214073
#% 238376
#% 282481
#% 593842
#! In a variety of modern mining applications, data are commonly viewed as infinite time ordered data streams rather as finite data sets stored on disk. This view challenges fundamental assumptions commonly made in the context of several data mining algorithms.In this paper, we study the problem of identifying correlations between multiple data streams. In particular, we propose algorithms capable of capturing correlations between multiple continuous data streams in a highly efficient and accurate manner. Our algorithms and techniques are applicable in the case of both synchronous and asynchronous data streaming environments. We capture correlations between multiple streams using the well known technique of Singular Value Decomposition (SVD). Correlations between data items, and the SVD technique in particular, have been repeatedly utilized in an off-line (non stream) data mining problems, for example forecasting, approximate query answering, and data reduction.We propose a methodology based on a combination of dimensionality reduction and sampling to make the SVD technique suitable for a data stream context. Our techniques are approximate, trading accuracy with performance, and we analytically quantify this tradeoff. We present a through experimental evaluation, using both real and synthetic data sets, from a prototype implementation of our technique, investigating the impact of various parameters in the accuracy of the overall computation. Our results indicate, that correlations between multiple data streams can be identified very efficiently and accurately. The algorithms proposed herein, are presented as generic tools, with a multitude of applications on data stream mining problems.

#index 729967
#* A Web page prediction model based on click-stream tree representation of user behavior
#@ Şule Gündüz;M. Tamer Özsu
#t 2003
#c 0
#% 216509
#% 309777
#% 342870
#% 463903
#% 614610
#% 630984
#% 993934
#! Predicting the next request of a user as she visits Web pages has gained importance as Web-based activity increases. Markov models and their variations, or models based on sequence mining have been found well suited for this problem. However, higher order Markov models are extremely complicated due to their large number of states whereas lower order Markov models do not capture the entire behavior of a user in a session. The models that are based on sequential pattern mining only consider the frequent sequences in the data set, making it difficult to predict the next request following a page that is not in the sequential pattern. Furthermore, it is hard to find models for mining two different kinds of information of a user session. We propose a new model that considers both the order information of pages in a session and the time spent on them. We cluster user sessions based on their pair-wise similarity and represent the resulting clusters by a click-stream tree. The new user session is then assigned to a cluster based on a similarity measure. The click-stream tree of that cluster is used to generate the recommendation set. The model can be used as part of a cache prefetching system as well as a recommendation model.

#index 729968
#* Natural communities in large linked networks
#@ John Hopcroft;Omar Khan;Brian Kulis;Bart Selman
#t 2003
#c 0
#% 36672
#% 249110
#% 249143
#% 310514
#% 310533
#% 584932
#% 1289272
#! We are interested in finding natural communities in large-scale linked networks. Our ultimate goal is to track changes over time in such communities. For such temporal tracking, we require a clustering algorithm that is relatively stable under small perturbations of the input data. We have developed an efficient, scalable agglomerative strategy and applied it to the citation graph of the NEC CiteSeer database (250,000 papers; 4.5 million citations). Agglomerative clustering techniques are known to be unstable on data in which the community structure is not strong. We find that some communities are essentially random and thus unstable while others are natural and will appear in most clusterings. These natural communities will enable us to track the evolution of communities over time.

#index 729969
#* Navigating massive data sets via local clustering
#@ Michael E. Houle
#t 2003
#c 0
#% 190611
#% 249321
#% 296738
#% 314054
#% 342827
#% 374537
#% 376266
#% 420144
#% 465014
#% 479973
#% 481290
#% 511663
#% 571079
#% 840583
#! This paper introduces a scalable method for feature extraction and navigation of large data sets by means of local clustering, where clusters are modeled as overlapping neighborhoods. Under the model, intra-cluster association and external differentiation are both assessed in terms of a natural confidence measure. Minor clusters can be identified even when they appear in the intersection of larger clusters. Scalability of local clustering derives from recent generic techniques for efficient approximate similarity search. The cluster overlap structure gives rise to a hierarchy that can be navigated and queried by users. Experimental results are provided for two large text databases.

#index 729970
#* Mining viewpoint patterns in image databases
#@ Wynne Hsu;Jing Dai;Mong Li Lee
#t 2003
#c 0
#% 248865
#% 342635
#% 481281
#% 481290
#% 527021
#% 577266
#% 584926
#% 728302
#% 1855132

#index 729971
#* Playing hide-and-seek with correlations
#@ Christopher Jermaine
#t 2003
#c 0
#% 53085
#% 152934
#% 248791
#% 270686
#% 280436
#% 299985
#% 300120
#% 310505
#% 789955
#! We present a method for very high-dimensional correlation analysis. The method relies equally on rigorous search strategies and on human interaction. At each step, the method conservatively "shaves off" a fraction of the database tuples and attributes, so that most of the correlations present in the data are not affected by the decomposition. Instead, the correlations become more obvious to the user, because they are hidden in a much smaller portion of the database. This process can be repeated iteratively and interactively, until only the most important correlations remain.The main technical difficulty of the approach is figuring out how to "shave off" part of the database so as to preserve most correlations. We develop an algorithm for this problem that has a polynomial running time and guarantees result quality.

#index 729972
#* Interactive exploration of coherent patterns in time-series gene expression data
#@ Daxin Jiang;Jian Pei;Aidong Zhang
#t 2003
#c 0
#% 273890
#% 397382
#% 469425
#! Discovering coherent gene expression patterns in time-series gene expression data is an important task in bioinformatics research and biomedical applications. In this paper, we propose an interactive exploration framework for mining coherent expression patterns in time-series gene expression data. We develop a novel tool, coherent pattern index graph, to give users highly confident indications of the existences of coherent patterns. To derive a coherent pattern index graph, we devise an attraction tree structure to record the genes in the data set and summarize the information needed for the interactive exploration. We present fast and scalable algorithms to construct attraction trees and coherent pattern index graphs from gene expression data sets. We conduct an extensive performance study on some real data sets to verify our design. The experimental results strongly show that our approach is more effective than the state-of-the-art methods in mining real gene expression data, and is scalable in mining large data sets.

#index 729973
#* Efficient decision tree construction on streaming data
#@ Ruoming Jin;Gagan Agrawal
#t 2003
#c 0
#% 273900
#% 310500
#% 333926
#% 342600
#% 378388
#% 378408
#% 397354
#% 452821
#% 459008
#% 479787
#% 481945
#% 594012
#! Decision tree construction is a well studied problem in data mining. Recently, there has been much interest in mining streaming data. Domingos and Hulten have presented a one-pass algorithm for decision tree construction. Their work uses Hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed.In this paper, we revisit this problem. We make the following two contributions: 1) We present a numerical interval pruning (NIP) approach for efficiently processing numerical attributes. Our results show an average of 39% reduction in execution times. 2) We exploit the properties of the gain function entropy (and gini) to reduce the sample size required for obtaining a given bound on the accuracy. Our experimental results show a 37% reduction in the number of data instances required.

#index 729974
#* A bag of paths model for measuring structural similarity in Web documents
#@ Sachindra Joshi;Neeraj Agrawal;Raghu Krishnapuram;Sumit Negi
#t 2003
#c 0
#% 248808
#% 287202
#% 289193
#% 300157
#% 348180
#% 442886
#% 465754
#% 480126
#% 480648
#% 480824
#% 534048
#! Structural information (such as layout and look-and-feel) has been extensively used in the literatuce for extraction of interesting or relevant data, efficient storage, and query optimization. Traditionally, tree models (such as DOM trees) have been used to represent structural information, especially in the case of HTML and XML documents. However, computation of structural similarity between documents based on the tree model is computationally expensive. In this paper, we propose an alternative scheme for representing the structural information of documents based on the paths contained in the corresponding tree model. Since the model includes partial information about parents, children and siblings, it allows us to define a new family of meaningful (and at the same time computationally simple) structural similarity measures. Our experimental results based on the SIGMOD XML data set as well as HTML document collections from ibm.com, dell.com, and amazon.com show that the representation is powerful enough to produce good clusters of structurally similar pages.

#index 729975
#* Nantonac collaborative filtering: recommendation based on order responses
#@ Toshihiro Kamishima
#t 2003
#c 0
#% 173879
#% 220706
#% 420121
#% 465906
#% 466647
#% 466913
#% 577224
#% 629710
#% 1271961
#% 1272396
#% 1650569
#! A recommender system suggests the items expected to be preferred by the users. Recommender systems use collaborative filtering to recommend items by summarizing the preferences of people who have tendencies similar to the user preference. Traditionally, the degree of preference is represented by a scale, for example, one that ranges from one to five. This type of measuring technique is called the semantic differential (SD) method. Web adopted the ranking method, however, rather than the SD method, since the SD method is intrinsically not suited for representing individual preferences. In the ranking method, the preferences are represented by orders, which are sorted item sequences according to the users' preferences. We here propose some methods to recommed items based on these order responses, and carry out the comparison experiments of these methods.

#index 729976
#* A two-way visualization method for clustered data
#@ Yehuda Koren;David Harel
#t 2003
#c 0
#% 296738
#% 399764
#% 438617
#% 641176
#! We describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low-dimensional embedding, thereby gaining the benefits of both approaches. In a single image one can view all the clusters, examine the relations between them and study many of their properties. The method is based on an algorithm for low-dimensional embedding of clustered data, with the property that separation between all clusters is guaranteed, regardless of their nature. In particular, the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data, so that every two disjoint clusters in the hierarchy are drawn separately.

#index 729977
#* Empirical comparisons of various voting methods in bagging
#@ Kelvin T. Leung;D. Stott Parker
#t 2003
#c 0
#! Finding effective methods for developing an ensemble of models has been an active research area of large-scale data mining in recent years. Models learned from data are often subject to some degree of uncertainty, for a variety of resoans. In classification, ensembles of models provide a useful means of averaging out error introduced by individual classifiers, hence reducing the generalization error of prediction.The plurality voting method is often chosen for bagging, because of its simplicity of implementation. However, the plurality approach to model reconciliation is ad-hoc. There are many other voting methods to choose from, including the anti-plurality method, the plurality method with elimination, the Borda count method, and Condorcet's method of pairwise comparisons. Any of these could lead to a better method for reconciliation.In this paper, we analyze the use of these voting methods in model reconciliation. We present empirical results comparing performance of these voting methods when applied in bagging. These results include some surprises, and among other things suggest that (1) plurality is not always the best voting method; (2) the number of classes can affect the performance of voting methods; and (3) the degree of dataset noise can affect the performance of voting methods. While it is premature to make final judgments about specific voting methods, the results of this work raise interesting questions, and they open the door to the application of voting theory in classification theory.

#index 729978
#* Mining data records in Web pages
#@ Bing Liu;Robert Grossman;Yanhong Zhai
#t 2003
#c 0
#% 65341
#% 235941
#% 240955
#% 271065
#% 273925
#% 275915
#% 312860
#% 330784
#% 348146
#! A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and non-contiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially.

#index 729979
#* On computing, storing and querying frequent patterns
#@ Guimei Liu;Hongjun Lu;Wenwu Lou;Jeffrey Xu Yu
#t 2003
#c 0
#% 152934
#% 201894
#% 227917
#% 300120
#% 329598
#% 342643
#% 466490
#% 481754
#% 577216
#% 577234
#% 587748
#% 678196
#% 729933
#! Extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns. However, frequent pattern mining remains a time-consuming process, especially for very large datasets. It is therefore desirable to adopt a "mining once and using many times" strategy. Unfortunately, there has been little work reported on managing and organizing a large set of patterns for future use. In this paper, we propose a disk-based data structure, CFP-tree (Condensed Frequent Pattern Tree), for organizing frequent patterns discovered from transactional databases. In addition to an efficient algorithm for CFP-tree construction, we also developed algorithms to efficiently support two important types of queries, namely queries with minimum support constraints and queries with item constraints, against the stored patterns, as these two types of queries are basic building blocks for complex frequent pattern related mining tasks. Comprehensive experimental study has been conducted to demonstrate the effectiveness of CFP-tree and efficiency of related algorithms.

#index 729980
#* Online novelty detection on temporal sequences
#@ Junshui Ma;Simon Perkins
#t 2003
#c 0
#% 169368
#% 280408
#% 566132
#% 577275
#% 617888
#% 725787
#! In this paper, we present a new framework for online novelty detection on temporal sequences. This framework include a mechanism for associating each detection result with a confidence value. Based on this framework, we develop a concrete online detection algorithm, by modeling the temporal sequence using an online support vector regression algorithm. Experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm.

#index 729981
#* Distributed cooperative mining for information consortia
#@ Satoshi Morinaga;Kenji Yamanishi;Jun-ichi Takeuchi
#t 2003
#c 0
#% 151225
#% 270531
#% 340291
#% 342816
#% 417610
#% 443085
#% 764480
#% 1809517
#! We consider the situation where a number of agents are distributed and each of them collects a data sequence generated according to an unknown probability distribution. Here each of the distributions is specified by common parameters and individual parameters e.g., a normal distribution with an identical mean and a different variance. Here we introduce a notion of an information consortium, which is a framework where the agents cannot show raw data to one another, but they like to enjoy significant information gain for estimating the respective distributions. Such an information consortium has recently received much interest in a broad range of areas including financial risk management, ubiquitous network mining, etc. In this paper we are concerned with the following three issues: 1) how to design a collaborative strategy for agents to estimate the respective distributions in the information consortium, 2) characterizing when each agent has a benefit in terms of information gain for estimating its distribution or information loss for predicting future data, and 3) charracterizing how much benefit each agent obtains. In this paper we yield a statistical formulation of information consortia and solve all of the above three problems for a general form of probability distributions. Specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit.

#index 729982
#* Learning relational probability trees
#@ Jennifer Neville;David Jensen;Lisa Friedland;Michael Hay
#t 2003
#c 0
#% 14749
#% 136350
#% 252221
#% 266215
#% 307109
#% 392781
#% 464449
#% 477978
#% 495944
#% 496116
#% 550551
#% 1499586
#! Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.

#index 729983
#* Graph-based anomaly detection
#@ Caleb C. Noble;Diane J. Cook
#t 2003
#c 0
#% 369349
#% 445369
#% 545599
#% 664713
#! Anomaly detection is an area that has received much attention in recent years. It has a wide variety of applications, including fraud detection and network intrusion detection. A good deal of research has been performed in this area, often using strings or attribute-value data as the medium from which anomalies are to be extracted. Little work, however, has focused on anomaly detection in graph-based data. In this paper, we introduce two techniques for graph-based anomaly detection. In addition, we introduce a new method for calculating the regularity of a graph, with applications to anomaly detection. We hypothesize that these methods will prove useful both for finding anomalies, and for determining the likelihood of successful anomaly detection within graph-based data. We provide experimental results using both real-world network intrusion data and artificially-created data.

#index 729984
#* Carpenter: finding closed patterns in long biological datasets
#@ Feng Pan;Gao Cong;Anthony K. H. Tung;Jiong Yang;Mohammed J. Zaki
#t 2003
#c 0
#% 273916
#% 300124
#% 338594
#% 481290
#% 678196
#% 729933
#! The growth of bioinformatics has resulted in datasets with new characteristics. These datasets typically contain a large number of columns and a small number of rows. For example, many gene expression datasets may contain 10,000-100,000 columns but only 100-1000 rows.Such datasets pose a great challenge for existing (closed) frequent pattern discovery algorithms, since they have an exponential dependence on the average row length. In this paper, we describe a new algorithm called CARPENTER that is specially designed to handle datasets having a large number of attributes and relatively small number of rows. Several experiments on real bioinformatics datasets show that CARPENTER is orders of magnitude better than previous closed pattern mining algorithms like CLOSET and CHARM.

#index 729985
#* New unsupervised clustering algorithm for large datasets
#@ William Peter;John Chiochetti;Clare Giardina
#t 2003
#c 0
#% 54761
#% 99716
#% 127107
#% 210173
#% 393792
#% 407116
#% 434347
#% 481281
#! A fast and accurate unsupervised clustering algorithm has been developed for clustering very large datasets. Though designed for very large volumes of geospatial data, the algorithm is general enough to be used in a wide variety of domain applications. The number of computations the algorithm requires is ~ O(N), and thus faster than hierarchical algorithms. Unlike the popular K-means heuristic, this algorithm does not require a series of iterations to converge to a solution. In addition, this method does not depend on initialization of a given number of cluster representatives, and so is insensitive to initial conditions. Being unsupervised, the algorithm can also "rank" each cluster based on density. The method relies on weighting a dataset to grid points on a mesh, and using a small number of rule-based agents to find the high density clusters. This method effectively reduces large datasets to the size of the grid, which is usually many orders of magnitude smaller. Numerical experiments are shown that demonstrate the advantages of this algorithm over other techniques.

#index 729986
#* Improving spatial locality of programs via data mining
#@ Karlton Sequeira;Mohammed Zaki;Boleslaw Szymanski;Christopher Carothers
#t 2003
#c 0
#% 68010
#% 82291
#% 278500
#% 295704
#% 381226
#% 414174
#% 425006
#% 429424
#% 443701
#% 463903
#% 464291
#% 669003
#% 978378
#! In most computer systems, page fault rate is currently minimized by generic page replacement algorithms which try to model the temporal locality inherent in programs. In this paper, we propose two algorithms, one greedy and the other stochastic, designed for program specific code restructuring as a means of increasing spatial locality within a program. Both algorithms effectively decrease average working set size and hence the page fault rate. Our methods are more effective than traditional approaches due to use of domain information. We illustrate the efficacy of our algorithms on actual data mining algorithms.

#index 729987
#* Mining phenotypes and informative genes from gene expression data
#@ Chun Tang;Aidong Zhang;Jian Pei
#t 2003
#c 0
#% 248792
#% 469422
#% 659967
#! Mining microarray gene expression data is an important research topic in bioinformatics with broad applications. While most of the previous studies focus on clustering either genes or samples, it is interesting to ask whether we can partition the complete set of samples into exclusive groups (called phenotypes) and find a set of informative genes that can manifest the phenotype structure. In this paper, we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data. Some statistics-based metrics are proposed to measure the quality of the mining results. Two interesting algorithms are developed: the heuristic search and the mutual reinforcing adjustment method. We present an extensive performance study on both real-world data sets and synthetic data sets. The mining results from the two proposed methods are clearly better than those from the previous methods. They are ready for the real-world applications. Between the two methods, the mutual reinforcing adjustment method is in general more scalable, more effective and with better quality of the mining results.

#index 729988
#* Weighted Association Rule Mining using weighted support and significance framework
#@ Feng Tao;Fionn Murtagh;Mohsen Farid
#t 2003
#c 0
#% 152934
#% 227917
#% 279120
#% 280487
#% 310541
#% 330332
#% 478770
#% 481290
#% 481588
#! We address the issues of discovering significant binary relationships in transaction datasets in a weighted setting. Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight. The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatornal explosion of insignificant relationships. We identify the challenge of using weights in the iterative process of generating large itemsets. The problem of invalidation of the "downward closure property" in the weighted setting is solved by using an improved model of weighted support measurements and exploiting a "weighted downward closure property". A new algorithm called WARM (Weighted Association Rule Mining) is developed based on the improved model. The algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on simulated datasets.

#index 729989
#* PaintingClass: interactive construction, visualization and exploration of decision trees
#@ Soon Tee Teoh;Kwan-Liu Ma
#t 2003
#c 0
#% 136350
#% 191910
#% 216500
#% 280511
#% 310517
#% 342593
#% 342601
#% 459008
#% 466474
#% 480940
#% 641171
#% 1780725
#% 1860637
#! Decision trees are commonly used for classification. We propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery, because visualizing the decision tree can reveal much valuable information in the data. We introduce PaintingClass, a system for interactive construction, visualization and exploration of decision trees. PaintingClass provides an intuitive layout and convenient navigation of the decision tree. PaintingClass also provides the user the means to interactively construct the decision tree. Each node in the decision tree is displayed as a visual projection of the data. Through actual examples and comparison with other classification methods, we show that the user can effectively use PaintingClass to construct a decision tree and explore the decision tree to gain additional knowledge.

#index 729990
#* Time and sample efficient discovery of Markov blankets and direct causal relations
#@ Ioannis Tsamardinos;Constantin F. Aliferis;Alexander Statnikov
#t 2003
#c 0
#% 44876
#% 67866
#% 400980
#! Data Mining with Bayesian Network learning has two important characteristics: under conditions learned edges between variables correspond to casual influences, and second, for every variable T in the network a special subset (Markov Blanket) identifiable by the network is the minimal variable set required to predict T. However, all known algorithms learning a complete BN do not scale up beyond a few hundred variables. On the other hand, all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region.The contribution of this paper is two-fold. We introduce a novel local algorithm that returns all variables with direct edges to and from a target variable T as well as a local algorithm that returns the Markov Blanket of T. Both algorithms (i) are sound, (ii) can be run efficiently in datasets with thousands of variables, and (iii) significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods. A fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region; this yields up to exponential savings in sample relative to previously known algorithms. The results presented here are promising not only for discovery of local causal structure, and variable selection for classification, but also for the induction of complete BNs.

#index 729991
#* Distributed multivariate regression based on influential observations
#@ Hang Yu;Ee-Chien Chang
#t 2003
#c 0
#% 280406
#% 296825
#% 329599

#index 729992
#* Efficiently handling feature redundancy in high-dimensional data
#@ Lei Yu;Huan Liu
#t 2003
#c 0
#% 136350
#% 169659
#% 243727
#% 243728
#% 290482
#% 332083
#% 385564
#% 420146
#% 464444
#% 465754
#% 465905
#% 466912
#% 564259
#% 1390189
#! High-dimensional data poses a severe challenge for data mining. Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining. Traditionally, feature selection is focused on removing irrelevant features. However, for high-dimensional data, removing redundant features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features.

#index 729993
#* An adaptive nearest neighbor search for a parts acquisition ePortal
#@ Rafael Alonso;Jeffrey A. Bloom;Hua Li;Chumki Basu
#t 2003
#c 0
#% 5182
#% 140588
#% 173879
#% 202011
#% 234992
#% 577224
#! One of the major hurdles in maintaining long-lived electronic systems is that electronic parts become obsolete, no longer available from the original suppliers. When this occurs, an engineer is tasked with resolving the problem by finding a replacement that is "as similar as possible" to the original part. The current approach involves a laborious manual search through several electronic portals and data books. The search is difficult because potential replacements may differ from the original and from each other by one or more parameters. Worse still, the cumbersome nature of this process may cause the engineers to miss appropriate solutions amid the many thousands of parts listed in industry catalogs.In this paper, we address this problem by introducing the notion of a parametric "distance" between electronic components. We use this distance to search a large parts data set and recommend likely replacements. Recommendations are based on an adaptive nearest-neighbor search through the parametric data set. For each user, we learn how to scale the axes of the feature space in which the nearest neighbors are sought. This allows the system to learn each user's judgment of the phrase "as similar as possible."

#index 729994
#* Architecting a knowledge discovery engine for military commanders utilizing massive runs of simulations
#@ Philip Barry;Jianping Zhang;Mary McDonald
#t 2003
#c 0
#% 74839
#% 316709
#% 637727
#! The Marine Corps' Project Albert seeks to model complex phenomenon by observing the behavior of relatively simple simulations over thousands of runs. A rich data base is developed by running the simulations thousands of times, varying the agent and scenario input parameters as well as the random seeds. Exploring this result space may provide significant insight into nonlinear, surprising, and emergent behaviors. Capturing these results can provide a path for making the results usable for decision support to a military commander. This paper presents two data mining approaches, rule discovery and Bayesian networks, for analyzing the Albert simulation data. The first approach generates rules from the data and then uses them to create descriptive model. The second generates Bayesian Networks which provide a quantitative belief model for decision support. Both of these approaches as well as the Project Albert simulations are framed in the context of a system architecture for decision support.

#index 729995
#* Data quality through knowledge engineering
#@ Tamraparni Dasu;Gregg T. Vesonder;Jon R. Wright
#t 2003
#c 0
#% 17144
#% 119374
#% 256488
#% 300136
#% 322670
#% 412755
#% 420072
#% 479791
#% 644182
#! Traditionally, data quality programs have acted as a preprocessing stage to make data suitable for a data mining or analysis operation. Recently, data quality concepts have been applied to databases that support business operations such as provisioning and billing. Incorporating business rules that drive operations and their associated data processes is critically important to the success of such projects. However, there are many practical complications. For example, documentation on business rules is often meager. Rules change frequently. Domain knowledge is often fragmented across experts, and those experts do not always agree. Typically, rules have to be gathered from subject matter experts iteratively, and are discovered out of logical or procedural sequence, like a jigsaw puzzle. Our approach is to impement business rules as constraints on data in a classical expert system formalism sometimes called production rules. Our system works by allowing good data to pass through a system of constraints unchecked. Bad data violate constraints and are flagged, and then fed back after correction. Constraints are added incrementally as better understanding of the business rules is gained. We include a real-life case study.

#index 729996
#* Similarity analysis on government regulations
#@ Gloria T. Lau;Kincho H. Law;Gio Wiederhold
#t 2003
#c 0
#% 46803
#% 252750
#% 268079
#% 280512
#% 361100
#% 387427
#% 427320
#% 840583
#! Government regulations are semi-structured text documents that are often voluminous, heavily cross-referenced between provisions and even ambiguous. Multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes. In this work, we propose a framework for regulation management and similarity analysis. An online repository for legal documents is created with the help of text mining tool, and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts. Our similarity analysis core identifies relevant provisions and brings them to the user's attention, and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions. Preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons.

#index 729997
#* Experimental design for solicitation campaigns
#@ Uwe F. Mayer;Armand Sarkissian
#t 2003
#c 0
#% 464280
#! Data mining techniques are routinely used by fundraisers to select those prospects from a large pool of candidates who are most likely to make a financial contribution. These techniques often rely on statistical models based on trial performance data. This trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool. Collecting this trial data involves a cost; therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospects.We describe an experimental design approach to optimally choose the trial prospects from an existing large pool of prospects. Prospects are clustered to render the problem practically tractable. We modify the standard D-optimality algorithm to prevent repeated selection of the same prospect cluster, since each prospect can only be solicited at most once.We assess the benefits of this approach on the KDD-98 data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size.

#index 729998
#* Towards NIC-based intrusion detection
#@ M. Otey;S. Parthasarathy;A. Ghoting;G. Li;S. Narravula;D. Panda
#t 2003
#c 0
#% 188026
#% 236454
#% 248820
#% 287242
#% 289519
#% 342600
#% 402489
#% 469931
#% 479658
#% 536877
#% 577250
#% 577251
#% 594012
#% 664547
#% 664711
#% 729998
#% 790040
#% 978633
#% 993960
#! We present and evaluate a NIC-based network intrusion detection system. Intrusion detection at the NIC makes the system potentially tamper-proof and is naturally extensible to work in a distributed setting. Simple anomaly detection and signature detection based models have been implemented on the NIC firmware, which has its own processor and memory. We empirically evaluate such systems from the perspective of quality and performance (bandwidth of acceptable messages) under varying conditions of host load. The preliminary results we obtain are very encouraging and lead us to believe that such NIC-based security schemes could very well be a crucial part of next generation network security systems.

#index 729999
#* Data-driven validation, completion and construction of event relationship networks
#@ Chang-Shing Perng;David Thoenen;Genady Grabarnik;Sheng Ma;Joseph Hellerstein
#t 2003
#c 0
#% 252201
#% 252206
#% 539906
#! Event management is a focal point in building and maintaining high quality information infrastructures. We have witnessed the shift of the paradigm of event management in practice from root cause analysis (RCA) to action-oriented analysis (AOA). IBM has developed a pioneer event management methodology (EMD) based on the AOA paradigm and applied it to more than two hundred production sites with success. Foreseeably, more and more event management professionals will apply AOA in different incarnations in building proactive management facilities. By that, building correct and effective Event Relationship Networks (ERNs) becomes the dominating activity in AOA service design process. Currently, the quality of ERNs and the cost of building them largely depend on the knowledge of domain experts. We believe that we can utilize historical event logs in shortening the ERNs design process and perfecting the quality of ERNs. In this paper, we describe in detail how to apply this data-driven approach in ERN validation, completion and construction.

#index 730000
#* Visualizing concept drift
#@ Kevin B. Pratt;Gleb Tschapek
#t 2003
#c 0
#% 1211
#% 204531
#% 260645
#% 286639
#% 316709
#% 342600
#% 345857
#% 411100
#% 420145
#% 619521
#% 729437
#! We describe a visualization technique that uses brushed, parallel histograms to aid in understanding concept drift in multidimensional problem spaces. This technique illustrates the relationship between changes in distributions of multiple antecedent feature values and the outcome distribution. We can also observe effects on the relative utilization of predictive rules. Our parallel histogram technique solves the over-plotting difficulty of parallel coordinate graphs and the difficulty of comparing distributions of brushed and original data. We demonstrate our technique's usefulness in understanding concept drifts in power demand and stock investment returns.

#index 730001
#* Experimental study of discovering essential information from customer inquiry
#@ Keiko Shimazu;Atsuhito Momma;Koichi Furukawa
#t 2003
#c 0
#% 259993
#% 271083
#% 379237
#% 481290
#% 516146
#% 546520
#% 577218
#% 584890
#% 755838
#% 771355
#% 786497
#% 1273700
#! This paper reports the result of our experimental study on a new method of applying an association rule miner to discover useful information from customer inquiry database in a call center of a company. It has been claimed that association rule mining is not suited for text mining. To overcome this problem, we propose (1) to generate sequential data set of words with dependency structure from the Japanese text database, and (2) to employ a new method for extracting meaningful association rules by applying a new rule selection criterion. Each inquiry in the sequential data was represented as a list of word pairs, each of which consists of a verb and its dependent noun. The association rules were induced regarding each pair of words as an item. The rule selection criterion comes from our principle that we put heavier weights to co-occurrence of multiple items more than single item occurrence. We regarded a rule important if the existence of the items in the rule body significantly affects the occurrence of the item in the rule head. The selected rules were then categorized to form meaningful information classes. With this method, we succeeded in extracting useful information classes from the text database, which were not acquired by only simple keyword retrieval. Also, inquiries with multiple aspects were properly classified into corresponding multiple categories.

#index 730002
#* Applying data mining in investigating money laundering crimes
#@ Zhongfei (Mark) Zhang;John J. Salerno;Philip S. Yu
#t 2003
#c 0
#% 188103
#% 202011
#% 249110
#% 343768
#! In this paper, we study the problem of applying data mining to facilitate the investigation of money laundering crimes (MLCs). We have identified a new paradigm of problems --- that of automatic community generation based on uni-party data, the data in which there is no direct or explicit link information available. Consequently, we have proposed a new methodology for Link Discovery based on Correlation Analysis (LDCA). We have used MLC group model generation as an exemplary application of this problem paradigm, and have focused on this application to develop a specific method of automatic MLC group model generation based on timeline analysis using the LDCA methodology, called CORAL. A prototype of CORAL method has been implemented, and preliminary testing and evaluations based on a real MLC case data are reported. The contributions of this work are: (1) identification of the uni-party data community generation problem paradigm, (2) proposal of a new methodology LDCA to solve for problems in this paradigm, (3) formulation of the MLC group model generation problem as an example of this paradigm, (4) application of the LDCA methodology in developing a specific solution (CORAL) to the MLC group model generation problem, and (5) development, evaluation, and testing of the CORAL prototype in a real MLC case data.

#index 769872
#* Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Won Kim;Ronny Kohavi;Johannes Gehrke;William DuMouchel
#t 2004
#c 0
#! KDD-2004, the Tenth ACM SIGMOD International Conference on Knowledge Discovery and Data Mining, is being held in Seattle, Washington, U.S.A., on August 22-25, 2004, with an optional day trip to Rainier National Park on August 26. KDD provides a forum for academic researchers and industry and government innovators to share in their results and experience. Data mining combines techniques and processes from allied data analytic disciplines such as statistics, machine learning, pattern recognition and visualization, while focusing on automated discovery of knowledge from databases that are often much more massive than those commonly tackled. With this conference, we mark a decade in which KDD has led the world in the exchange of theoretical research and practical experiences in the field of knowledge discovery and data mining.The KDD-2004 technical program features two parallel research tracks and an industrial/government track. The program also features keynote addresses by Eric Hazeltine and David Heckerman, eight workshops, six tutorials, and two panels. The 2004 KDD Cup competition focuses on supervised classification tasks in the analysis of two datasets in particle physics and bioinformatics. Exhibits from vendors and other organizations complement the program and underscore the breadth and impact of our field.Once again we have received a record number of submissions, and the selection process was extremely competitive. Each paper was independently reviewed by at least three members of the program committee for originality, significance, technical quality and clarity of presentation. This was followed by discussion among the reviewers and final decisions. Of the 337 research track submissions received, 40 were accepted as full papers for oral presentation, and 45 were accepted for poster presentation (12% and 13% of submissions, respectively). The industrial/government track received 47 submissions, of which 14 were accepted for oral presentation and 13 were accepted for poster presentation (30% and 28%, respectively).The resulting program is diverse and exciting. Topics include classification, clustering, frequent itemsets, scalability, Bayesian methods, graph and network analysis, dimensionality reduction, methods for spatial and temporal data, privacy preserving data mining, and many others. Application areas include astronomy, web text mining, microeconomics, spam and virus detection, medicine and genetics, and many others.A conference like KDD-2004 would not be possible without the dedicated effort of many individuals. In particular, we thank: the industrial/government track chairs, R. Bharat Rao and John Elder, and all the function chairs, for all their hard work putting together the multiple elements of the conference; the members of the program committee, the industrial/government track program committee, and the best paper awards committee, for their efforts reviewing and discussing papers; Won Kim and the SIGKDD Executive Committee, for their guidance and support; Jessica Wilmers and other ACM staff for their logistical support; Stacey Shirk at Cornell for administrative support, and the Microsoft CMT team, for providing the software and technical assistance to run the reviewing process. In addition to these individuals, we would like to thank Teresa Mah who helped with local arrangements, and Marina Meila who volunteered to lead the day trip to Rainer National Park.

#index 769873
#* User-centered design for KDD
#@ Eric Haseltine
#t 2004
#c 0
#! During initial development, KDD solutions often focus heavily on algorithms, architectures, software, hardware, and systems engineering challenges, without first thoroughly exploring how end-users will employ the new KDD technology. As a result of such "system-centered" design, many useless features are implemented that prolong development and significantly add to life cycle cost, while making the system hard to operate and use. This presentation will describe an alternate "user-centered" approach -- borrowed from the consumer products industry -- that can produce KDD solutions with shorter development cycles, lower costs, and much better usability.

#index 769874
#* Graphical models for data mining
#@ David Heckerman
#t 2004
#c 0
#! I will discuss the use of graphical models for data mining. I will review key research areas including structure learning, variational methods, a relational modeling, and describe applications ranging from web traffic analysis to AIDS vaccine design.

#index 769875
#* An iterative method for multi-class cost-sensitive learning
#@ Naoki Abe;Bianca Zadrozny;John Langford
#t 2004
#c 0
#% 136350
#% 169684
#% 209021
#% 235377
#% 280437
#% 342611
#% 342647
#% 458361
#% 466268
#% 714684
#% 722756
#% 727925
#! Cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification. In this paper, we present a method for solving multi-class cost-sensitive learning problems using any binary classification algorithm. This algorithm is derived using hree key ideas: 1) iterative weighting; 2) expanding data space; and 3) gradient boosting with stochastic ensembles. We establish some theoretical guarantees concerning the performance of this method. In particular, we show that a certain variant possesses the boosting property, given a form of weak learning assumption on the component binary classifier. We also empirically evaluate the performance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning, in terms of predictive performance (cost minimization) and, in many cases, computational efficiency.

#index 769876
#* Approximating a collection of frequent sets
#@ Foto Afrati;Aristides Gionis;Heikki Mannila
#t 2004
#c 0
#% 152934
#% 190611
#% 217812
#% 338609
#% 408396
#% 420063
#% 463903
#% 478770
#% 479795
#% 629606
#% 629644
#! One of the most well-studied problems in data mining is computing the collection of frequent item sets in large transactional databases. One obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users. Even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem.In this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem: What are the k sets that best approximate a collection of frequent item sets? Our measure of approximating a collection of sets by k sets is defined to be the size of the collection covered by the the k sets, i.e., the part of the collection that is included in one of the k sets. We also specify a bound on the number of extra sets that are allowed to be covered. We examine different problem variants for which we demonstrate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms. We give empirical evidence showing that the approximation methods work well in practice.

#index 769877
#* Mining reference tables for automatic text segmentation
#@ Eugene Agichtein;Venkatesh Ganti
#t 2004
#c 0
#% 137711
#% 248808
#% 271065
#% 273925
#% 283136
#% 333943
#% 420072
#% 466892
#% 478258
#% 480824
#% 531459
#% 654467
#% 744539
#% 748738
#% 769884
#% 770844
#% 817487
#% 854636
#! Automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legacy sources and text collections into a data warehouse for subsequent querying, analysis, mining and integration. In this paper, we mine tables present in data warehouses and relational databases to develop an automatic segmentation system. Thus, we overcome limitations of existing supervised text segmentation approaches, which require comprehensive manually labeled training data. Our segmentation system is robust, accurate, and efficient, and requires no additional manual effort. Thorough evaluation on real datasets demonstrates the robustness and accuracy of our system, with segmentation accuracy exceeding state of the art supervised approaches.

#index 769878
#* Recovering latent time-series from their observed sums: network tomography with particle filters.
#@ Edoardo Airoldi;Christos Faloutsos
#t 2004
#c 0
#% 342592
#% 428662
#% 446427
#% 449076
#% 578739
#% 580971
#% 646231
#% 1227446
#! Hidden variables, evolving over time, appear in multiple settings, where it is valuable to recover them, typically from observed sums. Our driving application is 'network tomography', where we need to estimate the origin-destination (OD) traffic flows to determine, e.g., who is communicating with whom in a local area network. This information allows network engineers and managers to solve problems in design, routing, configuration debugging, monitoring and pricing. Unfortunately the direct measurement of the OD traffic is usually difficult, or even impossible; instead, we can easily measure the loads on every link, that is, sums of desirable OD flows.In this paper we propose i-FILTER, a method to solve this problem, which improves the state-of-the-art by (a) introducing explicit time dependence, and by (b) using realistic, non-Gaussian marginals in the statistical models for the traffic flows, as never attempted before. We give experiments on real data, where i-FILTER scales linearly with new observations and out-performs the best existing solutions, in a wide variety of settings. Specifically, on real network traffic measured at CMU, and at AT&T, i-FILTER reduced the estimation errors between 15% and 46% in all cases.

#index 769879
#* Fast nonlinear regression via eigenimages applied to galactic morphology
#@ Brigham Anderson;Andrew Moore;Andrew Connolly;Robert Nichol
#t 2004
#c 0
#! Astronomy increasingly faces the issue of massive, unwieldly data sets. The Sloan Digital Sky Survey (SDSS) [11] has so far generated tens of millions of images of distant galaxies, of which only a tiny fraction have been morphologically classified. Morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image. This is a nonlinear regression problem, whose challenges are threefold, 1) blurring of the image caused by atmosphere and mirror imperfections, 2) large numbers of local minima, and 3) massive data sets.Our strategy is to use the eigenimages of the parametric model to form a new feature space, and then to map both target image and the model parameters into this feature space. In this low-dimensional space we search for the best image-to-parameter match. To search the space, we sample it by creating a database of many random parameter vectors (prototypes) and mapping them into the feature space. The search problem then becomes one of finding the best prototype match, so the fitting process a nearest-neighbor search.In addition to the savings realized by decomposing the original space into an eigenspace, we can use the fact that the model is a linear sum of functions to reduce the prototypes further: the only prototypes stored are the components of the model function. A modified form of nearest neighbor is used to search among them.Additional complications arise in the form of missing data and heteroscedasticity, both of which are addressed with weighted linear regression. Compared to existing techniques, speed-ups ach-ieved are between 2 and 3 orders of magnitude. This should enable the analysis of the entire SDSS dataset.

#index 769880
#* Clustering time series from ARMA models with clipped data
#@ A. J. Bagnall;G. J. Janacek
#t 2004
#c 0
#% 310543
#% 397631
#% 413458
#% 430746
#% 466507
#% 477479
#% 629711
#% 643518
#% 662750
#% 727905
#% 761281
#! Clustering time series is a problem that has applications in a wide variety of fields, and has recently attracted a large amount of research. In this paper we focus on clustering data derived from Autoregressive Moving Average (ARMA) models using k-means and k-medoids algorithms with the Euclidean distance between estimated model parameters. We justify our choice of clustering technique and distance metric by reproducing results obtained in related research. Our research aim is to assess the affects of discretising data into binary sequences of above and below the median, a process known as clipping, on the clustering of time series. It is known that the fitted AR parameters of clipped data tend asymptotically to the parameters for unclipped data. We exploit this result to demonstrate that for long series the clustering accuracy when using clipped data from the class of ARMA models is not significantly different to that achieved with unclipped data. Next we show that if the data contains outliers then using clipped data produces significantly better clusterings. We then demonstrate that using clipped series requires much less memory and operations such as distance calculations can be much faster. Finally, we demonstrate these advantages on three real world data sets.

#index 769881
#* A probabilistic framework for semi-supervised clustering
#@ Sugato Basu;Mikhail Bilenko;Raymond J. Mooney
#t 2004
#c 0
#% 115608
#% 252011
#% 277483
#% 311027
#% 370075
#% 387427
#% 425010
#% 460812
#% 464291
#% 464608
#% 464631
#% 466263
#% 592345
#% 593940
#% 727838
#% 729911
#% 729913
#% 748465
#% 1279294
#% 1650729
#! Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints, i.e., pairs of instances labeled as belonging to same or different clusters. In recent years, a number of algorithms have been proposed for enhancing clustering quality by employing such supervision. Such methods use the constraints to either modify the objective function, or to learn the distance measure. We propose a probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields (HMRFs) that provides a principled framework for incorporating supervision into prototype-based clustering. The model generalizes a previous approach that combines constraints and Euclidean distance learning, and allows the use of a broad range of clustering distortion measures, including Bregman divergences (e.g., Euclidean distance and I-divergence) and directional similarity measures (e.g., cosine similarity). We present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the HMRF model. Experimental results on several text data sets demonstrate the advantages of the proposed framework.

#index 769882
#* Data mining in metric space: an empirical analysis of supervised learning performance criteria
#@ Rich Caruana;Alexandru Niculescu-Mizil
#t 2004
#c 0
#% 580510
#! Many criteria can be used to evaluate the performance of supervised learning. Different criteria are appropriate in different settings, and it is not always clear which criteria to use. A further complication is that learning methods that perform well on one criterion may not perform well on other criteria. For example, SVMs and boosting are designed to optimize accuracy, whereas neural nets typically optimize squared error or cross entropy. We conducted an empirical study using a variety of learning methods (SVMs, neural nets, k-nearest neighbor, bagged and boosted trees, and boosted stumps) to compare nine boolean classification performance metrics: Accuracy, Lift, F-Score, Area under the ROC Curve, Average Precision, Precision/Recall Break-Even Point, Squared Error, Cross Entropy, and Probability Calibration. Multidimensional scaling (MDS) shows that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are interpreted as probabilities: squared error, cross entropy, and calibration, lay in one part of metric space far away from metrics that depend on the relative order of the predicted values: ROC area, average precision, break-even point, and lift. In between them fall two metrics that depend on comparing predictions to a threshold: accuracy and F-score. As expected, maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy, but perform poorly on probability metrics such as squared error. What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision. We introduce a new metric, SAR, that combines squared error, accuracy, and ROC area into one metric. MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics, suggesting that it is a good general purpose metric to use when more specific criteria are not known.

#index 769883
#* Fully automatic cross-associations
#@ Deepayan Chakrabarti;Spiros Papadimitriou;Dharmendra S. Modha;Christos Faloutsos
#t 2004
#c 0
#% 193743
#% 210173
#% 248027
#% 248790
#% 262217
#% 280819
#% 287267
#% 316709
#% 346696
#% 425010
#% 438137
#% 438444
#% 466425
#% 481290
#% 528174
#% 577252
#% 665658
#% 729418
#% 729918
#% 730064
#! Large, sparse binary matrices arise in numerous data mining applications, such as the analysis of market baskets, web graphs, social networks, co-citations, as well as information retrieval, collaborative filtering, sparse matrix reordering, etc. Virtually all popular methods for the analysis of such matrices---e.g., k-means clustering, METIS graph partitioning, SVD/PCA and frequent itemset mining---require the user to specify various parameters, such as the number of clusters, number of principal components, number of partitions, and "support." Choosing suitable values for such parameters is a challenging problem.Cross-association is a joint decomposition of a binary matrix into disjoint row and column groups such that the rectangular intersections of groups are homogeneous. Starting from first principles, we furnish a clear, information-theoretic criterion to choose a good cross-association as well as its parameters, namely, the number of row and column groups. We provide scalable algorithms to approach the optimal. Our algorithm is parameter-free, and requires no user intervention. In practice it scales linearly with the problem size, and is thus applicable to very large matrices. Finally, we present experiments on multiple synthetic and real-life datasets, where our method gives high-quality, intuitive results.

#index 769884
#* Exploiting dictionaries in named entity extraction: combining semi-Markov extraction processes and data integration methods
#@ William W. Cohen;Sunita Sarawagi
#t 2004
#c 0
#% 90041
#% 252034
#% 278104
#% 278107
#% 283180
#% 301241
#% 333943
#% 420495
#% 438103
#% 451055
#% 464434
#% 465919
#% 466892
#% 471758
#% 577238
#% 716732
#% 722903
#% 723243
#% 816181
#% 854636
#% 854637
#% 854814
#! We consider the problem of improving named entity recognition (NER) systems by using external dictionaries---more specifically, the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary. This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name; however, the most useful similarity measures score entire candidate names. To correct this mismatch we formalize a semi-Markov extraction process, which is based on sequentially classifying segments of several adjacent words, rather than single words. In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions, this formalism also allows the direct use of other useful entity-level features, and provides a more natural formulation of the NER problem than sequential word classification. Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER.

#index 769885
#* Adversarial classification
#@ Nilesh Dalvi;Pedro Domingos; Mausam;Sumit Sanghai;Deepak Verma
#t 2004
#c 0
#% 240955
#% 243728
#% 246831
#% 280437
#% 310495
#% 331909
#% 342600
#% 408396
#% 420064
#% 445304
#% 453325
#% 577250
#% 729947
#% 748016
#! Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.

#index 769886
#* Regularized multi--task learning
#@ Theodoros Evgeniou;Massimiliano Pontil
#t 2004
#c 0
#% 169358
#% 190581
#% 236495
#% 236497
#% 267027
#% 267046
#% 466750
#% 572779
#% 574235
#% 577258
#% 723239
#% 1271814
#! Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.

#index 769887
#* Fast discovery of connection subgraphs
#@ Christos Faloutsos;Kevin S. McCurley;Andrew Tomkins
#t 2004
#c 0
#% 63833
#% 249110
#% 283833
#% 291940
#% 319469
#% 342596
#% 348173
#% 438553
#% 577329
#% 728120
#% 729918
#% 729923
#% 730089
#% 770307
#! We define a connection subgraph as a small subgraph of a large graph that best captures the relationship between two nodes. The primary motivation for this work is to provide a paradigm for exploration and knowledge discovery in large social networks graphs. We present a formal definition of this problem, and an ideal solution based on electricity analogues. We then show how to accelerate the computations, to produce approximate, but high-quality connection subgraphs in real time on very large (disk resident) graphs.We describe our operational prototype, and we demonstrate results on a social network graph derived from the World Wide Web. Our graph contains 15 million nodes and 96 million edges, and our system still produces quality responses within seconds.

#index 769888
#* Systematic data selection to mine concept-drifting data streams
#@ Wei Fan
#t 2004
#c 0
#% 310500
#% 333931
#% 342600
#% 342639
#% 378388
#% 397380
#% 400847
#% 428155
#% 594012
#% 654489
#% 727888
#% 729932
#% 993958
#% 1016245
#% 1250172
#! One major problem of existing methods to mine data streams is that it makes ad hoc choices to combine most recent data with some amount of old data to search the new hypothesis. The assumption is that the additional old data always helps produce a more accurate hypothesis than using the most recent data only. We first criticize this notion and point out that using old data blindly is not better than "gambling"; in other words, it helps increase the accuracy only if we are "lucky." We discuss and analyze the situations where old data will help and what kind of old data will help. The practical problem on choosing the right example from old data is due to the formidable cost to compare different possibilities and models. This problem will go away if we have an algorithm that is extremely efficient to compare all sensible choices with little extra cost. Based on this observation, we propose a simple, efficient and accurate cross-validation decision tree ensemble method.

#index 769889
#* Efficient closed pattern mining in the presence of tough block constraints
#@ Krishna Gade;Jianyong Wang;George Karypis
#t 2004
#c 0
#% 227917
#% 248791
#% 274146
#% 280487
#% 300120
#% 310494
#% 316478
#% 399793
#% 399794
#% 464989
#% 465003
#% 466490
#% 466653
#% 480154
#% 481290
#% 576117
#% 577215
#% 631970
#% 729933
#% 729979
#! Various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemset-based constraints that better capture the underlying application requirements and characteristics. In this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern's items and its associated set of transactions. Block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets. However, developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier, these block constraints are tough as they are neither anti-monotone, monotone, nor convertible. To overcome this problem, we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called CBMiner to find the closed itemsets that satisfy the block constraints.

#index 769890
#* Discovering complex matchings across web query interfaces: a correlation mining approach
#@ Bin He;Kevin Chen-Chuan Chang;Jiawei Han
#t 2004
#c 0
#% 22948
#% 152934
#% 227919
#% 333932
#% 333990
#% 452846
#% 480645
#% 572314
#% 577214
#% 654459
#% 654467
#% 727869
#% 765410
#% 1712590
#! To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources. While complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this paper takes a conceptually novel approach by viewing schema matching as correlation mining, for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this "deep Web," query interfaces generally form complex matchings between attribute groups (e.g., [author] corresponds to [first name, last name] in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., [first name, last name]) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach. In particular, we develop the DCM framework, which consists of data preparation, dual mining of positive and negative correlations, and finally matching selection. Unlike previous correlation mining algorithms, which mainly focus on finding strong positive correlations, our algorithm cares both positive and negative correlations, especially the subtlety of negative correlations, due to its special importance in schema matching. This leads to the introduction of a new correlation measure, $H$-measure, distinct from those proposed in previous work. We evaluate our approach extensively and the results show good accuracy for discovering complex matchings.

#index 769891
#* Cyclic pattern kernels for predictive graph mining
#@ Tamás Horváth;Thomas Gärtner;Stefan Wrobel
#t 2004
#c 0
#% 116149
#% 190581
#% 232136
#% 269217
#% 281788
#% 299941
#% 331909
#% 342604
#% 466086
#% 466644
#% 577218
#% 629603
#% 727896
#% 740216
#! With applications in biology, the world-wide web, and several other areas, mining of graph-structured objects has received significant interest recently. One of the major research directions in this field is concerned with predictive data mining in graph databases where each instance is represented by a graph. Some of the proposed approaches for this task rely on the excellent classification performance of support vector machines. To control the computational cost of these approaches, the underlying kernel functions are based on frequent patterns. In contrast to these approaches, we propose a kernel function based on a natural set of cyclic and tree patterns independent of their frequency, and discuss its computational aspects. To practically demonstrate the effectiveness of our approach, we use the popular NCI-HIV molecule dataset. Our experimental results show that cyclic pattern kernels can be computed quickly and offer predictive performance superior to recent graph kernels based on frequent patterns.

#index 769892
#* Mining and summarizing customer reviews
#@ Minqing Hu;Bing Liu
#t 2004
#c 0
#% 71752
#% 78171
#% 118040
#% 180254
#% 194251
#% 211514
#% 279755
#% 280835
#% 457935
#% 481290
#% 529193
#% 577246
#% 577355
#% 741940
#% 746867
#% 746885
#% 755835
#% 756232
#% 786506
#% 786539
#% 815915
#% 854646
#% 1250237
#% 1478826
#! Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.

#index 769893
#* Interestingness of frequent itemsets using Bayesian networks as background knowledge
#@ Szymon Jaroszewicz;Dan A. Simovici
#t 2004
#c 0
#% 152934
#% 210182
#% 280409
#% 280436
#% 280522
#% 289947
#% 310494
#% 310496
#% 342597
#% 351595
#% 370075
#% 376266
#% 392618
#% 420062
#% 477784
#% 492767
#% 502132
#% 577214
#% 1271984
#! The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network. The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network. Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets, and for finding all attribute sets with a given minimum interestingness. Practical usefulness of the algorithms and their efficiency have been verified experimentally.

#index 769894
#* Mining the space of graph properties
#@ Glen Jeh;Jennifer Widom
#t 2004
#c 0
#% 36683
#% 216508
#% 282905
#% 309847
#% 348173
#% 479803
#% 481290
#% 546700
#% 577218
#% 577273
#% 577329
#% 629708
#% 660011
#! Existing data mining algorithms on graphs look for nodes satisfying specific properties, such as specific notions of structural similarity or specific measures of link-based importance. While such analyses for predetermined properties can be effective in well-understood domains, sometimes identifying an appropriate property for analysis can be a challenge, and focusing on a single property may neglect other important aspects of the data. In this paper, we develop a foundation for mining the properties themselves. We present a theoretical framework defining the space of graph properties, a variety of mining queries enabled by the framework, techniques to handle the enormous size of the query space, and an experimental system called F-Miner that demonstrates the utility and feasibility of property mining.

#index 769895
#* Web usage mining based on probabilistic latent semantic analysis
#@ Xin Jin;Yanzan Zhou;Bamshad Mobasher
#t 2004
#c 0
#% 200694
#% 266283
#% 268184
#% 280819
#% 308767
#% 308769
#% 309777
#% 330708
#% 413573
#% 420134
#% 425011
#% 457933
#% 466574
#% 577225
#% 586843
#% 630984
#% 661023
#% 724979
#% 739634
#% 768666
#% 963898
#% 1390152
#% 1650298
#! The primary goal of Web usage mining is the discovery of patterns in the navigational behavior of Web users. Standard approaches, such as clustering of user sessions and discovering association rules or frequent navigational paths, do not generally provide the ability to automatically characterize or quantify the unobservable factors that lead to common navigational patterns. It is, therefore, necessary to develop techniques that can automatically discover hidden semantic relationships among users as well as between users and Web objects. Probabilistic Latent Semantic Analysis (PLSA) is particularly useful in this context, since it can uncover latent semantic associations among users and pages based on the co-occurrence patterns of these pages in user sessions. In this paper, we develop a unified framework for the discovery and analysis of Web navigational patterns based on PLSA. We show the flexibility of this framework in characterizing various relationships among users and Web objects. Since these relationships are measured in terms of probabilities, we are able to use probabilistic inference to perform a variety of analysis tasks such as user segmentation, page classification, as well as predictive tasks such as collaborative recommendations. We demonstrate the effectiveness of our approach through experiments performed on real-world data sets.

#index 769896
#* Towards parameter-free data mining
#@ Eamonn Keogh;Stefano Lonardi;Chotirat Ann Ratanamahatana
#t 2004
#c 0
#% 61792
#% 201893
#% 234979
#% 281638
#% 297487
#% 310502
#% 310580
#% 342647
#% 420065
#% 453575
#% 465927
#% 466507
#% 577221
#% 617886
#% 617888
#% 650285
#% 662750
#% 727900
#% 729931
#% 729980
#! Most data mining algorithms require the setting of many input parameters. Two main dangers of working with parameter-laden algorithms are the following. First, incorrect settings may cause an algorithm to fail in finding the true patterns. Second, a perhaps more insidious problem is that the algorithm may report spurious patterns that do not really exist, or greatly overestimate the significance of the reported patterns. This is especially likely when the user fails to understand the role of parameters in the data mining process.Data mining algorithms should have as few parameters as possible, ideally none. A parameter-free algorithm would limit our ability to impose our prejudices, expectations, and presumptions on the problem at hand, and would let the data itself speak to us. In this work, we show that recent results in bioinformatics and computational theory hold great promise for a parameter-free data-mining paradigm. The results are motivated by observations in Kolmogorov complexity theory. However, as a practical matter, they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen or so lines of code. We will show that this approach is competitive or superior to the state-of-the-art approaches in anomaly/interestingness detection, classification, and clustering with empirical tests on time series/DNA/text/video datasets.

#index 769897
#* A graph-theoretic approach to extract storylines from search results
#@ Ravi Kumar;Uma Mahadevan;D. Sivakumar
#t 2004
#c 0
#% 118771
#% 262061
#% 268079
#% 281186
#% 281214
#% 290830
#% 594009
#% 729918
#% 788584
#! We present a graph-theoretic approach to discover storylines from search results. Storylines are windows that offer glimpses into interesting themes latent among the top search results for a query; they are different from, and complementary to, clusters obtained through traditional approaches. Our framework is axiomatically developed and combinatorial in nature, based on generalizations of the maximum induced matching problem on bipartite graphs. The core algorithmic task involved is to mine for signature structures in a robust graph representation of the search results. We present a very fast algorithm for this task based on local search. Experiments show that the collection of storylines extracted through our algorithm offers a concise organization of the wealth of information hidden beyond the first page of search results.

#index 769898
#* Incremental maintenance of quotient cube for median
#@ Cuiping Li;Gao Cong;Anthony K. H. Tung;Shan Wang
#t 2004
#c 0
#% 210182
#% 210208
#% 227869
#% 227880
#% 236410
#% 273916
#% 280448
#% 384416
#% 397388
#% 420053
#% 479450
#% 479646
#% 480141
#% 481951
#% 654446
#% 660006
#% 993996
#! Data cube pre-computation is an important concept for supporting OLAP(Online Analytical Processing) and has been studied extensively. It is often not feasible to compute a complete data cube due to the huge storage requirement. Recently proposed quotient cube addressed this issue through a partitioning method that groups cube cells into equivalence partitions. Such an approach is not only useful for distributive aggregate functions such as SUM but can also be applied to the holistic aggregate functions like MEDIAN.Maintaining a data cube for holistic aggregation is a hard problem since its difficulty lies in the fact that history tuple values must be kept in order to compute the new aggregate when tuples are inserted or deleted. The quotient cube makes the problem harder since we also need to maintain the equivalence classes. In this paper, we introduce two techniques called addset data structure and sliding window to deal with this problem. We develop efficient algorithms for maintaining a quotient cube with holistic aggregation functions that takes up reasonably small storage space. Performance study shows that our algorithms are effective, efficient and scalable over large databases.

#index 769899
#* Mining, indexing, and querying historical spatiotemporal data
#@ Nikos Mamoulis;Huiping Cao;George Kollios;Marios Hadjieleftheriou;Yufei Tao;David W. Cheung
#t 2004
#c 0
#% 287070
#% 310542
#% 425006
#% 427199
#% 452847
#% 458857
#% 464839
#% 464986
#% 480156
#% 480473
#% 480817
#% 481290
#% 527319
#% 631926
#% 745486
#! In many applications that track and analyze spatiotemporal data, movements obey periodic patterns; the objects follow the same routes (approximately) over regular time intervals. For example, people wake up at the same time and follow more or less the same route to their work everyday. The discovery of hidden periodic patterns in spatiotemporal data, apart from unveiling important information to the data analyst, can facilitate data management substantially. Based on this observation, we propose a framework that analyzes, manages, and queries object movements that follow such patterns. We define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic patterns. We also devise a novel, specialized index structure that can benefit from the discovered patterns to support more efficient execution of spatiotemporal queries. We evaluate our methods experimentally using datasets with object trajectories that exhibit periodicity.

#index 769900
#* Machine learning for online query relaxation
#@ Ion Muslea
#t 2004
#c 0
#% 6804
#% 45495
#% 136350
#% 166821
#% 213443
#% 245519
#% 333845
#% 340914
#% 387427
#% 436116
#% 442712
#% 445170
#% 458861
#% 462772
#% 462942
#% 463696
#% 479816
#% 504581
#% 715899
#! In this paper we provide a fast, data-driven solution to the failing query problem: given a query that returns an empty answer, how can one relax the query's constraints so that it returns a non-empty set of tuples? We introduce a novel algorithm, loqr, which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes. loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query.In a first step, loqr uses a small, randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute's value satisfies the constraints in the failing query; this query-driven operation is performed online for each failing query. In the second step, loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query; then it uses the attributes' values from this rule to relax the failing query's constraints. Our experiments on six application domains show that loqr is both robust and fast: it successfully relaxes more than 95% of the failing queries, and it takes under a second for processing queries that consist of up to 20 attributes (larger queries of up to 93 attributes are processed in several seconds).

#index 769901
#* Rapid detection of significant spatial clusters
#@ Daniel B. Neill;Andrew W. Moore
#t 2004
#c 0
#% 2115
#% 68091
#% 248792
#% 566128
#% 769901
#% 1290057
#! Given an N x N grid of squares, where each square has a count cij and an underlying population pij, our goal is to find the rectangular region with the highest density, and to calculate its significance by randomization. An arbitrary density function D, dependent on a region's total count C and total population P, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff's spatial scan statistic DK to find the most significant spatial disease cluster. A naive approach to finding the maximum density region requires O(N4) time, and is generally computationally infeasible. We present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufficiently dense regions, this method finds the maximum density region in O((N log N)2) time, in practice resulting in significant (20-2000x) speedups on both real and simulated datasets.

#index 769902
#* Turning CARTwheels: an alternating algorithm for mining redescriptions
#@ Naren Ramakrishnan;Deept Kumar;Bud Mishra;Malcolm Potts;Richard F. Helm
#t 2004
#c 0
#% 29587
#% 115608
#% 136350
#% 280419
#% 286671
#% 310494
#% 317130
#% 328429
#% 420109
#% 438133
#% 451052
#% 481290
#% 572314
#% 1272326
#! We present an unusual algorithm involving classification trees---CARTwheels---where two trees are grown in opposite directions so that they are joined at their leaves. This approach finds application in a new data mining task we formulate, called redescription mining. A redescription is a shift-of-vocabulary, or a different way of communicating information about a given subset of data; the goal of redescription mining is to find subsets of data that afford multiple descriptions. We highlight the importance of this problem in domains such as bioinformatics, which exhibit an underlying richness and diversity of data descriptors (e.g., genes can be studied in a variety of ways). CARTwheels exploits the duality between class partitions and path partitions in an induced classification tree to model and mine redescriptions. It helps integrate multiple forms of characterizing datasets, situates the knowledge gained from one dataset in the context of others, and harnesses high-level abstractions for uncovering cryptic and subtle features of data. Algorithm design decisions, implementation details, and experimental results are presented.

#index 769903
#* Selection, combination, and evaluation of effective software sensors for detecting abnormal computer usage
#@ Jude Shavlik;Mark Shavlik
#t 2004
#c 0
#% 158832
#% 451055
#% 536877
#! We present and empirically analyze a machine-learning approach for detecting intrusions on individual computers. Our Winnow-based algorithm continually monitors user and system behavior, recording such properties as the number of bytes transferred over the last 10 seconds, the programs that currently are running, and the load on the CPU. In all, hundreds of measurements are made and analyzed each second. Using this data, our algorithm creates a model that represents each particular computer's range of normal behavior. Parameters that determine when an alarm should be raised, due to abnormal activity, are set on a per-computer basis, based on an analysis of training data. A major issue in intrusion-detection systems is the need for very low false-alarm rates. Our empirical results suggest that it is possible to obtain high intrusion-detection rates (95%) and low false-alarm rates (less than one per day per computer), without "stealing" too many CPU cycles (less than 1%). We also report which system measurements are the most valuable in terms of detecting intrusions. A surprisingly large number of different measurements prove significantly useful.

#index 769904
#* A Bayesian network framework for reject inference
#@ Andrew Smith;Charles Elkan
#t 2004
#c 0
#% 476744
#% 577298
#% 716892
#% 727925
#% 770847
#% 1289281
#! Most learning methods assume that the training set is drawn randomly from the population to which the learned model is to be applied. However in many applications this assumption is invalid. For example, lending institutions create models of who is likely to repay a loan from training sets consisting of people in their records to whom loans were given in the past; however, the institution approved loan applications previously based on who was thought unlikely to default. Learning from only approved loans yields an incorrect model because the training set is a biased sample of the general population of applicants. The issue of including rejected samples in the learning process, or alternatively using rejected samples to adjust a model learned from accepted samples only, is called reject inference.The main contribution of this paper is a systematic analysis of different cases that arise in reject inference, with explanations of which cases arise in various real-world situations. We use Bayesian networks to formalize each case as a set of conditional independence relationships and identify eight cases, including the familiar missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) cases. For each case we present an overview of available learning algorithms. These algorithms have been published in separate fields of research, including epidemiology, econometrics, clinical trial evaluation, sociology, and credit scoring; our second major contribution is to describe these algorithms in a common framework.

#index 769905
#* Support envelopes: a technique for exploring the structure of association patterns
#@ Michael Steinbach;Pang-Ning Tan;Vipin Kumar
#t 2004
#c 0
#% 237200
#% 279120
#% 316709
#% 320944
#% 342610
#% 384416
#% 420062
#% 431033
#% 576118
#% 629704
#% 727897
#! This paper introduces support envelopes---a new tool for analyzing association patterns---and illustrates some of their properties, applications, and possible extensions. Specifically, the support envelope for a transaction data set and a specified pair of positive integers (m,n) consists of the items and transactions that need to be searched to find any association pattern involving m or more transactions and n or more items. For any transaction data set with M transactions and N items, there is a unique lattice of at most M*N support envelopes that captures the structure of the association patterns in that data set. Because support envelopes are not encumbered by a support threshold, this support lattice provides a complete view of the association structure of the data set, including association patterns that have low support. Furthermore, the boundary of the support lattice---the support boundary---has at most min(M,N) envelopes and is especially interesting since it bounds the maximum sizes of potential association patterns---not only for frequent, closed, and maximal itemsets, but also for patterns, such as error-tolerant itemsets, that are more general. The association structure can be represented graphically as a two-dimensional scatter plot of the (m,n) values associated with the support envelopes of the data set, a feature that is useful in the exploratory analysis of association patterns. Finally, the algorithm to compute support envelopes is simple and computationally efficient, and it is straightforward to parallelize the process of finding all the support envelopes.

#index 769906
#* Probabilistic author-topic models for information discovery
#@ Mark Steyvers;Padhraic Smyth;Michal Rosen-Zvi;Thomas Griffiths
#t 2004
#c 0
#% 118771
#% 220708
#% 280819
#% 304421
#% 310516
#% 318412
#% 438103
#% 578558
#% 584932
#% 722904
#% 729936
#% 788094
#! We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.

#index 769907
#* Scalable mining of large disk-based graph databases
#@ Chen Wang;Wei Wang;Jian Pei;Yongtai Zhu;Baile Shi
#t 2004
#c 0
#% 410276
#% 466644
#% 577218
#% 629603
#% 629646
#% 629708
#% 729938
#% 765429
#! Mining frequent structural patterns from graph databases is an interesting problem with broad applications. Most of the previous studies focus on pruning unfruitful search subspaces effectively, but few of them address the mining on large, disk-based databases. As many graph databases in applications cannot be held into main memory, scalable mining of large, disk-based graph databases remains a challenging problem. In this paper, we develop an effective index structure, ADI (for adjacency index), to support mining various graph patterns over large databases that cannot be held into main memory. The index is simple and efficient to build. Moreover, the new index structure can be easily adopted in various existing graph pattern mining algorithms. As an example, we adapt the well-known gSpan algorithm by using the ADI structure. The experimental results show that the new index structure enables the scalable graph pattern mining over large databases. In one set of the experiments, the new disk-based method can mine graph databases with one million graphs, while the original gSpan algorithm can only handle databases of up to 300 thousand graphs. Moreover, our new method is faster than gSpan when both can run in main memory.

#index 769908
#* Incorporating prior knowledge with weighted margin support vector machines
#@ Xiaoyun Wu;Rohini Srihari
#t 2004
#c 0
#% 190581
#% 262050
#% 269218
#% 272538
#% 280817
#% 304876
#% 402289
#% 458379
#% 464465
#% 466263
#% 466887
#% 642998
#% 1558464
#! Like many purely data-driven machine learning methods, Support Vector Machine (SVM) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance. In some applications, there might be human knowledge available that, in principle, could compensate for the lack of data. In this paper, we propose a simple generalization of SVM: Weighted Margin SVM (WMSVMs) that permits the incorporation of prior knowledge. We show that Sequential Minimal Optimization can be used in training WMSVM. We discuss the issues of incorporating prior knowledge using this rather general formulation. The experimental results show that the proposed methods of incorporating prior knowledge is effective.

#index 769909
#* Exploiting a support-based upper bound of Pearson's correlation coefficient for efficiently identifying strongly correlated pairs
#@ Hui Xiong;Shashi Shekhar;Pang-Ning Tan;Vipin Kumar
#t 2004
#c 0
#% 152934
#% 227919
#% 274146
#% 300120
#% 342597
#% 420112
#% 443466
#% 465003
#% 466669
#% 577215
#% 729971
#! Given a user-specified minimum correlation threshold θ and a market basket database with N items and T transactions, an all-strong-pairs correlation query finds all item pairs with correlations above the threshold θ. However, when the number of items and transactions are large, the computation cost of this query can be very high. In this paper, we identify an upper bound of Pearson's correlation coefficient for binary variables. This upper bound is not only much cheaper to compute than Pearson's correlation coefficient but also exhibits a special monotone property which allows pruning of many item pairs even without computing their upper bounds. A Two-step All-strong-Pairs corrElation que Ry (TAPER) algorithm is proposed to exploit these properties in a filter-and-refine manner. Furthermore, we provide an algebraic cost model which shows that the computation savings from pruning is independent or improves when the number of items is increased in data sets with common Zipf or linear rank-support distributions. Experimental results from synthetic and real data sets exhibit similar trends and show that the TAPER algorithm can be an order of magnitude faster than brute-force alternatives.

#index 769910
#* The complexity of mining maximal frequent itemsets and maximal frequent patterns
#@ Guizhen Yang
#t 2004
#c 0
#% 248791
#% 256604
#% 310507
#% 408396
#% 414902
#% 425006
#% 443503
#% 463903
#% 465003
#% 466644
#% 466664
#% 481290
#% 494028
#% 504526
#% 576118
#% 577218
#% 579314
#% 727909
#% 729933
#! Mining maximal frequent itemsets is one of the most fundamental problems in data mining. In this paper we study the complexity-theoretic aspects of maximal frequent itemset mining, from the perspective of counting the number of solutions. We present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions, given an arbitrary support threshold, is #P-complete, thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is NP-hard. This result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in P.We also extend our complexity analysis to other similar data mining problems dealing with complex data structures, such as sequences, trees, and graphs, which have attracted intensive research interests in recent years. Normally, in these problems a partial order among frequent patterns can be defined in such a way as to preserve the downward closure property, with maximal frequent patterns being those without any successor with respect to this partial order. We investigate several variants of these mining problems in which the patterns of interest are subsequences, subtrees, or subgraphs, and show that the associated problems of counting the number of maximal frequent patterns are all either #P-complete or #P-hard.

#index 769911
#* GPCA: an efficient dimension reduction scheme for image compression and retrieval
#@ Jieping Ye;Ravi Janardan;Qi Li
#t 2004
#c 0
#% 201893
#% 224113
#% 248798
#% 282481
#% 338442
#% 340309
#% 397387
#% 578399
#% 593842
#% 770769
#! Recent years have witnessed a dramatic increase in the quantity of image data collected, due to advances in fields such as medical imaging, reconnaissance, surveillance, astronomy, multimedia etc. With this increase has come the need to be able to store, transmit, and query large volumes of image data efficiently. A common operation on image databases is the retrieval of all images that are similar to a query image. For this, the images in the database are often represented as vectors in a high-dimensional space and a query is answered by retrieving all image vectors that are proximal to the query image in this space, under a suitable similarity metric. To overcome problems associated with high dimensionality, such as high storage and retrieval times, a dimension reduction step is usually applied to the vectors to concentrate relevant information in a small number of dimensions. Principal Component Analysis (PCA) is a well-known dimension reduction scheme. However, since it works with vectorized representations of images, PCA does not take into account the spatial locality of pixels in images. In this paper, a new dimension reduction scheme, called Generalized Principal Component Analysis (GPCA), is presented. This scheme works directly with images in their native state, as two-dimensional matrices, by projecting the images to a vector space that is the tensor product of two lower-dimensional vector spaces. Experiments on databases of face images show that, for the same amount of storage, GPCA is superior to PCA in terms of quality of the compressed images, query precision, and computational cost.

#index 769912
#* IDR/QR: an incremental dimension reduction algorithm via QR decomposition
#@ Jieping Ye;Qi Li;Hui Xiong;Haesun Park;Ravi Janardan;Vipin Kumar
#t 2004
#c 0
#% 80995
#% 212689
#% 224113
#% 235342
#% 251654
#% 294837
#% 315284
#% 324288
#% 342828
#% 581716
#% 726624
#% 729437
#% 1776405
#% 1828409
#% 1860188
#% 1862539
#! Dimension reduction is critical for many database and data mining applications, such as efficient storage and retrieval of high-dimensional data. In the literature, a well-known dimension reduction scheme is Linear Discriminant Analysis (LDA). The common aspect of previously proposed LDA based algorithms is the use of Singular Value Decomposition (SVD). Due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA, there is little work on designing incremental LDA algorithms. In this paper, we propose an LDA based incremental dimension reduction algorithm, called IDR/QR, which applies QR Decomposition rather than SVD. Unlike other LDA based algorithms, this algorithm does not require the whole data matrix in main memory. This is desirable for large data sets. More importantly, with the insertion of new data items, the IDR/QR algorithm can constrain the computational cost by applying efficient QR-updating techniques. Finally, we evaluate the effectiveness of the IDR/QR algorithm in terms of classification accuracy on the reduced dimensional space. Our experiments on several real-world data sets reveal that the accuracy achieved by the IDR/QR algorithm is very close to the best possible accuracy achieved by other LDA based algorithms. However, the IDR/QR algorithm has much less computational cost, especially when new data items are dynamically inserted.

#index 769913
#* On the discovery of significant statistical quantitative rules
#@ Hong Zhang;Balaji Padmanabhan;Alexander Tuzhilin
#t 2004
#c 0
#% 210160
#% 210162
#% 227917
#% 227919
#% 260150
#% 280422
#% 280456
#% 280458
#% 280477
#% 307109
#% 310496
#% 342631
#% 342640
#% 412588
#% 449566
#% 459025
#% 466068
#% 466494
#% 481290
#% 546821
#% 578689
#% 729914
#% 729935
#% 1290045
#! In this paper we study market share rules, rules that have a certain market share statistic associated with them. Such rules are particularly relevant for decision making from a business perspective. Motivated by market share rules, in this paper we consider statistical quantitative rules (SQ rules) that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule. Building on prior work, we present a statistical approach for learning all significant SQ rules, i.e., SQ rules for which a desired statistic lies outside a confidence interval computed for this rule. In particular we show how resampling techniques can be effectively used to learn significant rules. Since our method considers the significance of a large number of rules in parallel, it is susceptible to learning a certain number of "false" rules. To address this, we present a technique that can determine the number of significant SQ rules that can be expected by chance alone, and suggest that this number can be used to determine a "false discovery rate" for the learning procedure. We apply our methods to online consumer purchase data and report the results.

#index 769914
#* Fast mining of spatial collocations
#@ Xin Zhang;Nikos Mamoulis;David W. Cheung;Yutao Shou
#t 2004
#c 0
#% 2115
#% 152937
#% 210187
#% 248790
#% 342635
#% 342956
#% 420078
#% 427199
#% 481281
#% 481290
#% 527021
#% 527188
#% 727910
#% 728302
#% 729942
#! Spatial collocation patterns associate the co-existence of non-spatial features in a spatial neighborhood. An example of such a pattern can associate contaminated water reservoirs with certain deceases in their spatial neighborhood. Previous work on discovering collocation patterns converts neighborhoods of feature instances to itemsets and applies mining techniques for transactional data to discover the patterns. We propose a method that combines the discovery of spatial neighborhoods with the mining process. Our technique is an extension of a spatial join algorithm that operates on multiple inputs and counts long pattern instances. As demonstrated by experimentation, it yields significant performance improvements compared to previous approaches.

#index 769915
#* TiVo: making show recommendations using a distributed collaborative filtering architecture
#@ Kamal Ali;Wijnand van Stam
#t 2004
#c 0
#% 46803
#% 124010
#% 202009
#% 232117
#% 280447
#% 309095
#% 309845
#% 330687
#% 397153
#% 465928
#% 729437
#% 1650569
#! We describe the TiVo television show collaborative recommendation system which has been fielded in over one million TiVo clients for four years. Over this install base, TiVo currently has approximately 100 million ratings by users over approximately 30,000 distinct TV shows and movies. TiVo uses an item-item (show to show) form of collaborative filtering which obviates the need to keep any persistent memory of each user's viewing preferences at the TiVo server. Taking advantage of TiVo's client-server architecture has produced a novel collaborative filtering system in which the server does a minimum of work and most work is delegated to the numerous clients. Nevertheless, the server-side processing is also highly scalable and parallelizable. Although we have not performed formal empirical evaluations of its accuracy, internal studies have shown its recommendations to be useful even for multiple user households. TiVo's architecture also allows for throttling of the server so if more server-side resources become available, more correlations can be computed on the server allowing TiVo to make recommendations for niche audiences.

#index 769916
#* Predicting customer shopping lists from point-of-sale purchase data
#@ Chad Cumby;Andrew Fano;Rayid Ghani;Marko Krema
#t 2004
#c 0
#% 136350
#% 280456
#% 438371
#% 451055
#% 452633
#% 466078
#% 481290
#% 496282
#% 552192
#% 593786
#% 656741
#! This paper describes a prototype that predicts the shopping lists for customers in a retail store. The shopping list prediction is one aspect of a larger system we have developed for retailers to provide individual and personalized interactions with customers as they navigate through the retail store. Instead of using traditional personalization approaches, such as clustering or segmentation, we learn separate classifiers for each customer from historical transactional data. This allows us to make very fine-grained and accurate predictions about what items a particular individual customer will buy on a given shopping trip.We formally frame the shopping list prediction as a classification problem, describe the algorithms and methodology behind our system, its impact on the business case in which we frame it, and explore some of the properties of the data source that make it an interesting testbed for KDD algorithms. Our results show that we can predict a shopper's shopping list with high levels of accuracy, precision, and recall. We believe that this work impacts both the data mining and the retail business community. The formulation of shopping list prediction as a machine learning problem results in algorithms that should be useful beyond retail shopping list prediction. For retailers, the result is not only a practical system that increases revenues by up to 11%, but also enhances customer experience and loyalty by giving them the tools to individually interact with customers and anticipate their needs.

#index 769917
#* A rank sum test method for informative gene discovery
#@ Lin Deng;Jian Pei;Jinwen Ma;Dik Lun Lee
#t 2004
#c 0
#% 269217
#% 297684
#% 328374
#% 397641
#% 425048
#% 451139
#% 717415
#% 729437
#% 769919
#! Finding informative genes from microarray data is an important research problem in bioinformatics research and applications. Most of the existing methods rank features according to their discriminative capability and then find a subset of discriminative genes (usually top k genes). In particular, t-statistic criterion and its variants have been adopted extensively. This kind of methods rely on the statistics principle of t-test, which requires that the data follows a normal distribution. However, according to our investigation, the normality condition often cannot be met in real data sets.To avoid the assumption of the normality condition, in this paper, we propose a rank sum test method for informative gene discovery. The method uses a rank-sum statistic as the ranking criterion. Moreover, we propose using the significance level threshold, instead of the number of informative genes, as the parameter. The significance level threshold as a parameter carries the quality specification in statistics. We follow the Pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t-statistic method in theory.To verify the effectiveness of the rank sum method, we use support vector machine (SVM) to construct classifiers based on the identified informative genes on two well known data sets, namely colon data and leukemia data. The prediction accuracy reaches 96.2% on the colon data and 100% on the leukemia data. The results are clearly better than those from the previous feature ranking methods. By experiments, we also verify that using significance level threshold is more effective than directly specifying an arbitrary k.

#index 769918
#* Early detection of insider trading in option markets
#@ Steve Donoho
#t 2004
#c 0
#% 136350
#% 280413
#! "Inside information" comes in many forms: knowledge of a corporate takeover, a terrorist attack, unexpectedly poor earnings, the FDA's acceptance of a new drug, etc. Anyone who knows some piece of soon-to-break news possesses inside information. Historically, insider trading has been detected after the news is public, but this is often too late: fraud has been perpetrated, innocent investors have been disadvantaged, or terrorist acts have been carried out. This paper explores early detection of insider trading - detection before the news breaks. Data mining holds great promise for this emerging application, but the problem also poses significant challenges. We present the specific problem of insider trading in option markets, compare decision tree, logistic regression, and neural net results to results from an expert model, and discuss insights that knowledge discovery techniques shed upon this problem.

#index 769919
#* Mining coherent gene clusters from gene-sample-time microarray data
#@ Daxin Jiang;Jian Pei;Murali Ramanathan;Chun Tang;Aidong Zhang
#t 2004
#c 0
#% 328317
#% 397382
#% 469422
#% 727908
#% 729972
#% 729987
#! Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications. In this paper, we explore a novel type of gene-sample-time microarray data sets, which records the expression levels of various genes under a set of samples during a series of time points. In particular, we propose the mining of coherent gene clusters from such data sets. Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series. The coherent gene clusters may identify the samples corresponding to some phenotypes (e.g., diseases), and suggest the candidate genes correlated to the phenotypes. We present two efficient algorithms, namely the Sample-Gene Search and the Gene-Sample Search, to mine the complete set of coherent gene clusters. We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets. The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters.

#index 769920
#* Eigenspace-based anomaly detection in computer systems
#@ Tsuyoshi IDÉ;Hisashi KASHIMA
#t 2004
#c 0
#% 266065
#% 310552
#% 342621
#% 342622
#% 431105
#% 449074
#% 577295
#% 629630
#% 727845
#% 727893
#% 729911
#% 729968
#% 729983
#% 731608
#% 770344
#% 1289272
#% 1756868
#! We report on an automated runtime anomaly detection method at the application layer of multi-node computer systems. Although several network management systems are available in the market, none of them have sufficient capabilities to detect faults in multi-tier Web-based systems with redundancy. We model a Web-based system as a weighted graph, where each node represents a "service" and each edge represents a dependency between services. Since the edge weights vary greatly over time, the problem we address is that of anomaly detection from a time sequence of graphs.In our method, we first extract a feature vector from the adjacency matrix that represents the activities of all of the services. The heart of our method is to use the principal eigenvector of the eigenclusters of the graph. Then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived from the graph sequence. Given a critical probability, the threshold value is adaptively updated using a novel online algorithm.We demonstrate that a fault in a Web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system.

#index 769921
#* Effective localized regression for damage detection in large complex mechanical structures
#@ Aleksandar Lazarevic;Ramdev Kanapady;Chandrika Kamath
#t 2004
#c 0
#% 236497
#% 240267
#% 267036
#% 385564
#% 420078
#% 727924
#! In this paper, we propose a novel data mining technique for the efficient damage detection within the large-scale complex mechanical structures. Every mechanical structure is defined by the set of finite elements that are called structure elements. Large-scale complex structures may have extremely large number of structure elements, and predicting the failure in every single element using the original set of natural frequencies as features is exceptionally time-consuming task. Traditional data mining techniques simply predict failure in each structure element individually using global prediction models that are built considering all data records. In order to reduce the time complexity of these models, we propose a localized clustering-regression based approach that consists of two phases: (1) building a local cluster around a data record of interest and (2) predicting an intensity of damage only in those structure elements that correspond to data records from the built cluster. For each test data record, we first build a cluster of data records from training data around it. Then, for each data record that belongs to discovered cluster, we identify corresponding structure elements and we build a localized regression model for each of these structure elements. These regression models for specific structure elements are constructed using only a specific set of relevant natural frequencies and merely those data records that correspond to the failure of that structure element. Experiments performed on the problem of damage prediction in a large electric transmission tower frame indicate that the proposed localized clustering-regression based approach is significantly more accurate and more computationally efficient than our previous hierarchical clustering approach, as well as global prediction models.

#index 769922
#* Visually mining and monitoring massive time series
#@ Jessica Lin;Eamonn Keogh;Stefano Lonardi;Jeffrey P. Lankford;Donna M. Nystrom
#t 2004
#c 0
#% 28144
#% 172949
#% 280482
#% 285711
#% 310583
#% 333941
#% 345858
#% 397629
#% 434613
#% 443517
#% 494958
#% 529189
#% 546531
#% 577221
#% 577275
#% 617888
#% 619859
#% 632088
#% 641125
#% 641175
#% 662750
#% 729960
#% 729980
#% 814194
#% 1389010
#! Moments before the launch of every space vehicle, engineering discipline specialists must make a critical go/no-go decision. The cost of a false positive, allowing a launch in spite of a fault, or a false negative, stopping a potentially successful launch, can be measured in the tens of millions of dollars, not including the cost in morale and other more intangible detriments. The Aerospace Corporation is responsible for providing engineering assessments critical to the go/no-go decision for every Department of Defense space vehicle. These assessments are made by constantly monitoring streaming telemetry data in the hours before launch. We will introduce VizTree, a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments. VizTree was developed at the University of California, Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry. The use of a single tool for both aspects of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task. Our visualization approach works by transforming the time series into a symbolic representation, and encoding the data in a modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties. We demonstrate the utility of our system by comparing it with state-of-the-art batch algorithms on several real and synthetic datasets.

#index 769923
#* Learning to detect malicious executables in the wild
#@ Jeremy Z. Kolter;Marcus A. Maloof
#t 2004
#c 0
#% 38242
#% 92533
#% 116149
#% 136350
#% 157898
#% 161111
#% 246831
#% 260001
#% 269218
#% 288166
#% 290482
#% 296375
#% 310519
#% 312727
#% 331909
#% 341700
#% 376266
#% 385946
#% 391120
#% 424997
#% 441711
#% 458379
#% 465754
#% 664717
#% 963769
#% 963770
#% 1275354
#! In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.

#index 769924
#* Predicting prostate cancer recurrence via maximizing the concordance index
#@ Lian Yan;David Verbel;Olivier Saidi
#t 2004
#c 0
#% 73441
#% 270875
#% 918507
#% 1843691
#% 1860233
#! In order to effectively use machine learning algorithms, e.g., neural networks, for the analysis of survival data, the correct treatment of censored data is crucial. The concordance index (CI) is a typical metric for quantifying the predictive ability of a survival model. We propose a new algorithm that directly uses the CI as the objective function to train a model, which predicts whether an event will eventually occur or not. Directly optimizing the CI allows the model to make complete use of the information from both censored and non-censored observations. In particular, we approximate the CI via a differentiable function so that gradient-based methods can be used to train the model. We applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy. Compared with the traditional Cox proportional hazards model and several other algorithms based on neural networks and support vector machines, our algorithm achieves a significant improvement in being able to identify high-risk and low-risk groups of patients.

#index 769925
#* Density-based spam detector
#@ Kenichi YOSHIDA;Fuminori ADACHI;Takashi WASHIO;Hiroshi MOTODA;Teruaki HOMMA;Akihiro NAKASHIMA;Hiromitsu FUJIKAWA;Katsuyuki YAMAZAKI
#t 2004
#c 0
#% 25475
#% 35952
#% 136350
#% 252608
#% 268092
#% 269218
#% 376266
#% 406493
#% 798002
#% 1650665
#! The volume of mass unsolicited electronic mail, often known as spam, has recently increased enormously and has become a serious threat to not only the Internet but also to society. This paper proposes a new spam detection method which uses document space density information. Although it requires extensive e-mail traffic to acquire the necessary information, an unsupervised learning engine with a short white list can achieve a 98% recall rate and 100% precision. A direct-mapped cache method contributes handling of over 13,000 e-mails per second. Experimental results, which were conducted using over 50 million actual e-mails of traffic, are also reported in this paper.

#index 769926
#* V-Miner: using enhanced parallel coordinates to mine product design and test data
#@ Kaidi Zhao;Bing Liu;Thomas M. Tirpak;Andreas Schaller
#t 2004
#c 0
#% 25351
#% 65341
#% 136350
#% 286721
#% 436116
#% 481290
#% 481611
#% 529288
#! Analyzing data to find trends, correlations, and stable patterns is an important task in many industrial applications. This paper proposes a new technique based on parallel coordinate visualization. Previous work on parallel coordinate methods has shown that they are effective only when variables that are correlated and/or show similar patterns are displayed adjacently. Although current parallel coordinate tools allow the user to manually rearrange the order of variables, this process is very time-consuming when the number of variables is large. Automated assistance is required. This paper introduces an edit-distance based technique to rearrange variables so that interesting change patterns can be easily detected visually. The Visual Miner (V-Miner) software includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined or displayed by the system. In addition, the system can filter data according to rules sets imported from other data mining tools. This feature was found very helpful in practice, because it enables decision makers to visually identify interesting rules and data segments for further analysis or data mining. This paper begins with an introduction to the proposed techniques and the V-Miner system. Next, a case study illustrates how V-Miner has been used at Motorola to guide product design and test decisions.

#index 769927
#* On demand classification of data streams
#@ Charu C. Aggarwal;Jiawei Han;Jianyong Wang;Philip S. Yu
#t 2004
#c 0
#% 210173
#% 310500
#% 342600
#% 378388
#% 654489
#% 729437
#% 1015261
#! Current models of the classification problem do not effectively handle bursts of particular classes coming in at different times. In fact, the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets. Our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing streams are used for dynamic classification of data sets. This model reflects real life situations effectively, since it is desirable to classify test streams in real time over an evolving training and test stream. The aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream. In order to achieve this goal, we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier. The empirical results indicate that the system maintains a high classification accuracy in an evolving data stream, while providing an efficient solution to the classification task.

#index 769928
#* A generalized maximum entropy approach to bregman co-clustering and matrix approximation
#@ Arindam Banerjee;Inderjit Dhillon;Joydeep Ghosh;Srujana Merugu;Dharmendra S. Modha
#t 2004
#c 0
#% 115608
#% 382854
#% 469422
#% 729918
#! Co-clustering is a powerful data mining technique with varied applications such as text clustering, microarray analysis and recommender systems. Recently, an information-theoretic co-clustering approach applicable to empirical joint probability distributions was proposed. In many situations, co-clustering of more general matrices is desired. In this paper, we present a substantially generalized co-clustering framework wherein any Bregman divergence can be used in the objective function, and various conditional expectation based constraints can be considered based on the statistics that need to be preserved. Analysis of the co-clustering problem leads to the minimum Bregman information principle, which generalizes the maximum entropy principle, and yields an elegant meta algorithm that is guaranteed to achieve local optimality. Our methodology yields new algorithms and also encompasses several previously known clustering and co-clustering algorithms based on alternate minimization.

#index 769929
#* An objective evaluation criterion for clustering
#@ Arindam Banerjee;John Langford
#t 2004
#c 0
#% 36672
#% 115608
#% 425010
#% 722934
#% 729911
#% 1861357
#! We propose and test an objective criterion for evaluation of clustering performance: How well does a clustering algorithm run on unlabeled data aid a classification algorithm? The accuracy is quantified using the PAC-MDL bound [3] in a semisupervised setting. Clustering algorithms which naturally separate the data according to (hidden) labels with a small number of clusters perform well. A simple extension of the argument leads to an objective model selection method. Experimental results on text analysis datasets demonstrate that this approach empirically results in very competitive bounds on test set performance on natural datasets.

#index 769930
#* Column-generation boosting methods for mixture of kernels
#@ Jinbo Bi;Tong Zhang;Kristin P. Bennett
#t 2004
#c 0
#% 190581
#% 361100
#% 425033
#% 425063
#% 577213
#% 763697
#% 1762960
#! We devise a boosting approach to classification and regression based on column generation using a mixture of kernels. Traditional kernel methods construct models based on a single positive semi-definite kernel with the type of kernel predefined and kernel parameters chosen according to cross-validation performance. Our approach creates models that are mixtures of a library of kernel models, and our algorithm automatically determines kernels to be used in the final model. The 1-norm and 2-norm regularization methods are employed to restrict the ensemble of kernel models. The proposed method produces sparser solutions, and thus significantly reduces the testing time. By extending the column generation (CG) optimization which existed for linear programs with 1-norm regularization to quadratic programs with 2-norm regularization, we are able to solve many learning formulations by leveraging various algorithms for constructing single kernel models. By giving different priorities to columns to be generated, we are able to scale CG boosting to large datasets. Experimental results on benchmark data are included to demonstrate its effectiveness.

#index 769931
#* IncSpan: incremental mining of sequential patterns in large database
#@ Hong Cheng;Xifeng Yan;Jiawei Han
#t 2004
#c 0
#% 287242
#% 425006
#% 459006
#% 463903
#% 464204
#% 464996
#% 479971
#% 481754
#% 502121
#% 577256
#% 646296
#% 729938
#% 745515
#! Many real life sequence databases grow incrementally. It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow, or when some new sequences are added into the database. Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates. However, it is nontrivial to mine sequential patterns incrementally, especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones. In this study, we develop an efficient algorithm, IncSpan, for incremental mining of sequential patterns, by exploring some interesting properties. Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin.

#index 769932
#* Parallel computation of high dimensional robust correlation and covariance matrices
#@ James Chilson;Raymond Ng;Alan Wagner;Ruben Zamar
#t 2004
#c 0
#% 296697
#% 678708
#! The computation of covariance and correlation matrices are critical to many data mining applications and processes. Unfortunately the classical covariance and correlation matrices are very sensitive to outliers. Robust methods, such as QC and the Maronna method, have been proposed. However, existing algorithms for QC only give acceptable performance when the dimensionality of the matrix is in the hundreds; and the Maronna method is rarely used in practice because of its high computational cost.In this paper, we develop parallel algorithms for both QC and the Maronna method. We evaluate these parallel algorithms using a real data set of the gene expression of over 6,000 genes, giving rise to a matrix of over 18 million entries. In our experimental evaluation, we explore scalability in dimensionality and in the number of processors. We also compare the parallel behaviours of the two methods. After thorough experimentation, we conclude that for many data mining applications, both QC and Maronna are viable options. Less robust, but faster, QC is the recommended choice for small parallel platforms. On the other hand, the Maronna method is the recommended choice when a high degree of robustness is required, or when the parallel platform features a high number of processors.

#index 769933
#* Belief state approaches to signaling alarms in surveillance systems
#@ Kaustav Das;Andrew Moore;Jeff Schneider
#t 2004
#c 0
#% 252183
#% 777338
#! Surveillance systems have long been used to monitor industrial processes and are becoming increasingly popular in public health and anti-terrorism applications. Most early detection systems produce a time series of p-values or some other statistic as their output. Typically, the decision to signal an alarm is based on a threshold or other simple algorithm such as CUSUM that accumulates detection information temporally.We formulate a POMDP model of underlying events and observations from a detector. We solve the model and show how it is used for single-output detectors. When dealing with spatio-temporal data, scan statistics are a popular method of building detectors. We describe the use of scan statistics in surveillance and how our POMDP model can be used to perform alarm signaling with them. We compare the results obtained by our method with simple thresholding and CUSUM on synthetic and semi-synthetic health data.

#index 769934
#* Locating secret messages in images
#@ Ian Davidson;Goutam Paul
#t 2004
#c 0
#% 38014
#% 52008
#% 316709
#% 333929
#% 342638
#% 539566
#% 568295
#% 629685
#% 727892
#% 963756
#% 1970906
#! Steganography involves hiding messages in innocuous media such as images, while steganalysis is the field of detecting these secret messages. The ultimate goal of steganalysis is two-fold: making a binary classification of a file as stego-bearing or innocent, and secondly, locating the hidden message with an aim to extracting, sterilizing or manipulating it. Almost all steganalysis approaches (known as attacks) focus on the first of these two issues. In this paper, we explore the difficult related problem: given that we know an image file contains steganography, locate which pixels contain the message. We treat the hidden message location problem as outlier detection using probability/energy measures of images motivated by the image restoration community. Pixels contributing the most to the energy calculations of an image are deemed outliers. Typically, of the top third of one percent of most energized pixels (outliers), we find that 87% are stego-bearing in color images and 61% in grayscale images. In all image types only 1% of all pixels are stego-bearing indicating our techniques provides a substantial lift over random guessing.

#index 769935
#* Kernel k-means: spectral clustering and normalized cuts
#@ Inderjit S. Dhillon;Yuqiang Guan;Brian Kulis
#t 2004
#c 0
#% 266426
#% 309208
#% 313959
#% 594009
#% 629666
#% 724227
#% 1860974
#! Kernel k-means and spectral clustering have both been used to identify clusters that are non-linearly separable in input space. Despite significant research, these methods have remained only loosely related. In this paper, we give an explicit theoretical connection between them. We show the generality of the weighted kernel k-means objective function, and derive the spectral clustering objective of normalized cut as a special case. Given a positive definite similarity matrix, our results lead to a novel weighted kernel k-means algorithm that monotonically decreases the normalized cut. This has important implications: a) eigenvector-based algorithms, which can be computationally prohibitive, are not essential for minimizing normalized cuts, b) various techniques, such as local search and acceleration schemes, may be used to improve the quality as well as speed of kernel k-means. Finally, we present results on several interesting data sets, including diametrical clustering of large gene-expression matrices and a handwriting recognition data set.

#index 769936
#* A microeconomic data mining problem: customer-oriented catalog segmentation
#@ Martin Ester;Rong Ge;Wen Jin;Zengjian Hu
#t 2004
#c 0
#% 249305
#% 256685
#% 310548
#% 408396
#% 420082
#% 458833
#% 502130
#% 577291
#% 630986
#% 727906
#! The microeconomic framework for data mining [7] assumes that an enterprise chooses a decision maximizing the overall utility over all customers where the contribution of a customer is a function of the data available on that customer. In Catalog Segmentation, the enterprise wants to design k product catalogs of size r that maximize the overall number of catalog products purchased. However, there are many applications where a customer, once attracted to an enterprise, would purchase more products beyond the ones contained in the catalog. Therefore, in this paper, we investigate an alternative problem formulation, that we call Customer-Oriented Catalog Segmentation, where the overall utility is measured by the number of customers that have at least a specified minimum interest t in the catalogs. We formally introduce the Customer-Oriented Catalog Segmentation problem and discuss its complexity. Then we investigate two different paradigms to design efficient, approximate algorithms for the Customer-Oriented Catalog Segmentation problem, greedy (deterministic) and randomized algorithms. Since greedy algorithms may be trapped in a local optimum and randomized algorithms crucially depend on a reasonable initial solution, we explore a combination of these two paradigms. Our experimental evaluation on synthetic and real data demonstrates that the new algorithms yield catalogs of significantly higher utility compared to classical Catalog Segmentation algorithms.

#index 769937
#* k-TTP: a new privacy model for large-scale distributed environments
#@ Bobi Gilburd;Assaf Schuster;Ran Wolff
#t 2004
#c 0
#% 23638
#% 481290
#% 576761
#% 577289
#% 727926
#% 763583
#! Secure multiparty computation allows parties to jointly compute a function of their private inputs without revealing anything but the output. Theoretical results [2] provide a general construction of such protocols for any function. Protocols obtained in this way are, however, inefficient, and thus, practically speaking, useless when a large number of participants are involved.The contribution of this paper is to define a new privacy model -- k-privacy -- by means of an innovative, yet natural generalization of the accepted trusted third party model. This allows implementing cryptographically secure efficient primitives for real-world large-scale distributed systems.As an example for the usefulness of the proposed model, we employ k-privacy to introduce a technique for obtaining knowledge -- by way of an association-rule mining algorithm -- from large-scale Data Grids, while ensuring that the privacy is cryptographically secure.

#index 769938
#* Diagnosing extrapolation: tree-based density estimation
#@ Giles Hooker
#t 2004
#c 0
#% 136350
#! There has historically been very little concern with extrapolation in Machine Learning, yet extrapolation can be critical to diagnose. Predictor functions are almost always learned on a set of highly correlated data comprising a very small segment of predictor space. Moreover, flexible predictors, by their very nature, are not controlled at points of extrapolation. This becomes a problem for diagnostic tools that require evaluation on a product distribution. It is also an issue when we are trying to optimize a response over some variable in the input space. Finally, it can be a problem in non-static systems in which the underlying predictor distribution gradually drifts with time or when typographical errors misrecord the values of some predictors.We present a diagnosis for extrapolation as a statistical test for a point originating from the data distribution as opposed to a null hypothesis uniform distribution. This allows us to employ general classification methods for estimating such a test statistic. Further, we observe that CART can be modified to accept an exact distribution as an argument, providing a better classification tool which becomes our extrapolation-detection procedure. We explore some of the advantages of this approach and present examples of its practical application.

#index 769939
#* Discovering additive structure in black box functions
#@ Giles Hooker
#t 2004
#c 0
#% 209021
#% 215554
#% 331876
#% 481290
#% 579135
#! Many automated learning procedures lack interpretability, operating effectively as a black box: providing a prediction tool but no explanation of the underlying dynamics that drive it. A common approach to interpretation is to plot the dependence of a learned function on one or two predictors. We present a method that seeks not to display the behavior of a function, but to evaluate the importance of non-additive interactions within any set of variables. Should the function be close to a sum of low dimensional components, these components can be viewed and even modeled parametrically. Alternatively, the work here provides an indication of where intrinsically high-dimensional behavior takes place.The calculations used in this paper correspond closely with the functional ANOVA decomposition; a well-developed construction in Statistics. In particular, the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models. The algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes.

#index 769940
#* SPIN: mining maximal frequent subgraphs from graph databases
#@ Jun Huan;Wei Wang;Jan Prins;Jiong Yang
#t 2004
#c 0
#% 273922
#% 465003
#% 466644
#% 466664
#% 478622
#% 577218
#% 629603
#% 629606
#% 629646
#% 629708
#% 727828
#% 727845
#% 729938
#% 742493
#% 745514
#% 765429
#! One fundamental challenge for mining recurring subgraphs from semi-structured data sets is the overwhelming abundance of such patterns. In large graph databases, the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resources. In this paper, we propose a new algorithm that mines only maximal frequent subgraphs, i.e. subgraphs that are not a part of any other frequent subgraphs. This may exponentially decrease the size of the output set in the best case; in our experiments on practical data sets, mining maximal frequent subgraphs reduces the total number of mined patterns by two to three orders of magnitude.Our method first mines all frequent trees from a general graph database and then reconstructs all maximal subgraphs from the mined trees. Using two chemical structure benchmarks and a set of synthetic graph data sets, we demonstrate that, in addition to decreasing the output size, our algorithm can achieve a five-fold speed up over the current state-of-the-art subgraph mining algorithms.

#index 769941
#* On detecting space-time clusters
#@ Vijay S. Iyengar
#t 2004
#c 0
#% 51676
#% 115747
#% 248792
#% 369236
#% 1290662
#! Detection of space-time clusters is an important function in various domains (e.g., epidemiology and public health). The pioneering work on the spatial scan statistic is often used as the basis to detect and evaluate such clusters. State-of-the-art systems based on this approach detect clusters with restrictive shapes that cannot model growth and shifts in location over time. We extend these methods significantly by using the flexible square pyramid shape to model such effects. A heuristic search method is developed to detect the most likely clusters using a randomized algorithm in combination with geometric shapes processing. The use of Monte Carlo methods in the original scan statistic formulation is continued in our work to address the multiple hypothesis testing issues. Our method is applied to a real data set on brain cancer occurrences over a 19 year period. The cluster detected by our method shows both growth and movement which could not have been modeled with the simpler cylindrical shapes used earlier. Our general framework can be extended quite easily to handle other flexible shapes for the space-time clusters.

#index 769942
#* Why collective inference improves relational classification
#@ David Jensen;Jennifer Neville;Brian Gallagher
#t 2004
#c 0
#% 248810
#% 342596
#% 430761
#% 464449
#% 466896
#% 529493
#% 727834
#% 1289267
#% 1650403
#% 1786967
#! Procedures for collective inference make simultaneous statistical judgments about the same variables for a set of related data instances. For example, collective inference could be used to simultaneously classify a set of hyperlinked documents or infer the legitimacy of a set of related financial transactions. Several recent studies indicate that collective inference can significantly reduce classification error when compared with traditional inference techniques. We investigate the underlying mechanisms for this error reduction by reviewing past work on collective inference and characterizing different types of statistical models used for making inference in relational data. We show important differences among these models, and we characterize the necessary and sufficient conditions for reduced classification error based on experiments with real and simulated data.

#index 769943
#* When do data mining results violate privacy?
#@ Murat Kantarcioǧlu;Jiashun Jin;Chris Clifton
#t 2004
#c 0
#% 67453
#% 290482
#% 300184
#% 333876
#% 575966
#% 577233
#% 635215
#% 635219
#% 662761
#% 727904
#% 729930
#% 772829
#% 809530
#% 993988
#! Privacy-preserving data mining has concentrated on obtaining valid results when the input data is private. An extreme example is Secure Multiparty Computation-based methods, where only the results are revealed. However, this still leaves a potential privacy breach: Do the results themselves violate privacy? This paper explores this issue, developing a framework under which this question can be addressed. Metrics are proposed, along with analysis that those metrics are consistent in the face of apparent problems.

#index 769944
#* Improved robustness of signature-based near-replica detection via lexicon randomization
#@ Aleksander Kołcz;Abdur Chowdhury;Joshua Alspector
#t 2004
#c 0
#% 201935
#% 255137
#% 345087
#% 397204
#% 479973
#% 504572
#% 571725
#% 608643
#% 616528
#% 728115
#% 729027
#% 730067
#% 748016
#% 855034
#! Detection of near duplicate documents is an important problem in many data mining and information filtering applications. When faced with massive quantities of data, traditional duplicate detection techniques relying on direct inter-document similarity computation (e.g., using the cosine measure) are often not feasible given the time and memory performance constraints. On the other hand, fingerprint-based methods, such as I-Match, are very attractive computationally but may be brittle with respect to small changes to document content. We focus on approaches to near-replica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization. In experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional I-Match, with the relative improvement in duplicate-document recall reaching as high as 40-60%. The large gains in detection accuracy are offset by only small increases in computational requirements.

#index 769945
#* Learning spatially variant dissimilarity (SVaD) measures
#@ Krishna Kummamuru;Raghu Krishnapuram;Rakesh Agrawal
#t 2004
#c 0
#% 309128
#% 425010
#% 529216
#% 1788041
#! Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed (dis)similarity measure between the data points in feature space. This makes the type of clusters identified highly dependent on the assumed similarity measure. Building on recent work in this area, we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data. The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure. Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature.

#index 769946
#* Clustering moving objects
#@ Yifan Li;Jiawei Han;Jiong Yang
#t 2004
#c 0
#% 114667
#% 210173
#% 236700
#% 248790
#% 248792
#% 273890
#% 282141
#% 295512
#% 296738
#% 300174
#% 397377
#% 438137
#% 450723
#% 481281
#% 566128
#% 656803
#% 729917
#% 1015297
#! Due to the advances in positioning technologies, the real time information of moving objects becomes increasingly available, which has posed new challenges to the database research. As a long-standing technique to identify overall distribution patterns in data, clustering has achieved brilliant successes in analyzing static datasets. In this paper, we study the problem of clustering moving objects, which could catch interesting pattern changes during the motion process and provide better insight into the essence of the mobile data points. In order to catch the spatial-temporal regularities of moving objects and handle large amounts of data, micro-clustering [20] is employed. Efficient techniques are proposed to keep the moving micro-clusters geographically small. Important events such as the collisions among moving micro-clusters are also identified. In this way, high quality moving micro-clusters are dynamically maintained, which leads to fast and competitive clustering result at any given time instance. We validate our approaches with a through experimental evaluation, where orders of magnitude improvement on running time is observed over normal K-Means clustering method [14].

#index 769947
#* A framework for ontology-driven subspace clustering
#@ Jinze Liu;Wei Wang;Jiong Yang
#t 2004
#c 0
#% 248792
#% 273891
#% 280417
#% 300131
#% 342621
#% 397382
#% 397632
#% 469422
#% 480124
#! Traditional clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes. While domain knowledge is always the best way to justify clustering, few clustering algorithms have ever take domain knowledge into consideration. In this paper, the domain knowledge is represented by hierarchical ontology. We develop a framework by directly incorporating domain knowledge into clustering process, yielding a set of clusters with strong ontology implication. During the clustering process, ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithms. Meanwhile, the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace clusters with significant categorical enrichment onto the ontology hierarchy. Our experiments on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality. Meanwhile, many hierarchical organizations of gene clusters corresponding to a sub-hierarchies in gene ontology were also successfully captured.

#index 769948
#* The IOC algorithm: efficient many-class non-parametric classification for high-dimensional data
#@ Ting Liu;Ke Yang;Andrew W. Moore
#t 2004
#c 0
#% 2115
#% 92533
#% 105666
#% 264161
#% 317313
#% 479462
#% 479649
#% 479973
#% 528164
#% 697311
#% 884718
#! This paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets.K nearest neighbor remains a popular classification technique, especially in areas such as computer vision, drug activity prediction and astrophysics. Furthermore, many more modern classifiers, such as kernel-based Bayes classifiers or the prediction phase of SVMs, require computational regimes similar to k-NN. We believe that tractable k-NN algorithms therefore continue to be important.This paper relies on the insight that even with many classes, the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those k nearest neighbors. This insight was previously used in (Liu et al., 2003) in two algorithms called KNS2 and KNS3 which dealt with fast classification in the case of two classes. In this paper we show how a different approach, IOC (standing for the International Olympic Committee) can apply to the case of n classes where n 2.IOC assumes a slightly different processing of the datapoints in the neighborhood of the query. This allows it to search a set of metric trees, one for each class. During the searches it is possible to quickly prune away classes that cannot possibly be the majority.We give experimental results on datasets of up to 5.8 x 105 records and 1.5 x 103 attributes, frequently showing an order of magnitude acceleration compared with each of (i) conventional linear scan, (ii) a well-known independent SR-tree implementation of conventional k-NN and (iii) a highly optimized conventional k-NN metric tree search.

#index 769949
#* Sleeved coclustering
#@ Avraham A. Melkman;Eran Shaham
#t 2004
#c 0
#% 248792
#% 273891
#% 300131
#% 342621
#% 397382
#% 397384
#% 469422
#% 469425
#% 589430
#! A coCluster of a m x n matrix X is a submatrix determined by a subset of the rows and a subset of the columns. The problem of finding coClusters with specific properties is of interest, in particular, in the analysis of microarray experiments. In that case the entries of the matrix X are the expression levels of $m$ genes in each of $n$ tissue samples. One goal of the analysis is to extract a subset of the samples and a subset of the genes, such that the expression levels of the chosen genes behave similarly across the subset of the samples, presumably reflecting an underlying regulatory mechanism governing the expression level of the genes.We propose to base the similarity of the genes in a coCluster on a simple biological model, in which the strength of the regulatory mechanism in sample j is Hj, and the response strength of gene i to the regulatory mechanism is Gi. In other words, every two genes participating in a good coCluster should have expression values in each of the participating samples, whose ratio is a constant depending only on the two genes. Noise in the expression levels of genes is taken into account by allowing a deviation from the model, measured by a relative error criterion. The sleeve-width of the coCluster reflects the extent to which entry i,j in the coCluster is allowed to deviate, relatively, from being expressed as the product GiHj.We present a polynomial-time Monte-Carlo algorithm which outputs a list of coClusters whose sleeve-widths do not exceed a prespecified value. Moreover, we prove that the list includes, with fixed probability, a coCluster which is near-optimal in its dimensions. Extensive experimentation with synthetic data shows that the algorithm performs well.

#index 769950
#* Semantic representation: search and mining of multimedia content
#@ Apostol (Paul) Natsev;Milind R. Naphade;John R. Smith
#t 2004
#c 0
#% 190581
#% 252304
#% 342827
#% 480625
#% 1857902
#! Semantic understanding of multimedia content is critical in enabling effective access to all forms of digital media data. By making large media repositories searchable, semantic content descriptions greatly enhance the value of such data. Automatic semantic understanding is a very challenging problem and most media databases resort to describing content in terms of low-level features or using manually ascribed annotations. Recent techniques focus on detecting semantic concepts in video, such as indoor, outdoor, face, people, nature, etc. This approach works for a fixed lexicon for which annotated training examples exist. In this paper we consider the problem of using such semantic concept detection to map the video clips into semantic spaces. This is done by constructing a model vector that acts as a compact semantic representation of the underlying content. We then present experiments in the semantic spaces leveraging such information for enhanced semantic retrieval, classification, visualization, and data mining purposes. We evaluate these ideas using a large video corpus and demonstrate significant performance gains in retrieval effectiveness.

#index 769951
#* A quickstart in frequent structure mining can make a difference
#@ Siegfried Nijssen;Joost N. Kok
#t 2004
#c 0
#% 431105
#% 466644
#% 577218
#% 727845
#% 729938
#% 737334
#% 765125
#% 1289265
#! Given a database, structure mining algorithms search for substructures that satisfy constraints such as minimum frequency, minimum confidence, minimum interest and maximum frequency. Examples of substructures include graphs, trees and paths. For these substructures many mining algorithms have been proposed. In order to make graph mining more efficient, we investigate the use of the "quickstart principle", which is based on the fact that these classes of structures are contained in each other, thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity. We introduce the GrAph/Sequence/Tree extractiON (Gaston) algorithm that implements this idea by searching first for frequent paths, then frequent free trees and finally cyclic graphs. We investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives.

#index 769952
#* Automatic multimedia cross-modal correlation discovery
#@ Jia-Yu Pan;Hyung-Jeong Yang;Christos Faloutsos;Pinar Duygulu
#t 2004
#c 0
#% 248027
#% 262217
#% 268079
#% 282905
#% 313959
#% 348173
#% 359751
#% 434882
#% 438054
#% 457912
#% 465916
#% 480093
#% 489580
#% 642989
#% 721163
#% 722927
#% 741122
#% 741169
#% 1390190
#! Given an image (or video clip, or audio song), how do we automatically assign keywords to it? The general problem is to find correlations across the media in a collection of multimedia objects like video clips, with colors, and/or motion, and/or audio, and/or text scripts. We propose a novel, graph-based approach, "MMG", to discover such cross-modal correlations.Our "MMG" method requires no tuning, no clustering, no user-determined constants; it can be applied to any multimedia collection, as long as we have a similarity function for each medium; and it scales linearly with the database size. We report auto-captioning experiments on the "standard" Corel image database of 680 MB, where it outperforms domain specific, fine-tuned methods by up to 10 percentage points in captioning accuracy (50% relative improvement).

#index 769953
#* Estimating the size of the telephone universe: a Bayesian Mark-recapture approach
#@ David Poole
#t 2004
#c 0
#% 310488
#% 978813
#! Mark-recapture models have for many years been used to estimate the unknown sizes of animal and bird populations. In this article we adapt a finite mixture mark-recapture model in order to estimate the number of active telephone lines in the USA. The idea is to use the calling patterns of lines that are observed on the long distance network to estimate the number of lines that do not appear on the network. We present a Bayesian approach and use Markov chain Monte Carlo methods to obtain inference from the posterior distributions of the model parameters. At the state level, our results are in fairly good agreement with recent published reports on line counts. For lines that are easily classified as business or residence, the estimates have low variance. When the classification is unknown, the variability increases considerably. Results are insensitive to changes in the prior distributions. We discuss the significant computational and data mining challenges caused by the scale of the data, approximately 350 million call-detail records per day observed over a number of weeks.

#index 769954
#* Cluster-based concept invention for statistical relational learning
#@ Alexandrin Popescul;Lyle H. Ungar
#t 2004
#c 0
#% 373774
#% 392781
#% 398845
#% 398847
#% 406493
#% 425016
#% 584932
#% 729926
#% 777539
#! We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving "topics", and authors can be clustered based on documents they co-author giving "communities". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.

#index 769955
#* Identifying early buyers from purchase data
#@ Paat Rusmevichientong;Shenghuo Zhu;David Selinger
#t 2004
#c 0
#% 205305
#% 342596
#% 577217
#% 577356
#! Market research has shown that consumers exhibit a variety of different purchasing behaviors; specifically, some tend to purchase products earlier than other consumers. Identifying such early buyers can help personalize marketing strategies, potentially improving their effectiveness. In this paper, we present a non-parametric approach to the problem of identifying early buyers from purchase data. Our formulation takes as inputs the detailed purchase information of each consumer, with which we construct a weighted directed graph whose nodes correspond to consumers and whose edges correspond to purchases consumers have in common; the edge weights indicate how frequently consumers purchase products earlier than other consumers.Identifying early buyers corresponds to the problem of finding a subset of nodes in the graph with maximum difference between the weights of the outgoing and incoming edges. This problem is a variation of the maximum cut problem in a directed graph. We provide an approximation algorithm based on semidefinite programming (SDP) relaxations pioneered by Goemans and Williamson, and analyze its performance. We apply the algorithm to real purchase data from Amazon.com, providing new insights into consumer behaviors.

#index 769956
#* Privacy preserving regression modelling via distributed computation
#@ Ashish P. Sanil;Alan F. Karr;Xiaodong Lin;Jerome P. Reiter
#t 2004
#c 0
#% 31041
#% 132779
#% 329599
#% 346201
#% 575969
#% 577289
#% 729930
#! Reluctance of data owners to share their possibly confidential or proprietary data with others who own related databases is a serious impediment to conducting a mutually beneficial data mining analysis. We address the case of vertically partitioned data -- multiple data owners/agencies each possess a few attributes of every data record. We focus on the case of the agencies wanting to conduct a linear regression analysis with complete records without disclosing values of their own attributes. This paper describes an algorithm that enables such agencies to compute the exact regression coefficients of the global regression equation and also perform some basic goodness-of-fit diagnostics while protecting the confidentiality of their data. In more general settings beyond the privacy scenario, this algorithm can also be viewed as method for the distributed computation for regression analyses.

#index 769957
#* Dense itemsets
#@ Jouni K. Seppänen;Heikki Mannila
#t 2004
#c 0
#% 152934
#% 232136
#% 280456
#% 297675
#% 342610
#% 478770
#% 577259
#% 629644
#% 727913
#% 729437
#% 741027
#! Frequent itemset mining has been the subject of a lot of work in data mining research ever since association rules were introduced. In this paper we address a problem with frequent itemsets: that they only count rows where all their attributes are present, and do not allow for any noise. We show that generalizing the concept of frequency while preserving the performance of mining algorithms is nontrivial, and introduce a generalization of frequent itemsets, dense itemsets. Dense itemsets do not require all attributes to be present at the same time; instead, the itemset needs to define a sufficiently large submatrix that exceeds a given density threshold of attributes present.We consider the problem of computing all dense itemsets in a database. We give a levelwise algorithm for this problem, and also study the top-$k$ variations, i.e., finding the k densest sets with a given support, or the k best-supported sets with a given density. These algorithms select the other parameter automatically, which simplifies mining dense itemsets in an explorative way. We show that the concept captures natural facets of data sets, and give extensive empirical results on the performance of the algorithms. Combining the concept of dense itemsets with set cover ideas, we also show that dense itemsets can be used to obtain succinct descriptions of large datasets. We also discuss some variations of dense itemsets.

#index 769958
#* Generalizing the notion of support
#@ Michael Steinbach;Pang-Ning Tan;Hui Xiong;Vipin Kumar
#t 2004
#c 0
#% 238376
#% 310558
#% 316709
#% 320944
#% 342610
#% 452846
#% 463903
#% 481290
#% 727897
#! The goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non-traditional types of patterns and non-binary data. To that end, we describe a framework for generalizing support that is based on the simple, but useful observation that support can be viewed as the composition of two functions: a function that evaluates the strength or presence of a pattern in each object (transaction) and a function that summarizes these evaluations with a single number. A key goal of any framework is to allow people to more easily express, explore, and communicate ideas, and hence, we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns, such as frequent itemsets, general Boolean patterns, and error-tolerant itemsets. We also present two examples of the practical usefulness of generalized support. One example shows the usefulness of support functions for continuous data. Another example shows how the hyperclique pattern---an association pattern originally defined for binary data---can be extended to continuous data by generalizing a support function.

#index 769959
#* Ordering patterns by combining opinions from multiple sources
#@ Pang-Ning Tan;Rong Jin
#t 2004
#c 0
#% 152934
#% 209021
#% 235377
#% 272510
#% 280436
#% 466483
#% 564279
#% 577214
#! Pattern ordering is an important task in data mining because the number of patterns extracted by standard data mining algorithms often exceeds our capacity to manually analyze them. In this paper, we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate sources. Although rank aggregation techniques have been developed for applications such as meta-search engines, they are not directly applicable to pattern ordering for two reasons. First, the techniques are mostly supervised, i.e., they require a sufficient amount of labeled data. Second, the objects to be ranked are assumed to be independent and identically distributed (i.i.d), an assumption that seldom holds in pattern ordering. The method proposed in this paper is an adaptation of the original Hedge algorithm, modified to work in an unsupervised learning setting. Techniques for addressing the i.i.d. violation in pattern ordering are also presented. Experimental results demonstrate that our unsupervised Hedge algorithm outperforms many alternative techniques such as those based on weighted average ranking and singular value decomposition.

#index 769960
#* A generative probabilistic approach to visualizing sets of symbolic sequences
#@ Peter Tiño;Ata Kabán;Yi Sun
#t 2004
#c 0
#% 257039
#% 341446
#% 430755
#% 643519
#! There is a notable interest in extending probabilistic generative modeling principles to accommodate for more complex structured data types. In this paper we develop a generative probabilistic model for visualizing sets of discrete symbolic sequences. The model, a constrained mixture of discrete hidden Markov models, is a generalization of density-based visualization methods previously developed for static data sets. We illustrate our approach on sequences representing web-log data and chorals by J.S. Bach.

#index 769961
#* Rotation invariant distance measures for trajectories
#@ Michail Vlachos;D. Gunopulos;Gautam Das
#t 2004
#c 0
#% 38328
#% 230174
#% 282353
#% 435376
#% 504158
#% 654456
#% 659971
#% 729931
#% 993965
#! For the discovery of similar patterns in 1D time-series, it is very typical to perform a normalization of the data (for example a transformation so that the data follow a zero mean and unit standard deviation). Such transformations can reveal latent patterns and are very commonly used in datamining applications. However, when dealing with multidimensional time-series, which appear naturally in applications such as video-tracking, motion-capture etc, similar motion patterns can also be expressed at different orientations. It is therefore imperative to provide support for additional transformations, such as rotation. In this work, we transform the positional information of moving data, into a space that is translation, scale and rotation invariant. Our distance measure in the new space is able to detect elastic matches and can be efficiently lower bounded, thus being computationally tractable. The proposed methods are easy to implement, fast to compute and can have many applications for real world problems, in areas such as handwriting recognition and posture estimation in motion-capture data. Finally, we empirically demonstrate the accuracy and the efficiency of the technique, using real and synthetic handwriting data.

#index 769962
#* Privacy-preserving Bayesian network structure computation on distributed heterogeneous data
#@ Rebecca Wright;Zhiqiang Yang
#t 2004
#c 0
#% 129987
#% 270531
#% 300184
#% 340976
#% 390532
#% 577289
#% 629074
#% 653942
#% 654448
#% 729930
#% 1386180
#! As more and more activities are carried out using computers and computer networks, the amount of potentially sensitive data stored by business, governments, and other parties increases. Different parties may wish to benefit from cooperative use of their data, but privacy regulations and other privacy concerns may prevent the parties from sharing their data. Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data is not revealed.In this paper, we present a privacy-preserving protocol for a particular data mining task: learning the Bayesian network structure for distributed heterogeneous data. In this setting, two parties owning confidential databases wish to learn the structure of Bayesian network on the combination of their databases without revealing anything about their data to each other. We give an efficient and privacy-preserving version of the K2 algorithm to construct the structure of a Bayesian network for the parties' joint data.

#index 769963
#* Mining scale-free networks using geodesic clustering
#@ Andrew Y. Wu;Michael Garland;Jiawei Han
#t 2004
#c 0
#% 210173
#% 268079
#% 290830
#% 342596
#% 577217
#% 629708
#% 720338
#% 729936
#% 731608
#! Many real-world graphs have been shown to be scale-free---vertex degrees follow power law distributions, vertices tend to cluster, and the average length of all shortest paths is small. We present a new model for understanding scale-free networks based on multilevel geodesic approximation, using a new data structure called a multilevel mesh.Using this multilevel framework, we propose a new kind of graph clustering for data reduction of very large graph systems such as social, biological, or electronic networks. Finally, we apply our algorithms to real-world social networks and protein interaction graphs to show that they can reveal knowledge embedded in underlying graph structures. We also demonstrate how our data structures can be used to quickly answer approximate distance and shortest path queries on scale-free networks.

#index 769964
#* IMMC: incremental maximum margin criterion
#@ Jun Yan;Benyu Zhang;Shuicheng Yan;Qiang Yang;Hua Li;Zheng Chen;Wensi Xi;Weiguo Fan;Wei-Ying Ma;Qiansheng Cheng
#t 2004
#c 0
#% 577283
#% 719278
#% 729992
#% 763708
#! Subspace learning approaches have attracted much attention in academia recently. However, the classical batch algorithms no longer satisfy the applications on streaming data or large-scale data. To meet this desirability, Incremental Principal Component Analysis (IPCA) algorithm has been well established, but it is an unsupervised subspace learning approach and is not optimal for general classification tasks, such as face recognition and Web document categorization. In this paper, we propose an incremental supervised subspace learning algorithm, called Incremental Maximum Margin Criterion (IMMC), to infer an adaptive subspace by optimizing the Maximum Margin Criterion. We also present the proof for convergence of the proposed algorithm. Experimental results on both synthetic dataset and real world datasets show that IMMC converges to the similar subspace as that of batch approach.

#index 769965
#* 2PXMiner: an efficient two pass mining of frequent XML query patterns
#@ Liang Huai Yang;Mong Li Lee;Wynne Hsu;Xinyu Guo
#t 2004
#c 0
#% 378393
#% 397409
#% 481290
#% 577218
#% 587737
#% 1015260
#! Caching the results of frequent query patterns can improve the performance of query evaluation. This paper describes a 2-pass mining algorithm called 2PXMiner to discover frequent XML query patterns. We design 3 data structures to expedite the mining process. Experiments results indicate that 2PXMiner is both efficient and scalable.

#index 769966
#* Redundancy based feature selection for microarray data
#@ Lei Yu;Huan Liu
#t 2004
#c 0
#% 136350
#% 243727
#% 243728
#% 290482
#% 307109
#% 385564
#% 420146
#% 464444
#% 466410
#% 717417
#% 720010
#% 729972
#% 729987
#! In gene expression microarray data analysis, selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes. The problem becomes particularly challenging due to the large number of features (genes) and small sample size. Traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes. Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy. Hence, we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes. The efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets.

#index 769967
#* A cross-collection mixture model for comparative text mining
#@ ChengXiang Zhai;Atulya Velivelli;Bei Yu
#t 2004
#c 0
#% 262059
#% 280819
#% 283171
#% 397137
#% 438521
#% 722904
#% 722917
#% 729927
#% 786497
#! In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme. This general problem subsumes many interesting applications, including business intelligence and opinion summarization. We propose a generative probabilistic mixture model for comparative text mining. The model simultaneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model.

#index 769968
#* A data mining approach to modeling relationships among categories in image collection
#@ Ruofei Zhang;Zhongfei (Mark) Zhang;Sandeep Khanzode
#t 2004
#c 0
#% 203439
#% 212690
#% 238916
#% 261880
#% 393812
#% 437405
#% 443244
#% 592074
#% 626190
#% 1502530
#% 1860651
#! This paper proposes a data mining approach to modeling relationships among categories in image collection. In our approach, with image feature grouping, a visual dictionary is created for color, texture, and shape feature attributes respectively. Labeling each training image with the keywords in the visual dictionary, a classification tree is built. Based on the statistical properties of the feature space we define a structure, called α-Semantics Graph, to discover the hidden semantic relationships among the semantic categories embodied in the image collection. With the α-Semantics Graph, each semantic category is modeled as a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the categories in the feature space. The model is utilized in the semantics-intensive image retrieval application. An algorithm using the classification accuracy measures is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image. The experimental evaluations have demonstrated that the proposed approach models the semantic relationships effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency.

#index 769969
#* A DEA approach for model combination
#@ Zhiqiang Zheng;Balaji Padmanabhan;Haoqiang Zheng
#t 2004
#c 0
#% 212297
#% 218961
#% 290482
#% 331909
#% 386617
#% 424997
#% 629622
#! This paper proposes a novel Data Envelopment Analysis (DEA) based approach for model combination. We first prove that for the 2-class classification problems DEA models identify the same convex hull as the popular ROC analysis used for model combination. For general k-class classifiers, we then develop a DEA-based method to combine multiple classifiers. Experiments show that the method outperforms other benchmark methods and suggest that DEA can be a promising tool for model combination.

#index 769970
#* Optimal randomization for privacy preserving data mining
#@ Yu Zhu;Lei Liu
#t 2004
#c 0
#% 300184
#% 333876
#% 576111
#% 577233
#% 577289
#% 727904
#! Randomization is an economical and efficient approach for privacy preserving data mining (PPDM). In order to guarantee the performance of data mining and the protection of individual privacy, optimal randomization schemes need to be employed. This paper demonstrates the construction of optimal randomization schemes for privacy preserving density estimation. We propose a general framework for randomization using mixture models. The impact of randomization on data mining is quantified by performance degradation and mutual information loss, while privacy and privacy loss are quantified by interval-based metrics. Two different types of problems are defined to identify optimal randomization for PPDM. Illustrative examples and simulation results are reported.

#index 770074
#* Learning spatially variant dissimilarity (SVaD) measures
#@ Krishna Kummamuru;Raghu Krishnapuram;Rakesh Agrawal
#t 2004
#c 0
#% 309128
#% 425010
#% 529216
#% 1788041
#! Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed (dis)similarity measure between the data points in feature space. This makes the type of clusters identified highly dependent on the assumed similarity measure. Building on recent work in this area, we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data. The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure. Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature.

#index 771917
#* Cross channel optimized marketing by reinforcement learning
#@ Naoki Abe;Naval Verma;Chid Apte;Robert Schroko
#t 2004
#c 0
#% 342644
#% 384911
#% 464624
#% 577237
#% 746470
#% 1272286
#! The issues of cross channel integration and customer life time value modeling are two of the most important topics surrounding customer relationship management (CRM) today. In the present paper, we describe and evaluate a novel solution that treats these two important issues in a unified framework of Markov Decision Processes (MDP). In particular, we report on the results of a joint project between IBM Research and Saks Fifth Avenue to investigate the applicability of this technology to real world problems. The business problem we use as a testbed for our evaluation is that of optimizing direct mail campaign mailings for maximization of profits in the store channel. We identify a problem common to cross-channel CRM, which we call the Cross-Channel Challenge, due to the lack of explicit linking between the marketing actions taken in one channel and the customer responses obtained in another. We provide a solution for this problem based on old and new techniques in reinforcement learning. Our in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected, by employing a mailing policy automatically generated by our methodology. These results confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world.

#index 771918
#* Interactive training of advanced classifiers for mining remote sensing image archives
#@ Selim Aksoy;Krzysztof Koperski;Carsten Tusk;Giovanni Marchisio
#t 2004
#c 0
#% 17144
#% 136350
#% 198076
#% 212690
#% 403085
#% 729437
#% 1854745
#! Advances in satellite technology and availability of downloaded images constantly increase the sizes of remote sensing image archives. Automatic content extraction, classification and content-based retrieval have become highly desired goals for the development of intelligent remote sensing databases. The common approach for mining these databases uses rules created by analysts. However, incorporating GIS information and human expert knowledge with digital image processing improves remote sensing image analysis. We developed a system that uses decision tree classifiers for interactive learning of land cover models and mining of image archives. Decision trees provide a promising solution for this problem because they can operate on both numerical (continuous) and categorical (discrete) data sources, and they do not require any assumptions about neither the distributions nor the independence of attribute values. This is especially important for the fusion of measurements from different sources like spectral data, DEM data and other ancillary GIS data. Furthermore, using surrogate splits provides the capability of dealing with missing data during both training and classification, and enables handling instrument malfunctions or the cases where one or more measurements do not exist for some locations. Quantitative and qualitative performance evaluation showed that decision trees provide powerful tools for modeling both pixel and region contents of images and mining of remote sensing image archives.

#index 771919
#* Exploring the community structure of newsgroups
#@ Christian Borgs;Jennifer Chayes;Mohammad Mahdian;Amin Saberi
#t 2004
#c 0
#% 74120
#% 248027
#% 249110
#% 283833
#% 290830
#% 338443
#% 656714
#! We propose to use the community structure of Usenet for organizing and retrieving the information stored in newsgroups. In particular, we study the network formed by cross-posts, messages that are posted to two or more newsgroups simultaneously. We present what is, to our knowledge, by far the most detailed data that has been collected on Usenet cross-postings. We analyze this network to show that it is a small-world network with significant clustering. We also present a spectral algorithm which clusters newsgroups based on the cross-post matrix. The result of our clustering provides a topical classification of newsgroups. Our clustering gives many examples of significant relationships that would be missed by semantic clustering methods.

#index 771920
#* Feature selection in scientific applications
#@ Erick Cantú-Paz;Shawn Newsam;Chandrika Kamath
#t 2004
#c 0
#% 58636
#% 212690
#% 466912
#% 727662
#! Numerous applications of data mining to scientific data involve the induction of a classification model. In many cases, the collection of data is not performed with this task in mind, and therefore, the data might contain irrelevant or redundant features that affect negatively the accuracy of the induction algorithms. The size and dimensionality of typical scientific data make it difficult to use any available domain information to identify features that discriminate between the classes of interest. Similarly, exploratory data analysis techniques have limitations on the amount and dimensionality of the data they can process effectively. In this paper, we describe applications of efficient feature selection methods to data sets from astronomy, plasma physics, and remote sensing. We use variations of recently proposed filter methods as well as traditional wrapper approaches, where practical. We discuss the general challenges of feature selection in scientific datasets, the strategies for success that were common among our diverse applications, and the lessons learned in solving these problems.

#index 771921
#* A general approach to incorporate data quality matrices into data mining algorithms
#@ Ian Davidson;Ashish Grover;Ashwin Satyanarayana;Giri K. Tayi
#t 2004
#c 0
#% 54050
#% 204531
#% 209021
#% 262388
#% 262505
#% 269587
#% 269634
#% 736258
#! Data quality is a central issue for many information-oriented organizations. Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process. While routine errors, such as non-existent zip codes, can be detected and corrected using traditional data cleansing tools, many errors systemic to the manufacturing process cannot be addressed. Therefore, the product of the data manufacturing process is an imprecise recording of information about the entities of interest (i.e. customers, transactions or assets). In this way, the database is only one (flawed) version of the entities it is supposed to represent. Quality assurance systems such as Motorola's Six-Sigma and other continuous improvement methods document the data manufacturing process's shortcomings. A widespread method of documentation is quality matrices. In this paper, we explore the use of the readily available data quality matrices for the data mining classification task. We first illustrate that if we do not factor in these quality matrices, then our results for prediction are sub-optimal. We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance.

#index 771922
#* ANN quality diagnostic models for packaging manufacturing: an industrial data mining case study
#@ Nicolás de Abajo;Alberto B. Diez;Vanesa Lobato;Sergio R. Cuesta
#t 2004
#c 0
#! World steel trade becomes more competitive every day and new high international quality standards and productivity levels can only be achieved by applying the latest computational technologies. Data driven analysis of complex processes is necessary in many industrial applications where analytical modeling is not possible. This paper presents the deployment of KDD technology in one real industrial problem: the development of new tinplate quality diagnostic models.The electrodeposition of tin on steel strips is the most critical stage of a complex process that involves a great amount of variables and operating conditions. Its optimization is not only a great commercial and economic challenge but also a compulsion due to the social impact of the tinplate product-more than 90% of the production is used for food packaging. The necessary certification with standards, like ISO 9000, requires the use of diagnostic models to minimize the costs and the environmental impact. This aim has been achieved following the multi-stage DM methodology CRISP-DM and a novel application of pro-active maintenance methods, as FMEA, for the identification of the specific process anomalies. Three DM tools have been used for the development of the models. The final results include two ANN tinplate quality diagnostic models, that provide the estimated quality of the final product just seconds after its production and only based on the process data. The results have much better performance than the classical Faraday's models widely used for the estimation.

#index 771923
#* A system for automated mapping of bill-of-materials part numbers
#@ Jayant Kalagnanam;Moninder Singh;Sudhir Verma;Michael Patek;Yuk Wah Wong
#t 2004
#c 0
#% 283136
#% 348164
#% 722822
#! Part numbers are widely used within an enterprise throughout the manufacturing process. The point of entry of such part numbers into this process is normally via a Bill of Materials, or BOM, sent by a contact manufacturer or supplier. Each line of the BOM provides information about one part such as the supplier part number, the BOM receiver's corresponding internal part number, an unstructured textual part description, the supplier name, etc. However, in a substantial number of cases, the BOM receiver's internal part number is absent. Hence, before this part can be incorporated into the receiver's manufacturing process, it has to be mapped to an internal part (of the BOM receiver) based on the information of the part in the BOM. Historically, this mapping process has been done manually which is a highly time-consuming, labor intensive and error-prone process. This paper describes a system for automating the mapping of BOM part numbers. The system uses a two step modeling and mapping approach. First, the system uses historical BOM data, receiver's part specifications data and receiver's part taxonomic data along with domain knowledge to automatically learn classification models for mapping a given BOM part description to successively lower levels of the receiver's part taxonomy to reduce the set of potential internal parts to which the BOM part could map to. Then, information about various part parameters is extracted from the BOM part description and compared to the specifications data of the potential internal parts to choose the final mapped internal part. Mappings done by the system are very accurate, and the system is currently being deployed within IBM for mapping BOMs received by the corporate procurement/manufacturing divisions.

#index 771924
#* Tracking dynamics of topic trends using a finite mixture model
#@ Satoshi Morinaga;Kenji Yamanishi
#t 2004
#c 0
#% 262042
#% 262043
#% 277483
#% 287196
#% 309096
#% 310552
#% 397147
#% 450037
#% 577220
#% 577297
#% 641137
#% 643556
#% 1809407
#! In a wide range of business areas dealing with text data streams, including CRM, knowledge management, and Web monitoring services, it is an important issue to discover topic trends and analyze their dynamics in real-time. Specifically we consider the following three tasks in topic trend analysis: 1)Topic Structure Identification; identifying what kinds of main topics exist and how important they are, 2)Topic Emergence Detection; detecting the emergence of a new topic and recognizing how it grows, 3)Topic Characterization; identifying the characteristics for each of main topics. For real topic analysis systems, we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way, and be dealt with in a single framework. This paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically. In this framework we propose the usage of a time-stamp based discounting learning algorithm in order to realize real-time topic structure identification. This enables tracking the topic structure adaptively by forgetting out-of-date statistics. Further we apply the theory of dynamic model selection to detecting changes of main components in the finite mixture model in order to realize topic emergence detection. We demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamics of topic trends in a timely fashion.

#index 771925
#* Mining traffic data from probe-car system for travel time prediction
#@ Takayuki Nakata;Jun-ichi Takeuchi
#t 2004
#c 0
#% 13453
#% 131686
#% 749406
#! We are developing a technique to predict travel time of a vehicle for an objective road section, based on real time traffic data collected through a probe-car system. In the area of Intelligent Transport System (ITS), travel time prediction is an important subject. Probe-car system is an upcoming data collection method, in which a number of vehicles are used as moving sensors to detect actual traffic situation. It can collect data concerning much larger area, compared with traditional fixed detectors. Our prediction technique is based on statistical analysis using AR model with seasonal adjustment and MDL (Minimum Description Length) criterion. Seasonal adjustment is used to handle periodicities of 24 hours in traffic data. Alternatively, we employ state space model, which can handle time series with periodicities. It is important to select really effective data for prediction, among the data from widespread area, which are collected via probe-car system. We do this using MDL criterion. That is, we find the explanatory variables that really have influence on the future travel time. In this paper, we experimentally show effectiveness of our method using probe-car data collected in Nagoya Metropolitan Area in 2002.

#index 771926
#* Programming the K-means clustering algorithm in SQL
#@ Carlos Ordonez
#t 2004
#c 0
#% 210173
#% 248813
#% 280521
#% 300131
#% 300213
#% 342704
#% 413619
#% 466497
#% 662751
#% 662754
#% 766665
#! Using SQL has not been considered an efficient and feasible way to implement data mining algorithms. Although this is true for many data mining, machine learning and statistical algorithms, this work shows it is feasible to get an efficient SQL implementation of the well-known K-means clustering algorithm that can work on top of a relational DBMS. The article emphasizes both correctness and performance. From a correctness point of view the article explains how to compute Euclidean distance, nearest-cluster queries and updating clustering results in SQL. From a performance point of view it is explained how to cluster large data sets defining and indexing tables to store and retrieve intermediate and final results, optimizing and avoiding joins, optimizing and simplifying clustering aggregations, and taking advantage of sufficient statistics. Experiments evaluate scalability with synthetic data sets varying size and dimensionality. The proposed K-means implementation can cluster large data sets and exhibits linear scalability.

#index 771927
#* Document preprocessing for naive Bayes classification and clustering with mixture of multinomials
#@ Dmitry Pavlov;Ramnath Balasubramanyan;Byron Dom;Shyam Kapur;Jignashu Parikh
#t 2004
#c 0
#% 246831
#% 304935
#% 310547
#! Naive Bayes classifier has long been used for text categorization tasks. Its sibling from the unsupervised world, the probabilistic mixture of multinomial models, has likewise been successfully applied to text clustering problems. Despite the strong independence assumptions that these models make, their attractiveness come from low computational cost, relatively low memory consumption, ability to handle heterogeneous features and multiple classes, and often competitiveness with the top of the line models. Recently, there has been several attempts to alleviate the problems of Naive Bayes by performing heuristic feature transformations, such as IDF, normalization by the length of the documents and taking the logarithms of the counts. We justify the use of these techniques and apply them to two problems: classification of products in Yahoo! Shopping and clustering the vectors of collocated terms in user queries to Yahoo! Search. The experimental evaluation allows us to draw conclusions about the promise that these transformations carry with regard to alleviating the strong assumptions of the multinomial model.

#index 771928
#* Learning a complex metabolomic dataset using random forests and support vector machines
#@ Young Truong;Xiaodong Lin;Chris Beecher
#t 2004
#c 0
#% 400847
#% 754400
#! Metabolomics is the "omics" science of biochemistry. The associated data include the quantitative measurements of all small molecule metabolites in a biological sample. These datasets provide a window into dynamic biochemical networks and conjointly with other "omic" data, genes and proteins, have great potential to unravel complex human diseases. The dataset used in this study has 63 individuals, normal and diseased, and the diseased are drug treated or not, so there are three classes. The goal is to classify these individuals using the observed metabolite levels for 317 measured metabolites. There are a number of statistical challenges: non-normal data, the number of samples is less than the number of metabolites; there are missing data and the fact that data are missing is informative (assay values below detection limits can point to a specific class); also, there are high correlations among the metabolites. We investigate support vector machines (SVM), and random forest (RF), for outlier detection, variable selection and classification. We use the variables selected with RF in SVM and visa versa. The benefit of this study is insight into interplay of variable selection and classification methods. We link our selected predictors to the biochemistry of the disease.

#index 771929
#* 1-dimensional splines as building blocks for improving accuracy of risk outcomes models
#@ David S. Vogel;Morgan C. Wang
#t 2004
#c 0
#! Transformation of both the response variable and the predictors is commonly used in fitting regression models. However, these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors, especially when there are non-linear relationships between predictors and the response such as the medical data set used in this study. A spline based transformation method is proposed that is second order smooth, continuous, and minimizes the mean squared error between the response and each predictor. Since the computation time for generating this spline is O(n), the processing time is reasonable with massive data sets. In contrast to cubic smoothing splines, the resulting transformation equations also display a high level of efficiency for scoring. Data used for predicting health outcomes contains an abundance of non-linear relationships between predictors and the outcomes requiring an algorithm for modeling them accurately. Thus, a transformation that fits an adaptive cubic spline to each of a set of variables is proposed. These curves are used as a set of transformation functions on the predictors. A case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented. The results show significant improvement over the performance of the original variables in both linear and non-linear models.

#index 771930
#* Analytical view of business data
#@ Adam Yeh;Jonathan Tang;Youxuan Jin;Sam Skrivan
#t 2004
#c 0
#% 393641
#! This paper describes a logical extension to Microsoft Business Framework (MBF) called Analytical View (AV). AV consists of three components: Model Service for design time, Business Intelligence Entity (BIE) for programming model, and IntellDrill for runtime navigation between OLTP and OLAP data sources. AV feature-set fulfills enterprise application requirements for Analysis and Decision Support, complementing the transactional feature-set currently provided by MBF. Model Service automatically transforms an "object oriented model (transactional view)" to a "multi-dimensional model (analytical view)" without the traditional Extraction/Transformation/Loading (ETL) overhead and complexity. It infers dimensionality from the object layer where richer metadata is stored, eliminating the "guesswork" that a traditional data warehousing process requires when going through physical database schema. BI Entities are classes code-generated by Model Service. As an intrinsic part of the framework, BI Entities enable a consistent object oriented way of programming model with strong types and rich semantics for OLAP, similar to what MBF object persistence technology does for OLTP data. More importantly, data contained in BI Entities have a higher degree of "application awareness," such as the integrated application level security and customizability. IntelliDrill links together all the information islands in MBF using metadata. Because of the automatic transformation from transactional view to analytical view enabled by Model Service, we have the ability to understand natively what kind of drill-ability an object would have, thus making information navigation in MBF fully discover-able with built-in ontology.

#index 823320
#* Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining
#@ Robert Grossman;Roberto Bayardo;Kristin Bennett
#t 2005
#c 0
#! It is our great pleasure to welcome you to the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -- KDD'05. The KDD conferences provide a forum for novel research results and interesting applications in the areas of data mining and knowledge discovery. The conference series gives researchers and practitioners a unique opportunity to share their perspectives with others and to present new research ideas, applications, solutions, tools, systems, and research directions broadly related to knowledge discovery and data mining.The call for papers attracted 465 research submissions and 73 industrial submissions from around the world. The program committee accepted 40 research papers, 36 research posters, 14 industrial papers and 11 industrial posters. In addition, this year's conference includes three plenary talks and two panels.Putting together KDD'05 was a team effort. First, we would like to thank the authors for providing the content of the program and the program committee and external reviewers, who worked very hard in reviewing papers and providing suggestions for their improvements. Second, we would like to thank the Organizing Committee, who also worked very hard, and, as volunteers, didn't get much for it. I strongly encourage you to thank them, and even, perhaps, to let them get out of the elevator ahead of you. Finally, we would like to thank our sponsor, ACM SIGKDD, for their continued support.Daniel Hudson Burnham (1846-1912) was a partner in the Chicago based architecture firm Burnham and Root. Burnham and Root created the foundation for the modern skyscraper by using a floating foundation of cement that provided a stable foundation even when, as in many Chicago locations, it was not possible to reach bedrock. Burnham and Root was also the lead architect for, and in charge of, construction for the World Columbian Exposition (1893), which celebrated the 400th anniversary of the arrival of Columbus to North America. The World Columbian Exposition was the largest World's Fair to that date and drew 27.5 million attendees at a time when the US population was about 65 million. To achieve this, many logistic obstacles had to be overcome. Burnham's style is nicely captured by a quote associated with him: "Make no little plans. They have no magic to strike man's blood and probably will themselves not be realized.This is the theme we have chosen for this year's KDD Conference. The field of data mining and knowledge discovery is over ten years old, and is now mature enough to begin to make some big plans and to tackle some very difficult problems and challenges. We will begin discussions about these challenges at this year's conference and continue them throughout the year. Over the coming year, please look to the SIGKDD Explorations for further information.We hope that you will find this program interesting and thought provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.

#index 867870
#* Proceedings of the 1st international workshop on open source data mining: frequent pattern mining implementations
#@ Bart Goethals;Siegfried Nijssen;Mohammed J. Zaki
#t 2005
#c 0
#! Over the past decade tremendous progress has been made in data mining methods like clustering, classification, frequent pattern mining, and so on. Unfortunately, however, the advanced implementations are often not made publicly available, and thus the results cannot be independently verified. We believe that this hampers the rapid advances in the field. With this workshop we intend to promote open source data mining (OSDM) by creating a first meeting place to discuss open source data mining methods.The first steps towards an open source data mining workshop were set in previous years by the Frequent Itemset Mining Implementations workshops (FIMI), which enjoyed a large popularity. The OSDM workshop is held in the same spirit as these earlier workshops, and, in its first edition, the workshop therefore has a special focus on implementations of frequent pattern mining algorithms. It is our hope that in subsequent years the workshop will also focus on open source implementations for other data mining problems like clustering, classification, outlier detection, and so on.Frequent pattern mining is a core field of research in data mining encompassing the discovery of patterns such as itemsets, sequences, trees, graphs, and many other structures. Varied approaches to these problems appear in numerous papers across all data mining conferences. Generally speaking, the problem involves the identification of items, products, symptoms, characteristics, and so forth, that often occur together in a given dataset. As a fundamental operation in data mining, algorithms for FPM can be used as a building block for other, more sophisticated data mining processes. During the last decade, a huge number of algorithms have been developed in order to efficiently solve all kinds of FPM problems. A representative set of such algorithms can now be found in these proceedings, including papers about frequent itemset mining, frequent sequence mining and frequent graph mining.All submissions to this workshop were necessarily accompanied by source code. This source code can also be found on the homepage of the OSDM 2005 workshop:http://osdm.ua.ac.be/All papers were independently reviewed by the members of the program committee.

#index 867971
#* Proceedings of the 5th international workshop on Bioinformatics
#@ Mohammed Zaki;Srinivasan Parthasarathy;Wei Wang
#t 2005
#c 0
#! Bioinformatics is the science of managing, mining, and interpreting information from biological entities. Genome sequencing projects have contributed to an exponential growth in complete and partial sequence databases. The structural genomics initiative aims to catalog the structure-function information for proteins. Advances in technology such as microarrays have launched the subfield of genomics and proteomics to study the genes, proteins, and the regulatory gene expression circuitry inside the cell. What characterizes the state of the field is the flood of data that exists today or that is anticipated in the future; data that needs to be mined to help unlock the secrets of the cell. Knowledge extracted from such analysis can be used effectively to better design new drugs, offer better medical care via diagnostic tests that combine information from multiple sources, and improve scientific and clinical practice.While tremendous progress has been made over the years, many of the fundamental problems in bioinformatics, such as protein structure prediction or gene finding, are still open. Data mining will play a fundamental role in understanding gene expression, drug design and other emerging problems in genomics and proteomics. Furthermore, text mining will be fundamental in extracting knowledge from the growing literature in bioinformatics.The goal of this workshop was to encourage KDD researchers to take on the numerous challenges that Bioinformatics offers. The workshop features an invited talk from a noted expert in the field, and the latest data mining research in bioinformatics from world class researchers. We encouraged papers that propose novel data mining techniques for tasks such as: Gene expression analysis; Protein/RNA structure prediction; Phylogenetics; Sequence and structural motifs; Genomics and Proteomics; Gene finding; Drug design; RNAi and microRNA Analysis; Text mining in bioinformatics; Modeling of biochemical pathways; and Biomedical and clinical informatics.These proceedings contain 10 papers (5 long and 5 short), out of 20 submissions that were accepted for presentation at the workshop. Each paper was reviewed by at least three members of the program committee. In some cases where there was a wide variance in reviews a fourth was sought. Each long paper selected had at least two strong supporters and no strong detractor. Each short paper selected had at least one strong supporter and typically no strong detractor. As a result along with a distinguished invited talk, we were able to assemble a very exciting program.This workshop follows the previous four highly successful workshops: BIOKDD04, held in Seattle, BIOKDD03, held in Washington, DC; BIOKDD02, held in Edmonton, Canada; and BIOKDD01 held in San Francisco, CA. We expect BIOKDD05 to be equally successful.

#index 868083
#* Proceedings of the 3rd international workshop on Link discovery
#@ Jafar Adibi;Marko Grobelnik;Dunja Mladenic;Patrick Pantel
#t 2005
#c 0
#! The LinkKDD-2005 workshop aims to bring together a diverse group of researchers and industry practitioners to advance the state of the art in link discovery. Recently, there has been increasing interest in developing information technology for Link Discovery (LD). LD research studies and develops data mining techniques for extracting valuable patterns linking together seemingly unrelated items. LD, rooted in fields such as discreet mathematics, graph theory, social science, pattern analysis, link analysis and spatial databases, is relevant to a wide range of research topics that have been developed in past decades. Successful LD systems will discover the hidden structure of organizations, relate groups, identify fraudulent behaviour, model group activity and provide early detection of emerging threats. The broader context of this workshop invites both theoretical and applied contributions to LD spanning techniques from Data Mining, Machine Learning, Information Retrieval, Natural Language Processing, Social Networks Analysis, and general Graph Theory.Typical characteristics of link discovery problems are:.. Data is heterogeneous, arising from multiple sources;.. Data and patterns sought include representations of people, organizations, objects, actions and events, each of which has its own set of attributes, and particular types of relations linking them;.. The structure may include temporal, spatial, organizational, and/or transactional patterns;.. A relatively low number of observations for each entity can be recorded and the overall sample is typically small relative to the size of the population;.. The data becomes available over time, so the timing of when to make a decision based on the analysis is a central issue.LD problems are found in various areas such as homeland security, social network analysis, fraud detection, recommendation systems, and user modelling. The interdisciplinary nature of link discovery promotes a concerted effort from various researchers. The purpose of this workshop is to provide a forum to foster such interactions, discuss the new achievements and identify future research directions in link discovery.

#index 881452
#* Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Tina Eliassi-Rad;Lyle Ungar;Mark Craven;Dimitrios Gunopulos
#t 2006
#c 0
#! This volume serves as the written record of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-06) held in Philadelphia, Pennsylvania on August 20- 23, 2006. The KDD conference provides a forum for novel research results and important applications in the area of data mining and knowledge discovery. The vibrancy, excitement and breadth of the field is reflected by the strong lineup of research papers, invited talks, tutorials and workshops at the conference.KDD received a record number 457 Research Track submissions this year. The program committee accepted 50 papers for oral presentation at the conference and 55 for poster presentation. The Industrial and Government Applications Track received 74 submissions and accepted seven for oral presentation and eight for poster presentation.In addition to paper presentations, the conference also featured eight tutorials, nine workshops, one panel and invited talks by Rakesh Agrawal, Andrew Moore, and John Stankovic. The Industrial and Government Applications track this year also included invited talks by Michael Cavaretta, Jeff Jonas, William Kahn, and Andrew McCallum.

#index 881453
#* Self-Organizing wireless sensor networks in action
#@ John A. Stankovic
#t 2006
#c 0
#! Wireless sensor networks (WSN) composed of large numbers of small devices that self-organize are being investigated for a wide variety of applications. Two key advantages of these networks over more traditional sensor networks are that they can be dynamically and quickly deployed, and that they can provide fine-grained sensing. Applications, such as emergency response to natural or manmade disasters, detection and tracking, and fine grained sensing of the environment are key examples of applications that can benefit from these types of WSN. Current research for these systems is widespread. However, many of the proposed solutions are developed with simplifying assumptions about wireless communication and the environment, even though the realities of wireless communication and environmental sensing are well known. Many of the solutions are evaluated only by simulation. In this talk I describe a fully implemented system consisting of a suite of more than 30 synthesized protocols. The system supports a power aware surveillance, tracking and classification application running on 203 XSM motes and evaluated in a realistic, large-area environment. Technical details and evaluations are presented. I end with a discussion of opportunities and problems for data mining related to WSN.

#index 881454
#* New cached-sufficient statistics algorithms for quickly answering statistical questions
#@ Andrew Moore
#t 2006
#c 0
#! This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..

#index 881455
#* Next frontier
#@ Rakesh Agrawal
#t 2006
#c 0
#! This talk is about the next frontier in knowledge discovery and data mining.

#index 881456
#* Deriving quantitative models for correlation clusters
#@ Elke Achtert;Christian Böhm;Hans-Peter Kriegel;Peer Kröger;Arthur Zimek
#t 2006
#c 0
#% 210160
#% 248792
#% 273891
#% 290482
#% 300131
#% 342640
#% 397382
#% 397384
#% 469422
#% 659967
#% 727882
#% 727908
#% 765439
#% 785335
#% 785419
#% 785422
#% 810047
#% 818916
#% 905832
#! Correlation clustering aims at grouping the data set into correlation clusters such that the objects in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality. Several algorithms for this task have been proposed recently. However, all algorithms only compute the partitioning of the data into clusters. This is only a first step in the pipeline of advanced data analysis and system modelling. The second (post-clustering) step of deriving a quantitative model for each correlation cluster has not been addressed so far. In this paper, we describe an original approach to handle this second step. We introduce a general method that can extract quantitative information on the linear dependencies within a correlation clustering. Our concepts are independent of the clustering model and can thus be applied as a post-processing step to any correlation clustering algorithm. Furthermore, we show how these quantitative models can be used to predict the probability distribution that an object is created by these models. Our broad experimental evaluation demonstrates the beneficial impact of our method on several applications of significant practical importance.

#index 881457
#* Learning to rank networked entities
#@ Alekh Agarwal;Soumen Chakrabarti;Sunny Aggarwal
#t 2006
#c 0
#% 268079
#% 283833
#% 290830
#% 348173
#% 406493
#% 466891
#% 577224
#% 577329
#% 577337
#% 577338
#% 654442
#% 660011
#% 805850
#% 805896
#% 1272396
#% 1289460
#! Several algorithms have been proposed to learn to rank entities modeled as feature vectors, based on relevance feedback. However, these algorithms do not model network connections or relations between entities. Meanwhile, Pagerank and variants find the stationary distribution of a reasonable but arbitrary Markov walk over a network, but do not learn from relevance feedback. We present a framework for ranking networked entities based on Markov walks with parameterized conductance values associated with the network edges. We propose two flavors of conductance learning problems in our framework. In the first setting, relevance feedback comparing node-pairs hints that the user has one or more hidden preferred communities with large edge conductance, and the algorithm must discover these communities. We present a constrained maximum entropy network flow formulation whose dual can be solved efficiently using a cutting-plane approach and a quasi-Newton optimizer. In the second setting, edges have types, and relevance feedback hints that each edge type has a potentially different conductance, but this is fixed across the whole network. Our algorithm learns the conductances using an approximate Newton method.

#index 881458
#* Spatial scan statistics: approximations and performance study
#@ Deepak Agarwal;Andrew McGregor;Jeff M. Phillips;Suresh Venkatasubramanian;Zhengyuan Zhu
#t 2006
#c 0
#% 1331
#% 238182
#% 278835
#% 424759
#% 593957
#% 769901
#% 771386
#% 847161
#% 848219
#! Spatial scan statistics are used to determine hotspots in spatial data, and are widely used in epidemiology and biosurveillance. In recent years, there has been much effort invested in designing efficient algorithms for finding such "high discrepancy" regions, with methods ranging from fast heuristics for special cases, to general grid-based methods, and to efficient approximation algorithms with provable guarantees on performance and quality.In this paper, we make a number of contributions to the computational study of spatial scan statistics. First, we describe a simple exact algorithm for finding the largest discrepancy region in a domain. Second, we propose a new approximation algorithm for a large class of discrepancy functions (including the Kulldorff scan statistic) that improves the approximation versus run time trade-off of prior methods. Third, we extend our simple exact and our approximation algorithms to data sets which lie naturally on a grid or are accumulated onto a grid. Fourth, we conduct a detailed experimental comparison of these methods with a number of known methods, demonstrating that our approximation algorithm has far superior performance in practice to prior methods, and exhibits a good performance-accuracy trade-off.All extant methods (including those in this paper) are suitable for data sets that are modestly sized; if data sets are of the order of millions of data points, none of these methods scale well. For such massive data settings, it is natural to examine whether small-space streaming algorithms might yield accurate answers. Here, we provide some negative results, showing that any streaming algorithms that even provide approximately optimal answers to the discrepancy maximization problem must use space linear in the input.

#index 881459
#* Global distance-based segmentation of trajectories
#@ Aris Anagnostopoulos;Michail Vlachos;Marios Hadjieleftheriou;Eamonn Keogh;Philip S. Yu
#t 2006
#c 0
#% 179858
#% 228353
#% 333941
#% 397376
#% 458857
#% 460862
#% 627564
#% 729931
#% 810067
#% 824729
#% 844293
#! This work introduces distance-based criteria for segmentation of object trajectories. Segmentation leads to simplification of the original objects into smaller, less complex primitives that are better suited for storage and retrieval purposes. Previous work on trajectory segmentation attacked the problem locally, segmenting separately each trajectory of the database. Therefore, they did not directly optimize the inter-object separability, which is necessary for mining operations such as searching, clustering, and classification on large databases. In this paper we analyze the trajectory segmentation problem from a global perspective, utilizing data aware distance-based optimization techniques, which optimize pairwise distance estimates hence leading to more efficient object pruning. We first derive exact solutions of the distance-based formulation. Due to the intractable complexity of the exact solution, we present anapproximate, greedy solution that exploits forward searching of locally optimal solutions. Since the greedy solution also imposes a prohibitive computational cost, we also put forward more light weight variance-based segmentation techniques, which intelligently "relax" the pairwise distance only in the areas that affect the least the mining operation.

#index 881460
#* Group formation in large social networks: membership, growth, and evolution
#@ Lars Backstrom;Dan Huttenlocher;Jon Kleinberg;Xiangyang Lan
#t 2006
#c 0
#% 209021
#% 342596
#% 387427
#% 438553
#% 449588
#% 480635
#% 577217
#% 577220
#% 729923
#% 729968
#% 733847
#% 771919
#% 786841
#% 853535
#% 868469
#% 881498
#! The processes by which communities come together, attract new members, and develop over time is a central research issue in the social sciences - political movements, professional organizations, and religious denominations all provide fundamental examples of such communities. In the digital domain, on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as MySpace and LiveJournal. However, the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved: what are the structural features that influence whether individuals will join communities, which communities will grow rapidly, and how do the overlaps among pairs of communities change over time.Here we address these questions using two large sources of data: friendship links and community membership on LiveJournal, and co-authorship and conference publications in DBLP. Both of these datasets provide explicit user-defined communities, where conferences serve as proxies for communities in DBLP. We study how the evolution of these communities relates to properties such as the structure of the underlying social networks. We find that the propensity of individuals to join communities, and of communities to grow rapidly, depends in subtle ways on the underlying network structure. For example, the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community, but also crucially by how those friends are connected to one another. We use decision-tree techniques to identify the most significant structural determinants of these properties. We also develop a novel methodology for measuring movement of individuals between communities, and show how such movements are closely aligned with changes in the topics of interest within the communities.

#index 881461
#* Detecting outliers using transduction and statistical testing
#@ Daniel Barbará;Carlotta Domeniconi;James P. Rogers
#t 2006
#c 0
#% 14749
#% 234979
#% 248790
#% 300136
#% 300183
#% 321455
#% 342625
#% 446658
#% 458669
#% 465015
#% 466258
#% 479791
#% 479799
#% 481281
#% 501988
#% 574284
#% 729912
#% 729982
#% 789012
#! Outlier detection can uncover malicious behavior in fields like intrusion detection and fraud analysis. Although there has been a significant amount of work in outlier detection, most of the algorithms proposed in the literature are based on a particular definition of outliers (e.g., density-based), and use ad-hoc thresholds to detect them. In this paper we present a novel technique to detect outliers with respect to an existing clustering model. However, the test can also be successfully utilized to recognize outliers when the clustering information is not available. Our method is based on Transductive Confidence Machines, which have been previously proposed as a mechanism to provide individual confidence measures on classification decisions. The test uses hypothesis testing to prove or disprove whether a point is fit to be in each of the clusters of the model. We experimentally demonstrate that the test is highly robust, and produces very few misdiagnosed points, even when no clustering information is available. Furthermore, our experiments demonstrate the robustness of our method under the circumstances of data contaminated by outliers. We finally show that our technique can be successfully applied to identify outliers in a noisy data set for which no information is available (e.g., ground truth, clustering structure, etc.). As such our proposed methodology is capable of bootstrapping from a noisy data set a clean one that can be used to identify future outliers.

#index 881462
#* Robust information-theoretic clustering
#@ Christian Böhm;Christos Faloutsos;Jia-Yu Pan;Claudia Plant
#t 2006
#c 0
#% 210173
#% 248790
#% 248792
#% 273890
#% 300131
#% 309128
#% 375017
#% 375388
#% 466425
#% 481281
#% 765439
#% 769883
#% 810047
#% 844288
#! How do we find a natural clustering of a real world point set, which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? Most clustering algorithms were designed with certain assumptions (Gaussianity), they often require the user to give input parameters, and they are sensitive to noise. In this paper, we propose a robust framework for determining a natural clustering of a given data set, based on the minimum description length (MDL) principle. The proposed framework, Robust Information-theoretic Clustering (RIC), is orthogonal to any known clustering algorithm: given a preliminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously determines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods such as grid-based space partitioning. Moreover, RIC scales well with the data set size. Extensive experiments on synthetic and real world data sets validate the proposed RIC framework.

#index 881463
#* Efficient anonymity-preserving data collection
#@ Justin Brickell;Vitaly Shmatikov
#t 2006
#c 0
#% 23638
#% 300184
#% 333876
#% 354287
#% 514517
#% 525279
#% 554699
#% 575969
#% 576761
#% 576762
#% 664654
#% 729962
#% 743280
#% 769962
#% 823358
#% 840680
#% 963801
#! The output of a data mining algorithm is only as good as its inputs, and individuals are often unwilling to provide accurate data about sensitive topics such as medical history and personal finance. Individuals maybe willing to share their data, but only if they are assured that it will be used in an aggregate study and that it cannot be linked back to them. Protocols for anonymity-preserving data collection provide this assurance, in the absence of trusted parties, by allowing a set of mutually distrustful respondents to anonymously contribute data to an untrusted data miner.To effectively provide anonymity, a data collection protocol must be collusion resistant, which means that even if all dishonest respondents collude with a dishonest data miner in an attempt to learn the associations between honest respondents and their responses, they will be unable to do so. To achieve collusion resistance, previously proposed protocols for anonymity-preserving data collection have quadratically many communication rounds in the number of respondents, and employ (sometimes incorrectly) complicated cryptographic techniques such as zero-knowledge proofs.We describe a new protocol for anonymity-preserving, collusion resistant data collection. Our protocol has linearly many communication rounds, and achieves collusion resistance without relying on zero-knowledge proofs. This makes it especially suitable for data mining scenarios with a large number of respondents.

#index 881464
#* Out-of-core frequent pattern mining on a commodity PC
#@ Gregory Buehrer;Srinivasan Parthasarathy;Amol Ghoting
#t 2006
#c 0
#% 152934
#% 201894
#% 227919
#% 280409
#% 300120
#% 342368
#% 379325
#% 420063
#% 465003
#% 466664
#% 479484
#% 481290
#% 481754
#% 785343
#% 824655
#% 824699
#! In this work we focus on the problem of frequent itemset mining on large, out-of-core data sets. After presenting a characterization of existing out-of-core frequent itemset mining algorithms and their drawbacks, we introduce our efficient, highly scalable solution. Presented in the context of the FPGrowth algorithm, our technique involves several novel I/O-conscious optimizations, such as approximate hash-based sorting and blocking, and leverages recent architectural advancements in commodity computers, such as 64-bit processing. We evaluate the proposed optimizations on truly large data sets,up to 75GB, and show they yield greater than a 400-fold execution time improvement. Finally, we discuss the impact of this research in the context of other pattern mining challenges, such as sequence mining and graph mining.

#index 881465
#* Mining rank-correlated sets of numerical attributes
#@ Toon Calders;Bart Goethals;Szymon Jaroszewicz
#t 2006
#c 0
#% 92546
#% 152934
#% 210160
#% 280458
#% 317950
#% 318994
#% 769958
#% 823336
#% 1673590
#! We study the mining of interesting patterns in the presence of numerical attributes. Instead of the usual discretization methods, we propose the use of rank based measures to score the similarity of sets of numerical attributes. New support measures for numerical data are introduced, based on extensions of Kendall's tau, and Spearman's Footrule and rho. We show how these support measures are related. Furthermore, we introduce a novel type of pattern combining numerical and categorical attributes. We give efficient algorithms to find all frequent patterns for the proposed support measures, and evaluate their performance on real-life datasets.

#index 881466
#* NeMoFinder: dissecting genome-wide protein-protein interactions with meso-scale network motifs
#@ Jin Chen;Wynne Hsu;Mong Li Lee;See-Kiong Ng
#t 2006
#c 0
#% 629708
#% 727845
#% 769940
#% 832742
#% 832954
#% 1705223
#! Recent works in network analysis have revealed the existence of network motifs in biological networks such as the protein-protein interaction (PPI) networks. However, existing motif mining algorithms are not sufficiently scalable to find meso-scale network motifs. Also, there has been little or no work to systematically exploit the extracted network motifs for dissecting the vast interactomes.We describe an efficient network motif discovery algorithm, NeMoFinder, that can mine meso-scale network motifs that are repeated and unique in large PPI networks. Using NeMoFinder, we successfully discovered, for the first time, up to size-12 network motifs in a large whole-genome S. cerevisiae (Yeast) PPI network. We also show that such network motifs can be systematically exploited for indexing the reliability of PPI data that were generated via highly erroneous high-throughput experimental methods.

#index 881467
#* Estimating the global pagerank of web communities
#@ Jason V. Davis;Inderjit S. Dhillon
#t 2006
#c 0
#% 66165
#% 268079
#% 268087
#% 281251
#% 480479
#% 577328
#% 783528
#% 805897
#% 807302
#% 823348
#% 838406
#% 853940
#% 1016164
#! Localized search engines are small-scale systems that index a particular community on the web. They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build, and can provide more precise and complete search capability over their relevant domains. One disadvantage such systems have over large-scale search engines is the lack of global PageRank values. Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole. In this paper, we present well-motivated algorithms to estimate the global PageRank values of a local domain. The algorithms are all highly scalable in that, given a local domain of size n, they use O(n) resources that include computation time, bandwidth, and storage. We test our methods across a variety of localized domains, including site-specific domains and topic-specific domains. We demonstrate that by crawling as few as n or 2n additional pages, our methods can give excellent global PageRank estimates.

#index 881468
#* Orthogonal nonnegative matrix t-factorizations for clustering
#@ Chris Ding;Tao Li;Wei Peng;Haesun Park
#t 2006
#c 0
#% 200694
#% 252836
#% 316478
#% 329562
#% 342621
#% 342659
#% 420083
#% 478768
#% 643008
#% 755463
#% 766434
#% 770830
#% 793248
#% 823343
#% 823396
#% 823418
#% 848846
#% 1250561
#% 1650298
#% 1828410
#! Currently, most research on nonnegative matrix factorization (NMF)focus on 2-factor $X=FG^T$ factorization. We provide a systematicanalysis of 3-factor $X=FSG^T$ NMF. While it unconstrained 3-factor NMF is equivalent to it unconstrained 2-factor NMF, itconstrained 3-factor NMF brings new features to it constrained 2-factor NMF. We study the orthogonality constraint because it leadsto rigorous clustering interpretation. We provide new rules for updating $F,S, G$ and prove the convergenceof these algorithms. Experiments on 5 datasets and a real world casestudy are performed to show the capability of bi-orthogonal 3-factorNMF on simultaneously clustering rows and columns of the input datamatrix. We provide a new approach of evaluating the quality ofclustering on words using class aggregate distribution andmulti-peak distribution. We also provide an overview of various NMF extensions andexamine their relationships.

#index 881469
#* A general framework for accurate and fast regression by data summarization in random decision trees
#@ Wei Fan;Joe McCloskey;Philip S. Yu
#t 2006
#c 0
#% 236656
#% 342624
#% 400847
#% 727888
#% 844301
#% 844417
#! Predicting the values of continuous variable as a function of several independent variables is one of the most important problems for data mining. A very large number of regression methods, both parametric and nonparametric, have been proposed in the past. However, since the list is quite extensive and many of these models make rather explicit, strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options, choosing the appropriate regression methodology and then specifying the parameter values is a none-trivial, sometimes frustrating, task for data mining practitioners. Choosing the inappropriate methodology can have rather disappointing results. This issue is against the general utility of data mining software. For example,linear regression methods are straightforward and well-understood. However, since the linear assumption is very strong, its performance is compromised for complicated non-linear problems. Kernel-based methods perform quite well if the kernel functions are selected correctly. In this paper, we propose a straightforward approach based on summarizing the training data using an ensemble of random decisions trees. It requires very little knowledge from the user, yet is applicable to every type of regression problem that we are currently aware of. We have experimented on a wide range of problems including those that parametric methods performwell, a large selection of benchmark datasets for nonparametric regression, as well as highly non-linear stochastic problems. Our results are either significantly better than or identical to many approaches that are known to perform well on these problems.

#index 881470
#* Reverse testing: an efficient framework to select amongst classifiers under sample selection bias
#@ Wei Fan;Ian Davidson
#t 2006
#c 0
#% 17144
#% 190581
#% 214311
#% 342611
#% 376266
#% 769904
#% 770847
#% 844364
#! One of the most important assumptions made by many classification algorithms is that the training and test sets are drawn from the same distribution, i.e., the so-called "stationary distribution assumption" that the future and the past data sets are identical from a probabilistic standpoint. In many domains of real-world applications, such as marketing solicitation, fraud detection, drug testing, loan approval, sub-population surveys, school enrollment among others, this is rarely the case. This is because the only labeled sample available for training is biased in different ways due to a variety of practical reasons and limitations. In these circumstances, traditional methods to evaluate the expected generalization error of classification algorithms, such as structural risk minimization, ten-fold cross-validation, and leave-one-out validation, usually return poor estimates of which classification algorithm, when trained on biased dataset, will be the most accurate for future unbiased dataset, among a number of competing candidates. Sometimes, the estimated order of the learning algorithms' accuracy could be so poor that it is not even better than random guessing. Therefore,a method to determine the most accurate learner is needed for data mining under sample selection bias for many real-world applications. We present such an approach that can determine which learner will perform the best on an unbiased test set, given a possibly biased training set, in a fraction of the computational cost to use cross-validation based approaches.

#index 881471
#* Quantifying trends accurately despite classifier error and class imbalance
#@ George Forman
#t 2006
#c 0
#% 290482
#% 434614
#% 450870
#% 722935
#% 799043
#% 813970
#% 823344
#% 881565
#% 1699624
#! This paper promotes a new task for supervised machine learning research: quantification - the pursuit of learning methods for accurately estimating the class distribution of a test set, with no concern for predictions on individual cases. A variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers. These tasks cover a large and important family of applications that measure trends over time.The paper establishes a research methodology, and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications. In empirical tests, Median Sweep methods show outstanding ability to estimate the class distribution, despite wide disparity in testing and training conditions. The paper addresses shifting class priors and costs, but not concept drift in general.

#index 881472
#* Assessing data mining results via swap randomization
#@ Aristides Gionis;Heikki Mannila;Taneli Mielikäinen;Panayiotis Tsaparas
#t 2006
#c 0
#% 227919
#% 254911
#% 280433
#% 280456
#% 577214
#% 580741
#% 769909
#% 832742
#! The problem of assessing the significance of data mining results on high-dimensional 0-1 data sets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by, e.g., chi-square tests, or many other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are more difficult to apply to sets of patterns or other complex results of data mining. In this paper, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins with the given dataset, computing the results of interest on the randomized instances, and comparing them against the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and rankings. To generate random datasets with given margins, we use variations of a Markov chain approach, which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact, while for other datasets the discovered structure conveys meaningful information.

#index 881473
#* A new efficient probabilistic model for mining labeled ordered trees
#@ Kosuke Hashimoto;Kiyoko F. Aoki-Kinoshita;Nobuhisa Ueda;Minoru Kanehisa;Hiroshi Mamitsuka
#t 2006
#c 0
#% 44876
#% 279755
#% 291299
#% 349550
#% 464640
#% 571903
#% 729941
#% 778469
#% 780683
#% 813989
#% 813991
#! Mining frequent patterns is a general and important issue in data mining. Complex and unstructured (or semi-structured) datasets have appeared in major data mining applications, including text mining, web mining and bioinformatics. Mining patterns from these datasets is the focus of many of the current data mining approaches. We focus on labeled ordered trees, typical datasets of semi-structured data in data mining, and propose a new probabilistic model and its efficient learning scheme for mining labeled ordered trees. The proposed approach significantly improves the time and space complexity of an existing probabilistic modeling for labeled ordered trees, while maintaining its expressive power. We evaluated the performance of the proposed model, comparing it with that of the existing model, using synthetic as well as real datasets from the field of glycobiology. Experimental results showed that the proposed model drastically reduced the computation time of the competing model, keeping the predictive power and avoiding overfitting to the training data. Finally, we assessed our results using the proposed model on real data from a variety of biological viewpoints, verifying known facts in glycobiology.

#index 881474
#* Learning the unified kernel machines for classification
#@ Steven C. H. Hoi;Michael R. Lyu;Edward Y. Chang
#t 2006
#c 0
#% 236729
#% 266426
#% 341269
#% 446680
#% 464268
#% 466887
#% 646003
#% 763697
#% 765552
#% 869526
#% 1455666
#% 1478821
#! Kernel machines have been shown as the state-of-the-art learning techniques for classification. In this paper, we propose a novel general framework of learning the Unified Kernel Machines (UKM) from both labeled and unlabeled data. Our proposed framework integrates supervised learning, semi-supervised kernel learning, and active learning in a unified solution. In the suggested framework, we particularly focus our attention on designing a new semi-supervised kernel learning method, i.e., Spectral Kernel Learning (SKL), which is built on the principles of kernel target alignment and unsupervised kernel design. Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved. Empirical results have shown that our method is more effective and robust to learn the semi-supervised kernels than traditional approaches. Based on the framework, we present a specific paradigm of unified kernel machines with respect to Kernel Logistic Regresions (KLR), i.e., Unified Kernel Logistic Regression (UKLR). We evaluate our proposed UKLR classification scheme in comparison with traditional solutions. The promising results show that our proposed UKLR paradigm is more effective than the traditional classification approaches.

#index 881475
#* Frequent subgraph mining in outerplanar graphs
#@ Tamás Horváth;Jan Ramon;Stefan Wrobel
#t 2006
#c 0
#% 39702
#% 55438
#% 232136
#% 261505
#% 300033
#% 408396
#% 431105
#% 600046
#% 629708
#% 813990
#% 814196
#% 1897234
#! In recent years there has been an increased interest in algorithms that can perform frequent pattern discovery in large databases of graph structured objects. While the frequent connected subgraph mining problem for tree datasets can be solved in incremental polynomial time, it becomes intractable for arbitrary graph databases. Existing approaches have therefore resorted to various heuristic strategies and restrictions of the search space, but have not identified a practically relevant tractable graph class beyond trees. In this paper, we define the class of so called tenuous outerplanar graphs, a strict generalization of trees, develop a frequent subgraph mining algorithm for tenuous outerplanar graphs that works in incremental polynomial time, and evaluate the algorithm empirically on the NCI molecular graph dataset.

#index 881476
#* Adaptive event detection with time-varying poisson processes
#@ Alexander Ihler;Jon Hutchins;Padhraic Smyth
#t 2006
#c 0
#% 280408
#% 577220
#% 577275
#% 795275

#index 881477
#* Training linear SVMs in linear time
#@ Thorsten Joachims
#t 2006
#c 0
#% 260001
#% 269217
#% 269218
#% 342598
#% 428583
#% 431295
#% 458379
#% 577224
#% 722757
#% 722758
#% 763708
#% 794519
#% 840882
#% 872759
#! Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets.

#index 881478
#* Mining quantitative correlated patterns using an information-theoretic approach
#@ Yiping Ke;James Cheng;Wilfred Ng
#t 2006
#c 0
#% 115608
#% 152934
#% 210160
#% 227919
#% 280434
#% 340826
#% 342640
#% 443393
#% 443466
#% 451441
#% 452846
#% 466476
#% 727869
#% 727897
#% 729942
#% 769913
#! Existing research on mining quantitative databases mainly focuses on mining associations. However, mining associations is too expensive to be practical in many cases. In this paper, we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations. We propose a new notion of Quantitative Correlated Patterns (QCPs), which is founded on two formal concepts, mutual information and all-confidence. We first devise a normalization on mutual information and apply it to QCP mining to capture the dependency between the attributes. We further adopt all-confidence as a quality measure to control, at a finer granularity, the dependency between the attributes with specific quantitative intervals. We also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information, such that the interval combining is guided by the dependency between the attributes. We develop an algorithm, QCoMine, to efficiently mine QCPs by utilizing normalized mutual information and all-confidence to perform a two-level pruning. Our experiments verify the efficiency of QCoMine and the quality of the QCPs.

#index 881479
#* Maximally informative k-itemsets and their efficient discovery
#@ Arno J. Knobbe;Eric K. Y. Ho
#t 2006
#c 0
#% 70370
#% 115608
#% 152934
#% 176172
#% 217072
#% 232126
#% 269554
#% 300120
#% 340736
#% 398847
#% 458307
#% 465583
#% 477497
#% 577259
#% 722929
#! In this paper we present a new approach to mining binary data. We treat each binary feature (item) as a means of distinguishing two sets of examples. Our interest is in selecting from the total set of items an itemset of specified size, such that the database is partitioned with as uniform a distribution over the parts as possible. To achieve this goal, we propose the use of joint entropy as a quality measure for itemsets, and refer to optimal itemsets of cardinality k as maximally informative k-itemsets. We claim that this approach maximises distinctive power, as well as minimises redundancy within the feature set. A number of algorithms is presented for computing optimal itemsets efficiently.

#index 881480
#* Measuring and extracting proximity in networks
#@ Yehuda Koren;Stephen C. North;Chris Volinsky
#t 2006
#c 0
#% 70370
#% 238081
#% 249110
#% 310514
#% 408396
#% 730089
#% 769887
#% 830275
#% 1707190
#! Measuring distance or some other form of proximity between objects is a standard data mining tool. Connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks. We propose a new way of measuring and extracting proximity in networks called "cycle free effective conductance"(CFEC). Our proximity measure can handle more than two endpoints, directed edges, is statistically well-behaved, and produces an effectiveness score for the computed subgraphs. We provide an efficien talgorithm. Also, we report experimental results and show examples for three large network data sets: a telecommunications calling graph, the IMDB actors graph, and an academic co-authorship network.

#index 881481
#* Hierarchical topic segmentation of websites
#@ Ravi Kumar;Kunal Punera;Andrew Tomkins
#t 2006
#c 0
#% 248810
#% 279109
#% 292235
#% 319876
#% 340928
#% 379374
#% 465747
#% 577236
#% 577356
#% 577367
#% 637372
#% 642990
#% 658746
#% 724589
#% 730021
#% 764561
#% 769512
#% 789963
#% 807298
#% 809250
#% 824732
#% 869639
#% 1845595
#! In this paper, we consider the problem of identifying and segmenting topically cohesive regions in the URL tree of a large website. Each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier. We develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels. We propose a general framework to use these measures for describing the quality of a segmentation; we also provide an efficient algorithm to find the best segmentation in this framework. Extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations.

#index 881482
#* New EM derived from Kullback-Leibler divergence
#@ Longin Jan Latecki;Marc Sobel;Rolf Lakaemper
#t 2006
#c 0
#% 236301
#% 277483
#% 959438
#! We introduce a new EM framework in which it is possible not only to optimize the model parameters but also the number of model components. A key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the EM framework. While the classical EM algorithm estimates model parameters empirically using the data points themselves, we estimate them using nonparametric density estimates.There exist many possible applications that require optimal adjustment of model components. We present experimental results in two domains. One is polygonal approximation of laser range data, which is an active research topic in robot navigation. The other is grouping of edge pixels to contour boundaries, which still belongs to unsolved problems in computer vision.

#index 881483
#* Workload-aware anonymization
#@ Kristen LeFevre;David J. DeWitt;Raghu Ramakrishnan
#t 2006
#c 0
#% 149
#% 129980
#% 300184
#% 443463
#% 443478
#% 452821
#% 479787
#% 576762
#% 577233
#% 577239
#% 785363
#% 800514
#% 800515
#% 801690
#% 810011
#% 824734
#% 864406
#% 864412
#% 926881
#% 1700134
#! Protecting data privacy is an important problem in microdata distribution. Anonymization algorithms typically aim to protect individual privacy, with minimal impact on the quality of the resulting data. While the bulk of previous work has measured quality through one-size-fits-all measures, we argue that quality is best judged with respect to the workload for which the data will ultimately be used.This paper provides a suite of anonymization algorithms that produce an anonymous view based on a target class of workloads, consisting of one or more data mining tasks, as well as selection predicates. An extensive experimental evaluation indicates that this approach is often more effective than previous anonymization techniques.

#index 881484
#* Very sparse random projections
#@ Ping Li;Trevor J. Hastie;Kenneth W. Church
#t 2006
#c 0
#% 41374
#% 46803
#% 160390
#% 205305
#% 214073
#% 248027
#% 249321
#% 279755
#% 289340
#% 342605
#% 342617
#% 347225
#% 425047
#% 450924
#% 593839
#% 593928
#% 594029
#% 643568
#% 729964
#% 766420
#% 807385
#% 840009
#% 843878
#% 939408
#% 1562061
#% 1674804
#! There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.

#index 881485
#* Rule interestingness analysis using OLAP operations
#@ Bing Liu;Kaidi Zhao;Jeffrey Benkler;Weimin Xiao
#t 2006
#c 0
#% 136350
#% 172386
#% 190581
#% 280436
#% 280487
#% 310525
#% 310531
#% 316709
#% 389169
#% 417589
#% 420101
#% 434613
#% 443092
#% 443313
#% 481290
#% 481954
#% 501204
#% 577216
#% 577252
#% 729934
#% 769893
#% 769926
#% 844348
#! The problem of interestingness of discovered rules has been investigated by many researchers. The issue is that data mining algorithms often generate too many rules, which make it very hard for the user to find the interesting ones. Over the years many techniques have been proposed. However, few have made it to real-life applications. Since August 2004, we have been working on a major application for Motorola. The objective is to find causes of cellular phone call failures from a large amount of usage log data. Class association rules have been shown to be suitable for this type of diagnostic data mining application. We were also able to put several existing interestingness methods to the test, which revealed some major shortcomings. One of the main problems is that most existing methods treat rules individually. However, we discovered that users seldom regard a single rule to be interesting by itself. A rule is only interesting in the context of some other rules. Furthermore, in many cases, each individual rule may not be interesting, but a group of them together can represent an important piece of knowledge. This led us to discover a deficiency of the current rule mining paradigm. Using non-zero minimum support and non-zero minimum confidence eliminates a large amount of context information, which makes rule analysis difficult. This paper proposes a novel approach to deal with all of these issues, which casts rule analysis as OLAP operations and general impression mining. This approach enables the user to explore the knowledge space to find useful knowledge easily and systematically. It also provides a natural framework for visualization. As an evidence of its effectiveness, our system, called Opportunity Map, based on these ideas has been deployed, and it is in daily use in Motorola for finding actionable knowledge from its engineering and other types of data sets.

#index 881486
#* Fast mining of high dimensional expressive contrast patterns using zero-suppressed binary decision diagrams
#@ Elsa Loekito;James Bailey
#t 2006
#c 0
#% 3873
#% 147928
#% 178515
#% 210160
#% 233824
#% 233825
#% 280409
#% 300120
#% 329205
#% 420126
#% 564401
#% 582131
#% 661260
#% 662759
#% 727835
#% 729935
#% 729984
#% 809268
#% 814195
#% 844195
#% 864501
#% 865731
#% 1271849
#! Patterns of contrast are a very important way of comparing multi-dimensional datasets. Such patterns are able to capture regions of high difference between two classes of data, and are useful for human experts and the construction of classifiers. However, mining such patterns is particularly challenging when the number of dimensions is large. This paper describes a new technique for mining several varieties of contrast pattern, based on the use of Zero-Suppressed Binary Decision Diagrams (ZBDDs), a powerful data structure for manipulating sparse data. We study the mining of both simple contrast patterns, such as emerging patterns, and more novel and complex contrasts, which we call disjunctive emerging patterns. A performance study demonstrates our ZBDD technique is highly scalable, substantially improves on state of the art mining for emerging patterns and can be effective for discovering complex contrasts from datasets with thousands of attributes.

#index 881487
#* Unsupervised learning on k-partite graphs
#@ Bo Long;Xiaoyun Wu;Zhongfei (Mark) Zhang;Philip S. Yu
#t 2006
#c 0
#% 148149
#% 202286
#% 273891
#% 274612
#% 313959
#% 342621
#% 342659
#% 466675
#% 495929
#% 566189
#% 578670
#% 643009
#% 729911
#% 729918
#% 769928
#% 823328
#% 823343
#% 823396
#% 876018
#% 916785
#! Various data mining applications involve data objects of multiple types that are related to each other, which can be naturally formulated as a k-partite graph. However, the research on mining the hidden structures from a k-partite graph is still limited and preliminary. In this paper, we propose a general model, the relation summary network, to find the hidden structures (the local cluster structures and the global community structures) from a k-partite graph. The model provides a principal framework for unsupervised learning on k-partite graphs of various structures. Under this model, we derive a novel algorithm to identify the hidden structures of a k-partite graph by constructing a relation summary network to approximate the original k-partite graph under a broad range of distortion measures. Experiments on both synthetic and real datasets demonstrate the promise and effectiveness of the proposed model and algorithm. We also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the clustering approaches.

#index 881488
#* Tensor-CUR decompositions for tensor-based data
#@ Michael W. Mahoney;Mauro Maggioni;Petros Drineas
#t 2006
#c 0
#% 84545
#% 107150
#% 220706
#% 272510
#% 316143
#% 316150
#% 338443
#% 347192
#% 420515
#% 452563
#% 465928
#% 593891
#% 730049
#% 734915
#% 765311
#% 813966
#% 870224
#% 870225
#% 870226
#% 916799
#% 1650569
#% 1673017
#! Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices, we develop a tensor-based extension of the matrix CUR decomposition. The tensor-CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others. In this case, the tensor-CUR decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms ofthe processes generating the data. In order to demonstrate the general applicability of this tensor decomposition, we apply it to problems in two diverse domains of data analysis: hyperspectral medical image analysis and consumer recommendation system analysis. In the hyperspectral data application, the tensor-CUR decomposition is used to compress the data, and we show that classification quality is not substantially reduced even after substantial data compression. In the recommendation system application, the tensor-CUR decomposition is used to reconstruct missing entries in a user-product-product preference tensor, and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of product-product comparisons from a new user.

#index 881489
#* Generating semantic annotations for frequent patterns with context analysis
#@ Qiaozhu Mei;Dong Xin;Hong Cheng;Jiawei Han;ChengXiang Zhai
#t 2006
#c 0
#% 144034
#% 152934
#% 227919
#% 248791
#% 287285
#% 321635
#% 342629
#% 420481
#% 463903
#% 629644
#% 629708
#% 727896
#% 729418
#% 769876
#% 823356
#% 824710
#! As a fundamental data mining task, frequent pattern mining has widespread applications in many different domains. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important nextstep - interpreting the discovered frequent patterns. Although some recent work has studied the compression and summarization of frequent patterns, the proposed techniques can only annotate a frequent pattern with non-semantical information (e.g. support), which provides only limited help for a user to understand the patterns.In this paper, we propose the novel problem of generating semantic annotations for frequent patterns. The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. We propose a general approach to generate such anannotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations.

#index 881490
#* Aggregating time partitions
#@ Taneli Mielikäinen;Evimaria Terzi;Panayiotis Tsaparas
#t 2006
#c 0
#% 209021
#% 289519
#% 325683
#% 326303
#% 328351
#% 330769
#% 338425
#% 340031
#% 420063
#% 423634
#% 425065
#% 451127
#% 463903
#% 466506
#% 564281
#% 643520
#% 722902
#% 728028
#% 734915
#% 800530
#% 801673
#% 803762
#% 805093
#% 805798
#% 823339
#! Partitions of sequential data exist either per se or as a result of sequence segmentation algorithms. It is often the case that the same timeline is partitioned in many different ways. For example, different segmentation algorithms produce different partitions of the same underlying data points. In such cases, we are interested in producing an aggregate partition, i.e., a segmentation that agrees as much as possible with the input segmentations. Each partition is defined as a set of continuous non-overlapping segments of the timeline. We show that this problem can be solved optimally in polynomial time using dynamic programming. We also propose faster greedy heuristics that work well in practice. We experiment with our algorithms and we demonstrate their utility in clustering the behavior of mobile-phone users and combining the results of different segmentation algorithms on genomic sequences.

#index 881491
#* Using structure indices for efficient approximation of network properties
#@ Matthew J. Rattigan;Marc Maier;David Jensen
#t 2006
#c 0
#% 68247
#% 300078
#% 327432
#% 496116
#% 577219
#% 656745
#% 725363
#% 749529
#% 769887
#% 785120
#% 785353
#% 813718
#% 816485
#% 821922
#% 823342
#% 1289399
#! Statistics on networks have become vital to the study of relational data drawn from areas such as bibliometrics, fraud detection, bioinformatics, and the Internet. Calculating many of the most important measures - such as betweenness centrality, closeness centrality, and graph diameter-requires identifying short paths in these networks. However, finding these short paths can be intractable for even moderate-size networks. We introduce the concept of a network structure index (NSI), a composition of (1) a set of annotations on every node in the network and (2) a function that uses the annotations to estimate graph distance between pairs of nodes. We present several varieties of NSIs, examine their time and space complexity, and analyze their performance on synthetic and real data sets. We show that creating an NSI for a given network enables extremely efficient and accurate estimation of a wide variety of network statistics on that network.

#index 881492
#* Learning sparse metrics via linear programming
#@ Rómer Rosales;Glenn Fung
#t 2006
#c 0
#% 26125
#% 201893
#% 224113
#% 272510
#% 464291
#% 466084
#% 565538
#% 722898
#! Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. This calculation is crucial for efficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Moreover, the performance of these algorithms depends critically on choosing a good distance function. However, it is often the case that (1) the correct distance is unknown or chosen by hand, and (2) its calculation is computationally expensive (e.g., such as for large dimensional objects). In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). This method allows learning unknown distance functions (or approximating known functions) with the additional property of reducing distance computation time. We present an algorithm that given examples of proximity comparisons among triples of objects (object i is more like object j than object k), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships. Unlike other popular embedding algorithms, this method can easily generalize to new points, does not have local minima, and explicitly models computational efficiency by finding a mapping that is sparse, i.e. one that depends on a small subset of features or dimensions. Experimental evaluation shows that the proposed formulation compares favorably with a state-of-the art method in several publicly available datasets.

#index 881493
#* Beyond streams and graphs: dynamic tensor analysis
#@ Jimeng Sun;Dacheng Tao;Christos Faloutsos
#t 2006
#c 0
#% 120749
#% 124009
#% 152934
#% 248027
#% 258598
#% 268079
#% 282905
#% 342600
#% 457831
#% 480156
#% 570887
#% 594009
#% 729918
#% 810066
#% 812492
#% 824709
#% 844312
#% 846431
#% 894646
#! How do we find patterns in author-keyword associations, evolving over time? Or in Data Cubes, with product-branch-customer sales information? Matrix decompositions, like principal component analysis (PCA) and variants, are invaluable tools for mining, dimensionality reduction, feature selection, rule identification in numerous settings like streaming data, text, graphs, social networks and many more. However, they have only two orders, like author and keyword, in the above example.We propose to envision such higher order data as tensors,and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce the dynamic tensor analysis (DTA) method, and its variants. DTA provides a compact summary for high-order and high-dimensional data, and it also reveals the hidden correlations. Algorithmically, we designed DTA very carefully so that it is (a) scalable, (b) space efficient (it does not need to store the past) and (c) fully automatic with no need for user defined parameters. Moreover, we propose STA, a streaming tensor analysis method, which provides a fast, streaming approximation to DTA.We implemented all our methods, and applied them in two real settings, namely, anomaly detection and multi-way latent semantic indexing. We used two real, large datasets, one on network flow data (100GB over 1 month) and one from DBLP (200MB over 25 years). Our experiments show that our methods are fast, accurate and that they find interesting patterns and outliers on the real datasets.

#index 881494
#* Acclimatizing Taxonomic Semantics for Hierarchical Content Classification
#@ Lei Tang;Jianping Zhang;Huan Liu
#t 2006
#c 0
#% 280492
#% 280866
#% 309141
#% 413611
#% 420466
#% 465747
#% 466078
#% 466501
#% 479817
#% 571073
#% 642986
#% 722935
#% 770763
#% 770796
#% 783478
#% 783483
#% 796212
#% 807374
#% 829975
#% 840928
#% 844408
#! Hierarchical models have been shown to be effective in content classification. However, we observe through empirical study that the performance of a hierarchical model varies with given taxonomies; even a semantically sound taxonomy has potential to change its structure for better classification. By scrutinizing typical cases, we elucidate why a given semantics-based hierarchy does not work well in content classification, and how it could be improved for accurate hierarchical classification. With these understandings, we propose effective localized solutions that modify the given taxonomy for accurate hierarchical classification. We conduct extensive experiments on both toy and real-world data sets, report improved performance and interesting findings, and provide further analysis of algorithmic issues such as time complexity, robustness, and sensitivity to the number of features.

#index 881495
#* Mining distance-based outliers from large databases in any metric space
#@ Yufei Tao;Xiaokui Xiao;Shuigeng Zhou
#t 2006
#c 0
#% 300136
#% 300183
#% 342625
#% 479791
#% 479931
#% 479986
#% 570886
#% 729912
#% 803123
#% 823340
#! Let R be a set of objects. An object o ∈ R is an outlier, if there exist less than k objects in R whose distances to o are at most r. The values of k, r, and the distance metric are provided by a user at the run time. The objective is to return all outliers with the smallest I/O cost.This paper considers a generic version of the problem, where no information is available for outlier computation, except for objects' mutual distances. We prove an upper bound for the memory consumption which permits the discovery of all outliers by scanning the dataset 3 times. The upper bound turns out to be extremely low in practice, e.g., less than 1% of R. Since the actual memory capacity of a realistic DBMS is typically larger, we develop a novel algorithm, which integrates our theoretical findings with carefully-designed heuristics that leverage the additional memory to improve I/O efficiency. Our technique reports all outliers by scanning the dataset at most twice (in some cases, even once), and significantly outperforms the existing solutions by a factor up to an order of magnitude.

#index 881496
#* Center-piece subgraphs: problem definition and fast solutions
#@ Hanghang Tong;Christos Faloutsos
#t 2006
#c 0
#% 249110
#% 291940
#% 310514
#% 348173
#% 438553
#% 577273
#% 729918
#% 729923
#% 730089
#% 769887
#% 769952
#% 780688
#% 824710
#% 844334
#% 853538
#% 994033
#% 1016175
#% 1016176
#! Given Q nodes in a social network (say, authorship network), how can we find the node/author that is the center-piece, and has direct or indirect connections to all, or most of them? For example, this node could be the common advisor, or someone who started the research area that the Q nodes belong to. Isomorphic scenarios appear in law enforcement (find the master-mind criminal, connected to all current suspects), gene regulatory networks (find the protein that participates in pathways with all or most of the given Q proteins), viral marketing and many more.Connection subgraphs is an important first step, handling the case of Q=2 query nodes. Then, the connection subgraph algorithm finds the b intermediate nodes, that provide a good connection between the two original query nodes.Here we generalize the challenge in multiple dimensions: First, we allow more than two query nodes. Second, we allow a whole family of queries, ranging from 'OR' to 'AND', with 'softAND' in-between. Finally, we design and compare a fast approximation, and study the quality/speed trade-off.We also present experiments on the DBLP dataset. The experiments confirm that our proposed method naturally deals with multi-source queries and that the resulting subgraphs agree with our intuition. Wall-clock timing results on the DBLP dataset show that our proposed approximation achieve good accuracy for about 6:1 speedup.

#index 881497
#* Anonymizing sequential releases
#@ Ke Wang;Benjamin C. M. Fung
#t 2006
#c 0
#% 136350
#% 329858
#% 443463
#% 576761
#% 577239
#% 765449
#% 776625
#% 785363
#% 800514
#% 800515
#% 801690
#% 810011
#% 824727
#% 844340
#% 864406
#% 864412
#% 874988
#% 874989
#% 881546
#% 951837
#% 1700133
#% 1700134
#% 1719423
#! An organization makes a new release as new information become available, releases a tailored view for each data request, releases sensitive information and identifying information separately. The availability of related releases sharpens the identification of individuals by a global quasi-identifier consisting of attributes from related releases. Since it is not an option to anonymize previously released data, the current release must be anonymized to ensure that a global quasi-identifier is not effective for identification. In this paper, we study the sequential anonymization problem under this assumption. A key question is how to anonymize the current release so that it cannot be linked to previous releases yet remains useful for its own release purpose. We introduce the lossy join, a negative property in relational database design, as a way to hide the join relationship among releases, and propose a scalable and practical solution.

#index 881498
#* Topics over time: a non-Markov continuous-time model of topical trends
#@ Xuerui Wang;Andrew McCallum
#t 2006
#c 0
#% 577220
#% 722904
#% 788094
#% 823373
#% 840903
#% 868088
#% 875959
#% 1289476
#% 1650390
#! This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data, but also how the structure changes over time. Unlike other recent work that relies on Markov assumptions or discretization of time, here each topic is associated with a continuous distribution over timestamps, and for each generated document, the mixture distribution over topics is influenced by both word co-occurrences and the document's timestamp. Thus, the meaning of a particular topic can be relied upon as constant, but the topics' occurrence and correlations change significantly over time. We present results on nine months of personal email, 17 years of NIPS research papers and over 200 years of presidential state-of-the-union addresses, showing improved topics, better timestamp prediction, and interpretable trends.

#index 881499
#* Discovering significant rules
#@ Geoffrey I. Webb
#t 2006
#c 0
#% 152934
#% 227919
#% 280433
#% 280456
#% 342597
#% 342643
#% 420112
#% 420126
#% 478770
#% 769913
#% 772329
#% 789955
#% 799029
#% 953967
#! In many applications, association rules will only be interesting if they represent non-trivial correlations between all constituent items. Numerous techniques have been developed that seek to avoid false discoveries. However, while all provide useful solutions to aspects of this problem, none provides a generic solution that is both flexible enough to accommodate varying definitions of true and false discoveries and powerful enough to provide strict control over the risk of false discoveries. This paper presents generic techniques that allow definitions of true and false discoveries to be specified in terms of arbitrary statistical hypothesis tests and which provide strict control over the experiment wise risk of false discoveries.

#index 881500
#* Extracting redundancy-aware top-k patterns
#@ Dong Xin;Hong Cheng;Xifeng Yan;Jiawei Han
#t 2006
#c 0
#% 36672
#% 71901
#% 158687
#% 262112
#% 281656
#% 443092
#% 478770
#% 479816
#% 577214
#% 629644
#% 769876
#% 769893
#% 818209
#% 823344
#% 823356
#% 824710
#% 835872
#% 1845364
#! Observed in many applications, there is a potential need of extracting a small set of frequent patterns having not only high significance but also low redundancy. The significance is usually defined by the context of applications. Previous studies have been concentrating on how to compute top-k significant patterns or how to remove redundancy among patterns separately. There is limited work on finding those top-k patterns which demonstrate high-significance and low-redundancy simultaneously.In this paper, we study the problem of extracting redundancy-aware top-k patterns from a large collection of frequent patterns. We first examine the evaluation functions for measuring the combined significance of a pattern set and propose the MMS (Maximal Marginal Significance) as the problem formulation. The problem is known as NP-hard. We further present a greedy algorithm which approximates the optimal solution with performance bound O(log k) (with conditions on redundancy), where k is the number of reported patterns. The direct usage of redundancy-aware top-k patterns is illustrated through two real applications: disk block prefetch and document theme extraction. Our method can also be applied to processing redundancy-aware top-k queries in traditional database.

#index 881501
#* Regularized discriminant analysis for high dimensional, low sample size data
#@ Jieping Ye;Tie Wang
#t 2006
#c 0
#% 80995
#% 212689
#% 235342
#% 309208
#% 420077
#% 729437
#% 770770
#% 791368
#! Linear and Quadratic Discriminant Analysis have been used widely in many areas of data mining, machine learning, and bioinformatics. Friedman proposed a compromise between Linear and Quadratic Discriminant Analysis, called Regularized Discriminant Analysis (RDA), which has been shown to be more flexible in dealing with various class distributions. RDA applies the regularization techniques by employing two regularization parameters, which are chosen to jointly maximize the classification performance. The optimal pair of parameters is commonly estimated via cross-validation from a set of candidate pairs. It is computationally prohibitive for high dimensional data, especially when the candidate set is large, which limits the applications of RDA to low dimensional data.In this paper, a novel algorithm for RDA is presented for high dimensional data. It can estimate the optimal regularization parameters from a large set of parameter candidates efficiently. Experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency, and also show that, for a properly chosen pair of regularization parameters, RDA performs favorably in classification, in comparison with other existing classification methods.

#index 881502
#* Supervised probabilistic principal component analysis
#@ Shipeng Yu;Kai Yu;Volker Tresp;Hans-Peter Kriegel;Mingrui Wu
#t 2006
#c 0
#% 278011
#% 715096
#% 729437
#% 763698
#% 763708
#% 769886
#% 770770
#% 818234
#! Principal component analysis (PCA) has been extensively applied in data mining, pattern recognition and information retrieval for unsupervised dimensionality reduction. When labels of data are available, e.g., in a classification or regression task, PCA is however not able to use this information. The problem is more interesting if only part of the input data are labeled, i.e., in a semi-supervised setting. In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA, both of which are extensions of a probabilistic PCA model. The proposed models are able to incorporate the label information into the projection phase, and can naturally handle multiple outputs (i.e., in multi-task learning problems). We derive an efficient EM learning algorithm for both models, and also provide theoretical justifications of the model behaviors. SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks, and show not only promising performance but also good scalability.

#index 881503
#* Extracting key-substring-group features for text classification
#@ Dell Zhang;Wee Sun Lee
#t 2006
#c 0
#% 68236
#% 118771
#% 190581
#% 222437
#% 232650
#% 235941
#% 260001
#% 262096
#% 262128
#% 279755
#% 280817
#% 281186
#% 309208
#% 311034
#% 330784
#% 340899
#% 340903
#% 344447
#% 351094
#% 375017
#% 376266
#% 387427
#% 389155
#% 397141
#% 402289
#% 408466
#% 458369
#% 458379
#% 464630
#% 465754
#% 498538
#% 569860
#% 587959
#% 588082
#% 722803
#% 722925
#% 735077
#% 740416
#% 741122
#% 743284
#% 746867
#% 748499
#% 748738
#% 750863
#% 770810
#% 771841
#% 818235
#% 1715613
#% 1860941
#! In many text classification applications, it is appealing to take every document as a string of characters rather than a bag of words. Previous research studies in this area mostly focused on different variants of generative Markov chain models. Although discriminative machine learning methods like Support Vector Machine (SVM) have been quite successful in text classification with word features, it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features. In this paper, we propose to partition all substrings into statistical equivalence groups, and then pick those groups which are important (in the statistical sense) as features (named key-substring-group features) for text classification. In particular, we propose a suffix tree based algorithm that can extract such features in linear time (with respect to the total number of characters in the corpus). Our experiments on English, Chinese and Greek datasets show that SVM with key-substring-group features can achieve outstanding performance for various text classification tasks.

#index 881504
#* Event detection from evolution of click-through data
#@ Qiankun Zhao;Tie-Yan Liu;Sourav S. Bhowmick;Wei-Ying Ma
#t 2006
#c 0
#% 262042
#% 310567
#% 313959
#% 330617
#% 330678
#% 348155
#% 577224
#% 577273
#% 577297
#% 643014
#% 730021
#% 783482
#% 805877
#% 869517
#% 993965
#% 1719413
#! Previous efforts on event detection from the web have focused primarily on web content and structure data ignoring the rich collection of web log data. In this paper, we propose the first approach to detect events from the click-through data, which is the log data of web search engines. The intuition behind event detection from click-through data is that such data is often event-driven and each event can be represented as a set ofquery-page pairs that are not only semantically similar but also have similar evolution pattern over time. Given the click-through data, in our proposed approach, we first segment it into a sequence of bipartite graphs based on theuser-defined time granularity. Next, the sequence of bipartite graphs is represented as a vector-based graph, which records the semantic and evolutionary relationships between queries and pages. After that, the vector-based graph is transformed into its dual graph, where each node is a query-page pair that will be used to represent real world events. Then, the problem of event detection is equivalent to the problem of clustering the dual graph of the vector-based graph. The clustering process is based on a two-phase graph cut algorithm. In the first phase, query-page pairs are clustered based on thesemantic-based similarity such that each cluster in the result corresponds to a specific topic. In the second phase, query-page pairs related to the same topic are further clustered based on the evolution pattern-based similarity such that each cluster is expected to represent a specific event under the specific topic. Experiments with real click-through data collected from a commercial web search engine show that the proposed approach produces high quality results.

#index 881505
#* Simultaneous record detection and attribute labeling in web data extraction
#@ Jun Zhu;Zaiqing Nie;Ji-Rong Wen;Bo Zhang;Wei-Ying Ma
#t 2006
#c 0
#% 73441
#% 273925
#% 292235
#% 312860
#% 330784
#% 388024
#% 431536
#% 464434
#% 480824
#% 654469
#% 660272
#% 729939
#% 754078
#% 765411
#% 766464
#% 769884
#% 770844
#% 788107
#% 805845
#% 805846
#% 836836
#% 840966
#% 854813
#% 938708
#% 1271981
#% 1279275
#% 1502489
#! Recent work has shown the feasibility and promise of template-independent Web data extraction. However, existing approaches use decoupled strategies - attempting to do data record detection and attribute labeling in two separate phases. In this paper, we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously. In our approach, record detection can benefit from the availability of semantics required in attribute labeling and, at the same time, the accuracy of attribute labeling can be improved when data records are labeled in a collective manner. The proposed model is called Hierarchical Conditional Random Fields. It can efficiently integrate all useful features by learning their importance, and it can also incorporate hierarchical interactions which are very important for Web data extraction. We empirically compare the proposed model with existing decoupled approaches for product information extraction, and the results show significant improvements in both record detection and attribute labeling.

#index 881506
#* Outlier detection by active learning
#@ Naoki Abe;Bianca Zadrozny;John Langford
#t 2006
#c 0
#% 116165
#% 209021
#% 235377
#% 235380
#% 300136
#% 300183
#% 466095
#% 466586
#% 466674
#% 479791
#% 633259
#% 770807
#% 823340
#! Most existing approaches to outlier detection are based on density estimation methods. There are two notable issues with these methods: one is the lack of explanation for outlier flagging decisions, and the other is the relatively high computational requirement. In this paper, we present a novel approach to outlier detection based on classification, in an attempt to address both of these issues. Our approach isbased on two key ideas. First, we present a simple reduction of outlier detection to classification, via a procedure that involves applying classification to a labeled data set containing artificially generated examples that play the role of potential outliers. Once the task has been reduced to classification, we then invoke a selective sampling mechanism based on active learning to the reduced classification problem. We empirically evaluate the proposed approach using a number of data sets, and find that our method is superior to other methods based on the same reduction to classification, but using standard classification methods. We also show that it is competitive to the state-of-the-art outlier detection methods in the literature based on density estimation, while significantly improving the computational complexity and explanatory power.

#index 881507
#* On privacy preservation against adversarial data mining
#@ Charu C. Aggarwal;Jian Pei;Bo Zhang
#t 2006
#c 0
#% 1868
#% 300184
#% 333876
#% 342614
#% 577233
#% 577289
#% 740764
#% 769885
#% 800515
#% 993988
#! Privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data. A rudimentary way to preserve privacy is to simply hide the information in some of the sensitive fields picked by a user. However, such a method is far from satisfactory in its ability to prevent adversarial data mining. Real data records are not randomly distributed. As a result, some fields in the records may be correlated with one another. If the correlation is sufficiently high, it may be possible for an adversary to predict some of the sensitive fields using other fields.In this paper, we study the problem of privacy preservation against adversarial data mining, which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved. In other words, even by data mining, an adversary still cannot accurately recover the hidden data entries. We model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice. An extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach.

#index 881508
#* CCCS: a top-down associative classifier for imbalanced class distribution
#@ Bavani Arunasalam;Sanjay Chawla
#t 2006
#c 0
#% 227919
#% 466483
#% 481290
#% 765413
#% 766206
#% 785383
#% 810064
#% 835018
#! In this paper we propose CCCS, a new algorithm for classification based on association rule mining. The key innovation in CCCS is the use of a new measure, the "Complement Class Support (CCS)" whose application results in rules which are guaranteed to be positively correlated. Furthermore, the anti-monotonic property that CCS possesses has very different semantics vis-a-vis the traditional support measure. In particular, "good" rules have a low CCS value. This makes CCS an ideal measure to use in conjunction with a top-down algorithm. Finally, the nature of CCS allows the pruning of rules without the setting of any threshold parameter! To the best of our knowledge this is the first threshold-free algorithm in association rule mining for classification.

#index 881509
#* A framework for analysis of dynamic social networks
#@ Tanya Y. Berger-Wolf;Jared Saia
#t 2006
#c 0
#% 166862
#% 577360
#% 729923
#% 736155
#% 823342
#% 1414755
#! Finding patterns of social interaction within a population has wide-ranging applications including: disease modeling, cultural and information transmission, and behavioral ecology. Social interactions are often modeled with networks. A key characteristic of social interactions is their continual change. However, most past analyses of social networks are essentially static in that all information about the time that social interactions take place is discarded. In this paper, we propose a new mathematical and computational framework that enables analysis of dynamic social networks and that explicitly makes use of information about when social interactions occur.

#index 881510
#* Query-time entity resolution
#@ Indrajit Bhattacharya;Lise Getoor;Louis Licamele
#t 2006
#c 0
#% 201889
#% 310516
#% 577238
#% 654467
#% 766199
#% 777329
#% 810014
#% 810020
#% 830526
#% 870896
#% 993980
#! The goal of entity resolution is to reconcile database references corresponding to the same real-world entities. Given the abundance of publicly available databases where entities are not resolved, we motivate the problem of quickly processing queries that require resolved entities from such 'unclean' databases. We propose a two-stage collective resolution strategy for processing queries. We then show how it can be performed on-the-fly by adaptively extracting and resolving those database references that are the most helpful for resolving the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries can be answered in real time using our adaptive approach while preserving the gains of collective resolution.

#index 881511
#* Model compression
#@ Cristian Buciluǎ;Rich Caruana;Alexandru Niculescu-Mizil
#t 2006
#c 0
#% 132938
#% 209021
#% 290482
#% 328306
#% 400847
#% 465755
#% 466583
#% 770854
#% 840901
#% 1279286
#! Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.

#index 881512
#* Classification features for attack detection in collaborative recommender systems
#@ Robin Burke;Bamshad Mobasher;Chad Williams;Runa Bhaumik
#t 2006
#c 0
#% 173879
#% 754097
#% 754174
#% 783438
#% 807349
#% 836153
#% 844357
#% 883676
#% 926881
#% 1728733
#! Collaborative recommender systems are highly vulnerable to attack. Attackers can use automated means to inject a large number of biased profiles into such a system, resulting in recommendations that favor or disfavor given items. Since collaborative recommender systems must be open to user input, it is difficult to design a system that cannot be so attacked. Researchers studying robust recommendation have therefore begun to identify types of attacks and study mechanisms for recognizing and defeating them. In this paper, we propose and study different attributes derived from user profiles for their utility in attack detection. We show that a machine learning classification approach that includes attributes derived from attack models is more successful than more generalized detection algorithms previously studied.

#index 881513
#* Single-pass online learning: performance, voting schemes and online feature selection
#@ Vitor R. Carvalho;William W. Cohen
#t 2006
#c 0
#% 302390
#% 376266
#% 425046
#% 451055
#% 465754
#% 722935
#% 854646
#% 1289515
#! To learn concepts over massive data streams, it is essential to design inference and learning methods that operate in real time with limited memory. Online learning methods such as perceptron or Winnow are naturally suited to stream processing; however, in practice multiple passes over the same training data are required to achieve accuracy comparable to state-of-the-art batch learners. In the current work we address the problem of training an on-line learner with a single passover the data. We evaluate several existing methods, and also propose a new modification of Margin Balanced Winnow, which has performance comparable to linear SVM. We also explore the effect of averaging, a.k.a. voting, on online learning. Finally, we describe how the new Modified Margin Balanced Winnow algorithm can be naturally adapted to perform feature selection. This scheme performs comparably to widely-used batch feature selection methods like information gain or Chi-square, with the advantage of being able to select features on-the-fly. Taken together, these techniques allow single-pass online learning to be competitive with batch techniques, and still maintain the advantages of on-line learning.

#index 881514
#* Evolutionary clustering
#@ Deepayan Chakrabarti;Ravi Kumar;Andrew Tomkins
#t 2006
#c 0
#% 262042
#% 329562
#% 451052
#% 593737
#% 594012
#% 722762
#% 729437
#% 765412
#% 805839
#! We consider the problem of clustering data over time. An evolutionary clustering should simultaneously optimize two potentially conflicting criteria: first, the clustering at any point in time should remain faithful to the current data as much as possible; and second, the clustering should not shift dramatically from one timestep to the next. We present a generic framework for this problem, and discuss evolutionary versions of two widely-used clustering algorithms within this framework: k-means and agglomerative hierarchical clustering. We extensively evaluate these algorithms on real data sets and show that our algorithms can simultaneously attain both high accuracy in capturing today's data, and high fidelity in reflecting yesterday's clustering.

#index 881515
#* Algorithms for discovering bucket orders from data
#@ Aristides Gionis;Heikki Mannila;Kai Puolamäki;Antti Ukkonen
#t 2006
#c 0
#% 68611
#% 268079
#% 290830
#% 310515
#% 330769
#% 348173
#% 453464
#% 464451
#% 564281
#% 654466
#% 729922
#% 765418
#% 799636
#% 801673
#% 805798
#% 810018
#% 823353
#% 858155
#% 1016203
#% 1272396
#! Ordering and ranking items of different types are important tasks in various applications, such as query processing and scientific data mining. A total order for the items can be misleading, since there are groups of items that have practically equal ranks.We consider bucket orders, i.e., total orders with ties. They can be used to capture the essential order information without overfitting the data: they form a useful concept class between total orders and arbitrary partial orders. We address the question of finding a bucket order for a set of items, given pairwise precedence information between the items. We also discuss methods for computing the pairwise precedence data.We describe simple and efficient algorithms for finding good bucket orders. Several of the algorithms have a provable approximation guarantee, and they scale well to large datasets. We provide experimental results on artificial and a real data that show the usefulness of bucket orders and demonstrate the accuracy and efficiency of the algorithms.

#index 881516
#* Mining relational data through correlation-based multiple view validation
#@ Hongyu Guo;Herna L. Viktor
#t 2006
#c 0
#% 36160
#% 53706
#% 136350
#% 156186
#% 243728
#% 252011
#% 266396
#% 290482
#% 392781
#% 420077
#% 458257
#% 496116
#% 732227
#% 745491
#% 830277
#! Commercial relational databases currently store vast amounts of real-world data. The data within these relational repositories are represented by multiple relations, which are inter-connected by means of foreign key joins. The mining of such interrelated data poses a major challenge to the data mining community. Unfortunately, traditional data mining algorithms usually only explore one relation, the so-called target relation, thus excluding crucial knowledge embedded in the related so-called background relations. In this paper, we propose a novel approach for classifying relational such domains. This strategy employs multiple views to capture crucial information not only from the target relation, but also from related relations. This information is integrated into the relational mining process. The framework presented here, firstly, explore the relational domain to partition its features space into multiple subsets. Subsequently, these subsets are used to construct multiple uncorrelated views, based on a novel correlation-based view validation method, against the target concept. Finally, the knowledge possessed by multiple views are incorporated into a meta-learning mechanism to augment one another. Based on this framework, a wide range of conventional data mining methods can be applied to mine relational databases. Our experiments on benchmark real-world data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time, when compared with two other relational data mining approaches.

#index 881517
#* Recommendation method for extending subscription periods
#@ Tomoharu Iwata;Kazumi Saito;Takeshi Yamada
#t 2006
#c 0
#% 73441
#% 173879
#% 280422
#% 280442
#% 301259
#% 420121
#% 580590
#% 750187
#% 823392
#% 1777282
#% 1860661
#! Online stores providing subscription services need to extend user subscription periods as long as possible to increase their profits. Conventional recommendation methods recommend items that best coincide with user's interests to maximize the purchase probability, which does not necessarily contribute to extend subscription periods. We present a novel recommendation method for subscription services that maximizes the probability of the subscription period being extended. Our method finds frequent purchase patterns in the long subscription period users, and recommends items for a new user to simulate the found patterns. Using survival analysis techniques, we efficiently extract information from the log data for finding the patterns. Furthermore, we infer user's interests from purchase histories based on maximum entropy models, and use the interests to improve the recommendations. Since a longer subscription period is the result of greater user satisfaction, our method benefits users as well as online stores. We evaluate our method using the real log data of an online cartoon distribution service for cell-phone in Japan.

#index 881518
#* Dynamic, real-time forecasting of online auctions via functional models
#@ Wolfgang Jank;Galit Shmueli;Shanshan Wang
#t 2006
#c 0
#% 823366
#! We propose a dynamic model for forecasting price in online auctions. One of the key features of our model is that it operates during the live-auction, which makes it different from previous approaches that only consider static models. Our model is also different with respect to how information about price is incorporated. While one part of the model is based on the more traditional notion of an auction's price-level, another part incorporates its dynamics in the form of a price's velocity and acceleration. In that sense, it incorporates key features of a dynamic environment such as an online auction. The use of novel functional data methodology allows us to measure, and subsequently include, dynamic price characteristics. We illustrate our model on a diverse set of eBay auctions across many different book categories. We find significantly higher prediction accuracy compared to standard approaches.

#index 881519
#* Polynomial association rules with applications to logistic regression
#@ Szymon Jaroszewicz
#t 2006
#c 0
#% 152934
#% 210160
#% 520224
#% 769958
#% 823351
#% 1702640
#! A new class of associations (polynomial itemsets and polynomial association rules) is presented which allows for discovering nonlinear relationships between numeric attributes without discretization. For binary attributes, proposed associations reduce to classic itemsets and association rules. Many standard association rule mining algorithms can be adapted to finding polynomial itemsets and association rules. We applied polynomial associations to add non-linear terms to logistic regression models. Significant performance improvement was achieved over stepwise methods, traditionally used in statistics, with comparable accuracy.

#index 881520
#* CFI-Stream: mining closed frequent itemsets in data streams
#@ Nan Jiang;Le Gruenwald
#t 2006
#c 0
#% 338425
#% 481290
#% 729933
#% 729959
#% 785339
#% 843874
#% 993960
#! Mining frequent closed itemsets provides complete and condensed information for non-redundant association rules generation. Extensive studies have been done on mining frequent closed itemsets, but they are mainly intended for traditional transaction databases and thus do not take data stream characteristics into consideration. In this paper, we propose a novel approach for mining closed frequent itemsets over data streams. It computes and maintains closed itemsets online and incrementally, and can output the current closed frequent itemsets in real time based on users' specified thresholds. Experimental results show that our proposed method is both time and space efficient, has good scalability as the number of transactions processed increases and adapts very rapidly to the change in data streams.

#index 881521
#* Reducing the human overhead in text categorization
#@ Arnd Christian König;Eric Brill
#t 2006
#c 0
#% 116165
#% 260001
#% 266292
#% 269218
#% 287434
#% 289010
#% 458379
#% 544003
#% 544191
#% 708398
#% 722797
#% 741122
#% 769908
#% 854646
#% 938687
#% 1250186
#% 1289485
#! Many applications in text processing require significant human effort for either labeling large document collections (when learning statistical models) or extrapolating rules from them (when using knowledge engineering). In this work, we describe away to reduce this effort, while retaining the methods' accuracy, by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text patterns to complement machine learning. Using a standard sentiment-classification dataset and real customer feedback data, we demonstrate that the resulting technique results in significant reduction of the human effort required to obtain a given classification accuracy. Moreover, the hybrid text classifier also results in a significant boost in accuracy over machine-learning based classifiers when a comparable amount of labeled data is used.

#index 881522
#* Algorithms for storytelling
#@ Deept Kumar;Naren Ramakrishnan;Richard F. Helm;Malcolm Potts
#t 2006
#c 0
#% 226545
#% 273920
#% 572294
#% 731611
#% 733400
#% 765463
#% 769902
#% 823386
#% 1269491
#% 1272326
#% 1810871
#! We formulate a new data mining problem called it storytelling as a generalization of redescription mining. In traditional redescription mining, we are given a set of objects and a collection of subsets defined over these objects. The goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects. Storytelling, on the other hand, aims to explicitly relate object sets that are disjoint (and hence, maximally dissimilar) by finding a chain of (approximate) redescriptions between the sets. This problem finds applications in bioinformatics, for instance, where the biologist is trying to relate a set of genes expressed in one experiment to another set, implicated in a different pathway. We outline an efficient storytelling implementation that embeds the CART wheels redescription mining algorithm in an A* search procedure, using the former to supply next move operators on search branches to the latter. This approach is practical and effective for mining large datasets and, at the same time, exploits the structure of partitions imposed by the given vocabulary. Three application case studies are presented: a study of word overlaps in large English dictionaries, exploring connections between genesets in a bioinformatics dataset, and relating publications in the PubMed index of abstracts.

#index 881523
#* Structure and evolution of online social networks
#@ Ravi Kumar;Jasmine Novak;Andrew Tomkins
#t 2006
#c 0
#% 233648
#% 281214
#% 283833
#% 300078
#% 309749
#% 593994
#% 754058
#% 756015
#% 786841
#% 797693
#% 823342
#! In this paper, we consider the evolution of structure within large online social networks. We present a series of measurements of two such networks, together comprising in excess of five million people and ten million friendship links, annotated with metadata capturing the time of every event in the life of the network. Our measurements expose a surprising segmentation of these networks into three regions: singletons who do not participate in the network; isolated communities which overwhelmingly display star structure; and a giant component anchored by a well-connected core region which persists even in the absence of stars.We present a simple model of network growth which captures these aspects of component structure. The model follows our experimental results, characterizing users as either passive members of the network; inviters who encourage offline friends and acquaintances to migrate online; and linkers who fully participate in the social evolution of the network.

#index 881524
#* Cryptographically private support vector machines
#@ Sven Laur;Helger Lipmaa;Taneli Mielikäinen
#t 2006
#c 0
#% 190581
#% 301584
#% 743280
#% 743284
#% 769962
#% 799761
#% 874166
#% 963800
#% 1386180
#% 1669943
#% 1721384
#% 1722551
#! We propose private protocols implementing the Kernel Adatron and Kernel Perceptron learning algorithms, give private classification protocols and private polynomial kernel computation protocols. The new protocols return their outputs - either the kernel value, the classifier or the classifications - in encrypted form so that they can be decrypted only by a common agreement by the protocol participants. We show how to use the encrypted classifications to privately estimate many properties of the data and the classifier. The new SVM classifiers are the first to be proven private according to the standard cryptographic definitions.

#index 881525
#* Bias and controversy: beyond the statistical deviation
#@ Hady W. Lauw;Ee-Peng Lim;Ke Wang
#t 2006
#c 0
#% 249110
#% 290830
#% 577291
#% 728195
#% 729923
#% 769887
#% 844334
#! In this paper, we investigate how deviation in evaluation activities may reveal bias on the part of reviewers and controversy on the part of evaluated objects. We focus on a 'data-centric approach' where the evaluation data is assumed to represent the 'ground truth'. The standard statistical approaches take evaluation and deviation at face value. We argue that attention should be paid to the subjectivity of evaluation, judging the evaluation score not just on 'what is being said' (deviation), but also on 'who says it' (reviewer) as well as on 'whom it is said about' (object). Furthermore, we observe that bias and controversy are mutually dependent, as there is more bias if there is higher deviation on a less controversial object. To address this mutual dependency, we propose a reinforcement model to identify bias and controversy. We test our model on real-life data to verify its applicability.

#index 881526
#* Sampling from large graphs
#@ Jure Leskovec;Christos Faloutsos
#t 2006
#c 0
#% 194127
#% 283833
#% 577219
#% 656281
#% 823342
#% 853533
#% 1717175
#! Given a huge real graph, how can we derive a representative sample? There are many known algorithms to compute interesting measures (shortest paths, centrality, betweenness, etc.), but several of them become impractical for large graphs. Thus graph sampling is essential.The natural questions to ask are (a) which sampling method to use, (b) how small can the sample size be, and (c) how to scale up the measurements of the sample (e.g., the diameter), to get estimates for the large graph. The deeper, underlying question is subtle: how do we measure success?.We answer the above questions, and test our answers by thorough experiments on several, diverse datasets, spanning thousands nodes and edges. We consider several sampling methods, propose novel methods to check the goodness of sampling, and develop a set of scaling laws that describe relations between the properties of the original and the sample.In addition to the theoretical contributions, the practical conclusions from our work are: Sampling strategies based on edge selection do not perform well; simple uniform random node selection performs surprisingly well. Overall, best performing methods are the ones based on random-walks and "forest fire"; they match very accurately both static as well as evolutionary graph patterns, with sample sizes down to about 15% of the original graph.

#index 881527
#* Clustering pair-wise dissimilarity data into partially ordered sets
#@ Jinze Liu;Qi Zhang;Wei Wang;Leonard McMillan;Jan Prins
#t 2006
#c 0
#% 36672
#% 322619
#% 447743
#% 519431
#% 729437
#% 820098
#% 833027
#% 839668
#! Ontologies represent data relationships as hierarchies of possibly overlapping classes. Ontologies are closely related to clustering hierarchies, and in this article we explore this relationship in depth. In particular, we examine the space of ontologies that can be generated by pairwise dissimilarity matrices. We demonstrate that classical clustering algorithms, which take dissimilarity matrices as inputs, do not incorporate all available information. In fact, only special types of dissimilarity matrices can be exactly preserved by previous clustering methods. We model ontologies as a partially ordered set (poset) over the subset relation. In this paper, we propose a new clustering algorithm, that generates a partially ordered set of clusters from a dissimilarity matrix.

#index 881528
#* Visual data mining using principled projection algorithms and information visualization techniques
#@ Dharmesh M. Maniyar;Ian T. Nabney
#t 2006
#c 0
#% 103743
#% 115608
#% 257039
#% 349210
#% 434613
#% 434616
#% 493076
#% 593047
#% 619859
#% 726032
#% 796203
#% 1711513
#% 1786334
#! We introduce a flexible visual data mining framework which combines advanced projection algorithms from the machine learning domain and visual techniques developed in the information visualization domain. The advantage of such an interface is that the user is directly involved in the data mining process. We integrate principled projection algorithms, such as generative topographic mapping (GTM) and hierarchical GTM (HGTM), with powerful visual techniques, such as magnification factors, directional curvatures, parallel coordinates and billboarding, to provide a visual data mining framework. Results on a real-life chemoinformatics dataset using GTM are promising and have been analytically compared with the results from the traditional projection methods. It is also shown that the HGTM algorithm provides additional value for large datasets. The computational complexity of these algorithms is discussed to demonstrate their suitability for the visual data mining framework.

#index 881529
#* A mixture model for contextual text mining
#@ Qiaozhu Mei;ChengXiang Zhai
#t 2006
#c 0
#% 280819
#% 293974
#% 577220
#% 722904
#% 729980
#% 769906
#% 769967
#% 779958
#% 783535
#% 818215
#% 823344
#% 823383
#% 869516
#! Contextual text mining is concerned with extracting topical themes from a text collection with context information (e.g., time and location) and comparing/analyzing the variations of themes over different contexts. Since the topics covered in a document are usually related to the context of the document, analyzing topical themes within context can potentially reveal many interesting theme patterns. In this paper, we generalize some of these models proposed in the previous work and we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases. Specifically, we extend the probabilistic latent semantic analysis (PLSA) model by introducing context variables to model the context of a document. The proposed mixture model, called contextual probabilistic latent semantic analysis (CPLSA) model, can be applied to many interesting mining tasks, such as temporal text mining, spatiotemporal text mining, author-topic analysis, and cross-collection comparative analysis. Empirical experiments show that the proposed mixture model can discover themes and their contextual variations effectively.

#index 881531
#* Efficient multidimensional data representations based on multiple correspondence analysis
#@ Riadh Ben Messaoud;Omar Boussaid;Sabine Loudcher Rabaséda
#t 2006
#c 0
#! In the On Line Analytical Processing (OLAP) context, exploration of huge and sparse data cubes is a tedious task which does not always lead to efficient results. In this paper, we couple OLAP with the Multiple Correspondence Analysis (MCA) in order to enhance visual representations of data cubes and thus, facilitate their interpretations and analysis. We also provide a quality criterion to measure the relevance of obtained representations. The criterion is based on a geometric neighborhood concept and a similarity metric between cells of a data cube. Experimental results on real data proved the interest and the efficiency of our approach.

#index 881532
#* Algorithms for time series knowledge mining
#@ Fabian Moerchen
#t 2006
#c 0
#% 313891
#% 319244
#% 549586
#% 729928
#% 745515
#% 789006
#% 796210
#% 823400
#% 950014
#% 1781027
#! Temporal patterns composed of symbolic intervals are commonly formulated with Allen's interval relations originating in temporal reasoning. This representation has severe disadvantages for knowledge discovery. The Time Series Knowledge Representation (TSKR) is a new hierarchical language for interval patterns expressing the temporal concepts of coincidence and partial order. We present effective and efficient mining algorithms for such patterns based on itemset techniques. A novel form of search space pruning effectively reduces the size of the mining result to ease interpretation and speed up the algorithms. On a real data set a concise set of TSKR patterns can explain the underlying temporal phenomena, whereas the patterns found with Allen's relations are far more numerous yet only explain fragments of the data.

#index 881533
#* Clustering based large margin classification: a scalable approach using SOCP formulation
#@ J. Saketha Nath;C. Bhattacharyya;M. N. Murty
#t 2006
#c 0
#% 210173
#% 269217
#% 269218
#% 466589
#% 722901
#% 729940
#% 793246
#! This paper presents a novel Second Order Cone Programming (SOCP) formulation for large scale binary classification tasks. Assuming that the class conditional densities are mixture distributions, where each component of the mixture has a spherical covariance, the second order statistics of the components can be estimated efficiently using clustering algorithms like BIRCH. For each cluster, the second order moments are used to derive a second order cone constraint via a Chebyshev-Cantelli inequality. This constraint ensures that any data point in the cluster is classified correctly with a high probability. This leads to a large margin SOCP formulation whose size depends on the number of clusters rather than the number of training data points. Hence, the proposed formulation scales well for large datasets when compared to the state-of-the-art classifiers, Support Vector Machines (SVMs). Experiments on real world and synthetic datasets show that the proposed algorithm outperforms SVM solvers in terms of training time and achieves similar accuracies.

#index 881534
#* Statistical entity-topic models
#@ David Newman;Chaitanya Chemudugunta;Padhraic Smyth
#t 2006
#c 0
#% 179800
#% 642990
#% 722904
#% 769906
#% 779875
#% 853534
#! The primary purpose of news articles is to convey information about who, what, when and where. But learning and summarizing these relationships for collections of thousands to millions of articles is difficult. While statistical topic models have been highly successful at topically summarizing huge collections of text documents, they do not explicitly address the textual interactions between who/where, i.e. named entities (persons, organizations, locations) and what, i.e. the topics. We present new graphical models that directly learn the relationship between topics discussed in news articles and entities mentioned in each article. We show how these entity-topic models, through a better understanding of the entity-topic relationships, are better at making predictions about entities.

#index 881535
#* Mining for misconfigured machines in grid systems
#@ Noam Palatin;Arie Leizarowitz;Assaf Schuster;Ran Wolff
#t 2006
#c 0
#% 478624
#% 781774
#% 820339
#% 822306
#% 884465
#! Grid systems are proving increasingly useful for managing the batch computing jobs of organizations. One well-known example is Intel, whose internally developed NetBatch system manages tens of thousands of machines. The size, heterogeneity, and complexity of grid systems make them very difficult, however, to configure. This often results in misconfigured machines, which may adversely affect the entire system.We investigate a distributed data mining approach for detection of misconfigured machines. Our Grid Monitoring System (GMS) non-intrusively collects data from all sources (log files, system services, etc.) available throughout the grid system. It converts raw data to semantically meaningful data and stores this data on the machine it was obtained from, limiting incurred overhead and allowing scalability. Afterwards, when analysis is requested, a distributed outliers detection algorithm is employed to identify misconfigured machines. The algorithm itself is implemented as a recursive workflow of grid jobs. It is especially suited to grid systems, in which the machines might be unavailable most of the time and often fail altogether.

#index 881536
#* Automatic mining of fruit fly embryo images
#@ Jia-Yu Pan;André G. R. Balan;Eric P. Xing;Agma Juci Machado Traina;Christos Faloutsos
#t 2006
#c 0
#% 376266
#% 420077
#% 742474
#% 1854550
#! We present FEMine, an automatic system for image-based gene expression analysis. We perform experiments on the largest publicly available collection of Drosophila ISH (in situ hybridization) images, showing that our FEMine system achieves excellent performance in classification, clustering, and content-based image retrieval. The major innovation of FEMine is the use of automatically discovered latent spatial "themes" of gene expressions, LGEs, in the whole-embryo context, as opposed to patterns in nearly disjoint portions of an embryo proposed in previous methods.

#index 881537
#* Naïve filterbots for robust cold-start recommendations
#@ Seung-Taek Park;David Pennock;Omid Madani;Nathan Good;Dennis DeCoste
#t 2006
#c 0
#% 173879
#% 202009
#% 202011
#% 220709
#% 220711
#% 266281
#% 280447
#% 280852
#% 283169
#% 330687
#% 342687
#% 342767
#% 420515
#% 465928
#% 495929
#% 528156
#% 528182
#% 578684
#% 734593
#% 734594
#% 766448
#% 766508
#% 840924
#% 979690
#% 1389375
#% 1393622
#% 1650569
#! The goal of a recommender system is to suggest items of interest to a user based on historical behavior of a community of users. Given detailed enough history, item-based collaborative filtering (CF) often performs as well or better than almost any other recommendation method. However, in cold-start situations - where a user, an item, or the entire system is new - simple non-personalized recommendations often fare better. We improve the scalability and performance of a previous approach to handling cold-start situations that uses filterbots, or surrogate users that rate items based only on user or item attributes. We show that introducing a very small number of simple filterbots helps make CF algorithms more robust. In particular, adding just seven global filterbots improves both user-based and item-based CF in cold-start user, cold-start item, and cold-start system settings. Performance is better when data is scarce, performance is no worse when data is plentiful, and algorithm efficiency is negligibly affected. We systematically compare a non-personalized baseline, user-based CF, item-based CF, and our bot-augmented user- and item-based CF algorithms using three data sets (Yahoo! Movies, MovieLens, and EachMovie) with the normalized MAE metric in three types of cold-start situations. The advantage of our "naïve filterbot" approach is most pronounced for the Yahoo! data, the sparsest of the three data sets.

#index 881538
#* MONIC: modeling and monitoring cluster transitions
#@ Myra Spiliopoulou;Irene Ntoutsi;Yannis Theodoridis;Rene Schult
#t 2006
#c 0
#% 273693
#% 771924
#% 799779
#% 800176
#% 823344
#% 823409
#% 926881
#% 1720762
#% 1728235
#! There is much recent work on detecting and tracking change in clusters, often based on the study of the spatiotemporal properties of a cluster. For the many applications where cluster change is relevant, among them customer relationship management, fraud detection and marketing, it is also necessary to provide insights about the nature of cluster change: Is a cluster corresponding to a group of customers simply disappearing or are its members migrating to other clusters? Is a new emerging cluster reflecting a new target group of customers or does it rather consist of existing customers whose preferences shift? To answer such questions, we propose the framework MONIC for modeling and tracking of cluster transitions. Our cluster transition model encompasses changes that involve more than one cluster, thus allowing for insights on cluster change in the whole clustering. Our transition tracking mechanism is not based on the topological properties of clusters, which are only available for some types of clustering, but on the contents of the underlying data stream. We present our first results on monitoring cluster transitions over the ACM digital library.

#index 881539
#* Combining linguistic and statistical analysis to extract relations from web documents
#@ Fabian M. Suchanek;Georgiana Ifrim;Gerhard Weikum
#t 2006
#c 0
#% 151291
#% 205078
#% 278109
#% 301241
#% 342630
#% 504443
#% 531458
#% 741673
#% 754068
#% 756964
#% 757422
#% 805872
#% 815307
#% 815868
#% 1290067
#% 1698594
#% 1712541
#! The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.

#index 881540
#* Mining long-term search history to improve search accuracy
#@ Bin Tan;Xuehua Shen;ChengXiang Zhai
#t 2006
#c 0
#% 248058
#% 340948
#% 577224
#% 590523
#% 723328
#% 731615
#% 754126
#% 818207
#% 818259
#% 818335
#% 838547
#! Long-term search history contains rich information about a user's search preferences, which can be used as search context to improve retrieval performance. In this paper, we study statistical language modeling based methods to mine contextual information from long-term search history and exploit it for a more accurate estimate of the query language model. Experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries. The best performance is achieved when using clickthrough data of past searches that are related to the current query.

#index 881541
#* Efficient kernel feature extraction for massive data sets
#@ Ivor W. Tsang;Andras Kocsor;James T. Kwok
#t 2006
#c 0
#% 269218
#% 466087
#% 840949
#% 843893
#% 1699590
#! Maximum margin discriminant analysis (MMDA) was proposed that uses the margin idea for feature extraction. It often outperforms traditional methods like kernel principal component analysis (KPCA) and kernel Fisher discriminant analysis (KFD). However, as in other kernel methods, its time complexity is cubic in the number of training points m, and is thus computationally inefficient on massive data sets. In this paper, we propose an (1+ε)2-approximation algorithm for obtaining the MMDA features by extending the core vector machines. The resultant time complexity is only linear in m, while its space complexity is independent of m. Extensive comparisons with the original MMDA, KPCA, and KFD on a number of large data sets show that the proposed feature extractor can improve classification accuracy, and is also faster than these kernel-based methods by more than an order of magnitude.

#index 881542
#* Summarizing itemset patterns using probabilistic models
#@ Chao Wang;Srinivasan Parthasarathy
#t 2006
#c 0
#% 237200
#% 252472
#% 300120
#% 478770
#% 481290
#% 629644
#% 678196
#% 727667
#% 769876
#% 823356
#% 824699
#% 841959
#! In this paper, we propose a novel probabilistic approach to summarize frequent itemset patterns. Such techniques are useful for summarization, post-processing, and end-user interpretation, particularly for problems where the resulting set of patterns are huge. In our approach items in the dataset are modeled as random variables. We then construct a Markov Random Fields (MRF) on these variables based on frequent itemsets and their occurrence statistics. The summarization proceeds in a level-wise iterative fashion. Occurrence statistics of itemsets at the lowest level are used to construct an initial MRF. Statistics of itemsets at the next level can then be inferred from the model. We use those patterns whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner, repeating the procedure until all frequent itemsets can be modeled. The resulting MRF model affords a concise and useful representation of the original collection of itemsets. Extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approaches.

#index 881543
#* Suppressing model overfitting in mining concept-drifting data streams
#@ Haixun Wang;Jian Yin;Jian Pei;Philip S. Yu;Jeffrey Xu Yu
#t 2006
#c 0
#% 95730
#% 273900
#% 310500
#% 333931
#% 342600
#% 342639
#% 378388
#% 397380
#% 424997
#% 594012
#% 729932
#% 785339
#% 823333
#% 823408
#% 844341
#% 993958
#! Mining data streams of changing class distributions is important for real-time business decision support. The stream classifier must evolve to reflect the current class distribution. This poses a serious challenge. On the one hand, relying on historical data may increase the chances of learning obsolete models. On the other hand, learning only from the latest data may lead to biased classifiers, as the latest data is often an unrepresentative sample of the current class distribution. The problem is particularly acute in classifying rare events, when, for example, instances of the rare class do not even show up in the most recent training data. In this paper, we use a stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one: from the historical and the current training data that we have observed, find the most-likely current distribution, and learn a classifier based on the most-likely distribution. We derive an analytic solution and approximate this solution with an efficient algorithm, which calibrates the influence of historical data carefully to create an accurate classifier. We evaluate our algorithm with both synthetic and real-world datasets. Our results show that our algorithm produces accurate and efficient classification.

#index 881544
#* A large-scale analysis of query logs for assessing personalization opportunities
#@ Steve Wedig;Omid Madani
#t 2006
#c 0
#% 234992
#% 296646
#% 309162
#% 320432
#% 323135
#% 342619
#% 344447
#% 413615
#% 438557
#% 590523
#% 766447
#% 823348
#% 829992
#% 832349
#% 872033
#! Query logs, the patterns of activity left by millions of users, contain a wealth of information that can be mined to aid personalization. We perform a large-scale study of Yahoo! search engine logs, tracking 1.35 million browser-cookies over a period of 6 months. We define metrics to address questions such as 1) How much history is available?, 2) How do users' topical interests vary, as reflected by their queries?, and 3) What can we learn from user clicks? We find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user. We show that users exhibit consistent topical interests that vary between users. We also see that user clicks indicate a variety of special interests. Our findings shed light on user activity and can inform future personalization efforts.

#index 881545
#* Semi-supervised time series classification
#@ Li Wei;Eamonn Keogh
#t 2006
#c 0
#% 252011
#% 311027
#% 458379
#% 572113
#% 577221
#% 784540
#% 844310
#% 844343
#% 853064
#% 1702634
#! The problem of time series classification has attracted great interest in the last decade. However current research assumes the existence of large amounts of labeled training data. In reality, such data may be very difficult or expensive to obtain. For example, it may require the time and expertise of cardiologists, space launch technicians, or other domain specialists. As in many other domains, there are often copious amounts of unlabeled data available. For example, the PhysioBank archive contains gigabytes of ECG data. In this work we propose a semi-supervised technique for building time series classifiers. While such algorithms are well known in text domains, we will show that special considerations must be made to make them both efficient and effective for the time series domain. We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms, handwritten documents, and video datasets. The experimental results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers.

#index 881546
#* (α, k)-anonymity: an enhanced k-anonymity model for privacy preserving data publishing
#@ Raymond Chi-Wing Wong;Jiuyong Li;Ada Wai-Chee Fu;Ke Wang
#t 2006
#c 0
#% 300184
#% 443463
#% 576762
#% 740764
#% 785363
#% 800514
#% 801690
#% 810011
#% 844340
#% 864412
#% 1700134
#! Privacy preservation is an important issue in the release of data for mining purposes. The k-anonymity model has been introduced for protecting individual identification. Recent studies show that a more sophisticated model is necessary to protect the association of individuals to sensitive information. In this paper, we propose an (α, k)-anonymity model to protect both identifications and relationships to sensitive information in data. We discuss the properties of (α, k)-anonymity model. We prove that the optimal (α, k)-anonymity problem is NP-hard. We first presentan optimal global-recoding method for the (α, k)-anonymity problem. Next we propose a local-recoding algorithm which is more scalable and result in less data distortion. The effectiveness and efficiency are shown by experiments. We also describe how the model can be extended to more general case.

#index 881547
#* Incremental approximate matrix factorization for speeding up support vector machines
#@ Gang Wu;Edward Chang;Yen Kuang Chen;Christoper Hughes
#t 2006
#c 0
#% 3084
#% 117447
#% 190581
#% 197394
#% 209021
#% 269217
#% 342598
#% 428583
#% 466263
#% 582065
#% 722815
#% 729940
#% 757953
#% 840839
#! Traditional decomposition-based solutions to Support Vector Machines (SVMs) suffer from the widely-known scalability problem. For example, given a one-million training set, it takes about six days for SVMLight to run on a Pentium-4 sever with 8G-byte memory. In this paper, we propose an incremental algorithm, which performs approximate matrix-factorization operations, to speed up SVMs. Two approximate factorization schemes, Kronecker and incomplete Cholesky, are utilized in the primal-dual interior-point method (IPM) to directly solve the quadratic optimization problem in SVMs. We found out that a coarse approximate algorithm enjoys good speedup performance but may suffer from poor training accuracy. Conversely, a fine-grained approximate algorithm enjoys good training quality but may suffer from long training time. We subsequently propose an incremental training algorithm, which uses the approximate IPM solution of a coarse factorization to initialize the IPM of a fine-grained factorization. Extensive empirical studies show that our proposed incremental algorithm with approximate factorizations substantially speeds up SVM training while maintaining high training accuracy. In addition, we show that our proposed algorithm is highly parallelizable on an Intel dual-coreprocessor.

#index 881548
#* Outlier detection by sampling with accuracy guarantees
#@ Mingxi Wu;Christopher Jermaine
#t 2006
#c 0
#% 116165
#% 209021
#% 235377
#% 235380
#% 300136
#% 300183
#% 466095
#% 466586
#% 466674
#% 479791
#% 633259
#% 770807
#% 823340
#! An effective approach to detecting anomalous points in a data setis distance-based outlier detection. This paper describes a simplesampling algorithm to effciently detect distance-based outliers indomains where each and every distance computation is veryexpensive. Unlike any existing algorithms, the sampling algorithmrequires a xed number of distance computations and can return goodresults with accuracy guarantees. The most computationallyexpensive aspect of estimating the accuracy of the result issorting all of the distances computed by the sampling algorithm.The experimental study on two expensive domains as well as tenadditional real-life datasets demonstrates both the effciency andeffectiveness of the sampling algorithm in comparison with thestate-of-the-art algorithm and there liability of the accuracyguarantees.

#index 881549
#* Discovering interesting patterns through user's interactive feedback
#@ Dong Xin;Xuehua Shen;Qiaozhu Mei;Jiawei Han
#t 2006
#c 0
#% 36672
#% 309208
#% 443092
#% 577224
#% 801616
#% 818209
#% 823336
#% 823360
#% 824710
#% 1272396
#! In this paper, we study the problem of discovering interesting patterns through user's interactive feedback. We assume a set of candidate patterns (ie, frequent patterns) has already been mined. Our goal is to help a particular user effectively discover interesting patterns according to his specific interest. Without requiring a user to explicitly construct a prior knowledge to measure the interestingness of patterns, we learn the user's prior knowledge from his interactive feedback. We propose two models to represent a user's prior: the log linear model and biased belief model. The former is designed for item-set patterns, whereas the latter is also applicable to sequential and structural patterns. To learn these models, we present a two-stage approach, progressive shrinking and clustering, to select sample patterns for feedback. The experimental results on real and synthetic data sets demonstrate the effectiveness of our approach.

#index 881550
#* K-means clustering versus validation measures: a data distribution perspective
#@ Hui Xiong;Junjie Wu;Jian Chen
#t 2006
#c 0
#% 36672
#% 169777
#% 248790
#% 252836
#% 438137
#% 755463
#% 835018
#! K-means is a widely used partitional clustering method. While there are considerable research efforts to characterize the key features of K-means clustering, further investigation is needed to reveal whether and how the data distributions can have the impact on the performance of K-means clustering. Indeed, in this paper, we revisit the K-means clustering problem by answering three questions. First, how the "true" cluster sizes can make impact on the performance of K-means clustering? Second, is the entropy an algorithm-independent validation measure for K-means clustering? Finally, what is the distribution of the clustering results by K-means? To that end, we first illustrate that K-means tends to generate the clusters with the relatively uniform distribution on the cluster sizes. In addition, we show that the entropy measure, an external clustering validation measure, has the favorite on the clustering algorithms which tend to reduce high variation on the cluster sizes. Finally, our experimental results indicate that K-means tends to produce the clusters in which the variation of the cluster sizes, as measured by the Coefficient of Variation(CV), is in a specific range, approximately from 0.3 to 1.0.

#index 881551
#* Utility-based anonymization using local recoding
#@ Jian Xu;Wei Wang;Jian Pei;Xiaoyuan Wang;Baile Shi;Ada Wai-Chee Fu
#t 2006
#c 0
#% 248030
#% 443463
#% 576761
#% 576762
#% 577239
#% 800515
#% 801690
#% 810011
#% 824726
#% 864406
#% 1700134
#! Privacy becomes a more and more serious concern in applications involving microdata. Recently, efficient anonymization has attracted much research work. Most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. However, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. Moreover, anonymized data is often for analysis. As well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. The utility of attributes has not been considered in the previous methods.In this paper, we study the problem of utility-based anonymization. First, we propose a simple framework to specify utility of attributes. The framework covers both numeric and categorical data. Second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. Furthermore, our utility-based method can boost the quality of analysis using the anonymized data.

#index 881552
#* Integration of semantic-based bipartite graph representation and mutual refinement strategy for biomedical literature clustering
#@ Illhoi Yoo;Xiaohua Hu;Il-Yeol Song
#t 2006
#c 0
#% 46809
#% 118771
#% 169784
#% 218992
#% 228097
#% 262045
#% 273891
#% 280404
#% 309128
#% 375017
#% 397137
#% 465747
#% 465914
#% 577257
#% 591613
#% 748499
#% 766432
#% 874491
#! We introduce a novel document clustering approach that overcomes those problems by combining a semantic-based bipartite graph representation and a mutual refinement strategy. The primary contributions of this paper are the following. First, we introduce a new representation of documents using a bipartite graph between documents and co-occurrence concepts in the documents. Second, we show how to enhance clustering quality by applying the mutual refinement strategy to the initial clustering results. Third, through the experiments on MEDLINE documents, we show that our integrated method significantly enhances cluster quality and clustering reliability compared to existing clustering methods. Our approach improves on the average 29.5 cluster quality and 26.3 clustering reliability, in terms of misclassification index, over Bisecting K-means with the best parameters.

#index 881553
#* Coherent closed quasi-clique discovery from large dense graph databases
#@ Zhiping Zeng;Jianyong Wang;Lizhu Zhou;George Karypis
#t 2006
#c 0
#% 481290
#% 823347
#% 823357
#% 833120
#% 864460
#! Frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database, and mining frequently occurring coherent subgraphs from large dense graph databases has been witnessed several applications and received considerable attention in the graph mining community recently. In this paper, we study how to efficiently mine the complete set of coherent closed quasi-cliques from large dense graph databases, which is an especially challenging task due to the downward-closure property no longer holds. By fully exploring some properties of quasi-cliques, we propose several novel optimization techniques, which can prune the unpromising and redundant sub-search spaces effectively. Meanwhile, we devise an efficient closure checking scheme to facilitate the discovery of only closed quasi-cliques. We also develop a coherent closed quasi-clique mining algorithm, Cocain1 Thorough performance study shows that Cocain is very efficient and scalable for large dense graph databases.

#index 881554
#* Mining progressive confident rules
#@ Minghua Zhang;Wynne Hsu;Mong Li Lee
#t 2006
#c 0
#% 152934
#% 259993
#% 413550
#% 459006
#% 463903
#% 464996
#% 479971
#! Many real world objects have states that change over time. By tracking the state sequences of these objects, we can study their behavior and take preventive measures before they reach some undesirable states. In this paper, we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state. We give a formal definition of progressive confident rules and their concise set. We devise pruning strategies to reduce the enormous search space. Experiment result shows that the proposed algorithm is efficient and scalable. We also demonstrate the application of progressive confident rules in classification.

#index 881555
#* Attack detection in time series for recommender systems
#@ Sheng Zhang;Amit Chakrabarti;James Ford;Fillia Makedon
#t 2006
#c 0
#% 378367
#% 754097
#% 783438
#% 793805
#% 821933
#% 836153
#% 844310
#% 844357
#% 879629
#% 1728733
#! Recent research has identified significant vulnerabilities in recommender systems. Shilling attacks, in which attackers introduce biased ratings in order to influence future recommendations, have been shown to be effective against collaborative filtering algorithms. We postulate that the distribution of item ratings in time can reveal the presence of a wide range of shilling attacks given reasonable assumptions about their duration. To construct a time series of ratings for an item, we use a window size of k to group consecutive ratings for the item into disjoint windows and compute the sample average and sample entropy in each window. We derive a theoretically optimal window size to best detect an attack event if the number of attack profiles is known. For practical applications where this number is unknown, we propose a heuristic algorithm that adaptively changes the window size. Our experimental results demonstrate that monitoring rating distributions in time series is an effective approach for detecting shilling attacks.

#index 881556
#* Identifying bridging rules between conceptual clusters
#@ Shichao Zhang;Feng Chen;Xindong Wu;Chengqi Zhang
#t 2006
#c 0
#% 227919
#% 300136
#% 300163
#% 316709
#% 420126
#% 438137
#% 443313
#% 459025
#% 464603
#% 479791
#! A bridging rule in this paper has its antecedent and action from different conceptual clusters. We first design two algorithms for mining bridging rules between clusters in a database, and then propose two non-linear metrics for measuring the interestingness of bridging rules. Bridging rules can be distinct from association rules (or frequent itemsets). This is because (1) bridging rules can be generated by infrequent itemsets that are pruned in association rule mining; and (2) bridging rules are measured by the importance that includes the distance between two conceptual clusters, whereas frequent itemsets are measured by only the support.

#index 881557
#* Linear prediction models with graph regularization for web-page categorization
#@ Tong Zhang;Alexandrin Popescul;Byron Dom
#t 2006
#c 0
#% 248810
#% 266215
#% 318412
#% 333797
#% 420495
#% 420507
#% 722914
#% 765552
#% 769942
#% 769954
#! We present a risk minimization formulation for learning from both text and graph structures which is motivated by the problem of collective inference for hypertext document categorization. The method is based on graph regularization formulated as a well-formed convex optimization problem. We present numerical algorithms for our formulation, and show that such combination of local text features and link information can lead to improved predictive accuracy.

#index 881558
#* BLOSOM: a framework for mining arbitrary boolean expressions
#@ Lizhuang Zhao;Mohammed J. Zaki;Naren Ramakrishnan
#t 2006
#c 0
#% 232136
#% 282910
#% 338594
#% 342717
#% 384416
#% 464822
#% 765529
#% 767654
#% 769902
#% 796210
#% 799740
#% 823361
#! We introduce a novel framework, called BLOSOM, for mining (frequent) boolean expressions over binary-valued datasets. We organize the space of boolean expressions into four categories: pure conjunctions, pure disjunctions, conjunction of disjunctions, and disjunction of conjunctions. We focus on mining the simplest expressions the minimal generators for each class. We also propose a closure operator for each class that yields closed boolean expressions. BLOSOM efficiently mines frequent boolean expressions by utilizing a number of methodical pruning techniques. Experiments showcase the behavior of BLOSOM, and an application study on a real dataset is also given.

#index 881559
#* Introducing perpetual analytics
#@ Jeff Jonas
#t 2006
#c 0
#! Common strategies to liberate an organization's information assets for situational awareness frequently rely on infrastructure components such as data integration, enterprise search, federation, data warehousing, and so on. And while these traditional platforms enable analysts to get better and faster answers to their queries, the next big advance will change this paradigm. Users cannot be expected to formulate and ask every smart question every day. And to escape this impractical and un-scalable model, the new paradigm will involve technologies where "the data finds the data" and "relevance finds the user." Perpetual Analytics describes a class of application whereby enterprise context is assembled, in real-time, on data streams as fast as operational systems record observations. Context construction is a "data finds the data" activity which enables events of interest to be streamed to subscribers. In this talk, I will talk at some depth about the dynamics of such systems including scalability and sustainability.

#index 881560
#* Capital One's statistical problems: our top ten list
#@ William Kahn
#t 2006
#c 0
#! Capital One is a highly quantitatively driven diversified financial services firm. As such, we make broad and deep use of the entire repertory of highly quantitative techniques. This talk will present our top ten statistical problems. Indeed, one of them has, as a sub point, the data mining dimension, but it will likely be useful for data miners to see how their research needs to complement, and fit into, the entire range of hard statistical issues.

#index 881561
#* Information extraction, data mining and joint inference
#@ Andrew McCallum
#t 2006
#c 0
#! Although information extraction and data mining appear together in many applications, their interface in most current systems would better be described as serial juxtaposition than as tight integration. Information extraction populates slots in a database by identifying relevant subsequences of text, but is usually not aware of the emerging patterns and regularities in the database. Data mining methods begin from a populated database, and are often unaware of where the data came from, or its inherent uncertainties. The result is that the accuracy of both suffers, and accurate mining of complex text sources has been beyond reach.In this talk I will describe work in probabilistic models that perform joint inference across multiple components of an information processing pipeline in order to avoid the brittle accumulation of errors. After briefly introducing conditional random fields, I will describe recent work in information extraction leveraging factorial state representations, entity resolution, and transfer learning, as well as scalable methods of inference and learning. I'll close with some recent work on probabilistic models for social network analysis, and a demonstration of Rexa.info, a new research paper search engine.

#index 881562
#* Data mining challenges in the automotive domain
#@ Michael Cavaretta
#t 2006
#c 0
#! Automotive companies, such as Ford Motor Company, have no shortage of large databases with abundant opportunities for cost reduction and revenue enhancement. The Data Mining Group at Ford has worked in the areas of Quality, Customer Satisfaction and Warranty Analytics for close to ten years. In this time, we have developed a number of methods for building systems to help the business. One area of particular success has been in warranty analysis. While traditional hazard analysis has been applied at Ford for a number of years, we have used techniques from other industries (e.g. retail), as well as text mining to view warranty analytics in a new way. However, our success has been tempered by serious challenges particularly in the areas of data understanding, computing meaningful aggregations and implementation. Case studies from the automobile industry (warranty, quality, forecasting, etc.) as well as from other industries will be used.

#index 881563
#* Computer aided detection via asymmetric cascade of sparse hyperplane classifiers
#@ Jinbo Bi;Senthil Periaswamy;Kazunori Okada;Toshiro Kubota;Glenn Fung;Marcos Salganicoff;R. Bharat Rao
#t 2006
#c 0
#% 214401
#% 269225
#% 302391
#% 565538
#% 656642
#% 736300
#% 769930
#% 784883
#% 836706
#% 1113096
#% 1391749
#% 1704686
#! This paper describes a novel classification method for computer aided detection (CAD) that identifies structures of interest from medical images. CAD problems are challenging largely due to the following three characteristics. Typical CAD training data sets are large and extremely unbalanced between positive and negative classes. When searching for descriptive features, researchers often deploy a large set of experimental features, which consequently introduces irrelevant and redundant features. Finally, a CAD system has to satisfy stringent real-time requirements.This work is distinguished by three key contributions. The first is a cascade classification approach which is able to tackle all the above difficulties in a unified framework by employing an asymmetric cascade of sparse classifiers each trained to achieve high detection sensitivity and satisfactory false positive rates. The second is the incorporation of feature computational costs in a linear program formulation that allows the feature selection process to take into account different evaluation costs of various features. The third is a boosting algorithm derived from column generation optimization to effectively solve the proposed cascade linear programs.We apply the proposed approach to the problem of detecting lung nodules from helical multi-slice CT images. Our approach demonstrates superior performance in comparison against support vector machines, linear discriminant analysis and cascade AdaBoost. Especially, the resulting detection system is significantly sped up with our approach.

#index 881564
#* Onboard classifiers for science event detection on a remote sensing spacecraft
#@ Rebecca Castano;Dominic Mazzoni;Nghia Tang;Ron Greeley;Thomas Doggett;Ben Cichy;Steve Chien;Ashley Davies
#t 2006
#c 0
#% 136350
#% 197394
#! Typically, data collected by a spacecraft is downlinked to Earth and preprocessed before any analysis is performed. We have developed classifiers that can be used onboard a spacecraft to identify high priority data for downlink to Earth, providing a method for maximizing the use of a potentially bandwidth limited downlink channel. Onboard analysis can also enable rapid reaction to dynamic events, such as flooding, volcanic eruptions or sea ice break-up.Four classifiers were developed to identify cryosphere events using hyperspectral images. These classifiers include a manually constructed classifier, a Support Vector Machine (SVM), a Decision Tree and a classifier derived by searching over combinations of thresholded band ratios. Each of the classifiers was designed to run in the computationally constrained operating environment of the spacecraft. A set of scenes was hand-labeled to provide training and testing data. Performance results on the test data indicate that the SVM and manual classifiers outperformed the Decision Tree and band-ratio classifiers with the SVM yielding slightly better classifications than the manual classifier.The manual and SVM classifiers have been uploaded to the EO-1 spacecraft and have been running onboard the spacecraft for over a year. Results of the onboard analysis are used by the Autonomous Sciencecraft Experiment (ASE) of NASA's New Millennium Program onboard EO-1 to automatically target the spacecraft to collect follow-on imagery. The software demonstrates the potential for future deep space missions to use onboard decision making to capture short-lived science events.

#index 881565
#* Pragmatic text mining: minimizing human effort to quantify many issues in call logs
#@ George Forman;Evan Kirshenbaum;Jaap Suermondt
#t 2006
#c 0
#% 434614
#% 458379
#% 577257
#% 722935
#% 766426
#% 770807
#% 799043
#% 799751
#% 823344
#% 823379
#% 881471
#% 1699624
#! We discuss our experiences in analyzing customer-support issues from the unstructured free-text fields of technical-support call logs. The identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type, to appropriately target engineering resources, and to provide the best diagnosis, support and documentation for most common issues. We present a new set of techniques for doing this efficiently on an industrial scale, without requiring manual coding of calls in the call center. Our approach involves (1) a new text clustering method to identify common and emerging issues; (2) a method to rapidly train large numbers of categorizers in a practical, interactive manner; and (3) a method to accurately quantify categories, even in the face of inaccurate classifications and training sets that necessarily cannot match the class distribution of each new month's data. We present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at HP.

#index 881566
#* Mining for proposal reviewers: lessons learned at the national science foundation
#@ Seth Hettich;Michael J. Pazzani
#t 2006
#c 0
#% 27049
#% 46809
#% 118762
#% 243752
#% 249143
#% 262112
#% 271083
#% 280404
#% 406493
#% 769906
#% 874462
#! In this paper, we discuss a prototype application deployed at the U.S. National Science Foundation for assisting program directors in identifying reviewers for proposals. The application helps program directors sort proposals into panels and find reviewers for proposals. To accomplish these tasks, it extracts information from the full text of proposals both to learn about the topics of proposals and the expertise of reviewers. We discuss a variety of alternatives that were explored, the solution that was implemented, and the experience in using the solution within the workflow of NSF.

#index 881567
#* GPLAG: detection of software plagiarism by program dependence graph analysis
#@ Chao Liu;Chen Chen;Jiawei Han;Philip S. Yu
#t 2006
#c 0
#% 19622
#% 288990
#% 408396
#% 448355
#% 600046
#% 622212
#% 622419
#% 625932
#% 632029
#% 637298
#% 654447
#% 729938
#% 823218
#! Along with the blossom of open source projects comes the convenience for software plagiarism. A company, if less self-disciplined, may be tempted to plagiarize some open source projects for its own products. Although current plagiarism detection tools appear sufficient for academic use, they are nevertheless short for fighting against serious plagiarists. For example, disguises like statement reordering and code insertion can effectively confuse these tools. In this paper, we develop a new plagiarism detection tool, called GPLAG, which detects plagiarism by mining program dependence graphs (PDGs). A PDG is a graphic representation of the data and control dependencies within a procedure. Because PDGs are nearly invariant during plagiarism, GPLAG is more effective than state-of-the-art tools for plagiarism detection. In order to make GPLAG scalable to large programs, a statistical lossy filter is proposed to prune the plagiarism search space. Experiment study shows that GPLAG is both effective and efficient: It detects plagiarism that easily slips over existing tools, and it usually takes a few seconds to find (simulated) plagiarism in programs having thousands of lines of code.

#index 881568
#* Understandable models Of music collections based on exhaustive feature generation with temporal statistics
#@ Fabian Moerchen;Ingo Mierswa;Alfred Ultsch
#t 2006
#c 0
#% 132938
#% 136350
#% 137711
#% 260645
#% 269218
#% 334756
#% 451656
#% 643010
#% 723542
#% 741122
#% 741169
#% 782526
#% 799392
#% 899301
#% 1767420
#% 1861150
#! Data mining in large collections of polyphonic music has recently received increasing interest by companies along with the advent of commercial online distribution of music. Important applications include the categorization of songs into genres and the recommendation of songs according to musical similarity and the customer's musical preferences. Modeling genre or timbre of polyphonic music is at the core of these tasks and has been recognized as a difficult problem. Many audio features have been proposed, but they do not provide easily understandable descriptions of music. They do not explain why a genre was chosen or in which way one song is similar to another. We present an approach that combines large scale feature generation with meta learning techniques to obtain meaningful features for musical similarity. We perform exhaustive feature generation based on temporal statistics and train regression models to summarize a subset of these features into a single descriptor of a particular notion of music. Using several such models we produce a concise semantic description of each song. Genre classification models based on these semantic features are shown to be better understandable and almost as accurate as traditional methods.

#index 881569
#* Opportunity map: identifying causes of failure - a deployed data mining system
#@ Kaidi Zhao;Bing Liu;Jeffrey Benkler;Weimin Xiao
#t 2006
#c 0
#% 136350
#% 172386
#% 190581
#% 252367
#% 280436
#% 280487
#% 389169
#% 417589
#% 420101
#% 443313
#% 481290
#% 481954
#% 501204
#% 577214
#% 577216
#% 577252
#% 729934
#% 769893
#% 769926
#% 844348
#! In this paper, we report a deployed data mining application system for Motorola. Originally, its intended use was for identifying causes of cellular phone failures, but it has been found to be useful for many other engineering data sets as well. For this report, the case study is a dataset containing cellular phone call records. This data set is like any dataset used in classification applications, i.e., with a set of attributes which can be continuous or discrete, and a discrete class attribute. In our application, the classes are normally ended calls, calls which failed to setup, and calls which failed while in progress. However, the task is not to predict any failure, but to identify possible causes that resulted in failures. Then, engineering efforts may focus on improvements that can be made to the phones. In the course of the project, various classification techniques, e.g., decision trees, naïve Bayesian classification and SVM were tried. However, the results were unsatisfactory. After several demonstrations and interaction with domain experts, we finally designed and implemented an effective approach to perform the task. The final system is based on class association rules, general impressions and visualization. The system has been deployed and is in regular use at Motorola. In this paper, we first describe our experiences with some existing classification systems and discuss why they are not suitable for the task. We then present our techniques. As an illustration, we show several visualization screens in the case study, which reveal some important knowledge. Due to confidentiality, we will not give specifics but only present a general discussion about the results.

#index 881570
#* Identifying "best bet" web search results by mining past user behavior
#@ Eugene Agichtein;Zijian Zheng
#t 2006
#c 0
#% 169803
#% 268079
#% 387427
#% 406493
#% 433965
#% 577224
#% 590523
#% 731615
#% 754059
#% 783482
#% 805200
#% 805878
#% 818207
#% 818209
#% 818221
#% 823348
#% 840846
#% 879565
#% 879567
#! The top web search result is crucial for user satisfaction with the web search experience. We argue that the importance of the relevance at the top position necessitates special handling of the top web search result for some queries. We propose an effective approach of leveraging millions of past user interactions with a web search engine to automatically detect "best bet" top results preferred by majority of users. Interestingly, this problem can be more effectively addressed with classification than using state-of-the-art general ranking methods. Furthermore, we show that our general machine learning approach achieves precision comparable to a heavily tuned, domain-specific algorithm, with significantly higher coverage. Our experiments over millions of user interactions for thousands of queries demonstrate the effectiveness and robustness of our techniques.

#index 881571
#* Mining citizen science data to predict orevalence of wild bird species
#@ Rich Caruana;Mohamed Elhawary;Art Munson;Mirek Riedewald;Daria Sorokina;Daniel Fink;Wesley M. Hochachka;Steve Kelling
#t 2006
#c 0
#% 209021
#% 331909
#% 424997
#% 722929
#% 840913
#! The Cornell Laboratory of Ornithology's mission is to interpret and conserve the earth's biological diversity through research, education, and citizen science focused on birds. Over the years, the Lab has accumulated one of the largest and longest-running collections of environmental data sets in existence. The data sets are not only large, but also have many attributes, contain many missing values, and potentially are very noisy. The ecologists are interested in identifying which features have the strongest effect on the distribution and abundance of bird species as well as describing the forms of these relationships. We show how data mining can be successfully applied, enabling the ecologists to discover unanticipated relationships. We compare a variety of methods for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial results for the impact of important attributes on bird prevalence.

#index 881572
#* A component-based framework for knowledge discovery in bioinformatics
#@ Julien Etienne;Bernd Wachmann;Lei Zhang
#t 2006
#c 0
#% 168251
#% 314388
#% 314392
#% 414108
#% 427307
#% 833763
#! Motivation: In the field of bioinformatics there is an emerging need to integrate all knowledge discovery steps into a standardized modular framework. Indeed, component-based development can significantly enhance reusability and productivity for short timeline projects with a small team. We present Interactive Knowledge Discovery and Data mining (iKDD), an application framework written in Java that was specifically designed for these purposes.Results: iKDD consists of a component-based architecture and a web-based tool for pre-clinical research and prototype development. The platform provides an intuitive and consistent interface to create and maintain components, e.g., data structures, algorithms and utilities to load, save and visualize data and pipelines. The rich-featured tool supplies database connectivity, workflow processing and rapid prototype building. The architecture was carefully designed using an object-oriented approach that respects crucial goals: usability, openness, robustness and functionality, especially in the abstraction and description of the components, which distinguishes it from other packages. iKDD is well-suited to serve as a public repository of components, to run scientific experiments with a high-level of reproducibility, and also to rapidly build prototypes. This paper describes the general architecture, and demonstrates through examples the ease by which a complex scenario implementation can be facilitated with iKDD.

#index 881573
#* Discovering significant OPSM subspace clusters in massive gene expression data
#@ Byron J. Gao;Obi L. Griffith;Martin Ester;Steven J. M. Jones
#t 2006
#c 0
#% 248792
#% 397382
#% 459006
#% 463903
#% 469422
#% 727882
#% 778732
#! Order-preserving submatrixes (OPSMs) have been accepted as a biologically meaningful subspace cluster model, capturing the general tendency of gene expressions across a subset of conditions. In an OPSM, the expression levels of all genes induce the same linear ordering of the conditions. OPSM mining is reducible to a special case of the sequential pattern mining problem, in which a pattern and its supporting sequences uniquely specify an OPSM cluster. Those small twig clusters, specified by long patterns with naturally low support, incur explosive computational costs and would be completely pruned off by most existing methods for massive datasets containing thousands of conditions and hundreds of thousands of genes, which are common in today's gene expression analysis. However, it is in particular interest of biologists to reveal such small groups of genes that are tightly coregulated under many conditions, and some pathways or processes might require only two genes to act in concert. In this paper, we introduce the KiWi mining framework for massive datasets, that exploits two parameters k and w to provide a biased testing on a bounded number of candidates, substantially reducing the search space and problem scale, targeting on highly promising seeds that lead to significant clusters and twig clusters. Extensive biological and computational evaluations on real datasets demonstrate that KiWi can effectively mine biologically meaningful OPSM subspace clusters with good efficiency and scalability.

#index 881574
#* Maximum profit mining and its application in software development
#@ Charles X. Ling;Victor S. Sheng;Tilmann Bruckhaus;Nazim H. Madhavji
#t 2006
#c 0
#% 136350
#% 209021
#% 232102
#% 280437
#% 290482
#% 374518
#% 393792
#% 438360
#% 443509
#% 565525
#% 727925
#% 770791
#% 844392
#% 1272000
#% 1272369
#% 1289281
#! While most software defects (i.e., bugs) are corrected and tested as part of the lengthy software development cycle, enterprise software vendors often have to release software products before all reported defects are corrected, due to deadlines and limited resources. A small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost. In this paper, we develop an Escalation Prediction (EP) system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit. More specifically, we first describe a simple and general framework to convert the maximum net profit problem to cost-sensitive learning. We then apply and compare several well-known cost-sensitive learning approaches for EP. Our experiments suggest that the cost-sensitive decision tree is the best method for producing the highest positive net profit and comprehensible results. The EP system has been deployed successfully in the product group of an enterprise software vendor.

#index 881575
#* YALE: rapid prototyping for complex data mining tasks
#@ Ingo Mierswa;Michael Wurst;Ralf Klinkenberg;Martin Scholz;Timm Euler
#t 2006
#c 0
#% 46803
#% 290482
#% 321635
#% 466408
#% 645453
#% 742430
#% 799392
#% 822369
#% 823351
#% 876311
#% 998561
#% 1055665
#! KDD is a complex and demanding task. While a large number of methods has been established for numerous problems, many challenges remain to be solved. New tasks emerge requiring the development of new methods or processing schemes. Like in software development, the development of such solutions demands for careful analysis, specification, implementation, and testing. Rapid prototyping is an approach which allows crucial design decisions as early as possible. A rapid prototyping system should support maximal re-use and innovative combinations of existing methods, as well as simple and quick integration of new ones.This paper describes Yale, a free open-source environment forKDD and machine learning. Yale provides a rich variety of methods whichallows rapid prototyping for new applications and makes costlyre-implementations unnecessary. Additionally, Yale offers extensive functionality for process evaluation and optimization which is a crucial property for any KDD rapid prototyping tool. Following the paradigm of visual programming eases the design of processing schemes. While the graphical user interface supports interactive design, the underlying XML representation enables automated applications after the prototyping phase.After a discussion of the key concepts of Yale, we illustrate the advantages of rapid prototyping for KDD on case studies ranging from data pre-processing to result visualization. These case studies cover tasks like feature engineering, text mining, data stream mining and tracking drifting concepts, ensemble methods and distributed data mining. This variety of applications is also reflected in a broad user base, we counted more than 40,000 downloads during the last twelve months.

#index 881576
#* Camouflaged fraud detection in domains with complex relationships
#@ Sankar Virdhagriswaran;Gordon Dakin
#t 2006
#c 0
#% 708712
#% 727847
#% 765134
#% 845221
#! We describe a data mining system to detect frauds that are camouflaged to look like normal activities in domains with high number of known relationships. Examples include accounting fraud detection for rating and investment, insider attacks on corporate networks, and health care insurance fraud. Our goal is to help analysts who are overwhelmed with information about companies or on-line system access logs or insurance claims to focus their attentions on features that cause damage in the future. We focused on accounting fraud where the task is to detect the subset of companies that were potentially committing accounting fraud within the total population of public companies that file quarterly and annual filings with the Securities and Exchange Commission (SEC). Using (a) Representation of changes, (b) A mix of decision tree learning, locally weighted logistic regression, k-means clustering, and constant regression in a two phase pipe line, we developed models that rank companies based on the probability of forecasting future damaging performance. The learned models were tested extensively over four years with public data available from SEC filings and private data available from rating companies and investment firms. Cross validation experiments and analyst based validation of private experiments were found to show that the approach performed as well as or better than domain experts and discovered new relationships that domain experts did not use on a regular basis. Finally, the detections preceded public knowledge of such problems by six to eighteen months.

#index 881577
#* Beyond classification and ranking: constrained optimization of the ROI
#@ Lian Yan;Patrick Baldasare
#t 2006
#c 0
#% 73441
#% 744737
#% 823376
#% 840846
#% 1289281
#! Classification has been commonly used in many data mining projects in the financial service industry. For instance, to predict collectability of accounts receivable, a binary class label is created based on whether a payment is received within a certain period. However, optimization of the classifier does not necessarily lead to maximization of return on investment (ROI), since maximization of the true positive rate is often different from maximization of the collectable amount which determines the ROI under a fixed budget constraint. The typical cost sensitive learning does not solve this problem either since it involves an unknown opportunity cost due to the budget constraint. Learning the ranks of collectable amount would ultimately solve the problem, but it tries to tackle an unnecessarily difficult problem and often results in poorer results for our specific target. We propose a new algorithm that uses gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the ROI. By comparison with several classification, regression, and ranking algorithms, we demonstrate the new algorithm's substantial improvement of the financial impact on our clients in the financial service industry.

#index 881578
#* Is there a grand challenge or X-prize for data mining?
#@ Gregory Piatetsky-Shapiro;Robert Grossman;Chabane Djeraba;Ronen Feldman;Lise Getoor;Mohammed Zaki
#t 2006
#c 0
#! This panel will discuss possible exciting and motivating Grand Challenge problems for Data Mining, focusing on bioinformatics, multimedia mining, link mining, text mining, and web mining.

#index 989567
#* Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Pavel Berkhin;Rich Caruana;Xindong Wu
#t 2007
#c 0
#! This proceedings is the published record of the Thirteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-07) held in San Jose, California on August 12--15, 2007. The KDD-07 conference provides a forum for novel research results and important applications in the area of data mining and knowledge discovery. The vibrancy, excitement and breadth of the field are reflected by the strong lineup of research papers, invited talks, tutorials and workshops at the conference. KDD-07 received a record number of 513 Research Track submissions from 36 different countries. From these, the program committee accepted 92 papers for oral presentation at the conference. This year all accepted research-track papers are full papers that are giving both oral and poster presentations. KDD-07 also received 60 Industrial and Governmental Track submissions from 16 different countries. Of these, the Industry Track program committee accepted 11 regular papers and 8 short papers for presentation at the conference. As with the Research Track, all accepted Industrial and Governmental Track papers are giving both oral and poster presentations. In addition to the paper presentations, the conference also featured seven tutorials, twelve workshops, one panel, several Birds-of-a-Feather sessions (a new feature this year), the KDD-Cup Competition, a demo session, and invited talks by Chris Anderson (Wired Magazine), Usama Fayyad (Yahoo!), and Jon Kleinberg (Cornell). The Industrial and Government Applications track included invited presentations by Joshua Goodman (Microsoft) and Bharat Rao (Siemens).

#index 989568
#* Calculating latent demand in the long tail
#@ Chris Anderson
#t 2007
#c 0
#! An analytical framework for using powerlaw theory to estimate market size for niche products and consumer groups.

#index 989569
#* From mining the web to inventing the new sciences underlying the internet
#@ Usama M. Fayyad
#t 2007
#c 0
#! This is an abstract of the Invited Keynote Presentation to be presented at KDD-07. As the Internet continues to change the way we live, find information, communicate, and do business, it has also been taking on a dramatically increasing role in marketing and advertising. Unlike any prior mass medium, the Internet is a unique medium when it comes to interactivity and offers an ability to target and program messaging at the individual level. Coupled with its uniqueness in the richness of the data that is available for measurability, in the variety of ways to utilize the data, and in the great dependence of effective marketing on applications that are heavily data-driven, makes data mining and statistical data analysis, modeling, and reporting an essential mission-critical part of running the on-line business. However, because of its novelty and the scale of data sets involved, few companies have figured out how to properly make use of this data. In this talk, I will review some of the challenges and opportunities in the utilization of data to drive this new generation of marketing systems. I will provide several examples of how data is utilized in critical ways to drive some of these capabilities. The discussion will be framed with the More general framework of Grand Challenges for data mining: pragmatic and technical. I will conclude this presentation witha consideration of the larger issues surrounding the Internet as a technology that is ubiquitous in our lives, yet one where very little is understood, at the scientific level, in defining and understanding many of the basics the Internet enables: Community, Personalization, and the new Microeconomics of the web. This leads to an overview of the new Yahoo! Research organization and its aims: inventing the new sciences underlying what we do on the Internet, focusing on areas that have received little attention in the traditional academic circles. Some illustrative examples will be reviewed to make the ultimate goals more concrete.

#index 989570
#* Challenges in mining social network data: processes, privacy, and paradoxes
#@ Jon M. Kleinberg
#t 2007
#c 0
#% 300184
#% 333876
#% 342596
#% 576110
#% 576111
#% 577217
#% 729923
#% 754061
#% 754107
#% 809245
#% 836506
#% 868469
#% 874891
#% 881460
#% 881523
#% 956511
#% 1740518
#! The profileration of rich social media, on-line communities, and collectively produced knowledge resources has accelerated the convergence of technological and social networks, producing environments that reflect both the architecture of the underlying information systems and the social structure on their members. In studying the consequences of these developments, we are faced with the opportunity to analyze social network data at unprecedented levels of scale and temporal resolution; this has led to a growing body of research at the intersection of the computing and social sciences. We discuss some of the current challenges in the analysis of large-scale social network data, focusing on two themes in particular: the inference of social processes from data, and the problem of maintaining individual privacy in studies of social networks. While early research on this type of data focused on structural questions, recent work has extended this to consider the social processes that unfold within the networks. Particular lines of investigation have focused on processes in on-line social systems related to communication [1, 22], community formation [2, 8, 16, 23], information-seeking and collective problem-solving [20, 21, 18], marketing [12, 19, 24, 28], the spread of news [3, 17], and the dynamics of popularity [29]. There are a number of fundamental issues, however, for which we have relatively little understanding, including the extent to which the outcomes of these types of social processes are predictable from their early stages (see e.g. [29]), the differences between properties of individuals and properties of aggregate populations in these types of data, and the extent to which similar social phenomena in different domains have uniform underlying explanations. The second theme we pursue is concerned with the problem of privacy. While much of the research on large-scale social systems has been carried out on data that is public, some of the richest emerging sources of social interaction data come from settings such as e-mail, instant messaging, or phone communication in which users have strong expectations of privacy. How can such data be made available to researchers while protecting the privacy of the individuals represented in the data? Many of the standard approaches here are variations on the principle of anonymization - the names of individuals are replaced with meaningless unique identifiers, so that the network structure is maintained while private information has been suppressed. In recent joint work with Lars Backstrom and Cynthia Dwork, we have identified some fundamental limitations on the power of network anonymization to ensure privacy [7]. In particular, we describe a family of attacks such that even from a single anonymized copy of a social network, it is possible for an adversary to learn whether edges exist or not between specific targeted pairs of nodes. The attacks are based on the uniqueness of small random subgraphs embedded in an arbitrary network, using ideas related to those found in arguments from Ramsey theory [6, 14]. Combined with other recent examples of privacy breaches in data containing rich textual or time-series information [9, 26, 27, 30], these results suggest that anonymization contains pitfalls even in very simple settings. In this way, our approach can be seen as a step toward understanding how techniques of privacy-preserving data mining (see e.g. [4, 5, 10, 11, 13, 15, 25] and the references therein) can inform how we think about the protection of eventhe most skeletal social network data.

#index 989571
#* Efficient and effective explanation of change in hierarchical summaries
#@ Deepak Agarwal;Dhiman Barman;Dimitrios Gunopulos;Neal E. Young;Flip Korn;Divesh Srivastava
#t 2007
#c 0
#% 223781
#% 228717
#% 248822
#% 263387
#% 459025
#% 479795
#% 479957
#% 577220
#% 646218
#% 729943
#% 781692
#% 800176
#% 824685
#% 839169
#% 847111
#% 893160
#% 993995
#% 1015293
#% 1016144
#% 1702985
#! Dimension attributes in data warehouses are typically hierarchical (e.g., geographic locations in sales data, URLs in Web traffic logs). OLAP tools are used to summarize the measure attributes (e.g., total sales) along a dimension hierarchy, and to characterize changes (e.g., trends and anomalies) in a hierarchical summary over time. When thenumber of changes identified is large (e.g., total sales in many stores differed from their expected values), a parsimonious explanation of the most significant changes is desirable. In this paper, we propose a natural model of parsimonious explanation, as a composition of node weights along the root-to-leaf paths in a dimension hierarchy, which permits changes to be aggregated with maximal generalization along the dimension hierarchy. We formalize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious explanations on arbitrary rooted one dimensional tree hierarchies. We show that such explanations can be computed efficiently in time essentially proportional to the number of leaves and the depth of the hierarchy. Further, our method can produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals, making it widely applicable. Our experiments use real data sets to demonstrate the utility and robustness of our proposed model for explaining significant changes, as well as its superior parsimony compared to alternatives.

#index 989572
#* Estimating rates of rare events at multiple resolutions
#@ Deepak Agarwal;Andrei Zary Broder;Deepayan Chakrabarti;Dejan Diklic;Vanja Josifovski;Mayssam Sayyadian
#t 2007
#c 0
#% 404849
#% 995020
#% 1271973
#! We consider the problem of estimating occurrence rates of rare eventsfor extremely sparse data, using pre-existing hierarchies to perform inference at multiple resolutions. In particular, we focus on the problem of estimating click rates for (webpage, advertisement) pairs (called impressions) where both the pages and the ads are classified into hierarchies that capture broad contextual information at different levels of granularity. Typically the click rates are low and the coverage of the hierarchies is sparse. To overcome these difficulties we devise a sampling method whereby we analyze aspecially chosen sample of pages in the training set, and then estimate click rates using a two-stage model. The first stage imputes the number of (webpage, ad) pairs at all resolutions of the hierarchy to adjust for the sampling bias. The second stage estimates clickrates at all resolutions after incorporating correlations among sibling nodes through a tree-structured Markov model. Both models are scalable and suited to large scale data mining applications. On a real-world dataset consisting of 1/2 billion impressions, we demonstrate that even with 95% negative (non-clicked) events in the training set, our method can effectively discriminate extremely rare events in terms of their click propensity.

#index 989573
#* Predictive discrete latent factor models for large scale dyadic data
#@ Deepak Agarwal;Srujana Merugu
#t 2007
#c 0
#% 173879
#% 277483
#% 280819
#% 304425
#% 376266
#% 424885
#% 729918
#% 769883
#% 778215
#% 881487
#% 916785
#% 916796
#% 1014670
#% 1025261
#! We propose a novel statistical method to predict large scale dyadic response variables in the presence of covariate information. Our approach simultaneously incorporates the effect of covariates and estimates local structure that is induced by interactions among the dyads through a discrete latent factor model. The discovered latent factors provide a redictive model that is both accurate and interpretable. We illustrate our method by working in a framework of generalized linear models, which include commonly used regression techniques like linear regression, logistic regression and Poisson regression as special cases. We also provide scalable generalized EM-based algorithms for model fitting using both "hard" and "soft" cluster assignments. We demonstrate the generality and efficacy of our approach through large scale simulation studies and analysis of datasets obtained from certain real-world movie recommendation and internet advertising applications.

#index 989574
#* On string classification in data streams
#@ Charu C. Aggarwal;Philip S. Yu
#t 2007
#c 0
#% 235941
#% 310500
#% 333943
#% 342600
#% 481290
#% 577227
#% 682435
#% 682712
#% 769927
#! String data has recently become important because of its use in a number of applications such as computational and molecular biology, protein analysis, and market basket data. In many cases, these strings contain a wide variety of substructures which may have physical significance for that application. For example, such substructures could represent important fragments of a DNA string or an interesting portion of a fraudulent transaction. In such a case, it is desirable to determine the identity, location, and extent of that substructure in the data. This is a much more difficult generalization of the classification problem, since the latter problem labels entire strings rather than deal with the more complex task of determining string fragments with a particular kind of behavior. The problem becomes even more complicated when different kinds of substrings show complicated nesting patterns. Therefore, we define a somewhat different problem which we refer to as the generalized classification problem. We propose a scalable approach based on hidden markov models for this problem. We show how to implement the generalized string classification procedure for very large data bases and data streams. We present experimental results over a number of large data sets and data streams.

#index 989575
#* Xproj: a framework for projected structural clustering of xml documents
#@ Charu C. Aggarwal;Na Ta;Jianyong Wang;Jianhua Feng;Mohammed Zaki
#t 2007
#c 0
#% 36672
#% 210173
#% 262071
#% 273891
#% 413582
#% 464996
#% 466483
#% 480126
#% 481281
#% 577218
#% 629656
#% 631985
#% 729627
#% 729941
#% 740767
#% 745515
#% 1712591
#! XML has become a popular method of data representation both on the web and in databases in recent years. One of the reasons for the popularity of XML has been its ability to encode structural information about data records. However, this structural characteristic of data sets also makes it a challenging problem for a variety of data mining problems. One such problem is that of clustering, in which the structural aspects of the data result in a high implicit dimensionality of the data representation. As a result, it becomes more difficult to cluster the data in a meaningful way. In this paper, we propose an effective clustering algorithm for XML data which uses substructures of the documents in order to gain insights about the important underlying structures. We propose new ways of using multiple sub-structuralinformation in XML documents to evaluate the quality of intermediate cluster solutions, and guide the algorithms to a final solution which reflects the true structural behavior in individual partitions. We test the algorithm on a variety of real and synthetic data sets.

#index 989576
#* Show me the money!: deriving the pricing power of product features by mining consumer reviews
#@ Nikolay Archak;Anindya Ghose;Panagiotis G. Ipeirotis
#t 2007
#c 0
#% 577355
#% 722308
#% 769892
#% 805873
#% 815915
#% 823332
#% 828958
#% 854646
#% 854720
#% 878935
#% 939346
#% 939896
#% 1250237
#! The increasing pervasiveness of the Internet has dramatically changed the way that consumers shop for goods. Consumer-generated product reviews have become a valuable source of information for customers, who read the reviews and decide whether to buy the product based on the information provided. In this paper, we use techniques that decompose the reviews into segments that evaluate the individual characteristics of a product (e.g., image quality and battery life for a digital camera). Then, as a major contribution of this paper, we adapt methods from the econometrics literature, specifically the hedonic regression concept, to estimate: (a) the weight that customers place on each individual product feature, (b) the implicit evaluation score that customers assign to each feature, and (c) how these evaluations affect the revenue for a given product. Towards this goal, we develop a novel hybrid technique combining text mining and econometrics that models consumer product reviews as elements in a tensor product of feature and evaluation spaces. We then impute the quantitative impact of consumer reviews on product demand as a linear functional from this tensor product space. We demonstrate how to use a low-dimension approximation of this functional to significantly reduce the number of model parameters, while still providing good experimental results. We evaluate our technique using a data set from Amazon.com consisting of sales data and the related consumer reviews posted over a 15-month period for 242 products. Our experimental evaluation shows that we can extract actionable business intelligence from the data and better understand the customer preferences and actions. We also show that the textual portion of the reviews can improve product sales prediction compared to a baseline technique that simply relies on numeric data.

#index 989577
#* Temporal causal modeling with graphical granger methods
#@ Andrew Arnold;Yan Liu;Naoki Abe
#t 2007
#c 0
#% 68247
#% 277480
#% 278011
#% 771626
#% 961141
#% 1294172
#% 1650289
#! The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of "Granger causality", based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.

#index 989578
#* Extracting semantic relations from query logs
#@ Ricardo Baeza-Yates;Alessandro Tiberi
#t 2007
#c 0
#% 194299
#% 232713
#% 310567
#% 330617
#% 401405
#% 451355
#% 629672
#% 728105
#% 766433
#% 835045
#% 864612
#% 869500
#% 907609
#% 1712595
#% 1715593
#! In this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic relations that are implicitly captured in the actions of users submitting queries and clicking answers. Previous query log analyses were mostly done with just the queries and not the actions that followed after them. We first propose a novel way to represent queries in a vector space based on a graph derived from the query-click bipartite graph. We then analyze the graph produced by our query log, showing that it is less sparse than previous results suggested, and that almost all the measures of these graphs follow power laws, shedding some light on the searching user behavior as well as on the distribution of topics that people want in the Web. The representation we introduce allows to infer interesting semantic relationships between queries. Second, we provide an experimental analysis on the quality of these relations, showing that most of them are relevant. Finally we sketch an application that detects multitopical URLs.

#index 989579
#* Real-time ranking with concept drift using expert advice
#@ Hila Becker;Marta Arias
#t 2007
#c 0
#% 81507
#% 204531
#% 226674
#% 232319
#% 232728
#% 246747
#% 268079
#% 310498
#% 310500
#% 320081
#% 342600
#% 342636
#% 342639
#% 425041
#% 425046
#% 466408
#% 466510
#% 531952
#% 722905
#% 729932
#% 734915
#% 810542
#% 840846
#% 840891
#% 881457
#% 893723
#% 1016144
#% 1250097
#% 1663704
#% 1674801
#% 1693295
#% 1705506
#! In many practical applications, one is interested in generating a ranked list of items using information mined from continuous streams of data. For example, in the context of computer networks, one might want to generate lists of nodes ranked according to their susceptibility to attack. In addition, real-world data streams often exhibit concept drift, making the learning task even more challenging. We present an online learning approach to ranking with concept drift, using weighted majority techniques. By continuously modeling different snapshots of the data and tuning our measure of belief in these models over time, we capture changes in the underlying concept and adapt our predictions accordingly. We measure the performance of our algorithm on real electricity data as well as asynthetic data stream, and demonstrate that our approach to ranking from stream data outperforms previously known batch-learning methods and other online methods that do not account for concept drift.

#index 989580
#* Modeling relationships at multiple scales to improve accuracy of large recommender systems
#@ Robert Bell;Yehuda Koren;Chris Volinsky
#t 2007
#c 0
#% 124010
#% 220711
#% 272536
#% 280852
#% 330687
#% 420515
#% 452563
#% 813966
#% 879627
#% 1345710
#! The collaborative filtering approach to recommender systems predicts user preferences for products or services by learning past user-item relationships. In this work, we propose novel algorithms for predicting user ratings of items by integrating complementary models that focus on patterns at different scales. At a local scale, we use a neighborhood-based technique that infers ratings from observed ratings by similar users or of similar items. Unlike previous local approaches, our method is based on a formal model that accounts for interactions within the neighborhood, leading to improved estimation quality. At a higher, regional, scale, we use SVD-like matrix factorization for recovering the major structural patterns in the user-item rating matrix. Unlike previous approaches that require imputations in order to fill in the unknown matrix entries, our new iterative algorithm avoids imputation. Because the models involve estimation of millions, or even billions, of parameters, shrinkage of estimated values to account for sampling variability proves crucial to prevent overfitting. Both the local and the regional approaches, and in particular their combination through a unifying model, compare favorably with other approaches and deliver substantially better results than the commercial Netflix Cinematch recommender system on a large publicly available data set.

#index 989581
#* Content-based document routing and index partitioning for scalable similarity-based searches in a large corpus
#@ Deepavali Bhagwat;Kave Eshghi;Pankaj Mehra
#t 2007
#c 0
#% 55490
#% 109209
#% 201935
#% 213080
#% 232655
#% 255137
#% 296738
#% 311808
#% 337046
#% 340175
#% 340176
#% 342373
#% 398751
#% 406493
#% 453464
#% 459945
#% 482658
#% 505869
#% 616528
#% 653827
#% 657576
#% 724045
#% 793889
#% 800585
#% 805457
#% 805476
#% 823364
#% 879606
#% 960109
#% 960181
#% 978157
#% 1850764
#! We present a document routing and index partitioning scheme for scalable similarity-based search of documents in a large corpus. We consider the case when similarity-based search is performed by finding documents that have features in common with the query document. While it is possible to store all the features of all the documents in one index, this suffers from obvious scalability problems. Our approach is to partition the feature index into multiple smaller partitions that can be hosted on separate servers, enabling scalable and parallel search execution. When a document is ingested into the repository, a small number of partitions are chosen to store the features of the document. To perform similarity-based search, also, only a small number of partitions are queried. Our approach is stateless and incremental. The decision as to which partitions the features of the document should be routed to (for storing at ingestion time, and for similarity based search at query time) is solely based on the features of the document. Our approach scales very well. We show that executing similarity-based searches over such a partitioned search space has minimal impact on the precision and recall of search results, even though every search consults less than 3% of the total number of partitions.

#index 989582
#* Support feature machine for classification of abnormal brain activity
#@ Wanpracha Art Chaovalitwongse;Ya-Ju Fan;Rajesh C. Sachdeo
#t 2007
#c 0
#% 310545
#% 342598
#% 425048
#% 573226
#% 821868
#% 833915
#% 876074
#! In this study, a novel multidimensional time series classification technique, namely support feature machine (SFM), is proposed. SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data. This paper also describes an application of SFM for detecting abnormal brain activity. Epilepsy is a case in point in this study. In epilepsy studies, electroencephalograms (EEGs), acquired in multidimensional time series format, have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain. From multi-dimensional EEG time series data, SFM was used to identify seizure pre-cursors and detect seizure susceptibility (pre-seizure) periods. The empirical results showed that SFM achieved over 80% correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation. The proposed optimization model of SFM is very compact and scalable, and can be implemented as an online algorithm. The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification.

#index 989583
#* Nonlinear adaptive distance metric learning for clustering
#@ Jianhui Chen;Zheng Zhao;Jieping Ye;Huan Liu
#t 2007
#c 0
#% 80995
#% 457926
#% 593047
#% 723241
#% 743284
#% 757953
#% 763697
#% 770831
#% 770846
#% 770848
#% 835741
#% 875950
#% 875975
#% 876003
#% 876014
#% 961190
#% 983869
#% 989657
#% 1250593
#% 1665168
#! A good distance metric is crucial for many data mining tasks. To learn a metric in the unsupervised setting, most metric learning algorithms project observed data to a low-dimensional manifold, where geometric relationships such as pairwise distances are preserved. It can be extended to the nonlinear case by applying the kernel trick, which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space. In this paper, we propose a novel unsupervised Nonlinear Adaptive Metric Learning algorithm, called NAML, which performs clustering and distance metric learning simultaneously. NAML firstmaps the data to a high-dimensional space through a kernel function; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized; and finally performs clustering in the low-dimensional space. The performance of NAML depends on the selection of the kernel function and the projection. We show that the joint kernel learning, dimensionality reduction, and clustering can be formulated as a trace maximization problem, which can be solved via an iterative procedure in the EM framework. Experimental results demonstrated the efficacy of the proposed algorithm.

#index 989584
#* Density-based clustering for real-time stream data
#@ Yixin Chen;Li Tu
#t 2007
#c 0
#% 345859
#% 397426
#% 420078
#% 576113
#% 578388
#% 578560
#% 594012
#% 659972
#% 825482
#% 844161
#% 879741
#% 881938
#% 889089
#% 893104
#% 1015261
#% 1731169
#! Existing data-stream clustering algorithms such as CluStream arebased on k-means. These clustering algorithms are incompetent tofind clusters of arbitrary shapes and cannot handle outliers. Further, they require the knowledge of k and user-specified time window. To address these issues, this paper proposes D-Stream, a framework for clustering stream data using adensity-based approach. The algorithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the density. The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream. Exploiting the intricate relationships between the decay factor, data density and cluster structure, our algorithm can efficiently and effectively generate and adjust the clusters in real time. Further, a theoretically sound technique is developed to detect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system. The technique makes high-speed data stream clustering feasible without degrading the clustering quality. The experimental results show that our algorithm has superior quality and efficiency, can find clusters of arbitrary shapes, and can accurately recognize the evolving behaviors of real-time data streams.

#index 989585
#* Cross-language information retrieval using PARAFAC2
#@ Peter A. Chew;Brett W. Bader;Tamara G. Kolda;Ahmed Abdelali
#t 2007
#c 0
#% 49501
#% 200694
#% 732848
#% 829309
#% 1264279
#! A standard approach to cross-language information retrieval (CLIR) uses Latent Semantic Analysis (LSA) in conjunction with a multilingual parallel aligned corpus. This approach has been shown to be successful in identifying similar documents across languages - or more precisely, retrieving the most similar document in one language to a query in another language. However, the approach has severe drawbacks when applied to a related task, that of clustering documents "language-independently", so that documents about similar topics end up closest to one another in the semantic space regardless of their language. The problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language, but on the same topic. As a result, when using multilingual LSA, documents will in practice cluster by language, not by topic. We propose a novel application of PARAFAC2 (which is a variant of PARAFAC, a multi-way generalization of the singular value decomposition [SVD]) to overcome this problem. Instead of forming a single multilingual term-by-document matrix which, under LSA, is subjected to SVD, we form an irregular three-way array, each slice of which is a separate term-by-document matrix for a single language in the parallel corpus. The goal is to compute an SVD for each language such that V (the matrix of right singular vectors) is the same across all languages. Effectively, PARAFAC2 imposes the constraint, not present in standard LSA, that the "concepts" in all documents in the parallel corpus are the same regardless of language. Intuitively, this constraint makes sense, since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations. We tested this approach by comparing the performance of PARAFAC2 with standard LSA in solving a particular CLIR problem. From our results, we conclude that PARAFAC2 offers a very promising alternative to LSA not only for multilingual document clustering, but also for solving other problems in cross-language information retrieval.

#index 989586
#* Evolutionary spectral clustering by incorporating temporal smoothness
#@ Yun Chi;Xiaodan Song;Dengyong Zhou;Koji Hino;Belle L. Tseng
#t 2007
#c 0
#% 232768
#% 313959
#% 316143
#% 464291
#% 594012
#% 635713
#% 769935
#% 769946
#% 770830
#% 879615
#% 881514
#% 961204
#% 1015261
#! Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams. In evolutionary clustering, a good clustering result should fit the current data well, while simultaneously not deviate too dramatically from the recent history. To fulfill this dual purpose, a measure of temporal smoothness is integrated in the overall measure of clustering quality. In this paper, we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering. For both frameworks, we start with intuitions gained from the well-known k-means clustering problem, and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems. Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts. Furthermore, we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems. Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts.

#index 989587
#* Structural and temporal analysis of the blogosphere through community factorization
#@ Yun Chi;Shenghuo Zhu;Xiaodan Song;Junichi Tatemura;Belle L. Tseng
#t 2007
#c 0
#% 310514
#% 313959
#% 316143
#% 577360
#% 643008
#% 754107
#% 766432
#% 805906
#% 823342
#% 869516
#% 881460
#% 881498
#% 881514
#% 881523
#% 907491
#% 907492
#% 961565
#! The blogosphere has unique structural and temporal properties since blogs are typically used as communication media among human individuals. In this paper, we propose a novel technique that captures the structure and temporal dynamics of blog communities. In our framework, a community is a set of blogs that communicate with each other triggered by some events (such as a news article). The community is represented by its structure and temporal dynamics: a community graph indicates how often one blog communicates with another, and a community intensity indicates the activity level of the community that varies over time. Our method, community factorization, extracts such communities from the blogosphere, where the communication among blogs is observed as a set of subgraphs (i.e., threads of discussion). This community extraction is formulated as a factorization problem in the framework of constrained optimization, in which the objective is to best explain the observed interactions in the blogosphere over time. We further provide a scalable algorithm for computing solutions to the constrained optimization problems. Extensive experimental studies on both synthetic and real blog data demonstrate that our technique is able to discover meaningful communities that are not detectable by traditional methods.

#index 989588
#* Discovering the hidden structure of house prices with a non-parametric latent manifold model
#@ Sumit Chopra;Trivikraman Thampy;John Leahy;Andrew Caplin;Yann LeCun
#t 2007
#c 0
#% 234978
#! In many regression problems, the variable to be predicted depends not only on a sample-specific feature vector, but also on an unknown (latent) manifold that must satisfy known constraints. An example is house prices, which depend on the characteristics of the house, and on the desirability of the neighborhood, which is not directly measurable. The proposed method comprises two trainable components. The first one is a parametric model that predicts the "intrinsic" price of the house from its description. The second one is a smooth, non-parametric model of the latent "desirability" manifold. The predicted price of a house is the product of its intrinsic price and desirability. The two components are trained simultaneously using a deterministic form of the EM algorithm. The model was trained on a large dataset of houses from Los Angeles county. It produces better predictions than pure parametric and non-parametric models. It also produces useful estimates of the desirability surface at each location.

#index 989589
#* Stochastic processes and temporal data mining
#@ Paul Cotofrei;Kilian Stoffel
#t 2007
#c 0
#% 101955
#% 463948
#% 516320
#% 562528
#% 1499550
#! This article tries to give an answer to a fundamental question intemporal data mining: "Under what conditions a temporal rule extracted from up-to-date temporal data keeps its confidence/support for future data". A possible solution is given by using, on the one hand, a temporal logic formalism which allows the definition of the main notions (event, temporal rule, support, confidence) in a formal way and, on the other hand, the stochastic limit theory. Under this probabilistic temporal framework, the equivalence between the existence of the support of a temporal rule and the law of large numbers is systematically analyzed.

#index 989590
#* Exploiting underrepresented query aspects for automatic query expansion
#@ Daniel Wayne Crabtree;Peter Andreae;Xiaoying Gao
#t 2007
#c 0
#% 109190
#% 115473
#% 157905
#% 218978
#% 232644
#% 232719
#% 262045
#% 348155
#% 375017
#% 742666
#% 766498
#% 766525
#% 804805
#% 807395
#% 818267
#% 838399
#% 838532
#% 879613
#% 961606
#% 975019
#% 1682057
#! Users attempt to express their search goals through web search queries. When a search goal has multiple components or aspects, documents that represent all the aspects are likely to be more relevant than those that only represent some aspects. Current web search engines often produce result sets whose top ranking documents represent only a subset of the query aspects. By expanding the query using the right keywords, the search engine can find documents that represent more query aspects and performance improves. This paper describes AbraQ, an approach for automatically finding the right keywords to expand the query. AbraQ identifies the aspects in the query, identifies which aspects are underrepresented in the result set of the original query, and finally, for any particularly underrepresented aspect, identifies keywords that would enhance that aspect's representation and automatically expands the query using the best one. The paper presents experiments that show AbraQ significantly increases the precision of hard queries, whereas traditional automatic query expansion techniques have not improved precision. AbraQ also compared favourably against a range of interactive query expansion techniques that require user involvement including clustering, web-log analysis, relevance feedback, and pseudo relevance feedback.

#index 989591
#* Canonicalization of database records using adaptive similarity measures
#@ Aron Culotta;Michael Wick;Robert Hall;Matthew Marzilli;Andrew McCallum
#t 2007
#c 0
#% 73441
#% 350103
#% 382854
#% 854636
#% 893168
#% 939391
#% 961152
#% 1261595
#% 1289565
#! It is becoming increasingly common to construct databases from information automatically culled from many heterogeneous sources. For example, a research publication database can be constructed by automatically extracting titles, authors, and conference information from online papers. A common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways (e.g. abbreviations, aliases, and misspellings). Therefore, it can be difficult to construct a single, standard representation to present to the user. We refer to the task of constructing this representation as canonicalization. Despite its importance, there is little existing work on canonicalization. In this paper, we explore the use of edit distance measures to construct a canonical representation that is "central" in the sense that it is most similar to each of the disparate records. This approach reduces the impact of noisy records on the canonical representation. Furthermore, because the user may prefer different styles of canonicalization, we show how different edit distance costs can result in different forms of canonicalization. For example, reducing the cost of character deletions can result in representations that favor abbreviated forms over expanded forms (e.g. KDD versus Conference on Knowledge Discovery and Data Mining). We describe how to learn these costs from a small amount of manually annotated data using stochastic hill-climbing. Additionally, we investigate feature-based methods to learn ranking preferences over canonicalizations. These approaches can incorporate arbitrary textual evidence to select a canonical record. We evaluate our approach on a real-world publications database and show that our learning method results in a canonicalization solution that is robust to errors and easily customizable to user preferences.

#index 989592
#* Co-clustering based classification for out-of-domain documents
#@ Wenyuan Dai;Gui-Rong Xue;Qiang Yang;Yong Yu
#t 2007
#c 0
#% 115608
#% 116149
#% 127850
#% 236497
#% 252011
#% 311027
#% 376266
#% 464631
#% 465754
#% 466263
#% 577230
#% 727899
#% 729918
#% 879615
#% 1250587
#% 1272110
#! In many real world applications, labeled data are in short supply. It often happens that obtaining labeled data in a new domain is expensive and time consuming, while there may be plenty of labeled data from a related but different domain. Traditional machine learning is not able to cope well with learning across different domains. In this paper, we address this problem for a text-mining task, where the labeled data are under one distribution in one domain known as in-domain data, while the unlabeled data are under a related but different domain known as out-of-domain data. Our general goal is to learn from the in-domain and apply the learned knowledge to out-of-domain. We propose a co-clustering based classification (CoCC) algorithm to tackle this problem. Co-clustering is used as a bridge to propagate the class structure and knowledge from the in-domain to the out-of-domain. We present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results, even when the distributions between the two data are different. The experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms.

#index 989593
#* Detecting anomalous records in categorical datasets
#@ Kaustav Das;Jeff Schneider
#t 2007
#c 0
#% 152934
#% 466745
#% 577275
#% 578689
#% 629607
#% 769896
#% 777671
#% 823622
#% 860824
#% 963838
#% 978636
#% 1001828
#% 1013724
#% 1272326
#% 1784191
#! We consider the problem of detecting anomalies in high aritycategorical datasets. In most applications, anomalies are defined as datapoints that are "abnormal". Quite often we have access to data which consists mostly of normal records, a long with a small percentage of unlabelled anomalous records. We are interested in the problem of unsupervised anomaly detection, where we use the unlabelled data for training, and detect records that do not follow the definition of normality. A standard approach is to create a model of normal data, and compare test records against it. A probabilistic approach builds a likelihood model from the training data. Records are tested for anomalies based on the complete record likelihood given the probability model. For categorical attributes, bayes nets give a standard representation of the likelihood. While this approach is good at finding outliers in the dataset, it often tends to detect records with attribute values that are rare. Sometimes, just detecting rare values of an attribute is not desired and such outliers are not considered as anomalies in that context. We present an alternative definition of anomalies, and propose an approach of comparing against marginal distribution of attribute subsets. We show that this is a more meaningful way of detecting anomalies, and has a better performance over semi-synthetic as well as real world datasets.

#index 989594
#* Feature selection methods for text classification
#@ Anirban Dasgupta;Petros Drineas;Boulos Harb;Vanja Josifovski;Michael W. Mahoney
#t 2007
#c 0
#% 129316
#% 132927
#% 194285
#% 243727
#% 243728
#% 280817
#% 292664
#% 342598
#% 344447
#% 420507
#% 425047
#% 458379
#% 465754
#% 466266
#% 577228
#% 577267
#% 594000
#% 642998
#% 716271
#% 722929
#% 722935
#% 763708
#% 766438
#% 770810
#% 774854
#% 794857
#% 847160
#% 1039679
#! We consider feature selection for text classification both theoretically and empirically. Our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features. To the best of our knowledge, this is the first feature selection method with such guarantees. In addition, the analysis leads to insights as to when and why this feature selection strategy will perform well in practice. We then use the TechTC-100, 20-Newsgroups, and Reuters-RCV2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies. Our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used feature selection strategies. In addition, it performs better on certain datasets under very aggressive feature selection.

#index 989595
#* Efficient incremental constrained clustering
#@ Ian Davidson;S. S. Ravi;Martin Ester
#t 2007
#c 0
#% 408396
#% 466890
#% 769881
#% 829025
#% 948091
#% 1250560
#% 1663626
#! Clustering with constraints is an emerging area of data mining research. However, most work assumes that the constraints are given as one large batch. In this paper we explore the situation where the constraints are incrementally given. In this way the user after seeing a clustering can provide positive and negative feedback via constraints to critique a clustering solution. We consider the problem of efficiently updating a clustering to satisfy the new and old constraints rather than reclustering the entire data set. We show that the problem of incremental clustering under constraints is NP-hard in general, but identify several sufficient conditions which lead to efficiently solvable versions. These translate into a set of rules on the types of constraints thatcan be added and constraint set properties that must be maintained. We demonstrate that this approach is more efficient than re-clustering the entire data set and has several other advantages.

#index 989596
#* A framework for simultaneous co-clustering and learning from complex data
#@ Meghana Deodhar;Joydeep Ghosh
#t 2007
#c 0
#% 280852
#% 469422
#% 608341
#% 729918
#% 778215
#% 844369
#% 989596
#% 1014670
#% 1042787
#% 1704766
#% 1784736
#! For difficult classification or regression problems, practitioners often segment the data into relatively homogenous groups and then build a model for each group. This two-step procedure usually results in simpler, more interpretable and actionable models without any lossin accuracy. We consider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two groups. A pivoting operation can now result in the dependent variable showing up as entries in a "customer by product" data matrix. We present a model-based co-clustering (meta)-algorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suitable cost function. The framework not only generalizes co-clustering and collaborative filtering to model-basedco-clustering, but can also be viewed as simultaneous co-segmentation and classification or regression, which is better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to address classification and regression problems. We demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data.

#index 989597
#* A learning framework using Green's function and kernel regularization with application to recommender system
#@ Chris Ding;Horst D. Simon;Rong Jin;Tao Li
#t 2007
#c 0
#% 63833
#% 173879
#% 224113
#% 252011
#% 271082
#% 280852
#% 301590
#% 313959
#% 330687
#% 420515
#% 428272
#% 452563
#% 464615
#% 478768
#% 734594
#% 766449
#% 770851
#% 813966
#% 879627
#% 915310
#% 1650569
#! Green's function for the Laplace operator represents the propagation of influence of point sources and is the foundation for solving many physics problems. On a graph of pairwise similarities, the Green's function is the inverse of the combinatorial Laplacian; we resolve the zero-mode difficulty by showing its physical origin as the consequence of the Von Neumann boundary condition. We propose to use Green's function to propagate label information for both semi-supervised and unsupervised learning. We also derive this learning framework from the kernel regularization using Reproducing Kernel Hilbert Space theory at strong regularization limit. Green's function provides a well-defined distance metric on a generic weighted graph, either as the effective distance on the network of electric resistors, or the average commute time in random walks. We show that for unsupervised learning this approach is identical to Ratio Cut and Normalized Cut spectral clustering algorithms. Experiments on newsgroups and six UCI datasets illustrate the effectiveness of this approach. Finally, we propose a novel item-based recommender system using Green's function and show its effectiveness.

#index 989598
#* Development of NeuroElectroMagnetic ontologies(NEMO): a framework for mining brainwave ontologies
#@ Dejing Dou;Gwen Frishkoff;Jiawei Rong;Robert Frank;Allen Malony;Don Tucker
#t 2007
#c 0
#% 136350
#% 174161
#% 445448
#% 481290
#% 818916
#% 832641
#% 863392
#% 864624
#% 890351
#% 962460
#! Event-related potentials (ERP) are brain electrophysiological patterns created by averaging electroencephalographic (EEG) data, time-locking to events of interest (e.g., stimulus or response onset). In this paper, we propose a generic framework for mining anddeveloping domain ontologies and apply it to mine brainwave (ERP) ontologies. The concepts and relationships in ERP ontologies can be mined according to the following steps: pattern decomposition, extraction of summary metrics for concept candidates, hierarchical clustering of patterns for classes and class taxonomies, and clustering-based classification and association rules mining for relationships (axioms) of concepts. We have applied this process to several dense-array (128-channel) ERP datasets. Results suggest good correspondence between mined concepts and rules, on the one hand, and patterns and rules that were independently formulated by domain experts, on the other. Data mining results also suggest ways in which expert-defined rules might be refined to improve ontologyrepresentation and classification results. The next goal of our ERP ontology mining framework is to address some long-standing challenges in conducting large-scale comparison and integration of results across ERP paradigms and laboratories. In a more general context, this work illustrates the promise of an interdisciplinary research program, which combines data mining, neuroinformatics andontology engineering to address real-world problems.

#index 989599
#* Semi-supervised classification with hybrid generative/discriminative methods
#@ Gregory Druck;Chris Pal;Andrew McCallum;Xiaojin Zhu
#t 2007
#c 0
#% 236497
#% 252011
#% 266292
#% 466263
#% 854813
#% 875962
#% 883830
#% 884116
#% 900111
#% 961195
#% 1250575
#! We compare two recently proposed frameworks for combining generative and discriminative probabilistic classifiers and apply them to semi-supervised classification. In both cases we explore the tradeoff between maximizing a discriminative likelihood of labeled data and a generative likelihood of labeled and unlabeled data. While prominent semi-supervised learning methods assume low density regions between classes or are subject to generative modeling assumptions, we conjecture that hybrid generative/discriminative methods allow semi-supervised learning in the presence of strongly overlapping classes and reduce the risk of modeling structure in the unlabeled data that is irrelevant for the specific classification task of interest. We apply both hybrid approaches within naively structured Markov random field models and provide a thorough empirical comparison with two well-known semi-supervised learning methods on six text classification tasks. A semi-supervised hybrid generative/discriminative method provides the best accuracy in 75% of the experiments, and the multi-conditional learning hybrid approach achieves the highest overall mean accuracy across all tasks.

#index 989600
#* Finding tribes: identifying close-knit individuals from employment patterns
#@ Lisa Friedland;David Jensen
#t 2007
#c 0
#% 46803
#% 249110
#% 466745
#% 477479
#% 481609
#% 578775
#% 727932
#% 736155
#% 823370
#% 844322
#% 1414755
#! We present a family of algorithms to uncover tribes-groups of individuals who share unusual sequences of affiliations. While much work inferring community structure describes large-scale trends, we instead search for small groups of tightly linked individuals who behave anomalously with respect to those trends. We apply the algorithms to a large temporal and relational data set consisting of millions of employment records from the National Association of Securities Dealers. The resulting tribes contain individuals at higher risk for fraud, are homogenous with respect to risk scores, and are geographically mobile, all at significant levels compared to random or to other sets of individuals who share affiliations.

#index 989601
#* Time-dependent event hierarchy construction
#@ Gabriel Pui Cheong Fung;Jeffrey Xu Yu;Huan Liu;Philip S. Yu
#t 2007
#c 0
#% 9197
#% 46803
#% 109213
#% 115470
#% 173424
#% 230532
#% 249147
#% 262042
#% 262043
#% 287196
#% 309096
#% 309100
#% 340944
#% 344447
#% 397132
#% 445316
#% 461222
#% 577220
#% 641060
#% 643016
#% 677440
#% 748738
#% 765412
#% 771924
#% 818215
#% 823344
#% 824666
#% 843873
#% 881498
#! In this paper, an algorithm called Time Driven Documents-partition (TDD) is proposed to construct an event hierarchy in a text corpus based on a given query. Specifically, assume that a query contains only one feature - Election. Election is directly related to the events such as 2006 US Midterm Elections Campaign, 2004 US Presidential Election Campaign and 2004 Taiwan Presidential Election Campaign, where these events may further be divided into several smaller events (e.g. the 2006 US Midterm Elections Campaign can be broken down into events such as campaign for vote, election results and the resignation of Donald H. Rumsfeld). As such, an event hierarchy is resulted. Our proposed algorithm, TDD, tackles the problem by three major steps: (1)Identify the features that are related to the query according to both the timestamps and the contents of the documents. The features identified are regarded as bursty features; (2) Extract the documents that are highly related to the bursty features based on time; (3) Partition the extracted documents to form events and organize them in a hierarchicalstructure. To the best of our knowledge, there is little works targeting for constructing a feature-based event hierarchy for a text corpus. Practically, event hierarchies can assist us to efficiently locate our target information in a text corpus easily. Again, assume that Election is used for a query. Without an event hierarchy, it is very difficult to identify what are the major events related to it, when do these events happened, as well as the features and the news articles that are related to each of these events. We have archived two-year news articles to evaluate the feasibility of TDD. The encouraging results indicated that TDD is practically sound and highly effective.

#index 989602
#* The minimum consistent subset cover problem and its applications in data mining
#@ Byron J. Gao;Martin Ester;Jin-Yi Cai;Oliver Schulte;Hui Xiong
#t 2007
#c 0
#% 136350
#% 232768
#% 237200
#% 277919
#% 319789
#% 420084
#% 443157
#% 466425
#% 629644
#% 769876
#% 823356
#% 915259
#% 1138540
#! In this paper, we introduce and study the Minimum Consistent Subset Cover (MCSC) problem. Given a finite ground set X and a constraint t, find the minimum number of consistent subsets that cover X, where a subset of X is consistent if it satisfies t. The MCSC problem generalizes the traditional set covering problem and has Minimum Clique Partition, a dual problem of graph coloring, as an instance. Many practical data mining problems in the areas of rule learning, clustering, and frequent pattern mining can be formulated as MCSC instances. In particular, we discuss the Minimum Rule Set problem that minimizes model complexity of decision rules as well as some converse k-clustering problems that minimize the number of clusters satisfying certain distance constraints. We also show how the MCSC problem can find applications in frequent pattern summarization. For any of these MCSC formulations, our proposed novel graph-based generic algorithm CAG can be directly applicable. CAG starts by constructing a maximal optimal partial solution, then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set. Our experiments on benchmark datasets show that CAG achieves good results compared to existing popular heuristics.

#index 989603
#* Constraint-driven clustering
#@ Rong Ge;Martin Ester;Wen Jin;Ian Davidson
#t 2007
#c 0
#% 5335
#% 210173
#% 248030
#% 420082
#% 443478
#% 466890
#% 576761
#% 769936
#% 801690
#% 837639
#% 885383
#% 893725
#% 948091
#% 1250560
#% 1669941
#! Clustering methods can be either data-driven or need-driven. Data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements. Thus, need-driven (e.g. constrained) clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks, privacy preservation, and market segmentation. However, the existing methods of constrained clustering require users to provide the number of clusters, which is often unknown in advance, but has a crucial impact on the clustering result. In this paper, we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters. For this purpose, we introduce a novel cluster model, Constraint-Driven Clustering (CDC), which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints. Two general types of constraints are considered, i.e. minimum significance constraints and minimum variance constraints, as well as combinations of these two types. We prove the NP-hardness of the CDC problem with different constraints. We propose a novel dynamic data structure, the CD-Tree, which organizes data points in leaf nodes such that each leaf node approximately satisfies the CDC constraints and minimizes the objective function. Based on CD-Trees, we develop an efficient algorithm to solve the new clustering problem. Our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm.

#index 989604
#* Trajectory pattern mining
#@ Fosca Giannotti;Mirco Nanni;Fabio Pinelli;Dino Pedreschi
#t 2007
#c 0
#% 329537
#% 463903
#% 464996
#% 769899
#% 836159
#% 844292
#% 1720762
#! The increasing pervasiveness of location-acquisition technologies (GPS, GSM networks, etc.) is leading to the collection of large spatio-temporal datasets and to the opportunity of discovering usable knowledge about movement behaviour, which fosters novel applications and services. In this paper, we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects. We introduce trajectory patterns as concise descriptions of frequent behaviours, in terms of both space (i.e., the regions of space visited during movements) and time (i.e., the duration of movements). In this setting, we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity. The various approaches are then empirically evaluated over real data and synthetic benchmarks, comparing their strengths and weaknesses.

#index 989605
#* Enhanced max margin learning on multimodal data mining in a multimedia database
#@ Zhen Guo;Zhongfei Zhang;Eric Xing;Christos Faloutsos
#t 2007
#c 0
#% 190581
#% 302390
#% 318785
#% 457912
#% 464434
#% 466892
#% 642990
#% 722927
#% 757953
#% 769952
#% 770763
#% 770776
#% 840001
#% 840856
#% 840947
#% 875963
#% 905280
#% 1502531
#% 1858012
#! The problem of multimodal data mining in a multimedia database can be addressed as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables. In this paper, built upon the existing literature on the max margin based learning, we develop a new max margin learning approach called Enhanced Max Margin Learning (EMML) framework. In addition, we apply EMML framework to developing an effective and efficient solution to the multimodal data mining problem in a multimedia database. The main contributions include: (1) we have developed a new max margin learning approach - the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate, which is verified in empirical evaluations; (2) we have applied this EMML approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale, allowing facilitating a multimodal data mining querying to a very large scale multimedia database,and excelling many existing multimodal data mining methods in the literature that do not scale up at all; this advantage is also supported through the complexity analysis as well as empirical evaluations against a state-of-the-art multimodal data mining method from the literature. While EMML is a general framework, for the evaluation purpose, we apply it to the Berkeley Drosophila embryo image database, and report the performance comparison with a state-of-the-art multimodal data mining method.

#index 989606
#* Finding low-entropy sets and trees from binary data
#@ Hannes Heikinheimo;Eino Hinkkanen;Heikki Mannila;Taneli Mielikäinen;Jouni K. Seppänen
#t 2007
#c 0
#% 44876
#% 114739
#% 115608
#% 129987
#% 152934
#% 197387
#% 232136
#% 248792
#% 299985
#% 722753
#% 729922
#% 765518
#% 881472
#% 881479
#% 1012144
#% 1663621
#% 1663631
#% 1663670
#! The discovery of subsets with special properties from binary data hasbeen one of the key themes in pattern discovery. Pattern classes suchas frequent itemsets stress the co-occurrence of the value 1 in the data. While this choice makes sense in the context of sparse binary data, it disregards potentially interesting subsets of attributes that have some other type of dependency structure. We consider the problem of finding all subsets of attributes that have low complexity. The complexity is measured by either the entropy of the projection of the data on the subset, or the entropy of the data for the subset when modeled using a Bayesian tree, with downward or upward pointing edges. We show that the entropy measure on sets has a monotonicity property, and thus a levelwise approach can find all low-entropy itemsets. We also show that the tree-based measures are bounded above by the entropy of the corresponding itemset, allowing similar algorithms to be used for finding low-entropy trees. We describe algorithms for finding all subsets satisfying an entropy condition. We give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data. We also discuss the search for high-entropy subsets and the computation of the Vapnik-Chervonenkis dimension of the data.

#index 989607
#* Dynamic hybrid clustering of bioinformatics by incorporating text mining and citation analysis
#@ Frizo Janssens;Wolfgang Glänzel;Bart De Moor
#t 2007
#c 0
#% 32926
#% 36672
#% 200694
#% 268079
#% 290830
#% 300971
#% 387427
#% 406493
#% 413608
#% 466805
#% 793417
#% 823344
#% 967621
#! To unravel the concept structure and dynamics of the bioinformatics field, we analyze a set of 7401 publications from the Web of Science and MEDLINE databases, publication years 1981-2004. For delineating this complex, interdisciplinary field, a novel bibliometric retrieval strategy is used. Given that the performance of unsupervised clustering and classification of scientific publications is significantly improved by deeply merging textual contents with the structure of the citation graph, we proceed with a hybrid clustering method based on Fisher's inverse chi-square. The optimal number of clusters is determined by a compound semiautomatic strategy comprising a combination of distance-based and stability-based methods. We also investigate the relationship between number of Latent Semantic Indexing factors, number of clusters, and clustering performance. The HITS and PageRank algorithms are used to determine representative publications in each cluster. Next, we develop a methodology for dynamic hybrid clustering of evolving bibliographic data sets. The same clustering methodology is applied to consecutive periods defined by time windows on the set, and in a subsequent phase chains are formed by matching and tracking clusters through time. Term networks for the eleven resulting cluster chains present the cognitive structure of the field. Finally, we provide a view on how much attention the bioinformatics community has devoted to the different subfields through time.

#index 989608
#* Detecting research topics via the correlation between graphs and texts
#@ Yookyung Jo;Carl Lagoze;C. Lee Giles
#t 2007
#c 0
#% 152934
#% 310514
#% 577220
#% 729968
#% 755863
#% 769897
#% 769906
#% 805906
#% 818247
#% 823344
#% 823346
#% 869480
#% 874462
#% 879625
#% 881498
#% 1663619
#! In this paper we address the problem of detecting topics in large-scale linked document collections. Recently, topic detection has become a very active area of research due to its utility for information navigation, trend analysis, and high-level description of data. We present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the nodes are limited to the documents containing the term. This tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models. We develop a topic score measure for each term, using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity. Our approach is based on the intuition that if a term is relevant to a topic, the documents containing the term have denser connectivity than a random selection of documents. We extend our algorithm to detect a topic represented by a set of terms, using the intuition that if the co-occurrence of terms represents a new topic, the citation pattern should exhibit the synergistic effect. We test our algorithm on two electronic research literature collections,arXiv and Citeseer.Our evaluation shows that the approach is effective and reveals some novel aspects of topic detection.

#index 989609
#* Exploiting duality in summarization with deterministic guarantees
#@ Panagiotis Karras;Dimitris Sacharidis;Nikos Mamoulis
#t 2007
#c 0
#% 168862
#% 248822
#% 273902
#% 282942
#% 326303
#% 399763
#% 479648
#% 481266
#% 572308
#% 575972
#% 578390
#% 742562
#% 810030
#% 823333
#% 824685
#% 824686
#% 847111
#% 850727
#% 893160
#% 956456
#% 1015256
#% 1016154
#% 1688247
#% 1702985
#! Summarization is an important task in data mining. A major challenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guarantee, often expressed in terms of a maximum-error metric. Histograms and several hierarchical techniques have been proposed for this problem. However, their time and/or space complexities remain impractically high and depend not only on the data set size n, but also on the space budget B. These handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data. In this paper we develop an alternative methodology that dispels these deficiencies, thanks to a fruitful application of the solution to the dual problem: given a maximum allowed error, determine the minimum-space synopsis that achieves it. Compared to the state-of-the-art, our histogram construction algorithm reduces time complexity by (at least) a Blog2n over logε* factor and our hierarchical synopsis algorithm reduces the complexity by (at least) a factor of log2B over logε* + logn in time and B(1-log B over log n) in space, where ε* is the optimal error. These complexity advantages offer both a space-efficiency and a scalability that previous approaches lacked. We verify the benefits of our approach in practice by experimentation.

#index 989610
#* Correlation search in graph databases
#@ Yiping Ke;James Cheng;Wilfred Ng
#t 2007
#c 0
#% 227919
#% 452846
#% 466476
#% 466644
#% 577214
#% 601159
#% 629708
#% 769951
#% 769952
#% 800507
#% 850729
#% 863387
#% 881478
#% 893372
#% 907501
#% 912244
#% 960305
#! Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects. However, the research of correlation mining from graph databases is still lacking despite the fact that graph data, especially in various scientific domains, proliferate in recent years. In this paper, we propose a new problem of correlation mining from graph databases, called Correlated Graph Search (CGS). CGS adopts Pearson's correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs. However, the problem poses significant challenges, since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential. We derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database. With this result, we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates. To further improve the efficiency, we develop three heuristic rules and apply them on the candidate set to further reduce the search space. Our extensive experiments demonstrate the effectiveness of our method on candidate reduction. The results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets.

#index 989611
#* Raising the baseline for high-precision text classifiers
#@ Aleksander Kolcz;Wen-tau Yih
#t 2007
#c 0
#% 246831
#% 246832
#% 425047
#% 448801
#% 458369
#% 464445
#% 466252
#% 466266
#% 577232
#% 629713
#% 728350
#% 766436
#% 815864
#% 823337
#% 902457
#% 1289281
#% 1289531
#% 1700575
#% 1707924
#% 1860547
#! Many important application areas of text classifiers demand high precision andit is common to compare prospective solutions to the performance of Naive Bayes. This baseline is usually easy to improve upon, but in this work we demonstrate that appropriate document representation can make out performing this classifier much more challenging. Most importantly, we provide a link between Naive Bayes and the logarithmic opinion pooling of the mixture-of-experts framework, which dictates a particular type of document length normalization. Motivated by document-specific feature selection we propose monotonic constraints on document term weighting, which is shown as an effective method of fine-tuning document representation. The discussion is supported by experiments using three large email corpora corresponding to the problem of spam detection, where high precision is of particular importance.

#index 989612
#* A fast algorithm for finding frequent episodes in event streams
#@ Srivatsan Laxman;P. S. Sastry;K. P. Unnikrishnan
#t 2007
#c 0
#% 420063
#% 727902
#% 785333
#% 799764
#% 832572
#% 1112745

#index 989613
#* Cost-effective outbreak detection in networks
#@ Jure Leskovec;Andreas Krause;Carlos Guestrin;Christos Faloutsos;Jeanne VanBriesen;Natalie Glance
#t 2007
#c 0
#% 56558
#% 297675
#% 577217
#% 577360
#% 729923
#% 754107
#% 757953
#% 823367
#% 868469
#% 1845813
#! Given a water distribution network, where should we place sensors toquickly detect contaminants? Or, which blogs should we read to avoid missing important stories?. These seemingly different problems share common structure: Outbreak detection can be modeled as selecting nodes (sensor locations, blogs) in a network, in order to detect the spreading of a virus or information asquickly as possible. We present a general methodology for near optimal sensor placement in these and related problems. We demonstrate that many realistic outbreak detection objectives (e.g., detection likelihood, population affected) exhibit the property of "submodularity". We exploit submodularity to develop an efficient algorithm that scales to large problems, achieving near optimal placements, while being 700 times faster than a simple greedy algorithm. We also derive online bounds on the quality of the placements obtained by any algorithm. Our algorithms and bounds also handle cases where nodes (sensor locations, blogs) have different costs. We evaluate our approach on several large real-world problems,including a model of a water distribution network from the EPA, andreal blog data. The obtained sensor placements are provably near optimal, providing a constant fraction of the optimal solution. We show that the approach scales, achieving speedups and savings in storage of several orders of magnitude. We also show how the approach leads to deeper insights in both applications, answering multicriteria trade-off, cost-sensitivity and generalization questions.

#index 989614
#* Mining statistically important equivalence classes and delta-discriminative emerging patterns
#@ Jinyan Li;Guimei Liu;Limsoon Wong
#t 2007
#c 0
#% 152934
#% 248012
#% 280409
#% 300120
#% 338594
#% 342597
#% 420126
#% 466426
#% 577214
#% 722756
#% 729933
#% 729935
#% 729979
#% 763699
#% 771846
#% 809268
#% 810064
#% 824931
#% 865731
#% 867881
#% 881486
#% 1250571
#! The support-confidence framework is the most common measure used in itemset mining algorithms, for its antimonotonicity that effectively simplifies the search lattice. This computational convenience brings both quality and statistical flaws to the results as observed by many previous studies. In this paper, we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chi-square, risk ratio, odds ratio, etc. Our algorithm is based on the concept of equivalence classes. An equivalence class is a set of frequent itemsets that always occur together in the same set of transactions. Therefore, itemsets within an equivalence class all share the same level of statistical significance regardless of the variety of test statistics. As an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generators, we just mine closed patterns and generators, taking a simultaneous depth-first search scheme. This parallel approach has not been exploited by any prior work. We evaluate our algorithm on two aspects. In general, we compare to LCM and FPclose which are the best algorithms tailored for mining only closed patterns. In particular, we compare to epMiner which is the most recent algorithm for mining a type of relative risk patterns, known as minimal emerging patterns. Experimental results show that our algorithm is faster than all of them, sometimes even multiple orders of magnitude faster. These statistically ranked patterns and the efficiency have a high potential for real-life applications, especially in biomedical and financial fields where classical test statistics are of dominant interest.

#index 989615
#* Very sparse stable random projections for dimension reduction in lα (0
#@ Ping Li
#t 2007
#c 0
#% 41374
#% 46803
#% 160390
#% 214073
#% 243929
#% 248027
#% 249321
#% 262037
#% 279755
#% 289340
#% 333881
#% 342605
#% 342617
#% 347225
#% 378388
#% 425047
#% 450924
#% 578389
#% 593839
#% 593928
#% 593957
#% 594029
#% 616528
#% 643568
#% 656804
#% 723895
#% 729964
#% 780123
#% 807385
#% 829312
#% 843878
#% 866538
#% 866738
#% 879397
#% 881484
#% 918001
#% 939408
#% 939942
#% 993959
#% 995020
#% 1014686
#% 1396690
#% 1674804
#% 1815965
#% 1860548
#! The method of stable random projections is a useful tool for efficiently computing the lα (0 A ∈RnxD. If we multiply A with a projection matrix R ΕR Dxk (k« D),whose entries are i.i.d. samples of an α-stable distribution,then the projected matrix B = Ax R Ε R nxkx containsenough information to approximately recover the l α properties in A. We propose very sparse stable random projections, by replacing the α stable distribution with a (much simpler) mixture of a symmetric α Pareto distribution (with probability Β, 0 β Β 1) and a point mass at the origin(with probability 1-Β). This leads to a significant 1 over Β fold speedup for small Β when computing B = AxR and a 1 over Β-fold cost reduction in storing R}. By analyzing the convergence, we show that in"reasonable" datasets Β often can be very small (e.g.,D1/2 without hurting the estimation accuracy. Some numerical evaluations are conducted, on synthetic data, Web crawldata, and gene expression microarray data.

#index 989616
#* BoostCluster: boosting clustering by pairwise constraints
#@ Yi Liu;Rong Jin;Anil K. Jain
#t 2007
#c 0
#% 235377
#% 280817
#% 296738
#% 464291
#% 464608
#% 769881
#% 770811
#% 812399
#% 840892
#% 844372
#% 859289
#% 875995
#% 884027
#% 1250593
#% 1279294
#% 1279446
#% 1673558
#! Data clustering is an important task in many disciplines. A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints. However, these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints. We present a boosting framework for data clustering,termed as BoostCluster, that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints. The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised. The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are, on the one hand, adapted to the clustering results at previous iterations by the given algorithm, and on the other hand consistent with the given side information. Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithms (K-means, partitional SingleLink, spectral clustering), and its performance is comparable to the state-of-the-art algorithms for data clustering with side information.

#index 989617
#* Efficient mining of iterative patterns for software specification discovery
#@ David Lo;Siau-Cheng Khoo;Chao Liu
#t 2007
#c 0
#% 4614
#% 68696
#% 231941
#% 302788
#% 310559
#% 338609
#% 343052
#% 347810
#% 374518
#% 420063
#% 433476
#% 463903
#% 464996
#% 577243
#% 655134
#% 729938
#% 745515
#% 807078
#% 810060
#% 823217
#% 868127
#% 906081
#% 1707663
#% 1707664
#! Studies have shown that program comprehension takes up to 45% of software development costs. Such high costs are caused by the lack-of documented specification and further aggravated by the phenomenon of software evolution. There is a need for automated tools to extract specifications to aid program comprehension. In this paper, a novel technique to efficiently mine common software temporal patterns from traces is proposed. These patterns shed light on program behaviors, and are termed iterative patterns. They capture unique characteristic of software traces, typically not found in arbitrary sequences. Specifically, due to loops, interesting iterative patterns can occur multiple times within a trace. Furthermore, an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length. Since a program behavior can be manifested in numerous ways, analyzing a single trace will not be sufficient. Iterative pattern mining extends sequential pattern and episode minings to discover frequent iterative patterns which occur repetitively both within a program trace and across multiple traces. In this paper, we present CLIPER (CLosed Iterative Pattern minER) to efficiently mine a closed set of iterative patterns. A performance study on several simulated and real datasets shows the efficiency of our mining algorithm and effectiveness of our pruning strategy. Our case study on JBoss Application Server confirms the usefulness of mined patterns in discovering interesting software behavioral specification.

#index 989618
#* A probabilistic framework for relational clustering
#@ Bo Long;Zhongfei Mark Zhang;Philip S. Yu
#t 2007
#c 0
#% 148149
#% 202286
#% 274612
#% 313959
#% 342621
#% 342659
#% 392781
#% 464291
#% 466675
#% 495929
#% 550396
#% 566189
#% 577273
#% 578670
#% 643009
#% 729437
#% 729918
#% 731612
#% 769881
#% 769928
#% 823328
#% 823343
#% 823359
#% 823396
#% 881487
#% 893124
#% 916785
#% 1289267
#% 1650298
#% 1650729
#! Relational clustering has attracted more and more attention due to its phenomenal impact in various important applications which involve multi-type interrelated data objects, such as Web mining, search marketing, bioinformatics, citation analysis, and epidemiology. In this paper, we propose a probabilistic model for relational clustering, which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering, semi-supervised clustering, co-clustering and graph clustering. The proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects. Under this model, we propose parametric hard and soft relational clustering algorithms under a large number of exponential family distributions. The algorithms are applicable to relational data of various structures and at the same time unifies a number of stat-of-the-art clustering algorithms: co-clustering algorithms, the k-partite graph clustering, Bregman k-means, and semi-supervised clustering based on hidden Markov random fields.

#index 989619
#* Nestedness and segmented nestedness
#@ Heikki Mannila;Evimaria Terzi
#t 2007
#c 0
#% 268762
#% 656714
#% 741027
#% 770829
#% 881472
#! Consider each row of a 0-1 dataset as the subset of the columns for which the row has an 1. Then a dataset is nested, if for all pairs of rows one row is either a superset or subset of the other. The concept of nestedness has its origins in ecology, where approximate versions of it has been used to model the species distribution in different locations. We argue that nestedness and its extensions are interesting properties of datasets, and that they can be applied also to domains other than ecology. We first define natural measures of nestedness and study their properties. We then define the concept of k-nestedness: a dataset is (almost) k-nested if the set of columns can be partitioned to k parts so that each part is (almost) nested. We consider the algorithmic problems of computing how far a dataset is from being k-nested, and for finding a good partition of the columns into k parts. The algorithms are based on spectral partitioning, and scale to moderately large datasets. We apply the methods to real data from ecology and from other applications, and demonstrate the usefulness of the concept.

#index 989620
#* Automatic labeling of multinomial topic models
#@ Qiaozhu Mei;Xuehua Shen;ChengXiang Zhai
#t 2007
#c 0
#% 78171
#% 262112
#% 279755
#% 280819
#% 342707
#% 449746
#% 577285
#% 719598
#% 722819
#% 722904
#% 742441
#% 769906
#% 769967
#% 815290
#% 823344
#% 869516
#% 875959
#% 876017
#% 879587
#% 881498
#% 881529
#% 881534
#% 915215
#% 1414372
#! Multinomial distributions over words are frequently used to model topics in text collections. A common, major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model. Experiments with user study have been done on two text data sets with different genres.The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.

#index 989621
#* Expertise modeling for matching papers with reviewers
#@ David Mimno;Andrew McCallum
#t 2007
#c 0
#% 118762
#% 262096
#% 340948
#% 722904
#% 766409
#% 788094
#% 879570
#% 879587
#% 881566
#% 913206
#% 1289476
#! An essential part of an expert-finding task, such as matching reviewers to submitted papers, is the ability to model the expertise of a person based on documents. We evaluate several measures of the association between an author in an existing collection of research papers and a previously unseen document. We compare two language model based approaches with a novel topic model, Author-Persona-Topic (APT). In this model, each author can write under one or more "personas," which are represented as independent distributions over hidden topics. Examples of previous papers written by prospective reviewers are gathered from the Rexa database, which extracts and disambiguates author mentions from documents gathered from the web. We evaluate the models using a reviewer matching task based on human relevance judgments determining how well the expertise of proposed reviewers matches a submission. We find that the APT topic model outperforms the other models.

#index 989622
#* Joint cluster analysis of attribute and relationship data withouta-priori specification of the number of clusters
#@ Flavia Moser;Rong Ge;Martin Ester
#t 2007
#c 0
#% 36672
#% 210173
#% 248790
#% 273890
#% 313959
#% 314054
#% 361100
#% 376266
#% 406493
#% 438137
#% 466083
#% 466425
#% 466675
#% 600034
#! In many applications, attribute and relationship data areavailable, carrying complementary information about real world entities. In such cases, a joint analysis of both types of data can yield more accurate results than classical clustering algorithms that either use only attribute data or only relationship (graph) data. The Connected k-Center (CkC) has been proposed as the first joint cluster analysis model to discover k clusters which are cohesive on both attribute and relationship data. However, it is well-known that prior knowledge on the number of clusters is often unavailable in applications such as community dentification and hotspot analysis. In this paper, we introduce and formalize the problem of discovering an a-priori unspecified number of clusters in the context of joint cluster analysis of attribute and relationship data, called Connected X Clusters (CXC) problem. True clusters are assumed to be compact and distinctive from their neighboring clusters in terms of attribute data and internally connected in terms of relationship data. Different from classical attribute-based clustering methods, the neighborhood of clusters is not defined in terms of attribute data but in terms of relationship data. To efficiently solve the CXC problem, we present JointClust, an algorithm which adopts a dynamic two-phase approach. In the first phase, we find so called cluster atoms. We provide a probability analysis for thisphase, which gives us a probabilistic guarantee, that each true cluster is represented by at least one of the initial cluster atoms. In the second phase, these cluster atoms are merged in a bottom-up manner resulting in a dendrogram. The final clustering is determined by our objective function. Our experimental evaluation on several real datasets demonstrates that JointClust indeed discovers meaningful and accurate clusterings without requiring the user to specify the number of clusters.

#index 989623
#* Multiscale topic tomography
#@ Ramesh M. Nallapati;Susan Ditmore;John D. Lafferty;Kin Ung
#t 2007
#c 0
#% 169781
#% 303620
#% 722904
#% 766422
#% 875959
#% 881498
#% 1810176
#! Modeling the evolution of topics with time is of great value in automatic summarization and analysis of large document collections. In this work, we propose a new probabilistic graphical model to address this issue. The new model, which we call the Multiscale Topic Tomography Model (MTTM), employs non-homogeneous Poisson processes to model generation of word-counts. The evolution of topics is modeled through a multi-scale analysis using Haar wavelets. One of the new features of the model is its modeling the evolution of topics at various time-scales of resolution, allowing the user to zoom in and out of the time-scales. Our experiments on Science data using the new model uncovers some interesting patterns in topics. The new model is also comparable to LDA in predicting unseen data as demonstrated by our perplexity experiments.

#index 989624
#* Mining optimal decision trees from itemset lattices
#@ Siegfried Nijssen;Elisa Fromont
#t 2007
#c 0
#% 136350
#% 216508
#% 232136
#% 240854
#% 278833
#% 279120
#% 300120
#% 320107
#% 320807
#% 326878
#% 376266
#% 431033
#% 447912
#% 580510
#% 580511
#% 823356
#% 926881
#% 948116
#% 1010432
#% 1012294
#% 1279300
#% 1290030
#% 1403608
#! We present DL8, an exact algorithm for finding a decision tree that optimizes a ranking function under size, depth, accuracy and leaf constraints. Because the discovery of optimal trees has high theoretical complexity, until now few efforts have been made to compute such trees for real-world datasets. An exact algorithm is of both scientific and practical interest. From a scientific point of view, it can be used as a gold standard to evaluate the performance of heuristic constraint-based decision tree learners and to gain new insight in traditional decision tree learners. From the application point of view, it can be used to discover trees that cannot be found by heuristic decision tree learners. The key idea behind our algorithm is that there is a relation between constraints on decision trees and constraints on itemsets. We show that optimal decision trees can be extracted from lattices of itemsets in linear time. We give several strategies to efficiently build these lattices. Experiments show that under the same constraints, DL8 obtains better results than C4.5, which confirms that exhaustive search does not always imply overfitting. The results also show that DL8 is a useful and interesting tool to learn decision trees under constraints.

#index 989625
#* Association analysis-based transformations for protein interaction networks: a function prediction case study
#@ Gaurav Pandey;Michael Steinbach;Rohit Gupta;Tushar Garg;Vipin Kumar
#t 2007
#c 0
#% 152934
#% 232136
#% 452846
#% 481290
#% 769958
#% 825079
#% 833125
#% 835018
#% 849811
#% 893372
#% 897261
#% 906436
#% 1013086
#! Protein interaction networks are one of the most promising types of biological data for the discovery of functional modules and the prediction of individual protein functions. However, it is known that these networks are both incomplete and inaccurate, i.e., they have spurious edges and lackbiologically valid edges. One way to handle this problem is by transforming the original interaction graph into new graphs that remove spurious edges, add biologically valid ones, and assign reliability scores to the edges constituting the final network. We investigate currently existing methods, as well as propose a robust association analysis-based method for this task. This method is based on the concept of h-confidence, which is a measure that can be used to extract groups of objects having high similarity with each other. Experimental evaluation on several protein interaction data sets show that hyperclique-based transformations enhance the performance of standard function prediction algorithms significantly, and thus have merit.

#index 989626
#* Applying collaborative filtering techniques to movie search for better ranking and browsing
#@ Seung-Taek Park;David M. Pennock
#t 2007
#c 0
#% 124010
#% 173879
#% 202009
#% 202011
#% 220707
#% 220709
#% 280447
#% 280852
#% 282905
#% 283169
#% 330687
#% 348173
#% 420515
#% 465928
#% 495929
#% 528156
#% 528182
#% 578684
#% 734593
#% 734594
#% 770816
#% 840924
#% 875976
#% 881537
#% 1650569
#! We propose a new ranking method, which combines recommender systems with information search tools for better search and browsing. Our method uses a collaborative filtering algorithm to generate personal item authorities for each user and combines them with item proximities for better ranking. To demonstrate our approach, we build a prototype movie search and browsing engine called MAD6 (Movies, Actors and Directors; 6 degrees of separation). We conduct offline and online tests of our ranking algorithm. For offline testing, we use Yahoo! Search queries that resulted in a click on a Yahoo! Movies or Internet Movie Database (IMDB) movie URL. Our online test involved 44 Yahoo! employees providing subjective assessments of results quality. In both tests, our ranking methods show significantly better recall and quality than IMDB search and Yahoo! Movies current search.

#index 989627
#* Tracking multiple topics for finding interesting articles
#@ Raymond K. Pon;Alfonso F. Cardenas;David Buttler;Terence Critchlow
#t 2007
#c 0
#% 232646
#% 262043
#% 262085
#% 292143
#% 340942
#% 420530
#% 725227
#% 734976
#% 735078
#% 805848
#% 823340
#% 823344
#% 879600
#% 879605
#% 926881
#! We introduce multiple topic tracking (MTT) for iScore to better recommend news articles for users with multiple interests and to address changes in user interests over time. As an extension of the basic Rocchio algorithm, traditional topic detection and tracking, and single-pass clustering, MTT maintains multiple interest profiles to identify interesting articles for a specific user given user-feedback. Focusing on only interesting topics enables iScore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy. Also by relating a topic's interestingness to an article.s interestingness, iScore is able to achieve higher quality results than traditional methods such as the Rocchio algorithm. We identify several operating parameters that work well for MTT. Using the same parameters, we show that MTT alone yields high quality results for recommending interesting articles from several corpora. The inclusion of MTT improves iScore's performance by 9% in recommending news articles from the Yahoo! News RSS feeds and the TREC11 adaptive filter article collection. And through a small user study, we show that iScore can still perform well when only provided with little user feedback.

#index 989628
#* Active exploration for learning rankings from clickthrough data
#@ Filip Radlinski;Thorsten Joachims
#t 2007
#c 0
#% 57484
#% 529348
#% 577224
#% 735358
#% 766472
#% 770753
#% 823348
#% 824716
#% 840846
#% 840852
#% 879565
#% 879598
#% 946521
#% 1250379
#! We address the task of learning rankings of documents from search enginelogs of user behavior. Previous work on this problem has relied onpassively collected clickthrough data. In contrast, we show that anactive exploration strategy can provide data that leads to much fasterlearning. Specifically, we develop a Bayesian approach for selectingrankings to present users so that interactions result in more informativetraining data. Our results using the TREC-10 Web corpus, as well assynthetic data, demonstrate that a directed exploration strategy quicklyleads to users being presented improved rankings in an online learningsetting. We find that active exploration substantially outperformspassive observation and random exploration.

#index 989629
#* Hierarchical mixture models: a probabilistic analysis
#@ Mark Sandler
#t 2007
#c 0
#% 190611
#% 214116
#% 329569
#% 342669
#% 495795
#% 495929
#% 527854
#% 592236
#% 593789
#% 593919
#% 722904
#% 765311
#% 823350
#% 836493
#% 836538
#% 840872
#% 1675804
#! Mixture models form one of the most widely used classes of generative models for describing structured and clustered data. In this paper we develop a new approach for the analysis of hierarchical mixture models. More specifically, using a text clustering problem as a motivation, we describe a natural generative process that creates a hierarchical mixture model for the data. In this process, an adversary starts with an arbitrary base distribution and then builds a topic hierarchy via some evolutionary process, where he controls the parameters of the process. We prove that under our assumptions, given a subset of topics that represent generalizations of one another (such as baseball → sports → base), for any document which was produced via some topic in this hierarchy, we can efficiently determine the most specialized topic in this subset, it still belongs to. The quality of the classification is independent of the total number of topics in the hierarchy and our algorithm does not need to know the total number of topics in advance. Our approach also yields an algorithm for clustering and unsupervised topical tree reconstruction. We validate our model by showing that properties predicted by our theoretical results carry over to real data. We then apply our clustering algorithm to two different datasets: (i) "20 newsgroups"[19] and (ii) a snapshot of abstracts of arXiv {2} (15 categories, ~240,000 abstracts). In both cases our algorithm performs extremely well.

#index 989630
#* Knowledge discovery of multiple-topic document using parametric mixture model with dirichlet prior
#@ Issei Sato;Hiroshi Nakagawa
#t 2007
#c 0
#% 279755
#% 465754
#% 577287
#% 722904
#% 891559
#% 1650268
#! Documents, such as those seen on Wikipedia and Folksonomy, have tended to be assigned with multiple topics as a meta-data.Therefore, it is more and more important to analyze a relationship between a document and topics assigned to the document. In this paper, we proposed a novel probabilistic generative model of documents with multiple topics as a meta-data. By focusing on modeling the generation process of a document with multiple topics, we can extract specific properties of documents with multiple topics.Proposed model is an expansion of an existing probabilistic generative model: Parametric Mixture Model (PMM). PMM models documents with multiple topics by mixing model parameters of each single topic. Since, however, PMM assigns the same mixture ratio to each single topic, PMM cannot take into account the bias of each topic within a document. To deal with this problem, we propose a model that considers Dirichlet distribution as a prior distribution of the mixture ratio.We adopt Variational Bayes Method to infer the bias of each topic within a document. We evaluate the proposed model and PMM using MEDLINE corpus.The results of F-measure, Precision and Recall show that the proposed model is more effective than PMM on multiple-topic classification. Moreover, we indicate the potential of the proposed model that extracts topics and document-specific keywords using information about the assigned topics.

#index 989631
#* Using hierarchical clustering for learning theontologies used in recommendation systems
#@ Vincent Schickel-Zuber;Boi Faltings
#t 2007
#c 0
#% 280852
#% 330687
#% 397155
#% 451052
#% 452563
#% 734590
#% 766448
#% 789008
#% 806594
#% 885453
#% 1250380
#% 1274840
#! Ontologies are being successfully used to overcome semanticheterogeneity, and are becoming fundamental elements of the SemanticWeb. Recently, it has also been shown that ontologies can be used tobuild more accurate and more personalized recommendation systems byinferencing missing user's preferences. However, these systemsassume the existence of ontologies, without considering theirconstruction. With product catalogs changing continuously, newtechniques are required in order to build these ontologies in realtime, and autonomously from any expert intervention.This paper focuses on this problem and show that it is possible tolearn ontologies autonomously by using clustering algorithms. Results on the MovieLens and Jester data sets show that recommendersystem with learnt ontologies significantly outperform the classical recommendation approach.

#index 989632
#* Practical learning from one-sided feedback
#@ D. Sculley
#t 2007
#c 0
#% 169717
#% 170649
#% 236729
#% 240794
#% 248225
#% 309859
#% 314872
#% 384911
#% 393059
#% 451055
#% 464268
#% 466419
#% 961177
#% 987244
#! In many data mining applications, online labeling feedback is only available for examples which were predicted to belong to the positive class. Such applications includespam filtering in the case where users never checkemails marked "spam", document retrieval where users cannotgive relevance feedback on unretrieved documents,and online advertising where user behavior cannot beobserved for unshown advertisements. One-sided feedback can cripple the performance of classical mistake-driven online learners such as Perceptron. Previous work under the Apple Tasting framework showed how to transform standard online learners into successful learners from one sided feedback. However, we find in practice that this transformation may request more labels than necessary to achieve strong performance. In this paper,we employ two active learning methods which reduce the number of labels requested in practice. One method is the use of Label Efficient active learning. The other method,somewhat surprisingly, is the use of margin-based learners without modification, which we show combines implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff. Experimental results show that these methods can be significantly more effective in practice than those using the Apple Tasting transformation, even on minority class problems.

#index 989633
#* Information genealogy: uncovering the flow of ideas in non-hyperlinked document databases
#@ Benyah Shaparenko;Thorsten Joachims
#t 2007
#c 0
#% 46803
#% 86375
#% 94453
#% 109189
#% 109219
#% 150129
#% 217248
#% 230519
#% 262043
#% 279755
#% 290830
#% 309533
#% 577220
#% 722904
#% 766431
#% 769906
#% 823344
#% 823386
#% 874462
#% 876007
#% 879575
#% 881498
#% 1650298
#! We now have incrementally-grown databases of text documents ranging back for over a decade in areas ranging from personal email, to news-articles and conference proceedings. While accessing individual documents is easy, methods for overviewing and understanding these collections as a whole are lacking in number and in scope. In this paper, we address one such global analysis task, namely the problem of automatically uncovering how ideas spread through the collection over time. We refer to this problem as Information Genealogy. In contrast to bibliometric methods that are limited to collections with explicit citation structure, we investigate content-based methods requiring only the text and timestamps of the documents. In particular, we propose a language-modeling approach and a likelihood ratio test to detect influence between documents in a statistically well-founded way. Furthermore, we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection. Experiments on the NIPS conference proceedings and the Physics ArXiv show that our method is more effective than methods based on document similarity.

#index 989634
#* A concept-based model for enhancing text categorization
#@ Shady Shehata;Fakhri Karray;Mohamed Kamel
#t 2007
#c 0
#% 321635
#% 406493
#% 452991
#% 458379
#% 708948
#% 727824
#% 823311
#% 915285
#% 935763
#% 972328
#! Most of text categorization techniques are based on word and/or phrase analysis of the text. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes moreto the meaning of its sentences than the other term. Thus, the underlying model should indicate terms that capture these mantics of text. In this case, the model can capture terms that present the concepts of the sentence, which leads todiscover the topic of the document. A new concept-based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced. The concept-based model can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed model consists of concept-based statistical analyzer, conceptual ontological graph representation,and concept extractor. The term which contributes to the sentence semantics is assigned two different weights by the concept-based statistical analyzer and the conceptual ontological graph representation. These two weights are combined into a new weight. The concepts that have maximum combined weights are selected by the concept extractor. A set of experiments using the proposed concept-basedmodel on different datasets in text categorization is conducted. The experiments demonstrate the comparison between traditional weighting and the concept-based weighting obtained by the combined approach of the concept-based statistical analyzer and the conceptual ontological graph. The evaluation of results is relied on two quality measures, the Macro-averaged F1 and the Error rate. These quality measures are improved when the newly developedconcept-based model is used to enhance the quality of thetext categorization.

#index 989635
#* Partial example acquisition in cost-sensitive learning
#@ Victor S. Sheng;Charles X. Ling
#t 2007
#c 0
#% 136350
#% 280437
#% 464268
#% 464639
#% 466580
#% 477640
#% 722797
#% 735358
#% 763705
#% 770791
#% 785338
#% 785413
#% 829982
#% 832575
#% 1272369
#% 1289281
#% 1289639
#% 1673023
#% 1699589
#! It is often expensive to acquire data in real-world data mining applications. Most previous data mining and machine learning research, however, assumes that a fixed set of training examples is given. In this paper, we propose an online cost-sensitive framework that allows a learner to dynamically acquire examples as it learns, and to decide the ideal number of examples needed to minimize the total cost. We also propose a new strategy for Partial Example Acquisition (PAS), in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost. Experiments on UCI datasets show that the new PAS strategy is an effective method in reducing the total cost for data acquisition.

#index 989636
#* A spectral clustering approach to optimally combining numericalvectors with a modular network
#@ Motoki Shiga;Ichigaku Takigawa;Hiroshi Mamitsuka
#t 2007
#c 0
#% 313959
#% 466890
#% 734917
#% 739636
#% 769881
#% 769935
#% 826918
#% 840892
#! We address the issue of clustering numerical vectors with a network. The problem setting is basically equivalent to constrained clustering by Wagstaff and Cardie and semi-supervised clustering by Basu et al., but our focus is more on the optimal combination of two heterogeneous data sources. An application of this setting is web pages which can be numerically vectorized by their contents, e.g. term frequencies, and which are hyperlinked to each other, showing a network. Another typical application is genes whose behavior can be numerically measured and a gene network can be given from another data source.We first define a new graph clustering measure which we call normalized network modularity, by balancing the cluster size of the original modularity. We then propose a new clustering method which integrates the cost of clustering numerical vectors with the cost of maximizing the normalized network modularity into a spectral relaxation problem. Our learning algorithm is based on spectral clustering which makes our issue an eigenvalue problem and uses k-means for final cluster assignments. A significant advantage of our method is that we can optimize the weight parameter for balancing the two costs from the given data by choosing the minimum total cost. We evaluated the performance of our proposed method using a variety of datasets including synthetic data as well as real-world data from molecular biology. Experimental results showed that our method is effective enough to have good results for clustering by numerical vectors and a network.

#index 989637
#* Making generative classifiers robust to selection bias
#@ Andrew T. Smith;Charles Elkan
#t 2007
#c 0
#% 17144
#% 61272
#% 577298
#% 716892
#% 769904
#% 770847
#% 983814
#% 983939
#% 1579122
#! This paper presents approaches to semi-supervised learning when the labeled training data and test data are differently distributed. Specifically, the samples selected for labeling are a biased subset of some general distribution and the test set consists of samples drawn from either that general distribution or the distribution of the unlabeled samples. An example of the former appears in loan application approval, where samples with repay/default labels exist only for approved applicants and the goal is to model the repay/default behavior of all applicants. An example of the latter appears in spam filtering, in which the labeled samples can be out-dated due to the cost of labeling email by hand, but an unlabeled set of up-to-date emails exists and the goal is to build a filter to sort new incoming email.Most approaches to overcoming such bias in the literature rely on the assumption that samples are selected for labeling depending only on the features, not the labels, a case in which provably correct methods exist. The missing labels are said to be "missing at random" (MAR). In real applications, however, the selection bias can be more severe. When the MAR conditional independence assumption is not satisfied and missing labels are said to be "missing not at random" (MNAR), and no learning method is provably always correct.We present a generative classifier, the shifted mixture model (SMM), with separate representations of the distributions of the labeled samples and the unlabeled samples. The SMM makes no conditional independence assumptions and can model distributions of semi-labeled data sets with arbitrary bias in the labeling. We present a learning method based on the expectation maximization (EM) algorithm that, while not always able to overcome arbitrary labeling bias, learns SMMs with higher test-set accuracy in real-world data sets (with MNAR bias) than existing learning methods that are proven to overcome MAR bias.

#index 989638
#* Statistical change detection for multi-dimensional data
#@ Xiuyao Song;Mingxi Wu;Christopher Jermaine;Sanjay Ranka
#t 2007
#c 0
#% 300136
#% 420126
#% 444070
#% 570886
#% 654489
#% 769901
#% 847161
#% 881458
#! This paper deals with detecting change of distribution in multi-dimensional data sets. For a given baseline data set and a set of newly observed data points, we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set. We define a test statistic that is strictly distribution-free under the null hypothesis. Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection.

#index 989639
#* Use of ranked cross document evidence trails for hypothesis generation
#@ Rohini K. Srihari;Li Xu;Tushar Saxena
#t 2007
#c 0
#% 39703
#% 249110
#% 387427
#% 746871
#% 752367
#% 769887
#% 780683
#% 786511
#% 818227
#% 853647
#% 868088
#% 939349
#% 1041075
#! This paper focuses on detecting how concepts are linked across multiple textdocuments by generating an evidence trail explaining the connection. A traditional search involving, for example, two or more person names willattempt to find documents mentioning both of these individuals. This researchfocuses on a different interpretation of such a query: what is the best evidencetrail across documents that explains a connection between these individuals? For example, allmay be good golfers. A generalization ofthis task involves query terms representing general concepts (e.g. indictment,foreign policy). Such queries reflect a special case oftext mining. Previous attempts to solve this problem have focused on graphapproaches involving hyperlinked documents, and link analysis tools exploiting named entities. A new robust framework is presented, based on (i) generating concept chain graphs, a hybrid content representation, (ii) performing graph matching to select candidate subgraphs, and (iii) subsequently using graphical models to validate hypotheses using ranked evidence trails. We adapt the DUC data set for cross-document summarization to evaluate evidence trails generated by this approach.

#index 989640
#* GraphScope: parameter-free mining of large time-evolving graphs
#@ Jimeng Sun;Christos Faloutsos;Spiros Papadimitriou;Philip S. Yu
#t 2007
#c 0
#% 115608
#% 258598
#% 283833
#% 479969
#% 729918
#% 729983
#% 769883
#% 769896
#% 799747
#% 800526
#% 823342
#% 881460
#% 881493
#% 1016130
#% 1016172
#! How can we find communities in dynamic networks of socialinteractions, such as who calls whom, who emails whom, or who sells to whom? How can we spot discontinuity time-points in such streams of graphs, in an on-line, any-time fashion? We propose GraphScope, that addresses both problems, using information theoretic principles. Contrary to the majority of earlier methods, it needs no user-defined parameters. Moreover, it is designed to operate on large graphs, in a streaming fashion. We demonstrate the efficiency and effectiveness of our GraphScope on real datasets from several diverse domains. In all cases it produces meaningful time-evolving patterns that agree with human intuition.

#index 989641
#* Weighting versus pruning in rule validation for detecting network and host anomalies
#@ Gaurav Tandon;Philip K. Chan
#t 2007
#c 0
#% 165663
#% 188026
#% 226674
#% 321553
#% 346478
#% 423633
#% 451055
#% 481290
#% 616967
#% 664547
#% 664711
#% 727871
#% 790040
#% 862157
#% 864875
#% 963788
#% 963838
#% 978633
#! For intrusion detection, the LERAD algorithm learns a succinct set of comprehensible rules for detecting anomalies, which could be novel attacks. LERAD validates the learned rules on a separate held-out validation set and removes rules that cause false alarms. However, removing rules with possible high coverage can lead to missed detections. We propose to retain these rules and associate weights to them. We present three weighting schemes and our empirical results indicate that, for LERAD, rule weighting can detect more attacks than pruning with minimal computational overhead.

#index 989642
#* Enhancing semi-supervised clustering: a feature projection perspective
#@ Wei Tang;Hui Xiong;Shi Zhong;Jie Wu
#t 2007
#c 0
#% 80995
#% 115608
#% 248792
#% 272536
#% 273891
#% 291942
#% 329562
#% 464291
#% 464631
#% 729437
#% 729918
#% 765518
#% 769881
#% 800529
#! Semi-supervised clustering employs limited supervision in the form of labeled instances or pairwise instance constraints to aid unsupervised clustering and often significantly improves the clustering performance. Despite the vast amount of expert knowledge spent on this problem, most existing work is not designed for handling high-dimensional sparse data. This paper thus fills this crucial void by developing a Semi-supervised Clustering method based on spheRical K-mEans via fEature projectioN (SCREEN). Specifically, we formulate the problem of constraint-guided feature projection, which can be nicely integrated with semi-supervised clustering algorithms and has the ability to effectively reduce data dimension. Indeed, our experimental results on several real-world data sets show that the SCREEN method can effectively deal with high-dimensional data and provides an appealing clustering performance.

#index 989643
#* A framework for community identification in dynamic social networks
#@ Chayant Tantipathananandh;Tanya Berger-Wolf;David Kempe
#t 2007
#c 0
#% 164166
#% 166862
#% 249110
#% 281214
#% 438553
#% 494764
#% 729968
#% 754107
#% 853532
#% 871315
#% 881460
#% 881509
#% 1414755
#! We propose frameworks and algorithms for identifying communities in social networks that change over time. Communities are intuitively characterized as "unusually densely knit" subsets of a social network. This notion becomes more problematic if the social interactions change over time. Aggregating social networks over time can radically misrepresent the existing and changing community structure. Instead, we propose an optimization-based approach for modeling dynamic community structure. We prove that finding the most explanatory community structure is NP-hard and APX-hard, and propose algorithms based on dynamic programming, exhaustive search, maximum matching, and greedy heuristics. We demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and real-world examples.

#index 989644
#* A scalable modular convex solver for regularized risk minimization
#@ Choon Hui Teo;Alex Smola;S. V.N. Vishwanathan;Quoc Viet Le
#t 2007
#c 0
#% 190265
#% 269206
#% 269217
#% 304824
#% 388024
#% 464434
#% 493884
#% 562963
#% 575969
#% 722815
#% 783478
#% 816181
#% 829007
#% 829043
#% 840882
#% 855494
#% 855602
#% 879624
#% 881477
#% 961178
#% 1000452
#% 1815826
#! A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as l1 and l2 penalties. At present, our solver implements 20 different estimation problems, can be easily extended, scales to millions of observations, and is up to 10 times faster than specialized solvers for many applications. The open source code is freely available as part of the ELEFANT toolbox.

#index 989645
#* Fast best-effort pattern matching in large attributed graphs
#@ Hanghang Tong;Christos Faloutsos;Brian Gallagher;Tina Eliassi-Rad
#t 2007
#c 0
#% 445309
#% 451536
#% 629708
#% 660011
#% 740266
#% 765429
#% 769887
#% 769952
#% 823347
#% 823391
#% 841960
#% 881480
#% 881496
#% 893105
#% 915344
#% 1719486
#! We focus on large graphs where nodes have attributes, such as a social network where the nodes are labelled with each person's job title. In such a setting, we want to find subgraphs that match a user query pattern. For example, a "star" query would be, "find a CEO who has strong interactions with a Manager, a Lawyer,and an Accountant, or another structure as close to that as possible". Similarly, a "loop" query could help spot a money laundering ring. Traditional SQL-based methods, as well as more recent graph indexing methods, will return no answer when an exact match does not exist. This is the first main feature of our method. It can find exact-, as well as near-matches, and it will present them to the user in our proposed "goodness" order. For example, our method tolerates indirect paths between, say, the "CEO" and the "Accountant" of the above sample query, when direct paths don't exist. Its second feature is scalability. In general, if the query has nq nodes and the data graph has n nodes, the problem needs polynomial time complexity O(n n q), which is prohibitive. Our G-Ray ("Graph X-Ray") method finds high-quality subgraphs in time linear on the size of the data graph. Experimental results on the DLBP author-publication graph (with 356K nodes and 1.9M edges) illustrate both the effectiveness and scalability of our approach. The results agree with our intuition, and the speed is excellent. It takes 4 seconds on average fora 4-node query on the DBLP graph.

#index 989646
#* Fast direction-aware proximity for graph mining
#@ Hanghang Tong;Christos Faloutsos;Yehuda Koren
#t 2007
#c 0
#% 283833
#% 342596
#% 348173
#% 562963
#% 730089
#% 769887
#% 769952
#% 780688
#% 844334
#% 881480
#% 881496
#% 994033
#% 1016175
#% 1016176
#! In this paper we study asymmetric proximity measures on directed graphs, which quantify the relationships between two nodes or two groups of nodes. The measures are useful in several graph mining tasks, including clustering, link prediction and connection subgraph discovery. Our proximity measure is based on the conceptof escape probability. This way, we strive to summarize the multiple facets of nodes-proximity, while avoiding some of the pitfalls to which alternative proximity measures are susceptible. A unique feature of the measures is accounting for the underlying directional information. We put a special emphasis on computational efficiency, and develop fast solutions that are applicable in several settings. Our experimental study shows the usefulness of our proposed direction-aware proximity method for several applications, and that our algorithms achieve a significant speedup (up to 50,000x) over straight forward implementations.

#index 989647
#* Scalable look-ahead linear regression trees
#@ David S. Vogel;Ognian Asparouhov;Tobias Scheffer
#t 2007
#c 0
#% 465897
#% 498791
#% 577264
#% 644745
#% 846426
#% 926881
#! Most decision tree algorithms base their splitting decisions on a piecewise constant model. Often these splitting algorithms are extrapolated to trees with non-constant models at the leaf nodes. The motivation behind Look-ahead Linear Regression Trees (LLRT) is that out of all the methods proposed to date, there has been no scalable approach to exhaustively evaluate all possible models in the leaf nodes in order to obtain an optimal split. Using several optimizations, LLRT is able to generate and evaluate thousands of linear regression models per second. This allows for a near-exhaustive evaluation of all possible splits in a node, based on the quality of fit of linear regression models in the resulting branches. We decompose the calculation of the Residual Sum of Squares in such a way that a large part of it is pre-computed. The resulting method is highly scalable. We observe it to obtain high predictive accuracy for problems with strong mutual dependencies between attributes. We report on experiments with two simulated and seven real data sets.

#index 989648
#* Characterising the difference
#@ Jilles Vreeken;Matthijs van Leeuwen;Arno Siebes
#t 2007
#c 0
#% 280409
#% 769896
#% 814195
#% 972338
#% 1663670
#% 1815364
#% 1815525
#! Characterising the differences between two databases is an often occurring problem in Data Mining. Detection of change over time is a prime example, comparing databases from two branches is another one. The key problem is to discover the patterns that describe the difference. Emerging patterns provide only a partial answer to this question. In previous work, we showed that the data distribution can be captured in a pattern-based model using compression [12]. Here, we extend this approach to define a generic dissimilarity measure on databases. Moreover, we show that this approach can identify those patterns that characterise the differences between two distributions. Experimental results show that our method provides a well-founded way to independently measure database dissimilarity that allows for thorough inspection of the actual differences. This illustrates the use of our approach in real world data mining.

#index 989649
#* Privacy-preservation for gradient descent methods
#@ Li Wan;Wee Keong Ng;Shuguo Han;Vincent C. S. Lee
#t 2007
#c 0
#% 376266
#% 577289
#% 635215
#% 653942
#% 729930
#% 874166
#% 923645
#% 1290046
#% 1669943
#% 1706194
#% 1722551
#! Gradient descent is a widely used paradigm for solving many optimization problems. Stochastic gradient descent performs a series of iterations to minimize a target function in order to reach a local minimum. In machine learning or data mining, this function corresponds to a decision model that is to be discovered. The gradient descent paradigm underlies many commonly used techniques in data mining and machine learning, such as neural networks, Bayesian networks, genetic algorithms, and simulated annealing. To the best of our knowledge, there has not been any work that extends the notion of privacy preservation or secure multi-party computation to gradient-descent-based techniques. In this paper, we propose a preliminary approach to enable privacy preservation in gradient descent methods in general and demonstrate its feasibility in specific gradient descent methods.

#index 989650
#* Mining correlated bursty topic patterns from coordinated text streams
#@ Xuanhui Wang;ChengXiang Zhai;Xiao Hu;Richard Sproat
#t 2007
#c 0
#% 262042
#% 262043
#% 280819
#% 287196
#% 309096
#% 309100
#% 340897
#% 342600
#% 481609
#% 577220
#% 577360
#% 722904
#% 765412
#% 769927
#% 769967
#% 805839
#% 818916
#% 823344
#% 823405
#% 824666
#% 869516
#% 881529
#% 918001
#% 939510
#! Previous work on text mining has almost exclusively focused on a single stream. However, we often have available multiple text streams indexed by the same set of time points (called coordinated text streams), which offer new opportunities for text mining. For example, when a major event happens, all the news articles published by different agencies in different languages tend to cover the same event for a certain period, exhibiting a correlated bursty topic pattern in all the news article streams. In general, mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams. In this paper, we define and study this novel text mining problem. We propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies (e.g., English vs Chinese). Evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets: the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles (in English and Chinese, respectively), while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research. Since the proposed method is general and does not require the streams to share vocabulary, it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period.

#index 989651
#* Generalized component analysis for text with heterogeneous attributes
#@ Xuerui Wang;Chris Pal;Andrew McCallum
#t 2007
#c 0
#% 269188
#% 278011
#% 406493
#% 450888
#% 722904
#% 750863
#% 769906
#% 788043
#% 875959
#% 875987
#% 881498
#% 881502
#% 983903
#% 1250575
#% 1289476
#% 1650298
#% 1650387
#! We present a class of richly structured, undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities. Our model generalizes techniques such as principal component analysis to heterogeneous data types. In contrast to other approaches, this framework allows modalities such as words, authors and timestamps to be captured in their natural, probabilistic encodings. A latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method. We demonstrate the effectiveness of our framework on the task of author prediction from 13 years of the NIPS conference proceedings and for a recipient prediction task using a 10-month academic email archive of a researcher. Our approach should be more broadly applicable to many real-world applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework.

#index 989652
#* Mining favorable facets
#@ Raymond Chi-Wing Wong;Jian Pei;Ada Wai-Chee Fu;Ke Wang
#t 2007
#c 0
#% 287414
#% 289148
#% 427199
#% 465167
#% 480671
#% 800512
#% 806212
#% 810024
#% 824671
#% 824672
#% 864451
#% 875011
#% 915803
#% 992635
#% 993954
#% 1408811
#% 1712421
#! The importance of dominance and skyline analysis has been well recognized in multi-criteria decision making applications. Most previous studies assume a fixed order on the attributes. In practice, different customers may have different preferences on nominal attributes. In this paper, we identify an interesting data mining problem, finding favorable facets, which has not been studied before. Given a set of points in a multidimensional space, for a specific target point p we want to discover with respect to which combinations of orders (e.g., customer preferences) on the nominal attributes p is not dominated by any other points. Such combinations are called the favorable facets of p. We consider both the effectiveness and the efficiency of the mining. A given point may have many favorable facets. We propose the notion of minimal disqualifying condition (MDC) which is effective in summarizing favorable facets. We develop efficient algorithms for favorable facet mining for different application scenarios. The first method computes favorable facets on the fly. The second method pre-computes all minimal disqualifying conditions so that the favorable facets can be looked up in constant time. An extensive performance study using both synthetic and real data sets is reported to verify their effectiveness and efficiency.

#index 989653
#* Local decomposition for rare class analysis
#@ Junjie Wu;Hui Xiong;Peng Wu;Jian Chen
#t 2007
#c 0
#% 93228
#% 252836
#% 260149
#% 280437
#% 309208
#% 333934
#% 466268
#% 577241
#% 727925
#% 729437
#% 765520
#% 829253
#% 835018
#% 1271973
#% 1289281
#! Given its importance, the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great attentions in the literature. However, the rare-class problem remains a critical challenge, because there is no natural way developed for handling imbalanced class distributions. This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG (COG). Specifically, for a data set with an imbalanced class distribution, we perform clustering within each large class and produce sub-classes with relatively balanced sizes. Then, we apply traditional supervised learning algorithms, such as Support Vector Machines (SVMs), for classification. Indeed, our experimental results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods. Furthermore, we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions.

#index 989654
#* SCAN: a structural clustering algorithm for networks
#@ Xiaowei Xu;Nurcan Yuruk;Zhidan Feng;Thomas A. J. Schweiger
#t 2007
#c 0
#% 282905
#% 283833
#% 313959
#% 342596
#% 466675
#% 1394202
#! Network clustering (or graph partitioning) is an important task for the discovery of underlying structures in networks. Many algorithms find clusters by maximizing the number of intra-cluster edges. While such algorithms find useful and interesting structures, they tend to fail to identify and isolate two kinds of vertices that play special roles - vertices that bridge clusters (hubs) and vertices that are marginally connected to clusters (outliers). Identifying hubs is useful for applications such as viral marketing and epidemiology since hubs are responsible for spreading ideas or disease. In contrast, outliers have little or no influence, and may be isolated as noise in the data. In this paper, we proposed a novel algorithm called SCAN (Structural Clustering Algorithm for Networks), which detects clusters, hubs and outliers in networks. It clusters vertices based on a structural similarity measure. The algorithm is fast and efficient, visiting each vertex only once. An empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other methods such as the modularity-based algorithms.

#index 989655
#* Model-shared subspace boosting for multi-label classification
#@ Rong Yan;Jelena Tesic;John R. Smith
#t 2007
#c 0
#% 209021
#% 235377
#% 236497
#% 256615
#% 269217
#% 311034
#% 400847
#% 465751
#% 465754
#% 722927
#% 769950
#% 770854
#% 823349
#% 838412
#% 839962
#% 871052
#% 888942
#% 1389537
#! Typical approaches to the multi-label classification problem require learning an independent classifier for every label from all the examples and features. This can become a computational bottleneck for sizeable datasets with a large label space. In this paper, we propose an efficient and effective multi-label learning algorithm called model-shared subspace boosting (MSSBoost) as an attempt to reduce the information redundancy in the learning process. This algorithm automatically finds, shares and combines a number of base models across multiple labels, where each model is learned from random feature subspace and boots trap data samples. The decision functions for each label are jointly estimated and thus a small number of shared subspace models can support the entire label space. Our experimental results on both synthetic data and real multimedia collections have demonstrated that the proposed algorithm can achieve better classification performance than the non-ensemble baselineclassifiers with a significant speedup in the learning and prediction processes. It can also use a smaller number of base models to achieve the same classification performance as its non-model-shared counterpart.

#index 989656
#* Detecting time series motifs under uniform scaling
#@ Dragomir Yankov;Eamonn Keogh;Jose Medina;Bill Chiu;Victor Zordan
#t 2007
#c 0
#% 232767
#% 322309
#% 328321
#% 469571
#% 729437
#% 729960
#% 748556
#% 799397
#% 893161
#% 1016194
#! Time series motifs are approximately repeated patterns foundwithin the data. Such motifs have utility for many data mining algorithms, including rule-discovery,novelty-detection, summarization and clustering. Since the formalization of the problem and the introduction of efficient linear time algorithms, motif discovery has been successfully applied tomany domains, including medicine, motion capture, robotics and meteorology. In this work we show that most previous applications of time series motifs have been severely limited by the definition's brittleness to even slight changes of uniform scaling, the speed at which the patterns develop. We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling, and show that it produces objectively superior results in several important domains. Apart from being more general than all other motifdiscovery algorithms, a further contribution of our work isthat it is simpler than previous approaches, in particular we have drastically reduced the number of parameters that need to be specified.

#index 989657
#* Learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming
#@ Jieping Ye;Shuiwang Ji;Jianhui Chen
#t 2007
#c 0
#% 209961
#% 269218
#% 309208
#% 443790
#% 743284
#% 757953
#% 763697
#% 770831
#% 770846
#% 770848
#% 829010
#% 832903
#% 875950
#% 876003
#% 876014
#% 961190
#% 983941
#% 983953
#! The kernel function plays a central role in kernel methods. In this paper, we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Analysis (RKDA), which performs lineardiscriminant analysis in the feature space via the kernel trick. Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program (SDP), which is however computationally expensive, even with the recent advances in interior point methods. Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a Quadratically Constrained Quadratic Programming (QCQP) formulation for the kernel learning problem, which can be solved more efficiently than SDP. While most existing work on kernel learning deal with binary-class problems only, we show that our QCQP formulation can be extended naturally to the multi-class case. Experimental results on both binary-class and multi-class benchmarkdata sets show the efficacy of the proposed QCQP formulations.

#index 989658
#* From frequent itemsets to semantically meaningful visual patterns
#@ Junsong Yuan;Ying Wu;Ming Yang
#t 2007
#c 0
#% 152934
#% 300120
#% 313959
#% 316709
#% 342610
#% 632037
#% 729970
#% 760805
#% 762472
#% 769876
#% 769881
#% 769914
#% 784509
#% 823356
#% 824931
#% 836829
#% 853308
#% 881489
#% 881500
#% 881542
#% 985041
#% 1502463
#% 1727349
#! Data mining techniques that are successful in transaction and text data may not be simply applied to image data that contain high-dimensional features and have spatial structures. It is not a trivial task to discover meaningful visual patterns in image databases, because the content variations and spatial dependency in the visual data greatly challenge most existing methods. This paper presents a novel approach to coping with these difficulties for mining meaningful visual patterns. Specifically, the novelty of this work lies in the following new contributions: (1) a principled solution to the discovery of meaningful itemsets based on frequent itemset mining; (2) a self-supervised clustering scheme of the high-dimensional visual features by feeding back discovered patterns to tune the similarity measure through metric learning; and (3) a pattern summarization method that deals with the measurement noises brought by the image data. The experimental results in the real images show that our method can discover semantically meaningful patterns efficiently and effectively.

#index 989659
#* Information distance from a question to an answer
#@ Xian Zhang;Yu Hao;Xiaoyan Zhu;Ming Li;David R. Cheriton
#t 2007
#c 0
#% 234979
#% 275247
#% 397674
#% 397675
#% 397677
#% 397678
#% 397679
#% 577214
#% 654745
#% 664387
#% 730022
#% 769896
#% 794511
#% 830880
#% 849867
#% 857308
#% 867119
#% 883475
#% 905998
#% 939376
#% 939912
#% 975019
#% 1809406
#% 1815177
#% 1815364
#% 1815525
#! We provide three key missing pieces of a general theory of information distance [3, 23, 24]. We take bold steps in formulating a revised theory to avoid some pitfalls in practical applications. The new theory is then used to construct a question answering system. Extensive experiments are conducted to justify the new theory.

#index 989660
#* Mining templates from search result records of search engines
#@ Hongkun Zhao;Weiyi Meng;Clement Yu
#t 2007
#c 0
#% 248808
#% 271065
#% 273925
#% 275915
#% 330784
#% 344448
#% 397605
#% 480479
#% 480648
#% 480824
#% 577319
#% 654469
#% 660272
#% 724635
#% 729978
#% 730038
#% 805845
#% 805846
#% 805847
#% 838491
#% 869518
#% 881505
#% 889107
#% 1683893
#% 1683908
#! Metasearch engine, Comparison-shopping and Deep Web crawling applications need to extract search result records enwrapped in result pages returned from search engines in response to user queries. The search result records from a given search engine are usually formatted based on a template. Precisely identifying this template can greatly help extract and annotate the data units within each record correctly. In this paper, we propose a graph model to represent record template and develop a domain independent statistical method to automatically mine the record template for any search engine using sample search result records. Our approach can identify both template tags (HTML tags) and template texts (non-tag texts), and it also explicitly addresses the mismatches between the tag structures and the data structures of search result records. Our experimental results indicate that this approach is very effective.

#index 989661
#* Joint optimization of wrapper generation and template detection
#@ Shuyi Zheng;Ruihua Song;Ji-Rong Wen;Di Wu
#t 2007
#c 0
#% 46809
#% 271065
#% 275915
#% 330784
#% 346637
#% 348146
#% 397605
#% 480824
#% 632051
#% 654469
#% 729978
#% 754108
#% 779889
#% 805845
#% 805847
#% 938578
#! Many websites have large collections of pages generated dynamically from an underlying structured source like a database. The data of a category are typically encoded into similar pages by a common script or template. In recent years, some value-added services, such as comparison shopping and vertical search in a specific domain, have motivated the research of extraction technologies with high accuracy. Almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of URL. However, we observed that it is hard to distinguish different templates using dynamic URLs today. Moreover, since extraction accuracy heavily depends on how consistent input pages are, we argue that it is risky to determine whether pages share a common template solely based on URLs. Instead, we propose a new approach that utilizes similarity between pages to detect templates. Our approach separates pages with notable inner differences and then generates wrappers, respectively. Experimental results show that our proposed approach is feasible and effective for improving extraction accuracy.

#index 989662
#* Webpage understanding: an integrated approach
#@ Jun Zhu;Bo Zhang;Zaiqing Nie;Ji-Rong Wen;Hsiao-Wuen Hon
#t 2007
#c 0
#% 266216
#% 273925
#% 278109
#% 279755
#% 312860
#% 330784
#% 333943
#% 464434
#% 480824
#% 654469
#% 723243
#% 765411
#% 769884
#% 805845
#% 805846
#% 812412
#% 815918
#% 839840
#% 840966
#% 853865
#% 876044
#% 881505
#% 881539
#% 1271981
#% 1299530
#! Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling HTML elements. However, how to effectively segment and label the text contents inside HTML elements is still an open problem. Since many text contents on a webpage are often text fragments and not strictly grammatical, traditional natural language processing techniques, that typically expect grammatical sentences, are no longer directly applicable. In this paper, we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages. We propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model. In this model, semantic labels of page structure can be leveraged to help text content understanding, and semantic labels ofthe text phrases can be used in page structure understanding tasks such as data record detection. Thus, integration of both page structure and text content understanding leads to an integrated solution of webpage understanding. Experimental results on research homepage extraction show the feasibility and promise of our approach.

#index 989663
#* An event-based framework for characterizing the evolutionary behavior of interaction graphs
#@ Sitaram Asur;Srinivasan Parthasarathy;Duygu Ucar
#t 2007
#c 0
#% 164236
#% 438553
#% 650944
#% 729923
#% 730089
#% 822606
#% 823342
#% 881460
#% 881514
#% 881538
#% 961565
#% 1676017
#! Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs from a static point of view. The study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities, communities and the flow of information among them. In this work, we present an event-based characterization of critical behavioral patterns for temporally varying interaction graphs. We use non-overlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them. We use these events to characterize complex behavioral patterns of individuals and communities over time. We demonstrate the application of behavioral patterns for the purposes of modeling evolution, link prediction and influence maximization. Finally, we present a diffusion model for evolving networks, based on our framework.

#index 989664
#* On-board analysis of uncalibrated data for a spacecraft at mars
#@ Rebecca Castano;Kiri L. Wagstaff;Steve Chien;Timothy M. Stough;Benyang Tang
#t 2007
#c 0
#% 876060
#% 881564
#! Analyzing data on-board a spacecraft as it is collected enables several advanced spacecraft capabilities, such as prioritizing observations to make the best use of limited bandwidth and reacting to dynamic events as they happen. In this paper, we describe how we addressed the unique challenges associated with on-board mining of data as it is collected: uncalibrated data, noisy observations, and severe limitations on computational and memory resources. The goal of this effort, which falls into the emerging application area of spacecraft-based data mining, was to study three specific science phenomena on Mars. Following previous work that used a linear support vector machine (SVM) on-board the Earth Observing 1 (EO-1)spacecraft, we developed three data mining techniques for use on-board the Mars Odyssey spacecraft. These methods range from simple thresholding to state-of-the-art reduced-set SVM technology. We tested these algorithms on archived data in a flight software testbed. We also describe a significant, serendipitous science discovery of this data mining effort: the confirmation of a water ice annulus around the north polar cap of Mars. We conclude with a discussion on lessons learned in developing algorithms for use on-board a spacecraft.

#index 989665
#* iLink: search and routing in social networks
#@ Jeffrey Davitz;Jiye Yu;Sugato Basu;David Gutelius;Alexandra Harris
#t 2007
#c 0
#% 220708
#% 268079
#% 300078
#% 410602
#% 722141
#% 722934
#% 754122
#% 754162
#% 769881
#% 807416
#% 814436
#% 823342
#% 836506
#% 853535
#% 924327
#% 1289476
#! The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks. This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration. We propose a general interaction model for the underlying social networks and then a specific model (iLink for social search and message routing. A key contribution here is the development of a general learning framework for making such online peer production systems work at scale. The iLink model has been used to develop a system for FAQ generation in a social network (FAQtory), and experience with its application in the context of a full-scale learning-driven workflow application (CALO) is reported. We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems. Finally, the paper shows the connection of iLink to SQM, a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model.

#index 989666
#* Relational data pre-processing techniques for improved securities fraud detection
#@ Andrew Fast;Lisa Friedland;Marc Maier;Brian Taylor;David Jensen;Henry G. Goldberg;John Komoroske
#t 2007
#c 0
#% 549441
#% 580510
#% 729982
#% 823370
#% 989600
#% 1393858
#! Commercial datasets are often large, relational, and dynamic. They contain many records of people, places, things, events and their interactions over time. Such datasets are rarely structured appropriately for knowledge discovery, and they often contain variables whose meanings change across different subsets of the data. We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers(NASD). We describe several methods for data pre-processing that we applied to transform a large, dynamic, and relational dataset describing nearly the entirety of the U.S. securities industry, and we show how these methods made the dataset suitable for learning statistical relational models. To better utilize social structure, we first applied known consolidation and link formation techniques to associate individuals with branch office locations. In addition, we developed an innovative technique to infer professional associations by exploiting dynamic employment histories. Finally, we applied normalization techniques to create a suitable class label that adjusts for spatial, temporal, and other heterogeneity within the data. We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity.

#index 989667
#* Cleaning disguised missing data: a heuristic approach
#@ Ming Hua;Jian Pei
#t 2007
#c 0
#% 17144
#% 798813
#% 878941
#% 1272290
#! In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely, such as causing significant biases and misleading results in hypothesis tests, correlation analysis and regressions. The very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection. They highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. To tackle the problem of cleaning disguised missing data, in this paper, we first model the distribution of disguised missing data, and propose the embedded unbiased sample heuristic. Then, we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data. Our method does not require any domain background knowledge to find the suspicious disguise values. We report an empirical evaluation using real data sets, which shows that our method is effective - the frequently used disguise values found by our method match the values identified by the domain experts nicely. Our method is also efficient and scalable for processing large data sets.

#index 989668
#* Practical guide to controlled experiments on the web: listen to your customers not to the hippo
#@ Ron Kohavi;Randal M. Henne;Dan Sommerfield
#t 2007
#c 0
#% 402269
#% 768666
#% 918887
#% 935120
#% 1086734
#! The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments (single factor or factorial designs), A/B tests (and their generalizations), split tests, Control/Treatment tests, and parallel flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users can help guide the development of features. Our experience indicates that significant learning and return-on-investment (ROI) are seen when development teams listen to their customers, not to the Highest Paid Person's Opinion (HiPPO). We provide several examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.

#index 989669
#* Distributed classification in peer-to-peer networks
#@ Ping Luo;Hui Xiong;Kevin Lü;Zhongzhi Shi
#t 2007
#c 0
#% 209021
#% 342628
#% 424994
#% 424996
#% 431101
#% 593109
#% 595208
#% 738972
#% 763710
#% 765444
#% 823842
#% 884465
#% 888878
#% 961134
#% 999576
#% 1742091
#% 1781523
#! This work studies the problem of distributed classification in peer-to-peer(P2P) networks. While there has been a significant amount of work in distributed classification, most of existing algorithms are not designed for P2P networks. Indeed, as server-less and router-less systems, P2P networks impose several challenges for distributed classification: (1) it is not practical to have global synchronization in large-scale P2P networks; (2)there are frequent topology changes caused by frequent failure and recovery of peers; and (3) there are frequent on-the-fly data updates on each peer. In this paper, we propose an ensemble paradigm for distributed classification in P2P networks. Under this paradigm, each peer builds its local classifiers on the local data and the results from all local classifiers are then combined by plurality voting. To build local classifiers, we adopt the learning algorithm of pasting bites to generate multiple local classifierson each peer based on the local data. To combine local results, we propose a general form of Distributed Plurality Voting (DPV) protocol in dynamic P2P networks. This protocol keeps the single-site validity for dynamic networks, and supports the computing modes of both one-shot query and continuous monitoring. We theoretically prove that the condition (BOB CHECK THIS 'C')ω0 for sending messages used in DPV0 is locally communication-optimal to achieve the above properties. Finally, experimental results on real-world P2P networks show that: (1) the proposed ensemble paradigm is effective even if there are thousands of local classifiers; (2) in most cases, the DPV0 algorithm is local in the sense that voting is processed using information gathered from a very small vicinity, whose size is independent of the network size; (3) DPV0 is significantly more communication-efficient than existing algorithms for distributed plurality voting.

#index 989670
#* High-quantile modeling for customer wallet estimation and other applications
#@ Claudia Perlich;Saharon Rosset;Richard D. Lawrence;Bianca Zadrozny
#t 2007
#c 0
#% 209021
#% 465897
#% 961168
#% 961178
#% 1062937
#! In this paper we discuss the important practical problem of customer wallet estimation, i.e., estimation of potential spending by customers(rather than their expected spending). For this purpose we utilize quantile modeling, whose goal is to estimate a quantile of the discriminative conditional distribution of the response, rather than the mean, which is the implicit goal of most standard regression approaches. We argue that a notion of wallet can be captured through high quantile modeling (e.g, estimating the 90th percentile), and describe a wallet estimation implementation within IBM's Market Alignment Program (MAP). We also discuss the wide range of domains where high-quantile modeling can be practically important: estimating opportunities in sales and marketing domains, defining 'surprising' patterns for outlier and fraud detection and more. We survey some existing approaches for quantile modeling, and propose adaptations of nearest-neighbor and regression-tree approaches to quantile modeling. We demonstrate the various models' performance in high quantile estimation in several domains, including our motivating problem of estimating the 'realistic' IT wallets of IBM customers.

#index 989671
#* Mining complex power networks for blackout prevention
#@ Jun Hua Zhao;Zhao Yang Dong;Pei Zhang
#t 2007
#c 0
#% 227919
#% 342604
#% 393059
#% 577218
#% 629603
#% 727896
#% 729933
#% 729979
#% 765552
#% 769940
#% 881475
#% 881557
#! Following the recent devastating blackouts in North America, UK and Italy, blackout prevention has attracted significant attention, though it is known as a notoriously difficult task. To prevent the blackout, it is essential to accurately predict the instable status of power network components. In the large-scale power network however, existing analysis tools fail to perform accurate and in-time prediction of component instability, because of the sophisticated structure of real-world power networks and the huge amount of system variables to be analyzed. To prevent the blackout, we need an accurate and efficient method that (a) can discover interesting features and patterns relevant to the blackout, from the highly complex structure and ten thousands of system variables of a power network, and (b) can give accurate and fast prediction of system instability whenever required, so that the network operator can take necessary actions in time. In this paper, we report our tool developed for power network instability prediction. The proposed method consists of two major stages. In the first stage,a novel type of patterns namely Local Correlation Network Pattern (LCNP) is mined from the structure and system variables of the power network. Correlation rules, which are useful for the network operator to locate potentially instable components, can be further generated from the LCNP. In the second stage, a kernel based network classification method is developed to predict the system instability. By testing on a real world power network (the New England system), we demonstrate that the proposed tool is effective in predicting system instability and thus highly useful for blackout prevention.

#index 989672
#* Corroborate and learn facts from the web
#@ Shubin Zhao;Jonathan Betz
#t 2007
#c 0
#% 330784
#% 348146
#% 431536
#% 504443
#% 729978
#% 754068
#% 755834
#% 815868
#% 963669
#% 1663486
#! The web contains lots of interesting factual information about entities, such as celebrities, movies or products. This paper describes a robust bootstrapping approach to corroborate facts and learn more facts simultaneously. This approach starts with retrieving relevant pages from a crawl repository for each entity in the seed set. In each learning cycle, known facts of an entity are corroborated first in a relevant page to find fact mentions. When fact mentions are found, they are taken as examples for learning new facts from the page via HTML pattern discovery. Extracted new facts are added to the known fact set for the next learning cycle. The bootstrapping process continues until no new facts can be learned. This approach is language-independent. It demonstrated good performance in experiment on country facts. Results of a large scale experiment will also be shown with initial facts imported from wikipedia.

#index 989673
#* Extracting relevant named entities for automated expense reimbursement
#@ Guangyu Zhu;Timothy J. Bethea;Vikas Krishna
#t 2007
#c 0
#% 162505
#% 263239
#% 329442
#% 387749
#% 420495
#% 438103
#% 443727
#% 464434
#% 466892
#% 590530
#% 643004
#% 729437
#% 769884
#% 786564
#% 816181
#% 830520
#% 836906
#% 854636
#% 854799
#% 855114
#% 881578
#% 1502489
#! Expense reimbursement is a time-consuming and labor-intensive process across organizations. In this paper, we present a prototype expense reimbursement system that dramatically reduces the elapsed time and costs involved, by eliminating paper from the process life cycle. Our complete solution involves (1) an electronic submission infrastructure that provides multi- channel image capture, secure transport and centralized storage of paper documents; (2) an unconstrained data mining approach to extracting relevant named entities from un-structured document images; (3) automation of auditing procedures that enables automatic expense validation with minimum human interaction. Extracting relevant named entities robustly from document images with unconstrained layouts and diverse formatting is a fundamental technical challenge to image-based data mining, question answering, and other information retrieval tasks. In many applications that require such capability, applying traditional language modeling techniques to the stream of OCR text does not give satisfactory result due to the absence of linguistic context. We present an approach for extracting relevant named entities from document images by combining rich page layout features in the image space with language content in the OCR text using a discriminative conditional random field (CRF) framework. We integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collections of real-world receipt images provided by IBM World Wide Reimbursement Center.

#index 989674
#* Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Pavel Berkhin;Rich Caruana;Xindong Wu
#t 2007
#c 0

#index 989675
#* A framework for classification and segmentation of massive audio data streams
#@ Charu C. Aggarwal
#t 2007
#c 0
#% 210173
#% 1015261
#! In recent years, the proliferation of VOIP data has created a number of applications in which it is desirable to perform quick online classification and recognition of massive voice streams. Typically such applications are encountered in real time intelligence and surveillance. In many cases, the data streams can be in compressed format, and the rate of data processing can often run at the rate of Gigabits per second. All known techniques for speaker voice analysis require the use of an offline training phase in which the system is trained with known segments of speech. The state-of-the-art method for text-independent speaker recognition is known as Gaussian Mixture Modeling (GMM), and it requires an iterative Expectation Maximization Procedure for training, which cannot be implemented in real time. In this paper, we discuss the details of such an online voice recognition system. For this purpose, we use our micro-clustering algorithms to design concise signatures of the target speakers. One of the surprising and insightful observations from our experiences with such a system is that while it was originally designed only for efficiency, we later discovered that it was also more accurate than the widely used Gaussian Mixture Model (GMM). This was because of the conciseness of the micro-cluster model, which made it less prone to over training. This is evidence of the fact that it is often possible to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective.

#index 989676
#* Detecting changes in large data sets of payment card data: a case study
#@ Chris Curry;Robert L. Grossman;David Locke;Steve Vejcik;Joseph Bugajski
#t 2007
#c 0
#% 135968
#% 280408
#% 280413
#% 1016144
#! An important problem in data mining is detecting changes in large datasets. Although there are a variety of change detection algorithms that have been developed, in practice it can be a problem to scale these algorithms to large data sets due to the heterogeneity of the data. In this paper, we describe a case study involving payment card data in which we built and monitored a separate change detection model for each cell in a multi-dimensional data cube. We describe a system that has been in operation for the past two years that builds and monitors over 15,000 separate baseline models and the process that isused for generating and investigating alerts using these baselines.

#index 989677
#* Domain-constrained semi-supervised mining of tracking models in sensor networks
#@ Rong Pan;Junhui Zhao;Vincent Wenchen Zheng;Jeffrey Junfeng Pan;Dou Shen;Sinno Jialin Pan;Qiang Yang
#t 2007
#c 0
#% 339218
#% 464434
#% 819455
#% 1250210
#% 1289473
#% 1673026
#! Accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application. Specifically, the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points. Conventional data mining and machine learning methods can be applied to solve this problem. However, all of them require large amounts of labeled training data, which can be quite expensive. In this paper, we propose a probabilistic semi supervised learning approach to reduce the calibration effort and increase the tracking accuracy. Our method is based on semi-supervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively. To make our method more efficient, we exploit a Generalized EM algorithm coupled with domain constraints. We validate our method through extensive experiments in a real sensor network using Crossbow MICA2 sensors. The results demonstrate the advantages of methods compared to other state-of-the-art object-tracking algorithms.

#index 989678
#* Event summarization for system management
#@ Wei Peng;Charles Perng;Tao Li;Haixun Wang
#t 2007
#c 0
#% 729999
#% 749038
#% 785405
#% 820339
#% 823418
#% 963675
#% 1306057
#! In system management applications, an overwhelming amount of data are generated and collected in the form of temporal events. While mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts, users of the applications are overwhelmed by the mining results. The extracted patterns are generally of large volume and hard to interpret, they may be of no emphasis, intricate and meaningless to non-experts, even to domain experts. While traditional research efforts focus on finding interesting patterns, in this paper, we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data. Event summarization aims at providing a concise interpretation of the seemingly chaotic data, so that domain experts may take actions upon the summarized models. Event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset.

#index 989679
#* LungCAD: a clinically approved, machine learning system for lung cancer detection
#@ R. Bharat Rao;Jinbo Bi;Glenn Fung;Marcos Salganicoff;Nancy Obuchowski;David Naidich
#t 2007
#c 0
#% 881563
#% 1274874
#% 1665162
#! We present LungCAD, a computer aided diagnosis (CAD) system that employs a classification algorithm for detecting solid pulmonary nodules from CT thorax studies. We briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain. The most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system, is the requirement that the CAD system be tested in a clinical trial. We describe the clinical trial in which LungCAD was tested: a large scale multi-reader, multi-case (MRMC) retrospective observational study to evaluate the effect of CAD in clinical practice for detecting solid pulmonary nodules from CT thorax studies. The clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with LungCAD, both for detecting nodules and identifying potentially actionable nodules; this, along with other findings from the trial, has resulted in FDA approval for LungCAD in late 2006.

#index 989680
#* Machine learning for stock selection
#@ Robert J. Yan;Charles X. Ling
#t 2007
#c 0
#% 176705
#% 354275
#% 459849
#! In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PR's portfolio earns a much higher average return as well as a higher risk-adjusted return than Cooper's method, which shows that the PR method leads to a clear profit improvement.

#index 989681
#* IMDS: intelligent malware detection system
#@ Yanfang Ye;Dingding Wang;Tao Li;Dongyi Ye
#t 2007
#c 0
#% 300120
#% 481290
#% 629602
#% 725349
#% 769923
#% 789080
#% 800466
#% 814023
#% 818916
#% 835018
#% 926881
#% 963770
#% 1860941
#! The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based anti-virus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using Objective-Oriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA_Fast_FP-Growth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King-Soft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system out perform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.

#index 989682
#* Truth discovery with multiple conflicting information providers on the web
#@ Xiaoxin Yin;Jiawei Han;Philip S. Yu
#t 2007
#c 0
#% 282905
#% 799636
#! The world-wide web has become the most important information source for most of us. Unfortunately, there is no guarantee for the correctness of information on the web. Moreover, different web sites often provide conflicting information on a subject, such as different specifications for the same product. In this paper we propose a new problem called Veracity, i.e., conformity to truth, which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites. We design a general framework for the Veracity problem, and invent an algorithm called TruthFinder, which utilizes the relationships between web sites and their information, i.e., a web site is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy web sites. Our experiments show that TruthFinder successfully finds true facts among conflicting information, and identifies trustworthy web sites better than the popular search engines.

#index 989683
#* Data mining at the crossroads: successes, failures and learning from them
#@ Srinivasan Parthasarathy
#t 2007
#c 0
#% 268079
#! Since the 1989 workshop on knowledge discovery in databases, the field has seen sustained growth and interest and has attained significant maturity. The main objectives of this panel will be to reflect on the successes and failures in the field of data mining over the last eighteen years and to examine what insights we can take with us as we move forward.

#index 994922
#* Proceedings of the 2007 international workshop on Domain driven data mining
#@ Philips Yu
#t 2007
#c 0
#! In the last decade, data mining has emerged as one of the most vivacious areas in information technology. Classic data mining is heavily dependent on data itself and targets data-driven methodologies. Existing approaches either view data mining as an autonomous data-driven trial-and-error process, or analyze the business issues in an isolated and case-by-case manner. As a result, very often the knowledge discovered is not always generally targeted to real business needs. Real-world data mining should consider and involve the following factors in data mining: the domain expert's role, domain intelligence, network/web intelligence, in-depth data intelligence, and the constrained environments in practice. It is greatly expected by business users that the identified data mining results can be directly deployed to assist business decision making and to improve business processes. However, as pointed out by KDD panels, they are some grand challenges for the next-generation KDD. It is very challenging to involve them for actionable knowledge discovery and mining real-world domain problems. Domain Driven Data Mining aims to tackle such challenges. The "2007 ACM SIGKDD International Workshop on Domain Driven Data Mining (DDDM2007)" has provided a premier forum for sharing findings, knowledge, insight, experience and lessons in tackling potential challenges (1) to expose next-generation data mining methodology for actionable knowledge discovery, identifying how KDD techniques can better contribute to critical domain problems in theory and practice; (2) to uncover domain-driven data mining techniques identifying how KDD can better strengthen business intelligence in complex enterprise applications; (3) to disclose the applications of domain driven data mining identifying how KDD can be effectively deployed into solving complex practical problems; and (4) to identify challenges and directions for future research and development in the dialogue between academia and business. The DDDM2007 has competitively selected 8 papers including 5 regular papers and 3 short papers from 5 countries. These papers have addressed specific domain problems in social security, blog, business, healthcare, crime and finance as well as theoretical issues. We hope the above efforts can promote the research and development of discovering actionable knowledge from complex domain problems, enhancing interaction and reducing the gap between academia and business, and driving a paradigm shift from interesting hidden pattern mining to actionable knowledge discovery in varying data mining domains. The DDDM2007 is organized by the Faculty of Information Technology at the University of Technology, Sydney, Australia. In accompanying the DDDM2007, a special Trends & Controversies Department in IEEE Intelligent Systems magazine (Volume 22, No. 4 in 2007) has been successfully published.

#index 995803
#* Proceedings of the 4th international workshop on Data mining standards, services and platforms
#@ Robert Grossman;Shirley Connelly
#t 2006
#c 0
#! This year marks the sixth year that there has been a KDD workshop on the Predictive Model Markup Language (PMML) and related areas and the fourth year of a broader conference with the theme of Data Mining Standards, Services, and Platforms. Over the past several years, PMML and related data mining standards have matured to the point that conformance of applications to PMML and interoperability have become relevant for many users and for many applications. The first paper in the proceedings this year addresses this issue. Applications benefit by employing standards for specific types of data, such as text, images, and other unstructured information, as well as domain specific standards. Two of the papers in the proceedings are concerned with these issues. Streaming data is becoming more and more important for a variety of applications. One of the papers in the proceedings describes a platform for processing streaming data. Finally, one of the papers describes an open source PMML-compliant scoring engine.

#index 1023793
#* First International Workshop on Data Mining and Audience Intelligence for Advertising
#@ Ying Li;Dou Shen;Arun C. Surendran
#t 2007
#c 0

#index 1023794
#* KDD Cup and Workshop 2007
#@ Bing Liu;James Bennett;Charles Elkan;Padhraic Smyth;Domonkos Tikk
#t 2007
#c 0

#index 1023795
#* 2007 International Workshop on Domain Driven Data Mining
#@ Philips Yu;Chengqi Zhang;Graham Williams;Longbing Cao
#t 2007
#c 0

#index 1023796
#* First International Workshop on Knowledge Discovery from Sensor Data
#@ Auroop R. Ganguly;Joao Gama;Olufemi A. Omitaomu;Mohamed Medhat Gaber;Ranga Raju Vatsavai
#t 2007
#c 0

#index 1023797
#* The Joint Ninth WEBKDD and 1st SNA-KDD Workshop on Web Mining and Social Network Analysis
#@ Lee Giles;Andrew McCallum;Bamshad Mobasher;Olfa Nasraoui;Myra Spiliopoulou;Jaideep Srivastava;John Yen;Haizheng Zhang
#t 2007
#c 0

#index 1023798
#* First ACM SIGKDD International Workshop on Privacy, Security, and Trust in KDD
#@ Francesco Bonchi;Elena Ferrari;Bradley Malin;Yucel Saygin
#t 2007
#c 0

#index 1023799
#* Eighth International Workshop on Multimedia Data Mining
#@ K. Selcuk Candan;Zhongfei Zhang
#t 2007
#c 0

#index 1023800
#* Fifth International Workshop on Data Mining Standards, Services, and Platforms
#@ Robert Grossman;Shirley Connelly
#t 2007
#c 0

#index 1023801
#* Seventh International Workshop on Data Mining in Bioinformatics
#@ Jake Y. Chen;Stefano Lonardi;Mohammed Zaki
#t 2007
#c 0

#index 1023802
#* First International Workshop on Mining Multiple Information Sources
#@ Xingquan Zhu;Ruoming Jin;Gagan Agrawal
#t 2007
#c 0

#index 1023803
#* First International Workshop and Challenge on Time Series Classification
#@ Eamonn Keogh;Christian Shelton;Fabian Moerchen
#t 2007
#c 0

#index 1023804
#* Second Workshop on Data Mining Case Studies and Practice Prize
#@ Brendan Kitts;Gabor Melli;Gregory Piatetsky-Shapiro;Pip Courbois;Simeon J. Simoff;Jing Ying Zhang;Karl Rexer;Gang Wu;John Elder;Tom Osborn;Ed Freeman;Ying Li
#t 2007
#c 0

#index 1023805
#* Mining large time-evolving data using matrix and tensor tools
#@ Christos Faloutsos;Tamara G. Kolda;Jimeng Sun
#t 2007
#c 0

#index 1023806
#* A statistical framework for mining data streams
#@ Tamraparni Dasu;Simon Urbanek
#t 2007
#c 0

#index 1023807
#* Statistical modeling of relational data
#@ Pedro Domingos
#t 2007
#c 0

#index 1023808
#* Text mining and link analysis for web and semantic web
#@ Marko Grobelnik;Dunja Mladenic;Fortuna Blaz
#t 2007
#c 0

#index 1023809
#* Learning Bayesian networks
#@ Richard E. Neapolitan
#t 2007
#c 0

#index 1023810
#* From trees to forests and rule sets: a unified overview of ensemble methods
#@ Giovanni Seni;John Elder
#t 2007
#c 0

#index 1023811
#* Mining shape and time series databases with symbolic representations
#@ Eamonn Keogh
#t 2007
#c 0

#index 1035806
#* Proceedings of the 8th international workshop on Multimedia data mining: (associated with the ACM SIGKDD 2007)
#@ K. Selçuk Candan;Zhongfei Zhang
#t 2007
#c 0

#index 1040830
#* Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social network analysis
#@ Haizheng Zhang;Bamshad Mobasher;Lee Giles;Andrew McCallum;Olfa Nasraoui;Myra Spiliopoulou;Jaideep Srivastava;John Yen
#t 2007
#c 0
#! The first generation of the World Wide Web has been characterized by the interaction between the user and the medium: Web sites offer information and services; new applications and even new business models emerged for institutions that offer a better information search experience, more well-informed purchase decisions and, more recently, for the sharing and the collaborative treatment of content. The next generation of the Web is reflected in the proliferating platforms for sharing and collaboration and in the design of Web 2.0, which is proactively oriented towards social activities in the Web. The social flair of the Web poses new challenges for the data mining community. Social networking in the Web is a phenomenon of scientific interest per se; there is demand for flexible and robust community discovery technologies but also for interdisciplinary research on the rules and behavioral patterns that emerge and characterize community formation and evolution. Further, the social Web poses challenges for the individual; assistance and ultimately, personalization broad scope of activities, including the traditional search for documents, the less traditional search for multimedia content and the search for similar people and for answers to poorly articulated information needs. Moreover, the misuse potential of social formations cannot be stressed enough: People are confronted with unreliable or even malicious content, with surveillance and even stealth of information and of personal data; detection and protection mechanisms are needed here, based on a deep understanding of patterns of misuse and misbehavior. Finally, the diversity of social structures in the Web must be kept in mind: Social structures in the Web take many forms, including wikis and folksonomies where people contribute semantically rich content, platforms for collaborative annotations where people enrich existing content, bulletin boards and similar fora where people seek for advice but also establish new contacts, but also the implicit communities to be found in auction platforms and among the reviewers of products and services; these are communities where reputation and trust play a mission-critical role. Data miners are expected to deliver solutions for the challenges in searching, personalizing, understanding and protecting those social structures. In addition to online communities, this joint workshop will also invite submissions on generic social network systems, including social network modeling, growth and evolution dynamics of social networks, graph-related algorithms, multi-agent based social network simulation, trend prediction of social network evolution, and applications in related domains. The joint workshop of WEBKDD and SNA-KDD '2007 aims to bring together practitioners and researchers with a specific focus on the emerging trends and industry needs associated with the traditional Web, the social Web, and other forms of social networking systems. This includes (1) data mining advances on the discovery and analysis of communities, on personalization for solitary activities (like search) and social activities (like discovery of potential friends), on the analysis of user behavior in open fora (like conventional sites, blogs and fora) and in commercial platforms (like e-auctions) and on the associated security and privacy-preservation challenges; (2) social network modeling, scalable, customizable social network infrastructure construction , dynamic growth and evolution patterns identification and discovery using machine learning approaches or multi-agent based simulation.

#index 1040853
#* Proceedings of the 1st international workshop on Data mining and audience intelligence for advertising
#@ Ying Li
#t 2007
#c 0
#! Global advertising is a half-a-trillion dollar a year business. Although online advertising is currently a small part of this enterprise, it is rapidly growing. The explosion in the number of participants in online advertising marketplaces, and other online entities such as blogs and social networks, has generated large volumes of data and exciting data mining problems. Earlier research on these online entities has focused on information organization, retrieval and understanding. Recently there is increased research interest in the advertisement angle to all these avenues also. Further, the on-line and offline advertising worlds are fast converging; for example, digital marketplaces are migrating from the online world to TV and radio and audience understanding work from offline media is trickling into the online realm. Hence there is strong need for a single forum to bring together researchers and practitioners who are involved in all aspects of digital advertising. We are addressing this need with the First Workshop on Data Mining and Audience Intelligence for Advertising. The goal of this workshop is to encourage data mining researchers to take on the numerous challenges faced by the rapidly changing digital advertising industry, and to increase communication between researchers working on seemingly different pieces of the advertisement pie. We want to bring together auction theorists, social network researchers, natural language researchers, information retrieval experts, audience understanding researchers, television advertisement analysts and many others, to promote a fruitful exchange of ideas to advance the field.

#index 1083618
#* Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Ying Li;Bing Liu;Sunita Sarawagi
#t 2008
#c 0
#! We welcome you to the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'08) being held in Las Vegas, Nevada, USA, on August 24 - 27, 2008. We are pleased to present the proceedings of the conference as its published record. As the flagship conference in the field, KDD provides a highly competitive forum for reporting the latest and the best developments in the research and application of data mining and knowledge discovery worldwide. KDD'08 received a record number of 693 total submissions. The Research Track received 510 submissions from 30 different countries and the Industrial Track received 83 submissions from 10 different countries. For the Research Track, the program committee accepted 95 papers, of which, 50 (9.8% of the total) were chosen for a 25 minute oral presentation and 45 (8.8% of the total) were chosen for a 15 minute oral presentation. The corresponding numbers for the Industrial Track were 13 (15.7% of the total) and 10 (12.0% of the total) respectively. All accepted papers are given a presentation length of up to 9 pages in the proceedings, and all accepted papers are also given poster presentation opportunities in one of the two evening poster sessions during the conference. Apart from the paper presentations, the conference also features seven tutorials, thirteen workshops, one panel, the KDD-Cup competition, a demo session, and three invited talks by Trevor Hastie (Stanford University), Jitendra Malik (UC Berkeley) and Michael Schwarz (Yahoo! Research). The Industrial Track includes two additional invited presentations by Thore Graepel (Microsoft Research Cambridge, U.K.) and Udo Miletzki (Siemens, Germany).

#index 1083619
#* Internet advertising and optimal auction design
#@ Ben Edelman;Michael Schwarz
#t 2008
#c 0
#! This talk describes the optimal (revenue maximizing) auction for sponsored search advertising. We show that a search engine's optimal reserve price is independent of the number of bidders. Using simulations, we consider the changes that result from a search engine's choice of reserve price and from changes in the number of participating advertisers.

#index 1083620
#* Large scale data analysis and modelling in online services and advertising
#@ Thore Graepel;Ralf Herbrich
#t 2008
#c 0
#% 715096

#index 1083621
#* Regularization paths and coordinate descent
#@ Trevor Hastie;Jerome Friedman;Rob Tibshirani
#t 2008
#c 0
#! In a statistical world faced with an explosion of data, regularization has become an important ingredient. In a wide variety of problems we have many more input features than observations, and the lasso penalty and its hybrids have become increasingly useful for both feature selection and regularization. This talk presents some effective algorithms based on coordinate descent for fitting large scale regularization paths for a variety of problems.

#index 1083622
#* The future of image search
#@ Jitendra Malik
#t 2008
#c 0
#! There are billions of images on the Internet. Today, searching for a desired image is largely based on textual data such as filename or associated text on the web page; not much use is made of the image content. There are good reasons for this. The field of content-based image retrieval, which emerged during the 1990s, focused primarily on color and texture cues. These were easier to model than shape, but they turned out to be much less useful than originally hoped. I shall review some of the recent developments in the field of visual object recognition in the computer vision community that offer greater promise. Much better image features for characterizing shape, advances in machine learning techniques, and the availability of large amounts of training data lie at the heart of these approaches.

#index 1083623
#* Genesis of postal address reading, current state and future prospects: thirty years of pattern recognition on duty of postal services
#@ Udo Miletzki
#t 2008
#c 0
#! Abstract Intro: An overview is given of the largest industrial OCR application world wide: Postal Address Reading, how it came into being, how it evolved rapidly to its current state-of-the-art and what are its future prospects. Some prominent historical-, system-, methodological-, cultural- and social aspects are illuminated.

#index 1083624
#* Influence and correlation in social networks
#@ Aris Anagnostopoulos;Ravi Kumar;Mohammad Mahdian
#t 2008
#c 0
#% 729923
#% 881054
#% 881460
#! In many online social systems, social ties between users play an important role in dictating their behavior. One of the ways this can happen is through social influence, the phenomenon that the actions of a user can induce his/her friends to behave in a similar way. In systems where social influence exists, ideas, modes of behavior, or new technologies can diffuse through the network like an epidemic. Therefore, identifying and understanding social influence is of tremendous interest from both analysis and design points of view. This is a difficult task in general, since there are factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network. Distinguishing influence from these is essentially the problem of distinguishing correlation from causality, a notoriously hard statistical problem. In this paper we study this problem systematically. We define fairly general models that replicate the aforementioned sources of social correlation. We then propose two simple tests that can identify influence as a source of social correlation when the time series of user actions is available. We give a theoretical justification of one of the tests by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation. We also simulate our tests on a number of examples designed by randomly generating actions of nodes on a real social network (from Flickr) according to one of several models. Simulation results confirm that our test performs well on these data. Finally, we apply them to real tagging data on Flickr, exhibiting that while there is significant social correlation in tagging behavior on this system, this correlation cannot be attributed to social influence.

#index 1083625
#* Efficient semi-streaming algorithms for local triangle counting in massive graphs
#@ Luca Becchetti;Paolo Boldi;Carlos Castillo;Aristides Gionis
#t 2008
#c 0
#% 68247
#% 249238
#% 255137
#% 281214
#% 282505
#% 293720
#% 341100
#% 379443
#% 577273
#% 616528
#% 749449
#% 754117
#% 772018
#% 805904
#% 807320
#% 823342
#% 824711
#% 847113
#% 874902
#% 912202
#% 987245
#% 1719564
#! In this paper we study the problem of local triangle counting in large graphs. Namely, given a large graph G = (V;E) we want to estimate as accurately as possible the number of triangles incident to every node υ ∈ V in the graph. The problem of computing the global number of triangles in a graph has been considered before, but to our knowledge this is the first paper that addresses the problem of local triangle counting with a focus on the efficiency issues arising in massive graphs. The distribution of the local number of triangles and the related local clustering coefficient can be used in many interesting applications. For example, we show that the measures we compute can help to detect the presence of spamming activity in large-scale Web graphs, as well as to provide useful features to assess content quality in social networks. For computing the local number of triangles we propose two approximation algorithms, which are based on the idea of min-wise independent permutations (Broder et al. 1998). Our algorithms operate in a semi-streaming fashion, using O(jV j) space in main memory and performing O(log jV j) sequential scans over the edges of the graph. The first algorithm we describe in this paper also uses O(jEj) space in external memory during computation, while the second algorithm uses only main memory. We present the theoretical analysis as well as experimental results in massive graphs demonstrating the practical efficiency of our approach.

#index 1083626
#* Structured entity identification and document categorization: two tasks with one joint model
#@ Indrajit Bhattacharya;Shantanu Godbole;Sachindra Joshi
#t 2008
#c 0
#% 201889
#% 236497
#% 252011
#% 280819
#% 311027
#% 342621
#% 458379
#% 572314
#% 660001
#% 722904
#% 722914
#% 729927
#% 810014
#% 830526
#% 893143
#% 937552
#% 1673578
#! Traditionally, research in identifying structured entities in documents has proceeded independently of document categorization research. In this paper, we observe that these two tasks have much to gain from each other. Apart from direct references to entities in a database, such as names of person entities, documents often also contain words that are correlated with discriminative entity attributes, such age-group and income-level of persons. This happens naturally in many enterprise domains such as CRM, Banking, etc. Then, entity identification, which is typically vulnerable against noise and incompleteness in direct references to entities in documents, can benefit from document categorization with respect to such attributes. In return, entity identification enables documents to be categorized according to different label-sets arising from entity attributes without requiring any supervision. In this paper, we propose a probabilistic generative model for joint entity identification and document categorization. We show how the parameters of the model can be estimated using an EM algorithm in an unsupervised fashion. Using extensive experiments over real and semi-synthetic data, we demonstrate that the two tasks can benefit immensely from each other when performed jointly using the proposed model.

#index 1083627
#* Mining adaptively frequent closed unlabeled rooted trees in data streams
#@ Albert Bifet;Ricard Gavaldà
#t 2008
#c 0
#% 411375
#% 414993
#% 577218
#% 629617
#% 729938
#% 785339
#% 785428
#% 869611
#% 881520
#% 1077173
#% 1131675
#% 1408820
#% 1718448
#% 1738861
#! Closed patterns are powerful representatives of frequent patterns, since they eliminate redundant information. We propose a new approach for mining closed unlabeled rooted trees adaptively from data streams that change over time. Our approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees, and leads to an on-line strategy and an adaptive sliding window technique for dealing with changes over time. More precisely, we first present a general methodology to identify closed patterns in a data stream, using Galois Lattice Theory. Using this methodology, we then develop three closed tree mining algorithms: an incremental one IncTreeNat, a sliding-window based one, WinTreeNat, and finally one that mines closed trees adaptively from data streams, AdaTreeNat. To the best of our knowledge this is the first work on mining frequent closed trees in streaming data varying with time. We give a first experimental evaluation of the proposed algorithms.

#index 1083628
#* Effective label acquisition for collective classification
#@ Mustafa Bilgic;Lise Getoor
#t 2008
#c 0
#% 248810
#% 249143
#% 303620
#% 342611
#% 420495
#% 565531
#% 577217
#% 722797
#% 729923
#% 769942
#% 876046
#% 937549
#% 949164
#% 961278
#% 990220
#% 1030882
#% 1269764
#% 1269865
#% 1272282
#% 1289563
#% 1650403
#! Information diffusion, viral marketing, and collective classification all attempt to model and exploit the relationships in a network to make inferences about the labels of nodes. A variety of techniques have been introduced and methods that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the nodes in a network. However, in part because of the correlation between node labels that the techniques exploit, it is easy to find cases in which, once a misclassification is made, incorrect information propagates throughout the network. This problem can be mitigated if the system is allowed to judiciously acquire the labels for a small number of nodes. Unfortunately, under relatively general assumptions, determining the optimal set of labels to acquire is intractable. Here we propose an acquisition method that learns the cases when a given collective classification algorithm makes mistakes, and suggests acquisitions to correct those mistakes. We empirically show on both real and synthetic datasets that this method significantly outperforms a greedy approximate inference approach, a viral marketing approach, and approaches based on network structural measures such as node degree and network clustering. In addition to significantly improving accuracy with just a small amount of labeled data, our method is tractable on large networks.

#index 1083629
#* Topical query decomposition
#@ Francesco Bonchi;Carlos Castillo;Debora Donato;Aristides Gionis
#t 2008
#c 0
#% 218992
#% 256685
#% 298183
#% 302747
#% 308745
#% 310567
#% 330617
#% 341672
#% 387427
#% 397152
#% 413610
#% 464291
#% 600184
#% 754124
#% 766433
#% 807295
#% 835018
#% 838531
#% 869651
#% 872033
#% 943016
#% 1035578
#% 1055675
#% 1085668
#% 1603005
#% 1712595
#! We introduce the problem of query decomposition, where we are given a query and a document retrieval system, and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query. Ideally, these queries should represent coherent, conceptually well-separated topics. We provide an abstract formulation of the query decomposition problem, and we tackle it from two different perspectives. We first show how the problem can be instantiated as a specific variant of a set cover problem, for which we provide an efficient greedy algorithm. Next, we show how the same problem can be seen as a constrained clustering problem, with a very particular kind of constraint, i.e., clustering with predefined clusters. We develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming. Our experiments, conducted on a set of actual queries in a Web scale search engine, confirm the effectiveness of the proposed solutions.

#index 1083630
#* Unsupervised feature selection for principal components analysis
#@ Christos Boutsidis;Michael W. Mahoney;Petros Drineas
#t 2008
#c 0
#% 117841
#% 163503
#% 215859
#% 259089
#% 259090
#% 345824
#% 465741
#% 593842
#% 722941
#% 770810
#% 771842
#% 811479
#% 847159
#% 881488
#% 916789
#% 983948
#% 985909
#% 1300083
#% 1781559
#! Principal Components Analysis (PCA) is the predominant linear dimensionality reduction technique, and has been widely applied on datasets in all scientific domains. We consider, both theoretically and empirically, the topic of unsupervised feature selection for PCA, by leveraging algorithms for the so-called Column Subset Selection Problem (CSSP). In words, the CSSP seeks the "best" subset of exactly k columns from an m x n data matrix A, and has been extensively studied in the Numerical Linear Algebra community. We present a novel two-stage algorithm for the CSSP. From a theoretical perspective, for small to moderate values of k, this algorithm significantly improves upon the best previously-existing results [24, 12] for the CSSP. From an empirical perspective, we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis: finance, document-term data, and genetics. We pay particular attention to how this algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner. In all three application domains, we are able to identify k landmark features, i.e., columns of the data matrix, that capture nearly the same amount of information as does the subspace that is spanned by the top k "eigenfeatures."

#index 1083631
#* The cost of privacy: destruction of data-mining utility in anonymized data publishing
#@ Justin Brickell;Vitaly Shmatikov
#t 2008
#c 0
#% 67453
#% 300184
#% 340475
#% 443463
#% 576110
#% 576111
#% 576761
#% 576762
#% 577239
#% 765449
#% 800514
#% 800515
#% 809245
#% 810011
#% 824726
#% 844340
#% 864406
#% 864412
#% 864663
#% 864665
#% 874988
#% 874989
#% 881483
#% 881497
#% 881546
#% 893100
#% 960239
#% 960289
#% 960291
#% 1015140
#% 1022246
#% 1022266
#% 1670071
#% 1707132
#% 1725659
#! Re-identification is a major privacy threat to public datasets containing individual records. Many privacy protection algorithms rely on generalization and suppression of "quasi-identifier" attributes such as ZIP code and birthdate. Their objective is usually syntactic sanitization: for example, k-anonymity requires that each "quasi-identifier" tuple appear in at least k records, while l-diversity requires that the distribution of sensitive attributes for each quasi-identifier have high entropy. The utility of sanitized data is also measured syntactically, by the number of generalization steps applied or the number of records with the same quasi-identifier. In this paper, we ask whether generalization and suppression of quasi-identifiers offer any benefits over trivial sanitization which simply separates quasi-identifiers from sensitive attributes. Previous work showed that k-anonymous databases can be useful for data mining, but k-anonymization does not guarantee any privacy. By contrast, we measure the tradeoff between privacy (how much can the adversary learn from the sanitized records?) and utility, measured as accuracy of data-mining algorithms executed on the same sanitized records. For our experimental evaluation, we use the same datasets from the UCI machine learning repository as were used in previous research on generalization and suppression. Our results demonstrate that even modest privacy gains require almost complete destruction of the data-mining utility. In most cases, trivial sanitization provides equivalent utility and better privacy than k-anonymity, l-diversity, and similar methods based on generalization and suppression.

#index 1083632
#* Generating succinct titles for web URLs
#@ Deepayan Chakrabarti;Ravi Kumar;Kunal Punera
#t 2008
#c 0
#% 280840
#% 316519
#% 330780
#% 340884
#% 397137
#% 577224
#% 642985
#% 741106
#% 766437
#% 817581
#% 818226
#% 855368
#% 879594
#! How can a search engine automatically provide the best and most appropriate title for a result URL (link-title) so that users will be persuaded to click on the URL? We consider the problem of automatically generating link-titles for URLs and propose a general statistical framework for solving this problem. The framework is based on using information from a diverse collection of sources, each of which can be thought of as contributing one or more candidate link-titles for the URL. It can also incorporate the context in which the link-title will be used, along with constraints on its length. Our framework is applicable to several scenarios: obtaining succinct titles for displaying quicklinks, obtaining titles for URLs that lack a good title, constructing succinct sitemaps, etc. Extensive experiments show that our method is very effective, producing results that are at least 20% better than non-trivial baselines.

#index 1083633
#* Structured learning for non-smooth ranking losses
#@ Soumen Chakrabarti;Rajiv Khanna;Uma Sawant;Chiru Bhattacharyya
#t 2008
#c 0
#% 309095
#% 577224
#% 829043
#% 840846
#% 840882
#% 881477
#% 926881
#% 983815
#% 983820
#% 987226
#% 987240
#% 1035577
#! Learning to rank from relevance judgment is an active research area. Itemwise score regression, pairwise preference satisfaction, and listwise structured learning are the major techniques in use. Listwise structured learning has been applied recently to optimize important non-decomposable ranking criteria like AUC (area under ROC curve) and MAP (mean average precision). We propose new, almost-linear-time algorithms to optimize for two other criteria widely used to evaluate search systems: MRR (mean reciprocal rank) and NDCG (normalized discounted cumulative gain) in the max-margin structured learning framework. We also demonstrate that, for different ranking criteria, one may need to use different feature maps. Search applications should not be optimized in favor of a single criterion, because they need to cater to a variety of queries. E.g., MRR is best for navigational queries, while NDCG is best for informational queries. A key contribution of this paper is to fold multiple ranking loss functions into a multi-criteria max-margin optimization. The result is a single, robust ranking model that is close to the best accuracy of learners trained on individual criteria. In fact, experiments over the popular LETOR and TREC data sets show that, contrary to conventional wisdom, a test criterion is often not best served by training with the same individual criterion.

#index 1083634
#* Partitioned logistic regression for spam filtering
#@ Ming-wei Chang;Wen-tau Yih;Christopher Meek
#t 2008
#c 0
#% 190581
#% 246831
#% 251145
#% 272995
#% 309119
#% 642988
#% 788072
#% 815864
#% 823334
#% 823397
#% 916867
#% 939334
#% 940002
#% 987244
#% 989611
#% 1249460
#% 1860547
#! Naive Bayes and logistic regression perform well in different regimes. While the former is a very simple generative model which is efficient to train and performs well empirically in many applications,the latter is a discriminative model which often achieves better accuracy and can be shown to outperform naive Bayes asymptotically. In this paper, we propose a novel hybrid model, partitioned logistic regression, which has several advantages over both naive Bayes and logistic regression. This model separates the original feature space into several disjoint feature groups. Individual models on these groups of features are learned using logistic regression and their predictions are combined using the naive Bayes principle to produce a robust final estimation. We show that our model is better both theoretically and empirically. In addition, when applying it in a practical application, email spam filtering, it improves the normalized AUC score at 10% false-positive rate by 28.8% and 23.6% compared to naive Bayes and logistic regression, when using the exact same training examples.

#index 1083635
#* Learning subspace kernels for classification
#@ Jianhui Chen;Shuiwang Ji;Betul Ceran;Qi Li;Mingrui Wu;Jieping Ye
#t 2008
#c 0
#% 80995
#% 143238
#% 309208
#% 393059
#% 722809
#% 743284
#% 757953
#% 763697
#% 763698
#% 929605
#% 961190
#% 983906
#% 983907
#% 1000452
#% 1083738
#% 1274935
#% 1673681
#! Kernel methods have been applied successfully in many data mining tasks. Subspace kernel learning was recently proposed to discover an effective low-dimensional subspace of a kernel feature space for improved classification. In this paper, we propose to construct a subspace kernel using the Hilbert-Schmidt Independence Criterion (HSIC). We show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem. One limitation of the existing subspace kernel learning formulations is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification. To overcome this limitation, we propose a joint optimization framework, in which we learn the subspace kernel and subsequent classifiers simultaneously. In addition, we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel. Following the idea from multiple kernel learning, we extend the proposed formulations to the case when multiple kernels are available and need to be combined. We show that the integration of subspace kernels can be formulated as a semidefinite program (SDP) which is computationally expensive. To improve the efficiency of the SDP formulation, we propose an equivalent semi-infinite linear program (SILP) formulation which can be solved efficiently by the column generation technique. Experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms.

#index 1083636
#* Combinational collaborative filtering for personalized community recommendation
#@ Wen-Yen Chen;Dong Zhang;Edward Y. Chang
#t 2008
#c 0
#% 280819
#% 466574
#% 722902
#% 722904
#% 769906
#% 770861
#% 823403
#% 826918
#! Rapid growth in the amount of data available on social networking sites has made information retrieval increasingly challenging for users. In this paper, we propose a collaborative filtering method, Combinational Collaborative Filtering (CCF), to perform personalized community recommendations by considering multiple types of co-occurrences in social data at the same time. This filtering method fuses semantic and user information, then applies a hybrid training strategy that combines Gibbs sampling and Expectation-Maximization algorithm. To handle the large-scale dataset, parallel computing is used to speed up the model training. Through an empirical study on the Orkut dataset, we show CCF to be both effective and scalable.

#index 1083637
#* FAST: a roc-based feature selection metric for small samples and imbalanced data classification problems
#@ Xue-wen Chen;Michael Wasikowski
#t 2008
#c 0
#% 92554
#% 99398
#% 126894
#% 169659
#% 177826
#% 280437
#% 420064
#% 425048
#% 458320
#% 466266
#% 593105
#% 722929
#% 722935
#% 765520
#% 765525
#% 765527
#% 768669
#% 793239
#% 857105
#% 875974
#% 915342
#% 983823
#% 1271973
#% 1289281
#% 1502470
#! The class imbalance problem is encountered in a large number of practical applications of machine learning and data mining, for example, information retrieval and filtering, and the detection of credit card fraud. It has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifier's suboptimal performance. This is even more true when the imbalanced data are also high dimensional. In such cases, feature selection methods are critical to achieve optimal performance. In this paper, we propose a new feature selection method, Feature Assessment by Sliding Thresholds (FAST), which is based on the area under a ROC curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. FAST is compared to two commonly-used feature selection methods, correlation coefficient and RELevance In Estimating Features (RELIEF), for imbalanced data classification. The experimental results obtained on text mining, mass spectrometry, and microarray data sets showed that the proposed method outperformed both RELIEF and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred, the classification performance of the proposed method was significantly improved compared to correlation and RELIEF-based methods.

#index 1083638
#* Semi-supervised learning with data calibration for long-term time series forecasting
#@ Haibin Cheng;Pang-Ning Tan
#t 2008
#c 0
#% 252011
#% 425030
#% 466263
#% 516767
#% 565545
#% 768632
#% 784540
#% 875962
#% 915312
#% 990806
#% 1126727
#% 1269778
#% 1289496
#% 1669958
#! Many time series prediction methods have focused on single step or short term prediction problems due to the inherent difficulty in controlling the propagation of errors from one prediction step to the next step. Yet, there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecasting capabilities for strategic decision making. Training an accurate model that produces reliable long term predictions would require an extensive amount of historical data, which are either unavailable or expensive to acquire. For some of these domains, there are alternative ways to generate potential scenarios for the future using computer-driven simulation models, such as global climate and traffic demand models. However, the data generated by these models are currently utilized in a supervised learning setting, where a predictive model trained on past observations is used to estimate the future values. In this paper, we present a semi-supervised learning framework for long-term time series forecasting based on Hidden Markov Model Regression. A covariance alignment method is also developed to deal with the issue of inconsistencies between historical and model simulation data. We evaluated our approach on data sets from a variety of domains, including climate modeling. Our experimental results demonstrate the efficacy of the approach compared to other supervised learning methods for long-term time series forecasting.

#index 1083639
#* Reconstructing chemical reaction networks: data mining meets system identification
#@ Yong Ju Cho;Naren Ramakrishnan;Yang Cao
#t 2008
#c 0
#% 176726
#% 204636
#% 830619
#! We present an approach to reconstructing chemical reaction networks from time series measurements of the concentrations of the molecules involved. Our solution strategy combines techniques from numerical sensitivity analysis and probabilistic graphical models. By modeling a chemical reaction system as a Markov network (undirected graphical model), we show how systematically probing for sensitivities between molecular species can identify the topology of the network. Given the topology, our approach next uses detailed sensitivity profiles to characterize properties of reactions such as reversibility, enzyme-catalysis, and the precise stoichiometries of the reactants and products. We demonstrate applications to reconstructing key biological systems including the yeast cell cycle. In addition to network reconstruction, our algorithm finds applications in model reduction and model comprehension. We argue that our reconstruction algorithm can serve as an important primitive for data mining in systems biology applications.

#index 1083640
#* Automatic record linkage using seeded nearest neighbour and support vector machine classification
#@ Peter Christen
#t 2008
#c 0
#% 577238
#% 577247
#% 577263
#% 659991
#% 729913
#% 730039
#% 778320
#% 913783
#% 937552
#% 984083
#% 1064741
#% 1070283
#% 1411073
#% 1675764
#% 1693724
#! The task of linking databases is an important step in an increasing number of data mining projects, because linked data can contain information that is not available otherwise, or that would require time-consuming and expensive collection of specific data. The aim of linking is to match and aggregate all records that refer to the same entity. One of the major challenges when linking large databases is the efficient and accurate classification of record pairs into matches and non-matches. While traditionally classification was based on manually-set thresholds or on statistical procedures, many of the more recently developed classification methods are based on supervised learning techniques. They therefore require training data, which is often not available in real world situations or has to be prepared manually, an expensive, cumbersome and time-consuming process. The author has previously presented a novel two-step approach to automatic record pair classification [6, 7]. In the first step of this approach, training examples of high quality are automatically selected from the compared record pairs, and used in the second step to train a support vector machine (SVM) classifier. Initial experiments showed the feasibility of the approach, achieving results that outperformed k-means clustering. In this paper, two variations of this approach are presented. The first is based on a nearest-neighbour classifier, while the second improves a SVM classifier by iteratively adding more examples into the training sets. Experimental results show that this two-step approach can achieve better classification results than other unsupervised approaches.

#index 1083641
#* Feedback effects between similarity and social influence in online communities
#@ David Crandall;Dan Cosley;Daniel Huttenlocher;Jon Kleinberg;Siddharth Suri
#t 2008
#c 0
#% 173879
#% 202009
#% 202011
#% 406493
#% 414514
#% 813735
#% 817655
#% 868469
#% 881460
#% 881523
#% 905338
#% 936912
#% 967642
#! A fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties. People are similar to their neighbors in a social network for two distinct reasons: first, they grow to resemble their current friends due to social influence; and second, they tend to form new links to others who are already like them, a process often termed selection by sociologists. While both factors are present in everyday social processes, they are in tension: social influence can push systems toward uniformity of behavior, while selection can lead to fragmentation. As such, it is important to understand the relative effects of these forces, and this has been a challenge due to the difficulty of isolating and quantifying them in real settings. We develop techniques for identifying and modeling the interactions between social influence and selection, using data from online communities where both social interaction and changes in behavior over time can be measured. We find clear feedback effects between the two factors, with rising similarity between two individuals serving, in aggregate, as an indicator of future interaction -- but with similarity then continuing to increase steadily, although at a slower rate, for long periods after initial interactions. We also consider the relative value of similarity and social influence in modeling future behavior. For instance, to predict the activities that an individual is likely to do next, is it more useful to know the current activities of their friends, or of the people most similar to them?

#index 1083642
#* Anomaly pattern detection in categorical datasets
#@ Kaustav Das;Jeff Schneider;Daniel B. Neill
#t 2008
#c 0
#% 466745
#% 578689
#% 989593
#% 1272326
#% 1784191
#! We propose a new method for detecting patterns of anomalies in categorical datasets. We assume that anomalies are generated by some underlying process which affects only a particular subset of the data. Our method consists of two steps: we first use a "local anomaly detector" to identify individual records with anomalous attribute values, and then detect patterns where the number of anomalous records is higher than expected. Given the set of anomalies flagged by the local anomaly detector, we search over all subsets of the data defined by any set of fixed values of a subset of the attributes, in order to detect self-similar patterns of anomalies. We wish to detect any such subset of the test data which displays a significant increase in anomalous activity as compared to the normal behavior of the system (as indicated by the training data). We perform significance testing to determine if the number of anomalies in any subset of the test data is significantly higher than expected, and propose an efficient algorithm to perform this test over all such subsets of the data. We show that this algorithm is able to accurately detect anomalous patterns in real-world hospital, container shipping and network intrusion data.

#index 1083643
#* Bypass rates: reducing query abandonment using negative inferences
#@ Atish Das Sarma;Sreenivas Gollapudi;Samuel Ieong
#t 2008
#c 0
#% 63833
#% 262112
#% 282544
#% 577224
#% 642975
#% 818221
#% 872020
#% 879618
#% 975021
#% 987222
#% 989628
#% 1073970
#% 1250379
#! We introduce a new approach to analyzing click logs by examining both the documents that are clicked and those that are bypassed-documents returned higher in the ordering of the search results but skipped by the user. This approach complements the popular click-through rate analysis, and helps to draw negative inferences in the click logs. We formulate a natural objective that finds sets of results that are unlikely to be collectively bypassed by a typical user. This is closely related to the problem of reducing query abandonment. We analyze a greedy approach to optimizing this objective, and establish theoretical guarantees of its performance. We evaluate our approach on a large set of queries, and demonstrate that it compares favorably to the maximal marginal relevance approach on a number of metrics including mean average precision and mean reciprocal rank.

#index 1083644
#* De-duping URLs via rewrite rules
#@ Anirban Dasgupta;Ravi Kumar;Amit Sasturkar
#t 2008
#c 0
#% 255137
#% 289372
#% 318041
#% 347225
#% 600552
#% 616528
#% 728115
#% 800590
#% 879600
#% 901495
#% 907482
#% 956504
#% 956507
#% 967274
#% 993980
#! A large fraction of the URLs on the web contain duplicate (or near-duplicate) content. De-duping URLs is an extremely important problem for search engines, since all the principal functions of a search engine, including crawling, indexing, ranking, and presentation, are adversely impacted by the presence of duplicate URLs. Traditionally, the de-duping problem has been addressed by fetching and examining the content of the URL; our approach here is different. Given a set of URLs partitioned into equivalence classes based on the content (URLs in the same equivalence class have similar content), we address the problem of mining this set and learning URL rewrite rules that transform all URLs of an equivalence class to the same canonical form. These rewrite rules can then be applied to eliminate duplicates among URLs that are encountered for the first time during crawling, even without fetching their content. In order to express such transformation rules, we propose a simple framework that is general enough to capture the most common URL rewrite patterns occurring on the web; in particular, it encapsulates the DUST (Different URLs with similar text) framework [5]. We provide an efficient algorithm for mining and learning URL rewrite rules and show that under mild assumptions, it is complete, i.e., our algorithm learns every URL rewrite rule that is correct, for an appropriate notion of correctness. We demonstrate the expressiveness of our framework and the effectiveness of our algorithm by performing a variety of extensive large-scale experiments.

#index 1083645
#* Structured metric learning for high dimensional problems
#@ Jason V. Davis;Inderjit S. Dhillon
#t 2008
#c 0
#% 387427
#% 729437
#% 757953
#% 876008
#% 963196
#% 983830
#! The success of popular algorithms such as k-means clustering or nearest neighbor searches depend on the assumption that the underlying distance functions reflect domain-specific notions of similarity for the problem at hand. The distance metric learning problem seeks to optimize a distance function subject to constraints that arise from fully-supervised or semisupervised information. Several recent algorithms have been proposed to learn such distance functions in low dimensional settings. One major shortcoming of these methods is their failure to scale to high dimensional problems that are becoming increasingly ubiquitous in modern data mining applications. In this paper, we present metric learning algorithms that scale linearly with dimensionality, permitting efficient optimization, storage, and evaluation of the learned metric. This is achieved through our main technical contribution which provides a framework based on the log-determinant matrix divergence which enables efficient optimization of structured, low-parameter Mahalanobis distances. Experimentally, we evaluate our methods across a variety of high dimensional domains, including text, statistical software analysis, and collaborative filtering, showing that our methods scale to data sets with tens of thousands or more features. We show that our learned metric can achieve excellent quality with respect to various criteria. For example, in the context of metric learning for nearest neighbor classification, we show that our methods achieve 24% higher accuracy over the baseline distance. Additionally, our methods yield very good precision while providing recall measures up to 20% higher than other baseline methods such as latent semantic analysis.

#index 1083646
#* Constraint programming for itemset mining
#@ Luc De Raedt;Tias Guns;Siegfried Nijssen
#t 2008
#c 0
#% 232136
#% 280409
#% 300120
#% 464989
#% 466068
#% 580588
#% 765529
#% 785336
#% 832571
#% 867881
#% 915254
#% 924135
#% 928731
#% 942743
#% 1125934
#! The relationship between constraint-based mining and constraint programming is explored by showing how the typical constraints used in pattern mining can be formulated for use in constraint programming environments. The resulting framework is surprisingly flexible and allows us to combine a wide range of mining constraints in different ways. We implement this approach in off-the-shelf constraint programming systems and evaluate it empirically. The results show that the approach is not only very expressive, but also works well on complex benchmark problems.

#index 1083647
#* Learning classifiers from only positive and unlabeled data
#@ Charles Elkan;Keith Noto
#t 2008
#c 0
#% 17144
#% 577298
#% 722811
#% 727883
#% 729621
#% 732387
#% 769904
#% 843873
#% 846428
#% 855602
#% 891711
#% 906525
#% 989637
#% 992948
#% 1019061
#% 1705416
#! The input to an algorithm that learns a binary classifier normally consists of two sets of examples, where one set consists of positive examples of the concept to be learned, and the other set consists of negative examples. However, it is often the case that the available training data are an incomplete set of positive examples, and a set of unlabeled examples, some of which are positive and some of which are negative. The problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature. Under the assumption that the labeled examples are selected randomly from the positive examples, we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. We show how to use this result in two different ways to learn a classifier from a nontraditional training set. We then apply these two new methods to solve a real-world problem: identifying protein records that should be included in an incomplete specialized molecular biology database. Our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased SVM method for learning from positive and unlabeled examples.

#index 1083648
#* Locality sensitive hash functions based on concomitant rank order statistics
#@ Kave Eshghi;Shyamsundar Rajaram
#t 2008
#c 0
#% 249321
#% 321635
#% 347225
#% 760805
#% 762054
#% 780859
#% 847166
#% 881484
#% 898309
#% 939408
#% 961270
#% 990328
#! Locality Sensitive Hash functions are invaluable tools for approximate near neighbor problems in high dimensional spaces. In this work, we are focused on LSH schemes where the similarity metric is the cosine measure. The contribution of this work is a new class of locality sensitive hash functions for the cosine similarity measure based on the theory of concomitants, which arises in order statistics. Consider n i.i.d sample pairs, {(X1; Y1); (X2; Y2); : : : ;(Xn; Yn)} obtained from a bivariate distribution f(X, Y). Concomitant theory captures the relation between the order statistics of X and Y in the form of a rank distribution given by Prob(Rank(Yi)=j-Rank(Xi)=k). We exploit properties of the rank distribution towards developing a locality sensitive hash family that has excellent collision rate properties for the cosine measure. The computational cost of the basic algorithm is high for high hash lengths. We introduce several approximations based on the properties of concomitant order statistics and discrete transforms that perform almost as well, with significantly reduced computational cost. We demonstrate the practical applicability of our algorithms by using it for finding similar images in an image repository.

#index 1083649
#* Direct mining of discriminative and essential frequent patterns via model-based search tree
#@ Wei Fan;Kun Zhang;Hong Cheng;Jing Gao;Xifeng Yan;Jiawei Han;Philip Yu;Olivier Verscheure
#t 2008
#c 0
#% 300120
#% 342604
#% 463903
#% 464996
#% 466483
#% 466644
#% 481290
#% 729938
#% 769891
#% 810064
#% 813990
#% 823384
#% 824699
#% 840863
#% 915350
#% 937794
#% 1063502
#% 1206650
#! Frequent patterns provide solutions to datasets that do not have well-structured feature vectors. However, frequent pattern mining is non-trivial since the number of unique patterns is exponential but many are non-discriminative and correlated. Currently, frequent pattern mining is performed in two sequential steps: enumerating a set of frequent patterns, followed by feature selection. Although many methods have been proposed in the past few years on how to perform each separate step efficiently, there is still limited success in eventually finding highly compact and discriminative patterns. The culprit is due to the inherent nature of this widely adopted two-step approach. This paper discusses these problems and proposes a new and different method. It builds a decision tree that partitions the data onto different nodes. Then at each node, it directly discovers a discriminative pattern to further divide its examples into purer subsets. Since the number of examples towards leaf level is relatively small, the new approach is able to examine patterns with extremely low global support that could not be enumerated on the whole dataset by the two-step method. The discovered feature vectors are more accurate on some of the most difficult graph as well as frequent itemset problems than most recently proposed algorithms but the total size is typically 50% or more smaller. Importantly, the minimum support of some discriminative patterns can be extremely low (e.g. 0.03%). In order to enumerate these low support patterns, state-of-the-art frequent pattern algorithm either cannot finish due to huge memory consumption or have to enumerate 101 to 103 times more patterns before they can even be found. Software and datasets are available by contacting the author.

#index 1083650
#* Scaling up text classification for large file systems
#@ George Forman;Shyamsundar Rajaram
#t 2008
#c 0
#% 251145
#% 271346
#% 722935
#% 730065
#% 763708
#% 793417
#% 812364
#% 881471
#% 881477
#% 907507
#% 926881
#% 983921
#! We combine the speed and scalability of information retrieval with the generally superior classification accuracy offered by machine learning, yielding a two-phase text classifier that can scale to very large document corpora. We investigate the effect of different methods of formulating the query from the training set, as well as varying the query size. In empirical tests on the Reuters RCV1 corpus of 806,000 documents, we find runtime was easily reduced by a factor of 27x, with a somewhat surprising gain in F-measure compared with traditional text classification.

#index 1083651
#* SPIRAL: efficient and exact model identification for hidden Markov models
#@ Yasuhiro Fujiwara;Yasushi Sakurai;Masashi Yamamuro
#t 2008
#c 0
#% 210173
#% 219740
#% 252472
#% 763414
#% 840935
#% 983836
#% 1775366
#! Hidden Markov models (HMMs) have received considerable attention in various communities (e.g, speech recognition, neurology and bioinformatic) since many applications that use HMM have emerged. The goal of this work is to identify efficiently and correctly the model in a given dataset that yields the state sequence with the highest likelihood with respect to the query sequence. We propose SPIRAL, a fast search method for HMM datasets. To reduce the search cost, SPIRAL efficiently prunes a significant number of search candidates by applying successive approximations when estimating likelihood. We perform several experiments to verify the effectiveness of SPIRAL. The results show that SPIRAL is more than 500 times faster than the naive method.

#index 1083652
#* Using ghost edges for classification in sparsely labeled networks
#@ Brian Gallagher;Hanghang Tong;Tina Eliassi-Rad;Christos Faloutsos
#t 2008
#c 0
#% 248810
#% 400847
#% 722914
#% 729982
#% 858102
#% 915344
#% 926881
#% 961268
#% 961278
#% 1269763
#% 1269764
#% 1650403
#! We address the problem of classification in partially labeled networks (a.k.a. within-network classification) where observed class labels are sparse. Techniques for statistical relational learning have been shown to perform well on network classification tasks by exploiting dependencies between class labels of neighboring nodes. However, relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning (during training phase) and/or inference (during testing phase). This situation arises in real-world problems when observed labels are sparse. In this paper, we propose a novel approach to within-network classification that combines aspects of statistical relational learning and semi-supervised learning to improve classification performance in sparse networks. Our approach works by adding "ghost edges" to a network, which enable the flow of information from labeled to unlabeled nodes. Through experiments on real-world data sets, we demonstrate that our approach performs well across a range of conditions where existing approaches, such as collective classification and semi-supervised learning, fail. On all tasks, our approach improves area under the ROC curve (AUC) by up to 15 points over existing approaches. Furthermore, we demonstrate that our approach runs in time proportional to L • E, where L is the number of labeled nodes and E is the number of edges.

#index 1083653
#* Composition attacks and auxiliary information in data privacy
#@ Srivatsava Ranjit Ganta;Shiva Prasad Kasiviswanathan;Adam Smith
#t 2008
#c 0
#% 300184
#% 443478
#% 576110
#% 576111
#% 576761
#% 577233
#% 729452
#% 800513
#% 809245
#% 824727
#% 864406
#% 874989
#% 881483
#% 881497
#% 893100
#% 937550
#% 960291
#% 963241
#% 977011
#% 982549
#% 1022247
#% 1022266
#% 1029084
#% 1044457
#% 1061644
#% 1206678
#% 1670071
#% 1725659
#% 1727969
#% 1732708
#! Privacy is an increasingly important aspect of data publishing. Reasoning about privacy, however, is fraught with pitfalls. One of the most significant is the auxiliary information (also called external knowledge, background knowledge, or side information) that an adversary gleans from other channels such as the web, public records, or domain knowledge. This paper explores how one can reason about privacy in the face of rich, realistic sources of auxiliary information. Specifically, we investigate the effectiveness of current anonymization schemes in preserving privacy when multiple organizations independently release anonymized data about overlapping populations. 1. We investigate composition attacks, in which an adversary uses independent anonymized releases to breach privacy. We explain why recently proposed models of limited auxiliary information fail to capture composition attacks. Our experiments demonstrate that even a simple instance of a composition attack can breach privacy in practice, for a large class of currently proposed techniques. The class includes k-anonymity and several recent variants. 2. On a more positive note, certain randomization-based notions of privacy (such as differential privacy) provably resist composition attacks and, in fact, the use of arbitrary side information.This resistance enables "stand-alone" design of anonymization schemes, without the need for explicitly keeping track of other releases. We provide a precise formulation of this property, and prove that an important class of relaxations of differential privacy also satisfy the property. This significantly enlarges the class of protocols known to enable modular design.

#index 1083654
#* Entity categorization over large document collections
#@ Venkatesh Ganti;Arnd C. König;Rares Vernica
#t 2008
#c 0
#% 136740
#% 269218
#% 322884
#% 401434
#% 783553
#% 805883
#% 806217
#% 815922
#% 816392
#% 855119
#% 864415
#% 874992
#% 881521
#% 881529
#% 989576
#% 1275182
#% 1289516
#% 1663486
#! Extracting entities (such as people, movies) from documents and identifying the categories (such as painter, writer) they belong to enable structured querying and data analysis over unstructured document collections. In this paper, we focus on the problem of categorizing extracted entities. Most prior approaches developed for this task only analyzed the local document context within which entities occur. In this paper, we significantly improve the accuracy of entity categorization by (i) considering an entity's context across multiple documents containing it, and (ii) exploiting existing large lists of related entities (e.g., lists of actors, directors, books). These approaches introduce computational challenges because (a) the context of entities has to be aggregated across several documents and (b) the lists of related entities may be very large. We develop techniques to address these challenges. We present a thorough experimental study on real data sets that demonstrates the increase in accuracy and the scalability of our approaches.

#index 1083655
#* Knowledge transfer via multiple model local structure mapping
#@ Jing Gao;Wei Fan;Jing Jiang;Jiawei Han
#t 2008
#c 0
#% 229931
#% 236497
#% 269217
#% 424997
#% 729932
#% 769888
#% 786633
#% 983814
#% 983828
#% 989592
#% 1042787
#% 1117009
#% 1272110
#% 1275171
#! The effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribution that generates the training examples and the one from which test examples are to be drawn. The task can be especially difficult when the training examples are from one or several domains different from the test domain. In this paper, we propose a locally weighted ensemble framework to combine multiple models for transfer learning, where the weights are dynamically assigned according to a model's predictive power on each test example. It can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model, which can then be applied on a different domain. Importantly, different from many previously proposed methods, none of the base learning method is required to be specifically designed for transfer learning. We show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer. We then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain, and then weighting each model locally according to its consistency with the neighborhood structure around the test example. Experimental results on text classification, spam filtering and intrusion detection data sets demonstrate significant improvements in classification accuracy gained by the framework. On a transfer learning task of newsgroup message categorization, the proposed locally weighted ensemble framework achieves 97% accuracy when the best single model predicts correctly only on 73% of the test examples. In summary, the improvement in accuracy is over 10% and up to 30% across different problems.

#index 1083656
#* Banded structure in binary matrices
#@ Gemma C. Garriga;Esa Junttila;Heikki Mannila
#t 2008
#c 0
#% 70370
#% 152934
#% 160568
#% 268762
#% 387427
#% 602040
#% 606575
#% 687689
#% 749526
#% 761304
#% 823379
#% 892899
#% 989619
#! A 0--1 matrix has a banded structure if both rows and columns can be permuted so that the non-zero entries exhibit a staircase pattern of overlapping rows. The concept of banded matrices has its origins in numerical analysis, where entries can be viewed as descriptions between the problem variables; the bandedness corresponds to variables that are coupled over short distances. Banded data occurs also in other applications, for example in the physical mapping problem of the human genome, in paleontological data, in network data and in the discovery of overlapping communities without cycles. We study in this paper the banded structure of binary matrices, give a formal definition of the concept and discuss its theoretical properties. We consider the algorithmic problems of computing how far a matrix is from being banded, and of finding a good submatrix of the original data that exhibits approximate bandedness. Finally, we show by experiments on real data from ecology and other applications the usefulness of the concept. Our results reveal that bands exist in real datasets and that the final obtained ordering of rows and columns have natural interpretations.

#index 1083657
#* Quantitative evaluation of approximate frequent pattern mining algorithms
#@ Rohit Gupta;Gang Fang;Blayne Field;Michael Steinbach;Vipin Kumar
#t 2008
#c 0
#% 342610
#% 769905
#% 769957
#% 835018
#% 844394
#% 915216
#! Traditional association mining algorithms use a strict definition of support that requires every item in a frequent itemset to occur in each supporting transaction. In real-life datasets, this limits the recovery of frequent itemset patterns as they are fragmented due to random noise and other errors in the data. Hence, a number of methods have been proposed recently to discover approximate frequent itemsets in the presence of noise. These algorithms use a relaxed definition of support and additional parameters, such as row and column error thresholds to allow some degree of "error" in the discovered patterns. Though these algorithms have been shown to be successful in finding the approximate frequent itemsets, a systematic and quantitative approach to evaluate them has been lacking. In this paper, we propose a comprehensive evaluation framework to compare different approximate frequent pattern mining algorithms. The key idea is to select the optimal parameters for each algorithm on a given dataset and use the itemsets generated with these optimal parameters in order to compare different algorithms. We also propose simple variations of some of the existing algorithms by introducing an additional post-processing step. Subsequently, we have applied our proposed evaluation framework to a wide variety of synthetic datasets with varying amounts of noise and a real dataset to compare existing and our proposed variations of the approximate pattern mining algorithms. Source code and the datasets used in this study are made publicly available.

#index 1083658
#* Unsupervised deduplication using cross-field dependencies
#@ Rob Hall;Charles Sutton;Andrew McCallum
#t 2008
#c 0
#% 722904
#% 788107
#% 838435
#% 915340
#% 937552
#% 939624
#% 997869
#! Recent work in deduplication has shown that collective deduplication of different attribute types can improve performance. But although these techniques cluster the attributes collectively, they do not model them collectively. For example, in citations in the research literature, canonical venue strings and title strings are dependent -- because venues tend to focus on a few research areas -- but this dependence is not modeled by current unsupervised techniques. We call this dependence between fields in a record a cross-field dependence. In this paper, we present an unsupervised generative model for the deduplication problem that explicitly models cross-field dependence. Our model uses a single set of latent variables to control two disparate clustering models: a Dirichlet-multinomial model over titles, and a non-exchangeable string-edit model over venues. We show that modeling cross-field dependence yields a substantial improvement in performance -- a 58% reduction in error over a standard Dirichlet process mixture.

#index 1083659
#* Permu-pattern: discovery of mutable permutation patterns with proximity constraint
#@ Meng Hu;Jiong Yang;Wei Su
#t 2008
#c 0
#% 329537
#% 397383
#% 413550
#% 459006
#% 463903
#% 464996
#% 479971
#% 544054
#% 783497
#% 798033
#% 823384
#% 844401
#% 1386799
#! Pattern discovery in sequences is an important problem in many applications, especially in computational biology and text mining. However, due to the noisy nature of data, the traditional sequential pattern model may fail to reflect the underlying characteristics of sequence data in these applications. There are two challenges: First, the mutation noise exists in the data, and therefore symbols may be misrepresented by other symbols; Secondly, the order of symbols in sequences could be permutated. To address the above problems, in this paper we propose a new sequential pattern model called mutable permutation patterns. Since the Apriori property does not hold for our permutation pattern model, a novel Permu-pattern algorithm is devised to mine frequent mutable permutation patterns from sequence databases. A reachability property is identified to prune the candidate set. Last but not least, we apply the permutation pattern model to a real genome dataset to discover gene clusters, which shows the effectiveness of the model. A large amount of synthetic data is also utilized to demonstrate the efficiency of the Permu-pattern algorithm.

#index 1083660
#* Simultaneous tensor subspace selection and clustering: the equivalence of high order svd and k-means clustering
#@ Heng Huang;Chris Ding;Dijun Luo;Tao Li
#t 2008
#c 0
#% 71174
#% 248027
#% 316150
#% 336073
#% 457831
#% 465928
#% 727684
#% 769911
#% 770769
#% 770830
#% 883843
#% 1022958
#! Singular Value Decomposition (SVD)/Principal Component Analysis (PCA) have played a vital role in finding patterns from many datasets. Recently tensor factorization has been used for data mining and pattern recognition in high index/order data. High Order SVD (HOSVD) is a commonly used tensor factorization method and has recently been used in numerous applications like graphs, videos, social networks, etc. In this paper we prove that HOSVD does simultaneous subspace selection (data compression) and K-means clustering widely used for unsupervised learning tasks. We show how to utilize this new feature of HOSVD for clustering. We demonstrate these new results using three real and large datasets, two on face images datasets and one on hand-written digits dataset. Using this new HOSVD clustering feature we provide a dataset quality assessment on many frequently used experimental datasets with expected noise levels.

#index 1083661
#* Bridging centrality: graph mining from element level to group level
#@ Woochang Hwang;Taehyong Kim;Murali Ramanathan;Aidong Zhang
#t 2008
#c 0
#% 268079
#% 289053
#% 324431
#% 456216
#% 650944
#% 832817
#% 1707190
#! Despite the pervasiveness of networks as models for real world systems ranging from the Internet, the World Wide Web to gene regulation and scientific collaborations, only a limited number of metrics capable of characterizing these systems are available. The existing metrics for characterizing networks have broad specificity and lack the selectivity for many applications. The purpose of this paper is to identify and critically evaluate a metric, termed bridging centrality, which is highly selective for identifying bridges in networks. The properties of bridges are unique compared to the other network metrics. For a diverse range of data sets, we found that networks are highly susceptible to disruption but robust to loss structural integrity upon targeted deletion of bridging nodes. A novel graph clustering approach, termed `bridge cut', utilizing bridging edges as module boundary is also proposed. The modules identified by the bridge cut algorithm are more effective than the other graph clustering methods. Thus, bridging centrality is a network metric with unique properties that may aid in network analysis from element to group level in various areas including systems biology and national security applications.

#index 1083662
#* Interpretable nonnegative matrix decompositions
#@ Saara Hyvönen;Pauli Miettinen;Evimaria Terzi
#t 2008
#c 0
#% 25998
#% 420515
#% 811479
#% 985909
#% 1727914
#! A matrix decomposition expresses a matrix as a product of at least two factor matrices. Equivalently, it expresses each column of the input matrix as a linear combination of the columns in the first factor matrix. The interpretability of the decompositions is a key issue in many data-analysis tasks. We propose two new matrix-decomposition problems: the nonnegative CX and nonnegative CUR problems, that give naturally interpretable factors. They extend the recently-proposed column and column-row based decompositions, and are aimed to be used with nonnegative matrices. Our decompositions represent the input matrix as a nonnegative linear combination of a subset of its columns (or columns and rows). We present two algorithms to solve these problems and provide an extensive experimental evaluation where we assess the quality of our algorithms' results as well as the intuitiveness of nonnegative CX and CUR decompositions. We show that our algorithms return intuitive answers with smaller reconstruction errors than the previously-proposed methods for column and column-row decompositions.

#index 1083663
#* Fast logistic regression for text categorization with variable-length n-grams
#@ Georgiana Ifrim;Gökhan Bakir;Gerhard Weikum
#t 2008
#c 0
#% 68236
#% 260001
#% 279755
#% 280817
#% 397141
#% 420507
#% 458379
#% 465754
#% 478622
#% 569860
#% 577218
#% 588082
#% 722803
#% 735077
#% 746867
#% 881477
#% 881503
#% 881521
#% 1663635
#! A common representation used in text categorization is the bag of words model (aka. unigram model). Learning with this particular representation involves typically some preprocessing, e.g. stopwords-removal, stemming. This results in one explicit tokenization of the corpus. In this work, we introduce a logistic regression approach where learning involves automatic tokenization. This allows us to weaken the a-priori required knowledge about the corpus and results in a tokenization with variable-length (word or character) n-grams as basic tokens. We accomplish this by solving logistic regression using gradient ascent in the space of all ngrams. We show that this can be done very efficiently using a branch and bound approach which chooses the maximum gradient ascent direction projected onto a single dimension (i.e., candidate feature). Although the space is very large, our method allows us to investigate variable-length n-gram learning. We demonstrate the efficiency of our approach compared to state-of-the-art classifiers used for text categorization such as cyclic coordinate descent logistic regression and support vector machines.

#index 1083664
#* Probabilistic latent semantic visualization: topic model for visualizing documents
#@ Tomoharu Iwata;Takeshi Yamada;Naonori Ueda
#t 2008
#c 0
#% 73441
#% 257039
#% 641060
#% 642990
#% 722904
#% 788094
#% 881498
#% 976823
#% 983833
#% 995208
#% 1650298
#% 1663694
#! We propose a visualization method based on a topic model for discrete data such as documents. Unlike conventional visualization methods based on pairwise distances such as multi-dimensional scaling, we consider a mapping from the visualization space into the space of documents as a generative process of documents. In the model, both documents and topics are assumed to have latent coordinates in a two- or three-dimensional Euclidean space, or visualization space. The topic proportions of a document are determined by the distances between the document and the topics in the visualization space, and each word is drawn from one of the topics according to its topic proportions. A visualization, i.e. latent coordinates of documents, can be obtained by fitting the model to a given set of documents using the EM algorithm, resulting in documents with similar topics being embedded close together. We demonstrate the effectiveness of the proposed model by visualizing document and movie data sets, and quantitatively compare it with conventional visualization methods.

#index 1083665
#* Automatic identification of quasi-experimental designs for discovering causal knowledge
#@ David D. Jensen;Andrew S. Fast;Brian J. Taylor;Marc E. Maier
#t 2008
#c 0
#% 287631
#% 297171
#% 368551
#% 723406
#% 1390164
#% 1415854
#! Researchers in the social and behavioral sciences routinely rely on quasi-experimental designs to discover knowledge from large data-bases. Quasi-experimental designs (QEDs) exploit fortuitous circumstances in non-experimental data to identify situations (sometimes called "natural experiments") that provide the equivalent of experimental control and randomization. QEDs allow researchers in domains as diverse as sociology, medicine, and marketing to draw reliable inferences about causal dependencies from non-experimental data. Unfortunately, identifying and exploiting QEDs has remained a painstaking manual activity, requiring researchers to scour available databases and apply substantial knowledge of statistics. However, recent advances in the expressiveness of databases, and increases in their size and complexity, provide the necessary conditions to automatically identify QEDs. In this paper, we describe the first system to discover knowledge by applying quasi-experimental designs that were identified automatically. We demonstrate that QEDs can be identified in a traditional database schema and that such identification requires only a small number of extensions to that schema, knowledge about quasi-experimental design encoded in first-order logic, and a theorem-proving engine. We describe several key innovations necessary to enable this system, including methods for automatically constructing appropriate experimental units and for creating aggregate variables on those units. We show that applying the resulting designs can identify important causal dependencies in real domains, and we provide examples from academic publishing, movie making and marketing, and peer-production systems. Finally, we discuss the integration of QEDs with other approaches to causal discovery, including joint modeling and directed experimentation.

#index 1083666
#* Extracting shared subspace for multi-label classification
#@ Shuiwang Ji;Lei Tang;Shipeng Yu;Jieping Ye
#t 2008
#c 0
#% 80995
#% 458379
#% 465754
#% 577287
#% 723239
#% 763699
#% 763708
#% 810932
#% 818234
#% 829010
#% 838412
#% 884074
#% 906025
#% 916788
#% 983806
#% 989655
#! Multi-label problems arise in various domains such as multi-topic document categorization and protein function prediction. One natural way to deal with such problems is to construct a binary classifier for each label, resulting in a set of independent binary classification problems. Since the multiple labels share the same input space, and the semantics conveyed by different labels are usually correlated, it is essential to exploit the correlation information contained in different labels. In this paper, we consider a general framework for extracting shared structures in multi-label classification. In this framework, a common subspace is assumed to be shared among multiple labels. We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem, though the problem is non-convex. For high-dimensional problems, direct computation of the solution is expensive, and we develop an efficient algorithm for this case. One appealing feature of the proposed framework is that it includes several well-known algorithms as special cases, thus elucidating their intrinsic relationships. We have conducted extensive experiments on eleven multi-topic web page categorization tasks, and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms.

#index 1083667
#* Mining preferences from superior and inferior examples
#@ Bin Jiang;Jian Pei;Xuemin Lin;David W. Cheung;Jiawei Han
#t 2008
#c 0
#% 288976
#% 300170
#% 331835
#% 376266
#% 408396
#% 420117
#% 458873
#% 465167
#% 566111
#% 577224
#% 629667
#% 729437
#% 813974
#% 992635
#% 993957
#% 994017
#% 1026877
#% 1272396
#! Mining user preferences plays a critical role in many important applications such as customer relationship management (CRM), product and service recommendation, and marketing campaigns. In this paper, we identify an interesting and practical problem of mining user preferences: in a multidimensional space where the user preferences on some categorical attributes are unknown, from some superior and inferior examples provided by a user, can we learn about the user's preferences on those categorical attributes? We model the problem systematically and show that mining user preferences from superior and inferior examples is challenging. Although the problem has great potential in practice, to the best of our knowledge, it has not been explored systematically before. As the first attempt to tackle the problem, we propose a greedy method and show that our method is practical using real data sets and synthetic data sets.

#index 1083668
#* Effective and efficient itemset pattern summarization: regression-based approaches
#@ Ruoming Jin;Muad Abu-Ata;Yang Xiang;Ning Ruan
#t 2008
#c 0
#% 152934
#% 224113
#% 248791
#% 316709
#% 338609
#% 463903
#% 471500
#% 481290
#% 577218
#% 629644
#% 727667
#% 729941
#% 731608
#% 742493
#% 769876
#% 823356
#% 824710
#% 864404
#% 881500
#% 881542
#% 948087
#% 1016244
#% 1810385
#! In this paper, we propose a set of novel regression-based approaches to effectively and efficiently summarize frequent itemset patterns. Specifically, we show that the problem of minimizing the restoration error for a set of itemsets based on a probabilistic model corresponds to a non-linear regression problem. We show that under certain conditions, we can transform the nonlinear regression problem to a linear regression problem. We propose two new methods, k-regression and tree-regression, to partition the entire collection of frequent itemsets in order to minimize the restoration error. The K-regression approach, employing a K-means type clustering method, guarantees that the total restoration error achieves a local minimum. The tree-regression approach employs a decision-tree type of top-down partition process. In addition, we discuss alternatives to estimate the frequency for the collection of itemsets being covered by the k representative itemsets. The experimental evaluation on both real and synthetic datasets demonstrates that our approaches significantly improve the summarization performance in terms of both accuracy (restoration error), and computational cost.

#index 1083669
#* A sequential dual method for large scale multi-class linear svms
#@ S. Sathiya Keerthi;S. Sundararajan;Kai-Wei Chang;Cho-Jui Hsieh;Chih-Jen Lin
#t 2008
#c 0
#% 33917
#% 116149
#% 131165
#% 269217
#% 562950
#% 722816
#% 722903
#% 763699
#% 763708
#% 829043
#% 881477
#% 983815
#% 983905
#% 989644
#% 1073923
#% 1117688
#% 1264133
#% 1860941
#% 1861002
#! Efficient training of direct multi-class formulations of linear Support Vector Machines is very useful in applications such as text classification with a huge number examples as well as features. This paper presents a fast dual method for this training. The main idea is to sequentially traverse through the training set and optimize the dual variables associated with one example at a time. The speed of training is enhanced further by shrinking and cooling heuristics. Experiments indicate that our method is much faster than state of the art solvers such as bundle, cutting plane and exponentiated gradient methods.

#index 1083670
#* Constructing comprehensive summaries of large event sequences
#@ Jerry Kiernan;Evimaria Terzi
#t 2008
#c 0
#% 326303
#% 338425
#% 342633
#% 369349
#% 397383
#% 463903
#% 466506
#% 520221
#% 577226
#% 810058
#% 875024
#% 949146
#% 989609
#% 993961
#! Event sequences capture system and user activity over time. Prior research on sequence mining has mostly focused on discovering local patterns. Though interesting, these patterns reveal local associations and fail to give a comprehensive summary of the entire event sequence. Moreover, the number of patterns discovered can be large. In this paper, we take an alternative approach and build short summaries that describe the entire sequence, while revealing local associations among events. We formally define the summarization problem as an optimization problem that balances between shortness of the summary and accuracy of the data description. We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms. We also explore more efficient greedy alternatives and demonstrate that they work well on large datasets. Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results, and reveal interesting local structures in the data.

#index 1083671
#* Factorization meets the neighborhood: a multifaceted collaborative filtering model
#@ Yehuda Koren
#t 2008
#c 0
#% 124010
#% 280852
#% 319705
#% 330687
#% 397153
#% 452563
#% 722904
#% 734592
#% 813966
#% 983903
#% 989580
#% 1038334
#% 1038335
#% 1207139
#% 1345710
#! Recommender systems provide users with personalized suggestions for products or services. These systems often rely on Collaborating Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which directly profile both users and products, and neighborhood models, which analyze similarities between products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby building a more accurate combined model. Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a new evaluation metric, which highlights the differences among methods, based on their performance at a top-K recommendation task.

#index 1083672
#* The structure of information pathways in a social communication network
#@ Gueorgi Kossinets;Jon Kleinberg;Duncan Watts
#t 2008
#c 0
#% 35764
#% 287267
#% 300115
#% 320187
#% 646279
#% 754107
#% 868469
#% 1055763
#! Social networks are of interest to researchers in part because they are thought to mediate the flow of information in communities and organizations. Here we study the temporal dynamics of communication using on-line data, including e-mail communication among the faculty and staff of a large university over a two-year period. We formulate a temporal notion of "distance" in the underlying social network by measuring the minimum time required for information to spread from one node to another - a concept that draws on the notion of vector-clocks from the study of distributed computing systems. We find that such temporal measures provide structural insights that are not apparent from analyses of the pure social network topology. In particular, we define the network backbone to be the subgraph consisting of edges on which information has the potential to flow the quickest. We find that the backbone is a sparse graph with a concentration of both highly embedded edges and long-range bridges - a finding that sheds new light on the relationship between tie strength and connectivity in social networks.

#index 1083673
#* Angle-based outlier detection in high-dimensional data
#@ Hans-Peter Kriegel;Matthias S hubert;Arthur Zimek
#t 2008
#c 0
#% 230138
#% 296697
#% 300136
#% 300183
#% 310552
#% 333929
#% 342625
#% 342641
#% 459025
#% 478624
#% 479791
#% 479986
#% 480132
#% 501988
#% 581575
#% 721137
#% 729912
#% 731607
#% 784995
#% 785358
#% 844420
#% 915318
#% 1669935
#% 1669937
#! Detecting outliers in a large set of data objects is a major data mining task aiming at finding different mechanisms responsible for different groups of objects in a data set. All existing approaches, however, are based on an assessment of distances (sometimes indirectly by assuming certain distributions) in the full-dimensional Euclidean data space. In high-dimensional data, these approaches are bound to deteriorate due to the notorious "curse of dimensionality". In this paper, we propose a novel approach named ABOD (Angle-Based Outlier Detection) and some variants assessing the variance in the angles between the difference vectors of a point to the other points. This way, the effects of the "curse of dimensionality" are alleviated compared to purely distance-based approaches. A main advantage of our new approach is that our method does not rely on any parameter selection influencing the quality of the achieved ranking. In a thorough experimental evaluation, we compare ABOD to the well-established distance-based method LOF for various artificial and a real world data set and show ABOD to perform especially well on high-dimensional data.

#index 1083674
#* Stream prediction using a generative model based on frequent episodes in event sequences
#@ Srivatsan Laxman;Vikram Tankasali;Ryen W. White
#t 2008
#c 0
#% 1267
#% 420063
#% 624552
#% 629640
#% 734050
#% 807394
#% 832572
#% 989612
#% 1055851
#% 1117062
#% 1275193
#! This paper presents a new algorithm for sequence prediction over long categorical event streams. The input to the algorithm is a set of target event types whose occurrences we wish to predict. The algorithm examines windows of events that precede occurrences of the target event types in historical data. The set of significant frequent episodes associated with each target event type is obtained based on formal connections between frequent episodes and Hidden Markov Models (HMMs). Each significant episode is associated with a specialized HMM, and a mixture of such HMMs is estimated for every target event type. The likelihoods of the current window of events, under these mixture models, are used to predict future occurrences of target events in the data. The only user-defined model parameter in the algorithm is the length of the windows of events used during model estimation. We first evaluate the algorithm on synthetic data that was generated by embedding (in varying levels of noise) patterns which are preselected to characterize occurrences of target events. We then present an application of the algorithm for predicting targeted user-behaviors from large volumes of anonymous search session interaction logs from a commercially-deployed web browser tool-bar.

#index 1083675
#* Microscopic evolution of social networks
#@ Jure Leskovec;Lars Backstrom;Ravi Kumar;Andrew Tomkins
#t 2008
#c 0
#% 283833
#% 309749
#% 593994
#% 730089
#% 754058
#% 756015
#% 875958
#% 881460
#% 881523
#% 933558
#% 937549
#% 983866
#% 1396209
#! We present a detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals. For the first time at such a large scale, we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks. Using a methodology based on the maximum-likelihood principle, we investigate a wide variety of network formation strategies, and show that edge locality plays a critical role in evolution of networks. Our findings supplement earlier network models based on the inherently non-local preferential attachment. Based on our observations, we develop a complete model of network evolution, where nodes arrive at a prespecified rate and select their lifetimes. Each node then independently initiates edges according to a "gap" process, selecting a destination for each edge according to a simple triangle-closing model free of any parameters. We show analytically that the combination of the gap distribution with the node lifetime leads to a power law out-degree distribution that accurately reflects the true network in all four cases. Finally, we give model parameter settings that allow automatic evolution and generation of realistic synthetic networks of arbitrary scale.

#index 1083676
#* Cut-and-stitch: efficient parallel learning of linear dynamical systems on smps
#@ Lei Li;Wenjie Fu;Fan Guo;Todd C. Mowry;Christos Faloutsos
#t 2008
#c 0
#% 823384
#% 870132
#% 891559
#% 946709
#% 963669
#% 1017256
#! Multi-core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing. Our goal is to make use of the multi-core as well as multi-processor architectures to speed up data mining algorithms. Specifically, we present a parallel algorithm for approximate learning of Linear Dynamical Systems (LDS), also known as Kalman Filters (KF). LDSs are widely used in time series analysis such as motion capture modeling, visual tracking etc. We propose Cut-And-Stitch (CAS), a novel method to handle the data dependencies from the chain structure of hidden variables in LDS, so as to parallelize the EM-based parameter learning algorithm. We implement the algorithm using OpenMP on both a supercomputer and a quad-core commercial desktop. The experimental results show that parallel algorithms using Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the serial version. In addition, Cut-And-Stitch can be generalized to other models with similar linear structures such as Hidden Markov Models (HMM) and Switching Kalman Filters (SKF).

#index 1083677
#* Active learning with direct query construction
#@ Charles X. Ling;Jun Du
#t 2008
#c 0
#% 116165
#% 136350
#% 170649
#% 236729
#% 376266
#% 400847
#% 447606
#% 450951
#% 451056
#% 464268
#% 565531
#% 580510
#% 722797
#% 735357
#% 735358
#% 763705
#% 926881
#% 1289639
#% 1699589
#! Active learning may hold the key for solving the data scarcity problem in supervised learning, i.e., the lack of labeled data. Indeed, labeling data is a costly process, yet an active learner may request labels of only selected instances, thus reducing labeling work dramatically. Most previous works of active learning are, however, pool-based; that is, a pool of unlabeled examples is given and the learner can only select examples from the pool to query for their labels. This type of active learning has several weaknesses. In this paper we propose novel active learning algorithms that construct examples directly to query for labels. We study both a specific active learner based on the decision tree algorithm, and a general active learner that can work with any base learning algorithm. As there is no restriction on what examples to be queried, our methods are shown to often query fewer examples to reduce the predictive error quickly. This casts doubt on the usefulness of the pool in pool-based active learning. Nevertheless, our methods can be easily adapted to work with a given pool of unlabeled examples.

#index 1083678
#* Spectral domain-transfer learning
#@ Xiao Ling;Wenyuan Dai;Gui-Rong Xue;Qiang Yang;Yong Yu
#t 2008
#c 0
#% 236497
#% 313959
#% 465754
#% 466263
#% 466675
#% 466890
#% 770847
#% 770858
#% 840898
#% 879615
#% 983814
#% 983828
#% 989592
#% 1261539
#% 1269755
#% 1272110
#% 1279294
#% 1290055
#! Traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain. In many real world applications, however, we wish to make use of the labeled data from one domain (called in-domain) to classify the unlabeled data in a different domain (out-of-domain). This problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain. In general, this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain. In this paper, we formulate this domain-transfer learning problem under a novel spectral classification framework, where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure. Through optimization of the cost function, the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain. We conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many state-of-the-art algorithms.

#index 1083679
#* Mining multi-faceted overviews of arbitrary topics in a text collection
#@ Xu Ling;Qiaozhu Mei;ChengXiang Zhai;Bruce Schatz
#t 2008
#c 0
#% 78171
#% 214711
#% 218992
#% 280819
#% 281186
#% 297550
#% 325001
#% 375017
#% 722904
#% 754107
#% 766433
#% 769967
#% 805873
#% 823332
#% 881529
#% 940000
#% 956510
#% 987203
#% 992299
#% 1055681
#% 1055683
#! A common task in many text mining applications is to generate a multi-faceted overview of a topic in a text collection. Such an overview not only directly serves as an informative summary of the topic, but also provides a detailed view of navigation to different facets of the topic. Existing work has cast this problem as a categorization problem and requires training examples for each facet. This has three limitations: (1) All facets are predefined, which may not fit the need of a particular user. (2) Training examples for each facet are often unavailable. (3) Such an approach only works for a predefined type of topics. In this paper, we break these limitations and study a more realistic new setup of the problem, in which we would allow a user to flexibly describe each facet with keywords for an arbitrary topic and attempt to mine a multi-faceted overview in an unsupervised way. We attempt a probabilistic approach to solve this problem. Empirical experiments on different genres of text data show that our approach can effectively generate a multi-faceted overview for arbitrary topics; the generated overviews are comparable with those generated by supervised methods with training examples. They are also more informative than unstructured flat summaries. The method is quite general, thus can be applied to multiple text mining tasks in different application domains.

#index 1083680
#* Multi-class cost-sensitive boosting with p-norm loss functions
#@ Aurélie C. Lozano;Naoki Abe
#t 2008
#c 0
#% 136350
#% 169684
#% 235377
#% 280437
#% 290482
#% 302391
#% 342611
#% 342647
#% 466268
#% 466561
#% 714684
#% 727925
#% 769875
#% 1250597
#% 1814726
#! We propose a family of novel cost-sensitive boosting methods for multi-class classification by applying the theory of gradient boosting to p-norm based cost functionals. We establish theoretical guarantees including proof of convergence and convergence rates for the proposed methods. Our theoretical treatment provides interpretations for some of the existing algorithms in terms of the proposed family, including a generalization of the costing algorithm, DSE and GBSE-t, and the Average Cost method. We also experimentally evaluate the performance of our new algorithms against existing methods of cost sensitive boosting, including AdaCost, CSB2, and AdaBoost.M2 with cost-sensitive weight initialization. We show that our proposed scheme generally achieves superior results in terms of cost minimization and, with the use of higher order p-norm loss in certain cases, consistently outperforms the comparison methods, thus establishing its empirical advantage.

#index 1083681
#* On updates that constrain the features' connections during learning
#@ Omid Madani;Jian Huang
#t 2008
#c 0
#% 227736
#% 309141
#% 344447
#% 384911
#% 529804
#% 722924
#% 729437
#% 763699
#% 763708
#% 829007
#% 829975
#% 961152
#! In many multiclass learning scenarios, the number of classes is relatively large (thousands,...), or the space and time efficiency of the learning system can be crucial. We investigate two online update techniques especially suited to such problems. These updates share a sparsity preservation capacity: they allow for constraining the number of prediction connections that each feature can make. We show that one method, exponential moving average, is solving a "discrete" regression problem for each feature, changing the weights in the direction of minimizing the quadratic loss. We design the other method to improve a hinge loss subject to constraints, for better accuracy. We empirically explore the methods, and compare performance to previous indexing techniques, developed with the same goals, as well as other online algorithms based on prototype learning. We observe that while the classification accuracies are very promising, improving over previous indexing techniques, the scalability benefits are preserved.

#index 1083682
#* Weighted graphs and disconnected components: patterns and a generator
#@ Mary McGlohon;Leman Akoglu;Christos Faloutsos
#t 2008
#c 0
#% 283833
#% 438136
#% 577219
#% 799636
#% 823342
#% 867050
#% 880398
#% 881523
#% 956513
#% 989580
#% 989587
#% 1673564
#! The vast majority of earlier work has focused on graphs which are both connected (typically by ignoring all but the giant connected component), and unweighted. Here we study numerous, real, weighted graphs, and report surprising discoveries on the way in which new nodes join and form links in a social network. The motivating questions were the following: How do connected components in a graph form and change over time? What happens after new nodes join a network -- how common are repeated edges? We study numerous diverse, real graphs (citation networks, networks in social media, internet traffic, and others); and make the following contributions: (a) we observe that the non-giant connected components seem to stabilize in size, (b) we observe the weights on the edges follow several power laws with surprising exponents, and (c) we propose an intuitive, generative model for graph growth that obeys observed patterns.

#index 1083683
#* Finding non-redundant, statistically significant regions in high dimensional data: a novel approach to projected and subspace clustering
#@ Gabriela Moise;Jörg Sander
#t 2008
#c 0
#% 248792
#% 273891
#% 280417
#% 397384
#% 424759
#% 481290
#% 765518
#% 778729
#% 785355
#% 789010
#% 796202
#% 800529
#% 844313
#% 881458
#% 915305
#% 1117035
#% 1408779
#! Projected and subspace clustering algorithms search for clusters of points in subsets of attributes. Projected clustering computes several disjoint clusters, plus outliers, so that each cluster exists in its own subset of attributes. Subspace clustering enumerates clusters of points in all subsets of attributes, typically producing many overlapping clusters. One problem of existing approaches is that their objectives are stated in a way that is not independent of the particular algorithm proposed to detect such clusters. A second problem is the definition of cluster density based on user-defined parameters, which makes it hard to assess whether the reported clusters are an artifact of the algorithm or whether they actually stand out in the data in a statistical sense. We propose a novel problem formulation that aims at extracting axis-parallel regions that stand out in the data in a statistical sense. The set of axis-parallel, statistically significant regions that exist in a given data set is typically highly redundant. Therefore, we formulate the problem of representing this set through a reduced, non-redundant set of axis-parallel, statistically significant regions as an optimization problem. Exhaustive search is not a viable solution due to computational infeasibility, and we propose the approximation algorithm STATPC. Our comprehensive experimental evaluation shows that STATPC significantly outperforms existing projected and subspace clustering algorithms in terms of accuracy.

#index 1083684
#* Joint latent topic models for text and citations
#@ Ramesh M. Nallapati;Amr Ahmed;Eric P. Xing;William W. Cohen
#t 2008
#c 0
#% 290830
#% 722904
#% 730089
#% 875959
#% 876017
#% 891559
#% 983833
#% 987287
#% 989623
#% 989633
#! In this work, we address the problem of joint modeling of text and citations in the topic modeling framework. We present two different models called the Pairwise-Link-LDA and the Link-PLSA-LDA models. The Pairwise-Link-LDA model combines the ideas of LDA [4] and Mixed Membership Block Stochastic Models [1] and allows modeling arbitrary link structure. However, the model is computationally expensive, since it involves modeling the presence or absence of a citation (link) between every pair of documents. The second model solves this problem by assuming that the link structure is a bipartite graph. As the name indicates, Link-PLSA-LDA model combines the LDA and PLSA models into a single graphical model. Our experiments on a subset of Citeseer data show that both these models are able to predict unseen data better than the baseline model of Erosheva and Lafferty [8], by capturing the notion of topical similarity between the contents of the cited and citing documents. Our experiments on two different data sets on the link prediction task show that the Link-PLSA-LDA model performs the best on the citation prediction task, while also remaining highly scalable. In addition, we also present some interesting visualizations generated by each of the models.

#index 1083685
#* Classification with partial labels
#@ Nam Nguyen;Rich Caruana
#t 2008
#c 0
#% 464291
#% 464608
#% 464631
#% 722816
#% 770782
#% 770798
#% 812372
#% 852097
#% 983830
#% 983905
#% 983943
#! In this paper, we address the problem of learning when some cases are fully labeled while other cases are only partially labeled, in the form of partial labels. Partial labels are represented as a set of possible labels for each training example, one of which is the correct label. We introduce a discriminative learning approach that incorporates partial label information into the conventional margin-based learning framework. The partial label learning problem is formulated as a convex quadratic optimization minimizing the L2-norm regularized empirical risk using hinge loss. We also present an efficient algorithm for classification in the presence of partial labels. Experiments with different data sets show that partial label information improves the performance of classification when there is traditional fully-labeled data, and also yields reasonable performance in the absence of any fully labeled data.

#index 1083686
#* Discrimination-aware data mining
#@ Dino Pedreshi;Salvatore Ruggieri;Franco Turini
#t 2008
#c 0
#% 300184
#% 481290
#% 715344
#% 751575
#! In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority, without regard to individual merit. Rules extracted from databases by data mining techniques, such as classification or association rules, when used for decision tasks such as benefit or credit approval, can be discriminatory in the above sense. In this paper, the notion of discriminatory classification rules is introduced and studied. Providing a guarantee of non-discrimination is shown to be a non trivial task. A naive approach, like taking away all discriminatory attributes, is shown to be not enough when other background knowledge is available. Our approach leads to a precise formulation of the redlining problem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowledge. An empirical assessment of the results on the German credit dataset is also provided.

#index 1083687
#* Fast collapsed gibbs sampling for latent dirichlet allocation
#@ Ian Porteous;David Newman;Alexander Ihler;Arthur Asuncion;Padhraic Smyth;Max Welling
#t 2008
#c 0
#% 280463
#% 304932
#% 321455
#% 466425
#% 722904
#% 779875
#% 876017
#% 879587
#% 967299
#% 967300
#% 1344738
#! In this paper we introduce a novel collapsed Gibbs sampling method for the widely used latent Dirichlet allocation (LDA) model. Our new method results in significant speedups on real world text corpora. Conventional Gibbs sampling schemes for LDA require O(K) operations per sample where K is the number of topics in the model. Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample. On real-word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA. No approximations are necessary, and we show that our fast sampling scheme produces exactly the same results as the standard (but slower) sampling scheme. Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes. For the PubMed collection of over 8 million documents with a required computation time of 6 CPU months for LDA, our speedup of 5.7 can save 5 CPU months of computation.

#index 1083688
#* Partial least squares regression for graph mining
#@ Hiroto Saigo;Nicole Krämer;Koji Tsuda
#t 2008
#c 0
#% 243728
#% 299985
#% 342604
#% 425033
#% 425063
#% 568589
#% 629708
#% 769951
#% 785396
#% 813990
#% 857307
#% 876064
#% 876070
#% 905703
#% 983859
#% 983919
#% 1041287
#% 1063502
#% 1663621
#% 1742155
#! Attributed graphs are increasingly more common in many application domains such as chemistry, biology and text processing. A central issue in graph mining is how to collect informative subgraph patterns for a given learning task. We propose an iterative mining method based on partial least squares regression (PLS). To apply PLS to graph data, a sparse version of PLS is developed first and then it is combined with a weighted pattern mining algorithm. The mining algorithm is iteratively called with different weight vectors, creating one latent component per one mining call. Our method, graph PLS, is efficient and easy to implement, because the weight vector is updated with elementary matrix calculations. In experiments, our graph PLS algorithm showed competitive prediction accuracies in many chemical datasets and its efficiency was significantly superior to graph boosting (gBoost) and the naive method based on frequent graph mining.

#index 1083689
#* Knowledge discovery of semantic relationships between words using nonparametric bayesian graph model
#@ Issei Sato;Minoru Yoshida;Hiroshi Nakagawa
#t 2008
#c 0
#% 747738
#% 811281
#% 939545
#% 1250567
#! We developed a model based on nonparametric Bayesian modeling for automatic discovery of semantic relationships between words taken from a corpus. It is aimed at discovering semantic knowledge about words in particular domains, which has become increasingly important with the growing use of text mining, information retrieval, and speech recognition. The subject-predicate structure is taken as a syntactic structure with the noun as the subject and the verb as the predicate. This structure is regarded as a graph structure. The generation of this graph can be modeled using the hierarchical Dirichlet process and the Pitman-Yor process. The probabilistic generative model we developed for this graph structure consists of subject-predicate structures extracted from a corpus. Evaluation of this model by measuring the performance of graph clustering based on WordNet similarities demonstrated that it outperforms other baseline models.

#index 1083690
#* Mobile call graphs: beyond power-law and lognormal distributions
#@ Mukund Seshadri;Sridhar Machiraju;Ashwin Sridharan;Jean Bolot;Christos Faloutsos;Jure Leskove
#t 2008
#c 0
#% 283833
#% 309749
#% 340679
#% 342592
#% 479969
#% 549441
#% 808658
#% 907530
#% 1300556
#! We analyze a massive social network, gathered from the records of a large mobile phone operator, with more than a million users and tens of millions of calls. We examine the distributions of the number of phone calls per customer; the total talk minutes per customer; and the distinct number of calling partners per customer. We find that these distributions are skewed, and that they significantly deviate from what would be expected by power-law and lognormal distributions. To analyze our observed distributions (of number of calls, distinct call partners, and total talk time), we propose PowerTrack , a method which fits a lesser known but more suitable distribution, namely the Double Pareto LogNormal (DPLN) distribution, to our data and track its parameters over time. Using PowerTrack , we find that our graph changes over time in a way consistent with a generative process that naturally results in the DPLN distributions we observe. Furthermore, we show that this generative process lends itself to a natural and appealing social wealth interpretation in the context of social networks such as ours. We discuss the application of those results to our model and to forecasting.

#index 1083691
#* Efficient ticket routing by resolution sequence mining
#@ Qihong Shao;Yi Chen;Shu Tao;Xifeng Yan;Nikos Anerousis
#t 2008
#c 0
#% 258498
#% 259602
#% 329537
#% 420063
#% 459021
#% 463903
#% 464996
#% 577256
#% 630984
#% 631914
#% 768666
#% 772836
#% 864813
#% 1678914
#% 1709206
#% 1808839
#! IT problem management calls for quick identification of resolvers to reported problems. The efficiency of this process highly depends on ticket routing---transferring problem ticket among various expert groups in search of the right resolver to the ticket. To achieve efficient ticket routing, wise decision needs to be made at each step of ticket transfer to determine which expert group is likely to be, or to lead to the resolver. In this paper, we address the possibility of improving ticket routing efficiency by mining ticket resolution sequences alone, without accessing ticket content. To demonstrate this possibility, a Markov model is developed to statistically capture the right decisions that have been made toward problem resolution, where the order of the Markov model is carefully chosen according to the conditional entropy obtained from ticket data. We also design a search algorithm, called Variable-order Multiple active State search(VMS), that generates ticket transfer recommendations based on our model. The proposed framework is evaluated on a large set of real-world problem tickets. The results demonstrate that VMS significantly improves human decisions: Problem resolvers can often be identified with fewer ticket transfers.

#index 1083692
#* Get another label? improving data quality and data mining using multiple, noisy labelers
#@ Victor S. Sheng;Foster Provost;Panagiotis G. Ipeirotis
#t 2008
#c 0
#% 124467
#% 136350
#% 170649
#% 219753
#% 280437
#% 400847
#% 443509
#% 727925
#% 735358
#% 763705
#% 785413
#% 829985
#% 829994
#% 832575
#% 844399
#% 926881
#% 959215
#% 1272000
#% 1272369
#% 1289281
#% 1289639
#% 1673023
#% 1699589
#! This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.

#index 1083693
#* iSAX: indexing and mining terabyte sized time series
#@ Jin Shieh;Eamonn Keogh
#t 2008
#c 0
#% 172949
#% 285711
#% 477482
#% 729954
#% 765451
#% 800574
#% 876074
#% 992857
#% 1044456
#! Current research in indexing and mining time series data has produced many interesting algorithms and representations. However, the algorithms and the size of data considered have generally not been representative of the increasingly massive datasets encountered in science, engineering, and business domains. In this work, we show how a novel multi-resolution symbolic representation can be used to index datasets which are several orders of magnitude larger than anything else considered in the literature. Our approach allows both fast exact search and ultra fast approximate search. We show how to exploit the combination of both types of search as sub-routines in data mining algorithms, allowing for the exact mining of truly massive real world datasets, containing millions of time series.

#index 1083694
#* Efficient computation of personal aggregate queries on blogs
#@ Ka Cheung Sia;Junghoo Cho;Yun Chi;Belle L. Tseng
#t 2008
#c 0
#% 333854
#% 643008
#% 754107
#% 793248
#% 810013
#% 864454
#% 869516
#% 869536
#% 874975
#% 875022
#% 881468
#% 956521
#% 960244
#% 989626
#% 989650
#% 995168
#! There is an exploding amount of user-generated content on theWeb due to the emergence of "Web 2.0" services, such as Blogger,MySpace, Flickr, and del.icio.us. The participation of a large number of users in sharing their opinion on the Web has inspired researchers to build an effective "information filter" by aggregating these independent opinions. However, given the diverse groups of users on the Web nowadays, the global aggregation of the information may not be of much interest to different groups of users. In this paper, we explore the possibility of computing personalized aggregation over the opinions expressed on the Web based on a user's indication of trust over the information sources. The hope is that by employing such "personalized" aggregation, we can make the recommendation more likely to be interesting to the users. We address the challenging scalability issues by proposing an efficient method, that utilizes two core techniques: Non-Negative Matrix Factorization and Threshold Algorithm, to compute personalized aggregations when there are potentially millions of users and millions of sources within a system. We show that, through experiments on real-life dataset, our personalized aggregation approach indeed makes a significant difference in the items that are recommended and it reduces the query computational cost significantly, often more than 75%, while the result of personalized aggregation is kept accurate enough.

#index 1083695
#* Semi-supervised approach to rapid and reliable labeling of large data sets
#@ György J. Simon;Vipin Kumar;Zhi-Li Zhang
#t 2008
#c 0
#% 96688
#% 116165
#% 165663
#% 232319
#% 236729
#% 240794
#% 344732
#% 391356
#% 466263
#% 755462
#% 781694
#% 821929
#% 879624
#% 961177
#% 1094205
#% 1455666
#% 1715787
#! In this paper, we propose a method, where the labeling of the data set is carried out in a semi-supervised manner with user-specified guarantees about the quality of the labeling. In our scheme, we assume that for each class, we have some heuristics available, each of which can identify instances of one particular class. The heuristics are assumed to have reasonable performance but they do not need to cover all instances of the class nor do they need to be perfectly reliable. We further assume that we have an infallible expert, who is willing to manually label a few instances. The aim of the algorithm is to exploit the cluster structure of the problem, the predictions by the imperfect heuristics and the limited perfect labels provided by the expert to classify (label) the instances of the data set with guaranteed precision (specificed by the user) with regards to each class. The specified precision is not always attainable, so the algorithm is allowed to classify some instances as dontknow. The algorithm is evaluated by the number of instances labeled by the expert, the number of dontknow instances (global coverage) and the achieved quality of the labeling. On the KDD Cup Network Intrusion data set containing 500,000 instances, we managed to label 96.6% of the instances while guaranteeing a nominal precision of 90% (with 95% confidence) by having the expert label 630 instances; and by having the expert label 1200 instances, we managed to guarantee 95% nominal precision while labeling 96.4% of the data. We also provide a case study of applying our scheme to label the network traffic collected at a large campus network.

#index 1083696
#* Relational learning via collective matrix factorization
#@ Ajit P. Singh;Geoffrey J. Gordon
#t 2008
#c 0
#% 184486
#% 274189
#% 280819
#% 287631
#% 382854
#% 425021
#% 562954
#% 757953
#% 818234
#% 840924
#% 876018
#% 876031
#% 881502
#% 916785
#% 983875
#% 987253
#% 989573
#% 989618
#! Relational learning is concerned with predicting unknown values of a relation, given a database of entities and observed relations among entities. An example of relational learning is movie rating prediction, where entities could include users, movies, genres, and actors. Relations encode users' ratings of movies, movies' genres, and actors' roles in movies. A common prediction technique given one pairwise relation, for example a #users x #movies ratings matrix, is low-rank matrix factorization. In domains with multiple relations, represented as multiple matrices, we may improve predictive accuracy by exploiting information from one relation while predicting another. To this end, we propose a collective matrix factorization model: we simultaneously factor several matrices, sharing parameters among factors when an entity participates in multiple relations. Each relation can have a different value type and error distribution; so, we allow nonlinear relationships between the parameters and outputs, using Bregman divergences to measure error. We extend standard alternating projection algorithms to our model, and derive an efficient Newton update for the projection. Furthermore, we propose stochastic optimization methods to deal with large, sparse matrices. Our model generalizes several existing matrix factorization methods, and therefore yields new large-scale optimization algorithms for these problems. Our model can handle any pairwise relational schema and a wide variety of error models. We demonstrate its efficiency, as well as the benefit of sharing parameters among relations.

#index 1083697
#* A bayesian mixture model with linear regression mixing proportions
#@ Xiuyao Song;Chris Jermaine;Sanjay Ranka;John Gums
#t 2008
#c 0
#% 342600
#% 345829
#% 450519
#% 643520
#% 722904
#% 729960
#% 769880
#% 823346
#% 875959
#% 881476
#% 881498
#% 881514
#% 989586
#% 1015261
#! Classic mixture models assume that the prevalence of the various mixture components is fixed and does not vary over time. This presents problems for applications where the goal is to learn how complex data distributions evolve. We develop models and Bayesian learning algorithms for inferring the temporal trends of the components in a mixture model as a function of time. We show the utility of our models by applying them to the real-life problem of tracking changes in the rates of antibiotic resistance in Escherichia coli and Staphylococcus aureus. The results show that our methods can derive meaningful temporal antibiotic resistance patterns.

#index 1083698
#* Hypergraph spectral learning for multi-label classification
#@ Liang Sun;Shuiwang Ji;Jieping Ye
#t 2008
#c 0
#% 274703
#% 317525
#% 465754
#% 855563
#% 875947
#% 884074
#% 889101
#% 891559
#% 902459
#% 950571
#% 975142
#% 983940
#% 989655
#% 1034713
#% 1455666
#% 1838081
#! A hypergraph is a generalization of the traditional graph in which the edges are arbitrary non-empty subsets of the vertex set. It has been applied successfully to capture high-order relations in various domains. In this paper, we propose a hypergraph spectral learning formulation for multi-label classification, where a hypergraph is constructed to exploit the correlation information among different labels. We show that the proposed formulation leads to an eigenvalue problem, which may be computationally expensive especially for large-scale problems. To reduce the computational cost, we propose an approximate formulation, which is shown to be equivalent to a least squares problem under a mild condition. Based on the approximate formulation, efficient algorithms for solving least squares problems can be applied to scale the formulation to very large data sets. In addition, existing regularization techniques for least squares can be incorporated into the model for improved generalization performance. We have conducted experiments using large-scale benchmark data sets, and experimental results show that the proposed hypergraph spectral learning formulation is effective in capturing the high-order relations in multi-label problems. Results also indicate that the approximate formulation is much more efficient than the original one, while keeping competitive classification performance.

#index 1083699
#* Community evolution in dynamic multi-mode networks
#@ Lei Tang;Huan Liu;Jianping Zhang;Zohreh Nazeri
#t 2008
#c 0
#% 342621
#% 342659
#% 643009
#% 722902
#% 722904
#% 823328
#% 823342
#% 823396
#% 840965
#% 853535
#% 869480
#% 876018
#% 881460
#% 881468
#% 881514
#% 989586
#% 989618
#% 989643
#% 989663
#% 1014679
#% 1055740
#% 1117695
#% 1272187
#! A multi-mode network typically consists of multiple heterogeneous social actors among which various types of interactions could occur. Identifying communities in a multi-mode network can help understand the structural properties of the network, address the data shortage and unbalanced problems, and assist tasks like targeted marketing and finding influential actors within or between groups. In general, a network and the membership of groups often evolve gradually. In a dynamic multi-mode network, both actor membership and interactions can evolve, which poses a challenging problem of identifying community evolution. In this work, we try to address this issue by employing the temporal information to analyze a multi-mode network. A spectral framework and its scalability issue are carefully studied. Experiments on both synthetic data and real-world large scale networks demonstrate the efficacy of our algorithm and suggest its generality in solving problems with complex relationships.

#index 1083700
#* Colibri: fast mining of large static and dynamic graphs
#@ Hanghang Tong;Spiros Papadimitriou;Jimeng Sun;Philip S. Yu;Christos Faloutsos
#t 2008
#c 0
#% 80854
#% 248798
#% 249110
#% 283833
#% 309749
#% 438553
#% 594029
#% 729923
#% 729966
#% 769952
#% 823342
#% 824709
#% 824710
#% 844334
#% 870224
#% 870226
#% 881460
#% 881493
#% 938793
#% 989586
#% 989640
#% 1047785
#! Low-rank approximations of the adjacency matrix of a graph are essential in finding patterns (such as communities) and detecting anomalies. Additionally, it is desirable to track the low-rank structure as the graph evolves over time, efficiently and within limited storage. Real graphs typically have thousands or millions of nodes, but are usually very sparse. However, standard decompositions such as SVD do not preserve sparsity. This has led to the development of methods such as CUR and CMD, which seek a non-orthogonal basis by sampling the columns and/or rows of the sparse matrix. However, these approaches will typically produce overcomplete bases, which wastes both space and time. In this paper we propose the family of Colibri methods to deal with these challenges. Our version for static graphs, Colibri-S, iteratively finds a non-redundant basis and we prove that it has no loss of accuracy compared to the best competitors (CUR and CMD), while achieving significant savings in space and time: on real data, Colibri-S requires much less space and is orders of magnitude faster (in proportion to the square of the number of non-redundant columns). Additionally, we propose an efficient update algorithm for dynamic, time-evolving graphs, Colibri-D. Our evaluation on a large, real network traffic dataset shows that Colibri-D is over 100 times faster than the best published competitor (CMD).

#index 1083701
#* Can complex network metrics predict the behavior of NBA teams?
#@ Pedro O.S. Vaz de Melo;Virgilio A.F. Almeida;Antonio A.F. Loureiro
#t 2008
#c 0
#% 283833
#% 881460
#% 956578
#! The United States National Basketball Association (NBA) is one of the most popular sports league in the world and is well known for moving a millionary betting market that uses the countless statistical data generated after each game to feed the wagers. This leads to the existence of a rich historical database that motivates us to discover implicit knowledge in it. In this paper, we use complex network statistics to analyze the NBA database in order to create models to represent the behavior of teams in the NBA. Results of complex network-based models are compared with box score statistics, such as points, rebounds and assists per game. We show the box score statistics play a significant role for only a small fraction of the players in the league. We then propose new models for predicting a team success based on complex network metrics, such as clustering coefficient and node degree. Complex network-based models present good results when compared to box score statistics, which underscore the importance of capturing network relationships in a community such as the NBA.

#index 1083702
#* Model-based document clustering with a collapsed gibbs sampler
#@ Daniel David Walker;Eric K. Ringger
#t 2008
#c 0
#% 329531
#% 465895
#% 629666
#% 722904
#% 915330
#% 949485
#! Model-based algorithms are emerging as a preferred method for document clustering. As computing resources improve, methods such as Gibbs sampling have become more common for parameter estimation in these models. Gibbs sampling is well understood for many applications, but has not been extensively studied for use in document clustering. We explore the convergence rate, the possibility of label switching, and chain summarization methodologies for document clustering on a particular model, namely a mixture of multinomials model, and show that fairly simple methods can be employed, while still producing clusterings of superior quality compared to those produced with the EM algorithm.

#index 1083703
#* Building semantic kernels for text classification using wikipedia
#@ Pu Wang;Carlotta Domeniconi
#t 2008
#c 0
#% 169777
#% 228088
#% 309208
#% 458379
#% 465754
#% 577355
#% 633682
#% 743284
#% 854646
#% 878916
#% 961685
#% 1117027
#% 1250362
#% 1289518
#! Document classification presents difficult challenges due to the sparsity and the high dimensionality of text data, and to the complex semantics of the natural language. The traditional document representation is a word-based vector (Bag of Words, or BOW), where each dimension is associated with a term of the dictionary containing all the words that appear in the corpus. Although simple and commonly used, this representation has several limitations. It is essential to embed semantic information and conceptual patterns in order to enhance the prediction capabilities of classification algorithms. In this paper, we overcome the shortages of the BOW approach by embedding background knowledge derived from Wikipedia into a semantic kernel, which is then used to enrich the representation of documents. Our empirical evaluation with real data sets demonstrates that our approach successfully achieves improved classification accuracy with respect to the BOW technique, and to other recently developed methods.

#index 1083704
#* A unified approach for schema matching, coreference and canonicalization
#@ Michael L. Wick;Khashayar Rohanimanesh;Karl Schultz;Andrew McCallum
#t 2008
#c 0
#% 333990
#% 348187
#% 464434
#% 572314
#% 740995
#% 754068
#% 773291
#% 804877
#% 809460
#% 915340
#% 989591
#% 1271267
#% 1274820
#% 1289565
#! The automatic consolidation of database records from many heterogeneous sources into a single repository requires solving several information integration tasks. Although tasks such as coreference, schema matching, and canonicalization are closely related, they are most commonly studied in isolation. Systems that do tackle multiple integration problems traditionally solve each independently, allowing errors to propagate from one task to another. In this paper, we describe a discriminatively-trained model that reasons about schema matching, coreference, and canonicalization jointly. We evaluate our model on a real-world data set of people and demonstrate that simultaneously solving these tasks reduces errors over a cascaded or isolated approach. Our experiments show that a joint model is able to improve substantially over systems that either solve each task in isolation or with the conventional cascade. We demonstrate nearly a 50% error reduction for coreference and a 40% error reduction for schema matching.

#index 1083705
#* Information extraction from Wikipedia: moving down the long tail
#@ Fei Wu;Raphael Hoffmann;Daniel S. Weld
#t 2008
#c 0
#% 252011
#% 301241
#% 342398
#% 342630
#% 397160
#% 464434
#% 466078
#% 577318
#% 805872
#% 830520
#% 850430
#% 854668
#% 939601
#% 956564
#% 1019061
#% 1055735
#% 1269587
#% 1275182
#% 1696314
#! Not only is Wikipedia a comprehensive source of quality information, it has several kinds of internal structure (e.g., relational summaries known as infoboxes), which enable self-supervised information extraction. While previous efforts at extraction from Wikipedia achieve high precision and recall on well-populated classes of articles, they fail in a larger number of cases, largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data. This paper presents three novel techniques for increasing recall from Wikipedia's long tail of sparse classes: (1) shrinkage over an automatically-learned subsumption taxonomy, (2) a retraining technique for improving the training data, and (3) supplementing results by extracting from the broader Web. Our experiments compare design variations and show that, used in concert, these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision.

#index 1083706
#* SAIL: summation-based incremental learning for information-theoretic clustering
#@ Junjie Wu;Hui Xiong;Jian Chen
#t 2008
#c 0
#% 169777
#% 252836
#% 722934
#% 729918
#% 755463
#% 826918
#% 875981
#% 878207
#% 916785
#% 1117030
#! Information-theoretic clustering aims to exploit information theoretic measures as the clustering criteria. A common practice on this topic is so-called INFO-K-means, which performs K-means clustering with the KL-divergence as the proximity function. While expert efforts on INFO-K-means have shown promising results, a remaining challenge is to deal with high-dimensional sparse data. Indeed, it is possible that the centroids contain many zero-value features for high-dimensional sparse data. This leads to infinite KL-divergence values, which create a dilemma in assigning objects to the centroids during the iteration process of K-means. To meet this dilemma, in this paper, we propose a Summation-based Incremental Learning (SAIL) method for INFO-K-means clustering. Specifically, by using an equivalent objective function, SAIL replaces the computation of the KL-divergence by the computation of the Shannon entropy. This can avoid the zero-value dilemma caused by the use of the KL-divergence. Our experimental results on various real-world document data sets have shown that, with SAIL as a booster, the clustering performance of K-means can be significantly improved. Also, SAIL leads to quick convergence and a robust clustering performance on high-dimensional sparse data.

#index 1083707
#* Asymmetric support vector machines: low false-positive learning under the user tolerance
#@ Shan-Hung Wu;Keng-Pei Lin;Chung-Min Chen;Ming-Syan Chen
#t 2008
#c 0
#% 197394
#% 269211
#% 309119
#% 393059
#% 420077
#% 722810
#% 803668
#% 811373
#% 855602
#% 879580
#% 936754
#% 961230
#% 987244
#% 1809459
#% 1860547
#! Many practical applications of classification require the classifier to produce a very low false-positive rate. Although the Support Vector Machine (SVM) has been widely applied to these applications due to its superiority in handling high dimensional data, there are relatively little effort other than setting a threshold or changing the costs of slacks to ensure the low false-positive rate. In this paper, we propose the notion of Asymmetric Support VectorMachine (ASVM) that takes into account the false-positives and the user tolerance in its objective. Such a new objective formulation allows us to raise the confidence in predicting the positives, and therefore obtain a lower chance of false-positives. We study the effects of the parameters in ASVM objective and address some implementation issues related to the Sequential Minimal Optimization (SMO) to cope with large-scale data. An extensive simulation is conducted and shows that ASVM is able to yield either noticeable improvement in performance or reduction in training time as compared to the previous arts.

#index 1083708
#* Succinct summarization of transactional databases: an overlapped hyperrectangle scheme
#@ Yang Xiang;Ruoming Jin;David Fuhry;Feodor F. Dragan
#t 2008
#c 0
#% 248792
#% 481290
#% 769876
#% 769905
#% 778215
#% 823343
#% 826265
#% 850526
#% 915259
#% 985037
#% 989648
#% 991656
#% 993995
#% 1016130
#% 1663670
#! Transactional data are ubiquitous. Several methods, including frequent itemsets mining and co-clustering, have been proposed to analyze transactional databases. In this work, we propose a new research problem to succinctly summarize transactional databases. Solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets. We formulate this problem as a set covering problem using overlapped hyperrectangles; we then prove that this problem and its several variations are NP-hard. We develop an approximation algorithm HYPER which can achieve a ln(k) + 1 approximation ratio in polynomial time. We propose a pruning strategy that can significantly speed up the processing of our algorithm. Additionally, we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions. A detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases.

#index 1083709
#* Anonymizing transaction databases for publication
#@ Yabo Xu;Ke Wang;Ada Wai-Chee Fu;Philip S. Yu
#t 2008
#c 0
#% 152934
#% 227919
#% 348155
#% 576762
#% 577233
#% 740764
#% 800514
#% 824726
#% 844342
#% 864412
#% 956509
#% 956511
#% 956552
#% 956557
#! This paper considers the problem of publishing "transaction data" for research purposes. Each transaction is an arbitrary set of items chosen from a large universe. Detailed transaction data provides an electronic image of one's life. This has two implications. One, transaction data are excellent candidates for data mining research. Two, use of transaction data would raise serious concerns over individual privacy. Therefore, before transaction data is released for data mining, it must be made anonymous so that data subjects cannot be re-identified. The challenge is that transaction data has no structure and can be extremely high dimensional. Traditional anonymization methods lose too much information on such data. To date, there has been no satisfactory privacy notion and solution proposed for anonymizing transaction data. This paper proposes one way to address this issue.

#index 1083710
#* Local peculiarity factor and its application in outlier detection
#@ Jian Yang;Ning Zhong;Yiyu Yao;Jue Wang
#t 2008
#c 0
#% 209021
#% 300136
#% 300183
#% 479791
#% 487854
#% 574604
#% 641961
#% 659509
#% 785436
#% 823340
#% 832134
#% 842021
#% 881506
#% 900298
#% 992861
#% 1271973
#! Peculiarity oriented mining (POM), aiming to discover peculiarity rules hidden in a dataset, is a new data mining method. In the past few years, many results and applications on POM have been reported. However, there is still a lack of theoretical analysis. In this paper, we prove that the peculiarity factor (PF), one of the most important concepts in POM, can accurately characterize the peculiarity of data with respect to the probability density function of a normal distribution, but is unsuitable for more general distributions. Thus, we propose the concept of local peculiarity factor (LPF). It is proved that the LPF has the same ability as the PF for a normal distribution and is the so-called µ-sensitive peculiarity description for general distributions. To demonstrate the effectiveness of the LPF, we apply it to outlier detection problems and give a new outlier detection algorithm called LPF-Outlier. Experimental results show that LPF-Outlier is an effective outlier detection algorithm.

#index 1083711
#* A family of dissimilarity measures between nodes generalizing both the shortest-path and the commute-time distances
#@ Luh Yen;Marco Saerens;Amin Mantrach;Masashi Shimbo
#t 2008
#c 0
#% 63833
#% 137711
#% 224113
#% 268079
#% 352664
#% 418262
#% 464615
#% 729936
#% 743284
#% 758132
#% 769952
#% 770839
#% 787098
#% 798044
#% 823388
#% 840965
#% 871315
#% 878207
#% 878224
#% 881480
#% 889151
#% 915225
#% 922860
#% 955712
#% 975021
#% 989646
#% 1001363
#% 1013691
#% 1047785
#% 1275188
#% 1390190
#% 1393247
#! This work introduces a new family of link-based dissimilarity measures between nodes of a weighted directed graph. This measure, called the randomized shortest-path (RSP) dissimilarity, depends on a parameter θ and has the interesting property of reducing, on one end, to the standard shortest-path distance when θ is large and, on the other end, to the commute-time (or resistance) distance when θ is small (near zero). Intuitively, it corresponds to the expected cost incurred by a random walker in order to reach a destination node from a starting node while maintaining a constant entropy (related to θ) spread in the graph. The parameter θ is therefore biasing gradually the simple random walk on the graph towards the shortest-path policy. By adopting a statistical physics approach and computing a sum over all the possible paths (discrete path integral), it is shown that the RSP dissimilarity from every node to a particular node of interest can be computed efficiently by solving two linear systems of n equations, where n is the number of nodes. On the other hand, the dissimilarity between every couple of nodes is obtained by inverting an n x n matrix. The proposed measure can be used for various graph mining tasks such as computing betweenness centrality, finding dense communities, etc, as shown in the experimental section.

#index 1083712
#* Training structural svms with kernels using sampled cuts
#@ Chun-Nam John Yu;Thorsten Joachims
#t 2008
#c 0
#% 269218
#% 722815
#% 829043
#% 840882
#% 876066
#% 881477
#% 961188
#% 983905
#% 987226
#% 1073912
#% 1264133
#% 1387907
#! Discriminative training for structured outputs has found increasing applications in areas such as natural language processing, bioinformatics, information retrieval, and computer vision. Focusing on large-margin methods, the most general (in terms of loss function and model structure) training algorithms known to date are based on cutting-plane approaches. While these algorithms are very efficient for linear models, their training complexity becomes quadratic in the number of examples when kernels are used. To overcome this bottleneck, we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels. We prove that these algorithms have improved time complexity while providing approximation guarantees. In empirical evaluations, our algorithms produced solutions with training and test error rates close to those of exact solvers. Even on binary classification problems where highly optimized conventional training methods exist (e.g. SVM-light), our methods are about an order of magnitude faster than conventional training methods on large datasets, while remaining competitive in speed on datasets of medium size.

#index 1083713
#* Stable feature selection via dense feature groups
#@ Lei Yu;Chris Ding;Steven Loscalzo
#t 2008
#c 0
#% 243728
#% 349208
#% 425048
#% 443894
#% 464444
#% 717417
#% 720010
#% 722935
#% 736899
#% 770819
#% 793239
#% 796212
#% 813902
#% 832876
#% 833552
#% 844358
#% 906489
#% 926881
#% 977991
#% 983819
#% 983927
#% 984113
#! Many feature selection algorithms have been proposed in the past focusing on improving classification accuracy. In this work, we point out the importance of stable feature selection for knowledge discovery from high-dimensional data, and identify two causes of instability of feature selection algorithms: selection of a minimum subset without redundant features and small sample size. We propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results. The framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection. An efficient algorithm DRAGS (Dense Relevant Attribute Group Selector) is developed under this framework. We also introduce a general measure for assessing the stability of feature selection algorithms. Our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out, and the DRAGS algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability.

#index 1083714
#* Categorizing and mining concept drifting data streams
#@ Peng Zhang;Xingquan Zhu;Yong Shi
#t 2008
#c 0
#% 310500
#% 342600
#% 342639
#% 378388
#% 466408
#% 729932
#% 769927
#% 823408
#% 840891
#% 918001
#% 926881
#% 983814
#% 983828
#% 993958
#% 1016144
#% 1116999
#% 1117009
#% 1579122
#! Mining concept drifting data streams is a defining challenge for data mining research. Recent years have seen a large body of work on detecting changes and building prediction models from stream data, with a vague understanding on the types of the concept drifting and the impact of different types of concept drifting on the mining algorithms. In this paper, we first categorize concept drifting into two scenarios: Loose Concept Drifting (LCD) and Rigorous Concept Drifting (RCD), and then propose solutions to handle each of them separately. For LCD data streams, because concepts in adjacent data chunks are sufficiently close to each other, we apply kernel mean matching (KMM) method to minimize the discrepancy of the data chunks in the kernel space. Such a minimization process will produce weighted instances to build classifier ensemble and handle concept drifting data streams. For RCD data streams, because genuine concepts in adjacent data chunks may randomly and rapidly change, we propose a new Optimal Weights Adjustment (OWA) method to determine the optimum weight values for classifiers trained from the most recent (up-to-date) data chunk, such that those classifiers can form an accurate classifier ensemble to predict instances in the yet-to-come data chunk. Experiments on synthetic and real-world datasets will show that weighted instance approach is preferable when the concept drifting is mainly caused by the changing of the class prior probability; whereas the weighted classifier approach is preferable when the concept drifting is mainly triggered by the changing of the conditional probability.

#index 1083715
#* Fastanova: an efficient algorithm for genome-wide association study
#@ Xiang Zhang;Fei Zou;Wei Wang
#t 2008
#c 0
#% 385564
#% 833093
#% 1038915
#! Studying the association between quantitative phenotype (such as height or weight) and single nucleotide polymorphisms (SNPs) is an important problem in biology. To understand underlying mechanisms of complex phenotypes, it is often necessary to consider joint genetic effects across multiple SNPs. ANOVA (analysis of variance) test is routinely used in association study. Important findings from studying gene-gene (SNP-pair) interactions are appearing in the literature. However, the number of SNPs can be up to millions. Evaluating joint effects of SNPs is a challenging task even for SNP-pairs. Moreover, with large number of SNPs correlated, permutation procedure is preferred over simple Bonferroni correction for properly controlling family-wise error rate and retaining mapping power, which dramatically increases the computational cost of association study. In this paper, we study the problem of finding SNP-pairs that have significant associations with a given quantitative phenotype. We propose an efficient algorithm, FastANOVA, for performing ANOVA tests on SNP-pairs in a batch mode, which also supports large permutation test. We derive an upper bound of SNP-pair ANOVA test, which can be expressed as the sum of two terms. The first term is based on single-SNP ANOVA test. The second term is based on the SNPs and independent of any phenotype permutation. Furthermore, SNP-pairs can be organized into groups, each of which shares a common upper bound. This allows for maximum reuse of intermediate computation, efficient upper bound estimation, and effective SNP-pair pruning. Consequently, FastANOVA only needs to perform the ANOVA test on a small number of candidate SNP-pairs without the risk of missing any significant ones. Extensive experiments demonstrate that FastANOVA is orders of magnitude faster than the brute-force implementation of ANOVA tests on all SNP pairs.

#index 1083716
#* Cuts3vm: a fast semi-supervised svm algorithm
#@ Bin Zhao;Fei Wang;Changshui Zhang
#t 2008
#c 0
#% 269226
#% 304876
#% 420495
#% 466263
#% 757953
#% 763708
#% 875968
#% 875969
#% 876050
#% 881477
#% 961195
#% 1013680
#% 1269502
#% 1455666
#! Semi-supervised support vector machine (S3VM) attempts to learn a decision boundary that traverses through low data density regions by maximizing the margin over labeled and unlabeled examples. Traditionally, S3VM is formulated as a non-convex integer programming problem and is thus difficult to solve. In this paper, we propose the cutting plane semi-supervised support vector machine (CutS3VM) algorithm, to solve the S3VM problem. Specifically, we construct a nested sequence of successively tighter relaxations of the original S3VM problem, and each optimization problem in this sequence could be efficiently solved using the constrained concave-convex procedure (CCCP). Moreover, we prove theoretically that the CutS3VM algorithm takes time O(sn) to converge with guaranteed accuracy, where n is the total number of samples in the dataset and s is the average number of non-zero features, i.e. the sparsity. Experimental evaluations on several real world datasets show that CutS3VM performs better than existing S3VM methods, both in efficiency and accuracy.

#index 1083717
#* Identifying biologically relevant genes via multiple heterogeneous data sources
#@ Zheng Zhao;Jiangxin Wang;Huan Liu;Jieping Ye;Yung Chang
#t 2008
#c 0
#% 593047
#% 720010
#% 763697
#% 829010
#% 832876
#% 855573
#% 906494
#% 928386
#% 983941
#% 983948
#% 983949
#% 1038949
#! Selection of genes that are differentially expressed and critical to a particular biological process has been a major challenge in post-array analysis. Recent development in bioinformatics has made various data sources available such as mRNA and miRNA expression profiles, biological pathway and gene annotation, etc. Efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance. In this work, we studied a novel problem of multi-source gene selection: given multiple heterogeneous data sources (or data sets), select genes from expression profiles by integrating information from various data sources. We investigated how to effectively employ information contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection. We designed and conducted experiments to systematically compare the proposed approach with representative methods in terms of statistical and biological significance, and showed the efficacy and potential of the proposed approach with promising findings.

#index 1083718
#* Volatile correlation computation: a checkpoint view
#@ Wenjun Zhou;Hui Xiong
#t 2008
#c 0
#% 152934
#% 227919
#% 285555
#% 342597
#% 466669
#% 729971
#% 765455
#% 769909
#% 863387
#! Recent years have witnessed increased interest in computing strongly correlated pairs in very large databases. Most previous studies have been focused on static data sets. However, in real-world applications, input data are often dynamic and must continually be updated. With such large and growing data sets, new research efforts are expected to develop an incremental solution for correlation computing. Along this line, in this paper, we propose a CHECK-POINT algorithm that can efficiently incorporate new transactions for correlation computing as they become available. Specifically, we set a checkpoint to establish a computation buffer, which can help us determine an upper bound for the correlation. This checkpoint bound can be exploited to identify a list of candidate pairs, which will be maintained and computed for correlations as new transactions are added into the database. However, if the total number of new transactions is beyond the buffer size, a new upper bound is computed by the new checkpoint and a new list of candidate pairs is identified. Experimental results on real-world data sets show that CHECK-POINT can significantly reduce the correlation computing cost in dynamic data sets and has the advantage of compacting the use of memory space.

#index 1083719
#* Land cover change detection: a case study
#@ Shyam Boriah;Vipin Kumar;Michael Steinbach;Christopher Potter;Steven Klooster
#t 2008
#c 0
#% 135968
#% 280408
#% 577295
#% 729954
#% 844293
#% 1665188
#! The study of land cover change is an important problem in the Earth Science domain because of its impacts on local climate, radiation balance, biogeochemistry, hydrology, and the diversity and abundance of terrestrial species. Most well-known change detection techniques from statistics, signal processing and control theory are not well-suited for the massive high-dimensional spatio-temporal data sets from Earth Science due to limitations such as high computational complexity and the inability to take advantage of seasonality and spatio-temporal autocorrelation inherent in Earth Science data. In our work, we seek to address these challenges with new change detection techniques that are based on data mining approaches. Specifically, in this paper we have performed a case study for a new change detection technique for the land cover change detection problem. We study land cover change in the state of California, focusing on the San Francisco Bay Area and perform an extended study on the entire state. We also perform a comparative evaluation on forests in the entire state. These results demonstrate the utility of data mining techniques for the land cover change detection problem.

#index 1083720
#* Identifying authoritative actors in question-answering forums: the case of Yahoo! answers
#@ Mohamed Bouguessa;Benoît Dumoulin;Shengrui Wang
#t 2008
#c 0
#% 262061
#% 290830
#% 345829
#% 374537
#% 662755
#% 730082
#% 799636
#% 841615
#% 868094
#% 891162
#% 956516
#% 1035587
#! We consider the problem of identifying authoritative users in Yahoo! Answers. A common approach is to use link analysis techniques in order to provide a ranked list of users based on their degree of authority. A major problem for such an approach is determining how many users should be chosen as authoritative from a ranked list. To address this problem, we propose a method for automatic identification of authoritative actors. In our approach, we propose to model the authority scores of users as a mixture of gamma distributions. The number of components in the mixture is estimated by the Bayesian Information Criterion (BIC) while the parameters of each component are estimated using the Expectation-Maximization (EM) algorithm. This method allows us to automatically discriminate between authoritative and non-authoritative users. The suitability of our proposal is demonstrated in an empirical study using datasets from Yahoo! Answers.

#index 1083721
#* Context-aware query suggestion by mining click-through and session data
#@ Huanhuan Cao;Daxin Jiang;Jian Pei;Qi He;Zhen Liao;Enhong Chen;Hang Li
#t 2008
#c 0
#% 210173
#% 232719
#% 310567
#% 330617
#% 348155
#% 459006
#% 464996
#% 479962
#% 591792
#% 754125
#% 766440
#% 783475
#% 838531
#% 869500
#% 869501
#% 963669
#% 987193
#% 987212
#% 989578
#% 1712595
#! Query suggestion plays an important role in improving the usability of search engines. Although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context-aware - they do not take into account the immediately preceding queries as context in query suggestion. In this paper, we propose a novel context-aware query suggestion approach which is in two steps. In the offine model-learning step, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. Then, from session data a concept sequence suffix tree is constructed as the query suggestion model. In the online query suggestion step, a user's search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence sufix tree, our approach suggests queries to the user in a context-aware manner. We test our approach on a large-scale search log of a commercial search engine containing 1:8 billion search queries, 2:6 billion clicks, and 840 million query sessions. The experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions.

#index 1083722
#* The persuasive phase of visualization
#@ Christine H. Chih;Douglass S. Parker
#t 2008
#c 0
#% 18610
#% 28144
#% 33996
#% 68659
#% 85700
#% 95829
#% 172757
#% 214028
#% 274621
#% 284136
#% 304319
#% 361501
#% 379321
#% 420082
#% 443092
#% 641092
#% 641146
#% 661269
#% 750156
#% 834997
#% 837671
#% 917038
#! Research in visualization often revolves around visualizing information. However, visualization is a process that extends over time from initial exploration to hypothesis confirmation, and even to result presentation. It is rare that the final phases of visualization are solely about information. In this paper we present a more biased kind of visualization, in which there is a message or set of assumptions behind the presentation that is of interest to both the presenter and the viewer, and emphasizes points that the presenter wants to convey to the viewer. This kind of persuasive visualization -- presenting data in a way that emphasizes a point or message -- is not only common in visualization, but also often expected by the viewer. Persuasive visualization is implicit in the deliberate emphasis on interestingness and also in the deliberate use of graphical elements that are processed preattentively by the human visual system, which automatically groups these elements and guiding attention so that they "stand out". We discuss how these ideas have been implemented in the Morpherspective system for automated generation of information graphics.

#index 1083723
#* Detecting privacy leaks using corpus-based association rules
#@ Richard Chow;Philippe Golle;Jessica Staddon
#t 2008
#c 0
#% 232136
#% 240197
#% 279755
#% 306608
#% 320839
#% 481290
#% 575966
#% 578394
#% 794511
#% 805861
#% 853143
#% 939958
#% 1051901
#! Detecting inferences in documents is critical for ensuring privacy when sharing information. In this paper, we propose a refined and practical model of inference detection using a reference corpus. Our model is inspired by association rule mining: inferences are based on word co-occurrences. Using the model and taking the Web as the reference corpus, we can find inferences and measure their strength through web-mining algorithms that leverage search engines such as Google or Yahoo!. Our model also includes the important case of private corpora, to model inference detection in enterprise settings in which there is a large private document repository. We find inferences in private corpora by using analogues of our Web-mining algorithms, relying on an index for the corpus rather than a Web search engine. We present results from two experiments. The first experiment demonstrates the performance of our techniques in identifying all the keywords that allow for inference of a particular topic (e.g. "HIV") with confidence above a certain threshold. The second experiment uses the public Enron e-mail dataset. We postulate a sensitive topic and use the Enron corpus and the Web together to find inferences for the topic. These experiments demonstrate that our techniques are practical, and that our model of inference based on word co-occurrence is well-suited to efficient inference detection.

#index 1083724
#* Learning methods for lung tumor markerless gating in image-guided radiotherapy
#@ Ying Cui;Jennifer G. Dy;Gregory C. Sharp;Brian M. Alexander;Steve B. Jiang
#t 2008
#c 0
#% 188103
#% 190581
#% 209021
#% 269207
#% 361100
#! In an idealized gated radiotherapy treatment, radiation is delivered only when the tumor is at the right position. For gated lung cancer radiotherapy, it is difficult to generate accurate gating signals due to the large uncertainties when using external surrogates and the risk of pneumothorax when using implanted fiducial markers. In this paper, we investigate machine learning algorithms for markerless gated radiotherapy with fluoroscopic images. Previous approach utilizes template matching to localize the tumor position. Here, we investigate two ways to improve the precision of tumor target localization by applying: (1) an ensemble of templates where the representative templates are selected by Gaussian mixture clustering, and (2) a support vector machine (SVM) classifier with radial basis kernels. Template matching only considers images inside the gating window, but images outside the gating window might provide additional information. We take advantage of both states and re-cast the gating problem into a classification problem. Thus, we are able to use the SVM classifier for gated radiotherapy. To verify the effectiveness of the two proposed techniques, we apply them on five sequences of fluoroscopic images from five lung cancer patients against the gating signal of manually contoured tumors as ground truth. Our five-patient case study shows that both ensemble template matching and SVM are reasonable tools for image-guided markerless gated radiotherapy with an average of approximately 95% precision in terms of delivered target dose at approximately 35% duty cycle.

#index 1083725
#* Text classification, business intelligence, and interactivity: automating C-Sat analysis for services industry
#@ Shantanu Godbole;Shourya Roy
#t 2008
#c 0
#% 311027
#% 311034
#% 375017
#% 458379
#% 571073
#% 763708
#% 799753
#% 939593
#% 1117092
#% 1289485
#! Text classification has matured as a research discipline over the last decade. Independently, business intelligence over structured databases has long been a source of insights for enterprises. In this work, we bring the two together for Customer Satisfaction(C-Sat) analysis in the services industry. We present ITACS, a solution combining text classification and business intelligence integrated with a novel interactive text labeling interface. ITACS has been deployed in multiple client accounts in contact centers. It can be extended to any services industry setting to analyze unstructured text data and derive operational and business insights. We highlight importance of interactivity in real-life text classification settings. We bring out some unique research challenges about label-sets, measuring accuracy, and interpretability that need serious attention in both academic and industrial research. We recount invaluable experiences and lessons learned as data mining researchers working toward seeing research technology deployed in the services industry.

#index 1083726
#* Data mining using high performance data clouds: experimental studies using sector and sphere
#@ Robert Grossman;Yunhong Gu
#t 2008
#c 0
#% 295986
#% 340175
#% 723279
#% 760777
#% 914503
#% 946562
#% 963669
#% 1169639
#! We describe the design and implementation of a high performance cloud that we have used to archive, analyze and mine large distributed data sets. By a cloud, we mean an infrastructure that provides resources and/or services over the Internet. A storage cloud provides storage services, while a compute cloud provides compute services. We describe the design of the Sector storage cloud and how it provides the storage services required by the Sphere compute cloud. We also describe the programming paradigm supported by the Sphere compute cloud. Sector and Sphere are designed for analyzing large data sets using computer clusters connected with wide area high performance networks (for example, 10+ Gb/s). We describe a distributed data mining application that we have developed using Sector and Sphere. Finally, we describe some experimental studies comparing Sector/Sphere to Hadoop.

#index 1083727
#* Automated cyclone discovery and tracking using knowledge sharing in multiple heterogeneous satellite data
#@ Shen-Shyang Ho;Ashit Talukder
#t 2008
#c 0
#% 190581
#% 1455666
#% 1781053
#! Current techniques for cyclone detection and tracking employ NCEP (National Centers for Environmental Prediction) models from in-situ measurements. This solution does not provide true global coverage, unlike remote satellite observations. However it is impractical to use a single Earth orbiting satellite to detect and track events such as cyclones in a continuous manner due to limited spatial and temporal coverage. One solution to alleviate such persistent problems is to utilize heterogeneous sensor data from multiple orbiting satellites. However, this solution requires overcoming other new challenges such as varying spatial and temporal resolution between satellite sensor data, the need to establish correspondence between features from different satellite sensors, and the lack of definitive indicators for cyclone events in some sensor data. We describe an automated cyclone discovery and tracking approach using heterogeneous near real-time sensor data from multiple satellites. This approach addresses the unique challenges associated with knowledge discovery and mining from heterogeneous satellite data streams. We consider two remote sensor measurements in our current implementation, namely: QuikSCAT wind satellite measurements, and merged precipitation data from TRMM and other satellites. More satellites will be incorporated in the near future and our solution is sufficiently powerful that it generalizes to multiple sensor measurement modalities. Our approach consists of three main components: (i) feature extraction from each sensor measurement, (ii) an ensemble classifier for cyclone discovery, and (iii) knowledge sharing between the different remote sensor measurements based on a linear Kalman filter for predictive cyclone tracking. Experimental results on historical hurricane datasets demonstrate the superior performance of our approach compared to previous work.

#index 1083728
#* Spotting out emerging artists using geo-aware analysis of P2P query strings
#@ Noam Koenigstein;Yuval Shavitt;Tomer Tankel
#t 2008
#c 0
#% 216500
#% 342596
#% 778100
#% 823371
#% 1343870
#! Record label companies would like to identify potential artists as early as possible in their careers, before other companies approach the artists with competing contracts. The vast number of candidates makes the process of identifying the ones with high success potential time consuming and laborious. This paper demonstrates how datamining of P2P query strings can be used in order to mechanize most of this detection process. Using a unique intercepting system over the Gnutella network, we were able to capture an unprecedented amount of geographically identified (geo-aware) queries, allowing us to investigate the diffusion of music related queries in time and space. Our solution is based on the observation that emerging artists, especially rappers, have a discernible stronghold of fans in their hometown area, where they are able to perform and market their music. In a file sharing network, this is reflected as a delta function spatial distribution of content queries. Using this observation, we devised a detection algorithm for emerging artists, that looks for performers with sharp increase in popularity in a small geographic region though still unnoticable nation wide. The algorithm can suggest a short list of artists with breakthrough potential, from which we showed that about 30% translate the potential to national success.

#index 1083729
#* Customer targeting models using actively-selected web content
#@ Prem Melville;Saharon Rosset;Richard D. Lawrence
#t 2008
#c 0
#% 169806
#% 170649
#% 458379
#% 465754
#% 466266
#% 629616
#% 722935
#% 765527
#% 785413
#% 844399
#% 1062937
#% 1272282
#% 1289273
#% 1663660
#! We consider the problem of predicting the likelihood that a company will purchase a new product from a seller. The statistical models we have developed at IBM for this purpose rely on historical transaction data coupled with structured firmographic information like the company revenue, number of employees and so on. In this paper, we extend this methodology to include additional text-based features based on analysis of the content on each company's website. Empirical results demonstrate that incorporating such web content can significantly improve customer targeting. Furthermore, we present methods to actively select only the web content that is likely to improve our models, while reducing the costs of acquisition and processing.

#index 1083730
#* Anticipating annotations and emerging trends in biomedical literature
#@ Fabian Mörchen;Mathäus Dejori;Dmitriy Fradkin;Julien Etienne;Bernd Wachmann;Markus Bundschus
#t 2008
#c 0
#% 248218
#% 279755
#% 309096
#% 329569
#% 387427
#% 577220
#% 577224
#% 722904
#% 769906
#% 771924
#% 823344
#% 824666
#% 881498
#% 881534
#% 1650298
#% 1728235
#! The BioJournalMonitor is a decision support system for the analysis of trends and topics in the biomedical literature. Its main goal is to identify potential diagnostic and therapeutic biomarkers for specific diseases. Several data sources are continuously integrated to provide the user with up-to-date information on current research in this field. State-of-the-art text mining technologies are deployed to provide added value on top of the original content, including named entity detection, relation extraction, classification, clustering, ranking, summarization, and visualization. We present two novel technologies that are related to the analysis of temporal dynamics of text archives and associated ontologies. Currently, the MeSH ontology is used to annotate the scientific articles entering the PubMed database with medical terms. Both the maintenance of the ontology as well as the annotation of new articles is performed largely manually. We describe how probabilistic topic models can be used to annotate recent articles with the most likely MeSH terms. This provides our users with a competitive advantage because, when searching for MeSH terms, articles are found long before they are manually annotated. We further present a study on how to predict the inclusion of new terms in the MeSH ontology. The results suggest that early prediction of emerging trends is possible. The trend ranking functions are deployed in our system to enable interactive searches for the hottest new trends relating to a disease.

#index 1083731
#* Temporal pattern discovery for trends and transient effects: its application to patient records
#@ G. Niklas Norén;Andrew Bate;Johan Hopstadius;Kristina Star;I. Ralph Edwards
#t 2008
#c 0
#% 420063
#% 463903
#% 823417
#% 985041
#% 1002277
#% 1038321
#! We introduce a novel pattern discovery methodology for event history data focusing explicitly on the detailed temporal relationship between pairs of events. At the core is a graphical statistical approach to summarising and visualising event history data, which contrasts the observed to the expected incidence of the event of interest before and after an index event. Thus, pattern discovery is not restricted to a specific time window of interest, but encompasses extended parts of the underlying event histories. In order to effectively screen large collections of event history data for interesting temporal relationships, we introduce a new measure of temporal association. The proposed measure contrasts the observed-to-expected ratio in a time period of interest to that in a pre-defined control period. An important feature of both the observed-to-expected graph itself and the measure of association, is a statistical shrinkage towards the null hypothesis of no association. This provides protection against spurious associations and is an extension of the statistical shrinkage successfully applied to large-scale screening for associations between events in cross-sectional data, such as large collections of adverse drug reaction reports. We demonstrate the usefulness of the proposed pattern discovery methodology by a set of examples from a collection of over two million patient records in the United Kingdom. The identified patterns include temporal relationships between drug prescription and medical events suggestive of persistent or transient risks of adverse events, as well as temporal relationships between prescriptions of different drugs.

#index 1083732
#* Scalable and near real-time burst detection from eCommerce queries
#@ Nish Parikh;Neel Sundaresan
#t 2008
#c 0
#% 135968
#% 169454
#% 575972
#% 577220
#% 765412
#% 869482
#% 989650
#% 989676
#% 1275061
#! In large scale online systems like Search, eCommerce, or social network applications, user queries represent an important dimension of activities that can be used to study the impact on the system, and even the business. In this paper, we describe how to detect, characterize and classify bursts in user queries in a large scale eCommerce system. We build upon the approaches discussed in KDD 2002 "Bursty and Hierarchical Structure in Streams" [3] and apply them to a high volume industrial context. We describe how to identify bursts on a near real-time basis, classify them, and apply them to build interesting merchandizing applications.

#index 1083733
#* Identifying domain expertise of developers from source code
#@ Renuka Sindhgatta
#t 2008
#c 0
#% 36672
#% 67565
#% 319704
#% 411117
#% 413610
#% 637359
#% 661266
#% 755463
#% 943029
#! We are interested in identifying the domain expertise of developers of a software system. A developer gains expertise on the code base as well as the domain of the software system he/she develops. This information forms a useful input in allocating software implementation tasks to developers. Domain concepts represented by the system are discovered by taking into account the linguistic information available in the source code. The vocabulary contained in source code as identifiers such as class, method, variable names and comments are extracted. Concepts present in the code base are identified and grouped based on a well known text processing hypothesis - words are similar to the extent to which they share similar words. The developer's association with the source code and the concepts it represents is arrived at using the version repository information. In this line, the analysis first derives documents from source code by discarding all the programming language constructs. KMeans clustering is further used to cluster documents and extract closely related concepts. The key concepts present in the documents authored by the developer determine his/her domain expertise. To validate our approach we apply it on large software systems, two of which are presented in detail in this paper.

#index 1083734
#* ArnetMiner: extraction and mining of academic social networks
#@ Jie Tang;Jing Zhang;Limin Yao;Juanzi Li;Li Zhang;Zhong Su
#t 2008
#c 0
#% 197394
#% 220708
#% 280819
#% 387427
#% 464434
#% 643007
#% 722904
#% 760866
#% 766409
#% 769881
#% 769906
#% 788094
#% 805885
#% 809459
#% 817555
#% 874510
#% 879570
#% 879587
#% 939393
#% 956501
#% 989621
#% 1117023
#% 1250184
#! This paper addresses several key issues in the ArnetMiner system, which aims at extracting and mining academic social networks. Specifically, the system focuses on: 1) Extracting researcher profiles automatically from the Web; 2) Integrating the publication data into the network from existing digital libraries; 3) Modeling the entire academic network; and 4) Providing search services for the academic network. So far, 448,470 researcher profiles have been extracted using a unified tagging approach. We integrate publications from online Web databases and propose a probabilistic framework to deal with the name ambiguity problem. Furthermore, we propose a unified modeling approach to simultaneously model topical aspects of papers, authors, and publication venues. Search services such as expertise search and people association search have been provided based on the modeling results. In this paper, we describe the architecture and main features of the system. We also present the empirical evaluation of the proposed methods.

#index 1083735
#* Tagmark: reliable estimations of RFID tags for business processes
#@ Leonardo Weiss Ferreira Chaves;Erik Buchmann;Klemens Böhm
#t 2008
#c 0
#% 102316
#% 299989
#% 514868
#% 625677
#% 644230
#% 654487
#% 765425
#% 864394
#% 873104
#% 890628
#% 893102
#% 893167
#% 1016201
#% 1206747
#% 1719095
#! Radio Frequency Identification (RFID) promises optimization of commodity flows in all industry segments. But due to physical constraints, RFID technology cannot detect all RFID tags from an assembly of items. This poses problems when integrating RFID data with enterprise-backend systems for tasks like inventory management or shelf replenishment. In this paper we propose the TagMark method to accomplish this integration. TagMark targets at a retailer scenario, where it estimates the number of tagged items from samples like the sales history or the tags read by smart shelves. The problem is challenging because most existing estimation methods depend on assumptions that do not hold in typical RFID applications, e.g., static item sets, simple random samples, or the availability of samples with user-defined sizes. TagMark adapts mark-recapture-methods in order to provide guarantees for the accuracy of the estimation and bounds for the sample sizes. It can be implemented as a database extension, allowing seamless integration into existing enterprise backend systems. A study with RFID-equipped goods acknowledges that our approach is effective in realistic scenarios, and database experiments with up to 1,000,000 items confirm that it can be efficiently implemented. Finally, we explore a broad range of extreme conditions that might stress TagMark, including a thief who knows the location of unread items.

#index 1083736
#* Experimental comparison of scalable online ad serving
#@ Gang Wu;Brendan Kitts
#t 2008
#c 0
#% 723556
#! Online Ad Servers attempt to find best ads to serve for a given triggering user event. The performance of ads may be measured in several ways. We suggest a formulation in which the ad network tries to maximize revenue subject to relevance constraints. We describe several algorithms for ad selection and review their complexity. We tested these algorithms using Microsoft ad network from October 1 2006 to February 8 2007. Over 3 billion impressions, 8 million combinations of triggers with ads, and a number of algorithms were tested over this period. We discover curious differences between ad-servers aimed at revenue versus clickthrough rate.

#index 1083737
#* A visual-analytic toolkit for dynamic interaction graphs
#@ Xintian Yang;Sitaram Asur;Srinivasan Parthasarathy;Sameep Mehta
#t 2008
#c 0
#% 164236
#% 270633
#% 311808
#% 729923
#% 730014
#% 823342
#% 824693
#% 825418
#% 844513
#% 881460
#% 881514
#% 885038
#% 910808
#% 910864
#% 910865
#% 910872
#% 987257
#% 989663
#% 1063503
#% 1289495
#! In this article we describe a visual-analytic tool for the interrogation of evolving interaction network data such as those found in social, bibliometric, WWW and biological applications. The tool we have developed incorporates common visualization paradigms such as zooming, coarsening and filtering while naturally integrating information extracted by a previously described event-driven framework for characterizing the evolution of such networks. The visual front-end provides features that are specifically useful in the analysis of interaction networks, capturing the dynamic nature of both individual entities as well as interactions among them. The tool provides the user with the option of selecting multiple views, designed to capture different aspects of the evolving graph from the perspective of a node, a community or a subset of nodes of interest. Standard visual templates and cues are used to highlight critical changes that have occurred during the evolution of the network. A key challenge we address in this work is that of scalability - handling large graphs both in terms of the efficiency of the back-end, and in terms of the efficiency of the visual layout and rendering. Two case studies based on bibliometric and Wikipedia data are presented to demonstrate the utility of the toolkit for visual knowledge discovery.

#index 1083738
#* Heterogeneous data fusion for alzheimer's disease study
#@ Jieping Ye;Kewei Chen;Teresa Wu;Jing Li;Zheng Zhao;Rinkal Patel;Min Bae;Ravi Janardan;Huan Liu;Gene Alexander;Eric Reiman
#t 2008
#c 0
#% 126894
#% 169659
#% 316150
#% 385564
#% 415767
#% 425048
#% 457831
#% 722929
#% 722935
#% 743284
#% 763697
#% 770774
#% 771029
#% 796212
#% 815967
#% 832876
#% 832903
#% 840887
#% 846431
#% 906491
#% 946463
#% 983941
#% 983948
#% 989657
#% 1022958
#% 1787018
#! Effective diagnosis of Alzheimer's disease (AD) is of primary importance in biomedical research. Recent studies have demonstrated that neuroimaging parameters are sensitive and consistent measures of AD. In addition, genetic and demographic information have also been successfully used for detecting the onset and progression of AD. The research so far has mainly focused on studying one type of data source only. It is expected that the integration of heterogeneous data (neuroimages, demographic, and genetic measures) will improve the prediction accuracy and enhance knowledge discovery from the data, such as the detection of biomarkers. In this paper, we propose to integrate heterogeneous data for AD prediction based on a kernel method. We further extend the kernel framework for selecting features (biomarkers) from heterogeneous data sources. The proposed method is applied to a collection of MRI data from 59 normal healthy controls and 59 AD patients. The MRI data are pre-processed using tensor factorization. In this study, we treat the complementary voxel-based data and region of interest (ROI) data from MRI as two data sources, and attempt to integrate the complementary information by the proposed method. Experimental results show that the integration of multiple data sources leads to a considerable improvement in the prediction accuracy. Results also show that the proposed algorithm identifies biomarkers that play more significant roles than others in AD diagnosis.

#index 1083739
#* Privacy-preserving cox regression for survival analysis
#@ Shipeng Yu;Glenn Fung;Romer Rosales;Sriram Krishnan;R. Bharat Rao;Cary Dehing-Oberije;Philippe Lambin
#t 2008
#c 0
#% 300184
#% 742048
#% 843878
#% 844360
#% 847675
#% 874166
#% 881492
#% 881524
#% 918507
#! Privacy-preserving data mining (PPDM) is an emergent research area that addresses the incorporation of privacy preserving concerns to data mining techniques. In this paper we propose a privacy-preserving (PP) Cox model for survival analysis, and consider a real clinical setting where the data is horizontally distributed among different institutions. The proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem. Our approach differs from the commonly used random projection approach since it instead finds a projection that is optimal at preserving the properties of the data that are important for the specific problem at hand. Since our proposed approach produces an sparse mapping, it also generates a PP mapping that not only projects the data to a lower dimensional space but it also depends on a smaller subset of the original features (it provides explicit feature selection). Real data from several European healthcare institutions are used to test our model for survival prediction of non-small-cell lung cancer patients. These results are also confirmed using publicly available benchmark datasets. Our experimental results show that we are able to achieve a near-optimal performance without directly sharing the data across different data sources. This model makes it possible to conduct large-scale multi-centric survival analysis without violating privacy-preserving requirements.

#index 1083740
#* Using predictive analysis to improve invoice-to-cash collection
#@ Sai Zeng;Prem Melville;Christian A. Lang;Ioana Boier-Martin;Conrad Murphy
#t 2008
#c 0
#% 73372
#% 136350
#% 465922
#% 1650665
#! It is commonly agreed that accounts receivable (AR) can be a source of financial difficulty for firms when they are not efficiently managed and are underperforming. Experience across multiple industries shows that effective management of AR and overall financial performance of firms are positively correlated. In this paper we address the problem of reducing outstanding receivables through improvements in the collections strategy. Specifically, we demonstrate how supervised learning can be used to build models for predicting the payment outcomes of newly-created invoices, thus enabling customized collection actions tailored for each invoice or customer. Our models can predict with high accuracy if an invoice will be paid on time or not and can provide estimates of the magnitude of the delay. We illustrate our techniques in the context of real-world transaction data from multiple firms. Finally, simulation results show that our approach can reduce collection time up to a factor of four compared to a baseline that is not model-driven.

#index 1083741
#* Learning from multi-topic web documents for contextual advertisement
#@ Yi Zhang;Arun C. Surendran;John C. Platt;Mukund Narasimhan
#t 2008
#c 0
#% 105628
#% 136350
#% 190581
#% 224755
#% 272527
#% 464621
#% 465916
#% 565537
#% 729939
#% 766437
#% 835998
#% 854646
#% 938687
#% 1040858
#! Contextual advertising on web pages has become very popular recently and it poses its own set of unique text mining challenges. Often advertisers wish to either target (or avoid) some specific content on web pages which may appear only in a small part of the page. Learning for these targeting tasks is difficult since most training pages are multi-topic and need expensive human labeling at the sub-document level for accurate training. In this paper we investigate ways to learn for sub-document classification when only page level labels are available - these labels only indicate if the relevant content exists in the given page or not. We propose the application of multiple-instance learning to this task to improve the effectiveness of traditional methods. We apply sub-document classification to two different problems in contextual advertising. One is "sensitive content detection" where the advertiser wants to avoid content relating to war, violence, pornography, etc. even if they occur only in a small part of a page. The second problem involves opinion mining from review sites - the advertiser wants to detect and avoid negative opinion about their product when positive, negative and neutral sentiments co-exist on a page. In both these scenarios we present experimental results to show that our proposed system is able to get good block level labeling for free and improve the performance of traditional learning methods.

#index 1083742
#* Social networks: looking ahead
#@ Ravi Kumar;Alexander Tuzhilin;Christos Faloutsos;David Jensen;Gueorgi Kossinets;Jure Leskovec;Andrew Tomkins
#t 2008
#c 0
#! By now, online social networks have become an indispensable part of both online and offline lives of human beings. A large fraction of time spent online by a user is directly influence by the social networks to which he/she belongs. This calls for a deeper examination of social networks as large-scale dynamic objects that foster efficient person-person interaction. The goal of our panel is to discuss social networks from various research angles. In particular, we plan to focus on the following broad research-related topics: large scale data mining, algorithmic questions, sociological aspects, privacy, web search, etc. We will also discuss the business and societal impacts of social networks. Each of these topics has generated a lot of research in recent years and while taking stock of what has been done, we will also be discussing the directions in which these topics are headed, from both science and society points of view. Our panel will consist of eminent researchers, who have worked/been working on an eclectic and diverse mix of problems in social networks

#index 1083743
#* An inductive database prototype based on virtual mining views
#@ Hendrik Blockeel;Toon Calders;Elisa Fromont;Bart Goethals;Adriana Prado;Céline Robardet
#t 2008
#c 0
#% 216508
#% 376266
#% 481290
#% 881575
#% 926881
#% 1403608
#% 1663654
#! We present a prototype of an inductive database. Our system enables the user to query not only the data stored in the database but also generalizations (e.g. rules or trees) over these data through the use of virtual mining views. The mining views are relational tables that virtually contain the complete output of data mining algorithms executed over a given dataset. The prototype implemented into PostgreSQL currently integrates frequent itemset, association rule and decision tree mining. We illustrate the interactive and iterative capabilities of our system with a description of a complete data mining scenario.

#index 1083744
#* Febrl -: an open source data cleaning, deduplication and record linkage system with a graphical user interface
#@ Peter Christen
#t 2008
#c 0
#% 201889
#% 577263
#% 587758
#% 844199
#% 972264
#% 984083
#% 1064741
#% 1070283
#% 1083640
#% 1411073
#! Matching records that refer to the same entity across data-bases is becoming an increasingly important part of many data mining projects, as often data from multiple sources needs to be matched in order to enrich data or improve its quality. Significant advances in record linkage techniques have been made in recent years. However, many new techniques are either implemented in research proof-of-concept systems only, or they are hidden within expensive 'black box' commercial software. This makes it difficult for both researchers and practitioners to experiment with new record linkage techniques, and to compare existing techniques with new ones. The Febrl (Freely Extensible Biomedical Record Linkage) system aims to fill this gap. It contains many recently developed techniques for data cleaning, deduplication and record linkage, and encapsulates them into a graphical user interface (GUI). Febrl thus allows even inexperienced users to learn and experiment with both traditional and new record linkage techniques. Because Febrl is written in Python and its source code is available, it is fairly easy to integrate new record linkage techniques into it. Therefore, Febrl can be seen as a tool that allows researchers to compare various existing record linkage techniques with their own ones, enabling the record linkage research community to conduct their work more efficiently. Additionally, Febrl is suitable as a training tool for new record linkage users, and it can also be used for practical linkage projects with data sets that contain up to several hundred thousand records.

#index 1083745
#* Using tagflake for condensing navigable tag hierarchies from tag clouds
#@ Luigi Di Caro;K. Selçuk Candan;Maria Luisa Sapino
#t 2008
#c 0
#% 769514
#% 775965
#% 789224
#% 870897
#% 881050
#% 907535
#% 1026937
#% 1048456
#! We present the tagFlake system, which supports semantically informed navigation within a tag cloud. tagFlake relies on TMine for organizing tags extracted from textual content in hierarchical organizations, suitable for navigation, visualization, classification, and tracking. TMine extracts the most significant tag/terms from text documents and maps them onto a hierarchy in such a way that descendant terms are contextually dependent on their ancestors within the given corpus of documents. This provides tagFlake with a mechanism for enabling navigation within the tag space and for classification of the text documents based on the contextual structure captured by the created hierarchy. tagFlake is language neutral, since it does not rely on any natural language processing technique and is unsupervised.

#index 1083746
#* An integrated system for automatic customer satisfaction analysis in the services industry
#@ Shantanu Godbole;Shourya Roy
#t 2008
#c 0
#% 763708
#% 1083725
#! Text classification has matured well as a research discipline over the years. At the same time, business intelligence over databases has long been a source of insights for enterprises. With the growing importance of the services industry, customer relationship management and contact center operations have become very important. Specifically, the voice of the customer and customer satisfaction (C-Sat) have emerged as invaluable sources of insights about how an enterprise's products and services are percieved by customers. In this demonstration, we present the IBM Technology to Automate Customer Satisfaction analysis (ITACS) system that combines text classification technology, and a business intelligence solution along with an interactive document labeling interface for automating C-Sat analysis. This system has been successfully deployed in client accounts in large contact centers and can be extended to any services industry setting for analyzing unstructured text data. This demonstration will highlight the importance of intervention and interactivity in real-world text classification settings. We will point out unique research challenges in this domain regarding label-sets, measuring accuracy, and interpretability of results and we will discuss solutions and open questions.

#index 1083747
#* DiMaC: a disguised missing data cleaning tool
#@ Ming Hua;Jian Pei
#t 2008
#c 0
#% 17144
#% 185079
#% 878941
#% 989667
#% 1272290
#! In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely. The very limited previous studies on cleaning disguised missing data highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. Recently, we have studied the problem of cleaning disguised missing data systematically, and proposed an effective heuristic approach [2]. In this paper, we present a demonstration of DiMaC, a Disguised Missing Data Cleaning tool which can find the frequently used disguise values in data sets without any domain background knowledge. In this demo, we will show (1) the critical techniques of finding suspicious disguise values; (2) the architecture and user interface of DiMaC system; (3) an empirical case study on both real and synthetic data sets, which verifies the effectiveness and the efficiency of the techniques; and (4) some challenges arising from real applications and several direction for future work.

#index 1083748
#* Pattern-Miner: integrated management and mining over data mining models
#@ Evangelos E. Kotsifakos;Irene Ntoutsi;Yannis Vrahoritis;Yannis Theodoridis
#t 2008
#c 0
#% 799779
#% 881538
#% 915244
#% 926881
#% 957155
#! This demo presents Pattern-Miner, an integrated environment for pattern management and mining that deals with the whole lifecycle of patterns, from their generation (using data mining techniques) to their storage and querying, putting also emphasis on the comparison between patterns and meta-mining operations over the extracted patterns. Pattern comparison (comparing results of the data mining process) and meta-mining are high level pattern operations that can be applied in a variety of applications, from database change management to image comparison and retrieval.

#index 1083749
#* CRO: a system for online review structurization
#@ Hongyan Liu;Hui Yang;Wenbo Li;Wei Wei;Jun He;Xiaoyong Du
#t 2008
#c 0
#% 755835
#% 769892
#% 786539
#% 805873
#% 855282
#% 878935
#% 939896
#% 987340
#% 1250237
#% 1275196
#! In this paper, we present a system called CRO (Chinese Review Observer) for online product review structurization. By Structurization, we mean identifying, extracting and summarizing information from unstructured review text to a structured table. The core tasks include review collection, product feature and user opinion extraction, and polarity analysis of opinions. Existing research in this area is mainly English text oriented. To deal with Chinese effectively, we propose several novel approaches for fulfilling the core tasks. Then we integrated these approaches and implement the whole procedure of review structurization in the system CRO. Running results for reviews of real products show its performance is satisfactory.

#index 1083750
#* Morpheus: interactive exploration of subspace clustering
#@ Emmanuel Müller;Ira Assent;Ralph Krieger;Timm Jansen;Thomas Seidl
#t 2008
#c 0
#% 248792
#% 273891
#% 434613
#% 785355
#% 915305
#% 1038322
#% 1117035
#! Data mining techniques extract interesting patterns out of large data resources. Meaningful visualization and interactive exploration of patterns are crucial for knowledge discovery. Visualization techniques exist for traditional clustering in low dimensional spaces. In high dimensional data, clusters typically only exist in subspace projections. This subspace clustering, however, lacks interactive visualization tools. Challenges arise from typically large result sets in different subspace projections that hinder comparability, visualization and understandability. In this work, we describe Morpheus, a tool that supports the knowledge discovery process through visualization and interactive exploration of subspace clusterings. Users may browse an overview of the entire subspace clustering, analyze subspace cluster characteristics in-depth and zoom into object groupings. Bracketing of different parameter settings enables users to immediately see the effects of parameters and to provide feedback to further improve the subspace clustering. Furthermore, Morpheus may serve as a teaching and exploration tool for the data mining community to visually assess different subspace clustering paradigms.

#index 1083751
#* A software system for buzz-based recommendations
#@ Hill Nguyen;Nish Parikh;Neel Sundaresan
#t 2008
#c 0
#% 577220
#% 869500
#% 869651
#% 1083732
#! In this paper, we present an outline of a software system for buzz-based recommendations. This system is based on a large source of queries in an eCommerce application. The buzz events are detected based on query bursts linked to external entities like news and inventory information. A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood. The system follows the paradigm of limited quantity merchandizing, in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity and promoting user activity and stickiness. The system demonstrates the deployment of an interesting application based on KDD principles applied to a high volume industrial context.

#index 1083752
#* Pictor: an interactive system for importing data from a website
#@ Shuyi Zheng;Matthew R. Scott;Ruihua Song;Ji-Rong Wen
#t 2008
#c 0
#% 397605
#% 577319
#% 654469
#% 729978
#% 805845
#% 938578
#% 989661
#! We present a demonstration of an interactive wrapper induction system, called Pictor, which is able to minimize labeling cost, yet extract data with high accuracy from a website. Our demonstration will introduce two proposed technologies: record-level wrappers and a wrapper-assisted labeling strategy. These approaches allow Pictor to exploit previously generated wrappers, in order to predict similar labels in a partially labeled webpage or a completely new webpage. Our experiment results show the effectiveness of the Pictor system.

#index 1174987
#* Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008
#@ Alejandro Jaimes;Jia-Yu (Tim) Pan;Maria Luisa Sapino
#t 2008
#c 0

#index 1182042
#* Proceedings of the 2nd International Workshop on Data Mining and Audience Intelligence for Advertising
#@ Ying Li;Arun C. Surendran;Dou Shen
#t 2008
#c 0
#! Global advertising is projected to exceed half-a-trillion dollars by the year 2010. Although online advertising is currently only a small part of this large enterprise, it is growing at a rapid pace. The explosion in the number of participants in the online advertising marketplace has generated large volumes of data and exciting data mining problems. Earlier research on search logs, web pages, social network and blogs had focused on information organization, retrieval and understanding. Recently there has been strong research interest in the advertisement angle to all these information sources. Researchers have tackled several challenging problems on online monetization like sponsored search, contextual advertising for web pages, understanding user intent and user demographics for advertisements, mining user reviews for product pricing, predicting click-through rates for ads, just to name a few. Further, the on-line and offline advertising worlds are fast converging; for example, digital marketplaces are migrating from the online world to TV and radio, and audience understanding work from offline media is trickling into the online realm. Since data mining researchers and practitioners in all these areas come from different communities, there is strong need for a single forum to bring together people involved in all aspects of digital advertising. We are addressing this need with the Workshop on Data Mining and Audience Intelligence for Advertising. The goal of this workshop is to not only increase communication between researchers working on seemingly different pieces of the advertisement pie, but to encourage data mining researchers to bring new ideas from related areas to solve the numerous challenges faced by the rapidly changing digital advertising industry. We want to bring together auction theorists, social network researchers, natural language researchers, information retrieval experts, audience understanding researchers, television advertisement analysts and many others, to promote a fruitful exchange of ideas to advance the field.

#index 1210516
#* Graph Mining and Graph Kernels
#@ Karsten Michael Borgwardt;Xifeng Yan
#t 2008
#c 0

#index 1210517
#* Mining Massive RFID, Trajectory, and Traffic Data Sets
#@ Jiawei Han;Jae-Gil Lee;Hector Gonzalez;Xiaolei Li
#t 2008
#c 0

#index 1210518
#* Blogosphere: Research Issues, Applications, and Tools
#@ Nitin Agarwal;Huan Liu
#t 2008
#c 0

#index 1210519
#* Mining Uncertain and Probabilistic Data: problems, Challenges, Methods, and Applications
#@ Jian Pei;Ming Hua
#t 2008
#c 0

#index 1214616
#* Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ John Elder;Françoise Soulié Fogelman;Peter Flach;Mohammed Zaki
#t 2009
#c 0
#! It is our great pleasure to welcome you to the Fifteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'09). This year is special in that it marks the first time that KDD is held in Europe, and where better to host it than beautiful Paris! As the flagship conference in the field, KDD continues to provide a highly competitive forum for reporting the latest and the best developments in the research and application of data mining and knowledge discovery worldwide. KDD'09 received a record number of 659 submissions, more than 10% up from last year. The Research Track received 537 submissions and the Industrial and Government Applications Track received 122 submissions (an increase of nearly half over last year). For the Research Track, the program committee accepted 105 papers, 50 of which (9.3% of the submitted papers) were chosen for a 25 minute oral presentation and the remaining 55 (10.2%) for a 15 minute presentation. The corresponding numbers for the Industrial Track are 12 (9.8%) and 22 (18.0%). All accepted papers are given up to 9 pages in the proceedings, and all accepted papers are also given poster presentation opportunities in one of the two evening poster sessions during the conference. In addition to the paper presentations, the conference features nine tutorials, eleven workshops, one panel, the hugely successful KDD-Cup competition, a demo session, an exhibition, evening industry sessions (new!), and three invited talks by David Hand (Imperial College London, UK), Heikki Mannila (University of Helsinki, Finland) and Stanley Wasserman (Indiana University, USA). The Industrial Track includes two additional invited presentations by Ravi Kumar (Yahoo! Research, USA) and Ashok Srivastava (NASA Ames, USA).

#index 1214617
#* Mismatched models, wrong results, and dreadful decisions: on choosing appropriate data mining tools
#@ David J. Hand
#t 2009
#c 0
#! Data mining techniques use 'score functions' to quantify how well a model fits a given data set. Parameters are estimated by optimising the fit, as measured by the chosen score function, and model choice is guided by the size of the scores for the different models. Since different score functions summarise the fit in different ways, it is important to choose a function which matches the objectives of the data mining exercise. For predictive classification problems, a wide variety of score functions exist, including measures such as precision and recall, the F measure, misclassification rate, the area under the ROC curve (the AUC), and others. The first four of these require a 'classification threshold' to be chosen, a choice which may not be easy, or may even be impossible, especially when the classification rule is to be applied in the future. In contrast, the AUC does not require the specification of a classification threshold, but summarises performance over the range of possible threshold choices. However, unfortunately, and despite the widespread use of the AUC, it has a previously unrecognised fundamental incoherence lying at the core of its definition. This means that using the AUC can lead to poor model choice and unecessary misclassifications. The AUC is set in context, its deficiency explained and the implications illustrated - with the bottom line being that the AUC should not be used. A family of coherent alternative scores is described. The ideas are illustrated with examples from bank loans, fraud, face recognition, and health screening.

#index 1214618
#* Mining web logs: applications and challenges
#@ Ravi Kumar
#t 2009
#c 0
#! Web logs record the primary interaction of users with web pages in general and search engines in particular. There are two sources for such logs: user trails obtained from toolbars and query/click information obtained from search engines. In this talk we will address the task of mining this rich data to improve user experience on the web. We will illustrate a few applications, together with the modeling and algorithmic challenges that stem from these applications. We will also discuss the privacy issues that arise in this context.

#index 1214619
#* Randomization methods in data mining
#@ Heikki Mannila
#t 2009
#c 0
#! Data mining research has developed many algorithms for various analysis tasks on large and complex datasets. However, assessing the significance of data mining results has received less attention. Analytical methods are rarely available, and hence one has to use computationally intensive methods. Randomization approaches based on null models provide, at least in principle, a general approach that can be used to obtain empirical p-values for various types of data mining approaches. I review some of the recent work in this area, outlining some of the open questions and problems.

#index 1214620
#* Data mining at NASA: from theory to applications
#@ Ashok N. Srivastava
#t 2009
#c 0
#! NASA has some of the largest and most complex data sources in the world, with data sources ranging from the earth sciences, space sciences, and massive distributed engineering data sets from commercial aircraft and spacecraft. This talk will discuss some of the issues and algorithms developed to analyze and discover patterns in these data sets. We will also provide an overview of a large research program in Integrated Vehicle Health Management. The goal of this program is to develop advanced technologies to automatically detect, diagnose, predict, and mitigate adverse events during the flight of an aircraft. A case study will be presented on a recent data mining analysis performed to support the Flight Readiness Review of the Space Shuttle Mission STS-119.

#index 1214621
#* Network science: an introduction to recent statistical approaches
#@ Stanley Wasserman
#t 2009
#c 0
#! Network science focuses on relationships between social entities. It is used widely in the social and behavioral sciences, as well as in political science, economics, organizational science, and industrial engineering. The social network perspective has been developed over the last sixty years by researchers in psychology, sociology, and anthropology, and morerecently, to a lesser extent, in physics. Network science is gaining recognition and standing in the general social and behavioral science communities as the theoretical basis for examining social structures. This basis has been clearly defined by many theorists, and the paradigm convincingly applied to important substantive problems. However, the paradigm requires a new and different set of concepts and analytic tools, beyond those provided by standard quantitative (particularly, statistical) methods. These concepts and tools are the topics of this talk.

#index 1214622
#* Open standards and cloud computing: KDD-2009 panel report
#@ Michael Zeller;Robert Grossman;Christoph Lingenfelder;Michael R. Berthold;Erik Marcade;Rick Pechter;Mike Hoskins;Wayne Thompson;Rich Holada
#t 2009
#c 0
#% 1083726
#% 1087064
#% 1199768
#% 1228061
#! At KDD-2009 in Paris, a panel on open standards and cloud computing addressed emerging trends for data mining applications in science and industry. This report summarizes the answers from a distinguished group of thought leaders representing key software vendors in the data mining industry. Supporting open standards and the Predictive Model Markup Language (PMML) in particular, the panel members discuss topics regarding the adoption of prevailing standards, benefits of interoperability for business users, and the practical application of predictive models. We conclude with an assessment of emerging technology trends and the impact that cloud computing will have on applications as well as licensing models for the predictive analytics industry.

#index 1214623
#* Regression-based latent factor models
#@ Deepak Agarwal;Bee-Chung Chen
#t 2009
#c 0
#% 220709
#% 269195
#% 280819
#% 280852
#% 283169
#% 397155
#% 840924
#% 881537
#% 891549
#% 987198
#% 989573
#% 989580
#% 1014670
#% 1073982
#% 1190057
#! We propose a novel latent factor model to accurately predict response for large scale dyadic data in the presence of features. Our approach is based on a model that predicts response as a multiplicative function of row and column latent factors that are estimated through separate regressions on known row and column features. In fact, our model provides a single unified framework to address both cold and warm start scenarios that are commonplace in practical applications like recommender systems, online advertising, web search, etc. We provide scalable and accurate model fitting methods based on Iterated Conditional Mode and Monte Carlo EM algorithms. We show our model induces a stochastic process on the dyadic space with kernel (covariance) given by a polynomial function of features. Methods that generalize our procedure to estimate factors in an online fashion for dynamic applications are also considered. Our method is illustrated on benchmark datasets and a novel content recommendation application that arises in the context of Yahoo! Front Page. We report significant improvements over several commonly used methods on all datasets.

#index 1214624
#* Frequent pattern mining with uncertain data
#@ Charu C. Aggarwal;Yan Li;Jianyong Wang;Jing Wang
#t 2009
#c 0
#% 248791
#% 300120
#% 329598
#% 466490
#% 481290
#% 832571
#% 866990
#% 1063531
#% 1179162
#! This paper studies the problem of frequent pattern mining with uncertain data. We will show how broad classes of algorithms can be extended to the uncertain data setting. In particular, we will study candidate generate-and-test algorithms, hyper-structure algorithms and pattern growth based algorithms. One of our insightful observations is that the experimental behavior of different classes of algorithms is very different in the uncertain case as compared to the deterministic case. In particular, the hyper-structure and the candidate generate-and-test algorithms perform much better than tree-based algorithms. This counter-intuitive behavior is an important observation from the perspective of algorithm design of the uncertain variation of the problem. We will test the approach on a number of real and synthetic data sets, and show the effectiveness of two of our approaches over competitive techniques.

#index 1214625
#* Structured correspondence topic models for mining captioned figures in biological literature
#@ Amr Ahmed;Eric P. Xing;William W. Cohen;Robert F. Murphy
#t 2009
#c 0
#% 262096
#% 589404
#% 642989
#% 642990
#% 722927
#% 729961
#% 768039
#% 833064
#% 881534
#! A major source of information (often the most crucial and informative part) in scholarly articles from scientific journals, proceedings and books are the figures that directly provide images and other graphical illustrations of key experimental results and other scientific contents. In biological articles, a typical figure often comprises multiple panels, accompanied by either scoped or global captioned text. Moreover, the text in the caption contains important semantic entities such as protein names, gene ontology, tissues labels, etc., relevant to the images in the figure. Due to the avalanche of biological literature in recent years, and increasing popularity of various bio-imaging techniques, automatic retrieval and summarization of biological information from literature figures has emerged as a major unsolved challenge in computational knowledge extraction and management in the life science. We present a new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figures, and we derive an efficient inference algorithm based on collapsed Gibbs sampling for information retrieval and visualization. The resulting program constitutes one of the key IR engines in our SLIF system that has recently entered the final round (4 out 70 competing systems) of the Elsevier Grand Challenge on Knowledge Enhancement in the Life Science. Here we present various evaluations on a number of data mining tasks to illustrate our method.

#index 1214626
#* Name-ethnicity classification from open sources
#@ Anurag Ambekar;Charles Ward;Jahangir Mohammed;Swapna Male;Steven Skiena
#t 2009
#c 0
#% 910797
#% 1026825
#% 1179991
#% 1672943
#% 1739426
#! The problem of ethnicity identification from names has a variety of important applications, including biomedical research, demographic studies, and marketing. Here we report on the development of an ethnicity classifier where all training data is extracted from public, non-confidential (and hence somewhat unreliable) sources. Our classifier uses hidden Markov models (HMMs) and decision trees to classify names into 13 cultural/ethnic groups with individual group accuracy comparable accuracy to earlier binary (e.g., Spanish/non-Spanish) classifiers. We have applied this classifier to over 20 million names from a large-scale news corpus, identifying interesting temporal and spatial trends on the representation of particular cultural/ethnic groups.

#index 1214627
#* Detection of unique temporal segments by information theoretic meta-clustering
#@ Shin Ando;Einoshin Suzuki
#t 2009
#c 0
#% 278011
#% 310512
#% 799393
#% 832892
#% 844310
#% 906424
#% 915222
#% 989577
#% 1015783
#% 1036799
#% 1074370
#% 1081545
#% 1117093
#% 1143811
#% 1176977
#! The central challenge in temporal data analysis is to obtain knowledge about its underlying dynamics. In this paper, we address the observation of noisy, stochastic processes and attempt to detect temporal segments that are related to inconsistencies and irregularities in its dynamics. Many conventional anomaly detection approaches detect anomalies based on the distance between patterns, and often provide only limited intuition about the generative process of the anomalies. Meanwhile, model-based approaches have difficulty in identifying a small, clustered set of anomalies. We propose Information-theoretic Meta-clustering (ITMC), a formalization of model-based clustering principled by the theory of lossy data compression. ITMC identifies a 'unique' cluster whose distribution diverges significantly from the entire dataset. Furthermore, ITMC employs a regularization term derived from the preference for high compression rate, which is critical to the precision of detection. For empirical evaluation, we apply ITMC to two temporal anomaly detection tasks. Datasets are taken from generative processes involving heterogeneous and inconsistent dynamics. A comparison to baseline methods shows that the proposed algorithm detects segments from irregular states with significantly high precision and recall.

#index 1214628
#* Collusion-resistant anonymous data collection method
#@ Mafruz Zaman Ashrafi;See Kiong Ng
#t 2009
#c 0
#% 46786
#% 98834
#% 261357
#% 319353
#% 427807
#% 514517
#% 576111
#% 577233
#% 654448
#% 664654
#% 725295
#% 823358
#% 881463
#% 904965
#! The availability and the accuracy of the data dictate the success of a data mining application. Increasingly, there is a need to resort to on-line data collection to address the problem of data availability. However, participants in on-line data collection applications are naturally distrustful of the data collector as well as their peer respondents, resulting in inaccurate data collected as the respondents refuse to provide truthful data in fear of collusion attacks. The current anonymity-preserving solutions for on-line data collection are unable to adequately resist such attacks in a scalable fashion. In this paper, we present an efficient anonymous data collection protocol for a malicious environment such as the Internet. The protocol employs cryptographic and random shuffling techniques to preserve participants' anonymity. The proposed method is collusion-resistant and guarantees that an attacker will be unable to breach an honest participant's anonymity unless she controls all N-1 participants. In addition, our method is efficient and achieved 15-42% communication overhead reduction in comparison to the prior state-of-the-art methods.

#index 1214629
#* A viewpoint-based approach for interaction graph analysis
#@ Sitaram Asur;Srinivasan Parthasarathy
#t 2009
#c 0
#% 268079
#% 729923
#% 769887
#% 822606
#% 824693
#% 881460
#% 881480
#% 881496
#% 881523
#% 915243
#% 987246
#% 989640
#% 989643
#% 989663
#! Recent innovations have resulted in a plethora of social applications on the Web, such as blogs, social networks, and community photo and video sharing applications. Such applications can typically be represented as evolving interaction graphs with nodes denoting entities and edges representing their interactions. The study of entities and communities and how they evolve in such large dynamic graphs is both important and challenging. While much of the past work in this area has focused on static analysis, more recently researchers have investigated dynamic analysis. In this paper, in a departure from recent efforts, we consider the problem of analyzing patterns and critical events that affect the dynamic graph from the viewpoint of a single node, or a selected subset of nodes. Defining and extracting a relevant viewpoint neighborhood efficiently, while also quantifying the key relationships among nodes involved are the key challenges we address. We also examine the evolution of viewpoint neighborhoods for different entities over time to identify key structural and behavioral transformations that occur.

#index 1214630
#* Optimizing web traffic via the media scheduling problem
#@ Lars Backstrom;Jon Kleinberg;Ravi Kumar
#t 2009
#c 0
#% 330708
#% 750148
#% 956521
#% 1071515
#% 1164955
#% 1189857
#! Website traffic varies through time in consistent and predictable ways, with highest traffic in the middle of the day. When providing media content to visitors, it is important to present repeat visitors with new content so that they keep coming back. In this paper we present an algorithm to balance the need to keep a website fresh with new content with the desire to present the best content to the most visitors at times of peak traffic. We formulate this as the media scheduling problem, where we attempt to maximize total clicks, given the overall traffic pattern and the time varying clickthrough rates of available media content. We present an efficient algorithm to perform this scheduling under certain conditions and apply this algorithm to real data obtained from server logs, showing evidence of significant improvements in traffic from our algorithmic schedules. Finally, we analyze the click data, presenting models for why and how the clickthrough rate for new content declines as it ages.

#index 1214631
#* Improving clustering stability with combinatorial MRFs
#@ Ron Bekkerman;Martin Scholz;Krishnamurthy Viswanathan
#t 2009
#c 0
#% 448779
#% 722902
#% 729918
#% 763861
#% 770836
#% 840840
#% 856734
#% 889294
#% 902497
#% 1117068
#% 1125382
#% 1130917
#% 1674762
#% 1699576
#! As clustering methods are often sensitive to parameter tuning, obtaining stability in clustering results is an important task. In this work, we aim at improving clustering stability by attempting to diminish the influence of algorithmic inconsistencies and enhance the signal that comes from the data. We propose a mechanism that takes m clusterings as input and outputs m clusterings of comparable quality, which are in higher agreement with each other. We call our method the Clustering Agreement Process (CAP). To preserve the clustering quality, CAP uses the same optimization procedure as used in clustering. In particular, we study the stability problem of randomized clustering methods (which usually produce different results at each run). We focus on methods that are based on inference in a combinatorial Markov Random Field (or Comraf, for short) of a simple topology. We instantiate CAP as inference within a more complex, bipartite Comraf. We test the resulting system on four datasets, three of which are medium-sized text collections, while the fourth is a large-scale user/movie dataset. First, in all the four cases, our system significantly improves the clustering stability measured in terms of the macro-averaged Jaccard index. Second, in all the four cases our system managed to significantly improve clustering quality as well, achieving the state-of-the-art results. Third, our system significantly improves stability of consensus clustering built on top of the randomized clustering solutions.

#index 1214632
#* Temporal mining for interactive workflow data analysis
#@ Michele Berlingerio;Fabio Pinelli;Mirco Nanni;Fosca Giannotti
#t 2009
#c 0
#% 245723
#% 346653
#% 449018
#% 459021
#% 549584
#% 572941
#% 733140
#% 749040
#% 844326
#% 874164
#% 881532
#% 889077
#% 946399
#% 953601
#% 1027773
#% 1030830
#% 1063499
#% 1663668
#% 1698979
#! In the past few years there has been an increasing interest in the analysis of process logs. Several proposed techniques, such as workflow mining, are aimed at automatically deriving the underlying workflow models. However, current approaches only pay little attention on an important piece of information contained in process logs: the timestamps, which are used to define a sequential ordering of the performed tasks. In this work we try to overcome these limitations by explicitly including time in the extracted knowledge, thus making the temporal information a first-class citizen of the analysis process. This makes it possible to discern between apparently identical process executions that are performed with different transition times between consecutive tasks. This paper proposes a framework for the user-interactive exploration of a condensed representation of groups of executions of a given process. The framework is based on the use of an existing mining paradigm: Temporally-Annotated Sequences (TAS). These are aimed at extracting sequential patterns where each transition between two events is annotated with a typical transition time that emerges from input data. With the extracted TAS, which represent sets of possible frequent executions with their typical transition times, a few factorizing operators are built. These operators condense such executions according to possible parallel or possible mutual exclusive executions. Lastly, such condensed representation is rendered to the user via the exploration graph, namely the Temporally-Annotated Graph (TAG). The user, the domain expert, is allowed to explore the different and alternative factorizations corresponding to different interpretations of the actual executions. According to the user choices, the system discards or retains certain hypotheses on actual executions and shows the consequent scenarios resulting from the coresponding re-aggregation of the actual data.

#index 1214633
#* Probabilistic frequent itemset mining in uncertain databases
#@ Thomas Bernecker;Hans-Peter Kriegel;Matthias Renz;Florian Verhein;Andreas Zuefle
#t 2009
#c 0
#% 300120
#% 766201
#% 893167
#% 893189
#% 915348
#% 992830
#% 1030778
#% 1063531
#% 1206646
#% 1206717
#% 1393138
#% 1411036
#% 1669490
#! Probabilistic frequent itemset mining in uncertain transaction databases semantically and computationally differs from traditional techniques applied to standard "certain" transaction databases. The consideration of existential uncertainty of item(sets), indicating the probability that an item(set) occurs in a transaction, makes traditional techniques inapplicable. In this paper, we introduce new probabilistic formulations of frequent itemsets based on possible world semantics. In this probabilistic context, an itemset X is called frequent if the probability that X occurs in at least minSup transactions is above a given threshold τ. To the best of our knowledge, this is the first approach addressing this problem under possible worlds semantics. In consideration of the probabilistic formulations, we present a framework which is able to solve the Probabilistic Frequent Itemset Mining (PFIM) problem efficiently. An extensive experimental evaluation investigates the impact of our proposed techniques and shows that our approach is orders of magnitude faster than straight-forward approaches.

#index 1214634
#* The offset tree for learning with partial labels
#@ Alina Beygelzimer;John Langford
#t 2009
#c 0
#% 235377
#% 272518
#% 290482
#% 593734
#% 722906
#% 727925
#% 803574
#% 876056
#% 961172
#% 1073927
#! We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse any existing, fully supervised binary classification algorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classification. In particular, it has regret at most (k-1) times the regret of the binary classifier it uses (where k is the number of choices), and no reduction to binary classification can do better. This reduction is also computationally optimal, both at training and test time, requiring just O(log2 k) work to train on an example or make a prediction. Experiments with the Offset Tree show that it generally performs better than several alternative approaches.

#index 1214635
#* New ensemble methods for evolving data streams
#@ Albert Bifet;Geoff Holmes;Bernhard Pfahringer;Richard Kirkby;Ricard Gavaldà
#t 2009
#c 0
#% 310500
#% 342600
#% 342636
#% 342639
#% 451036
#% 452821
#% 459008
#% 479787
#% 480940
#% 481945
#% 565528
#% 729965
#% 926881
#% 999694
#% 1083714
#% 1406871
#! Advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applications demanding such processing increases. Online mining when such data streams evolve over time, that is when concepts drift or change completely, is becoming one of the core issues. When tackling non-stationary concepts, ensembles of classifiers have several advantages over single classifier methods: they are easy to scale and parallelize, they can adapt to change quickly by pruning under-performing parts of the ensemble, and they therefore usually also generate more accurate concept descriptions. This paper proposes a new experimental data stream framework for studying concept drift, and two new variants of Bagging: ADWIN Bagging and Adaptive-Size Hoeffding Tree (ASHT) Bagging. Using the new experimental framework, an evaluation study on synthetic and real-world datasets comprising up to ten million examples shows that the new ensemble methods perform very well compared to several known methods.

#index 1214636
#* CoCo: coding cost for parameter-free outlier detection
#@ Christian Böhm;Katrin Haegler;Nikola S. Müller;Claudia Plant
#t 2009
#c 0
#% 300136
#% 466425
#% 479791
#% 479986
#% 629669
#% 681822
#% 769896
#% 881462
#% 899716
#% 1063483
#% 1202160
#% 1810265
#% 1855583
#! How can we automatically spot all outstanding observations in a data set? This question arises in a large variety of applications, e.g. in economy, biology and medicine. Existing approaches to outlier detection suffer from one or more of the following drawbacks: The results of many methods strongly depend on suitable parameter settings being very difficult to estimate without background knowledge on the data, e.g. the minimum cluster size or the number of desired outliers. Many methods implicitly assume Gaussian or uniformly distributed data, and/or their result is difficult to interpret. To cope with these problems, we propose CoCo, a technique for parameter-free outlier detection. The basic idea of our technique relates outlier detection to data compression: Outliers are objects which can not be effectively compressed given the data set. To avoid the assumption of a certain data distribution, CoCo relies on a very general data model combining the Exponential Power Distribution with Independent Components. We define an intuitive outlier factor based on the principle of the Minimum Description Length together with an novel algorithm for outlier detection. An extensive experimental evaluation on synthetic and real world data demonstrates the benefits of our technique. Availability: The source code of CoCo and the data sets used in the experiments are available at: http://www.dbs.ifi.lmu.de/Forschung/KDD/Boehm/CoCo.

#index 1214637
#* Efficient anomaly monitoring over moving object trajectory streams
#@ Yingyi Bu;Lei Chen;Ada Wai-Chee Fu;Dawei Liu
#t 2009
#c 0
#% 172949
#% 227924
#% 300136
#% 300183
#% 333941
#% 342625
#% 387427
#% 413606
#% 460862
#% 462231
#% 479791
#% 481279
#% 570886
#% 631923
#% 654456
#% 729912
#% 729931
#% 729980
#% 765412
#% 765451
#% 781774
#% 810049
#% 844310
#% 869517
#% 881476
#% 881495
#% 953324
#% 960283
#% 993961
#% 993965
#% 1016195
#! Lately there exist increasing demands for online abnormality monitoring over trajectory streams, which are obtained from moving object tracking devices. This problem is challenging due to the requirement of high speed data processing within limited space cost. In this paper, we present a novel framework for monitoring anomalies over continuous trajectory streams. First, we illustrate the importance of distance-based anomaly monitoring over moving object trajectories. Then, we utilize the local continuity characteristics of trajectories to build local clusters upon trajectory streams and monitor anomalies via efficient pruning strategies. Finally, we propose a piecewise metric index structure to reschedule the joining order of local clusters to further reduce the time cost. Our extensive experiments demonstrate the effectiveness and efficiency of our methods.

#index 1214638
#* Connections between the lines: augmenting social networks with text
#@ Jonathan Chang;Jordan Boyd-Graber;David M. Blei
#t 2009
#c 0
#% 249110
#% 280819
#% 722904
#% 748024
#% 788094
#% 868088
#% 868092
#% 881534
#% 995512
#% 1001362
#% 1036030
#% 1055681
#% 1055685
#% 1055741
#% 1083624
#% 1083626
#% 1083675
#% 1083684
#% 1269756
#! Network data is ubiquitous, encoding collections of relationships between entities such as people, places, genes, or corporations. While many resources for networks of interesting entities are emerging, most of these can only annotate connections in a limited fashion. Although relationships between entities are rich, it is impractical to manually devise complete characterizations of these relationships for every pair of entities on large, real-world corpora. In this paper we present a novel probabilistic topic model to analyze text corpora and infer descriptions of its entities and of relationships between those entities. We develop variational methods for performing approximate inference on our model and demonstrate that our model can be practically deployed on large corpora such as Wikipedia. We show qualitatively and quantitatively that our model can construct and annotate graphs of relationships and make useful predictions.

#index 1214639
#* Extracting discriminative concepts for domain adaptation in text mining
#@ Bo Chen;Wai Lam;Ivor Tsang;Tak-Lam Wong
#t 2009
#c 0
#% 402289
#% 961218
#% 983899
#% 997093
#% 1083666
#% 1083678
#% 1261539
#% 1270196
#! One common predictive modeling challenge occurs in text mining problems is that the training data and the operational (testing) data are drawn from different underlying distributions. This poses a great difficulty for many statistical learning methods. However, when the distribution in the source domain and the target domain are not identical but related, there may exist a shared concept space to preserve the relation. Consequently a good feature representation can encode this concept space and minimize the distribution gap. To formalize this intuition, we propose a domain adaptation method that parameterizes this concept space by linear transformation under which we explicitly minimize the distribution difference between the source domain with sufficient labeled data and target domains with only unlabeled data, while at the same time minimizing the empirical loss on the labeled data in the source domain. Another characteristic of our method is its capability for considering multiple classes and their interactions simultaneously. We have conducted extensive experiments on two common text mining problems, namely, information extraction and document classification to demonstrate the effectiveness of our proposed method.

#index 1214640
#* Constrained optimization for validation-guided conditional random field learning
#@ Minmin Chen;Yixin Chen;Michael R. Brent;Aaron E. Tenney
#t 2009
#c 0
#% 464434
#% 816181
#% 836906
#% 873954
#% 938659
#% 1270249
#% 1290045
#! Conditional random fields(CRFs) are a class of undirected graphical models which have been widely used for classifying and labeling sequence data. The training of CRFs is typically formulated as an unconstrained optimization problem that maximizes the conditional likelihood. However, maximum likelihood training is prone to overfitting. To address this issue, we propose a novel constrained nonlinear optimization formulation in which the prediction accuracy of cross-validation sets are included as constraints. Instead of requiring multiple passes of training, the constrained formulation allows the cross-validation be handled in one pass of constrained optimization. The new formulation is discontinuous, and classical Lagrangian based constraint handling methods are not applicable. A new constrained optimization algorithm based on the recently proposed extended saddle point theory is developed to learn the constrained CRF model. Experimental results on gene and stock-price prediction tasks show that the constrained formulation is able to significantly improve the generalization ability of CRF training.

#index 1214641
#* Efficient influence maximization in social networks
#@ Wei Chen;Yajun Wang;Siyu Yang
#t 2009
#c 0
#% 68247
#% 243166
#% 342596
#% 577217
#% 729923
#% 989613
#% 1663638
#! Influence maximization is the problem of finding a small subset of nodes (seed nodes) in a social network that could maximize the spread of influence. In this paper, we study the efficient influence maximization from two complementary directions. One is to improve the original greedy algorithm of [5] and its improvement [7] to further reduce its running time, and the second is to propose new degree discount heuristics that improves influence spread. We evaluate our algorithms by experiments on two large academic collaboration graphs obtained from the online archival database arXiv.org. Our experimental results show that (a) our improved greedy algorithm achieves better running time comparing with the improvement of [7] with matching influence spread, (b) our degree discount heuristics achieve much better influence spread than classic degree and centrality-based heuristics, and when tuned for a specific influence cascade model, it achieves almost matching influence thread with the greedy algorithm, and more importantly (c) the degree discount heuristics run only in milliseconds while even the improved greedy algorithms run in hours in our experiment graphs with a few tens of thousands of nodes. Based on our results, we believe that fine-tuned heuristics may provide truly scalable solutions to the influence maximization problem with satisfying influence spread and blazingly fast running time. Therefore, contrary to what implied by the conclusion of [5] that traditional heuristics are outperformed by the greedy approximation algorithm, our results shed new lights on the research of heuristic algorithms.

#index 1214642
#* Large-scale behavioral targeting
#@ Ye Chen;Dmitry Pavlov;John F. Canny
#t 2009
#c 0
#% 317279
#% 755399
#% 766422
#% 1023420
#! Behavioral targeting (BT) leverages historical user behavior to select the ads most relevant to users to display. The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate (CTR) from user history. We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce framework. With our parallel algorithm and the resulting system, we can build above 450 BT-category models from the entire Yahoo's user base within one day, the scale that one can not even imagine with prior systems. Moreover, our approach has yielded 20% CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset. Specifically, our major contributions include: (1) A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism, task parallelism, and load balance in spite of the typically skewed distribution of domain data. (2) An in-place feature vector generation algorithm with linear time complexity O(n) regardless of the granularity of sliding target window. (3) An in-memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical. (4) Highly efficient data structures and sparse representations of models and data to enable fast model updates. We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general. Finally, we report comprehensive experimental results, using industrial proprietary codebase and datasets.

#index 1214643
#* On compressing social networks
#@ Flavio Chierichetti;Ravi Kumar;Silvio Lattanzi;Michael Mitzenmacher;Alessandro Panconesi;Prabhakar Raghavan
#t 2009
#c 0
#% 290703
#% 311808
#% 337580
#% 342385
#% 408396
#% 570319
#% 656242
#% 656274
#% 656281
#% 656282
#% 737340
#% 739861
#% 740507
#% 754117
#% 790068
#% 824711
#% 881523
#% 1029105
#% 1035579
#% 1077150
#% 1392439
#! Motivated by structural properties of the Web graph that support efficient data structures for in memory adjacency queries, we study the extent to which a large network can be compressed. Boldi and Vigna (WWW 2004), showed that Web graphs can be compressed down to three bits of storage per edge; we study the compressibility of social networks where again adjacency queries are a fundamental primitive. To this end, we propose simple combinatorial formulations that encapsulate efficient compressibility of graphs. We show that some of the problems are NP-hard yet admit effective heuristics, some of which can exploit properties of social networks such as link reciprocity. Our extensive experiments show that social networks and the Web graph exhibit vastly different compressibility characteristics.

#index 1214644
#* Regret-based online ranking for a growing digital library
#@ Erick Delage
#t 2009
#c 0
#% 290830
#% 420495
#% 577224
#% 818221
#% 840846
#% 871302
#% 989628
#% 1000325
#% 1073970
#% 1705504
#! The most common environment in which ranking is used takes a very specific form. Users sequentially generate queries in a digital library. For each query, ranking is applied to order a set of relevant items from which the user selects his favorite. This is the case when ranking search results for pages on the World Wide Web or for merchandize on an e-commerce site. In this work, we present a new online ranking algorithm, called NoRegret KLRank. Our algorithm is designed to use "clickthrough" information as it is provided by the users to improve future ranking decisions. More importantly, we show that its long term average performance will converge to the best rate achievable by any competing fixed ranking policy selected with the benefit of hindsight. We show how to ensure that this property continues to hold as new items are added to the set thus requiring a richer class of ranking policies. Finally, our empirical results show that, while in some context NoRegret KLRank might be considered conservative, a greedy variant of this algorithm actually outperforms many popular ranking algorithms.

#index 1214645
#* A generalized Co-HITS algorithm and its application to bipartite graphs
#@ Hongbo Deng;Michael R. Lyu;Irwin King
#t 2009
#c 0
#% 262096
#% 268079
#% 290830
#% 310567
#% 387427
#% 397129
#% 397169
#% 750863
#% 805896
#% 818241
#% 818266
#% 838528
#% 869501
#% 878624
#% 879568
#% 879575
#% 918017
#% 987203
#% 987222
#% 989578
#% 1055675
#% 1055681
#% 1055712
#% 1074093
#% 1130879
#% 1130921
#% 1166526
#! Recently many data types arising from data mining and Web search applications can be modeled as bipartite graphs. Examples include queries and URLs in query logs, and authors and papers in scientific literature. However, one of the issues is that previous algorithms only consider the content and link information from one side of the bipartite graph. There is a lack of constraints to make sure the final relevance of the score propagation on the graph, as there are many noisy edges within the bipartite graph. In this paper, we propose a novel and general Co-HITS algorithm to incorporate the bipartite graph with the content information from both sides as well as the constraints of relevance. Moreover, we investigate the algorithm based on two frameworks, including the iterative and the regularization frameworks, and illustrate the generalized Co-HITS algorithm from different views. For the iterative framework, it contains HITS and personalized PageRank as special cases. In the regularization framework, we successfully build a connection with HITS, and develop a new cost function to consider the direct relationship between two entity sets, which leads to a significant improvement over the baseline method. To illustrate our methodology, we apply the Co-HITS algorithm, with many different settings, to the application of query suggestion by mining the AOL query log data. Experimental results demonstrate that CoRegu-0.5 (i.e., a model of the regularization framework) achieves the best performance with consistent and promising improvements.

#index 1214646
#* Mining for the most certain predictions from dyadic data
#@ Meghana Deodhar;Joydeep Ghosh
#t 2009
#c 0
#% 375986
#% 493256
#% 798967
#% 877820
#% 891559
#% 989573
#% 989596
#% 992620
#% 1211720
#% 1672479
#! In several applications involving regression or classification, along with making predictions it is important to assess how accurate or reliable individual predictions are. This is particularly important in cases where due to finite resources or domain requirements, one wants to make decisions based only on the most reliable rather than on the entire set of predictions. This paper introduces novel and effective ways of ranking predictions by their accuracy for problems involving large-scale, heterogeneous data with a dyadic structure, i.e., where the independent variables can be naturally decomposed into three groups associated with two sets of elements and their combination. These approaches are based on modeling the data by a collection of localized models learnt while simultaneously partitioning (co-clustering) the data. For regression this leads to the concept of "certainty lift". We also develop a robust predictive modeling technique that identifies and models only the most coherent regions of the data to give high predictive accuracy on the selected subset of response values. Extensive experimentation on real life datasets highlights the utility of our proposed approaches.

#index 1214647
#* Efficiently learning the accuracy of labeling sources for selective sampling
#@ Pinar Donmez;Jaime G. Carbonell;Jeff Schneider
#t 2009
#c 0
#% 124467
#% 169717
#% 235377
#% 331916
#% 696947
#% 829985
#% 829994
#% 844399
#% 959215
#% 1083692
#% 1130870
#% 1197791
#% 1264744
#% 1499584
#% 1699589
#! Many scalable data mining tasks rely on active learning to provide the most useful accurately labeled instances. However, what if there are multiple labeling sources ('oracles' or 'experts') with different but unknown reliabilities? With the recent advent of inexpensive and scalable online annotation tools, such as Amazon's Mechanical Turk, the labeling process has become more vulnerable to noise - and without prior knowledge of the accuracy of each individual labeler. This paper addresses exactly such a challenge: how to jointly learn the accuracy of labeling sources and obtain the most informative labels for the active learning task at hand minimizing total labeling effort. More specifically, we present IEThresh (Interval Estimate Threshold) as a strategy to intelligently select the expert(s) with the highest estimated labeling accuracy. IEThresh estimates a confidence interval for the reliability of each expert and filters out the one(s) whose estimated upper-bound confidence interval is below a threshold - which jointly optimizes expected accuracy (mean) and need to better estimate the expert's accuracy (variance). Our framework is flexible enough to work with a wide range of different noise levels and outperforms baselines such as asking all available experts and random expert selection. In particular, IEThresh achieves a given level of accuracy with less than half the queries issued by all-experts labeling and less than a third the queries required by random expert selection on datasets such as the UCI mushroom one. The results show that our method naturally balances exploration and exploitation as it gains knowledge of which experts to rely upon, and selects them with increasing frequency.

#index 1214648
#* Large human communication networks: patterns and a utility-driven generator
#@ Nan Du;Christos Faloutsos;Bai Wang;Leman Akoglu
#t 2009
#c 0
#% 287267
#% 342592
#% 653861
#% 807297
#% 823342
#% 844334
#% 847046
#% 867050
#% 892745
#% 955712
#% 972277
#% 989519
#% 991194
#% 1083675
#% 1083682
#% 1095877
#% 1127498
#% 1202160
#% 1206639
#! Given a real, and weighted person-to-person network which changes over time, what can we say about the cliques that it contains? Do the incidents of communication, or weights on the edges of a clique follow any pattern? Real, and in-person social networks have many more triangles than chance would dictate. As it turns out, there are many more cliques than one would expect, in surprising patterns. In this paper, we study massive real-world social networks formed by direct contacts among people through various personal communication services, such as Phone-Call, SMS, IM etc. The contributions are the following: (a) we discover surprising patterns with the cliques, (b) we report power-laws of the weights on the edges of cliques, (c) our real networks follow these patterns such that we can trust them to spot outliers and finally, (d) we propose the first utility-driven graph generator for weighted time-evolving networks, which match the observed patterns. Our study focused on three large datasets, each of which is a different type of communication service, with over one million records, and spans several months of activity.

#index 1214649
#* Learning with a non-exhaustive training dataset: a case study: detection of bacteria cultures using optical-scattering technology
#@ M. Murat Dundar;E. Daniel Hirleman;Arun K. Bhunia;J. Paul Robinson;Bartek Rajwa
#t 2009
#c 0
#% 80995
#% 190581
#% 302406
#% 644560
#% 855602
#! For a training dataset with a nonexhaustive list of classes, i.e. some classes are not yet known and hence are not represented, the resulting learning problem is ill-defined. In this case a sample from a missing class is incorrectly classified to one of the existing classes. For some applications the cost of misclassifying a sample could be negligible. However, the significance of this problem can better be acknowledged when the potentially undesirable consequences of incorrectly classifying a food pathogen as a nonpathogen are considered. Our research is directed towards the real-time detection of food pathogens using optical-scattering technology. Bacterial colonies consisting of the progeny of a single parent cell scatter light at 635 nm to produce unique forward-scatter signatures. These spectral signatures contain descriptive characteristics of bacterial colonies, which can be used to identify bacteria cultures in real time. One bottleneck that remains to be addressed is the nonexhaustive nature of the training library. It is very difficult if not impractical to collect samples from all possible bacteria colonies and construct a digital library with an exhaustive set of scatter signatures. This study deals with the real-time detection of samples from a missing class and the associated problem of learning with a nonexhaustive training dataset. Our proposed method assumes a common prior for the set of all classes, known and missing. The parameters of the prior are estimated from the samples of the known classes. This prior is then used to generate a large number of samples to simulate the space of missing classes. Finally a Bayesian maximum likelihood classifier is implemented using samples from real as well as simulated classes. Experiments performed with samples collected for 28 bacteria subclasses favor the proposed approach over the state of the art.

#index 1214650
#* Turning down the noise in the blogosphere
#@ Khalid El-Arini;Gaurav Veda;Dafna Shahaf;Carlos Guestrin
#t 2009
#c 0
#% 262112
#% 297675
#% 452563
#% 642975
#% 722904
#% 818266
#% 871302
#% 939376
#% 956521
#% 989613
#% 1008125
#! In recent years, the blogosphere has experienced a substantial increase in the number of posts published daily, forcing users to cope with information overload. The task of guiding users through this flood of information has thus become critical. To address this issue, we present a principled approach for picking a set of posts that best covers the important stories in the blogosphere. We define a simple and elegant notion of coverage and formalize it as a submodular optimization problem, for which we can efficiently compute a near-optimal solution. In addition, since people have varied interests, the ideal coverage algorithm should incorporate user preferences in order to tailor the selected posts to individual tastes. We define the problem of learning a personalized coverage function by providing an appropriate user-interaction model and formalizing an online learning framework for this task. We then provide a no-regret algorithm which can quickly learn a user's preferences from limited feedback. We evaluate our coverage and personalization algorithms extensively over real blog data. Results from a user study show that our simple coverage algorithm does as well as most popular blog aggregation sites, including Google Blog Search, Yahoo! Buzz, and Digg. Furthermore, we demonstrate empirically that our algorithm can successfully adapt to user preferences. We believe that our technique, especially with personalization, can dramatically reduce information overload.

#index 1214651
#* Feature shaping for linear SVM classifiers
#@ George Forman;Martin Scholz;Shyamsundar Rajaram
#t 2009
#c 0
#% 385564
#% 722929
#% 722935
#% 728350
#% 763697
#% 770810
#% 926881
#% 928386
#% 1074002
#% 1130833
#% 1177819
#! Linear classifiers have been shown to be effective for many discrimination tasks. Irrespective of the learning algorithm itself, the final classifier has a weight to multiply by each feature. This suggests that ideally each input feature should be linearly correlated with the target variable (or anti-correlated), whereas raw features may be highly non-linear. In this paper, we attempt to re-shape each input feature so that it is appropriate to use with a linear weight and to scale the different features in proportion to their predictive value. We demonstrate that this pre-processing is beneficial for linear SVM classifiers on a large benchmark of text classification tasks as well as UCI datasets.

#index 1214652
#* A multi-relational approach to spatial classification
#@ Richard Frank;Martin Ester;Arno Knobbe
#t 2009
#c 0
#% 73005
#% 279852
#% 458257
#% 478762
#% 526851
#% 737336
#% 745491
#% 799746
#% 803394
#% 998577
#% 1289267
#! Spatial classification is the task of learning models to predict class labels based on the features of entities as well as the spatial relationships to other entities and their features. Spatial data can be represented as multi-relational data, however it presents novel challenges not present in multi-relational problems. One such problem is that spatial relationships are embedded in space, unknown a priori, and it is part of the algorithm's task to determine which relationships are important and what properties to consider. In order to determine when two entities are spatially related in an adaptive and non-parametric way, we propose a Voronoi-based neighbourhood definition upon which spatial literals can be built. Properties of these neighbourhoods also need to be described and used for classification purposes. Non-spatial aggregation literals already exist within the multi-relational framework, but are not sufficient for comprehensive spatial classification. A formal set of additions to the multi-relational data mining framework is proposed, to be able to represent spatial aggregations as well as spatial features and literals. These additions allow for capturing more complex interactions and spatial occurrences such as spatial trends. In order to more efficiently perform the rule learning and exploit powerful multi-processor machines, a scalable parallelized method capable of reducing the runtime by several factors is presented. The method is compared against existing methods by experimental evaluation on a real world crime dataset which demonstrate the importance of the neighbourhood definition and the advantages of parallelization.

#index 1214653
#* Scalable pseudo-likelihood estimation in hybrid random fields
#@ Antonino Freno;Edmondo Trentin;Marco Gori
#t 2009
#c 0
#% 21142
#% 44876
#% 73441
#% 129987
#% 197387
#% 246831
#% 376266
#% 521046
#% 722754
#% 724593
#% 729990
#% 906384
#% 919561
#% 1275197
#% 1393622
#% 1650569
#% 1650783
#! Learning probabilistic graphical models from high-dimensional datasets is a computationally challenging task. In many interesting applications, the domain dimensionality is such as to prevent state-of-the-art statistical learning techniques from delivering accurate models in reasonable time. This paper presents a hybrid random field model for pseudo-likelihood estimation in high-dimensional domains. A theoretical analysis proves that the class of pseudo-likelihood distributions representable by hybrid random fields strictly includes the class of joint probability distributions representable by Bayesian networks. In order to learn hybrid random fields from data, we develop the Markov Blanket Merging algorithm. Theoretical and experimental evidence shows that Markov Blanket Merging scales up very well to high-dimensional datasets. As compared to other widely used statistical learning techniques, Markov Blanket Merging delivers accurate results in a number of link prediction tasks, while achieving also significant improvements in terms of computational efficiency. Our software implementation of the models investigated in this paper is publicly available at http://www.dii.unisi.it/~freno/. The same website also hosts the datasets used in this work that are not available elsewhere in the same preprocessing used for our experiments.

#index 1214654
#* Issues in evaluation of stream learning algorithms
#@ João Gama;Raquel Sebastião;Pedro Pereira Rodrigues
#t 2009
#c 0
#% 135968
#% 204531
#% 310500
#% 342600
#% 342639
#% 576113
#% 729965
#% 737348
#% 737351
#% 881575
#% 961134
#% 998561
#% 1718518
#! Learning from data streams is a research area of increasing importance. Nowadays, several stream learning algorithms have been developed. Most of them learn decision models that continuously evolve over time, run in resource-aware environments, detect and react to changes in the environment generating data. One important issue, not yet conveniently addressed, is the design of experimental work to evaluate and compare decision models that evolve over time. There are no golden standards for assessing performance in non-stationary environments. This paper proposes a general framework for assessing predictive stream learning algorithms. We defend the use of Predictive Sequential methods for error estimate - the prequential error. The prequential error allows us to monitor the evolution of the performance of models that evolve over time. Nevertheless, it is known to be a pessimistic estimator in comparison to holdout estimates. To obtain more reliable estimators we need some forgetting mechanism. Two viable alternatives are: sliding windows and fading factors. We observe that the prequential error converges to an holdout estimator when estimated over a sliding window or using fading factors. We present illustrative examples of the use of prequential error estimators, using fading factors, for the tasks of: i) assessing performance of a learning algorithm; ii) comparing learning algorithms; iii) hypothesis testing using McNemar test; and iv) change detection using Page-Hinkley test. In these tasks, the prequential error estimated using fading factors provide reliable estimators. In comparison to sliding windows, fading factors are faster and memory-less, a requirement for streaming applications. This paper is a contribution to a discussion in the good-practices on performance assessment when learning dynamic models that evolve over time.

#index 1214655
#* Heterogeneous source consensus learning via decision propagation and negotiation
#@ Jing Gao;Wei Fan;Yizhou Sun;Jiawei Han
#t 2009
#c 0
#% 252011
#% 420495
#% 424997
#% 722902
#% 728025
#% 770836
#% 837616
#% 844317
#% 866325
#% 937551
#% 961268
#% 961278
#% 987253
#% 1047784
#% 1074074
#% 1083655
#% 1130817
#% 1133031
#! Nowadays, enormous amounts of data are continuously generated not only in massive scale, but also from different, sometimes conflicting, views. Therefore, it is important to consolidate different concepts for intelligent decision making. For example, to predict the research areas of some people, the best results are usually achieved by combining and consolidating predictions obtained from the publication network, co-authorship network and the textual content of their publications. Multiple supervised and unsupervised hypotheses can be drawn from these information sources, and negotiating their differences and consolidating decisions usually yields a much more accurate model due to the diversity and heterogeneity of these models. In this paper, we address the problem of "consensus learning" among competing hypotheses, which either rely on outside knowledge (supervised learning) or internal structure (unsupervised clustering). We argue that consensus learning is an NP-hard problem and thus propose to solve it by an efficient heuristic method. We construct a belief graph to first propagate predictions from supervised models to the unsupervised, and then negotiate and reach consensus among them. Their final decision is further consolidated by calculating each model's weight based on its degree of consistency with other models. Experiments are conducted on 20 Newsgroups data, Cora research papers, DBLP author-conference network, and Yahoo! Movies datasets, and the results show that the proposed method improves the classification accuracy and the clustering quality measure (NMI) over the best base model by up to 10%. Furthermore, it runs in time proportional to the number of instances, which is very efficient for large scale data sets.

#index 1214656
#* Multi-focal learning and its application to customer service support
#@ Yong Ge;Hui Xiong;Wenjun Zhou;Ramendra Sahoo;Xiaofeng Gao;Weili Wu
#t 2009
#c 0
#% 438137
#% 624479
#% 739457
#% 741441
#% 785375
#% 835018
#% 989653
#% 1083746
#! In this study, we formalize a multi-focal learning problem, where training data are partitioned into several different focal groups and the prediction model will be learned within each focal group. The multi-focal learning problem is motivated by numerous real-world learning applications. For instance, for the same type of problems encountered in a customer service center, the problem descriptions from different customers can be quite different. The experienced customers usually give more precise and focused descriptions about the problem. In contrast, the inexperienced customers usually provide more diverse descriptions. In this case, the examples from the same class in the training data can be naturally in different focal groups. As a result, it is necessary to identify those natural focal groups and exploit them for learning at different focuses. The key developmental challenge is how to identify those focal groups in the training data. As a case study, we exploit multi-focal learning for profiling problems in customer service centers. The results show that multifocal learning can significantly boost the learning accuracies of existing learning algorithms, such as Support Vector Machines (SVMs), for classifying customer problems.

#index 1214657
#* Co-clustering on manifolds
#@ Quanquan Gu;Jie Zhou
#t 2009
#c 0
#% 313959
#% 342621
#% 593047
#% 643008
#% 729918
#% 732522
#% 757953
#% 769935
#% 881468
#% 891559
#% 915294
#% 995140
#% 1176865
#! Co-clustering is based on the duality between data points (e.g. documents) and features (e.g. words), i.e. data points can be grouped based on their distribution on features, while features can be grouped based on their distribution on the data points. In the past decade, several co-clustering algorithms have been proposed and shown to be superior to traditional one-side clustering. However, existing co-clustering algorithms fail to consider the geometric structure in the data, which is essential for clustering data on manifold. To address this problem, in this paper, we propose a Dual Regularized Co-Clustering (DRCC) method based on semi-nonnegative matrix tri-factorization. We deem that not only the data points, but also the features are sampled from some manifolds, namely data manifold and feature manifold respectively. As a result, we construct two graphs, i.e. data graph and feature graph, to explore the geometric structure of data manifold and feature manifold. Then our co-clustering method is formulated as semi-nonnegative matrix tri-factorization with two graph regularizers, requiring that the cluster labels of data points are smooth with respect to the data manifold, while the cluster labels of features are smooth with respect to the feature manifold. We will show that DRCC can be solved via alternating minimization, and its convergence is theoretically guaranteed. Experiments of clustering on many benchmark data sets demonstrate that the proposed method outperforms many state of the art clustering methods.

#index 1214658
#* Analyzing patterns of user content generation in online social networks
#@ Lei Guo;Enhua Tan;Songqing Chen;Xiaodong Zhang;Yihong (Eric) Zhao
#t 2009
#c 0
#% 209895
#% 577217
#% 577360
#% 659965
#% 729923
#% 754107
#% 832271
#% 868089
#% 868469
#% 903190
#% 963495
#% 981648
#% 1055741
#% 1055763
#% 1080079
#% 1082684
#% 1083675
#% 1083690
#! Various online social networks (OSNs) have been developed rapidly on the Internet. Researchers have analyzed different properties of such OSNs, mainly focusing on the formation and evolution of the networks as well as the information propagation over the networks. In knowledge-sharing OSNs, such as blogs and question answering systems, issues on how users participate in the network and how users "generate/contribute" knowledge are vital to the sustained and healthy growth of the networks. However, related discussions have not been reported in the research literature. In this work, we empirically study workloads from three popular knowledge-sharing OSNs, including a blog system, a social bookmark sharing network, and a question answering social network to examine these properties. Our analysis consistently shows that (1) users' posting behavior in these networks exhibits strong daily and weekly patterns, but the user active time in these OSNs does not follow exponential distributions; (2) the user posting behavior in these OSNs follows stretched exponential distributions instead of power-law distributions, indicating the influence of a small number of core users cannot dominate the network; (3) the distributions of user contributions on high-quality and effort-consuming contents in these OSNs have smaller stretch factors for the stretched exponential distribution. Our study provides insights into user activity patterns and lays out an analytical foundation for further understanding various properties of these OSNs.

#index 1214659
#* Tell me something I don't know: randomization strategies for iterative data mining
#@ Sami Hanhijärvi;Markus Ojala;Niko Vuokko;Kai Puolamäki;Nikolaj Tatti;Heikki Mannila
#t 2009
#c 0
#% 948087
#% 976826
#% 1001365
#% 1058703
#! There is a wide variety of data mining methods available, and it is generally useful in exploratory data analysis to use many different methods for the same dataset. This, however, leads to the problem of whether the results found by one method are a reflection of the phenomenon shown by the results of another method, or whether the results depict in some sense unrelated properties of the data. For example, using clustering can give indication of a clear cluster structure, and computing correlations between variables can show that there are many significant correlations in the data. However, it can be the case that the correlations are actually determined by the cluster structure. In this paper, we consider the problem of randomizing data so that previously discovered patterns or models are taken into account. The randomization methods can be used in iterative data mining. At each step in the data mining process, the randomization produces random samples from the set of data matrices satisfying the already discovered patterns or models. That is, given a data set and some statistics (e.g., cluster centers or co-occurrence counts) of the data, the randomization methods sample data sets having similar values of the given statistics as the original data set. We use Metropolis sampling based on local swaps to achieve this. We describe experiments on real data that demonstrate the usefulness of our approach. Our results indicate that in many cases, the results of, e.g., clustering actually imply the results of, say, frequent pattern discovery.

#index 1214660
#* Exploiting Wikipedia as external knowledge for document clustering
#@ Xiaohua Hu;Xiaodan Zhang;Caimei Lu;E. K. Park;Xiaohua Zhou
#t 2009
#c 0
#% 466657
#% 826918
#% 881552
#% 987328
#% 987333
#% 1055680
#% 1074073
#% 1083703
#% 1250362
#% 1275012
#% 1408776
#! In traditional text clustering methods, documents are represented as "bags of words" without considering the semantic information of each document. For instance, if two documents use different collections of core words to represent the same topic, they may be falsely assigned to different clusters due to the lack of shared core words, although the core words they use are probably synonyms or semantically associated in other forms. The most common way to solve this problem is to enrich document representation with the background knowledge in an ontology. There are two major issues for this approach: (1) the coverage of the ontology is limited, even for WordNet or Mesh, (2) using ontology terms as replacement or additional features may cause information loss, or introduce noise. In this paper, we present a novel text clustering method to address these two issues by enriching document representation with Wikipedia concept and category information. We develop two approaches, exact match and relatedness-match, to map text documents to Wikipedia concepts, and further to Wikipedia categories. Then the text documents are clustered based on a similarity metric which combines document content information, concept information as well as category information. The experimental results using the proposed clustering framework on three datasets (20-newsgroup, TDT2, and LA Times) show that clustering performance improves significantly by enriching document representation with Wikipedia concepts and categories.

#index 1214661
#* TrustWalker: a random walk model for combining trust-based and item-based recommendation
#@ Mohsen Jamali;Martin Ester
#t 2009
#c 0
#% 124010
#% 268079
#% 330687
#% 577217
#% 790459
#% 842605
#% 989580
#% 1001279
#% 1055691
#% 1083641
#% 1083671
#% 1083957
#% 1127466
#% 1130901
#! Collaborative filtering is the most popular approach to build recommender systems and has been successfully employed in many applications. However, it cannot make recommendations for so-called cold start users that have rated only a very small number of items. In addition, these methods do not know how confident they are in their recommendations. Trust-based recommendation methods assume the additional knowledge of a trust network among users and can better deal with cold start users, since users only need to be simply connected to the trust network. On the other hand, the sparsity of the user item ratings forces the trust-based approach to consider ratings of indirect neighbors that are only weakly trusted, which may decrease its precision. In order to find a good trade-off, we propose a random walk model combining the trust-based and the collaborative filtering approach for recommendation. The random walk model allows us to define and to measure the confidence of a recommendation. We performed an evaluation on the Epinions dataset and compared our model with existing trust-based and collaborative filtering methods.

#index 1214662
#* Drosophila gene expression pattern annotation using sparse features and term-term interactions
#@ Shuiwang Ji;Lei Yuan;Ying-Xin Li;Zhi-Hua Zhou;Sudhir Kumar;Jieping Ye
#t 2009
#c 0
#% 760805
#% 769886
#% 812535
#% 824956
#% 853308
#% 1036031
#% 1116393
#% 1126371
#% 1128929
#% 1305521
#% 1667698
#! The Drosophila gene expression pattern images document the spatial and temporal dynamics of gene expression and they are valuable tools for explicating the gene functions, interaction, and networks during Drosophila embryogenesis. To provide text-based pattern searching, the images in the Berkeley Drosophila Genome Project (BDGP) study are annotated with ontology terms manually by human curators. We present a systematic approach for automating this task, because the number of images needing text descriptions is now rapidly increasing. We consider both improved feature representation and novel learning formulation to boost the annotation performance. For feature representation, we adapt the bag-of-words scheme commonly used in visual recognition problems so that the image group information in the BDGP study is retained. Moreover, images from multiple views can be integrated naturally in this representation. To reduce the quantization error caused by the bag-of-words representation, we propose an improved feature representation scheme based on the sparse learning technique. In the design of learning formulation, we propose a local regularization framework that can incorporate the correlations among terms explicitly. We further show that the resulting optimization problem admits an analytical solution. Experimental results show that the representation based on sparse learning outperforms the bag-of-words representation significantly. Results also show that incorporation of the term-term correlations improves the annotation performance consistently.

#index 1214663
#* Cartesian contour: a concise representation for a collection of frequent sets
#@ Ruoming Jin;Yang Xiang;Lin Liu
#t 2009
#c 0
#% 152934
#% 217812
#% 248791
#% 316709
#% 629644
#% 769876
#% 823356
#% 824710
#% 832571
#% 881500
#% 881542
#% 948087
#% 1013604
#% 1083668
#% 1083708
#! In this paper, we consider a novel scheme referred to as Cartesian contour to concisely represent the collection of frequent itemsets. Different from the existing works, this scheme provides a complete view of these itemsets by covering the entire collection of them. More interestingly, it takes a first step in deriving a generative view of the frequent pattern formulation, i.e., how a small number of patterns interact with each other and produce the complexity of frequent itemsets. We perform a theoretical investigation of the concise representation problem and link it to the biclique set cover problem and prove its NP-hardness. We develop a novel approach utilizing the technique developed in frequent itemset mining, set cover, and max k-cover to approximate the minimal biclique set cover problem. In addition, we consider several heuristic techniques to speedup the construction of Cartesian contour. The detailed experimental study demonstrates the effectiveness and efficiency of our approach.

#index 1214664
#* Genre-based decomposition of email class noise
#@ Aleksander Kolcz;Gordon V. Cormack
#t 2009
#c 0
#% 397149
#% 566793
#% 769885
#% 960411
#% 961230
#% 987244
#% 1127962
#% 1378224
#% 1393032
#! Corruption of data by class-label noise is an important practical concern impacting many classification problems. Studies of data cleaning techniques often assume a uniform label noise model, however, which is seldom realized in practice. Relatively little is understood, as to how the natural label noise distribution can be measured or simulated. Using email spam-filtering data, we demonstrate that class noise can have substantial content specific bias. We also demonstrate that noise detection techniques based on classifier confidence tend to identify instances that human assessors are likely to label in error. We show that genre modeling can be very informative in identifying potential areas of mislabeling. Moreover, we are able to show that genre decomposition can also be used to substantially improve spam filtering accuracy, with our results outperforming the best published figures for the trec05-p1 and ceas-2008 benchmark collections.

#index 1214665
#* Characteristic relational patterns
#@ Arne Koopman;Arno Siebes
#t 2009
#c 0
#% 144520
#% 152934
#% 169370
#% 266230
#% 303403
#% 420087
#% 745491
#% 936820
#% 957735
#! Research in relational data mining has two major directions: finding global models of a relational database and the discovery of local relational patterns within a database. While relational patterns show how attribute values co-occur in detail, their huge numbers hamper their usage in data analysis. Global models, on the other hand, only provide a summary of how different tables and their attributes relate to each other, lacking detail of what is going on at the local level. In this paper we introduce a new approach that combines the positive properties of both directions: it provides a detailed description of the complete database using a small set of patterns. More in particular, we utilise a rich pattern language and show how a database can be encoded by such patterns. Then, based on the MDLprinciple, the novel RDB-KRIMP algorithm selects the set of patterns that allows for the most succinct encoding of the database. This set, the code table, is a compact description of the database in terms of local relational patterns. We show that this resulting set is very small, both in terms of database size and in number of its local relational patterns: a reduction of up to 4 orders of magnitude is attained.

#index 1214666
#* Collaborative filtering with temporal dynamics
#@ Yehuda Koren
#t 2009
#c 0
#% 204531
#% 330687
#% 342639
#% 397153
#% 452563
#% 727880
#% 729932
#% 754126
#% 838504
#% 983903
#% 989580
#% 1038335
#% 1083671
#% 1116993
#% 1127448
#% 1127483
#% 1358068
#% 1358069
#! Customer preferences for products are drifting over time. Product perception and popularity are constantly changing as new selection emerges. Similarly, customer inclinations are evolving, leading them to ever redefine their taste. Thus, modeling temporal dynamics should be a key when designing recommender systems or general customer preference models. However, this raises unique challenges. Within the eco-system intersecting multiple products and customers, many different characteristics are shifting simultaneously, while many of them influence each other and often those shifts are delicate and associated with a few data instances. This distinguishes the problem from concept drift explorations, where mostly a single concept is tracked. Classical time-window or instance-decay approaches cannot work, as they lose too much signal when discarding data instances. A more sensitive approach is required, which can make better distinctions between transient effects and long term patterns. The paradigm we offer is creating a model tracking the time changing behavior throughout the life span of the data. This allows us to exploit the relevant components of all data instances, while discarding only what is modeled as being irrelevant. Accordingly, we revamp two leading collaborative filtering recommendation approaches. Evaluation is made on a large movie rating dataset by Netflix. Results are encouraging and better than those previously reported on this dataset.

#index 1214667
#* Collective annotation of Wikipedia entities in web text
#@ Sayali Kulkarni;Amit Singh;Ganesh Ramakrishnan;Soumen Chakrabarti
#t 2009
#c 0
#% 198055
#% 277396
#% 408396
#% 577318
#% 757276
#% 785089
#% 1019082
#% 1130858
#% 1164874
#% 1166537
#% 1190105
#! To take the first step beyond keyword-based search toward entity-based search, suitable token spans ("spots") on documents must be identified as references to real-world entities from an entity catalog. Several systems have been proposed to link spots on Web pages to entities in Wikipedia. They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity. Two recent systems exploit inter-label dependencies, but in limited ways. We propose a general collective disambiguation approach. Our premise is that coherent documents refer to entities from one or a few related topics or domains. We give formulations for the trade-off between local spot-to-entity compatibility and measures of global coherence between entities. Optimizing the overall entity assignment is NP-hard. We investigate practical solutions based on local hill-climbing, rounding integer linear programs, and pre-clustering entities followed by local optimization within clusters. In experiments involving over a hundred manually-annotated Web pages and tens of thousands of spots, our approaches significantly outperform recently-proposed algorithms.

#index 1214668
#* Finding a team of experts in social networks
#@ Theodoros Lappas;Kun Liu;Evimaria Terzi
#t 2009
#c 0
#% 321577
#% 341672
#% 539108
#% 757953
#% 816332
#% 866279
#% 881460
#% 883793
#% 1075875
#% 1178476
#! Given a task T, a pool of individuals X with different skills, and a social network G that captures the compatibility among these individuals, we study the problem of finding X, a subset of X, to perform the task. We call this the TEAM FORMATION problem. We require that members of X' not only meet the skill requirements of the task, but can also work effectively together as a team. We measure effectiveness using the communication cost incurred by the subgraph in G that only involves X'. We study two variants of the problem for two different communication-cost functions, and show that both variants are NP-hard. We explore their connections with existing combinatorial problems and give novel algorithms for their solution. To the best of our knowledge, this is the first work to consider the TEAM FORMATION problem in the presence of a social network of individuals. Experiments on the DBLP dataset show that our framework works well in practice and gives useful and intuitive results.

#index 1214669
#* On burstiness-aware search for document sequences
#@ Theodoros Lappas;Benjamin Arai;Manolis Platakis;Dimitrios Kotsakos;Dimitrios Gunopulos
#t 2009
#c 0
#% 144288
#% 210347
#% 333854
#% 345611
#% 469401
#% 577220
#% 577360
#% 729943
#% 765412
#% 824666
#% 847161
#% 937800
#% 956682
#% 987218
#% 1022338
#% 1117049
#% 1265149
#! As the number and size of large timestamped collections (e.g. sequences of digitized newspapers, periodicals, blogs) increase, the problem of efficiently indexing and searching such data becomes more important. Term burstiness has been extensively researched as a mechanism to address event detection in the context of such collections. In this paper, we explore how burstiness information can be further utilized to enhance the search process. We present a novel approach to model the burstiness of a term, using discrepancy theory concepts. This allows us to build a parameter-free, linear-time approach to identify the time intervals of maximum burstiness for a given term. Finally, we describe the first burstiness-driven search framework and thoroughly evaluate our approach in the context of different scenarios.

#index 1214670
#* Improving data mining utility with projective sampling
#@ Mark Last
#t 2009
#c 0
#% 136350
#% 280406
#% 733620
#% 735358
#% 757953
#% 794519
#% 818916
#% 881575
#% 989635
#% 1030850
#% 1083692
#% 1085125
#! Overall performance of the data mining process depends not just on the value of the induced knowledge but also on various costs of the process itself such as the cost of acquiring and pre-processing training examples, the CPU cost of model induction, and the cost of committed errors. Recently, several progressive sampling strategies for maximizing the overall data mining utility have been proposed. All these strategies are based on repeated acquisitions of additional training examples until a utility decrease is observed. In this paper, we present an alternative, projective sampling strategy, which fits functions to a partial learning curve and a partial run-time curve obtained from a small subset of potentially available data and then uses these projected functions to analytically estimate the optimal training set size. The proposed approach is evaluated on a variety of benchmark datasets using the RapidMiner environment for machine learning and data mining processes. The results show that the learning and run-time curves projected from only several data points can lead to a cheaper data mining process than the common progressive sampling methods.

#index 1214671
#* Meme-tracking and the dynamics of the news cycle
#@ Jure Leskovec;Lars Backstrom;Jon Kleinberg
#t 2009
#c 0
#% 164166
#% 311496
#% 577220
#% 641137
#% 722904
#% 754106
#% 754107
#% 786841
#% 868089
#% 875959
#% 881498
#% 989650
#! Tracking new topics, ideas, and "memes" across the Web has been an issue of considerable interest. Recent work has developed methods for tracking topic shifts over long time scales, as well as abrupt spikes in the appearance of particular named entities. However, these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events. We develop a framework for tracking short, distinctive phrases that travel relatively intact through on-line text; developing scalable algorithms for clustering textual variants of such phrases, we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis. As our principal domain of study, we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis. We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle. In particular, we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively, with divergent behavior around the overall peak and a "heartbeat"-like pattern in the handoff between news and blogs. We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits.

#index 1214672
#* DynaMMo: mining and summarization of coevolving sequences with missing values
#@ Lei Li;James McCann;Nancy S. Pollard;Christos Faloutsos
#t 2009
#c 0
#% 457815
#% 632090
#% 654538
#% 662750
#% 765402
#% 765452
#% 781491
#% 815986
#% 874633
#% 894013
#% 915265
#% 983864
#% 1015301
#% 1016194
#% 1083693
#% 1147403
#% 1206639
#! Given multiple time sequences with missing values, we propose DynaMMo which summarizes, compresses, and finds latent variables. The idea is to discover hidden variables and learn their dynamics, making our algorithm able to function even when there are missing values. We performed experiments on both real and synthetic datasets spanning several megabytes, including motion capture sequences and chlorine levels in drinking water. We show that our proposed DynaMMo method (a) can successfully learn the latent variables and their evolution; (b) can provide high compression for little loss of reconstruction accuracy; (c) can extract compact but powerful features for segmentation, interpretation, and forecasting; (d) has complexity linear on the duration of sequences.

#index 1214673
#* On the tradeoff between privacy and utility in data publishing
#@ Tiancheng Li;Ninghui Li
#t 2009
#c 0
#% 300120
#% 443463
#% 576761
#% 577239
#% 785363
#% 800514
#% 800515
#% 824726
#% 844340
#% 864406
#% 864412
#% 874988
#% 881483
#% 881546
#% 893100
#% 960289
#% 1022246
#% 1083631
#% 1176943
#% 1206745
#% 1670071
#! In data publishing, anonymization techniques such as generalization and bucketization have been designed to provide privacy protection. In the meanwhile, they reduce the utility of the data. It is important to consider the tradeoff between privacy and utility. In a paper that appeared in KDD 2008, Brickell and Shmatikov proposed an evaluation methodology by comparing privacy gain with utility gain resulted from anonymizing the data, and concluded that "even modest privacy gains require almost complete destruction of the data-mining utility". This conclusion seems to undermine existing work on data anonymization. In this paper, we analyze the fundamental characteristics of privacy and utility, and show that it is inappropriate to directly compare privacy with utility. We then observe that the privacy-utility tradeoff in data publishing is similar to the risk-return tradeoff in financial investment, and propose an integrated framework for considering privacy-utility tradeoff, borrowing concepts from the Modern Portfolio Theory for financial investment. Finally, we evaluate our methodology on the Adult dataset from the UCI machine learning repository. Our results clarify several common misconceptions about data utility and provide data publishers useful guidelines on choosing the right tradeoff between privacy and utility.

#index 1214674
#* MetaFac: community discovery via relational hypergraph factorization
#@ Yu-Ru Lin;Jimeng Sun;Paul Castro;Ravi Konuru;Hari Sundaram;Aisling Kelliher
#t 2009
#c 0
#% 309095
#% 496116
#% 528182
#% 840840
#% 881514
#% 881523
#% 910167
#% 987253
#% 989618
#% 989640
#% 1055740
#% 1130902
#% 1250567
#! This paper aims at discovering community structure in rich media social networks, through analysis of time-varying, multi-relational data. Community structure represents the latent social context of user actions. It has important applications in information tasks such as search and recommendation. Social media has several unique challenges. (a) In social media, the context of user actions is constantly changing and co-evolving; hence the social context contains time-evolving multi-dimensional relations. (b) The social context is determined by the available system features and is unique in each social media website. In this paper we propose MetaFac (MetaGraph Factorization), a framework that extracts community structures from various social contexts and interactions. Our work has three key contributions: (1) metagraph, a novel relational hypergraph representation for modeling multi-relational and multi-dimensional social data; (2) an efficient factorization method for community extraction on a given metagraph; (3) an on-line method to handle time-varying relations through incremental metagraph factorization. Extensive experiments on real-world social data collected from the Digg social media website suggest that our technique is scalable and is able to extract meaningful communities based on the social media contexts. We illustrate the usefulness of our framework through prediction tasks. We outperform baseline methods (including aspect model and tensor analysis) by an order of magnitude.

#index 1214675
#* BBM: bayesian browsing model from petabyte-scale data
#@ Chao Liu;Fan Guo;Christos Faloutsos
#t 2009
#c 0
#% 296646
#% 310500
#% 378388
#% 528330
#% 577224
#% 578388
#% 734915
#% 783482
#% 803033
#% 818221
#% 823348
#% 824795
#% 840846
#% 869651
#% 879565
#% 879567
#% 881544
#% 946521
#% 956546
#% 963669
#% 987240
#% 1035578
#% 1055676
#% 1074092
#% 1166517
#% 1173704
#% 1415766
#% 1815596
#! Given a quarter of petabyte click log data, how can we estimate the relevance of each URL for a given query? In this paper, we propose the Bayesian Browsing Model (BBM), a new modeling technique with following advantages: (a) it does exact inference; (b) it is single-pass and parallelizable; (c) it is effective. We present two sets of experiments to test model effectiveness and efficiency. On the first set of over 50 million search instances of 1.1 million distinct queries, BBM out-performs the state-of-the-art competitor by 29.2% in log-likelihood while being 57 times faster. On the second click-log set, spanning a quarter of petabyte data, we showcase the scalability of BBM: we implemented it on a commercial MapReduce cluster, and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs.

#index 1214676
#* Large-scale sparse logistic regression
#@ Jun Liu;Jianhui Chen;Jieping Ye
#t 2009
#c 0
#% 389155
#% 588616
#% 722937
#% 757953
#% 763708
#% 770857
#% 803771
#% 846432
#% 1014657
#% 1041211
#% 1041405
#% 1073906
#% 1184909
#% 1211772
#% 1250570
#! Logistic Regression is a well-known classification method that has been used widely in many applications of data mining, machine learning, computer vision, and bioinformatics. Sparse logistic regression embeds feature selection in the classification framework using the l1-norm regularization, and is attractive in many applications involving high-dimensional data. In this paper, we propose Lassplore for solving large-scale sparse logistic regression. Specifically, we formulate the problem as the l1-ball constrained smooth convex optimization, and propose to solve the problem using the Nesterov's method, an optimal first-order black-box method for smooth convex optimization. One of the critical issues in the use of the Nesterov's method is the estimation of the step size at each of the optimization iterations. Previous approaches either applies the constant step size which assumes that the Lipschitz gradient is known in advance, or requires a sequence of decreasing step size which leads to slow convergence in practice. In this paper, we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate. Empirical comparisons with several state-of-the-art algorithms demonstrate the efficiency of the proposed Lassplore algorithm for large-scale problems.

#index 1214677
#* Classification of software behaviors for failure detection: a discriminative pattern mining approach
#@ David Lo;Hong Cheng;Jiawei Han;Siau-Cheng Khoo;Chengnian Sun
#t 2009
#c 0
#% 231941
#% 339045
#% 343052
#% 420063
#% 463903
#% 481290
#% 643965
#% 729437
#% 765387
#% 765521
#% 813990
#% 902158
#% 906081
#% 935763
#% 989617
#% 1039055
#% 1063502
#% 1707664
#! Software is a ubiquitous component of our daily life. We often depend on the correct working of software systems. Due to the difficulty and complexity of software systems, bugs and anomalies are prevalent. Bugs have caused billions of dollars loss, in addition to privacy and security threats. In this work, we address software reliability issues by proposing a novel method to classify software behaviors based on past history or runs. With the technique, it is possible to generalize past known errors and mistakes to capture failures and anomalies. Our technique first mines a set of discriminative features capturing repetitive series of events from program execution traces. It then performs feature selection to select the best features for classification. These features are then used to train a classifier to detect failures. Experiments and case studies on traces of several benchmark software systems and a real-life concurrency bug from MySQL server show the utility of the technique in capturing failures and anomalies. On average, our pattern-based classification technique outperforms the baseline approach by 24.68% in accuracy.

#index 1214678
#* Consensus group stable feature selection
#@ Steven Loscalzo;Lei Yu;Chris Ding
#t 2009
#c 0
#% 180945
#% 243728
#% 376266
#% 424997
#% 425048
#% 443894
#% 465905
#% 717417
#% 722902
#% 722935
#% 770819
#% 793239
#% 796212
#% 799760
#% 832876
#% 926881
#% 977991
#% 983823
#% 983907
#% 1083713
#! Stability is an important yet under-addressed issue in feature selection from high-dimensional and small sample data. In this paper, we show that stability of feature selection has a strong dependency on sample size. We propose a novel framework for stable feature selection which first identifies consensus feature groups from subsampling of training samples, and then performs feature selection by treating each consensus feature group as a single entity. Experiments on both synthetic and real-world data sets show that an algorithm developed under this framework is effective at alleviating the problem of small sample size and leads to more stable feature selection results and comparable or better generalization performance than state-of-the-art feature selection algorithms. Synthetic data sets and algorithm source code are available at http://www.cs.binghamton.edu/~lyu/KDD09/.

#index 1214679
#* Grouped graphical Granger modeling methods for temporal causal modeling
#@ Aurelie C. Lozano;Naoki Abe;Yan Liu;Saharon Rosset
#t 2009
#c 0
#% 944713
#% 961169
#% 989577
#! We develop and evaluate an approach to causal modeling based on time series data, collectively referred to as "grouped graphical Granger modeling methods." Graphical Granger modeling uses graphical modeling techniques on time series data and invokes the notion of "Granger causality" to make assertions on causality among a potentially large number of time series variables through inference on time-lagged effects. The present paper proposes a novel enhancement to the graphical Granger methodology by developing and applying families of regression methods that are sensitive to group information among variables, to leverage the group structure present in the lagged temporal variables according to the time series they belong to. Additionally, we propose a new family of algorithms we call group boosting, as an improved component of grouped graphical Granger modeling over the existing regression methods with grouped variable selection in the literature (e.g group Lasso). The introduction of group boosting methods is primarily motivated by the need to deal with non-linearity in the data. We perform empirical evaluation to confirm the advantage of the grouped graphical Granger methods over the standard (non-grouped) methods, as well as that specific to the methods based on group boosting. This advantage is also demonstrated for the real world application of gene regulatory network discovery from time-course microarray data.

#index 1214680
#* Spatial-temporal causal modeling for climate change attribution
#@ Aurelie C. Lozano;Hongfei Li;Alexandru Niculescu-Mizil;Yan Liu;Claudia Perlich;Jonathan Hosking;Naoki Abe
#t 2009
#c 0
#! Attribution of climate change to causal factors has been based predominantly on simulations using physical climate models, which have inherent limitations in describing such a complex and chaotic system. We propose an alternative, data centric, approach that relies on actual measurements of climate observations and human and natural forcing factors. Specifically, we develop a novel method to infer causality from spatial-temporal data, as well as a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate events, such as heatwaves. Our experimental results on a real world dataset indicate that changes in temperature are not solely accounted for by solar radiance, but attributed more significantly to CO2 and other greenhouse gases. Combined with extreme value modeling, we also show that there has been a significant increase in the intensity of extreme temperatures, and that such changes in extreme temperature are also attributable to greenhouse gases. These preliminary results suggest that our approach can offer a useful alternative to the simulation-based approach to climate modeling and attribution, and provide valuable insights from a fresh perspective.

#index 1214681
#* Using graph-based metrics with empirical risk minimization to speed up active learning on networked data
#@ Sofus A. Macskassy
#t 2009
#c 0
#% 236729
#% 248810
#% 266215
#% 280413
#% 420495
#% 464268
#% 464449
#% 464466
#% 466887
#% 565531
#% 565545
#% 729982
#% 770851
#% 876068
#% 961278
#% 1272282
#% 1289267
#! Active and semi-supervised learning are important techniques when labeled data are scarce. Recently a method was suggested for combining active learning with a semi-supervised learning algorithm that uses Gaussian fields and harmonic functions. This classifier is relational in nature: it relies on having the data presented as a partially labeled graph (also known as a within-network learning problem). This work showed yet again that empirical risk minimization (ERM) was the best method to find the next instance to label and provided an efficient way to compute ERM with the semi-supervised classifier. The computational problem with ERM is that it relies on computing the risk for all possible instances. If we could limit the candidates that should be investigated, then we can speed up active learning considerably. In the case where the data is graphical in nature, we can leverage the graph structure to rapidly identify instances that are likely to be good candidates for labeling. This paper describes a novel hybrid approach of using of community finding and social network analytic centrality measures to identify good candidates for labeling and then using ERM to find the best instance in this candidate set. We show on real-world data that we can limit the ERM computations to a fraction of instances with comparable performance.

#index 1214682
#* Characterizing individual communication patterns
#@ R. Dean Malmgren;Jake M. Hofman;Luis A.N. Amaral;Duncan J. Watts
#t 2009
#c 0
#% 814194
#% 891559
#% 1055763
#% 1082200
#% 1378458
#! The increasing availability of electronic communication data, such as that arising from e-mail exchange, presents social and information scientists with new possibilities for characterizing individual behavior and, by extension, identifying latent structure in human populations. Here, we propose a model of individual e-mail communication that is sufficiently rich to capture meaningful variability across individuals, while remaining simple enough to be interpretable. We show that the model, a cascading non-homogeneous Poisson process, can be formulated as a double-chain hidden Markov model, allowing us to use an efficient inference algorithm to estimate the model parameters from observed data. We then apply this model to two e-mail data sets consisting of 404 and 6,164 users, respectively, that were collected from two universities in different countries and years. We find that the resulting best-estimate parameter distributions for both data sets are surprisingly similar, indicating that at least some features of communication dynamics generalize beyond specific contexts. We also find that variability of individual behavior over time is significantly less than variability across the population, suggesting that individuals can be classified into persistent "types". We conclude that communication patterns may prove useful as an additional class of attribute data, complementing demographic and network data, for user classification and outlier detection-a point that we illustrate with an interpretable clustering of users based on their inferred model parameters.

#index 1214683
#* Large-scale graph mining using backbone refinement classes
#@ Andreas Maunz;Christoph Helma;Stefan Kramer
#t 2009
#c 0
#% 299985
#% 342604
#% 629708
#% 729938
#% 769951
#% 881475
#% 1117006
#% 1663621
#% 1673586
#! We present a new approach to large-scale graph mining based on so-called backbone refinement classes. The method efficiently mines tree-shaped subgraph descriptors under minimum frequency and significance constraints, using classes of fragments to reduce feature set size and running times. The classes are defined in terms of fragments sharing a common backbone. The method is able to optimize structural inter-feature entropy as opposed to occurrences, which is characteristic for open or closed fragment mining. In the experiments, the proposed method reduces feature set sizes by 90 % and 30 % compared to complete tree mining and open tree mining, respectively. Evaluation using crossvalidation runs shows that their classification accuracy is similar to the complete set of trees but significantly better than that of open trees. Compared to open or closed fragment mining, a large part of the search space can be pruned due to an improved statistical constraint (dynamic upper bound adjustment), which is also confirmed in the experiments in lower running times compared to ordinary (static) upper bound pruning. Further analysis using large-scale datasets yields insight into important properties of the proposed descriptors, such as the dataset coverage and the class size represented by each descriptor. A final cross-validation run confirms that the novel descriptors render large training sets feasible which previously might have been intractable.

#index 1214684
#* Differentially private recommender systems: building privacy into the net
#@ Frank McSherry;Ilya Mironov
#t 2009
#c 0
#% 397153
#% 576761
#% 616944
#% 785136
#% 809245
#% 824726
#% 1059443
#% 1080356
#% 1083631
#% 1083653
#% 1083671
#% 1095897
#% 1116993
#% 1206678
#% 1414540
#% 1415843
#% 1670071
#% 1732708
#% 1740518
#! We consider the problem of producing recommendations from collective user behavior while simultaneously providing guarantees of privacy for these users. Specifically, we consider the Netflix Prize data set, and its leading algorithms, adapted to the framework of differential privacy. Unlike prior privacy work concerned with cryptographically securing the computation of recommendations, differential privacy constrains a computation in a way that precludes any inference about the underlying records from its output. Such algorithms necessarily introduce uncertainty--i.e., noise--to computations, trading accuracy for privacy. We find that several of the leading approaches in the Netflix Prize competition can be adapted to provide differential privacy, without significantly degrading their accuracy. To adapt these algorithms, we explicitly factor them into two parts, an aggregation/learning phase that can be performed with differential privacy guarantees, and an individual recommendation phase that uses the learned correlations and an individual's data to provide personalized recommendations. The adaptations are non-trivial, and involve both careful analysis of the per-record sensitivity of the algorithms to calibrate noise, as well as new post-processing steps to mitigate the impact of this noise. We measure the empirical trade-off between accuracy and privacy in these adaptations, and find that we can provide non-trivial formal privacy guarantees while still outperforming the Cinematch baseline Netflix provides.

#index 1214685
#* WhereNext: a location predictor on trajectory pattern mining
#@ Anna Monreale;Fabio Pinelli;Roberto Trasarti;Fosca Giannotti
#t 2009
#c 0
#% 720033
#% 827132
#% 989604
#% 1206625
#% 1737192
#! The pervasiveness of mobile devices and location based services is leading to an increasing volume of mobility data.This side eect provides the opportunity for innovative methods that analyse the behaviors of movements. In this paper we propose WhereNext, which is a method aimed at predicting with a certain level of accuracy the next location of a moving object. The prediction uses previously extracted movement patterns named Trajectory Patterns, which are a concise representation of behaviors of moving objects as sequences of regions frequently visited with a typical travel time. A decision tree, named T-pattern Tree, is built and evaluated with a formal training and test process. The tree is learned from the Trajectory Patterns that hold a certain area and it may be used as a predictor of the next location of a new trajectory finding the best matching path in the tree. Three dierent best matching methods to classify a new moving object are proposed and their impact on the quality of prediction is studied extensively. Using Trajectory Patterns as predictive rules has the following implications: (I) the learning depends on the movement of all available objects in a certain area instead of on the individual history of an object; (II) the prediction tree intrinsically contains the spatio-temporal properties that have emerged from the data and this allows us to define matching methods that striclty depend on the properties of such movements. In addition, we propose a set of other measures, that evaluate a priori the predictive power of a set of Trajectory Patterns. This measures were tuned on a real life case study. Finally, an exhaustive set of experiments and results on the real dataset are presented.

#index 1214686
#* Correlated itemset mining in ROC space: a constraint programming approach
#@ Siegfried Nijssen;Tias Guns;Luc De Raedt
#t 2009
#c 0
#% 280409
#% 280436
#% 280477
#% 299985
#% 342604
#% 477497
#% 479643
#% 565974
#% 570158
#% 580588
#% 631970
#% 799042
#% 813990
#% 844421
#% 889273
#% 942743
#% 1063502
#% 1083646
#% 1083649
#% 1206650
#% 1289482
#% 1673557
#% 1742009
#! Correlated or discriminative pattern mining is concerned with finding the highest scoring patterns w.r.t. a correlation measure (such as information gain). By reinterpreting correlation measures in ROC space and formulating correlated itemset mining as a constraint programming problem, we obtain new theoretical insights with practical benefits. More specifically, we contribute 1) an improved bound for correlated itemset miners, 2) a novel iterative pruning algorithm to exploit the bound, and 3) an adaptation of this algorithm to mine all itemsets on the convex hull in ROC space. The algorithm does not depend on a minimal frequency threshold and is shown to outperform several alternative approaches by orders of magnitude, both in runtime and in memory requirements.

#index 1214687
#* TANGENT: a novel, 'Surprise me', recommendation algorithm
#@ Kensuke Onuma;Hanghang Tong;Christos Faloutsos
#t 2009
#c 0
#% 173879
#% 249110
#% 278500
#% 309749
#% 348173
#% 729983
#% 792297
#% 823342
#% 823370
#% 823391
#% 824710
#% 853537
#% 860672
#% 881460
#% 881480
#% 881496
#% 915310
#% 975021
#% 989580
#% 1023420
#% 1047785
#% 1055741
#% 1083628
#% 1390190
#! Most of recommender systems try to find items that are most relevant to the older choices of a given user. Here we focus on the "surprise me" query: A user may be bored with his/her usual genre of items (e.g., books, movies, hobbies), and may want a recommendation that is related, but off the beaten path, possibly leading to a new genre of books/movies/hobbies. How would we define, as well as automate, this seemingly selfcontradicting request? We introduce TANGENT, a novel recommendation algorithm to solve this problem. The main idea behind TANGENT is to envision the problem as node selection on a graph, giving high scores to nodes that are well connected to the older choices, and at the same time well connected to unrelated choices. The method is carefully designed to be (a) parameter-free (b) effective and (c) fast. We illustrate the benefits of TANGENT with experiments on both synthetic and real data sets. We show that TANGENT makes reasonable, yet surprising, horizon-broadening recommendations. Moreover, it is fast and scalable, since it can easily use existing fast algorithms on graph node proximity.

#index 1214688
#* Mind the gaps: weighting the unknown in large-scale one-class collaborative filtering
#@ Rong Pan;Martin Scholz
#t 2009
#c 0
#% 327
#% 68247
#% 209021
#% 338443
#% 734592
#% 765520
#% 765521
#% 765525
#% 813966
#% 840924
#% 915253
#% 956521
#% 1176909
#% 1176959
#% 1650569
#! One-Class Collaborative Filtering (OCCF) is a task that naturally emerges in recommender system settings. Typical characteristics include: Only positive examples can be observed, classes are highly imbalanced, and the vast majority of data points are missing. The idea of introducing weights for missing parts of a matrix has recently been shown to help in OCCF. While existing weighting approaches mitigate the first two problems above, a sparsity preserving solution that would allow to efficiently utilize data sets with e.g., hundred thousands of users and items has not yet been reported. In this paper, we study three different collaborative filtering frameworks: Low-rank matrix approximation, probabilistic latent semantic analysis, and maximum-margin matrix factorization. We propose two novel algorithms for large-scale OCCF that allow to weight the unknowns. Our experimental results demonstrate their effectiveness and efficiency on different problems, including the Netflix Prize data.

#index 1214689
#* An association analysis approach to biclustering
#@ Gaurav Pandey;Gowtham Atluri;Michael Steinbach;Chad L. Myers;Vipin Kumar
#t 2009
#c 0
#% 210160
#% 213977
#% 443466
#% 469422
#% 481290
#% 729918
#% 769905
#% 769958
#% 778215
#% 785383
#% 832775
#% 833003
#% 863387
#% 867053
#% 881465
#% 906367
#% 906512
#% 985041
#% 1019564
#% 1038958
#% 1126456
#% 1176960
#% 1669934
#! The discovery of biclusters, which denote groups of items that show coherent values across a subset of all the transactions in a data set, is an important type of analysis performed on real-valued data sets in various domains, such as biology. Several algorithms have been proposed to find different types of biclusters in such data sets. However, these algorithms are unable to search the space of all possible biclusters exhaustively. Pattern mining algorithms in association analysis also essentially produce biclusters as their result, since the patterns consist of items that are supported by a subset of all the transactions. However, a major limitation of the numerous techniques developed in association analysis is that they are only able to analyze data sets with binary and/or categorical variables, and their application to real-valued data sets often involves some lossy transformation such as discretization or binarization of the attributes. In this paper, we propose a novel association analysis framework for exhaustively and efficiently mining "range support" patterns from such a data set. On one hand, this framework reduces the loss of information incurred by the binarization- and discretization-based approaches, and on the other, it enables the exhaustive discovery of coherent biclusters. We compared the performance of our framework with two standard biclustering algorithms through the evaluation of the similarity of the cellular functions of the genes constituting the patterns/biclusters derived by these algorithms from microarray data. These experiments show that the real-valued patterns discovered by our framework are better enriched by small biologically interesting functional classes. Also, through specific examples, we demonstrate the ability of the RAP framework to discover functionally enriched patterns that are not found by the commonly used biclustering algorithm ISA. The source code and data sets used in this paper, as well as the supplementary material, are available at http://www.cs.umn.edu/vk/gaurav/rap.

#index 1214690
#* CP-summary: a concise representation for browsing frequent itemsets
#@ Ardian Kristanto Poernomo;Vivekanand Gopalkrishnan
#t 2009
#c 0
#% 152934
#% 172386
#% 237200
#% 248792
#% 502141
#% 765429
#% 769876
#% 823356
#% 824710
#% 866685
#% 881542
#% 915254
#% 948087
#% 1083668
#% 1206650
#! This paper tackles the problem of summarizing frequent itemsets. We observe that previous notions of summaries cannot be directly used for analyzing frequent itemsets. In order to be used for analysis, one requirement is that the analysts should be able to browse all frequent itemsets by only having the summary. For this purpose, we propose to build the summary based upon a novel formulation, conditional profile (or c-profile). Several features of our proposed summary are: (1) each profile in the summary can be analyzed independently, (2) it provides error guarantee (ε-adequate), and (3) it produces no false positives or false negatives. Having the formulation, the next challenge is to produce the most concise summary which satisfies the requirement. In this paper, we also designed an algorithm which is both effective and efficient for this task. The quality of our approach is justified by extensive experiments. The implementations for the algorithms are available from www.cais.ntu.edu.sg/~vivek/pubs/cprofile09.

#index 1214691
#* Towards efficient mining of proportional fault-tolerant frequent itemsets
#@ Ardian Kristanto Poernomo;Vivekanand Gopalkrishnan
#t 2009
#c 0
#% 280456
#% 342610
#% 481290
#% 769957
#% 844394
#% 915288
#% 1083657
#% 1117021
#% 1742003
#% 1742005
#! Fault-tolerant frequent itemsets (FTFI) are variants of frequent itemsets for representing and discovering generalized knowledge. However, despite growing interest in this field, no previous approach mines proportional FTFIs with their exact support (FT-support). This problem is difficult because of two concerns: (a) non anti-monotonic property of FT-support when relaxation is proportional, and (b) difficulty in computing FT-support. Previous efforts on this problem either simplify the general problem by adding constraints, or provide approximate solutions without any error guarantees. In this paper, we address these concerns in the general FTFI mining problem. We limit the search space by providing provably correct anti monotone bounds for FT-support and develop practically efficient means of achieving them. Besides, we also provide an efficient and exact FT-support counting procedure. Extensive experiments using real datasets validate that our solution is reasonably efficient for completely mining FTFIs. Implementations for the algorithms are available from www.cais.ntu.edu.sg/~vivek/pubs/ftfim09.

#index 1214692
#* Audience selection for on-line brand advertising: privacy-friendly social network targeting
#@ Foster Provost;Brian Dalessandro;Rod Hook;Xiaohan Zhang;Alan Murray
#t 2009
#c 0
#% 577273
#% 729936
#% 748026
#% 844334
#% 881480
#% 975021
#% 987262
#% 990206
#% 990220
#% 1055694
#% 1083647
#% 1200862
#% 1259854
#! This paper describes and evaluates privacy-friendly methods for extracting quasi-social networks from browser behavior on user-generated content sites, for the purpose of finding good audiences for brand advertising (as opposed to click maximizing, for example). Targeting social-network neighbors resonates well with advertisers, and on-line browsing behavior data counterintuitively can allow the identification of good audiences anonymously. Besides being one of the first papers to our knowledge on data mining for on-line brand advertising, this paper makes several important contributions. We introduce a framework for evaluating brand audiences, in analogy to predictive-modeling holdout evaluation. We introduce methods for extracting quasi-social networks from data on visitations to social networking pages, without collecting any information on the identities of the browsers or the content of the social-network pages. We introduce measures of brand proximity in the network, and show that audiences with high brand proximity indeed show substantially higher brand affinity. Finally, we provide evidence that the quasi-social network embeds a true social network, which along with results from social theory offers one explanation for the increase in brand affinity of the selected audiences.

#index 1214693
#* A principled and flexible framework for finding alternative clusterings
#@ ZiJie Qi;Ian Davidson
#t 2009
#c 0
#% 313959
#% 785341
#% 915231
#% 948091
#% 983829
#% 989595
#% 1073891
#% 1117008
#% 1176992
#! The aim of data mining is to find novel and actionable insights in data. However, most algorithms typically just find a single (possibly non-novel/actionable) interpretation of the data even though alternatives could exist. The problem of finding an alternative to a given original clustering has received little attention in the literature. Current techniques (including our previous work) are unfocused/unrefined in that they broadly attempt to find an alternative clustering but do not specify which properties of the original clustering should or should not be retained. In this work, we explore a principled and flexible framework in order to find alternative clusterings of the data. The approach is principled since it poses a constrained optimization problem, so its exact behavior is understood. It is flexible since the user can formally specify positive and negative feedback based on the existing clustering, which ranges from which clusters to keep (or not) to making a trade-off between alternativeness and clustering quality.

#index 1214694
#* Learning optimal ranking with tensor factorization for tag recommendation
#@ Steffen Rendle;Leandro Balby Marinho;Alexandros Nanopoulos;Lars Schmidt-Thieme
#t 2009
#c 0
#% 316143
#% 316150
#% 770788
#% 840924
#% 840934
#% 1074115
#% 1074117
#% 1127455
#% 1127481
#% 1130816
#% 1156304
#% 1176933
#! Tag recommendation is the task of predicting a personalized list of tags for a user given an item. This is important for many websites with tagging capabilities like last.fm or delicious. In this paper, we propose a method for tag recommendation based on tensor factorization (TF). In contrast to other TF methods like higher order singular value decomposition (HOSVD), our method RTF ('ranking with tensor factorization') directly optimizes the factorization model for the best personalized ranking. RTF handles missing values and learns from pairwise ranking constraints. Our optimization criterion for TF is motivated by a detailed analysis of the problem and of interpretation schemes for the observed data in tagging systems. In all, RTF directly optimizes for the actual problem using a correct interpretation of the data. We provide a gradient descent algorithm to solve our optimization problem. We also provide an improved learning and prediction method with runtime complexity analysis for RTF. The prediction runtime of RTF is independent of the number of observations and only depends on the factorization dimensions. Besides the theoretical analysis, we empirically show that our method outperforms other state-of-the-art tag recommendation methods like FolkRank, PageRank and HOSVD both in quality and prediction runtime.

#index 1214695
#* Scalable graph clustering using stochastic flows: applications to community discovery
#@ Venu Satuluri;Srinivasan Parthasarathy
#t 2009
#c 0
#% 274612
#% 313959
#% 344228
#% 594009
#% 769887
#% 833596
#% 840950
#% 867050
#% 1013696
#% 1055741
#! Algorithms based on simulating stochastic flows are a simple and natural solution for the problem of clustering graphs, but their widespread use has been hampered by their lack of scalability and fragmentation of output. In this article we present a multi-level algorithm for graph clustering using flows that delivers significant improvements in both quality and speed. The graph is first successively coarsened to a manageable size, and a small number of iterations of flow simulation is performed on the coarse graph. The graph is then successively refined, with flows from the previous graph used as initializations for brief flow simulations on each of the intermediate graphs. When we reach the final refined graph, the algorithm is run to convergence and the high-flow regions are clustered together, with regions without any flow forming the natural boundaries of the clusters. Extensive experimental results on several real and synthetic datasets demonstrate the effectiveness of our approach when compared to state-of-the-art algorithms.

#index 1214696
#* Measuring the effects of preprocessing decisions and network forces in dynamic network analysis
#@ Jerry Scripps;Pang-Ning Tan;Abdol-Hossein Esfahanian
#t 2009
#c 0
#% 300115
#% 730089
#% 823342
#% 844322
#% 853533
#% 853534
#% 853536
#% 1083624
#% 1083641
#% 1176963
#% 1279355
#% 1396216
#% 1650403
#! Social networks have become a major focus of research in recent years, initially directed towards static networks but increasingly, towards dynamic ones. In this paper, we investigate how different pre-processing decisions and different network forces such as selection and influence affect the modeling of dynamic networks. We also present empirical justification for some of the modeling assumptions made in dynamic network analysis (e.g., first-order Markovian assumption) and develop metrics to measure the alignment between links and attributes under different strategies of using the historical network data. We also demonstrate the effect of attribute drift, that is, the importance of individual attributes in forming links change over time.

#index 1214697
#* Mining discrete patterns via binary matrix factorization
#@ Bao-Hong Shen;Shuiwang Ji;Jieping Ye
#t 2009
#c 0
#% 53085
#% 224113
#% 262217
#% 729924
#% 755399
#% 772862
#% 796209
#% 866685
#% 867067
#% 1022958
#! Mining discrete patterns in binary data is important for subsampling, compression, and clustering. We consider rank-one binary matrix approximations that identify the dominant patterns of the data, while preserving its discrete property. A best approximation on such data has a minimum set of inconsistent entries, i.e., mismatches between the given binary data and the approximate matrix. Due to the hardness of the problem, previous accounts of such problems employ heuristics and the resulting approximation may be far away from the optimal one. In this paper, we show that the rank-one binary matrix approximation can be reformulated as a 0-1 integer linear program (ILP). However, the ILP formulation is computationally expensive even for small-size matrices. We propose a linear program (LP) relaxation, which is shown to achieve a guaranteed approximation error bound. We further extend the proposed formulations using the regularization technique, which is commonly employed to address overfitting. The LP formulation is restricted to medium-size matrices, due to the large number of variables involved for large matrices. Interestingly, we show that the proposed approximate formulation can be transformed into an instance of the minimum s-t cut problem, which can be solved efficiently by finding maximum flows. Our empirical study shows the efficiency of the proposed algorithm based on the maximum flow. Results also confirm the established theoretical bounds.

#index 1214698
#* Anomalous window discovery through scan statistics for linear intersecting paths (SSLIP)
#@ Lei Shi;Vandana P. Janeja
#t 2009
#c 0
#% 481290
#% 769941
#% 810524
#% 844378
#% 1117703
#! Anomalous windows are the contiguous groupings of data points. In this paper, we propose an approach for discovering anomalous windows using Scan Statistics for Linear Intersecting Paths (SSLIP). A linear path refers to a path represented by a line with a single dimensional spatial coordinate marking an observation point. Our approach for discovering anomalous windows along linear paths comprises of the following distinct steps: (a) Cross Path Discovery: where we identify a subset of intersecting paths to be considered, (b) Anomalous Window Discovery: where we outline three order invariant algorithms, namely SSLIP, Brute Force-SSLIP and Central Brute Force-SSLIP, for the traversal of the cross paths to identify varying size directional windows along the paths. For identifying an anomalous window we compute an unusualness metric, in the form of a likelihood ratio to indicate the degree of unusualness of this window with respect to the rest of the data. We identify the window with the highest likelihood ratio as our anomalous window, and (c) Monte Carlo Simulations: to ascertain whether this window is truly anomalous and not just a random occurrence we perform hypothesis testing by computing a p-value using Monte Carlo Simulations. We present extensive experimental results in real world accident datasets for various highways with known issues(code and data available from [27], [21]). Our results show that our approach indeed is effective in identifying anomalous traffic accident windows along multiple intersecting highways.

#index 1214699
#* User grouping behavior in online forums
#@ Xiaolin Shi;Jun Zhu;Rui Cai;Lei Zhang
#t 2009
#c 0
#% 73441
#% 277467
#% 464434
#% 823342
#% 862088
#% 868469
#% 879628
#% 881460
#% 1035581
#% 1055736
#% 1055737
#% 1083641
#% 1673048
#! Online forums represent one type of social media that is particularly rich for studying human behavior in information seeking and diffusing. The way users join communities is a reflection of the changing and expanding of their interests toward information. In this paper, we study the patterns of user participation behavior, and the feature factors that influence such behavior on different forum datasets. We find that, despite the relative randomness and lesser commitment of structural relationships in online forums, users' community joining behaviors display some strong regularities. One particularly interesting observation is that the very weak relationships between users defined by online replies have similar diffusion curves as those of real friendships or co-authorships. We build social selection models, Bipartite Markov Random Field (BiMRF), to quantitatively evaluate the prediction performance of those feature factors and their relationships. Using these models, we show that some features carry supplementary information, and the effectiveness of different features vary in different types of forums. Moreover, the results of BiMRF with two-star configurations suggest that the feature of user similarity defined by frequency of communication or number of common friends is inadequate to predict grouping behavior, but adding node-level features can improve the fit of the model.

#index 1214700
#* Causality quantification and its applications: structuring and modeling of multivariate time series
#@ Takashi Shibuya;Tatsuya Harada;Yasuo Kuniyoshi
#t 2009
#c 0
#% 832997
#! Time series prediction is an important issue in a wide range of areas. There are various real world processes whose states vary continuously, and those processes may have influences on each other. If the past information of one process X improves the predictability of another process Y, X is said to have a causal influence on Y. In order to make good predictions, it is necessary to identify the appropriate causal relationships. In addition, the processes to be modeled may include symbolic data as well as numerical data. Therefore, it is important to deal with symbolic and numerical time series seamlessly when attempting to detect causality. In this paper, we propose a new method for quantifying the strength of the causal influence from one time series to another. The proposed method can represent the strength of causality as the number of bits, whether each of two time series is symbolic or numerical. The proposed method can quantify causality even from a small number of samples. In addition, we propose structuring and modeling methods for multivariate time series using causal relationships of two time series. Our structuring and modeling methods can also deal with data sets which include both types of time series. Experimental results demonstrate that our methods can perform well even if the number of samples is small.

#index 1214701
#* Ranking-based clustering of heterogeneous information networks with star network schema
#@ Yizhou Sun;Yintao Yu;Jiawei Han
#t 2009
#c 0
#% 268079
#% 283833
#% 290830
#% 466675
#% 577273
#% 592143
#% 750863
#% 769906
#% 769967
#% 805896
#% 840840
#% 876018
#% 989654
#% 1063503
#% 1063512
#% 1074127
#% 1181261
#% 1650298
#! A heterogeneous information network is an information network composed of multiple types of objects. Clustering on such a network may lead to better understanding of both hidden structures of the network and the individual role played by every object in each cluster. However, although clustering on homogeneous networks has been studied over decades, clustering on heterogeneous networks has not been addressed until recently. A recent study proposed a new algorithm, RankClus, for clustering on bi-typed heterogeneous networks. However, a real-world network may consist of more than two types, and the interactions among multi-typed objects play a key role at disclosing the rich semantics that a network carries. In this paper, we study clustering of multi-typed heterogeneous networks with a star network schema and propose a novel algorithm, NetClus, that utilizes links across multityped objects to generate high-quality net-clusters. An iterative enhancement method is developed that leads to effective ranking-based clustering in such heterogeneous networks. Our experiments on DBLP data show that NetClus generates more accurate clustering results than the baseline topic model algorithm PLSA and the recently proposed algorithm, RankClus. Further, NetClus generates informative clusters, presenting good ranking and cluster membership information for each attribute object in each net-cluster.

#index 1214702
#* Social influence analysis in large-scale networks
#@ Jie Tang;Jimeng Sun;Chi Wang;Zi Yang
#t 2009
#c 0
#% 92145
#% 280819
#% 283833
#% 310514
#% 492962
#% 722904
#% 766409
#% 956521
#% 956540
#% 963669
#% 1055681
#% 1055737
#% 1083624
#% 1083641
#% 1083734
#% 1176930
#% 1176961
#% 1810385
#! In large social networks, nodes (users, entities) are influenced by others for various reasons. For example, the colleagues have strong influence on one's work, while the friends have strong influence on one's daily life. How to differentiate the social influences from different angles(topics)? How to quantify the strength of those social influences? How to estimate the model on real large networks? To address these fundamental questions, we propose Topical Affinity Propagation (TAP) to model the topic-level social influence on large networks. In particular, TAP can take results of any topic modeling and the existing network structure to perform topic-level influence propagation. With the help of the influence analysis, we present several important applications on real data sets such as 1) what are the representative nodes on a given topic? 2) how to identify the social influences of neighboring nodes on a particular node? To scale to real large networks, TAP is designed with efficient distributed learning algorithms that is implemented and tested under the Map-Reduce framework. We further present the common characteristics of distributed learning algorithms for Map-Reduce. Finally, we demonstrate the effectiveness and efficiency of TAP on real large data sets.

#index 1214703
#* Relational learning via latent social dimensions
#@ Lei Tang;Huan Liu
#t 2009
#c 0
#% 74846
#% 248810
#% 274612
#% 464615
#% 729968
#% 729982
#% 769942
#% 770763
#% 830281
#% 833012
#% 853535
#% 867050
#% 928386
#% 961278
#% 995140
#% 1000502
#% 1021186
#% 1083652
#% 1083699
#% 1117695
#% 1190076
#% 1250573
#% 1650403
#! Social media such as blogs, Facebook, Flickr, etc., presents data in a network format rather than classical IID distribution. To address the interdependency among data instances, relational learning has been proposed, and collective inference based on network connectivity is adopted for prediction. However, connections in social media are often multi-dimensional. An actor can connect to another actor for different reasons, e.g., alumni, colleagues, living in the same city, sharing similar interests, etc. Collective inference normally does not differentiate these connections. In this work, we propose to extract latent social dimensions based on network information, and then utilize them as features for discriminative learning. These social dimensions describe diverse affiliations of actors hidden in the network, and the discriminative learning can automatically determine which affiliations are better aligned with the class labels. Such a scheme is preferred when multiple diverse relations are associated with the same network. We conduct extensive experiments on social media data (one from a real-world blog site and the other from a popular content sharing site). Our model outperforms representative relational learning methods based on collective inference, especially when few labeled data are available. The sensitivity of this model and its connection to existing methods are also examined.

#index 1214704
#* Constant-factor approximation algorithms for identifying dynamic communities
#@ Chayant Tantipathananandh;Tanya Berger-Wolf
#t 2009
#c 0
#% 70370
#% 166862
#% 249110
#% 281214
#% 438553
#% 729923
#% 729968
#% 754107
#% 798967
#% 853532
#% 858102
#% 871315
#% 881460
#% 881509
#% 989640
#% 989643
#% 1414755
#! We propose two approximation algorithms for identifying communities in dynamic social networks. Communities are intuitively characterized as "unusually densely knit" subsets of a social network. This notion becomes more problematic if the social interactions change over time. Aggregating social networks over time can radically misrepresent the existing and changing community structure. Recently, we have proposed an optimization-based framework for modeling dynamic community structure. Also, we have proposed an algorithm for finding such structure based on maximum weight bipartite matching. In this paper, we analyze its performance guarantee for a special case where all actors can be observed at all times. In such instances, we show that the algorithm is a small constant factor approximation of the optimum. We use a similar idea to design an approximation algorithm for the general case where some individuals are possibly unobserved at times, and to show that the approximation factor increases twofold but remains a constant regardless of the input size. This is the first algorithm for inferring communities in dynamic networks with a provable approximation guarantee. We demonstrate the general algorithm on real data sets. The results confirm the efficiency and effectiveness of the algorithm in identifying dynamic communities.

#index 1214705
#* DOULION: counting triangles in massive graphs with a coin
#@ Charalampos E. Tsourakakis;U. Kang;Gary L. Miller;Christos Faloutsos
#t 2009
#c 0
#% 327
#% 23614
#% 214073
#% 338442
#% 379443
#% 599542
#% 874902
#% 898279
#% 963669
#% 1021533
#% 1083625
#% 1124590
#% 1176970
#% 1254809
#% 1682599
#! Counting the number of triangles in a graph is a beautiful algorithmic problem which has gained importance over the last years due to its significant role in complex network analysis. Metrics frequently computed such as the clustering coefficient and the transitivity ratio involve the execution of a triangle counting algorithm. Furthermore, several interesting graph mining applications rely on computing the number of triangles in the graph of interest. In this paper, we focus on the problem of counting triangles in a graph. We propose a practical method, out of which all triangle counting algorithms can potentially benefit. Using a straightforward triangle counting algorithm as a black box, we performed 166 experiments on real-world networks and on synthetic datasets as well, where we show that our method works with high accuracy, typically more than 99% and gives significant speedups, resulting in even ≈ 130 times faster performance.

#index 1214706
#* Category detection using hierarchical mean shift
#@ Pavan Vatturi;Weng-Keen Wong
#t 2009
#c 0
#% 2115
#% 281611
#% 318790
#% 349208
#% 443894
#% 724162
#% 724164
#% 770782
#% 925099
#% 1000327
#% 1250593
#% 1290057
#! Many applications in surveillance, monitoring, scientific discovery, and data cleaning require the identification of anomalies. Although many methods have been developed to identify statistically significant anomalies, a more difficult task is to identify anomalies that are both interesting and statistically significant. Category detection is an emerging area of machine learning that can help address this issue using a "human-in-the-loop" approach. In this interactive setting, the algorithm asks the user to label a query data point under an existing category or declare the query data point to belong to a previously undiscovered category. The goal of category detection is to bring to the user's attention a representative data point from each category in the data in as few queries as possible. In a data set with imbalanced categories, the main challenge is in identifying the rare categories or anomalies; hence, the task is often referred to as rare category detection. We present a new approach to rare category detection based on hierarchical mean shift. In our approach, a hierarchy is created by repeatedly applying mean shift with an increasing bandwidth on the data. This hierarchy allows us to identify anomalies in the data set at different scales, which are then posed as queries to the user. The main advantage of this methodology over existing approaches is that it does not require any knowledge of the dataset properties such as the total number of categories or the prior probabilities of the categories. Results on real-world data sets show that our hierarchical mean shift approach performs consistently better than previous techniques.

#index 1214707
#* Learning, indexing, and diagnosing network faults
#@ Ting Wang;Mudhakar Srivatsa;Dakshi Agrawal;Ling Liu
#t 2009
#c 0
#% 44876
#% 58365
#% 198091
#% 210173
#% 307424
#% 427199
#% 722530
#% 770889
#% 821933
#% 821993
#% 835188
#% 938092
#% 967001
#% 1206598
#% 1829875
#! Modern communication networks generate massive volume of operational event data, e.g., alarm, alert, and metrics, which can be used by a network management system (NMS) to diagnose potential faults. In this work, we introduce a new class of indexable fault signatures that encode temporal evolution of events generated by a network fault as well as topological relationships among the nodes where these events occur. We present an efficient learning algorithm to extract such fault signatures from noisy historical event data, and with the help of novel space-time indexing structures, we show how to perform efficient, online signature matching. We provide results from extensive experimental studies to explore the efficacy of our approach and point out potential applications of such signatures for many different types of networks including social and information networks.

#index 1214708
#* Mining broad latent query aspects from search sessions
#@ Xuanhui Wang;Deepayan Chakrabarti;Kunal Punera
#t 2009
#c 0
#% 194301
#% 296646
#% 347225
#% 348155
#% 387427
#% 577301
#% 730007
#% 765412
#% 804805
#% 805839
#% 835018
#% 838531
#% 869501
#% 987193
#% 1035578
#% 1055677
#% 1083721
#% 1130868
#% 1130878
#! Search queries are typically very short, which means they are often underspecified or have senses that the user did not think of. A broad latent query aspect is a set of keywords that succinctly represents one particular sense, or one particular information need, that can aid users in reformulating such queries. We extract such broad latent aspects from query reformulations found in historical search session logs. We propose a framework under which the problem of extracting such broad latent aspects reduces to that of optimizing a formal objective function under constraints on the total number of aspects the system can store, and the number of aspects that can be shown in response to any given query. We present algorithms to find a good set of aspects, and also to pick the best k aspects matching any query. Empirical results on real-world search engine logs show significant gains over a strong baseline that uses single-keyword reformulations: a gain of 14% and 23% in terms of human-judged accuracy and click-through data respectively, and around 20% in terms of consistency among aspects predicted for "similar" queries. This demonstrates both the importance of broad query aspects, and the efficacy of our algorithms for extracting them.

#index 1214709
#* Adapting the right measures for K-means clustering
#@ Junjie Wu;Hui Xiong;Jian Chen
#t 2009
#c 0
#% 36672
#% 280404
#% 375017
#% 397597
#% 650937
#% 755463
#% 840907
#% 878207
#% 881550
#% 939129
#% 1117030
#! Clustering validation is a long standing challenge in the clustering literature. While many validation measures have been developed for evaluating the performance of clustering algorithms, these measures often provide inconsistent information about the clustering performance and the best suitable measures to use in practice remain unknown. This paper thus fills this crucial void by giving an organized study of 16 external validation measures for K-means clustering. Specifically, we first introduce the importance of measure normalization in the evaluation of the clustering performance on data with imbalanced class distributions. We also provide normalization solutions for several measures. In addition, we summarize the major properties of these external measures. These properties can serve as the guidance for the selection of validation measures in different application scenarios. Finally, we reveal the interrelationships among these external measures. By mathematical transformation, we show that some validation measures are equivalent. Also, some measures have consistent validation performances. Most importantly, we provide a guide line to select the most suitable validation measures for K-means clustering.

#index 1214710
#* A LRT framework for fast spatial anomaly detection
#@ Mingxi Wu;Xiuyao Song;Chris Jermaine;Sanjay Ranka;John Gums
#t 2009
#c 0
#% 443531
#% 566128
#% 769901
#% 847161
#% 881458
#! Given a spatial data set placed on an n x n grid, our goal is to find the rectangular regions within which subsets of the data set exhibit anomalous behavior. We develop algorithms that, given any user-supplied arbitrary likelihood function, conduct a likelihood ratio hypothesis test (LRT) over each rectangular region in the grid, rank all of the rectangles based on the computed LRT statistics, and return the top few most interesting rectangles. To speed this process, we develop methods to prune rectangles without computing their associated LRT statistics.

#index 1214711
#* Quantification and semi-supervised classification methods for handling changes in class distribution
#@ Jack Chongjie Xue;Gary M. Weiss
#t 2009
#c 0
#% 136350
#% 331909
#% 342611
#% 450870
#% 464298
#% 466086
#% 765520
#% 926881
#% 961248
#% 1055503
#% 1085129
#% 1271973
#% 1272000
#% 1289281
#% 1412715
#% 1699624
#! In realistic settings the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier. Further complicating this scenario is the fact that labeled data is often scarce and expensive. In this paper we address the problem where the class distribution changes and only unlabeled examples are available from the new distribution. We design and evaluate a number of methods for coping with this problem and compare the performance of these methods. Our quantification-based methods estimate the class distribution of the unlabeled data from the changed distribution and adjust the original classifier accordingly, while our semi-supervised methods build a new classifier using the examples from the new (unlabeled) distribution which are supplemented with predicted class values. We also introduce a hybrid method that utilizes both quantification and semi-supervised learning. All methods are evaluated using accuracy and F-measure on a set of benchmark data sets. Our results demonstrate that our methods yield substantial improvements in accuracy and F-measure.

#index 1214712
#* Fast approximate spectral clustering
#@ Donghui Yan;Ling Huang;Michael I. Jordan
#t 2009
#c 0
#% 202286
#% 264161
#% 274612
#% 296738
#% 299535
#% 313959
#% 347211
#% 375388
#% 399587
#% 400847
#% 420139
#% 443984
#% 466083
#% 466597
#% 722815
#% 724227
#% 732552
#% 755402
#% 785121
#% 948249
#% 961204
#% 991230
#% 1013696
#% 1014669
#% 1042226
#% 1061636
#% 1705522
#% 1809518
#! Spectral clustering refers to a flexible class of clustering procedures that can produce high-quality clusterings on small data sets but which has limited applicability to large-scale problems due to its computational complexity of O(n3) in general, with n the number of data points. We extend the range of spectral clustering by developing a general framework for fast approximate spectral clustering in which a distortion-minimizing local transformation is first applied to the data. This framework is based on a theoretical analysis that provides a statistical characterization of the effect of local distortion on the mis-clustering rate. We develop two concrete instances of our general framework, one based on local k-means clustering (KASP) and one based on random projection trees (RASP). Extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy. Specifically, our algorithms outperform k-means by a large margin in terms of accuracy, and run several times faster than approximate spectral clustering based on the Nystrom method, with comparable accuracy and significantly smaller memory footprint. Remarkably, our algorithms make it possible for a single machine to spectral cluster data sets with a million observations within several minutes.

#index 1214713
#* Effective multi-label active learning for text classification
#@ Bishan Yang;Jian-Tao Sun;Tengjiao Wang;Zheng Chen
#t 2009
#c 0
#% 116165
#% 169717
#% 197394
#% 340904
#% 402289
#% 464268
#% 466576
#% 577287
#% 714351
#% 722797
#% 724192
#% 763708
#% 829013
#% 992948
#! Labeling text data is quite time-consuming but essential for automatic text classification. Especially, manually creating multiple labels for each document may become impractical when a very large amount of data is needed for training multi-label text classifiers. To minimize the human-labeling efforts, we propose a novel multi-label active learning approach which can reduce the required labeled data without sacrificing the classification accuracy. Traditional active learning algorithms can only handle single-label problems, that is, each data is restricted to have one label. Our approach takes into account the multi-label information, and select the unlabeled data which can lead to the largest reduction of the expected model loss. Specifically, the model loss is approximated by the size of version space, and the reduction rate of the size of version space is optimized with Support Vector Machines (SVM). An effective label prediction method is designed to predict possible labels for each unlabeled data point, and the expected loss for multi-label data is approximated by summing up losses on all labels according to the most confident result of label prediction. Experiments on several real-world data sets (all are publicly available) demonstrate that our approach can obtain promising classification result with much fewer labeled data than state-of-the-art methods.

#index 1214714
#* Combining link and content for community detection: a discriminative approach
#@ Tianbao Yang;Rong Jin;Yun Chi;Shenghuo Zhu
#t 2009
#c 0
#% 280819
#% 290830
#% 313959
#% 420495
#% 466574
#% 722904
#% 722914
#% 771841
#% 868089
#% 891559
#% 983833
#% 987253
#% 1083684
#! In this paper, we consider the problem of combining link and content analysis for community detection from networked data, such as paper citation networks and Word Wide Web. Most existing approaches combine link and content information by a generative model that generates both links and contents via a shared set of community memberships. These generative models have some shortcomings in that they failed to consider additional factors that could affect the community memberships and isolate the contents that are irrelevant to community memberships. To explicitly address these shortcomings, we propose a discriminative model for combining the link and content analysis for community detection. First, we propose a conditional model for link analysis and in the model, we introduce hidden variables to explicitly model the popularity of nodes. Second, to alleviate the impact of irrelevant content attributes, we develop a discriminative model for content analysis. These two models are unified seamlessly via the community memberships. We present efficient algorithms to solve the related optimization problems based on bound optimization and alternating projection. Extensive experiments with benchmark data sets show that the proposed framework significantly outperforms the state-of-the-art approaches for combining link and content analysis for community detection.

#index 1214715
#* Efficient methods for topic model inference on streaming document collections
#@ Limin Yao;David Mimno;Andrew McCallum
#t 2009
#c 0
#% 211044
#% 722904
#% 823373
#% 1083687
#! Topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace. Fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive. With today's large-scale, constantly expanding document collections, it is useful to be able to infer topic distributions for new documents without retraining the model. In this paper, we empirically evaluate the performance of several methods for topic inference in previously unseen documents, including methods based on Gibbs sampling, variational inference, and a new method inspired by text classification. The classification-based inference method produces results similar to iterative inference methods, but requires only a single matrix multiplication. In addition to these inference methods, we present SparseLDA, an algorithm and data structure for evaluating Gibbs sampling distributions. Empirical results indicate that SparseLDA can be approximately 20 times faster than traditional LDA and provide twice the speedup of previously published fast sampling methods, while also using substantially less memory.

#index 1214716
#* Time series shapelets: a new primitive for data mining
#@ Lexiang Ye;Eamonn Keogh
#t 2009
#c 0
#% 420065
#% 577221
#% 729960
#% 737331
#% 876074
#% 893161
#! Classification of time series has been attracting great interest over the past decade. Recent empirical evidence has strongly suggested that the simple nearest neighbor algorithm is very difficult to beat for most time series problems. While this may be considered good news, given the simplicity of implementing the nearest neighbor algorithm, there are some negative consequences of this. First, the nearest neighbor algorithm requires storing and searching the entire dataset, resulting in a time and space complexity that limits its applicability, especially on resource-limited sensors. Second, beyond mere classification accuracy, we often wish to gain some insight into the data. In this work we introduce a new time series primitive, time series shapelets, which addresses these limitations. Informally, shapelets are time series subsequences which are in some sense maximally representative of a class. As we shall show with extensive empirical evaluations in diverse domains, algorithms based on the time series shapelet primitives can be interpretable, more accurate and significantly faster than state-of-the-art classifiers.

#index 1214717
#* Exploring social tagging graph for web object classification
#@ Zhijun Yin;Rui Li;Qiaozhu Mei;Jiawei Han
#t 2009
#c 0
#% 248810
#% 451617
#% 458379
#% 466922
#% 785366
#% 869504
#% 869525
#% 869527
#% 956544
#% 1006582
#% 1035588
#% 1055739
#% 1055743
#% 1074116
#% 1074129
#% 1131829
#% 1269755
#! This paper studies web object classification problem with the novel exploration of social tags. Automatically classifying web objects into manageable semantic categories has long been a fundamental preprocess for indexing, browsing, searching, and mining these objects. The explosive growth of heterogeneous web objects, especially non-textual objects such as products, pictures, and videos, has made the problem of web classification increasingly challenging. Such objects often suffer from a lack of easy-extractable features with semantic information, interconnections between each other, as well as training examples with category labels. In this paper, we explore the social tagging data to bridge this gap. We cast web object classification problem as an optimization problem on a graph of objects and tags. We then propose an efficient algorithm which not only utilizes social tags as enriched semantic features for the objects, but also infers the categories of unlabeled objects from both homogeneous and heterogeneous labeled objects, through the implicit connection of social tags. Experiment results show that the exploration of social tags effectively boosts web object classification. Our algorithm significantly outperforms the state-of-the-art of general classification methods.

#index 1214718
#* Mining social networks for personalized email prioritization
#@ Shinjae Yoo;Yiming Yang;Frank Lin;Il-Chul Moon
#t 2009
#c 0
#% 268079
#% 290830
#% 736155
#% 803668
#% 905347
#% 936754
#% 963699
#% 1074167
#% 1137779
#% 1650300
#! Email is one of the most prevalent communication tools today, and solving the email overload problem is pressingly urgent. A good way to alleviate email overload is to automatically prioritize received messages according to the priorities of each user. However, research on statistical learning methods for fully personalized email prioritization (PEP) has been sparse due to privacy issues, since people are reluctant to share personal messages and importance judgments with the research community. It is therefore important to develop and evaluate PEP methods under the assumption that only limited training examples can be available, and that the system can only have the personal email data of each user during the training and testing of the model for that user. This paper presents the first study (to the best of our knowledge) under such an assumption. Specifically, we focus on analysis of personal social networks to capture user groups and to obtain rich features that represent the social roles from the viewpoint of a particular user. We also developed a novel semi-supervised (transductive) learning algorithm that propagates importance labels from training examples to test examples through message and user nodes in a personal email network. These methods together enable us to obtain an enriched vector representation of each new email message, which consists of both standard features of an email message (such as words in the title or body, sender and receiver IDs, etc.) and the induced social features from the sender and receivers of the message. Using the enriched vector representation as the input in SVM classifiers to predict the importance level for each test message, we obtained significant performance improvement over the baseline system (without induced social features) in our experiments on a multi-user data collection. We obtained significant performance improvement over the baseline system (without induced social features) in our experiments on a multi-user data collection: the relative error reduction in MAE was 31% in micro-averaging, and 14% in macro-averaging.

#index 1214719
#* Learning patterns in the dynamics of biological networks
#@ Chang hun You;Lawrence B. Holder;Diane J. Cook
#t 2009
#c 0
#% 260974
#% 408396
#% 443515
#% 445369
#% 446083
#% 466644
#% 629708
#% 881493
#% 972380
#% 989640
#% 1083700
#% 1159228
#% 1176934
#% 1387584
#! Our dynamic graph-based relational mining approach has been developed to learn structural patterns in biological networks as they change over time. The analysis of dynamic networks is important not only to understand life at the system-level, but also to discover novel patterns in other structural data. Most current graph-based data mining approaches overlook dynamic features of biological networks, because they are focused on only static graphs. Our approach analyzes a sequence of graphs and discovers rules that capture the changes that occur between pairs of graphs in the sequence. These rules represent the graph rewrite rules that the first graph must go through to be isomorphic to the second graph. Then, our approach feeds the graph rewrite rules into a machine learning system that learns general transformation rules describing the types of changes that occur for a class of dynamic biological networks. The discovered graph-rewriting rules show how biological networks change over time, and the transformation rules show the repeated patterns in the structural changes. In this paper, we apply our approach to biological networks to evaluate our approach and to understand how the biosystems change over time. We evaluate our results using coverage and prediction metrics, and compare to biological literature.

#index 1214720
#* Toward autonomic grids: analyzing the job flow with affinity streaming
#@ Xiangliang Zhang;Cyril Furtlehner;Julien Perez;Cecile Germain-Renaud;Michèle Sebag
#t 2009
#c 0
#% 232106
#% 256620
#% 578388
#% 626015
#% 729965
#% 876023
#% 881535
#% 891549
#% 1030788
#% 1041340
#% 1169537
#% 1861554
#! The Affinity Propagation (AP) clustering algorithm proposed by Frey and Dueck (2007) provides an understandable, nearly optimal summary of a dataset, albeit with quadratic computational complexity. This paper, motivated by Autonomic Computing, extends AP to the data streaming framework. Firstly a hierarchical strategy is used to reduce the complexity to O(N1+ε); the distortion loss incurred is analyzed in relation with the dimension of the data items. Secondly, a coupling with a change detection test is used to cope with non-stationary data distribution, and rebuild the model as needed. The presented approach StrAP is applied to the stream of jobs submitted to the EGEE Grid, providing an understandable description of the job flow and enabling the system administrator to spot online some sources of failures.

#index 1214721
#* Parallel community detection on large networks with propinquity dynamics
#@ Yuzhou Zhang;Jianyong Wang;Yi Wang;Lizhu Zhou
#t 2009
#c 0
#% 823347
#% 956459
#! Graphs or networks can be used to model complex systems. Detecting community structures from large network data is a classic and challenging task. In this paper, we propose a novel community detection algorithm, which utilizes a dynamic process by contradicting the network topology and the topology-based propinquity, where the propinquity is a measure of the probability for a pair of nodes involved in a coherent community structure. Through several rounds of mutual reinforcement between topology and propinquity, the community structures are expected to naturally emerge. The overlapping vertices shared between communities can also be easily identified by an additional simple postprocessing. To achieve better efficiency, the propinquity is incrementally calculated. We implement the algorithm on a vertex-oriented bulk synchronous parallel(BSP) model so that the mining load can be distributed on thousands of machines. We obtained interesting experimental results on several real network data.

#index 1214722
#* Co-evolution of social and affiliation networks
#@ Elena Zheleva;Hossam Sharara;Lise Getoor
#t 2009
#c 0
#% 283833
#% 853532
#% 867050
#% 881460
#% 1002007
#% 1083675
#% 1083682
#! In our work, we address the problem of modeling social network generation which explains both link and group formation. Recent studies on social network evolution propose generative models which capture the statistical properties of real-world networks related only to node-to-node link formation. We propose a novel model which captures the co-evolution of social and affiliation networks. We provide surprising insights into group formation based on observations in several real-world networks, showing that users often join groups for reasons other than their friends. Our experiments show that the model is able to capture both the newly observed and previously studied network properties. This work is the first to propose a generative model which captures the statistical properties of these complex networks. The proposed model facilitates controlled experiments which study the effect of actors' behavior on the evolution of affiliation networks, and it allows the generation of realistic synthetic datasets.

#index 1214723
#* Information theoretic regularization for semi-supervised boosting
#@ Lei Zheng;Shaojun Wang;Yan Liu;Chi-Hoon Lee
#t 2009
#c 0
#% 73372
#% 115608
#% 131258
#% 226495
#% 235377
#% 252011
#% 299255
#% 311027
#% 425065
#% 577240
#% 757953
#% 770758
#% 939527
#% 1073918
#% 1808946
#! We present novel semi-supervised boosting algorithms that incrementally build linear combinations of weak classifiers through generic functional gradient descent using both labeled and unlabeled training data. Our approach is based on extending information regularization framework to boosting, bearing loss functions that combine log loss on labeled data with the information-theoretic measures to encode unlabeled data. Even though the information-theoretic regularization terms make the optimization non-convex, we propose simple sequential gradient descent optimization algorithms, and obtain impressively improved results on synthetic, benchmark and real world tasks over supervised boosting algorithms which use the labeled data alone and a state-of-the-art semi-supervised boosting algorithm.

#index 1214724
#* Cross domain distribution adaptation via kernel mapping
#@ Erheng Zhong;Wei Fan;Jing Peng;Kun Zhang;Jiangtao Ren;Deepak Turaga;Olivier Verscheure
#t 2009
#c 0
#% 766438
#% 842682
#% 844346
#% 857439
#% 926881
#% 961218
#% 983814
#% 983828
#% 983939
#% 1083655
#% 1270196
#% 1663657
#! When labeled examples are limited and difficult to obtain, transfer learning employs knowledge from a source domain to improve learning accuracy in the target domain. However, the assumption made by existing approaches, that the marginal and conditional probabilities are directly related between source and target domains, has limited applicability in either the original space or its linear transformations. To solve this problem, we propose an adaptive kernel approach that maps the marginal distribution of target-domain and source-domain data into a common kernel space, and utilize a sample selection strategy to draw conditional probabilities between the two domains closer. We formally show that under the kernel-mapping space, the difference in distributions between the two domains is bounded; and the prediction error of the proposed approach can also be bounded. Experimental results demonstrate that the proposed method outperforms both traditional inductive classifiers and the state-of-the-art boosting-based transfer algorithms on most domains, including text categorization and web page ratings. In particular, it can achieve around 10% higher accuracy than other approaches for the text categorization problem. The source code and datasets are available from the authors.

#index 1214725
#* Mining rich session context to improve web search
#@ Guangyu Zhu;Gilad Mishne
#t 2009
#c 0
#% 290830
#% 296646
#% 309095
#% 324129
#% 399058
#% 411762
#% 577224
#% 729437
#% 754059
#% 769506
#% 805897
#% 818221
#% 818255
#% 879567
#% 881540
#% 881570
#% 955762
#% 956495
#% 956509
#% 963669
#% 1016177
#% 1019076
#% 1035576
#% 1055676
#% 1055715
#% 1064167
#% 1074093
#% 1074107
#% 1130852
#% 1130868
#% 1272396
#! User browsing information, particularly their non-search related activity, reveals important contextual information on the preferences and the intent of web users. In this paper, we expand the use of browsing information for web search ranking and other applications, with an emphasis on analyzing individual user sessions for creating aggregate models. In this context, we introduce ClickRank, an efficient, scalable algorithm for estimating web page and web site importance from browsing information. We lay out the theoretical foundation of ClickRank based on an intentional surfer model and analyze its properties. We evaluate its effectiveness for the problem of web search ranking, showing that it contributes significantly to retrieval performance as a novel web search feature. We demonstrate that the results produced by ClickRank for web search ranking are highly competitive with those produced by other approaches, yet achieved at better scalability and substantially lower computational costs. Finally, we discuss novel applications of ClickRank in providing enriched user web search experience, highlighting the usefulness of our approach for non-ranking tasks.

#index 1214726
#* Primal sparse Max-margin Markov networks
#@ Jun Zhu;Eric P. Xing;Bo Zhang
#t 2009
#c 0
#% 360691
#% 464434
#% 722760
#% 757953
#% 770763
#% 957325
#% 983808
#% 983842
#% 989644
#% 1073906
#% 1074029
#% 1117681
#% 1211849
#% 1264133
#! Max-margin Markov networks (M3N) have shown great promise in structured prediction and relational learning. Due to the KKT conditions, the M3N enjoys dual sparsity. However, the existing M3N formulation does not enjoy primal sparsity, which is a desirable property for selecting significant features and reducing the risk of over-fitting. In this paper, we present an l1-norm regularized max-margin Markov network (l1-M3N), which enjoys dual and primal sparsity simultaneously. To learn an l1-M3N, we present three methods including projected sub-gradient, cutting-plane, and a novel EM-style algorithm, which is based on an equivalence between l1-M3N and an adaptive M3N. We perform extensive empirical studies on both synthetic and real data sets. Our experimental results show that: (1) l1-M3N can effectively select significant features; (2) l1-M3N can perform as well as the pseudo-primal sparse Laplace M3N in prediction accuracy, while consistently outperforms other competing methods that enjoy either primal or dual sparsity; and (3) the EM-algorithm is more robust than the other two in pre-diction accuracy and time efficiency.

#index 1214727
#* Augmenting the generalized hough transform to enable the mining of petroglyphs
#@ Qiang Zhu;Xiaoyue Wang;Eamonn Keogh;Sang-Hee Lee
#t 2009
#c 0
#% 321652
#% 435374
#% 664387
#% 881536
#% 885438
#% 893161
#% 957326
#% 1010569
#% 1068995
#% 1095843
#! Rock art is an archaeological term for human-made markings on stone. It is believed that there are millions of petroglyphs in North America alone, and the study of this valued cultural resource has implications even beyond anthropology and history. Surprisingly, although image processing, information retrieval and data mining have had large impacts on many human endeavors, they have had essentially zero impact on the study of rock art. In this work we identify the reasons for this, and introduce a novel distance measure and algorithms which allow efficient and effective data mining of large collections of rock art.

#index 1214728
#* Modeling and predicting user behavior in sponsored search
#@ Josh Attenberg;Sandeep Pandey;Torsten Suel
#t 2009
#c 0
#% 268079
#% 325198
#% 590523
#% 751830
#% 805200
#% 805878
#% 818221
#% 823348
#% 879567
#% 881570
#% 946521
#% 956495
#% 987212
#% 1035576
#% 1047435
#% 1055676
#% 1074101
#% 1074107
#% 1127449
#% 1130811
#% 1130910
#! Implicit user feedback, including click-through and subsequent browsing behavior, is crucial for evaluating and improving the quality of results returned by search engines. Several recent studies [1, 2, 3, 13, 25] have used post-result browsing behavior including the sites visited, the number of clicks, and the dwell time on site in order to improve the ranking of search results. In this paper, we first study user behavior on sponsored search results (i.e., the advertisements displayed by search engines next to the organic results), and compare this behavior to that of organic results. Second, to exploit post-result user behavior for better ranking of sponsored results, we focus on identifying patterns in user behavior and predict expected on-site actions in future instances. In particular, we show how post-result behavior depends on various properties of the queries, advertisement, sites, and users, and build a classifier using properties such as these to predict certain aspects of the user behavior. Additionally, we develop a generative model to mimic trends in observed user activity using a mixture of pareto distributions. We conduct experiments based on billions of real navigation trails collected by a major search engine's browser toolbar.

#index 1214729
#* Enabling analysts in managed services for CRM analytics
#@ Indrajit Bhattacharya;Shantanu Godbole;Ajay Gupta;Ashish Verma;Jeff Achtermann;Kevin English
#t 2009
#c 0
#% 269217
#% 413639
#% 960315
#% 1083692
#% 1083725
#% 1119026
#% 1207032
#! Data analytics tools and frameworks abound, yet rapid deployment of analytics solutions that deliver actionable insights from business data remains a challenge. The primary reason is that on-field practitioners are required to be both technically proficient and knowledgeable about the business. The recent abundance of unstructured business data has thrown up new opportunities for analytics, but has also multiplied the deployment challenge, since interpretation of concepts derived from textual sources require a deep understanding of the business. In such a scenario, a managed service for analytics comes up as the best alternative. A managed analytics service is centered around a business analyst who acts as a liaison between the business and the technology. This calls for new tools that assist the analyst to be efficient in the tasks that she needs to execute. Also, the analytics needs to be repeatable, in that the delivered insights should not depend heavily on the expertise of specific analysts. These factors lead us to identify new areas that open up for KDD research in terms of 'time-to-insight' and repeatability for these analysts. We present our analytics framework in the form of a managed service offering for CRM analytics. We describe different analyst-centric tools using a case study from real-life engagements and demonstrate their effectiveness.

#index 1214730
#* Applying syntactic similarity algorithms for enterprise information management
#@ Ludmila Cherkasova;Kave Eshghi;Charles B. Morrey;Joseph Tucek;Alistair Veitch
#t 2009
#c 0
#% 201935
#% 204673
#% 249238
#% 255137
#% 342373
#% 345087
#% 347225
#% 504572
#% 571725
#% 616528
#% 818223
#% 823364
#% 879600
#% 963462
#% 978157
#% 1603006
#! For implementing content management solutions and enabling new applications associated with data retention, regulatory compliance, and litigation issues, enterprises need to develop advanced analytics to uncover relationships among the documents, e.g., content similarity, provenance, and clustering. In this paper, we evaluate the performance of four syntactic similarity algorithms. Three algorithms are based on Broder's "shingling" technique while the fourth algorithm employs a more recent approach, "content-based chunking". For our experiments, we use a specially designed corpus of documents that includes a set of "similar" documents with a controlled number of modifications. Our performance study reveals that the similarity metric of all four algorithms is highly sensitive to settings of the algorithms' parameters: sliding window size and fingerprint sampling frequency. We identify a useful range of these parameters for achieving good practical results, and compare the performance of the four algorithms in a controlled environment. We validate our results by applying these algorithms to finding near-duplicates in two large collections of HP technical support documents.

#index 1214731
#* A case study of behavior-driven conjoint analysis on Yahoo!: front page today module
#@ Wei Chu;Seung-Taek Park;Todd Beaupre;Nitin Motgi;Amit Phadke;Seinjuti Chakraborty;Joe Zachariah
#t 2009
#c 0
#% 107148
#% 771247
#% 857076
#% 889104
#% 959291
#% 1190124
#% 1396086
#! Conjoint analysis is one of the most popular market research methodologies for assessing how customers with heterogeneous preferences appraise various objective characteristics in products or services, which provides critical inputs for many marketing decisions, e.g. optimal design of new products and target market selection. Nowadays it becomes practical in e-commercial applications to collect millions of samples quickly. However, the large-scale data sets make traditional conjoint analysis coupled with sophisticated Monte Carlo simulation for parameter estimation computationally prohibitive. In this paper, we report a successful large-scale case study of conjoint analysis on click through stream in a real-world application at Yahoo!. We consider identifying users' heterogenous preferences from millions of click/view events and building predictive models to classify new users into segments of distinct behavior pattern. A scalable conjoint analysis technique, known as tensor segmentation, is developed by utilizing logistic tensor regression in standard partworth framework for solutions. In offline analysis on the samples collected from a random bucket of Yahoo! Front Page Today Module, we compare tensor segmentation against other segmentation schemes using demographic information, and study user preferences on article content within tensor segments. Our knowledge acquired in the segmentation results also provides assistance to editors in content management and user targeting. The usefulness of our approach is further verified by the observations in a bucket test launched in Dec. 2008.

#index 1214732
#* Seven pitfalls to avoid when running controlled experiments on the web
#@ Thomas Crook;Brian Frasca;Ron Kohavi;Roger Longbotham
#t 2009
#c 0
#% 420132
#% 433965
#% 768666
#% 805200
#% 927337
#% 989668
#% 1154062
#! Controlled experiments, also called randomized experiments and A/B tests, have had a profound influence on multiple fields, including medicine, agriculture, manufacturing, and advertising. While the theoretical aspects of offline controlled experiments have been well studied and documented, the practical aspects of running them in online settings, such as web sites and services, are still being developed. As the usage of controlled experiments grows in these online settings, it is becoming more important to understand the opportunities and pitfalls one might face when using them in practice. A survey of online controlled experiments and lessons learned were previously documented in Controlled Experiments on the Web: Survey and Practical Guide (Kohavi, et al., 2009). In this follow-on paper, we focus on pitfalls we have seen after running numerous experiments at Microsoft. The pitfalls include a wide range of topics, such as assuming that common statistical formulas used to calculate standard deviation and statistical power can be applied and ignoring robots in analysis (a problem unique to online settings). Online experiments allow for techniques like gradual ramp-up of treatments to avoid the possibility of exposing many customers to a bad (e.g., buggy) Treatment. With that ability, we discovered that it's easy to incorrectly identify the winning Treatment because of Simpson's paradox.

#index 1214733
#* Pervasive parallelism in data mining: dataflow solution to co-clustering large and sparse Netflix data
#@ Srivatsava Daruru;Nena M. Marin;Matt Walker;Joydeep Ghosh
#t 2009
#c 0
#% 722934
#% 729437
#% 844369
#% 989573
#% 989580
#% 989596
#% 1014670
#% 1083671
#! All Netflix Prize algorithms proposed so far are prohibitively costly for large-scale production systems. In this paper, we describe an efficient dataflow implementation of a collaborative filtering (CF) solution to the Netflix Prize problem [1] based on weighted coclustering [5]. The dataflow library we use facilitates the development of sophisticated parallel programs designed to fully utilize commodity multicore hardware, while hiding traditional difficulties such as queuing, threading, memory management, and deadlocks. The dataflow CF implementation first compresses the large, sparse training dataset into co-clusters. Then it generates recommendations by combining the average ratings of the co-clusters with the biases of the users and movies. When configured to identify 20x20 co-clusters in the Netflix training dataset, the implementation predicted over 100 million ratings in 16.31 minutes and achieved an RMSE of 0.88846 without any fine-tuning or domain knowledge. This is an effective real-time prediction runtime of 9.7 us per rating which is far superior to previously reported results. Moreover, the implemented co-clustering framework supports a wide variety of other large-scale data mining applications and forms the basis for predictive modeling on large, dyadic datasets [4, 7].

#index 1214734
#* Entity discovery and assignment for opinion mining applications
#@ Xiaowen Ding;Bing Liu;Lei Zhang
#t 2009
#c 0
#% 464434
#% 577355
#% 746885
#% 769892
#% 815915
#% 818916
#% 854646
#% 855279
#% 939352
#% 939848
#% 939896
#% 940000
#% 956510
#% 1035591
#% 1117045
#% 1250367
#% 1251616
#% 1261526
#% 1261566
#! Opinion mining became an important topic of study in recent years due to its wide range of applications. There are also many companies offering opinion mining services. One problem that has not been studied so far is the assignment of entities that have been talked about in each sentence. Let us use forum discussions about products as an example to make the problem concrete. In a typical discussion post, the author may give opinions on multiple products and also compare them. The issue is how to detect what products have been talked about in each sentence. If the sentence contains the product names, they need to be identified. We call this problem entity discovery. If the product names are not explicitly mentioned in the sentence but are implied due to the use of pronouns and language conventions, we need to infer the products. We call this problem entity assignment. These problems are important because without knowing what products each sentence talks about the opinion mined from the sentence is of little use. In this paper, we study these problems and propose two effective methods to solve the problems. Entity discovery is based on pattern discovery and entity assignment is based on mining of comparative sentences. Experimental results using a large number of forum posts demonstrate the effectiveness of the technique. Our system has also been successfully tested in a commercial setting.

#index 1214735
#* Migration motif: a spatial - temporal pattern mining approach for financial markets
#@ Xiaoxi Du;Ruoming Jin;Liang Ding;Victor E. Lee;John H. Thornton, Jr.
#t 2009
#c 0
#% 280416
#% 318330
#% 463903
#% 487887
#% 659971
#% 799397
#% 960283
#% 975358
#% 989604
#! A recent study by two prominent finance researchers, Fama and French, introduces a new framework for studying risk vs. return: the migration of stocks across size-value portfolio space. Given the financial events of 2008, this first attempt to disentangle the relationships between migration behavior and stock returns is especially timely. Their work, however, derives results only for market segments, not individual companies, and only for one-year moves. Thus, we see a new challenge for financial data mining: how to capture and categorize the migration of individual companies, and how such behavior affects their returns. We propose a novel data mining approach to study the multi-year movement of individual companies. Specifically, we address the question: "How does one discover frequent migration patterns in the stock market?" We present a new trajectory mining algorithm to discover migration motifs in financial markets. Novel features of this algorithm are its handling of approximate pattern matching through a graph theoretical method, maximal clique identification, and incorporation of temporal and spatial constraints. We have performed a detailed study of the NASDAQ, NYSE, and AMEX stock markets, over a 43-year span. We successfully find migration motifs that confirm existing finance theories and other motifs that may lead to new financial models.

#index 1214736
#* Improving classification accuracy using automatically extracted training data
#@ Ariel Fuxman;Anitha Kannan;Andrew B. Goldberg;Rakesh Agrawal;Panayiotis Tsaparas;John Shafer
#t 2009
#c 0
#% 115608
#% 577224
#% 722495
#% 748550
#% 815796
#% 817693
#% 818281
#% 869550
#% 939958
#% 946521
#% 983808
#% 1074093
#% 1166522
#% 1682429
#! Classification is a core task in knowledge discovery and data mining, and there has been substantial research effort in developing sophisticated classification models. In a parallel thread, recent work from the NLP community suggests that for tasks such as natural language disambiguation even a simple algorithm can outperform a sophisticated one, if it is provided with large quantities of high quality training data. In those applications, training data occurs naturally in text corpora, and high quality training data sets running into billions of words have been reportedly used. We explore how we can apply the lessons from the NLP community to KDD tasks. Specifically, we investigate how to identify data sources that can yield training data at low cost and study whether the quantity of the automatically extracted training data can compensate for its lower quality. We carry out this investigation for the specific task of inferring whether a search query has commercial intent. We mine toolbar and click logs to extract queries from sites that are predominantly commercial (e.g., Amazon) and non-commercial (e.g., Wikipedia). We compare the accuracy obtained using such training data against manually labeled training data. Our results show that we can have large accuracy gains using automatically extracted training data at much lower cost.

#index 1214737
#* Address standardization with latent semantic association
#@ Honglei Guo;Huijia Zhu;Zhili Guo;XiaoXun Zhang;Zhong Su
#t 2009
#c 0
#% 78171
#% 280819
#% 333943
#% 466231
#% 722797
#% 722904
#% 769877
#% 815878
#% 855108
#% 855114
#% 855123
#% 855291
#% 879587
#% 938727
#% 1261584
#! Address standardization is a very challenging task in data cleansing. To provide better customer relationship management and business intelligence for customer-oriented cooperates, millions of free-text addresses need to be converted to a standard format for data integration, de-duplication and householding. Existing commercial tools usually employ lots of hand-craft, domain-specific rules and reference data dictionary of cities, states etc. These rules work better for the region they are designed. However, rule-based methods usually require more human efforts to rewrite these rules for each new domain since address data are very irregular and varied with countries and regions. Supervised learning methods usually are more adaptable than rule-based approaches. However, supervised methods need large-scale labeled training data. It is a labor-intensive and time-consuming task to build a large-scale annotated corpus for each target domain. For minimizing human efforts and the size of labeled training data set, we present a free-text address standardization method with latent semantic association (LaSA). LaSA model is constructed to capture latent semantic association among words from the unlabeled corpus. The original term space of the target domain is projected to a concept space using LaSA model at first, then the address standardization model is active learned from LaSA features and informative samples. The proposed method effectively captures the data distribution of the domain. Experimental results on large-scale English and Chinese corpus show that the proposed method significantly enhances the performance of standardization with less efforts and training data.

#index 1214738
#* Catching the drift: learning broad matches from clickthrough data
#@ Sonal Gupta;Mikhail Bilenko;Matthew Richardson
#t 2009
#c 0
#% 235941
#% 243728
#% 262105
#% 302390
#% 394984
#% 565237
#% 729913
#% 748499
#% 818221
#% 854636
#% 869500
#% 869501
#% 879567
#% 956546
#% 975021
#% 989668
#% 1004294
#% 1042610
#% 1055677
#% 1055685
#% 1055694
#% 1055713
#% 1074101
#% 1074127
#% 1074148
#% 1083692
#% 1083721
#! Identifying similar keywords, known as broad matches, is an important task in online advertising that has become a standard feature on all major keyword advertising platforms. Effective broad matching leads to improvements in both relevance and monetization, while increasing advertisers' reach and making campaign management easier. In this paper, we present a learning-based approach to broad matching that is based on exploiting implicit feedback in the form of advertisement clickthrough logs. Our method can utilize arbitrary similarity functions by incorporating them as features. We present an online learning algorithm, Amnesiac Averaged Perceptron, that is highly efficient yet able to quickly adjust to the rapidly-changing distributions of bidded keywords, advertisements and user behavior. Experimental results obtained from (1) historical logs and (2) live trials on a large-scale advertising platform demonstrate the effectiveness of the proposed algorithm and the overall success of our approach in identifying high-quality broad match mappings.

#index 1214739
#* COA: finding novel patents through text analysis
#@ Mohammad Al Hasan;W. Scott Spangler;Thomas Griffin;Alfredo Alba
#t 2009
#c 0
#% 268079
#% 282905
#% 740916
#% 967552
#% 989633
#! In recent years, the number of patents filed by the business enterprises in the technology industry are growing rapidly, thus providing unprecedented opportunities for knowledge discovery in patent data. One important task in this regard is to employ data mining techniques to rank patents in terms of their potential to earn money through licensing. Availability of such ranking can substantially reduce enterprise IP (Intellectual Property) management costs. Unfortunately, the existing software systems in the IP domain do not address this task directly. Through our research, we build a patent ranking software, named COA (Claim Originality Analysis) that rates a patent based on its value by measuring the recency and the impact of the important phrases that appear in the "claims" section of a patent. Experiments show that COA produces meaningful ranking when comparing it with other indirect patent evaluation metrics--citation count, patent status, and attorney's rating. In reallife settings, this tool was used by beta-testers in the IBM IP department. Lawyers found it very useful in patent rating, specifically, in highlighting potentially valuable patents in a patent cluster. In this article, we describe the ranking techniques and system architecture of COA. We also present the results that validate its effectiveness.

#index 1214740
#* Network anomaly detection based on Eigen equation compression
#@ Shunsuke Hirose;Kenji Yamanishi;Takayuki Nakata;Ryohei Fujimaki
#t 2009
#c 0
#% 224113
#% 280408
#% 577295
#% 769920
#% 823375
#% 844334
#% 915316
#% 922861
#% 1117054
#! This paper addresses the issue of unsupervised network anomaly detection. In recent years, networks have played more and more critical roles. Since their outages cause serious economic losses, it is quite significant to monitor their changes over time and to detect anomalies as early as possible. In this paper, we specifically focus on the management of the whole network. In it, it is important to detect anomalies which make great impact on the whole network, and the other local anomalies should be ignored. Further, when we detect the former anomalies, it is required to localize nodes responsible for them. It is challenging to simultaneously perform the above two tasks taking into account the nonstationarity and strong correlations between nodes. We propose a network anomaly detection method which resolves the above two tasks in a unified way. The key ideas of the method are: (1)construction of quantities representing feature of a whole network and each node from the same input based on eigen equation compression, and (2)incremental anomalousness scoring based on learning the probability distribution of the quantities. We demonstrate through the experimental results using two benchmark data sets and a simulation data set that anomalies of a whole network and nodes responsible for them can be detected by the proposed method.

#index 1214741
#* OpinionMiner: a novel machine learning system for web opinion mining and extraction
#@ Wei Jin;Hung Hay Ho;Rohini K. Srihari
#t 2009
#c 0
#% 577355
#% 722308
#% 755861
#% 769892
#% 815915
#% 829973
#% 854646
#% 907489
#% 938687
#% 939896
#% 1035591
#! Merchants selling products on the Web often ask their customers to share their opinions and hands-on experiences on products they have purchased. Unfortunately, reading through all customer reviews is difficult, especially for popular items, the number of reviews can be up to hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision. The OpinionMiner system designed in this work aims to mine customer reviews of a product and extract high detailed product entities on which reviewers express their opinions. Opinion expressions are identified and opinion orientations for each recognized product entity are classified as positive or negative. Different from previous approaches that employed rule-based or statistical techniques, we propose a novel machine learning approach built under the framework of lexicalized HMMs. The approach naturally integrates multiple important linguistic features into automatic learning. In this paper, we describe the architecture and main components of the system. The evaluation of the proposed method is presented based on processing the online product reviews from Amazon and other publicly available datasets.

#index 1214742
#* Query result clustering for object-level search
#@ Jongwuk Lee;Seung-won Hwang;Zaiqing Nie;Ji-Rong Wen
#t 2009
#c 0
#% 218992
#% 248792
#% 262045
#% 273891
#% 280417
#% 296738
#% 300131
#% 342613
#% 464291
#% 643008
#% 765518
#% 766433
#% 778729
#% 800529
#% 805896
#% 844289
#% 871026
#% 879615
#% 912246
#% 956501
#% 958200
#% 987203
#% 1074118
#! Query result clustering has recently attracted a lot of attention to provide users with a succinct overview of relevant results. However, little work has been done on organizing the query results for object-level search. Object-level search result clustering is challenging because we need to support diverse similarity notions over object-specific features (such as the price and weight of a product) of heterogeneous domains. To address this challenge, we propose a hybrid subspace clustering algorithm called Hydra. Algorithm Hydra captures the user perception of diverse similarity notions from millions of Web pages and disambiguates different senses using feature-based subspace locality measures. Our proposed solution, by combining wisdom of crowds and wisdom of data, achieves robustness and efficiency over existing approaches. We extensively evaluate our proposed framework and demonstrate how to enrich user experiences in object-level search using a real-world product search scenarios.

#index 1214743
#* Grocery shopping recommendations based on basket-sensitive random walk
#@ Ming Li;Benjamin M. Dias;Ian Jarman;Wael El-Deredy;Paulo J.G. Lisboa
#t 2009
#c 0
#% 202011
#% 220709
#% 220711
#% 268079
#% 280456
#% 282905
#% 330687
#% 348173
#% 577329
#% 734594
#% 768664
#% 769952
#% 1001293
#% 1074061
#% 1127466
#% 1130903
#% 1650569
#! We describe a recommender system in the domain of grocery shopping. While recommender systems have been widely studied, this is mostly in relation to leisure products (e.g. movies, books and music) with non-repeated purchases. In grocery shopping, however, consumers will make multiple purchases of the same or very similar products more frequently than buying entirely new items. The proposed recommendation scheme offers several advantages in addressing the grocery shopping problem, namely: 1) a product similarity measure that suits a domain where no rating information is available; 2) a basket sensitive random walk model to approximate product similarities by exploiting incomplete neighborhood information; 3) online adaptation of the recommendation based on the current basket and 4) a new performance measure focusing on products that customers have not purchased before or purchase infrequently. Empirical results benchmarking on three real-world data sets demonstrate a performance improvement of the proposed method over other existing collaborative filtering models.

#index 1214744
#* Learning dynamic temporal graphs for oil-production equipment monitoring system
#@ Yan Liu;Jayant R. Kalagnanam;Oivind Johnsen
#t 2009
#c 0
#% 277396
#% 770828
#% 983844
#% 989577
#% 1045541
#% 1083638
#% 1214679
#% 1250570
#! Learning temporal graph structures from time series data reveals important dependency relationships between current observations and histories. Most previous work focuses on learning and predicting with "static" temporal graphs only. However, in many applications such as mechanical systems and biology systems, the temporal dependencies might change over time. In this paper, we develop a dynamic temporal graphical models based on hidden Markov model regression and lasso-type algorithms. Our method is able to integrate two usually separate tasks, i.e. inferring underlying states and learning temporal graphs, in one unified model. The output temporal graphs provide better understanding about complex systems, i.e. how their dependency graphs evolve over time, and achieve more accurate predictions. We examine our model on two synthetic datasets as well as a real application dataset for monitoring oil-production equipment to capture different stages of the system, and achieve promising results.

#index 1214745
#* Towards combining web classification and web information extraction: a case study
#@ Ping Luo;Fen Lin;Yuhong Xiong;Yong Zhao;Zhongzhi Shi
#t 2009
#c 0
#% 73441
#% 464434
#% 466250
#% 815304
#% 840966
#% 843716
#% 881505
#% 955490
#% 1083626
#! Web content analysis often has two sequential and separate steps: Web Classification to identify the target Web pages, and Web Information Extraction to extract the metadata contained in the target Web pages. This decoupled strategy is highly ineffective since the errors in Web classification will be propagated to Web information extraction and eventually accumulate to a high level. In this paper we study the mutual dependencies between these two steps and propose to combine them by using a model of Conditional Random Fields (CRFs). This model can be used to simultaneously recognize the target Web pages and extract the corresponding metadata. Systematic experiments in our project OfCourse for online course search show that this model significantly improves the F1 value for both of the two steps. We believe that our model can be easily generalized to many Web applications.

#index 1214746
#* Beyond blacklists: learning to detect malicious web sites from suspicious URLs
#@ Justin Ma;Lawrence K. Saul;Stefan Savage;Geoffrey M. Voelker
#t 2009
#c 0
#% 393059
#% 891559
#% 956558
#% 956559
#% 1002545
#% 1014521
#% 1051898
#% 1051905
#% 1072120
#% 1164774
#% 1164781
#% 1211775
#% 1399025
#! Malicious Web sites are a cornerstone of Internet criminal activities. As a result, there has been broad interest in developing systems to prevent the end user from visiting such sites. In this paper, we describe an approach to this problem based on automated URL classification, using statistical methods to discover the tell-tale lexical and host-based properties of malicious Web site URLs. These methods are able to learn highly predictive models by extracting and automatically analyzing tens of thousands of features potentially indicative of suspicious URLs. The resulting classifiers obtain 95-99% accuracy, detecting large numbers of malicious Web sites from their URLs, with only modest false positives.

#index 1214747
#* Clustering event logs using iterative partitioning
#@ Adetokunbo A.O. Makanju;A. Nur Zincir-Heywood;Evangelos E. Milios
#t 2009
#c 0
#% 300120
#% 464986
#% 823418
#% 848846
#% 1009844
#% 1016303
#% 1113416
#! The importance of event logs, as a source of information in systems and network management cannot be overemphasized. With the ever increasing size and complexity of today's event logs, the task of analyzing event logs has become cumbersome to carry out manually. For this reason recent research has focused on the automatic analysis of these log files. In this paper we present IPLoM (Iterative Partitioning Log Mining), a novel algorithm for the mining of clusters from event logs. Through a 3-Step hierarchical partitioning process IPLoM partitions log data into its respective clusters. In its 4th and final stage IPLoM produces cluster descriptions or line formats for each of the clusters produced. Unlike other similar algorithms IPLoM is not based on the Apriori algorithm and it is able to find clusters in data whether or not its instances appear frequently. Evaluations show that IPLoM outperforms the other algorithms statistically significantly, and it is also able to achieve an average F-Measure performance 78% when the closest other algorithm achieves an F-Measure performance of 10%.

#index 1214748
#* SNARE: a link analytic system for graph labeling and risk detection
#@ Mary McGlohon;Stephen Bay;Markus G. Anderle;David M. Steier;Christos Faloutsos
#t 2009
#c 0
#% 290830
#% 420064
#% 580307
#% 748026
#% 754098
#% 769942
#% 823370
#% 842682
#% 846184
#% 915233
#% 956513
#% 989600
#% 989666
#! Classifying nodes in networks is a task with a wide range of applications. It can be particularly useful in anomaly and fraud detection. Many resources are invested in the task of fraud detection due to the high cost of fraud, and being able to automatically detect potential fraud quickly and precisely allows human investigators to work more efficiently. Many data analytic schemes have been put into use; however, schemes that bolster link analysis prove promising. This work builds upon the belief propagation algorithm for use in detecting collusion and other fraud schemes. We propose an algorithm called SNARE (Social Network Analysis for Risk Evaluation). By allowing one to use domain knowledge as well as link knowledge, the method was very successful for pinpointing misstated accounts in our sample of general ledger data, with a significant improvement over the default heuristic in true positive rates, and a lift factor of up to 6.5 (more than twice that of the default heuristic). We also apply SNARE to the task of graph labeling in general on publicly-available datasets. We show that with only some information about the nodes themselves in a network, we get surprisingly high accuracy of labels. Not only is SNARE applicable in a wide variety of domains, but it is also robust to the choice of parameters and highly scalable-linearly with the number of edges in a graph.

#index 1214749
#* Sentiment analysis of blogs by combining lexical knowledge with text classification
#@ Prem Melville;Wojciech Gryc;Richard D. Lawrence
#t 2009
#c 0
#% 73372
#% 170665
#% 248218
#% 464465
#% 715258
#% 769892
#% 769908
#% 815915
#% 854646
#% 855226
#% 855282
#% 879626
#% 907489
#% 938687
#% 939848
#% 939897
#% 983599
#% 1026841
#% 1074125
#% 1176920
#% 1250186
#% 1409618
#! The explosion of user-generated content on the Web has led to new opportunities and significant challenges for companies, that are increasingly concerned about monitoring the discussion around their products. Tracking such discussion on weblogs, provides useful insight on how to improve products or market them more effectively. An important component of such analysis is to characterize the sentiment expressed in blogs about specific brands and products. Sentiment Analysis focuses on this task of automatically identifying whether a piece of text expresses a positive or negative opinion about the subject matter. Most previous work in this area uses prior lexical knowledge in terms of the sentiment-polarity of words. In contrast, some recent approaches treat the task as a text classification problem, where they learn to classify sentiment based only on labeled training data. In this paper, we present a unified framework in which one can use background lexical information in terms of word-class associations, and refine this information for specific domains using any available training examples. Empirical results on diverse domains show that our approach performs better than using background knowledge or training data in isolation, as well as alternative approaches to using lexical knowledge with text classification.

#index 1214750
#* Anonymizing healthcare data: a case study on the blood transfusion service
#@ Noman Mohammed;Benjamin C.M. Fung;Patrick C.K. Hung;Cheuk-kwong Lee
#t 2009
#c 0
#% 136350
#% 300184
#% 443463
#% 576761
#% 577239
#% 800515
#% 824726
#% 881497
#% 881546
#% 893100
#% 937550
#% 951837
#% 975045
#% 1070890
#% 1074831
#% 1083709
#% 1083739
#% 1127361
#% 1176943
#% 1181232
#% 1206581
#% 1292020
#! Sharing healthcare data has become a vital requirement in healthcare system management; however, inappropriate sharing and usage of healthcare data could threaten patients' privacy. In this paper, we study the privacy concerns of the blood transfusion information-sharing system between the Hong Kong Red Cross Blood Transfusion Service (BTS) and public hospitals, and identify the major challenges that make traditional data anonymization methods not applicable. Furthermore, we propose a new privacy model called LKC-privacy, together with an anonymization algorithm, to meet the privacy and information requirements in this BTS case. Experiments on the real-life data demonstrate that our anonymization algorithm can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets.

#index 1214751
#* Towards a universal marketplace over the web: statistical multi-label classification of service provider forms with simulated annealing
#@ Kivanc Ozonat;Donald Young
#t 2009
#c 0
#% 281251
#% 311034
#% 348138
#% 480309
#% 607815
#% 729437
#% 770783
#% 783472
#% 956537
#% 1275209
#! There is a growing number of service providers that a consumer can interact with over the web to learn their service terms. The service terms, such as price and time to completion of the service, depend on the consumer's particular specifications. For instance, a printing services provider would need from its customers specifications such as the size of paper, type of ink, proofing and perforation. In a few sectors, there exist marketplace sites that provide consumers with specifications forms, which the consumer can fill out to learn the service terms of multiple service providers. Unfortunately, there are only a few such marketplace sites, and they cover a few sectors. At HP Labs, we are working towards building a universal marketplace site, i.e., a marketplace site that covers thousands of sectors and hundreds of providers per sector. One issue in this domain is the automated discovery/retrieval of the specifications for each sector. We address it through extracting and analyzing content from the websites of the service providers listed in business directories. The challenge is that each service provider is often listed under multiple service categories in a business directory, making it infeasible to utilize standard supervised learning techniques. We address this challenge through employing a multilabel statistical clustering approach within an expectation-maximization framework. We implement our solution to retrieve specifications for 3000 sectors, representing more than 300,000 service providers. We discuss our results within the context of the services needed to design a marketing campaign for a small business.

#index 1214752
#* Sustainable operation and management of data center chillers using temporal data mining
#@ Debprakash Patnaik;Manish Marwah;Ratnesh Sharma;Naren Ramakrishnan
#t 2009
#c 0
#% 420063
#% 629607
#% 662750
#% 729960
#% 824709
#% 832572
#% 882221
#% 893211
#% 983936
#% 989656
#% 992857
#% 1035721
#% 1047470
#% 1083674
#% 1092317
#% 1117064
#% 1127609
#! Motivation: Data centers are a critical component of modern IT infrastructure but are also among the worst environmental offenders through their increasing energy usage and the resulting large carbon footprints. Efficient management of data centers, including power management, networking, and cooling infrastructure, is hence crucial to sustainability. In the absence of a 'first-principles' approach to manage these complex components and their interactions, data-driven approaches have become attractive and tenable. Results: We present a temporal data mining solution to model and optimize performance of data center chillers, a key component of the cooling infrastructure. It helps bridge raw, numeric, time-series information from sensor streams toward higher level characterizations of chiller behavior, suitable for a data center engineer. To aid in this transduction, temporal data streams are first encoded into a symbolic representation, next run-length encoded segments are mined to form frequent motifs in time series, and finally these metrics are evaluated by their contributions to sustainability. A key innovation in our application is the ability to intersperse "don't care" transitions (e.g., transients) in continuous-valued time series data, an advantage we inherit by the application of frequent episode mining to symbolized representations of numeric time series. Our approach provides both qualitative and quantitative characterizations of the sensor streams to the data center engineer, to aid him in tuning chiller operating characteristics. This system is currently being prototyped for a data center managed by HP and experimental results from this application reveal the promise of our approach.

#index 1214753
#* BGP-lens: patterns and anomalies in internet routing updates
#@ B. Aditya Prakash;Nicholas Valler;David Andersen;Michalis Faloutsos;Christos Faloutsos
#t 2009
#c 0
#% 107408
#% 116390
#% 149237
#% 172949
#% 227857
#% 236753
#% 333941
#% 394984
#% 413548
#% 446418
#% 730109
#% 770889
#% 781961
#% 821916
#% 840577
#% 1015301
#! The Border Gateway Protocol (BGP) is one of the fundamental computer communication protocols. Monitoring and mining BGP update messages can directly reveal the health and stability of Internet routing. Here we make two contributions: firstly we find patterns in BGP updates, like self-similarity, power-law and lognormal marginals; secondly using these patterns, we find anomalies. Specifically, we develop BGP-lens, an automated BGP updates analysis tool, that has three desirable properties: (a) It is effective, able to identify phenomena that would otherwise go unnoticed, such as a peculiar 'clothesline' behavior or prolonged 'spikes' that last as long as 8 hours; (b) It is scalable, using algorithms are all linear on the number of time-ticks; and (c) It is admin-friendly, giving useful leads for phenomenon of interest. We showcase the capabilities of BGP-lens by identifying surprising phenomena verified by syadmins, over a massive trace of BGP updates spanning 2 years, from the publicly available site datapository.net.

#index 1214754
#* Predicting bounce rates in sponsored search advertisements
#@ D. Sculley;Robert G. Malkin;Sugato Basu;Roberto J. Bayardo
#t 2009
#c 0
#% 73441
#% 252011
#% 393059
#% 402289
#% 818221
#% 818265
#% 879565
#% 879567
#% 891559
#% 956546
#% 983894
#% 983905
#% 987262
#% 987263
#% 1019076
#% 1040857
#% 1055694
#% 1055713
#% 1074101
#% 1130909
#% 1130910
#! This paper explores an important and relatively unstudied quality measure of a sponsored search advertisement: bounce rate. The bounce rate of an ad can be informally defined as the fraction of users who click on the ad but almost immediately move on to other tasks. A high bounce rate can lead to poor advertiser return on investment, and suggests search engine users may be having a poor experience following the click. In this paper, we first provide quantitative analysis showing that bounce rate is an effective measure of user satisfaction. We then address the question, can we predict bounce rate by analyzing the features of the advertisement? An affirmative answer would allow advertisers and search engines to predict the effectiveness and quality of advertisements before they are shown. We propose solutions to this problem involving large-scale learning methods that leverage features drawn from ad creatives in addition to their keywords and landing pages.

#index 1214755
#* Mining brain region connectivity for alzheimer's disease study via sparse inverse covariance estimation
#@ Liang Sun;Rinkal Patel;Jun Liu;Kewei Chen;Teresa Wu;Jing Li;Eric Reiman;Jieping Ye
#t 2009
#c 0
#% 397854
#% 771626
#% 790061
#% 803567
#% 875956
#% 970468
#% 1074353
#% 1083738
#% 1125541
#! Effective diagnosis of Alzheimer's disease (AD), the most common type of dementia in elderly patients, is of primary importance in biomedical research. Recent studies have demonstrated that AD is closely related to the structure change of the brain network, i.e., the connectivity among different brain regions. The connectivity patterns will provide useful imaging-based biomarkers to distinguish Normal Controls (NC), patients with Mild Cognitive Impairment (MCI), and patients with AD. In this paper, we investigate the sparse inverse covariance estimation technique for identifying the connectivity among different brain regions. In particular, a novel algorithm based on the block coordinate descent approach is proposed for the direct estimation of the inverse covariance matrix. One appealing feature of the proposed algorithm is that it allows the user feedback (e.g., prior domain knowledge) to be incorporated into the estimation process, while the connectivity patterns can be discovered automatically. We apply the proposed algorithm to a collection of FDG-PET images from 232 NC, MCI, and AD subjects. Our experimental results demonstrate that the proposed algorithm is promising in revealing the brain region connectivity differences among these groups.

#index 1214756
#* Can we learn a template-independent wrapper for news article extraction from a single training site?
#@ Junfeng Wang;Chun Chen;Can Wang;Jian Pei;Jiajun Bu;Ziyu Guan;Wei Vivian Zhang
#t 2009
#c 0
#% 271065
#% 275915
#% 312860
#% 334313
#% 397605
#% 480824
#% 577321
#% 654469
#% 754078
#% 754108
#% 779889
#% 805845
#% 805846
#% 818233
#% 869518
#% 938578
#% 956642
#% 989660
#% 1269910
#! Automatic news extraction from news pages is important in many Web applications such as news aggregation. However, the existing news extraction methods based on template-level wrapper induction have three serious limitations. First, the existing methods cannot correctly extract pages belonging to an unseen template. Second, it is costly to maintain up-to-date wrappers for a large amount of news websites, because any change of a template may invalidate the corresponding wrapper. Last, the existing methods can merely extract unformatted plain texts, and thus are not user friendly. In this paper, we tackle the problem of template-independent Web news extraction in a user-friendly way. We formalize Web news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site. Novel features dedicated to news titles and bodies are developed. Correlations between news titles and news bodies are exploited. Our template-independent wrapper can extract news pages from different sites regardless of templates. Moreover, our approach can extract not only texts, but also images and animates within the news bodies and the extracted news articles are in the same visual style as in the original pages. In our experiments, a wrapper learned from 40 pages from a single news site achieved an accuracy of 98.1% on 3,973 news pages from 12 news sites.

#index 1214757
#* PSkip: estimating relevance ranking quality from web search clickthrough data
#@ Kuansan Wang;Toby Walker;Zijian Zheng
#t 2009
#c 0
#% 157045
#% 169803
#% 194269
#% 262097
#% 262102
#% 262107
#% 262276
#% 310567
#% 320432
#% 340890
#% 411762
#% 577224
#% 590523
#% 731615
#% 766409
#% 805200
#% 823348
#% 850133
#% 879565
#% 879632
#% 946521
#% 956495
#% 987222
#% 1001644
#% 1014390
#% 1035578
#% 1074092
#% 1083643
#% 1130811
#% 1166517
#! In this article, we report our efforts in mining the information encoded as clickthrough data in the server logs to evaluate and monitor the relevance ranking quality of a commercial web search engine. We describe a metric called pSkip that aims to quantify the ranking quality by estimating the probability of users encountering non relevant results that cost them the efforts to read and skip. A search engine with a lower pSkip is regarded as having a better ranking quality. A key design goal of pSkip is to integrate the findings from two sets of user studies that utilize eye-tracking devices to track users' browsing patterns on the search result pages, and that use specially instrumented browsers to actively solicit users' explicit judgments on their search activities. We present the derivation of the maximum likelihood estimation of pSkip and demonstrate its efficacy in describing the user study data. The mathematical properties of pSkip are further analyzed and compared with several objective metrics as well as the cumulated gain method that uses subjective judgments. Experimental data show that pSkip can measure aspects of the search quality that these existing metrics are not designed or fail to address, such as identifying the real search intents expressed in the ambiguous queries. Although effective and superior in many ways, we also report a series of experiments that show pSkip may be influenced by system issues that are not directly related to relevance ranking, suggesting that measurements complementary to pSkip are still needed in order to form a holistic and accurate characterization of the ranking quality.

#index 1214758
#* Named entity mining from click-through data using weakly supervised latent dirichlet allocation
#@ Gu Xu;Shuang-Hong Yang;Hang Li
#t 2009
#c 0
#% 280819
#% 348155
#% 577224
#% 577235
#% 591792
#% 709765
#% 722904
#% 727883
#% 742424
#% 815341
#% 830520
#% 855119
#% 869501
#% 879567
#% 879587
#% 956503
#% 983833
#% 989630
#% 1019130
#% 1055681
#% 1083721
#! This paper addresses Named Entity Mining (NEM), in which we mine knowledge about named entities such as movies, games, and books from a huge amount of data. NEM is potentially useful in many applications including web search, online advertisement, and recommender system. There are three challenges for the task: finding suitable data source, coping with the ambiguities of named entity classes, and incorporating necessary human supervision into the mining process. This paper proposes conducting NEM by using click-through data collected at a web search engine, employing a topic model that generates the click-through data, and learning the topic model by weak supervision from humans. Specifically, it characterizes each named entity by its associated queries and URLs in the click-through data. It uses the topic model to resolve ambiguities of named entity classes by representing the classes as topics. It employs a method, referred to as Weakly Supervised Latent Dirichlet Allocation (WS-LDA), to accurately learn the topic model with partially labeled named entities. Experiments on a large scale click-through data containing over 1.5 billion query-URL pairs show that the proposed approach can conduct very accurate NEM and significantly outperforms the baseline.

#index 1214759
#* Incorporating site-level knowledge for incremental crawling of web forums: a list-wise strategy
#@ Jiang-Ming Yang;Rui Cai;Chunsong Wang;Hua Huang;Lei Zhang;Wei-Ying Ma
#t 2009
#c 0
#% 281251
#% 309746
#% 330604
#% 340924
#% 348137
#% 438251
#% 480309
#% 731406
#% 805879
#% 807302
#% 879601
#% 902460
#% 989661
#% 1055715
#% 1055716
#% 1074108
#% 1074109
#! We study in this paper the problem of incremental crawling of web forums, which is a very fundamental yet challenging step in many web applications. Traditional approaches mainly focus on scheduling the revisiting strategy of each individual page. However, simply assigning different weights for different individual pages is usually inefficient in crawling forum sites because of the different characteristics between forum sites and general websites. Instead of treating each individual page independently, we propose a list-wise strategy by taking into account the site-level knowledge. Such site-level knowledge is mined through reconstructing the linking structure, called sitemap, for a given forum site. With the sitemap, posts from the same thread but distributed on various pages can be concatenated according to their timestamps. After that, for each thread, we employ a regression model to predict the time when the next post arrives. Based on this model, we develop an efficient crawler which is 260% faster than some state-of-the-art methods in terms of fetching new generated content; and meanwhile our crawler also ensure a high coverage ratio. Experimental results show promising performance of Coverage, Bandwidth utilization, and Timeliness of our crawler on 18 various forums.

#index 1214760
#* Intelligent file scoring system for malware detection from the gray list
#@ Yanfang Ye;Tao Li;Qingshan Jiang;Zhixue Han;Li Wan
#t 2009
#c 0
#% 132938
#% 152934
#% 296375
#% 300120
#% 424994
#% 459008
#% 466483
#% 577242
#% 629618
#% 631970
#% 664717
#% 729940
#% 738972
#% 769923
#% 789080
#% 803575
#% 814023
#% 989681
#% 994157
#% 998622
#% 999226
#% 1271973
#% 1860941
#! Currently, the most significant line of defense against malware is anti-virus products which focus on authenticating valid software from a white list, blocking invalid software from a black list, and running any unknown software (i.e., the gray list) in a controlled manner. The gray list, containing unknown software programs which could be either normal or malicious, is usually authenticated or rejected manually by virus analysts. Unfortunately, along with the development of the malware writing techniques, the number of file samples in the gray list that need to be analyzed by virus analysts on a daily basis is constantly increasing. In this paper, we develop an intelligent file scoring system (IFSS for short) for malware detection from the gray list by an ensemble of heterogeneous base-level classifiers derived by different learning methods, using different feature representations on dynamic training sets. To the best of our knowledge, this is the first work of applying such ensemble methods for malware detection. IFSS makes it practical for virus analysts to identify malware samples from the huge gray list and improves the detection ability of anti-virus software. It has already been incorporated into the scanning tool of Kingsoft's Anti-Virus software. The case studies on large and real daily collection of the gray list illustrate that the detection ability and efficiency of our IFSS system outperforms other popular scanning tools such as NOD32 and Kaspersky.

#index 1214761
#* OLAP on search logs: an infrastructure supporting data-driven applications in search engines
#@ Bin Zhou;Daxin Jiang;Jian Pei;Hang Li
#t 2009
#c 0
#% 235941
#% 348155
#% 459006
#% 577224
#% 591792
#% 818226
#% 838531
#% 869501
#% 869536
#% 879565
#% 879567
#% 939629
#% 956546
#% 956552
#% 960303
#% 963669
#% 987203
#% 987212
#% 1055677
#% 1063518
#% 1074092
#% 1083721
#! Search logs, which contain rich and up-to-date information about users' needs and preferences, have become a critical data source for search engines. Recently, more and more data-driven applications are being developed in search engines based on search logs, such as query suggestion, keyword bidding, and dissatisfactory query analysis. In this paper, by observing that many data-driven applications in search engines highly rely on online mining of search logs, we develop an OLAP system on search logs which serves as an infrastructure supporting various data-driven applications. An empirical study using real data of over two billion query sessions demonstrates the usefulness and feasibility of our design.

#index 1219121
#* Proceedings of the KDD-09 Workshop on Statistical and Relational Learning in Bioinformatics
#@ Christophe Costa Florencio;Fabrizio Costa Jan Ramon;Joost Kok
#t 2009
#c 0
#! The KDD-09 Workshop on Statistical and Relational Learning in Bioinformatics (StReBio) will be held at the University of Antwerp, Paris, France, on Sunday June, 28th, 2009. This second edition of the workshop is intended to bring together researchers from both the fields of statistical and relational learning and the field of bioinformatics. Hopefully this will contribute to communication between these communities and will stimulate the development of new relational techniques for application in the biological domain. Bioinformatics is a domain where information is naturally represented in terms of relations between heterogeneous objects. Modern experimentation and data acquisition techniques make the study of complex interactions in biological systems possible, yielding huge amounts of data. This raises challenges for the machine learning and data mining communities, where interest in relational and statistical learning has been growing in the last few years. Apart from the amount of (relational) data, the information is often incomplete, and measurements may be noisy, creating additional challenges. We have tried to choose a workshop format which promotes as much as possible the exchange of ideas. Aspects which can help to achieve our goal are the problem statement presentations, the time allocated in the program for questions and discussion, the informal proceedings and the workshop website (http://www.cs.kuleuven.be/~dtai/events/StReBio09/) where the abstracts, papers and some presentations will be available. We selected eight papers for full presentation. All these contributions, in varying degrees, fit the format of the workshop: they apply statistical or relational techniques to problems from the biological domain. Furthermore, we accepted one problem statement to be presented at the workshop. The idea of the problem statement track is to stimulate the interaction between biologists and computer scientists by presenting biological problems in a more mathematical form, making the structure and the source of uncertainty of the data more explicit. We hope that the problem statements will lead to lively discussions and the exchange of ideas. Finally, a tentative solution to a problem statement from last year's edition is presented. We feel honoured to have David Balding and as invited speaker, who is an authority in the field of statistical genetics.

#index 1219831
#* Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery: Integrating Automated Analysis with Interactive Exploration
#@ Kai Puolamäki;Heikki Mannila;Alessio Bertone;Silvia Miksch
#t 2009
#c 0

#index 1235695
#* Proceedings of the 2nd Workshop on Data Mining using Matrices and Tensors
#@ Chris Ding;Tao Li
#t 2009
#c 0
#! The 2009 Workshop on Data Mining using Matrices and Tensors (DMMT'09) is the second workshop on this theme held annually with the SIGKDD Conference. Through the workshop, we expect to bring together leading researchers on many topic areas (e.g., computer scientists, computational and applied mathematicians) to assess the state-of-the-art, share ideas and form collaborations. We also wish to attract practitioners who seek novel ideals for applications. In summary, this workshop will strive to emphasize the following aspects: •Presenting recent advances in algorithms and methods using matrix and scientific computing/applied mathematics •Addressing the fundamental challenges in data mining using matrices and tensors •Identifying killer applications and key industry drivers (where theories and applications meet) •Fostering interactions among researchers (from different backgrounds) sharing the same interest to promote cross-fertilization of ideas •Exploring benchmark data for better evaluation of the techniques The field of pattern recognition, data mining and machine learning increasingly adapt methods and algorithms from advanced matrix computations, graph theory and optimization. Prominent examples are spectral clustering, non-negative matrix factorization, Principal component analysis (PCA) and Singular Value Decomposition (SVD) related clustering and dimension reduction, tensor analysis such as 2DSVD and high order SVD, L-1 regularization, etc. Compared to probabilistic and information theoretic approaches, matrix-based methods are fast, easy to understand and implement; they are especially suitable for parallel and distributed-memory computers to solve large scale challenging problems such as searching and extracting patterns from the entire Web. Hence the area of data mining using matrices and tensors is a popular and growing are of research activities. This workshop will present recent advances in algorithms and methods using matrix and scientific computing/applied mathematics for modeling and analyzing massive, high-dimensional, and nonlinear-structured data.

#index 1246492
#* Proceedings of the Third International Workshop on Data Mining and Audience Intelligence for Advertising
#@ Ying Li;Arun C. Surendran;Dou Shen
#t 2009
#c 0

#index 1251757
#* Proceedings of the ACM SIGKDD Workshop on CyberSecurity and Intelligence Informatics
#@ Hsinchun Chen;Marc Dacier;Marie-Francine Moens;Gerhard Paass;Christopher C. Yang
#t 2009
#c 0
#! Computer supported communication and infrastructure are integral parts of modern economy. Their security is of incredible importance to a wide variety of practical domains ranging from Internet service providers to the banking industry and e-commerce, from corporate networks to the intelligence community. The CSI-KDD workshop focuses on novel knowledge discovery methods addressing CyberSecurity and intelligence issues as well as innovative applications demonstrating the effectiveness of data mining in solving real-world security problems. The challenge for novel methods originates from the emergence of new types of contents and protocols, and only an integrated view on all modes promises optimal results. Innovative applications are essential as IT-communication as well as computer-supported technical and social infrastructure have an extremely complex structure and require a comprehensive approach to prevent criminal activities.

#index 1252602
#* Proceedings of the ACM SIGKDD Workshop on Human Computation
#@ Paul Bennett;Raman Chandrasekar;Max Chickering;Panos Ipeirotis;Edith Law;Anton Mityagin;Foster Provost;Luis von Ahn
#t 2009
#c 0
#! Most research in data mining and knowledge discovery relies heavily on the availability of datasets. With the rapid growth of user generated content on the internet, there is now an abundance of sources from which data can be drawn. Compared to the amount of work in the field on techniques for pattern discovery and knowledge extraction, there has been relatively little effort directed at the study of effective methods for collecting and evaluating the quality of data. Human computation is a new research area that studies the process of channeling the vast internet population to perform tasks or provide data towards solving difficult problems that no known efficient computer algorithms can yet solve. There are various genres of human computation applications available today. Games with a purpose (e.g., the ESP Game) specifically target online gamers who, in the process of playing an enjoyable game, generate useful data (e.g., image tags). Crowdsourcing marketplaces (e.g. Amazon Mechanical Turk) are human computation applications that coordinate workers to perform tasks in exchange for monetary rewards. In identity verification tasks, users need to perform some computation in order to access some online content; a recent example of such a human computation application is reCAPTCHA, which leverages millions of users who solve CAPTCHAs every day to correct words in books that optical character recognition (OCR) programs fail to recognize with certainty. While there has been active work in this area, there is no dedicated forum to discuss these research ideas. The Human Computation Workshop (HCOMP 2009), held in conjunction with the 15th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), is the first workshop bringing together academic and industry researchers to discuss existing human computation applications and future directions of this new subject area. An integral part of this workshop is a demo session where participants can showcase their human computation applications. Our call for papers resulted in 33 high-quality submissions from a wide variety of perspectives. All papers were thoroughly reviewed by the program committee and external reviewers. Given the short, half-day duration of the workshop, only one-third of the submissions could be accommodated and appear in the proceedings. The accepted papers have been divided into four sessions: "Games", "Human Computation In Practice", "Game Theory" and "Labeling Cost and Efficiency".

#index 1252620
#* Proceedings of the ACM SIGKDD Workshop on Human Computation
#@ Paul Bennett;Raman Chandrasekar;Max Chickering;Panos Ipeirotis;Edith Law;Anton Mityagin;Foster Provost;Luis von Ahn
#t 2009
#c 0

#index 1254202
#* Proceedings of the Third International Workshop on Knowledge Discovery from Sensor Data
#@ Olufemi A. Omitaomu;Auroop R. Ganguly;Joao Gama;Ranga Raju Vatsavai;Nitesh V. Chawla;Mohamed Medhat Gaber
#t 2009
#c 0
#! Wide-area sensor infrastructures, remote sensors, RFIDs, and wireless sensor networks yield massive volumes of disparate, dynamic, and geographically distributed data. As such sensors are becoming ubiquitous, a set of broad requirements is beginning to emerge across high-priority applications including adaptability to climate change, electric grid monitoring, disaster preparedness and management, national or homeland security, and the management of critical infrastructures. The raw data from sensors need to be efficiently managed and transformed to usable information through data fusion, which in turn must be converted to predictive insights via knowledge discovery, ultimately facilitating automated or humaninduced tactical decisions or strategic policy based on decision sciences and decision support systems.

#index 1261978
#* Proceedings of the 1st ACM SIGKDD Workshop on Knowledge Discovery from Uncertain Data
#@ Jian Pei;Lise Getoor;Ander de Keijzer
#t 2009
#c 0
#! The importance of uncertain data is growing quickly in many essential applications such as environmental surveillance, mobile object tracking and data integration. Recently, storing, collecting, processing, and analyzing uncertain data has attracted increasing attention from both academia and industry. Analyzing and mining uncertain data needs collaboration and joint effort from multiple research communities including reasoning under uncertainty, uncertain databases and mining uncertain data. For example, statistics and probabilistic reasoning can provide support with models for representing uncertainty. The uncertain database community can provide methods for storing and managing uncertain data, while research in mining uncertain data can provide data analysis tasks and methods. It is important to build connections among those communities to tackle the overall problem of analyzing and mining uncertain data. There are many common challenges among the communities. One is to understand the different modeling assumptions made, and how they impact the methods, both in terms of accuracy and efficiency. Different researchers hold different assumptions and this is one of the major obstacles in the research of mining uncertain data. Another is the scalability of proposed management and analysis methods. Finally, to make analysis and mining useful and practical, we need real data sets for testing. Unfortunately, uncertain data sets are often hard to get. The goal of the First ACM SIGKDD Workshop on Knowledge Discovery from Uncertain Data (U'09) is to discuss in depth the challenges, opportunities and techniques on the topic of analyzing and mining uncertain data. The theme of this workshop is to make connections among the research areas of uncertain databases, probabilistic reasoning, and data mining, as well as to build bridges among the aspects of models, data, applications, novel mining tasks and effective solutions. By making connections among different communities, we aim at understanding each other in terms of scientific foundation as well as commonality and differences in research methodology. The workshop program is very stimulating and exciting. We are pleased to feature two invited talks by pioneers in mining uncertain data. Christopher Jermaine will give an invited talk titled "Managing and Mining Uncertain Data: What Might We Do Better?" Matthias Renz will address the topic "Querying and Mining Uncertain Data: Methods, Applications, and Challenges". Moreover, 8 accepted papers in 4 full presentations and 4 concise presentations will cover a bunch of interesting topics and on-going research projects about uncertain data mining.

#index 1358065
#* Proceedings of the 2nd KDD Workshop on Large-Scale Recommender Systems and the Netflix Prize Competition
#@ Alex Tuzhilin;Yehuda Koren
#t 2008
#c 0
#! In October 2006, Netflix released a large dataset and launched the Netflix Prize Competition that attracted over 20,000 participants from 167 countries. As a part of this competition, many interesting data mining recommendation techniques have been developed that, by the very nature of the Netflix dataset, should be scalable to large recommendation problems. This workshop follows on the previously held KDD 2007 workshop with the goal to exchange the ideas and present and discuss novel recommendation methods that specifically focus on the scalability and performance issues pertaining to large-scale datasets for recommender systems and the Netflix competition in particular. The call for papers for the workshop attracted 10 submissions, out of which 5 papers were selected for the presentation at the workshop among which 3 papers were by the authors of leading teams in the Netflix Prize competition. The workshop is opened with the keynote address on "Exploring User Opinions in Recommender Systems" by Bing Liu from the University of Illinois who provides his views on future developments of recommender systems based on opinion mining and sentiment analysis and sets a stage for the subsequent presentations and discussions among the participants.

#index 1366204
#* Proceedings of the 3rd Workshop on Social Network Mining and Analysis
#@ C. Lee Giles;Prasenjit Mitra;Igor Perisic;John Yen;Haizheng Zhang
#t 2009
#c 0

#index 1432570
#* Proceedings of the Tenth International Workshop on Multimedia Data Mining
#@ 
#t 2010
#c 0

#index 1434109
#* Proceedings of the ACM SIGKDD Workshop on Useful Patterns
#@ Bart Goethals;Nikolaj Tatti;Jilles Vreeken
#t 2010
#c 0
#! Pattern mining is an important aspect of data mining, concerned with finding local structure in data. Traditionally, the focus of research in pattern mining has been on completeness and efficiency. That is, trying to find all potentially interesting patterns as fast as possible. This focus, important as it is, has led our attention away from the most important aspect of the exercise: leading to useful results. Let's consider the following example. Say a domain expert wants to extract novel knowledge from some data, and specifically wants to know what patterns are present in the data. To do so, the expert involves a data analyst. The analyst is provided with the data, and runs his favorite pattern mining algorithm. Due to the pattern explosion, the number of discovered patterns the analyst will find will be enormous; the result of the mining exercise often being much larger than the original data. Nevertheless, let us assume our expert patiently considers the result. Although he might stumble upon some interesting patterns, he will mostly encounter very many patterns that convey roughly the same information. Perhaps worse, however, is that he will find that many of the patterns represent information that is already known. All things considered, even when convinced of the potential, in the above case the expert would not be very impressed by the usefulness of pattern mining. Unlike in other fields of data mining, such as clustering, in pattern mining presentation and visualization has not been a priority. However, even when we forget about presentation to a user, patterns are not yet as useful as they could be. While they provide highly detailed descriptions of phenomena in data, it remains difficult to make good use of them in, say, e.g., classification or clustering. While this is mostly due to the huge number of discovered patterns, making the result unwieldy at best, it does pose interesting research questions like 'how to select patterns such that they are useful?'. Techniques that summarize the result exist, but focus primarily on being able to reconstruct the full set, instead of targeting the usability of the summarized set. As such, research into techniques that mine small sets of high-quality patterns is required, where high-quality is directly related their intended use.

#index 1446944
#* Proceedings of the Eighth Workshop on Mining and Learning with Graphs
#@ Ulf Brefeld;Lise Getoor;Sofus A. Macskassy
#t 2010
#c 0
#! There is a great deal of interest in analyzing data that is best represented as a graph. Examples include the WWW, social networks, biological networks, communication networks, and many others. The importance of being able to effectively mine and learn from such data is growing, as more and more structured and semi-structured data is becoming available. Traditionally, a number of subareas have worked with mining and learning from graph structured data, including communities in graph mining, learning from structured data, statistical relational learning, inductive logic programming, and, moving beyond sub-disciplines in computer science, social network analysis, and, more broadly network science. The objective of this workshop is to bring together researchers from a variety of these areas, and discuss commonality and differences in challenges faced, survey some of the different approaches, and provide a forum to present and learn about some of the most cutting edge research in this area. As an outcome, we expect participants to walk away with a better sense of the variety of different tools available for graph mining and learning, and an appreciation for some of the interesting emerging applications for mining and learning from graphs.

#index 1448879
#* Proceedings of the First International Workshop on Novel Data Stream Pattern Mining Techniques
#@ Margaret H. Dunham;Michael Hahsler;Myra Spiliopoulou
#t 2010
#c 0
#! Data stream mining gained in importance over the last years because it is indispensable for many real applications such as prediction and evolution of weather phenomena; security and anomaly detection in networks; evaluating satellite data; and mining health monitoring streams. Stream mining algorithms must take account of the unique properties of stream data: infinite data, temporal ordering, concept drifts and shifts, demand for scalability etc. Learning on streams has followed two threads thus far: mining (classification, clustering, frequent itemset discovery) and probabilistic modeling. In both threads, scholars devise solutions to the above problems. Stream clustering algorithms are more oriented towards scalability and tracing of model changes, while dynamic probabilistic modeling, e.g. dynamic topic modeling, encompasses methods that adapt seamlessly to drifts. At the same time, research on unsupervised stream learning seems to be scattered along the many application areas. Examples of areas that seem to evolve independently are sensor mining, mining on clickstreams and other logs in stream form, topic modeling on document streams, and temporal mining on data that are actually streams. This workshop brings together scholars working in different areas of learning on streams, including sensor data and other forms of accumulating data. Most of the papers in the next pages are on unsupervised learning with clustering methods. Issues addressed include the detection of outliers and anomalies, evolutionary clustering and incremental clustering, learning in subspaces of the complete feature space and learning with exploitation of context, deriving models from text streams and visualizing them.

#index 1451135
#* Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Bharat Rao;Balaji Krishnapuram;Andrew Tomkins;Qiang Yang
#t 2010
#c 0
#! KDD-2010, the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, is being held in Washington, DC, USA, on July 24--28, 2010. KDD is the leading international forum for the exchange of research results and practical experience in the field of knowledge discovery and data mining. As the quantity of data available to organizations and individuals continues to grow rapidly, and the need to extract useful knowledge from them becomes more intense, scientists, government workers and business people turn to the KDD community for solutions. This volume contains a snapshot of a year of developments in this field; we hope you will find it useful and rewarding. The KDD-2010 technical program features four parallel research tracks and an industrial / government track. The program also features keynotes from leading creators and consumers of KDD technology, 12 workshops, 12 tutorials and one panel. The 2010 KDD Cup competition focuses on educational data mining to support improvements in the field of computer aided instruction. Dozens of technical demonstrations and exhibits from vendors and other organizations underscore the conference's dual role as the leading industry and academic forum to discuss the advances in this field of research. The call for papers attracted 578 research papers and 101 industrial and government submissions from around the world. Each paper was independently reviewed by three members of the program committee for originality, significance, technical quality, and clarity of presentation. This year's research track introduced an author-feedback phase in the review process, in which authors were invited to comment on the preliminary reviews that they received. The objective of the feedback phase is to ensure greater transparency and fairness, as the authors' responses are taken into account in a subsequent discussion phase moderated by Senior Program Committee (SPC) members. There was much discussion among the reviewers in the subsequent discussion phase before the final decisions. In the end, the program committee accepted 77 papers for long presentations and 24 papers for short presentations into the research track, representing an aggregated acceptance rate of 17.4%. This year's Industry and Government track emphasized the successful uses of KDD technology, including deployed applications incorporating KDD technologies and discoveries of valid, novel, understandable, and demonstrably useful patterns from large datasets in industry and government, as well as emerging applications and technology, including challenges and issues arising from attempts to deploy KDD technology to solve specific industry or government problems. The industry and government track of the conference accepted 11 papers for long presentations and 9 papers for short presentations into the program, representing an aggregated acceptance rate of 19.8%. We are glad to see that the conference remains strongly competitive and of very high quality. The resulting program was notable for its diversity and vitality. Alongside traditional KDD topics like classification, clustering, frequent sets, and temporal data, we also saw papers in rapidly growing areas like social network and graph mining, privacy, recommendation, ranking, topic modeling and transfer learning. Application areas included the Web, bio-informatics, mobility data, healthcare, marketing, and many others.

#index 1451136
#* Data mining in the online services industry
#@ Qi Lu
#t 2010
#c 0
#! The online services industry is a rapidly growing industry with a worldwide online ad market projected to grow from $48 billion in 2011 to $67 billion in 2013, of which 47% will come from display advertising and 53% from search advertising. Online Services Division (OSD) within Microsoft is a leader in the consumer cloud space today with a strong portfolio of a set of 3 mutually reinforcing businesses: Search, Portal, Advertising. They are supported by a shared foundational asset of Intent & Knowledge Stores and a shared technology platform supporting large scale data and high performance systems. MSN (Portal) and Bing (Search) generate the content, traffic and data, that make for an exciting fertile environment for large scale data mining practice and system development. Our advertisers are thus given more valuable targeting opportunities and better ROI, which in turn, provide better economics, usability data, and allows for a higher quality services for our advertisers and experience for our users. The ability to transform data into meaningful, actionable insight is an important source of competitive advantage for OSD. The data mining initiatives within the division continue to strive for excellence around the following goals: actionable insights through deep data analysis, data mining and data modeling at scale and with speed, increased productivity from deployed large scale data systems and tools, improved product and service development and decision making gained from effective measurement and experimentation, and a mature data culture in product teams that made the above possible. With many technical and data challenges ahead of us, we are committed to utilizing our huge data asset well to understand the need, intent, and behavior of our users for the purpose of serving them better.

#index 1451137
#* Data winnowing
#@ Yoav Freund
#t 2010
#c 0
#! Massive quantities of digital data are being collected in every aspect of modern life. Examples include Personal photos and videos, biological and medical images and recordings from sensor arrays. To transform these massive data streams into useful information we use a sequence of "winnowing" stages. Each step reduces the size of the data by an order of magnitude; extracting the wheat form the chaff. In this talk I will describe this approach in a variety of contexts, ranging from the analysis of genetic pathways in fruit-fly embryos and C-Elegans worms to counting birds and helping elderly people living alone keep in touch with their family and caregivers.

#index 1451138
#* The quantification of advertising: (+ lessons from building businesses based on large scale data mining)
#@ Konrad Feldman
#t 2010
#c 0
#! As electronic communication, media and commerce increasingly permeate every aspect of modern life, real-time personalization of consumer experience through data-mining becomes practical. Effective classification, prediction and change modeling of consumer interests, behaviors and purchasing habits using machine learning and statistical methods drives efficiency, insights and consumer relevance that were never before possible. The internet has brought on a rapid evolution in advertising. Everything about behavior on the internet can be quantified and responses to behavior can occur in real time. This dynamic interaction with the user has created opportunities to better understand the way in which individuals move from awareness of a product to considering a purchase, through to intent and ultimately a sale for the marketer. When a marketer can answer the question 'did those TV ads cause consumers to switch shampoo brands?' they can model behavior change and adjust marketing strategies accordingly. Underpinning this shift in how the world's trillion dollar marketing budget is spent is transactional data on an unprecedented scale, creating new challenges for software that must interpret this stream and make real time decisions tens, even hundreds of thousands of times every second. I will explore advances in modeling media consumption, advertising response and the real-time evaluation of media opportunities through reference to Quantcast, a business launched in September 2006 which today interprets in excess of 10 billion new digital media consumption records every day. We will examine the challenges of applying machine learning to non-search advertising and in doing so explore the creation of business environments - organization, infrastructure, tools, processes (and costs considerations) - in which scientists can quickly develop new petabyte scale algorithmic approaches, migrate them rapidly to real-time production and deliver fully customized experiences for marketers, publishers and consumers alike.

#index 1451139
#* Evaluating online ad campaigns in a pipeline: causal models at scale
#@ David Chan;Rong Ge;Ori Gershony;Tim Hesterberg;Diane Lambert
#t 2010
#c 0
#% 769904
#! Display ads proliferate on the web, but are they effective? Or are they irrelevant in light of all the other advertising that people see? We describe a way to answer these questions, quickly and accurately, without randomized experiments, surveys, focus groups or expert data analysts. Doubly robust estimation protects against the selection bias that is inherent in observational data, and a nonparametric test that is based on irrelevant outcomes provides further defense. Simulations based on realistic scenarios show that the resulting estimates are more robust to selection bias than traditional alternatives, such as regression modeling or propensity scoring. Moreover, computations are fast enough that all processing, from data retrieval through estimation, testing, validation and report generation, proceeds in an automated pipeline, without anyone needing to see the raw data.

#index 1451140
#* Overlapping experiment infrastructure: more, better, faster experimentation
#@ Diane Tang;Ashish Agarwal;Deirdre O'Brien;Mike Meyer
#t 2010
#c 0
#% 577224
#% 956546
#% 989572
#% 1154062
#% 1562244
#! At Google, experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experience. Such changes include not only obvious user-visible changes such as modifications to a user interface, but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection. Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments, how to run experiments that produce better decisions, and how to run them faster. In this paper, we describe Google's overlapping experiment infrastructure that is a key component to solving these problems. In addition, because an experiment infrastructure alone is insufficient, we also discuss the associated tools and educational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper specifically describes the experiment system and experimental processes we have in place at Google, we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications.

#index 1451141
#* Exploitation and exploration in a performance based contextual advertising system
#@ Wei Li;Xuerui Wang;Ruofei Zhang;Ying Cui;Jianchang Mao;Rong Jin
#t 2010
#c 0
#% 227736
#% 384911
#% 722906
#% 740191
#% 835024
#% 871302
#% 1055713
#% 1073927
#% 1318590
#! The dynamic marketplace in online advertising calls for ranking systems that are optimized to consistently promote and capitalize better performing ads. The streaming nature of online data inevitably makes an advertising system choose between maximizing its expected revenue according to its current knowledge in short term (exploitation) and trying to learn more about the unknown to improve its knowledge (exploration), since the latter might increase its revenue in the future. The exploitation and exploration (EE) tradeoff has been extensively studied in the reinforcement learning community, however, not been paid much attention in online advertising until recently. In this paper, we develop two novel EE strategies for online advertising. Specifically, our methods can adaptively balance the two aspects of EE by automatically learning the optimal tradeoff and incorporating confidence metrics of historical performance. Within a deliberately designed offline simulation framework we apply our algorithms to an industry leading performance based contextual advertising system and conduct extensive evaluations with real online event log data. The experimental results and detailed analysis reveal several important findings of EE behaviors in online advertising and demonstrate that our algorithms perform superiorly in terms of ad reach and click-through-rate (CTR).

#index 1451142
#* MineFleet®: an overview of a widely adopted distributed vehicle performance data mining system
#@ Hillol Kargupta;Kakali Sarkar;Michael Gilligan
#t 2010
#c 0
#% 736198
#% 888878
#% 1211988
#% 1671830
#! This paper describes the MineFleet distributed vehicle performance data mining system designed for commercial fleets. MineFleet analyzes high throughput data streams onboard the vehicle, generates the analytics, sends those to the remote server over the wide-area wireless networks and offers them to the fleet managers using stand-alone and web-based user-interface. The paper describes the overall architecture of the system, business needs, and shares experience from successful large-scale commercial deployments. MineFleet is probably one of the first commercially successful distributed data stream mining systems. This patented technology has been adopted, productized, and commercially offered by many large companies in the mobile resource management and GPS fleet tracking industry. This paper offers an overview of the system and offers a detailed analysis of what made it work.

#index 1451143
#* Multiple kernel learning for heterogeneous anomaly detection: algorithm and aviation safety case study
#@ Santanu Das;Bryan L. Matthews;Ashok N. Srivastava;Nikunj C. Oza
#t 2010
#c 0
#% 302406
#% 320220
#% 420077
#% 629607
#% 729912
#% 763697
#% 770846
#% 855602
#% 883981
#% 1202160
#% 1301516
#% 1860548
#! The world-wide aviation system is one of the most complex dynamical systems ever developed and is generating data at an extremely rapid rate. Most modern commercial aircraft record several hundred flight parameters including information from the guidance, navigation, and control systems, the avionics and propulsion systems, and the pilot inputs into the aircraft. These parameters may be continuous measurements or binary or categorical measurements recorded in one second intervals for the duration of the flight. Currently, most approaches to aviation safety are reactive, meaning that they are designed to react to an aviation safety incident or accident. In this paper, we discuss a novel approach based on the theory of multiple kernel learning to detect potential safety anomalies in very large data bases of discrete and continuous data from world-wide operations of commercial fleets. We pose a general anomaly detection problem which includes both discrete and continuous data streams, where we assume that the discrete streams have a causal influence on the continuous streams. We also assume that atypical sequences of events in the discrete streams can lead to off-nominal system performance. We discuss the application domain, novel algorithms, and also discuss results on real-world data sets. Our algorithm uncovers operationally significant events in high dimensional data streams in the aviation industry which are not detectable using state of the art methods.

#index 1451144
#* Discovery of significant emerging trends
#@ Saurabh Goorha;Lyle Ungar
#t 2010
#c 0
#% 350859
#% 578388
#% 769892
#% 843716
#% 854646
#% 855119
#% 875959
#% 881498
#% 939896
#% 1117045
#% 1176853
#% 1214671
#% 1250367
#! We describe a system that monitors social and mainstream media to determine shifts in what people are thinking about a product or company. We process over 100,000 news articles, blog posts, review sites, and tweets a day for mentions of items (e.g., products) of interest, extract phrases that are mentioned near them, and determine which of the phrases are of greatest possible interest to, for example, brand managers. Case studies show a good ability to rapidly pinpoint emerging subjects buried deep in large volumes of data and then highlight those that are rising or falling in significance as they relate to the firms interests. The tool and algorithm improves the signal-to-noise ratio and pinpoints precisely the opportunities and risks that matter most to communications professionals and their organizations.

#index 1451145
#* Data mining to predict and prevent errors in health insurance claims processing
#@ Mohit Kumar;Rayid Ghani;Zhu-Song Mei
#t 2010
#c 0
#% 232102
#% 272527
#% 466263
#% 766436
#% 881477
#% 989676
#% 1272365
#% 1301004
#! Health insurance costs across the world have increased alarmingly in recent years. A major cause of this increase are payment errors made by the insurance companies while processing claims. These errors often result in extra administrative effort to re-process (or rework) the claim which accounts for up to 30% of the administrative staff in a typical health insurer. We describe a system that helps reduce these errors using machine learning techniques by predicting claims that will need to be reworked, generating explanations to help the auditors correct these claims, and experiment with feature selection, concept drift, and active learning to collect feedback from the auditors to improve over time. We describe our framework, problem formulation, evaluation metrics, and experimental results on claims data from a large US health insurer. We show that our system results in an order of magnitude better precision (hit rate) over existing approaches which is accurate enough to potentially result in over $15-25 million in savings for a typical insurer. We also describe interesting research problems in this domain as well as design choices made to make the system easily deployable across health insurance companies.

#index 1451146
#* Optimizing debt collections using constrained reinforcement learning
#@ Naoki Abe;Prem Melville;Cezar Pendus;Chandan K. Reddy;David L. Jensen;Vince P. Thomas;James J. Bennett;Gary F. Anderson;Brent R. Cooley;Melissa Kowalczyk;Mark Domick;Timothy Gardinier
#t 2010
#c 0
#% 280437
#% 342644
#% 363744
#% 384911
#% 464280
#% 577237
#% 746470
#% 771917
#% 990220
#% 1272286
#% 1289281
#! The problem of optimally managing the collections process by taxation authorities is one of prime importance, not only for the revenue it brings but also as a means to administer a fair taxing system. The analogous problem of debt collections management in the private sector, such as banks and credit card companies, is also increasingly gaining attention. With the recent successes in the applications of data analytics and optimization to various business areas, the question arises to what extent such collections processes can be improved by use of leading edge data modeling and optimization techniques. In this paper, we propose and develop a novel approach to this problem based on the framework of constrained Markov Decision Process (MDP), and report on our experience in an actual deployment of a tax collections optimization system at New York State Department of Taxation and Finance (NYS DTF).

#index 1451147
#* Detecting abnormal coupled sequences and sequence changes in group-based manipulative trading behaviors
#@ Longbing Cao;Yuming Ou;Philip S. Yu;Gang Wei
#t 2010
#c 0
#% 246836
#% 313955
#% 329537
#% 459006
#% 464996
#% 577256
#% 895512
#% 989638
#% 1016144
#% 1114499
#% 1176861
#% 1176872
#! In capital market surveillance, an emerging trend is that a group of hidden manipulators collaborate with each other to manipulate three trading sequences: buy-orders, sell-orders and trades, through carefully arranging their prices, volumes and time, in order to mislead other investors, affect the instrument movement, and thus maximize personal benefits. If the focus is on only one of the above three sequences in attempting to analyze such hidden group based behavior, or if they are merged into one sequence as per an investor, the coupling relationships among them indicated through trading actions and their prices/volumes/times would be missing, and the resulting findings would have a high probability of mismatching the genuine fact in business. Therefore, typical sequence analysis approaches, which mainly identify patterns on a single sequence, cannot be used here. This paper addresses a novel topic, namely coupled behavior analysis in hidden groups. In particular, we propose a coupled Hidden Markov Models (HMM)-based approach to detect abnormal group-based trading behaviors. The resulting models cater for (1) multiple sequences from a group of people, (2) interactions among them, (3) sequence item properties, and (4) significant change among coupled sequences. We demonstrate our approach in detecting abnormal manipulative trading behaviors on orderbook-level stock data. The results are evaluated against alerts generated by the exchange's surveillance system from both technical and computational perspectives. It shows that the proposed coupled and adaptive HMMs outperform a standard HMM only modeling any single sequence, or the HMM combining multiple single sequences, without considering the coupling relationship. Further work on coupled behavior analysis, including coupled sequence/event analysis, hidden group analysis and behavior dynamics are very critical.

#index 1451148
#* Automatic malware categorization using cluster ensemble
#@ Yanfang Ye;Tao Li;Yong Chen;Qingshan Jiang
#t 2010
#c 0
#% 344447
#% 387427
#% 464888
#% 579655
#% 664717
#% 722902
#% 765518
#% 770836
#% 800530
#% 837616
#% 982763
#% 983869
#% 989681
#% 1085668
#% 1105703
#% 1117063
#% 1305447
#% 1305494
#% 1403007
#% 1861495
#% 1959964
#! In this paper, resting on the analysis of instruction frequency and function-based instruction sequences, we develop an Automatic Malware Categorization System (AMCS) for automatically grouping malware samples into families that share some common characteristics using a cluster ensemble by aggregating the clustering solutions generated by different base clustering algorithms. We propose a principled cluster ensemble framework for combining individual clustering solutions based on the consensus partition. The domain knowledge in the form of sample-level constraints can be naturally incorporated in the ensemble framework. In addition, to account for the characteristics of feature representations, we propose a hybrid hierarchical clustering algorithm which combines the merits of hierarchical clustering and k-medoids algorithms and a weighted subspace K-medoids algorithm to generate base clusterings. The categorization results of our AMCS system can be used to generate signatures for malware families that are useful for malware detection. The case studies on large and real daily malware collection from Kingsoft Anti-Virus Lab demonstrate the effectiveness and efficiency of our AMCS system.

#index 1451149
#* Beyond heuristics: learning to classify vulnerabilities and predict exploits
#@ Mehran Bozorgi;Lawrence K. Saul;Stefan Savage;Geoffrey M. Voelker
#t 2010
#c 0
#% 438340
#% 449102
#% 458369
#% 888994
#% 918176
#% 963764
#% 1117691
#! The security demands on modern system administration are enormous and getting worse. Chief among these demands, administrators must monitor the continual ongoing disclosure of software vulnerabilities that have the potential to compromise their systems in some way. Such vulnerabilities include buffer overflow errors, improperly validated inputs, and other unanticipated attack modalities. In 2008, over 7,400 new vulnerabilities were disclosed--well over 100 per week. While no enterprise is affected by all of these disclosures, administrators commonly face many outstanding vulnerabilities across the software systems they manage. Vulnerabilities can be addressed by patches, reconfigurations, and other workarounds; however, these actions may incur down-time or unforeseen side-effects. Thus, a key question for systems administrators is which vulnerabilities to prioritize. From publicly available databases that document past vulnerabilities, we show how to train classifiers that predict whether and how soon a vulnerability is likely to be exploited. As input, our classifiers operate on high dimensional feature vectors that we extract from the text fields, time stamps, cross references, and other entries in existing vulnerability disclosure reports. Compared to current industry-standard heuristics based on expert knowledge and static formulas, our classifiers predict much more accurately whether and how soon individual vulnerabilities are likely to be exploited.

#index 1451150
#* Diagnosing memory leaks using graph mining on heap dumps
#@ Evan K. Maxwell;Godmar Back;Naren Ramakrishnan
#t 2010
#c 0
#% 251063
#% 300120
#% 318178
#% 466644
#% 538464
#% 581053
#% 629708
#% 729938
#% 778264
#% 823198
#% 896760
#% 912481
#% 956459
#% 963195
#% 963231
#% 1001163
#% 1017245
#% 1044519
#% 1056161
#% 1063502
#% 1117041
#% 1124083
#% 1131137
#% 1206650
#% 1227871
#% 1260276
#% 1268739
#% 1664490
#% 1732825
#! Memory leaks are caused by software programs that prevent the reclamation of memory that is no longer in use. They can cause significant slowdowns, exhaustion of available storage space and, eventually, application crashes. Detecting memory leaks is challenging because real-world applications are built on multiple layers of software frameworks, making it difficult for a developer to know whether observed references to objects are legitimate or the cause of a leak. We present a graph mining solution to this problem wherein we analyze heap dumps to automatically identify subgraphs which could represent potential memory leak sources. Although heap dumps are commonly analyzed in existing heap profiling tools, our work is the first to apply a graph grammar mining solution to this problem. Unlike classical graph mining work, we show that it suffices to mine the dominator tree of the heap dump, which is significantly smaller than the underlying graph. Our approach identifies not just leaking candidates and their structure, but also provides aggregate information about the access path to the leaks. We demonstrate several synthetic as well as real-world examples of heap dumps for which our approach provides more insight into the problem than state-of-the-art tools such as Eclipse's MAT.

#index 1451151
#* Using data mining techniques to address critical information exchange needs in disaster affected public-private networks
#@ Li Zheng;Chao Shen;Liang Tang;Tao Li;Steve Luis;Shu-Ching Chen;Vagelis Hristidis
#t 2010
#c 0
#% 202002
#% 406493
#% 464434
#% 502122
#% 814023
#% 816181
#% 818916
#% 835018
#% 1055944
#% 1227682
#% 1860941
#! Crisis Management and Disaster Recovery have gained immense importance in the wake of recent man and nature inflicted calamities. A critical problem in a crisis situation is how to efficiently discover, collect, organize, search and disseminate real-time disaster information. In this paper, we address several key problems which inhibit better information sharing and collaboration between both private and public sector participants for disaster management and recovery. We design and implement a web based prototype implementation of a Business Continuity Information Network (BCIN) system utilizing the latest advances in data mining technologies to create a user-friendly, Internet-based, information-rich service and acting as a vital part of a company's business continuity process. Specifically, information extraction is used to integrate the input data from different sources; the content recommendation engine and the report summarization module provide users personalized and brief views of the disaster information; the community generation module develops spatial clustering techniques to help users build dynamic community in disasters. Currently, BCIN has been exercised at Miami-Dade County Emergency Management.

#index 1451152
#* Tropical cyclone event sequence similarity search via dimensionality reduction and metric learning
#@ Shen-Shyang Ho;Wenqing Tang;W. Timothy Liu
#t 2010
#c 0
#% 269226
#% 466890
#% 659971
#% 801266
#% 810049
#% 927131
#% 1016195
#% 1127609
#% 1164194
#% 1211774
#% 1286845
#% 1305484
#% 1486234
#! The Earth Observing System Data and Information System (EOSDIS) is a comprehensive data and information system which archives, manages, and distributes Earth science data from the EOS spacecrafts. One non-existent capability in the EOSDIS is the retrieval of satellite sensor data based on weather events (such as tropical cyclones) similarity query output. In this paper, we propose a framework to solve the similarity search problem given user-defined instance-level constraints for tropical cyclone events, represented by arbitrary length multidimensional spatio-temporal data sequences. A critical component for such a problem is the similarity/metric function to compare the data sequences. We describe a novel Longest Common Subsequence (LCSS) parameter learning approach driven by nonlinear dimensionality reduction and distance metric learning. Intuitively, arbitrary length multidimensional data sequences are projected into a fixed dimensional manifold for LCSS parameter learning. Similarity search is achieved through consensus among the (similar) instance-level constraints based on ranking orders computed using the LCSS-based similarity measure. Experimental results using a combination of synthetic and real tropical cyclone event data sequences are presented to demonstrate the feasibility of our parameter learning approach and its robustness to variability in the instance constraints. We, then, use a similarity query example on real tropical cyclone event data sequences from 2000 to 2008 to discuss (i) a problem of scientific interest, and (ii) challenges and issues related to the weather event similarity search problem.

#index 1451153
#* Malstone: towards a benchmark for analytics on large data clouds
#@ Collin Bennett;Robert L. Grossman;David Locke;Jonathan Seidman;Steve Vejcik
#t 2010
#c 0
#% 723279
#% 963669
#% 978404
#% 1020403
#% 1023420
#! Developing data mining algorithms that are suitable for cloud computing platforms is currently an active area of research, as is developing cloud computing platforms appropriate for data mining. Currently, the most common benchmark for cloud computing is the Terasort (and related) benchmarks. Although the Terasort Benchmark is quite useful, it was not designed for data mining per se. In this paper, we introduce a benchmark called MalStone that is specifically designed to measure the performance of cloud computing middleware that supports the type of data intensive computing common when building data mining models. We also introduce MalGen, which is a utility for generating data on clouds that can be used with MalStone.

#index 1451154
#* TIARA: a visual exploratory text analytic system
#@ Furu Wei;Shixia Liu;Yangqiu Song;Shimei Pan;Michelle X. Zhou;Weihong Qian;Lei Shi;Li Tan;Qiang Zhang
#t 2010
#c 0
#% 214711
#% 296738
#% 406493
#% 434614
#% 722904
#% 823344
#% 879587
#% 881498
#% 910807
#% 1065169
#% 1083664
#% 1147594
#% 1166520
#% 1183211
#% 1202162
#% 1214671
#% 1227579
#% 1286734
#% 1286772
#% 1286773
#% 1292520
#% 1292696
#% 1451248
#% 1939236
#! In this paper, we present a novel exploratory visual analytic system called TIARA (Text Insight via Automated Responsive Analytics), which combines text analytics and interactive visualization to help users explore and analyze large collections of text. Given a collection of documents, TIARA first uses topic analysis techniques to summarize the documents into a set of topics, each of which is represented by a set of keywords. In addition to extracting topics, TIARA derives time-sensitive keywords to depict the content evolution of each topic over time. To help users understand the topic-based summarization results, TIARA employs several interactive text visualization techniques to explain the summarization results and seamlessly link such results to the original text. We have applied TIARA to several real-world applications, including email summarization and patient record analysis. To measure the effectiveness of TIARA, we have conducted several experiments. Our experimental results and initial user feedback suggest that TIARA is effective in aiding users in their exploratory text analytic tasks.

#index 1451155
#* Metric forensics: a multi-level approach for mining volatile graphs
#@ Keith Henderson;Tina Eliassi-Rad;Christos Faloutsos;Leman Akoglu;Lei Li;Koji Maruhashi;B. Aditya Prakash;Hanghang Tong
#t 2010
#c 0
#% 248812
#% 283833
#% 300136
#% 309749
#% 438553
#% 481620
#% 577220
#% 659965
#% 729983
#% 730089
#% 769883
#% 769920
#% 799747
#% 821933
#% 824710
#% 881460
#% 881493
#% 893548
#% 989572
#% 989586
#% 1016175
#% 1047785
#% 1202160
#% 1206639
#% 1214740
#% 1214753
#% 1710593
#! Advances in data collection and storage capacity have made it increasingly possible to collect highly volatile graph data for analysis. Existing graph analysis techniques are not appropriate for such data, especially in cases where streaming or near-real-time results are required. An example that has drawn significant research interest is the cyber-security domain, where internet communication traces are collected and real-time discovery of events, behaviors, patterns, and anomalies is desired. We propose MetricForensics, a scalable framework for analysis of volatile graphs. MetricForensics combines a multi-level "drill down" approach, a collection of user-selected graph metrics, and a collection of analysis techniques. At each successive level, more sophisticated metrics are computed and the graph is viewed at finer temporal resolutions. In this way, MetricForensics scales to highly volatile graphs by only allocating resources for computationally expensive analysis when an interesting event is discovered at a coarser resolution first. We test MetricForensics on three real-world graphs: an enterprise IP trace, a trace of legitimate and malicious network traffic from a research institution, and the MIT Reality Mining proximity sensor data. Our largest graph has 3M vertices and 32M edges, spanning 4.5 days. The results demonstrate the scalability and capability of MetricForensics in analyzing volatile graphs; and highlight four novel phenomena in such graphs: elbows, broken correlations, prolonged spikes, and lightweight stars.

#index 1451156
#* Active learning for biomedical citation screening
#@ Byron C. Wallace;Kevin Small;Carla E. Brodley;Thomas A. Trikalinos
#t 2010
#c 0
#% 169717
#% 190581
#% 194284
#% 204531
#% 236729
#% 252011
#% 458379
#% 466887
#% 565531
#% 763705
#% 844323
#% 907554
#% 961194
#% 987202
#% 1074125
#% 1083692
#% 1130870
#% 1211809
#% 1220998
#% 1272126
#% 1338536
#! Active learning (AL) is an increasingly popular strategy for mitigating the amount of labeled data required to train classifiers, thereby reducing annotator effort. We describe a real-world, deployed application of AL to the problem of biomedical citation screening for systematic reviews at the Tufts Medical Center's Evidence-based Practice Center. We propose a novel active learning strategy that exploits a priori domain knowledge provided by the expert (specifically, labeled features)and extend this model via a Linear Programming algorithm for situations where the expert can provide ranked labeled features. Our methods outperform existing AL strategies on three real-world systematic review datasets. We argue that evaluation must be specific to the scenario under consideration. To this end, we propose a new evaluation framework for finite-pool scenarios, wherein the primary aim is to label a fixed set of examples rather than to simply induce a good predictive model. We use a method from medical decision theory for eliciting the relative costs of false positives and false negatives from the domain expert, constructing a utility measure of classification performance that integrates the expert preferences. Our findings suggest that the expert can, and should, provide more information than instance labels alone. In addition to achieving strong empirical results on the citation screening problem, this work outlines many important steps for moving away from simulated active learning and toward deploying AL for real-world applications.

#index 1451157
#* An integrated machine learning approach to stroke prediction
#@ Aditya Khosla;Yu Cao;Cliff Chiung-Yu Lin;Hsu-Kuang Chiu;Junling Hu;Honglak Lee
#t 2010
#c 0
#% 464444
#% 722929
#% 770857
#% 773753
#% 840882
#% 929722
#% 1100067
#! Stroke is the third leading cause of death and the principal cause of serious long-term disability in the United States. Accurate prediction of stroke is highly valuable for early intervention and treatment. In this study, we compare the Cox proportional hazards model with a machine learning approach for stroke prediction on the Cardiovascular Health Study (CHS) dataset. Specifically, we consider the common problems of data imputation, feature selection, and prediction in medical datasets. We propose a novel automatic feature selection algorithm that selects robust features based on our proposed heuristic: conservative mean. Combined with Support Vector Machines (SVMs), our proposed feature selection algorithm achieves a greater area under the ROC curve (AUC) as compared to the Cox proportional hazards model and L1 regularized Cox feature selection algorithm. Furthermore, we present a margin-based censored regression algorithm that combines the concept of margin-based classifiers with censored regression to achieve a better concordance index than the Cox model. Overall, our approach outperforms the current state-of-the-art in both metrics of AUC and concordance index. In addition, our work has also identified potential risk factors that have not been discovered by traditional approaches. Our method can be applied to clinical prediction of other diseases, where missing data are common and risk factors are not well understood.

#index 1451158
#* Medical coding classification by leveraging inter-code relationships
#@ Yan Yan;Glenn Fung;Jennifer G. Dy;Romer Rosales
#t 2010
#c 0
#% 311034
#% 342621
#% 397142
#% 559222
#% 722929
#% 722943
#% 889101
#% 1100067
#! Medical coding or classification is the process of transforming information contained in patient medical records into standard predefined medical codes. There are several worldwide accepted medical coding conventions associated with diagnoses and medical procedures; however, in the United States the Ninth Revision of ICD(ICD-9) provides the standard for coding clinical records. Accurate medical coding is important since it is used by hospitals for insurance billing purposes. Since after discharge a patient can be assigned or classified to several ICD-9 codes, the coding problem can be seen as a multi-label classification problem. In this paper, we introduce a multi-label large-margin classifier that automatically learns the underlying inter-code structure and allows the controlled incorporation of prior knowledge about medical code relationships. In addition to refining and learning the code relationships, our classifier can also utilize this shared information to improve its performance. Experiments on a publicly available dataset containing clinical free text and their associated medical codes showed that our proposed multi-label classifier outperforms related multi-label models in this problem.

#index 1451159
#* Mining advisor-advisee relationships from research publication networks
#@ Chi Wang;Jiawei Han;Yuntao Jia;Jie Tang;Duo Zhang;Yintao Yu;Jingyi Guo
#t 2010
#c 0
#% 261205
#% 268079
#% 387427
#% 727824
#% 801324
#% 891559
#% 1000502
#% 1176886
#% 1214701
#% 1214703
#% 1269756
#% 1465175
#% 1810385
#! Information network contains abundant knowledge about relationships among people or entities. Unfortunately, such kind of knowledge is often hidden in a network where different kinds of relationships are not explicitly categorized. For example, in a research publication network, the advisor-advisee relationships among researchers are hidden in the coauthor network. Discovery of those relationships can benefit many interesting applications such as expert finding and research community analysis. In this paper, we take a computer science bibliographic network as an example, to analyze the roles of authors and to discover the likely advisor-advisee relationships. In particular, we propose a time-constrained probabilistic factor graph model (TPFG), which takes a research publication network as input and models the advisor-advisee relationship mining problem using a jointly likelihood objective function. We further design an efficient learning algorithm to optimize the objective function. Based on that our model suggests and ranks probable advisors for every author. Experimental results show that the proposed approach infer advisor-advisee relationships efficiently and achieves a state-of-the-art accuracy (80-90%). We also apply the discovered advisor-advisee relationships to bole search, a specific expert finding task and empirical study shows that the search performance can be effectively improved (+4.09% by NDCG@5).

#index 1451160
#* Estimating rates of rare events with multiple hierarchies through scalable log-linear models
#@ Deepak Agarwal;Rahul Agrawal;Rajiv Khanna;Nagaraj Kota
#t 2010
#c 0
#% 342597
#% 956546
#% 983835
#% 989572
#% 1039685
#% 1190066
#% 1211829
#% 1214623
#% 1278124
#! We consider the problem of estimating rates of rare events for high dimensional, multivariate categorical data where several dimensions are hierarchical. Such problems are routine in several data mining applications including computational advertising, our main focus in this paper. We propose LMMH, a novel log-linear modeling method that scales to massive data applications with billions of training records and several million potential predictors in a map-reduce framework. Our method exploits correlations in aggregates observed at multiple resolutions when working with multiple hierarchies; stable estimates at coarser resolution provide informative prior information to improve estimates at finer resolutions. Other than prediction accuracy and scalability, our method has an inbuilt variable screening procedure based on a "spike and slab prior" that provides parsimony by removing non-informative predictors without hurting predictive accuracy. We perform large scale experiments on data from real computational advertising applications and illustrate our approach on datasets with several billion records and hundreds of millions of predictors. Extensive comparisons with other benchmark methods show significant improvements in prediction accuracy.

#index 1451161
#* User browsing models: relevance versus examination
#@ Ramakrishnan Srikant;Sugato Basu;Ni Wang;Daryl Pregibon
#t 2010
#c 0
#% 577224
#% 818221
#% 823348
#% 879567
#% 954300
#% 956546
#% 1035578
#% 1074092
#% 1166517
#% 1190055
#% 1190056
#% 1214675
#% 1214754
#% 1355034
#% 1355048
#% 1451140
#! There has been considerable work on user browsing models for search engine results, both organic and sponsored. The click-through rate (CTR) of a result is the product of the probability of examination (will the user look at the result) times the perceived relevance of the result (probability of a click given examination). Past papers have assumed that when the CTR of a result varies based on the pattern of clicks in prior positions, this variation is solely due to changes in the probability of examination. We show that, for sponsored search results, a substantial portion of the change in CTR when conditioned on prior clicks is in fact due to a change in the relevance of results for that query instance, not just due to a change in the probability of examination. We then propose three new user browsing models, which attribute CTR changes solely to changes in relevance, solely to changes in examination (with an enhanced model of user behavior), or to both changes in relevance and examination. The model that attributes all the CTR change to relevance yields substantially better predictors of CTR than models that attribute all the change to examination, and does only slightly worse than the model that attributes CTR change to both relevance and examination. For predicting relevance, the model that attributes all the CTR change to relevance again does better than the model that attributes the change to examination. Surprisingly, we also find that one model might do better than another in predicting CTR, but worse in predicting relevance. Thus it is essential to evaluate user browsing models with respect to accuracy in predicting relevance, not just CTR.

#index 1451162
#* Suggesting friends using the implicit social graph
#@ Maayan Roth;Assaf Ben-David;David Deutscher;Guy Flysher;Ilan Horn;Ari Leichtberg;Naty Leiser;Yossi Matias;Ron Merom
#t 2010
#c 0
#% 915232
#% 1055741
#% 1055763
#% 1183091
#% 1190126
#% 1194122
#% 1214718
#% 1282012
#% 1399963
#% 1415735
#! Although users of online communication tools rarely categorize their contacts into groups such as "family", "co-workers", or "jogging buddies", they nonetheless implicitly cluster contacts, by virtue of their interactions with them, forming implicit groups. In this paper, we describe the implicit social graph which is formed by users' interactions with contacts and groups of contacts, and which is distinct from explicit social graphs in which users explicitly add other individuals as their "friends". We introduce an interaction-based metric for estimating a user's affinity to his contacts and groups. We then describe a novel friend suggestion algorithm that uses a user's implicit social graph to generate a friend group, given a small seed set of contacts which the user has already labeled as friends. We show experimental results that demonstrate the importance of both implicit group relationships and interaction-based affinity ranking in suggesting friends. Finally, we discuss two applications of the Friend Suggest algorithm that have been released as Gmail Labs features.

#index 1451163
#* New perspectives and methods in link prediction
#@ Ryan N. Lichtenwalter;Jake T. Lussier;Nitesh V. Chawla
#t 2010
#c 0
#% 136350
#% 209021
#% 220708
#% 331909
#% 400847
#% 853536
#% 926881
#% 955712
#% 1117026
#% 1271973
#! This paper examines important factors for link prediction in networks and provides a general, high-performance framework for the prediction task. Link prediction in sparse networks presents a significant challenge due to the inherent disproportion of links that can form to links that do form. Previous research has typically approached this as an unsupervised problem. While this is not the first work to explore supervised learning, many factors significant in influencing and guiding classification remain unexplored. In this paper, we consider these factors by first motivating the use of a supervised framework through a careful investigation of issues such as network observational period, generality of existing methods, variance reduction, topological causes and degrees of imbalance, and sampling approaches. We also present an effective flow-based predicting algorithm, offer formal bounds on imbalance in sparse network link prediction, and employ an evaluation method appropriate for the observed imbalance. Our careful consideration of the above issues ultimately leads to a completely general framework that outperforms unsupervised link prediction methods by more than 30% AUC.

#index 1451164
#* UP-Growth: an efficient algorithm for high utility itemset mining
#@ Vincent S. Tseng;Cheng-Wei Wu;Bai-En Shie;Philip S. Yu
#t 2010
#c 0
#% 300120
#% 481290
#% 727894
#% 829993
#% 1019450
#% 1327654
#% 1401374
#% 1411078
#% 1914469
#! Mining high utility itemsets from a transactional database refers to the discovery of itemsets with high utility like profits. Although a number of relevant approaches have been proposed in recent years, they incur the problem of producing a large number of candidate itemsets for high utility itemsets. Such a large number of candidate itemsets degrades the mining performance in terms of execution time and space requirement. The situation may become worse when the database contains lots of long transactions or long high utility itemsets. In this paper, we propose an efficient algorithm, namely UP-Growth (Utility Pattern Growth), for mining high utility itemsets with a set of techniques for pruning candidate itemsets. The information of high utility itemsets is maintained in a special data structure named UP-Tree (Utility Pattern Tree) such that the candidate itemsets can be generated efficiently with only two scans of the database. The performance of UP-Growth was evaluated in comparison with the state-of-the-art algorithms on different types of datasets. The experimental results show that UP-Growth not only reduces the number of candidates effectively but also outperforms other algorithms substantially in terms of execution time, especially when the database contains lots of long transactions.

#index 1451165
#* Frequent regular itemset mining
#@ Salvatore Ruggieri
#t 2010
#c 0
#% 338594
#% 431033
#% 464873
#% 733265
#% 772329
#% 794935
#% 796210
#% 867872
#% 878942
#% 948087
#% 1083506
#% 1250929
#% 1663672
#% 1710149
#% 1716938
#! Concise representations of frequent itemsets sacrifice readability and direct interpretability by a data analyst of the concise patterns extracted. In this paper, we introduce an extension of itemsets, called regular, with an immediate semantics and interpretability, and a conciseness comparable to closed itemsets. Regular itemsets allow for specifying that an item may or may not be present; that any subset of an itemset may be present; and that any non-empty subset of an itemset may be present. We devise a procedure, called RegularMine, for mining a set of regular itemsets that is a concise representation of frequent itemsets. The procedure computes a covering, in terms of regular itemsets, of the frequent itemsets in the class of equivalence of a closed one. We report experimental results on several standard dense and sparse datasets that validate the proposed approach.

#index 1451166
#* Mining uncertain data with probabilistic guarantees
#@ Liwen Sun;Reynold Cheng;David W. Cheung;Jiefeng Cheng
#t 2010
#c 0
#% 190611
#% 246002
#% 248791
#% 252608
#% 265420
#% 654487
#% 823402
#% 832571
#% 835018
#% 841959
#% 850727
#% 873104
#% 960257
#% 1016178
#% 1016201
#% 1063531
#% 1147652
#% 1189215
#% 1214624
#% 1214633
#% 1217251
#% 1318641
#% 1393138
#% 1408839
#! Data uncertainty is inherent in applications such as sensor monitoring systems, location-based services, and biological databases. To manage this vast amount of imprecise information, probabilistic databases have been recently developed. In this paper, we study the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics. This is technically challenging, since a probabilistic database can have an exponential number of possible worlds. We propose two effcient algorithms, which discover frequent patterns in bottom-up and top-down manners. Both algorithms can be easily extended to discover maximal frequent patterns. We also explain how to use these patterns to generate association rules. Extensive experiments, using real and synthetic datasets, were conducted to validate the performance of our methods.

#index 1451167
#* Mining top-k frequent items in a data stream with flexible sliding windows
#@ Hoang Thanh Lam;Toon Calders
#t 2010
#c 0
#% 124888
#% 211059
#% 548479
#% 723384
#% 874906
#% 1084210
#% 1117002
#! We study the problem of finding the k most frequent items in a stream of items for the recently proposed max-frequency measure. Based on the properties of an item, the max-frequency of an item is counted over a sliding window of which the length changes dynamically. Besides being parameterless, this way of measuring the support of items was shown to have the advantage of a faster detection of bursts in a stream, especially if the set of items is heterogeneous. The algorithm that was proposed for maintaining all frequent items, however, scales poorly when the number of items becomes large. Therefore, in this paper we propose, instead of reporting all frequent items, to only mine the top-k most frequent ones. First we prove that in order to solve this problem exactly, we still need a prohibitive amount of memory (at least linear in the number of items). Yet, under some reasonable conditions, we show both theoretically and empirically that a memory-efficient algorithm exists. A prototype of this algorithm is implemented and we present its performance w.r.t. memory-efficiency on real-life data and in controlled experiments with synthetic data.

#index 1451168
#* Probably the best itemsets
#@ Nikolaj Tatti
#t 2010
#c 0
#% 227919
#% 232136
#% 248012
#% 323981
#% 388024
#% 727667
#% 769893
#% 989606
#% 1127267
#% 1176968
#% 1214659
#% 1348648
#! One of the main current challenges in itemset mining is to discover a small set of high-quality itemsets. In this paper we propose a new and general approach for measuring the quality of itemsets. The method is solidly founded in Bayesian statistics and decreases monotonically, allowing for efficient discovery of all interesting itemsets. The measure is defined by connecting statistical models and collections of itemsets. This allows us to score individual itemsets with the probability of them occuring in random models built on the data. As a concrete example of this framework we use exponential models. This class of models possesses many desirable properties. Most importantly, Occam's razor in Bayesian model selection provides a defence for the pattern explosion. As general exponential models are infeasible in practice, we use decomposable models; a large sub-class for which the measure is solvable. For the actual computation of the score we sample models from the posterior distribution using an MCMC approach. Experimentation on our method demonstrates the measure works in practice and results in interpretable and insightful itemsets for both synthetic and real-world data.

#index 1451169
#* Grafting-light: fast, incremental feature selection and structure learning of Markov random fields
#@ Jun Zhu;Ni Lao;Eric P. Xing
#t 2010
#c 0
#% 73441
#% 126894
#% 226495
#% 243728
#% 277467
#% 464434
#% 464444
#% 465762
#% 722929
#% 722937
#% 722943
#% 816181
#% 853697
#% 876066
#% 983808
#% 983842
#% 1074353
#% 1100067
#% 1214726
#% 1328357
#% 1650579
#% 1673026
#! Feature selection is an important task in order to achieve better generalizability in high dimensional learning, and structure learning of Markov random fields (MRFs) can automatically discover the inherent structures underlying complex data. Both problems can be cast as solving an l1-norm regularized parameter estimation problem. The existing Grafting method can avoid doing inference on dense graphs in structure learning by incrementally selecting new features. However, Grafting performs a greedy step to optimize over free parameters once new features are included. This greedy strategy results in low efficiency when parameter learning is itself non-trivial, such as in MRFs, in which parameter learning depends on an expensive subroutine to calculate gradients. The complexity of calculating gradients in MRFs is typically exponential to the size of maximal cliques. In this paper, we present a fast algorithm called Grafting-Light to solve the l1-norm regularized maximum likelihood estimation of MRFs for efficient feature selection and structure learning. Grafting-Light iteratively performs one-step of orthant-wise gradient descent over free parameters and selects new features. This lazy strategy is guaranteed to converge to the global optimum and can effectively select significant features. On both synthetic and real data sets, we show that Grafting-Light is much more efficient than Grafting for both feature selection and structure learning, and performs comparably with the optimal batch method that directly optimizes over all the features for feature selection but is much more efficient and accurate for structure learning of MRFs.

#index 1451170
#* A scalable two-stage approach for a class of dimensionality reduction techniques
#@ Liang Sun;Betul Ceran;Jieping Ye
#t 2010
#c 0
#% 80995
#% 90834
#% 317525
#% 317535
#% 393059
#% 722887
#% 763708
#% 803769
#% 829010
#% 855563
#% 875947
#% 891559
#% 1034713
#% 1074000
#% 1083698
#% 1211812
#% 1305486
#% 1350938
#% 1742155
#! Dimensionality reduction plays an important role in many data mining applications involving high-dimensional data. Many existing dimensionality reduction techniques can be formulated as a generalized eigenvalue problem, which does not scale to large-size problems. Prior work transforms the generalized eigenvalue problem into an equivalent least squares formulation, which can then be solved efficiently. However, the equivalence relationship only holds under certain assumptions without regularization, which severely limits their applicability in practice. In this paper, an efficient two-stage approach is proposed to solve a class of dimensionality reduction techniques, including Canonical Correlation Analysis, Orthonormal Partial Least Squares, linear Discriminant Analysis, and Hypergraph Spectral Learning. The proposed two-stage approach scales linearly in terms of both the sample size and data dimensionality. The main contributions of this paper include (1) we rigorously establish the equivalence relationship between the proposed two-stage approach and the original formulation without any assumption; and (2) we show that the equivalence relationship still holds in the regularization setting. We have conducted extensive experiments using both synthetic and real-world data sets. Our experimental results confirm the equivalence relationship established in this paper. Results also demonstrate the scalability of the proposed two-stage approach.

#index 1451171
#* An efficient algorithm for a class of fused lasso problems
#@ Jun Liu;Lei Yuan;Jieping Ye
#t 2010
#c 0
#% 323099
#% 397854
#% 416797
#% 416838
#% 416846
#% 799024
#% 1072363
#% 1077165
#% 1193366
#% 1211747
#% 1214676
#% 1302843
#% 1302853
#% 1386006
#% 1417091
#! The fused Lasso penalty enforces sparsity in both the coefficients and their successive differences, which is desirable for applications with features ordered in some meaningful way. The resulting problem is, however, challenging to solve, as the fused Lasso penalty is both non-smooth and non-separable. Existing algorithms have high computational complexity and do not scale to large-size problems. In this paper, we propose an Efficient Fused Lasso Algorithm (EFLA) for optimizing this class of problems. One key building block in the proposed EFLA is the Fused Lasso Signal Approximator (FLSA). To efficiently solve FLSA, we propose to reformulate it as the problem of finding an "appropriate" subgradient of the fused penalty at the minimizer, and develop a Subgradient Finding Algorithm (SFA). We further design a restart technique to accelerate the convergence of SFA, by exploiting the special "structures" of both the original and the reformulated FLSA problems. Our empirical evaluations show that, both SFA and EFLA significantly outperform existing solvers. We also demonstrate several applications of the fused Lasso.

#index 1451172
#* Unsupervised feature selection for multi-cluster data
#@ Deng Cai;Chiyuan Zhang;Xiaofei He
#t 2010
#c 0
#% 105567
#% 243728
#% 313959
#% 341596
#% 443790
#% 722932
#% 729437
#% 771842
#% 772864
#% 796212
#% 865332
#% 878207
#% 916789
#% 983948
#% 1083630
#% 1117001
#% 1246228
#% 1270185
#% 1350938
#! In many data analysis tasks, one is often confronted with very high dimensional data. Feature selection techniques are designed to find the relevant feature subset of the original features which can facilitate clustering, classification and retrieval. In this paper, we consider the feature selection problem in unsupervised learning scenario, which is particularly difficult due to the absence of class labels that would guide the search for relevant information. The feature selection problem is essentially a combinatorial optimization problem which is computationally expensive. Traditional unsupervised feature selection methods address this issue by selecting the top ranked features based on certain scores computed independently for each feature. These approaches neglect the possible correlation between different features and thus can not produce an optimal feature subset. Inspired from the recent developments on manifold learning and L1-regularized models for subset selection, we propose in this paper a new approach, called Multi-Cluster Feature Selection (MCFS), for unsupervised feature selection. Specifically, we select those features such that the multi-cluster structure of the data can be best preserved. The corresponding optimization problem can be efficiently solved since it only involves a sparse eigen-problem and a L1-regularized least squares problem. Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm.

#index 1451173
#* Feature selection for support vector regression using probabilistic prediction
#@ Jian-Bo Yang;Chong-Jin Ong
#t 2010
#c 0
#% 269217
#% 304935
#% 361100
#% 425038
#% 425048
#% 722929
#% 722938
#% 729437
#% 770786
#% 770857
#% 929722
#% 983907
#% 1022056
#% 1042718
#% 1861283
#% 1862285
#! This paper presents a novel wrapper-based feature selection method for Support Vector Regression (SVR) using its probabilistic predictions. The method computes the importance of a feature by aggregating the difference, over the feature space, of the conditional density functions of the SVR prediction with and without the feature. As the exact computation of this importance measure is expensive, two approximations are proposed. The effectiveness of the measure using these approximations, in comparison to several other existing feature selection methods for SVR, is evaluated on both artificial and real-world problems. The result of the experiment shows that the proposed method generally performs better, and at least as well as the existing methods, with notable advantage when the data set is sparse.

#index 1451174
#* Versatile publishing for privacy preservation
#@ Xin Jin;Mingyang Zhang;Nan Zhang;Gautam Das
#t 2010
#c 0
#% 319789
#% 408396
#% 443463
#% 576111
#% 797999
#% 800514
#% 801690
#% 810011
#% 824727
#% 844340
#% 864406
#% 864412
#% 874988
#% 881546
#% 893100
#% 960289
#% 1015140
#% 1022246
#% 1022247
#% 1022265
#% 1022266
#% 1083631
#% 1083653
#% 1206582
#% 1206882
#% 1206883
#% 1214673
#% 1217156
#% 1328175
#% 1328177
#% 1372693
#% 1372735
#% 1424595
#% 1670071
#! Motivated by the insufficiency of the existing quasi-identifier/sensitive-attribute (QI-SA) framework on modeling real-world privacy requirements for data publishing, we propose a novel versatile publishing scheme with which privacy requirements can be specified as an arbitrary set of privacy rules over attributes in the microdata table. To enable versatile publishing, we introduce the Guardian Normal Form (GNF), a novel method of publishing multiple sub-tables such that each sub-table is anonymized by an existing QI-SA publishing algorithm, while the combination of all published tables guarantees all privacy rules. We devise two algorithms, Guardian Decomposition (GD) and Utility-aware Decomposition (UAD), for decomposing a microdata table into GNF, and present extensive experiments over real-world datasets to demonstrate the effectiveness of both algorithms.

#index 1451175
#* Privacy-preserving outsourcing support vector machines with random transformation
#@ Keng-Pei Lin;Ming-Syan Chen
#t 2010
#c 0
#% 300184
#% 378173
#% 393059
#% 397367
#% 443463
#% 512307
#% 576761
#% 576762
#% 577233
#% 577289
#% 765448
#% 772829
#% 844360
#% 874166
#% 881524
#% 1022211
#% 1035790
#% 1089781
#% 1176883
#% 1206938
#% 1217157
#% 1669943
#% 1861262
#% 1861857

#index 1451176
#* On the quality of inferring interests from social neighbors
#@ Zhen Wen;Ching-Yung Lin
#t 2010
#c 0
#% 124010
#% 269217
#% 722904
#% 730089
#% 818259
#% 860021
#% 879567
#% 1019076
#% 1035588
#% 1055737
#% 1081487
#% 1083624
#% 1083641
#% 1083672
#% 1127482
#% 1130827
#% 1130901
#% 1183173
#% 1194120
#% 1214702
#% 1227622
#% 1704240
#! This paper intends to provide some insights of a scientific problem: how likely one's interests can be inferred from his/her social connections -- friends, friends' friends, 3-degree friends, etc? Is "Birds of a Feather Flocks Together" a norm? We do not consider the friending activity on online social networking sites. Instead, we conduct this study by implementing a privacy-preserving large distribute social sensor system in a large global IT company to capture the multifaceted activities of 30,000+ people, including communications (e.g., emails, instant messaging, etc) and Web 2.0 activities (e.g., social bookmarking, file sharing, blogging, etc). These activities occupy the majority of employees' time in work, and thus, provide a high quality approximation to the real social connections of employees in the workplace context. In addition to such "informal networks", we investigated the "formal networks", such as their hierarchical structure, as well as the demographic profile data such as geography, job role, self-specified interests, etc. Because user ID matching across multiple sources on the Internet is very difficult, and most user activity logs have to be anonymized before they are processed, no prior studies could collect comparable multifaceted activity data of individuals. That makes this study unique. In this paper, we present a technique to predict the inference quality by utilizing (1) network analysis and network autocorrelation modeling of informal and formal networks, and (2) regression models to predict user interest inference quality from network characteristics. We verify our findings with experiments on both implicit user interests indicated by the content of communications or Web 2.0 activities, and explicit user interests specified in user profiles. We demonstrate that the inference quality prediction increases the inference quality of implicit interests by 42.8%, and inference quality of explicit interests by up to 101%.

#index 1451177
#* DUST: a generalized notion of similarity between uncertain time series
#@ Smruti R. Sarangi;Karin Murthy
#t 2010
#c 0
#% 172949
#% 814194
#% 893102
#% 992857
#% 1127609
#% 1181271
#% 1189215
#% 1206640
#% 1291115
#% 1695788
#! Large-scale sensor deployments and an increased use of privacy-preserving transformations have led to an increasing interest in mining uncertain time series data. Traditional distance measures such as Euclidean distance or dynamic time warping are not always effective for analyzing uncertain time series data. Recently, some measures have been proposed to account for uncertainty in time series data. However, we show in this paper that their applicability is limited. In specific, these approaches do not provide an intuitive way to compare two uncertain time series and do not easily accommodate multiple error functions. In this paper, we provide a theoretical framework that generalizes the notion of similarity between uncertain time series. Secondly, we propose DUST, a novel distance measure that accommodates uncertainty and degenerates to the Euclidean distance when the distance is large compared to the error. We provide an extensive experimental validation of our approach for the following applications: classification, top-k motif search, and top-k nearest-neighbor queries.

#index 1451178
#* Cold start link prediction
#@ Vincent Leroy;B. Barla Cambazoglu;Francesco Bonchi
#t 2010
#c 0
#% 342596
#% 464280
#% 466086
#% 577298
#% 722914
#% 729923
#% 770757
#% 845263
#% 853534
#% 858102
#% 915267
#% 955712
#% 956511
#% 1127360
#% 1166530
#% 1190108
#% 1190127
#% 1259854
#% 1523884
#% 1678453
#! In the traditional link prediction problem, a snapshot of a social network is used as a starting point to predict, by means of graph-theoretic measures, the links that are likely to appear in the future. In this paper, we introduce cold start link prediction as the problem of predicting the structure of a social network when the network itself is totally missing while some other information regarding the nodes is available. We propose a two-phase method based on the bootstrap probabilistic graph. The first phase generates an implicit social network under the form of a probabilistic graph. The second phase applies probabilistic graph-based measures to produce the final prediction. We assess our method empirically over a large data collection obtained from Flickr, using interest groups as the initial information. The experiments confirm the effectiveness of our approach.

#index 1451179
#* Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Bharat Rao;Balaji Krishnapuram;Andrew Tomkins;Qiang Yang
#t 2010
#c 0

#index 1451180
#* Learning with cost intervals
#@ Xu-Ying Liu;Zhi-Hua Zhou
#t 2010
#c 0
#% 280437
#% 342611
#% 443509
#% 565245
#% 727925
#% 819448
#% 829987
#% 843876
#% 893461
#% 915260
#% 983880
#% 1083680
#% 1085130
#% 1250582
#% 1289281
#% 1378224
#% 1781594
#! Existing cost-sensitive learning methods require that the unequal misclassification costs should be given as precise values. In many real-world applications, however, it is generally difficult to have a precise cost value since the user maybe only knows that one type of mistake is much more severe than another type, yet it is infeasible to give a precise description. In such situations, it is more meaningful to work with a cost interval instead of a precise cost value. In this paper we report the first study along this direction. We propose the CISVM method, a support vector machine, to work with cost interval information. Experiments show that when there are only cost intervals available, CISVM is significantly superior to standard cost-sensitive SVMs using any of the minimal cost, mean cost and maximal cost to learn. Moreover, considering that in some cases other information about costs can be obtained in addition to cost intervals, such as the distribution of costs, we propose a general approach CODIS for using the distribution information to help improve performance. Experiments show that this approach can reduce 60% more risks than the standard cost-sensitive SVM which assumes the expected cost is the true value.

#index 1451181
#* The new iris data: modular data generators
#@ Iris Adä;Michael R. Berthold
#t 2010
#c 0
#% 152934
#% 819358
#% 994153
#% 1060627
#% 1301004
#! In this paper we introduce a modular, highly flexible, open-source environment for data generation. Using an existing graphical data flow tool, the user can combine various types of modules for numeric and categorical data generators. Additional functionality is added via the data processing framework in which the generator modules are embedded. The resulting data flows can be used to document, deploy, and reuse the resulting data generators. We describe the overall environment and individual modules and demonstrate how they can be used for the generation of a sample, complex customer/product database with corresponding shopping basket data, including various artifacts and outliers.

#index 1451182
#* Why label when you can search?: alternatives to active learning for applying human resources to build classification models under extreme class imbalance
#@ Josh Attenberg;Foster Provost
#t 2010
#c 0
#% 116165
#% 169717
#% 280406
#% 280437
#% 722797
#% 770771
#% 1019070
#% 1083692
#% 1100053
#% 1100102
#% 1211829
#% 1214746
#% 1250678
#% 1251728
#% 1270759
#% 1271973
#% 1272000
#% 1301405
#% 1400012
#! This paper analyses alternative techniques for deploying low-cost human resources for data acquisition for classifier induction in domains exhibiting extreme class imbalance - where traditional labeling strategies, such as active learning, can be ineffective. Consider the problem of building classifiers to help brands control the content adjacent to their on-line advertisements. Although frequent enough to worry advertisers, objectionable categories are rare in the distribution of impressions encountered by most on-line advertisers - so rare that traditional sampling techniques do not find enough positive examples to train effective models. An alternative way to deploy human resources for training-data acquisition is to have them "guide" the learning by searching explicitly for training examples of each class. We show that under extreme skew, even basic techniques for guided learning completely dominate smart (active) strategies for applying human resources to select cases for labeling. Therefore, it is critical to consider the relative cost of search versus labeling, and we demonstrate the tradeoffs for different relative costs. We show that in cost/skew settings where the choice between search and active labeling is equivocal, a hybrid strategy can combine the benefits.

#index 1451183
#* Discovering significant relaxed order-preserving submatrices
#@ Qiong Fang;Wilfred Ng;Jianlin Feng
#t 2010
#c 0
#% 397632
#% 469422
#% 617211
#% 654466
#% 727882
#% 778215
#% 881573
#% 906512
#% 1063473
#% 1206649
#! Mining order-preserving submatrix (OPSM) patterns has received much attention from researchers, since in many scientific applications, such as those involving gene expression data, it is natural to express the data in a matrix and also important to find the order-preserving submatrix patterns. However, most current work assumes the noise-free OPSM model and thus is not practical in many real situations when sample contamination exists. In this paper, we propose a relaxed OPSM model called ROPSM. The ROPSM model supports mining more reasonable noise-corrupted OPSM patterns than another well-known model called AOPC (approximate order-preserving cluster). While OPSM mining is known to be an NP-hard problem, mining ROPSM patterns is even a harder problem. We propose a novel method called ROPSM-Growth to mine ROPSM patterns. Specifically, two pattern growing strategies, such as column-centric strategy and row-centric strategy, are presented, which are effective to grow the seed OPSMs into significant ROPSMs. An effective median-rank based method is also developed to discover the underlying true order of conditions involved in an ROPSM pattern. Our experiments on a biological dataset show that the ROPSM model better captures the characteristics of noise in gene expression data matrix compared to the AOPC model. Importantly, we find that our approach is able to detect more quality biologically significant patterns with comparable efficiency with the counterparts of AOPC. Specifically, at least 26.6% (75 out of 282) of the patterns mined by our approach are strongly associated with more than 10 gene categories (high biological significance), which is 3 times better than that obtained from using the AOPC approach.

#index 1451184
#* Topic dynamics: an alternative model of bursts in streams of topics
#@ Dan He;D. Stott Parker
#t 2010
#c 0
#% 287196
#% 309096
#% 577220
#% 729943
#% 771924
#% 776054
#% 810821
#% 864533
#% 874170
#% 881490
#% 885066
#% 1133466
#% 1214671
#! For some time there has been increasing interest in the problem of monitoring the occurrence of topics in a stream of events, such as a stream of news articles. This has led to different models of bursts in these streams, i.e., periods of elevated occurrence of events. Today there are several burst definitions and detection algorithms, and their differences can produce very different results in topic streams. These definitions also share a fundamental problem: they define bursts in terms of an arrival rate. This approach is limiting; other stream dimensions can matter. We reconsider the idea of bursts from the standpoint of a simple kind of physics. Instead of focusing on arrival rates, we reconstruct bursts as a dynamic phenomenon, using kinetics concepts from physics -- mass and velocity -- and derive momentum, acceleration, and force from these. We refer to the result as topic dynamics, permitting a hierarchical, expressive model of bursts as intervals of increasing momentum. As a sample application, we present a topic dynamics model for the large PubMed/MEDLINE database of biomedical publications, using the MeSH (Medical Subject Heading) topic hierarchy. We show our model is able to detect bursts for MeSH terms accurately as well as efficiently.

#index 1451185
#* Extracting temporal signatures for comprehending systems biology models
#@ Naren Sundaravaradan;K.S.M. Tozammel Hossain;Vandana Sreedharan;Douglas J. Slotta;John Paul C. Vergara;Lenwood S. Heath;Naren Ramakrishnan
#t 2010
#c 0
#% 44876
#% 115608
#% 243727
#% 464451
#% 729975
#% 832973
#% 910652
#% 987243
#% 1177641
#% 1272396
#! Systems biology has made massive strides in recent years, with capabilities to model complex systems including cell division, stress response, energy metabolism, and signaling pathways. Concomitant with their improved modeling capabilities, however, such biochemical network models have also become notoriously complex for humans to comprehend. We propose network comprehension as a key problem for the KDD community, where the goal is to create explainable representations of complex biological networks. We formulate this problem as one of extracting temporal signatures from multi-variate time series data, where the signatures are composed of ordinal comparisons between time series components. We show how such signatures can be inferred by formulating the data mining problem as one of feature selection in rank-order space. We propose five new feature selection strategies for rank-order space and assess their selective superiorities. Experimental results on budding yeast cell cycle models demonstrate compelling results comparable to human interpretations of the cell cycle.

#index 1451186
#* Negative correlations in collaboration: concepts and algorithms
#@ Jinyan Li;Qian Liu;Tao Zeng
#t 2010
#c 0
#% 90990
#% 152934
#% 464873
#% 765518
#% 778215
#% 824931
#% 832921
#% 864476
#% 905728
#% 1060810
#% 1190130
#! This paper studies efficient mining of negative correlations that pace in collaboration. A collaborating negative correlation is a negative correlation between two sets of variables rather than traditionally between a pair of variables. It signifies a synchronized value rise or fall of all variables within one set whenever all variables in the other set go jointly at the opposite trend. The time complexity is exponential in mining. The high efficiency of our algorithm is attributed to two factors: (i) the transformation of the original data into a bipartite graph database, and (ii) the mining of transpose closures from a wide transactional database. Applying to a Yeast gene expression data, we evaluate, by using Pearson's correlation coefficient and P-value, the biological relevance of collaborating negative correlations as an example among many real-life domains.

#index 1451187
#* k-Support anonymity based on pseudo taxonomy for outsourcing of frequent itemset mining
#@ Chih-Hua Tai;Philip S. Yu;Ming-Syan Chen
#t 2010
#c 0
#% 154992
#% 248030
#% 300184
#% 381870
#% 463903
#% 481588
#% 481758
#% 577233
#% 818916
#% 844335
#% 937550
#% 1021945
#% 1022211
#% 1063476
#% 1128617
#% 1206763
#% 1318619
#% 1562157
#! For any outsourcing service, privacy is a major concern. This paper focuses on outsourcing frequent itemset mining and examines the issue on how to protect privacy against the case where the attackers have precise knowledge on the supports of some items. We propose a new approach referred to as k-support anonymity to protect each sensitive item with k-1 other items of similar support. To achieve k-support anonymity, we introduce a pseudo taxonomy tree and have the third party mine the generalized frequent itemsets under the corresponding generalized association rules instead of association rules. The pseudo taxonomy is a construct to facilitate hiding of the original items, where each original item can map to either a leaf node or an internal node in the taxonomy tree. The rationale for this approach is that with a taxonomy tree, the k nodes to satisfy the k-support anonymity may be any k nodes in the taxonomy tree with the appropriate supports. So this approach can provide more candidates for k-support anonymity with limited fake items as only the leaf nodes, not the internal nodes, of the taxonomy tree need to appear in the transactions. Otherwise for the association rule mining, the k nodes to satisfy the k-support anonymity have to correspond to the leaf nodes in the taxonomy tree. This is far more restricted. The challenge is thus on how to generate the pseudo taxonomy tree to facilitate k-support anonymity and to ensure the conservation of original frequent itemsets. The experimental results showed that our methods of k-support anonymity can achieve very good privacy protection with moderate storage overhead.

#index 1451188
#* Collusion-resistant privacy-preserving data mining
#@ Bin Yang;Hiroshi Nakagawa;Issei Sato;Jun Sakuma
#t 2010
#c 0
#% 23638
#% 114671
#% 259504
#% 319994
#% 341437
#% 512307
#% 554524
#% 729930
#% 743280
#% 891559
#% 923645
#% 954159
#% 963800
#% 1066745
#% 1068712
#% 1074831
#% 1128844
#% 1386180
#% 1404126
#% 1721181
#! Recent research in privacy-preserving data mining (PPDM) has become increasingly popular due to the wide application of data mining and the increased concern regarding the protection of private and personal information. Lately, numerous methods of privacy-preserving data mining have been proposed. Most of these methods are based on an assumption that semi-honest is and collusion is not present. In other words, every party follows such protocol properly with the exception that it keeps a record of all its intermediate computations without sharing the record with others. In this paper, we focus our attention on the problem of collusions, in which some parties may collude and share their record to deduce the private information of other parties. In particular, we consider a general problem in PPDM - multiparty secure computation of some functions of secure summations of data spreading around multiple parties. To solve such a problem, we propose a new method that entails a high level of security - full-privacy. With this method, no sensitive information of a party will be revealed even when all other parties collude. In addition, this method is efficient with a running time of O(m). We will also show that by applying this general method, a large number of problems in PPDM can be solved with enhanced security.

#index 1451189
#* Data mining with differential privacy
#@ Arik Friedman;Assaf Schuster
#t 2010
#c 0
#% 136350
#% 310500
#% 449568
#% 449588
#% 809245
#% 926881
#% 1029084
#% 1061644
#% 1083653
#% 1141473
#% 1198225
#% 1206678
#% 1214684
#% 1217148
#% 1414540
#% 1670071
#% 1740518
#! We consider the problem of data mining with formal privacy guarantees, given a data access interface based on the differential privacy framework. Differential privacy requires that computations be insensitive to changes in any particular individual's record, thereby restricting data leaks through the results. The privacy preserving interface ensures unconditionally safe access to the data and does not require from the data miner any expertise in privacy. However, as we show in the paper, a naive utilization of the interface to construct privacy preserving data mining algorithms could lead to inferior data mining results. We address this problem by considering the privacy and the algorithmic requirements simultaneously, focusing on decision tree induction as a sample application. The privacy mechanism has a profound effect on the performance of the methods chosen by the data miner. We demonstrate that this choice could make the difference between an accurate classifier and a completely useless one. Moreover, an improved algorithm can achieve the same level of accuracy and privacy as the naive implementation but with an order of magnitude fewer learning samples.

#index 1451190
#* Discovering frequent patterns in sensitive data
#@ Raghav Bhaskar;Srivatsan Laxman;Adam Smith;Abhradeep Thakurta
#t 2010
#c 0
#% 152934
#% 316709
#% 341700
#% 420063
#% 463903
#% 576111
#% 576761
#% 731608
#% 800513
#% 1029084
#% 1061644
#% 1083653
#% 1083731
#% 1190072
#% 1217148
#% 1372651
#% 1670071
#% 1740518
#! Discovering frequent patterns from data is a popular exploratory technique in datamining. However, if the data are sensitive (e.g., patient health records, user behavior records) releasing information about significant patterns or trends carries significant risk to privacy. This paper shows how one can accurately discover and release the most significant patterns along with their frequencies in a data set containing sensitive information, while providing rigorous guarantees of privacy for the individuals whose information is stored there. We present two efficient algorithms for discovering the k most frequent patterns in a data set of sensitive records. Our algorithms satisfy differential privacy, a recently introduced definition that provides meaningful privacy guarantees in the presence of arbitrary external information. Differentially private algorithms require a degree of uncertainty in their output to preserve privacy. Our algorithms handle this by returning 'noisy' lists of patterns that are close to the actual list of k most frequent patterns in the data. We define a new notion of utility that quantifies the output accuracy of private top-k pattern mining algorithms. In typical data sets, our utility criterion implies low false positive and false negative rates in the reported lists. We prove that our methods meet the new utility criterion; we also demonstrate the performance of our algorithms through extensive experiments on the transaction data sets from the FIMI repository. While the paper focuses on frequent pattern mining, the techniques developed here are relevant whenever the data mining output is a list of elements ordered according to an appropriately 'robust' measure of interest.

#index 1451191
#* Fast nearest-neighbor search in disk-resident graphs
#@ Purnamrita Sarkar;Andrew W. Moore
#t 2010
#c 0
#% 274612
#% 577273
#% 730089
#% 765261
#% 818328
#% 869492
#% 898311
#% 956551
#% 956613
#% 1016176
#% 1016177
#% 1063716
#% 1127445
#% 1130854
#% 1214643
#% 1667624
#! Link prediction, personalized graph search, fraud detection, and many such graph mining problems revolve around the computation of the most "similar" k nodes to a given query node. One widely used class of similarity measures is based on random walks on graphs, e.g., personalized pagerank, hitting and commute times, and simrank. There are two fundamental problems associated with these measures. First, existing online algorithms typically examine the local neighborhood of the query node which can become significantly slower whenever high-degree nodes are encountered (a common phenomenon in real-world graphs). We prove that turning high degree nodes into sinks results in only a small approximation error, while greatly improving running times. The second problem is that of computing similarities at query time when the graph is too large to be memory-resident. The obvious solution is to split the graph into clusters of nodes and store each cluster on a disk page; ideally random walks will rarely cross cluster boundaries and cause page-faults. Our contributions here are twofold: (a) we present an efficient deterministic algorithm to find the k closest neighbors (in terms of personalized pagerank) of any query node in such a clustered graph, and (b) we develop a clustering algorithm (RWDISK) that uses only sequential sweeps over data files. Empirical results on several large publicly available graphs like DBLP, Citeseer and Live-Journal (~ 90 M edges) demonstrate that turning high degree nodes into sinks not only improves running time of RWDISK by a factor of 3 but also boosts link prediction accuracy by a factor of 4 on average. We also show that RWDISK returns more desirable (high conductance and small size) clusters than the popular clustering algorithm METIS, while requiring much less memory. Finally our deterministic algorithm for computing nearest neighbors incurs far fewer page-faults (factor of 5) than actually simulating random walks.

#index 1451192
#* Balanced allocation with succinct representation
#@ Saeed Alaei;Ravi Kumar;Azarakhsh Malekian;Erik Vee
#t 2010
#c 0
#% 53085
#% 91357
#% 122671
#% 224218
#% 963273
#% 1055688
#% 1091266
#% 1164954
#% 1222625
#% 1336448
#! Motivated by applications in guaranteed delivery in computational advertising, we consider the general problem of balanced allocation in a bipartite supply-demand setting. Our formulation captures the notion of deviation from being balanced by a convex penalty function. While this formulation admits a convex programming solution, we strive for more robust and scalable algorithms. For the case of L1 penalty functions we obtain a simple combinatorial algorithm based on min-cost flow in graphs and show how to precompute a linear amount of information such that the allocation along any edge can be approximated in constant time. We then extend our combinatorial solution to any convex function by solving a convex cost flow. These scalable methods may have applications in other contexts stipulating balanced allocation. We study the performance of our algorithms on large real-world graphs and show that they are efficient, scalable, and robust in practice.

#index 1451193
#* Neighbor query friendly compression of social networks
#@ Hossein Maserrat;Jian Pei
#t 2010
#c 0
#% 311808
#% 399764
#% 410276
#% 656242
#% 656281
#% 656282
#% 740507
#% 754117
#% 1035579
#% 1214643
#! Compressing social networks can substantially facilitate mining and advanced analysis of large social networks. Preferably, social networks should be compressed in a way that they still can be queried efficiently without decompression. Arguably, neighbor queries, which search for all neighbors of a query vertex, are the most essential operations on social networks. Can we compress social networks effectively in a neighbor query friendly manner, that is, neighbor queries still can be answered in sublinear time using the compression? In this paper, we develop an effective social network compression approach achieved by a novel Eulerian data structure using multi-position linearizations of directed graphs. Our method comes with a nontrivial theoretical bound on the compression rate. To the best of our knowledge, our approach is the first that can answer both out-neighbor and in-neighbor queries in sublinear time. An extensive empirical study on more than a dozen benchmark real data sets verifies our design.

#index 1451194
#* Parallel SimRank computation on large graphs with iterative aggregation
#@ Guoming He;Haijun Feng;Cuiping Li;Hong Chen
#t 2010
#c 0
#% 66165
#% 290830
#% 447948
#% 577273
#% 729938
#% 769460
#% 769887
#% 805904
#% 810059
#% 810072
#% 818218
#% 823342
#% 838516
#% 853940
#% 874997
#% 881460
#% 881480
#% 881493
#% 931290
#% 961564
#% 989586
#% 989643
#% 1063508
#% 1117375
#% 1127384
#% 1190097
#% 1328067
#! Recently there has been a lot of interest in graph-based analysis. One of the most important aspects of graph-based analysis is to measure similarity between nodes in a graph. SimRank is a simple and influential measure of this kind, based on a solid graph theoretical model. However, existing methods on SimRank computation suffer from two limitations: 1) the computing cost can be very high in practice; and 2) they can only be applied on static graphs. In this paper, we exploit the inherent parallelism and high memory bandwidth of graphics processing units (GPU) to accelerate the computation of SimRank on large graphs. Furthermore, based on the observation that SimRank is essentially a first-order Markov Chain, we propose to utilize the iterative aggregation techniques for uncoupling Markov chains to compute SimRank scores in parallel for large graphs. The iterative aggregation method can be applied on dynamic graphs. Moreover, it can handle not only the link-updating problem but also the node-updating problem. Extensive experiments on synthetic and real data sets verify that the proposed methods are efficient and effective.

#index 1451195
#* Dynamics of conversations
#@ Ravi Kumar;Mohammad Mahdian;Mary McGlohon
#t 2010
#c 0
#% 190611
#% 233648
#% 375076
#% 593994
#% 733847
#% 847218
#% 868469
#% 881523
#% 937549
#% 953073
#% 1035581
#% 1040835
#% 1083675
#% 1214671
#% 1669913
#! How do online conversations build? Is there a common model that human communication follows? In this work we explore these questions in detail. We analyze the structure of conversations in three different social datasets, namely, Usenet groups, Yahoo! Groups, and Twitter. We propose a simple mathematical model for the generation of basic conversation structures and then refine this model to take into account the identities of each member of the conversation.

#index 1451196
#* Flexible constrained spectral clustering
#@ Xiang Wang;Ian Davidson
#t 2010
#c 0
#% 313959
#% 466890
#% 732539
#% 765545
#% 848482
#% 879615
#% 983829
#% 995140
#% 1073891
#% 1085668
#% 1279294
#! Constrained clustering has been well-studied for algorithms like K-means and hierarchical agglomerative clustering. However, how to encode constraints into spectral clustering remains a developing area. In this paper, we propose a flexible and generalized framework for constrained spectral clustering. In contrast to some previous efforts that implicitly encode Must-Link and Cannot-Link constraints by modifying the graph Laplacian or the resultant eigenspace, we present a more natural and principled formulation, which preserves the original graph Laplacian and explicitly encodes the constraints. Our method offers several practical advantages: it can encode the degree of belief (weight) in Must-Link and Cannot-Link constraints; it guarantees to lower-bound how well the given constraints are satisfied using a user-specified threshold; and it can be solved deterministically in polynomial time through generalized eigendecomposition. Furthermore, by inheriting the objective function from spectral clustering and explicitly encoding the constraints, much of the existing analysis of spectral clustering techniques is still valid. Consequently our work can be posed as a natural extension to unconstrained spectral clustering and be interpreted as finding the normalized min-cut of a labeled graph. We validate the effectiveness of our approach by empirical results on real-world data sets, with applications to constrained image segmentation and clustering benchmark data sets with both binary and degree-of-belief constraints.

#index 1451197
#* A hierarchical information theoretic technique for the discovery of non linear alternative clusterings
#@ Xuan-Hong Dang;James Bailey
#t 2010
#c 0
#% 464291
#% 466890
#% 770782
#% 842373
#% 915231
#% 915244
#% 1117008
#% 1137063
#% 1176992
#% 1502452
#! Discovery of alternative clusterings is an important method for exploring complex datasets. It provides the capability for the user to view clustering behaviour from different perspectives and thus explore new hypotheses. However, current algorithms for alternative clustering have focused mainly on linear scenarios and may not perform as desired for datasets containing clusters with non linear shapes. Our goal in this paper is to address this challenge of non linearity. In particular, we propose a novel algorithm to uncover an alternative clustering that is distinctively different from an existing, reference clustering. Our technique is information theory based and aims to ensure alternative clustering quality by maximizing the mutual information between clustering labels and data observations, whilst at the same time ensuring alternative clustering distinctiveness by minimizing the information sharing between the two clusterings. We perform experiments to assess our method against a large range of alternative clustering algorithms in the literature. We show our technique's performance is generally better for non-linear scenarios and furthermore, is highly competitive even for simpler, linear scenarios.

#index 1451198
#* Clustering by synchronization
#@ Christian Böhm;Claudia Plant;Junming Shao;Qinli Yang
#t 2010
#c 0
#% 36672
#% 210173
#% 248790
#% 248792
#% 273890
#% 349208
#% 466425
#% 481281
#% 881462
#% 1063483
#% 1211824
#! Synchronization is a powerful basic concept in nature regulating a large variety of complex processes ranging from the metabolism in the cell to social behavior in groups of individuals. Therefore, synchronization phenomena have been extensively studied and models robustly capturing the dynamical synchronization process have been proposed, e.g. the Extensive Kuramoto Model. Inspired by the powerful concept of synchronization, we propose Sync, a novel approach to clustering. The basic idea is to view each data object as a phase oscillator and simulate the interaction behavior of the objects over time. As time evolves, similar objects naturally synchronize together and form distinct clusters. Inherited from synchronization, Sync has several desirable properties: The clusters revealed by dynamic synchronization truly reflect the intrinsic structure of the data set, Sync does not rely on any distribution assumption and allows detecting clusters of arbitrary number, shape and size. Moreover, the concept of synchronization allows natural outlier handling, since outliers do not synchronize with cluster objects. For fully automatic clustering, we propose to combine Sync with the Minimum Description Length principle. Extensive experiments on synthetic and real world data demonstrate the effectiveness and efficiency of our approach.

#index 1451199
#* Unifying dependent clustering and disparate clustering for non-homogeneous data
#@ M. Shahriar Hossain;Satish Tadepalli;Layne T. Watson;Ian Davidson;Richard F. Helm;Naren Ramakrishnan
#t 2010
#c 0
#% 403771
#% 464291
#% 528174
#% 729918
#% 769883
#% 770782
#% 827225
#% 881487
#% 915231
#% 916785
#% 1137062
#% 1214693
#! Modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relationships between these entities. We present an approach to cluster such non-homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints. Unlike prior work that views constraints as boolean criteria, we present a formulation that allows constraints to be satisfied or violated in a smooth manner. This enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function. We present results on both synthetic data as well as several real-world datasets.

#index 1451200
#* Fast euclidean minimum spanning tree: algorithm, analysis, and applications
#@ William B. March;Parikshit Ram;Alexander G. Gray
#t 2010
#c 0
#% 1451
#% 2115
#% 10130
#% 14513
#% 24077
#% 46809
#% 107462
#% 131062
#% 181234
#% 281738
#% 303072
#% 317178
#% 317313
#% 334462
#% 344423
#% 347264
#% 593794
#% 679728
#% 875957
#% 1010466
#% 1010733
#% 1061651
#% 1068288
#! The Euclidean Minimum Spanning Tree problem has applications in a wide range of fields, and many efficient algorithms have been developed to solve it. We present a new, fast, general EMST algorithm, motivated by the clustering and analysis of astronomical data. Large-scale astronomical surveys, including the Sloan Digital Sky Survey, and large simulations of the early universe, such as the Millennium Simulation, can contain millions of points and fill terabytes of storage. Traditional EMST methods scale quadratically, and more advanced methods lack rigorous runtime guarantees. We present a new dual-tree algorithm for efficiently computing the EMST, use adaptive algorithm analysis to prove the tightest (and possibly optimal) runtime bound for the EMST problem to-date, and demonstrate the scalability of our method on astronomical data sets.

#index 1451201
#* Mining program workflow from interleaved traces
#@ Jian-Guang Lou;Qiang Fu;Shengqi Yang;Jiang Li;Bin Wu
#t 2010
#c 0
#% 343052
#% 459021
#% 749034
#% 749036
#% 749038
#% 772836
#% 789478
#% 823352
#% 868127
#% 902158
#% 906081
#% 994159
#% 999469
#% 1010312
#% 1056196
#% 1126777
#% 1249008
#% 1286584
#% 1289961
#% 1318622
#% 1326873
#% 1379125
#% 1379131
#% 1468543
#! Successful software maintenance is becoming increasingly critical due to the increasing dependence of our society and economy on software systems. One key problem of software maintenance is the difficulty in understanding the evolving software systems. Program workflows can help system operators and administrators to understand system behaviors and verify system executions so as to greatly facilitate system maintenance. In this paper, we propose an algorithm to automatically discover program workflows from event traces that record system events during system execution. Different from existing workflow mining algorithms, our approach can construct concurrent workflows from traces of interleaved events. Our workflow mining approach is a three-step coarse-to-fine algorithm. At first, we mine temporal dependencies for each pair of events. Then, based on the mined pair-wise tem-poral dependencies, we construct a basic workflow model by a breadth-first path pruning algorithm. After that, we refine the workflow by verifying it with all training event traces. The re-finement algorithm tries to find out a workflow that can interpret all event traces with minimal state transitions and threads. The results of both simulation data and real program data show that our algorithm is highly effective.

#index 1451202
#* Connecting the dots between news articles
#@ Dafna Shahaf;Carlos Guestrin
#t 2010
#c 0
#% 118736
#% 230532
#% 268079
#% 309100
#% 445316
#% 729923
#% 754106
#% 783535
#% 823344
#% 1214650
#% 1415744
#% 1678465
#! The process of extracting useful knowledge from large datasets has become one of the most pressing problems in today's society. The problem spans entire sectors, from scientists to intelligence analysts and web users, all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this paper, we investigate methods for automatically connecting the dots -- providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the ongoing health-care debate. We formalize the characteristics of a good chain and provide an efficient algorithm (with theoretical guarantees) to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate the algorithm's effectiveness in helping users understanding the news.

#index 1451203
#* Discovering frequent subgraphs over uncertain graph databases under probabilistic semantics
#@ Zhaonian Zou;Hong Gao;Jianzhong Li
#t 2010
#c 0
#% 629708
#% 769910
#% 772830
#% 1063531
#% 1068580
#% 1083509
#% 1214633
#% 1292524
#% 1305464
#% 1328170
#% 1372657
#% 1393168
#% 1464049
#! Frequent subgraph mining has been extensively studied on certain graph data. However, uncertainties are inherently accompanied with graph data in practice, and there is very few work on mining uncertain graph data. This paper investigates frequent subgraph mining on uncertain graphs under probabilistic semantics. Specifically, a measure called φ-frequent probability is introduced to evaluate the degree of recurrence of subgraphs. Given a set of uncertain graphs and two numbers 0 S with probability at least (1 - δ/2)s, where s is the number of edges of S. In addition, it is thoroughly discussed how to set δ to guarantee the overall approximation quality of the algorithm. The extensive experiments on real uncertain graph data verify that the algorithm is efficient and that the mining results have very high quality.

#index 1451204
#* Boosting with structure information in the functional space: an application to graph classification
#@ Hongliang Fei;Jun Huan
#t 2010
#c 0
#% 73372
#% 198701
#% 302391
#% 425048
#% 520224
#% 727845
#% 983828
#% 983919
#% 989655
#% 1060798
#% 1063502
#% 1073918
#% 1073947
#% 1073948
#% 1083680
#% 1083688
#% 1130907
#% 1183448
#% 1190275
#% 1211727
#% 1211744
#% 1211756
#% 1211768
#% 1214723
#% 1292523
#% 1292525
#! Boosting is a very successful classification algorithm that produces a linear combination of "weak" classifiers (a.k.a. base learners) to obtain high quality classification models. In this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space. Though such relationships are generic, our work is particularly motivated by the emerging topic of pattern based classification for semi-structured data including graphs. Towards an efficient incorporation of the structure information, we have designed a general model where we use an undirected graph to capture the relationship of subgraph-based base learners. In our method, we combine both L1 norm and Laplacian based L2 norm penalty with Logit loss function of Logit Boost. In this approach, we enforce model sparsity and smoothness in the functional space spanned by the basis functions. We have derived efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features. Using comprehensive experimental study, we have demonstrated the effectiveness of the proposed learning methods.

#index 1451205
#* Discriminative topic modeling based on manifold learning
#@ Seungil Huh;Stephen E. Fienberg
#t 2010
#c 0
#% 46803
#% 280819
#% 420083
#% 722904
#% 961218
#% 1130899
#% 1211703
#! Topic modeling has been popularly used for data analysis in various domains including text documents. Previous topic models, such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA), have shown impressive success in discovering low-rank hidden structures for modeling text documents. These models, however, do not take into account the manifold structure of data, which is generally informative for the non-linear dimensionality reduction mapping. More recent models, namely Laplacian PLSI (LapPLSI) and Locally-consistent Topic Model (LTM), have incorporated the local manifold structure into topic models and have shown the resulting benefits. But these approaches fall short of the full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs. In this paper, we propose Discriminative Topic Model (DTM) that separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together, thereby preserving the global manifold structure as well as improving the local consistency. We also present a novel model fitting algorithm based on the generalized EM and the concept of Pareto improvement. As a result, DTM achieves higher classification performance in a semi-supervised setting by effectively exposing the manifold structure of data. We provide empirical evidence on text corpora to demonstrate the success of DTM in terms of classification accuracy and robustness to parameters compared to state-of-the-art techniques.

#index 1451206
#* Online multiscale dynamic topic models
#@ Tomoharu Iwata;Takeshi Yamada;Yasushi Sakurai;Naonori Ueda
#t 2010
#c 0
#% 643007
#% 722904
#% 824709
#% 875959
#% 876067
#% 881498
#% 989623
#% 1073975
#% 1083664
#% 1176853
#% 1275221
#% 1305518
#% 1417055
#% 1650298
#! We propose an online topic model for sequentially analyzing the time evolution of topics in document collections. Topics naturally evolve with multiple timescales. For example, some words may be used consistently over one hundred years, while other words emerge and disappear over periods of a few days. Thus, in the proposed model, current topic-specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch. Considering both the long-timescale dependency as well as the short-timescale dependency yields a more robust model. We derive efficient online inference procedures based on a stochastic EM algorithm, in which the model is sequentially updated using newly obtained data; this means that past data are not required to make the inference. We demonstrate the effectiveness of the proposed method in terms of predictive performance and computational efficiency by examining collections of real documents with timestamps.

#index 1451207
#* Topic models with power-law using Pitman-Yor process
#@ Issei Sato;Hiroshi Nakagawa
#t 2010
#c 0
#% 280819
#% 722904
#% 769906
#% 788094
#% 875959
#% 881534
#% 939624
#% 1083664
#% 1083684
#% 1211725
#% 1328314
#! One important approach for knowledge discovery and data mining is to estimate unobserved variables because latent variables can indicate hidden specific properties of observed data. The latent factor model assumes that each item in a record has a latent factor; the co-occurrence of items can then be modeled by latent factors. In document modeling, a record indicates a document represented as a "bag of words," meaning that the order of words is ignored, an item indicates a word and a latent factor indicates a topic. Latent Dirichlet allocation (LDA) is a widely used Bayesian topic model applying the Dirichlet distribution over the latent topic distribution of a document having multiple topics. LDA assumes that latent topics, i.e., discrete latent variables, are distributed according to a multinomial distribution whose parameters are generated from the Dirichlet distribution. LDA also models a word distribution by using a multinomial distribution whose parameters follows the Dirichlet distribution. This Dirichlet-multinomial setting, however, cannot capture the power-law phenomenon of a word distribution, which is known as Zipf's law in linguistics. We therefore propose a novel topic model using the Pitman-Yor(PY) process, called the PY topic model. The PY topic model captures two properties of a document; a power-law word distribution and the presence of multiple topics. In an experiment using real data, this model outperformed LDA in document modeling in terms of perplexity.

#index 1451208
#* The topic-perspective model for social tagging systems
#@ Caimei Lu;Xiaohua Hu;Xin Chen;Jung-Ran Park;TingTing He;Zhoujun Li
#t 2010
#c 0
#% 642990
#% 722904
#% 788094
#% 855601
#% 869504
#% 881534
#% 905319
#% 946524
#% 1035588
#% 1055743
#% 1074117
#% 1100174
#% 1130816
#% 1130827
#% 1166510
#% 1211773
#% 1214717
#% 1292515
#% 1292643
#% 1318718
#% 1650298
#% 1650387
#! In this paper, we propose a new probabilistic generative model, called Topic-Perspective Model, for simulating the generation process of social annotations. Different from other generative models, in our model, the tag generation process is separated from the content term generation process. While content terms are only generated from resource topics, social tags are generated by resource topics and user perspectives together. The proposed probabilistic model can produce more useful information than any other models proposed before. The parameters learned from this model include: (1) the topical distribution of each document, (2) the perspective distribution of each user, (3) the word distribution of each topic, (4) the tag distribution of each topic, (5) the tag distribution of each user perspective, (6) and the probabilistic of each tag being generated from resource topics or user perspectives. Experimental results show that the proposed model has better generalization performance or tag prediction ability than other two models proposed in previous research.

#index 1451209
#* Combining predictions for accurate recommender systems
#@ Michael Jahrer;Andreas Töscher;Robert Legenstein
#t 2010
#c 0
#% 209021
#% 400847
#% 448194
#% 770854
#% 866298
#% 983903
#% 1083671
#% 1116993
#% 1127483
#% 1214666
#% 1358069
#! We analyze the application of ensemble learning to recommender systems on the Netflix Prize dataset. For our analysis we use a set of diverse state-of-the-art collaborative filtering (CF) algorithms, which include: SVD, Neighborhood Based Approaches, Restricted Boltzmann Machine, Asymmetric Factor Model and Global Effects. We show that linearly combining (blending) a set of CF algorithms increases the accuracy and outperforms any single CF algorithm. Furthermore, we show how to use ensemble methods for blending predictors in order to outperform a single blending algorithm. The dataset and the source code for the ensemble blending are available online.

#index 1451210
#* Fast online learning through offline initialization for time-sensitive recommendation
#@ Deepak Agarwal;Bee-Chung Chen;Pradheep Elango
#t 2010
#c 0
#% 131258
#% 221640
#% 235061
#% 956494
#% 956521
#% 956546
#% 983874
#% 989580
#% 1055713
#% 1127449
#% 1190124
#% 1214623
#% 1214642
#% 1214666
#% 1287221
#% 1396090
#% 1650569
#! Recommender problems with large and dynamic item pools are ubiquitous in web applications like content optimization, online advertising and web search. Despite the availability of rich item meta-data, excess heterogeneity at the item level often requires inclusion of item-specific "factors" (or weights) in the model. However, since estimating item factors is computationally intensive, it poses a challenge for time-sensitive recommender problems where it is important to rapidly learn factors for new items (e.g., news articles, event updates, tweets) in an online fashion. In this paper, we propose a novel method called FOBFM (Fast Online Bilinear Factor Model) to learn item-specific factors quickly through online regression. The online regression for each item can be performed independently and hence the procedure is fast, scalable and easily parallelizable. However, the convergence of these independent regressions can be slow due to high dimensionality. The central idea of our approach is to use a large amount of historical data to initialize the online models based on offline features and learn linear projections that can effectively reduce the dimensionality. We estimate the rank of our linear projections by taking recourse to online model selection based on optimizing predictive likelihood. Through extensive experiments, we show that our method significantly and uniformly outperforms other competitive methods and obtains relative lifts that are in the range of 10-15% in terms of predictive log-likelihood, 200-300% for a rank correlation metric on a proprietary My Yahoo! dataset; it obtains 9% reduction in root mean squared error over the previously best method on a benchmark MovieLens dataset using a time-based train/test data split.

#index 1451211
#* Training and testing of recommender systems on data missing not at random
#@ Harald Steck
#t 2010
#c 0
#% 17144
#% 349550
#% 983903
#% 1038329
#% 1083671
#% 1100072
#% 1176909
#% 1287220
#! Users typically rate only a small fraction of all available items. We show that the absence of ratings carries useful information for improving the top-k hit rate concerning all items, a natural accuracy measure for recommendations. As to test recommender systems, we present two performance measures that can be estimated, under mild assumptions, without bias from data even when ratings are missing not at random (MNAR). As to achieve optimal test results, we present appropriate surrogate objective functions for efficient training on MNAR data. Their main property is to account for all ratings - whether observed or missing in the data. Concerning the top-k hit rate on test data, our experiments indicate dramatic improvements over even sophisticated methods that are optimized on observed ratings only.

#index 1451212
#* Temporal recommendation on graphs via long- and short-term preference fusion
#@ Liang Xiang;Quan Yuan;Shiwan Zhao;Li Chen;Xiatian Zhang;Qing Yang;Jimeng Sun
#t 2010
#c 0
#% 173879
#% 204531
#% 342687
#% 348173
#% 452563
#% 528337
#% 751597
#% 838504
#% 881493
#% 961613
#% 982675
#% 989640
#% 1055761
#% 1074061
#% 1121272
#% 1156304
#% 1176909
#% 1214661
#% 1214666
#% 1214743
#% 1227740
#! Accurately capturing user preferences over time is a great practical challenge in recommender systems. Simple correlation over time is typically not meaningful, since users change their preferences due to different external events. User behavior can often be determined by individual's long-term and short-term preferences. How to represent users' long-term and short-term preferences? How to leverage them for temporal recommendation? To address these challenges, we propose Session-based Temporal Graph (STG) which simultaneously models users' long-term and short-term preferences over time. Based on the STG model framework, we propose a novel recommendation algorithm Injected Preference Fusion (IPF) and extend the personalized Random Walk for temporal recommendation. Finally, we evaluate the effectiveness of our method using two real datasets on citations and social bookmarking, in which our proposed method IPF gives 15%-34% improvement over the previous state-of-the-art.

#index 1451213
#* Generative models for ticket resolution in expert networks
#@ Gengxin Miao;Louise E. Moser;Xifeng Yan;Shu Tao;Yi Chen;Nikos Anerousis
#t 2010
#c 0
#% 280817
#% 458379
#% 465754
#% 730061
#% 868135
#% 879570
#% 940104
#% 1083691
#% 1130922
#% 1176887
#% 1381853
#% 1392465
#% 1704240
#% 1815596
#! Ticket resolution is a critical, yet challenging, aspect of the delivery of IT services. A large service provider needs to handle, on a daily basis, thousands of tickets that report various types of problems. Many of those tickets bounce among multiple expert groups before being transferred to the group with the right expertise to solve the problem. Finding a methodology that reduces such bouncing and hence shortens ticket resolution time is a long-standing challenge. In this paper, we present a unified generative model, the Optimized Network Model (ONM), that characterizes the lifecycle of a ticket, using both the content and the routing sequence of the ticket. ONM uses maximum likelihood estimation, to represent how the information contained in a ticket is used by human experts to make ticket routing decisions. Based on ONM, we develop a probabilistic algorithm to generate ticket routing recommendations for new tickets in a network of expert groups. Our algorithm calculates all possible routes to potential resolvers and makes globally optimal recommendations, in contrast to existing classification methods that make static and locally optimal recommendations. Experiments show that our method significantly outperforms existing solutions.

#index 1451214
#* Learning to combine discriminative classifiers: confidence based
#@ Chi-Hoon Lee
#t 2010
#c 0
#% 135331
#% 209021
#% 231803
#% 342624
#% 448194
#% 578681
#% 642982
#% 853542
#% 876011
#% 926729
#% 1016177
#% 1214652
#% 1214717
#% 1214736
#% 1227577
#% 1250570
#% 1499573
#! Much of research in data mining and machine learning has led to numerous practical applications. Spam filtering, fraud detection, and user query-intent analysis has relied heavily on machine learned classifiers, and resulted in improvements in robust classification accuracy. Combining multiple classifiers (a.k.a. Ensemble Learning) is a well studied and has been known to improve effectiveness of a classifier. To address two key challenges in Ensemble Learning-- (1) learning weights of individual classifiers and (2) the combination rule of their weighted responses, this paper proposes a novel Ensemble classifier, EnLR, that computes weights of responses from discriminative classifiers and combines their weighted responses to produce a single response for a test instance. The combination rule is based on aggregating weighted responses, where a weight of an individual classifier is inversely based on their respective variances around their responses. Here, variance quantifies the uncertainty of the discriminative classifiers' parameters, which in turn depends on the training samples. As opposed to other ensemble methods where the weight of each individual classifier is learned as a part of parameter learning and thus the same weight is applied to all testing instances, our model is actively adjusted as individual classifiers become confident at its decision for a test instance. Our empirical experiments on various data sets demonstrate that our combined classifier produces "effective" results when compared with a single classifier. Our novel classifier shows statistically significant better accuracy when compared to well known Ensemble methods -- Bagging and AdaBoost. In addition to robust accuracy, our model is extremely efficient dealing with high volumes of training samples due to the independent learning paradigm among its multiple classifiers. It is simple to implement in a distributed computing environment such as Hadoop.

#index 1451215
#* Mining positive and negative patterns for relevance feature discovery
#@ Yuefeng Li;Abdulmohsen Algarni;Ning Zhong
#t 2010
#c 0
#% 118731
#% 169806
#% 262096
#% 287253
#% 298183
#% 316709
#% 318412
#% 324129
#% 329537
#% 340901
#% 342707
#% 344447
#% 375017
#% 465895
#% 466101
#% 584888
#% 629623
#% 643005
#% 726856
#% 763708
#% 779877
#% 783474
#% 818236
#% 823356
#% 863392
#% 879588
#% 879595
#% 915323
#% 987227
#% 987231
#% 989594
#% 989627
#% 989634
#% 1019140
#% 1074065
#% 1074078
#% 1074081
#% 1074128
#% 1077150
#% 1083663
#% 1083679
#% 1130853
#% 1130911
#% 1181094
#% 1279298
#% 1555377
#! It is a big challenge to guarantee the quality of discovered relevance features in text documents for describing user preferences because of the large number of terms, patterns, and noise. Most existing popular text mining and classification methods have adopted term-based approaches. However, they have all suffered from the problems of polysemy and synonymy. Over the years, people have often held the hypothesis that pattern-based methods should perform better than term-based ones in describing user preferences, but many experiments do not support this hypothesis. The innovative technique presented in paper makes a breakthrough for this difficulty. This technique discovers both positive and negative patterns in text documents as higher level features in order to accurately weight low-level features (terms) based on their specificity and their distributions in the higher level features. Substantial experiments using this technique on Reuters Corpus Volume 1 and TREC topics show that the proposed approach significantly outperforms both the state-of-the-art term-based methods underpinned by Okapi BM25, Rocchio or Support Vector Machine and pattern based methods on precision, recall and F measures.

#index 1451216
#* Document clustering via dirichlet process mixture model with feature selection
#@ Guan Yu;Ruizhang Huang;Zhaojun Wang
#t 2010
#c 0
#% 311027
#% 329562
#% 772864
#% 840903
#% 875981
#% 893463
#! One essential issue of document clustering is to estimate the appropriate number of clusters for a document collection to which documents should be partitioned. In this paper, we propose a novel approach, namely DPMFS, to address this issue. The proposed approach is designed 1) to group documents into a set of clusters while the number of document clusters is determined by the Dirichlet process mixture model automatically; 2) to identify the discriminative words and separate them from irrelevant noise words via stochastic search variable selection technique. We explore the performance of our proposed approach on both a synthetic dataset and several realistic document datasets. The comparison between our proposed approach and stage-of-the-art document clustering approaches indicates that our approach is robust and effective for document clustering.

#index 1451217
#* Semantic relation extraction with kernels over typed dependency trees
#@ Frank Reichartz;Hannes Korte;Gerhard Paass
#t 2010
#c 0
#% 190581
#% 458379
#% 722803
#% 722926
#% 743284
#% 756964
#% 817472
#% 935763
#% 938706
#% 938713
#% 939944
#% 1039850
#% 1260489
#% 1275182
#% 1289520
#% 1310476
#% 1338672
#% 1499981
#% 1665151
#! An important step for understanding the semantic content of text is the extraction of semantic relations between entities in natural language documents. Automatic extraction techniques have to be able to identify different versions of the same relation which usually may be expressed in a great variety of ways. Therefore these techniques benefit from taking into account many syntactic and semantic features, especially parse trees generated by automatic sentence parsers. Typed dependency parse trees are edge and node labeled parse trees whose labels and topology contains valuable semantic clues. This information can be exploited for relation extraction by the use of kernels over structured data for classification. In this paper we present new tree kernels for relation extraction over typed dependency parse trees. On a public benchmark data set we are able to demonstrate a significant improvement in terms of relation extraction quality of our new kernels over other state-of-the-art kernels.

#index 1451218
#* Latent aspect rating analysis on review text data: a rating regression approach
#@ Hongning Wang;Yue Lu;Chengxiang Zhai
#t 2010
#c 0
#% 309095
#% 420077
#% 465754
#% 577246
#% 577355
#% 722904
#% 769892
#% 805873
#% 854646
#% 879595
#% 907489
#% 939346
#% 939848
#% 939896
#% 1190068
#% 1250356
#% 1260722
#% 1292504
#% 1299754
#! In this paper, we define and study a new opinionated text data analysis problem called Latent Aspect Rating Analysis (LARA), which aims at analyzing opinions expressed about an entity in an online review at the level of topical aspects to discover each individual reviewer's latent opinion on each aspect as well as the relative emphasis on different aspects when forming the overall judgment of the entity. We propose a novel probabilistic rating regression model to solve this new text mining problem in a general way. Empirical experiments on a hotel review data set show that the proposed latent rating regression model can effectively solve the problem of LARA, and that the detailed analysis of opinions at the level of topical aspects enabled by the proposed model can support a wide range of application tasks, such as aspect opinion summarization, entity ranking based on aspect ratings, and analysis of reviewers rating behavior.

#index 1451219
#* Semi-supervised feature selection for graph classification
#@ Xiangnan Kong;Philip S. Yu
#t 2010
#c 0
#% 466644
#% 478274
#% 629603
#% 629708
#% 727845
#% 769951
#% 829025
#% 1063502
#% 1411129
#! The problem of graph classification has attracted great interest in the last decade. Current research on graph classification assumes the existence of large amounts of labeled training graphs. However, in many applications, the labels of graph data are very expensive or difficult to obtain, while there are often copious amounts of unlabeled graph data available. In this paper, we study the problem of semi-supervised feature selection for graph classification and propose a novel solution, called gSSC, to efficiently search for optimal subgraph features with labeled and unlabeled graphs. Different from existing feature selection methods in vector spaces which assume the feature set is given, we perform semi-supervised feature selection for graph data in a progressive way together with the subgraph feature mining process. We derive a feature evaluation criterion, named gSemi, to estimate the usefulness of subgraph features based upon both labeled and unlabeled graphs. Then we propose a branch-and-bound algorithm to efficiently search for optimal subgraph features by judiciously pruning the subgraph search space. Empirical studies on several real-world tasks demonstrate that our semi-supervised feature selection approach can effectively boost graph classification performances with semi-supervised feature selection and is very efficient by pruning the subgraph search space using both labeled and unlabeled graphs.

#index 1451220
#* Modeling relational events via latent classes
#@ Christopher DuBois;Padhraic Smyth
#t 2010
#c 0
#% 918685
#% 1083675
#% 1117695
#% 1250567
#% 1254832
#% 1312988
#% 1377383
#! Many social networks can be characterized by a sequence of dyadic interactions between individuals. Techniques for analyzing such events are of increasing interest. In this paper, we describe a generative model for dyadic events, where each event arises from one of C latent classes, and the properties of the event (sender, recipient, and type) are chosen from distributions over these entities conditioned on the chosen class. We present two algorithms for inference in this model: an expectation-maximization algorithm as well as a Markov chain Monte Carlo procedure based on collapsed Gibbs sampling. To analyze the model's predictive accuracy, the algorithms are applied to multiple real-world data sets involving email communication, international political events, and animal behavior data.

#index 1451221
#* On community outliers and their efficient detection in information networks
#@ Jing Gao;Feng Liang;Wei Fan;Chi Wang;Yizhou Sun;Jiawei Han
#t 2010
#c 0
#% 191603
#% 274612
#% 290830
#% 300136
#% 313959
#% 342638
#% 466745
#% 570886
#% 577273
#% 729983
#% 731721
#% 769881
#% 785358
#% 812382
#% 844334
#% 916785
#% 975040
#% 989618
#% 989654
#% 995140
#% 1000502
#% 1063629
#% 1130929
#% 1214714
#% 1318671
#! Linked or networked data are ubiquitous in many applications. Examples include web data or hypertext documents connected via hyperlinks, social networks or user profiles connected via friend links, co-authorship and citation information, blog data, movie reviews and so on. In these datasets (called "information networks"), closely related objects that share the same properties or interests form a community. For example, a community in blogsphere could be users mostly interested in cell phone reviews and news. Outlier detection in information networks can reveal important anomalous and interesting behaviors that are not obvious if community information is ignored. An example could be a low-income person being friends with many rich people even though his income is not anomalously low when considered over the entire population. This paper first introduces the concept of community outliers (interesting points or rising stars for a more positive sense), and then shows that well-known baseline approaches without considering links or community information cannot find these community outliers. We propose an efficient solution by modeling networked data as a mixture model composed of multiple normal communities and a set of randomly generated outliers. The probabilistic model characterizes both data and links simultaneously by defining their joint distribution based on hidden Markov random fields (HMRF). Maximizing the data likelihood and the posterior of the model gives the solution to the outlier inference problem. We apply the model on both synthetic data and DBLP data sets, and the results demonstrate importance of this concept, as well as the effectiveness and efficiency of the proposed approach.

#index 1451222
#* Redefining class definitions using constraint-based clustering: an application to remote sensing of the earth's surface
#@ Dan R. Preston;Carla E. Brodley;Roni Khardon;Damien Sulla-Menashe;Mark Friedl
#t 2010
#c 0
#% 232106
#% 424177
#% 464291
#% 464631
#% 855342
#% 959467
#% 995140
#% 1085668
#% 1156095
#! Two aspects are crucial when constructing any real world supervised classification task: the set of classes whose distinction might be useful for the domain expert, and the set of classifications that can actually be distinguished by the data. Often a set of labels is defined with some initial intuition but these are not the best match for the task. For example, labels have been assigned for land cover classification of the Earth but it has been suspected that these labels are not ideal and some classes may be best split into subclasses whereas others should be merged. This paper formalizes this problem using three ingredients: the existing class labels, the underlying separability in the data, and a special type of input from the domain expert. We require a domain expert to specify an L × L matrix of pairwise probabilistic constraints expressing their beliefs as to whether the L classes should be kept separate, merged, or split. This type of input is intuitive and easy for experts to supply. We then show that the problem can be solved by casting it as an instance of penalized probabilistic clustering (PPC). Our method, Class-Level PPC (CPPC) extends PPC showing how its time complexity can be reduced from O(N2) to O(NL) for the problem of class re-definition. We further extend the algorithm by presenting a heuristic to measure adherence to constraints, and providing a criterion for determining the model complexity (number of classes) for constraint-based clustering. We demonstrate and evaluate CPPC on artificial data and on our motivating domain of land cover classification. For the latter, an evaluation by domain experts shows that the algorithm discovers novel class definitions that are better suited to land cover classification than the original set of labels.

#index 1451223
#* Large linear classification when data cannot fit in memory
#@ Hsiang-Fu Yu;Cho-Jui Hsieh;Kai-Wei Chang;Chih-Jen Lin
#t 2010
#c 0
#% 131165
#% 269217
#% 562950
#% 729940
#% 825642
#% 881477
#% 983905
#% 1073923
#% 1117691
#% 1232034
#% 1305483
#% 1318710
#! Recent advances in linear classification have shown that for applications such as document classification, the training can be extremely efficient. However, most of the existing training methods are designed by assuming that data can be stored in the computer memory. These methods cannot be easily applied to data larger than the memory capacity due to the random access to the disk. We propose and analyze a block minimization framework for data larger than the memory size. At each step a block of data is loaded from the disk and handled by certain learning methods. We investigate two implementations of the proposed framework for primal and dual SVMs, respectively. As data cannot fit in memory, many design considerations are very different from those for traditional algorithms. Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method.

#index 1451224
#* Class-specific error bounds for ensemble classifiers
#@ Ryan J. Prenger;Tracy D. Lemmond;Kush R. Varshney;Barry Y. Chen;William G. Hanley
#t 2010
#c 0
#% 400847
#% 466268
#% 629583
#% 983880
#% 1260981
#! The generalization error, or probability of misclassification, of ensemble classifiers has been shown to be bounded above by a function of the mean correlation between the constituent (i.e., base) classifiers and their average strength. This bound suggests that increasing the strength and/or decreasing the correlation of an ensemble's base classifiers may yield improved performance under the assumption of equal error costs. However, this and other existing bounds do not directly address application spaces in which error costs are inherently unequal. For applications involving binary classification, Receiver Operating Characteristic (ROC) curves, performance curves that explicitly trade off false alarms and missed detections, are often utilized to support decision making. To address performance optimization in this context, we have developed a lower bound for the entire ROC curve that can be expressed in terms of the class-specific strength and correlation of the base classifiers. We present empirical analyses demonstrating the efficacy of these bounds in predicting relative classifier performance. In addition, we specify performance regions of the ROC curve that are naturally delineated by the class-specific strengths of the base classifiers and show that each of these regions can be associated with a unique set of guidelines for performance optimization of binary classifiers within unequal error cost regimes.

#index 1451225
#* Designing efficient cascaded classifiers: tradeoff between accuracy and cost
#@ Vikas C. Raykar;Balaji Krishnapuram;Shipeng Yu
#t 2010
#c 0
#% 420507
#% 812497
#% 844399
#% 942925
#% 989635
#% 989679
#! We propose a method to train a cascade of classifiers by simultaneously optimizing all its stages. The approach relies on the idea of optimizing soft cascades. In particular, instead of optimizing a deterministic hard cascade, we optimize a stochastic soft cascade where each stage accepts or rejects samples according to a probability distribution induced by the previous stage-specific classifier. The overall system accuracy is maximized while explicitly controlling the expected cost for feature acquisition. Experimental results on three clinically relevant problems show the effectiveness of our proposed approach in achieving the desired tradeoff between accuracy and feature acquisition cost.

#index 1451226
#* Direct mining of discriminative patterns for classifying uncertain data
#@ Chuancong Gao;Jianyong Wang
#t 2010
#c 0
#% 136350
#% 458257
#% 466483
#% 729418
#% 810064
#% 902452
#% 1030778
#% 1083649
#% 1206650
#% 1206933
#% 1206939
#% 1214624
#% 1214633
#% 1292501
#% 1393138
#% 1411036
#! Classification is one of the most essential tasks in data mining. Unlike other methods, associative classification tries to find all the frequent patterns existing in the input categorical data satisfying a user-specified minimum support and/or other discrimination measures like minimum confidence or information-gain. Those patterns are used later either as rules for rule-based classifier or training features for support vector machine (SVM) classifier, after a feature selection procedure which usually tries to cover as many as the input instances with the most discriminative patterns in various manners. Several algorithms have also been proposed to mine the most discriminative patterns directly without costly feature selection. Previous empirical results show that associative classification could provide better classification accuracy over many datasets. Recently, many studies have been conducted on uncertain data, where fields of uncertain attributes no longer have certain values. Instead probability distribution functions are adopted to represent the possible values and their corresponding probabilities. The uncertainty is usually caused by noise, measurement limits, or other possible factors. Several algorithms have been proposed to solve the classification problem on uncertain data recently, for example by extending traditional rule-based classifier and decision tree to work on uncertain data. In this paper, we propose a novel algorithm uHARMONY which mines discriminative patterns directly and effectively from uncertain data as classification features/rules, to help train either SVM or rule-based classifier. Since patterns are discovered directly from the input database, feature selection usually taking a great amount of time could be avoided completely. Effective method for computation of expected confidence of the mined patterns used as the measurement of discrimination is also proposed. Empirical results show that using SVM classifier our algorithm uHARMONY outperforms the state-of-the-art uncertain data classification algorithms significantly with 4% to 10% improvements on average in accuracy on 30 categorical datasets under varying uncertain degree and uncertain attribute number.

#index 1451227
#* Ensemble pruning via individual contribution ordering
#@ Zhenyu Lu;Xindong Wu;Xingquan Zhu;Josh Bongard
#t 2010
#c 0
#% 132938
#% 136350
#% 209021
#% 235377
#% 256615
#% 312727
#% 400847
#% 400985
#% 451221
#% 458196
#% 551723
#% 565528
#% 727888
#% 875965
#% 876021
#% 915214
#% 926881
#% 961181
#% 1023380
#% 1164190
#% 1214635
#% 1223456
#% 1408623
#! An ensemble is a set of learned models that make decisions collectively. Although an ensemble is usually more accurate than a single learner, existing ensemble methods often tend to construct unnecessarily large ensembles, which increases the memory consumption and computational cost. Ensemble pruning tackles this problem by selecting a subset of ensemble members to form subensembles that are subject to less resource consumption and response time with accuracy that is similar to or better than the original ensemble. In this paper, we analyze the accuracy/diversity trade-off and prove that classifiers that are more accurate and make more predictions in the minority group are more important for subensemble construction. Based on the gained insights, a heuristic metric that considers both accuracy and diversity is proposed to explicitly evaluate each individual classifier's contribution to the whole ensemble. By incorporating ensemble members in decreasing order of their contributions, subensembles are formed such that users can select the top $p$ percent of ensemble members, depending on their resource availability and tolerable waiting time, for predictions. Experimental results on 26 UCI data sets show that subensembles formed by the proposed EPIC (Ensemble Pruning via Individual Contribution ordering) algorithm outperform the original ensemble and a state-of-the-art ensemble pruning method, Orientation Ordering (OO).

#index 1451228
#* Fast query execution for retrieval models based on path-constrained random walks
#@ Ni Lao;William W. Cohen
#t 2010
#c 0
#% 510723
#% 641979
#% 660011
#% 805896
#% 879568
#% 915344
#% 960259
#% 983808
#% 993987
#% 1127445
#% 1227635
#% 1264812
#% 1289460
#% 1663624
#! Many recommendation and retrieval tasks can be represented as proximity queries on a labeled directed graph, with typed nodes representing documents, terms, and metadata, and labeled edges representing the relationships between them. Recent work has shown that the accuracy of the widely-used random-walk-based proximity measures can be improved by supervised learning - in particular, one especially effective learning technique is based on Path-Constrained Random Walks (PCRW), in which similarity is defined by a learned combination of constrained random walkers, each constrained to follow only a particular sequence of edge labels away from the query nodes. The PCRW based method significantly outperformed unsupervised random walk based queries, and models with learned edge weights. Unfortunately, PCRW query systems are expensive to evaluate. In this study we evaluate the use of approximations to the computation of the PCRW distributions, including fingerprinting, particle filtering, and truncation strategies. In experiments on several recommendation and retrieval problems using two large scientific publications corpora we show speedups of factors of 2 to 100 with little loss in accuracy.

#index 1451229
#* Trust network inference for online rating data using generative models
#@ Freddy Chong Tat Chua;Ee-Peng Lim
#t 2010
#c 0
#% 44876
#% 722904
#% 730089
#% 734590
#% 754098
#% 842605
#% 875974
#% 1001279
#% 1190129
#% 1190130
#% 1214638
#% 1214661
#% 1269378
#% 1269889
#% 1399997
#% 1400002
#% 1668087
#! In an online rating system, raters assign ratings to objects contributed by other users. In addition, raters can develop trust and distrust on object contributors depending on a few rating and trust related factors. Previous study has shown that ratings and trust links can influence each other but there has been a lack of a formal model to relate these factors together. In this paper, we therefore propose Trust Antecedent Factor (TAF) Model, a novel probabilistic model that generate ratings based on a number of rater's and contributor's factors. We demonstrate that parameters of the model can be learnt by Collapsed Gibbs Sampling. We then apply the model to predict trust and distrust between raters and review contributors using a real data-set. Our experiments have shown that the proposed model is capable of predicting both trust and distrust in a unified way. The model can also determine user factors which otherwise cannot be observed from the rating and trust data.

#index 1451230
#* An energy-efficient mobile recommender system
#@ Yong Ge;Hui Xiong;Alexander Tuzhilin;Keli Xiao;Marco Gruteser;Michael Pazzani
#t 2010
#c 0
#% 140998
#% 245087
#% 297236
#% 304425
#% 339003
#% 428272
#% 465167
#% 480671
#% 806212
#% 813966
#% 825128
#% 939312
#% 1061913
#% 1122549
#% 1298913
#% 1781298
#! The increasing availability of large-scale location traces creates unprecedent opportunities to change the paradigm for knowledge discovery in transportation systems. A particularly promising area is to extract energy-efficient transportation patterns (green knowledge), which can be used as guidance for reducing inefficiencies in energy consumption of transportation sectors. However, extracting green knowledge from location traces is not a trivial task. Conventional data analysis tools are usually not customized for handling the massive quantity, complex, dynamic, and distributed nature of location traces. To that end, in this paper, we provide a focused study of extracting energy-efficient transportation patterns from location traces. Specifically, we have the initial focus on a sequence of mobile recommendations. As a case study, we develop a mobile recommender system which has the ability in recommending a sequence of pick-up points for taxi drivers or a sequence of potential parking positions. The goal of this mobile recommendation system is to maximize the probability of business success. Along this line, we provide a Potential Travel Distance (PTD) function for evaluating each candidate sequence. This PTD function possesses a monotone property which can be used to effectively prune the search space. Based on this PTD function, we develop two algorithms, LCP and SkyRoute, for finding the recommended routes. Finally, experimental results show that the proposed system can provide effective mobile sequential recommendation and the knowledge extracted from location traces can be used for coaching drivers and leading to the efficient use of energy.

#index 1451231
#* Mixture models for learning low-dimensional roles in high-dimensional data
#@ Manas Somaiya;Christopher Jermaine;Sanjay Ranka
#t 2010
#c 0
#% 248792
#% 273891
#% 310512
#% 342594
#% 722904
#% 798509
#% 1021188
#% 1073921
#! Archived data often describe entities that participate in multiple roles. Each of these roles may influence various aspects of the data. For example, a register transaction collected at a retail store may have been initiated by a person who is a woman, a mother, an avid reader, and an action movie fan. Each of these roles can influence various aspects of the customer's purchase: the fact that the customer is a mother may greatly influence the purchase of a toddler-sized pair of pants, but have no influence on the purchase of an action-adventure novel. The fact that the customer is an action move fan and an avid reader may influence the purchase of the novel, but will have no effect on the purchase of a shirt. In this paper, we present a generic, Bayesian framework for capturing exactly this situation. In our framework, it is assumed that multiple roles exist, and each data point corresponds to an entity (such as a retail customer, or an email, or a news article) that selects various roles which compete to influence the various attributes associated with the data point. We develop robust, MCMC algorithms for learning the models under the framework.

#index 1451232
#* Towards mobility-based clustering
#@ Siyuan Liu;Yunhuai Liu;Lionel M. Ni;Jianping Fan;Minglu Li
#t 2010
#c 0
#% 210173
#% 280416
#% 379241
#% 387427
#% 420078
#% 443531
#% 769946
#% 874892
#% 881514
#% 960389
#% 975358
#% 989584
#% 1063595
#% 1127375
#% 1176860
#% 1176881
#% 1176919
#% 1176923
#% 1176952
#% 1176975
#% 1206640
#% 1290947
#% 1409360
#! Identifying hot spots of moving vehicles in an urban area is essential to many smart city applications. The practical research on hot spots in smart city presents many unique features, such as highly mobile environments, supremely limited size of sample objects, and the non-uniform, biased samples. All these features have raised new challenges that make the traditional density-based clustering algorithms fail to capture the real clustering property of objects, making the results less meaningful. In this paper we propose a novel, non-density-based approach called mobility-based clustering. The key idea is that sample objects are employed as "sensors" to perceive the vehicle crowdedness in nearby areas using their instant mobility, rather than the "object representatives". As such the mobility of samples is naturally incorporated. Several key factors beyond the vehicle crowdedness have been identified and techniques to compensate these effects are proposed. We evaluate the performance of mobility-based clustering based on real traffic situations. Experimental results show that using 0.3% of vehicles as the samples, mobility-based clustering can accurately identify hot spots which can hardly be obtained by the latest representative algorithm UMicro.

#index 1451233
#* PET: a statistical model for popular events tracking in social communities
#@ Cindy Xide Lin;Bo Zhao;Qiaozhu Mei;Jiawei Han
#t 2010
#c 0
#% 338741
#% 342707
#% 577220
#% 722904
#% 729943
#% 754107
#% 769967
#% 823344
#% 824666
#% 868469
#% 881460
#% 881476
#% 907511
#% 989650
#% 1055681
#% 1083732
#% 1214669
#% 1269909
#% 1292518
#% 1318691
#% 1399993
#% 1650298
#% 1655304
#! User generated information in online communities has been characterized with the mixture of a text stream and a network structure both changing over time. A good example is a web-blogging community with the daily blog posts and a social network of bloggers. An important task of analyzing an online community is to observe and track the popular events, or topics that evolve over time in the community. Existing approaches usually focus on either the burstiness of topics or the evolution of networks, but ignoring the interplay between textual topics and network structures. In this paper, we formally define the problem of popular event tracking in online communities (PET), focusing on the interplay between texts and networks. We propose a novel statistical method that models the the popularity of events over time, taking into consideration the burstiness of user interest, information diffusion on the network structure, and the evolution of textual topics. Specifically, a Gibbs Random Field is defined to model the influence of historic status and the dependency relationships in the graph; thereafter a topic model generates the words in text content of the event, regularized by the Gibbs Random Field. We prove that two classic models in information diffusion and text burstiness are special cases of our model under certain situations. Empirical experiments with two different communities and datasets (i.e., Twitter and DBLP) show that our approach is effective and outperforms existing approaches.

#index 1451234
#* The community-search problem and how to plan a successful cocktail party
#@ Mauro Sozio;Aristides Gionis
#t 2010
#c 0
#% 274612
#% 310514
#% 438553
#% 480743
#% 511151
#% 769887
#% 824711
#% 881496
#% 898311
#% 956540
#% 1001363
#% 1034723
#% 1055741
#% 1207029
#% 1214629
#% 1214668
#% 1221417
#% 1292670
#% 1692830
#! A lot of research in graph mining has been devoted in the discovery of communities. Most of the work has focused in the scenario where communities need to be discovered with only reference to the input graph. However, for many interesting applications one is interested in finding the community formed by a given set of nodes. In this paper we study a query-dependent variant of the community-detection problem, which we call the community-search problem: given a graph G, and a set of query nodes in the graph, we seek to find a subgraph of G that contains the query nodes and it is densely connected. We motivate a measure of density based on minimum degree and distance constraints, and we develop an optimum greedy algorithm for this measure. We proceed by characterizing a class of monotone constraints and we generalize our algorithm to compute optimum solutions satisfying any set of monotone constraints. Finally we modify the greedy algorithm and we present two heuristic algorithms that find communities of size no greater than a specified upper bound. Our experimental evaluation on real datasets demonstrates the efficiency of the proposed algorithms and the quality of the solutions we obtain.

#index 1451235
#* Growing a tree in the forest: constructing folksonomies by integrating structured metadata
#@ Anon Plangprasopchok;Kristina Lerman;Lise Getoor
#t 2010
#c 0
#% 280849
#% 459483
#% 756964
#% 855601
#% 869525
#% 924747
#% 937552
#% 939601
#% 946524
#% 960271
#% 987205
#% 1190133
#% 1270263
#% 1328333
#! Many social Web sites allow users to annotate the content with descriptive metadata, such as tags, and more recently to organize content hierarchically. These types of structured metadata provide valuable evidence for learning how a community organizes knowledge. For instance, we can aggregate many personal hierarchies into a common taxonomy, also known as a folksonomy, that will aid users in visualizing and browsing social content, and also to help them in organizing their own content. However, learning from social metadata presents several challenges, since it is sparse, shallow, ambiguous, noisy, and inconsistent. We describe an approach to folksonomy learning based on relational clustering, which exploits structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree. We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr, and demonstrate that the proposed approach addresses the challenges. Moreover, comparing to previous work, the approach produces larger, more accurate folksonomies, and in addition, scales better.

#index 1451236
#* A probabilistic model for personalized tag prediction
#@ Dawei Yin;Zhenzhen Xue;Liangjie Hong;Brian D. Davison
#t 2010
#c 0
#% 531952
#% 956544
#% 1074070
#% 1074117
#% 1083671
#% 1100174
#% 1127455
#% 1127458
#% 1130816
#% 1131157
#% 1166510
#% 1190237
#% 1192277
#% 1214666
#% 1214694
#% 1214717
#% 1227644
#% 1355024
#% 1667787
#! Social tagging systems have become increasingly popular for sharing and organizing web resources. Tag prediction is a common feature of social tagging systems. Social tagging by nature is an incremental process, meaning that once a user has saved a web page with tags, the tagging system can provide more accurate predictions for the user, based on user's incremental behaviors. However, existing tag prediction methods do not consider this important factor, in which their training and test datasets are either split by a fixed time stamp or randomly sampled from a larger corpus. In our temporal experiments, we perform a time-sensitive sampling on an existing public dataset, resulting in a new scenario which is much closer to "real-world". In this paper, we address the problem of tag prediction by proposing a probabilistic model for personalized tag prediction. The model is a Bayesian approach, and integrates three factors - ego-centric effect, environmental effects and web page content. Two methods - both intuitive calculation and learning optimization - are provided for parameter estimation. Pure graphbased methods which may have significant constraints (such as every user, every item and every tag has to occur in at least p posts), cannot make a prediction in most of "real world" cases while our model improves the F-measure by over 30% compared to a leading algorithm, in our "real-world" use case.

#index 1451237
#* BioSnowball: automated population of Wikis
#@ Xiaojiang Liu;Zaiqing Nie;Nenghai Yu;Ji-Rong Wen
#t 2010
#c 0
#% 262112
#% 301241
#% 464434
#% 504443
#% 815131
#% 838060
#% 846586
#% 881505
#% 915340
#% 946522
#% 983808
#% 1019061
#% 1055735
#% 1190065
#% 1190118
#% 1260698
#% 1264785
#% 1269496
#% 1275040
#% 1275182
#% 1289520
#% 1291356
#! Internet users regularly have the need to find biographies and facts of people of interest. Wikipedia has become the first stop for celebrity biographies and facts. However, Wikipedia can only provide information for celebrities because of its neutral point of view (NPOV) editorial policy. In this paper we propose an integrated bootstrapping framework named BioSnowball to automatically summarize the Web to generate Wikipedia-style pages for any person with a modest web presence. In BioSnowball, biography ranking and fact extraction are performed together in a single integrated training and inference process using Markov Logic Networks (MLNs) as its underlying statistical model. The bootstrapping framework starts with only a small number of seeds and iteratively finds new facts and biographies. As biography paragraphs on the Web are composed of the most important facts, our joint summarization model can improve the accuracy of both fact extraction and biography ranking compared to decoupled methods in the literature. Empirical results on both a small labeled data set and a real Web-scale data set show the effectiveness of BioSnowball. We also empirically show that BioSnowball outperforms the decoupled methods.

#index 1451238
#* Combined regression and ranking
#@ D. Sculley
#t 2010
#c 0
#% 577224
#% 734915
#% 763708
#% 840846
#% 840882
#% 868445
#% 875954
#% 881477
#% 891559
#% 983820
#% 983905
#% 987226
#% 987241
#% 1014657
#% 1035575
#% 1055694
#% 1055713
#% 1073906
#% 1214754
#% 1232034
#% 1378224
#! Many real-world data mining tasks require the achievement of two distinct goals when applied to unseen data: first, to induce an accurate preference ranking, and second to give good regression performance. In this paper, we give an efficient and effective Combined Regression and Ranking method (CRR) that optimizes regression and ranking objectives simultaneously. We demonstrate the effectiveness of CRR for both families of metrics on a range of large-scale tasks, including click prediction for online advertisements. Results show that CRR often achieves performance equivalent to the best of both ranking-only and regression-only approaches. In the case of rare events or skewed distributions, we also find that this combination can actually improve regression performance due to the addition of informative ranking constraints.

#index 1451239
#* Mass estimation and its applications
#@ Kai Ming Ting;Guang-Tong Zhou;Fei Tony Liu;James Swee Chuan Tan
#t 2010
#c 0
#% 300136
#% 729437
#% 729912
#% 780688
#% 879447
#% 915338
#% 1176944
#% 1432572
#! This paper introduces mass estimation--a base modelling mechanism in data mining. It provides the theoretical basis of mass and an efficient method to estimate mass. We show that it solves problems very effectively in tasks such as information retrieval, regression and anomaly detection. The models, which use mass in these three tasks, perform at least as good as and often better than a total of eight state-of-the-art methods in terms of task-specific performance measures. In addition, mass estimation has constant time and space complexities.

#index 1451240
#* Multi-label learning by exploiting label dependency
#@ Min-Ling Zhang;Kun Zhang
#t 2010
#c 0
#% 115608
#% 311034
#% 465754
#% 478470
#% 818236
#% 838412
#% 889101
#% 950571
#% 989655
#% 997067
#% 1083666
#% 1095861
#% 1100077
#% 1176915
#% 1264044
#% 1388992
#% 1417383
#! In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the conditional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.

#index 1451241
#* DivRank: the interplay of prestige and diversity in information networks
#@ Qiaozhu Mei;Jian Guo;Dragomir Radev
#t 2010
#c 0
#% 262112
#% 290830
#% 348173
#% 397133
#% 592143
#% 642975
#% 805841
#% 816173
#% 818266
#% 840903
#% 840965
#% 1074127
#% 1074133
#% 1166473
#% 1190093
#% 1272053
#% 1312812
#! Information networks are widely used to characterize the relationships between data items such as text documents. Many important retrieval and mining tasks rely on ranking the data items based on their centrality or prestige in the network. Beyond prestige, diversity has been recognized as a crucial objective in ranking, aiming at providing a non-redundant and high coverage piece of information in the top ranked results. Nevertheless, existing network-based ranking approaches either disregard the concern of diversity, or handle it with non-optimized heuristics, usually based on greedy vertex selection. We propose a novel ranking algorithm, DivRank, based on a reinforced random walk in an information network. This model automatically balances the prestige and the diversity of the top ranked vertices in a principled way. DivRank not only has a clear optimization explanation, but also well connects to classical models in mathematics and network science. We evaluate DivRank using empirical experiments on three different networks as well as a text summarization task. DivRank outperforms existing network-based ranking methods in terms of enhancing diversity in prestige.

#index 1451242
#* Inferring networks of diffusion and influence
#@ Manuel Gomez Rodriguez;Jure Leskovec;Andreas Krause
#t 2010
#c 0
#% 297675
#% 729923
#% 754107
#% 786841
#% 823342
#% 832271
#% 868469
#% 983866
#% 989613
#% 1055741
#% 1214671
#% 1650289
#% 1669913
#! Information diffusion and virus propagation are fundamental processes talking place in networks. While it is often possible to directly observe when nodes become infected, observing individual transmissions (i.e., who infects whom or who influences whom) is typically very difficult. Furthermore, in many applications, the underlying network over which the diffusions and propagations spread is actually unobserved. We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate. Given the times when nodes adopt pieces of information or become infected, we identify the optimal network that best explains the observed infection times. Since the optimization problem is NP-hard to solve exactly, we develop an efficient approximation algorithm that scales to large datasets and in practice gives provably near-optimal performance. We demonstrate the effectiveness of our approach by tracing information cascades in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space. We find that the diffusion network of news tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web. These sites tend to have stable circles of influence with more general news media sites acting as connectors between them.

#index 1451243
#* Scalable influence maximization for prevalent viral marketing in large-scale social networks
#@ Wei Chen;Chi Wang;Yajun Wang
#t 2010
#c 0
#% 256685
#% 268079
#% 300079
#% 341672
#% 342596
#% 577217
#% 729923
#% 754107
#% 989613
#% 1214641
#% 1214702
#% 1663638
#! Influence maximization, defined by Kempe, Kleinberg, and Tardos (2003), is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models. The scalability of influence maximization is a key factor for enabling prevalent viral marketing in large-scale online social networks. Prior solutions, such as the greedy algorithm of Kempe et al. (2003) and its improvements are slow and not scalable, while other heuristic algorithms do not provide consistently good performance on influence spreads. In this paper, we design a new heuristic algorithm that is easily scalable to millions of nodes and edges in our experiments. Our algorithm has a simple tunable parameter for users to control the balance between the running time and the influence spread of the algorithm. Our results from extensive simulations on several real-world and synthetic networks demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem: (a) our algorithm scales beyond million-sized graphs where the greedy algorithm becomes infeasible, and (b) in all size ranges, our algorithm performs consistently well in influence spread --- it is always among the best algorithms, and in most cases it significantly outperforms all other scalable heuristics to as much as 100%--260% increase in influence spread.

#index 1451244
#* Community-based greedy algorithm for mining top-K influential nodes in mobile social networks
#@ Yu Wang;Gao Cong;Guojie Song;Kunqing Xie
#t 2010
#c 0
#% 313959
#% 342596
#% 729923
#% 989613
#% 989654
#% 1040834
#% 1040840
#% 1117075
#% 1130830
#% 1190222
#% 1214641
#% 1269888
#% 1318719
#% 1663638
#% 1676017
#! With the proliferation of mobile devices and wireless technologies, mobile social network systems are increasingly available. A mobile social network plays an essential role as the spread of information and influence in the form of "word-of-mouth". It is a fundamental issue to find a subset of influential individuals in a mobile social network such that targeting them initially (e.g. to adopt a new product) will maximize the spread of the influence (further adoptions of the new product). The problem of finding the most influential nodes is unfortunately NP-hard. It has been shown that a Greedy algorithm with provable approximation guarantees can give good approximation; However, it is computationally expensive, if not prohibitive, to run the greedy algorithm on a large mobile network. In this paper we propose a new algorithm called Community-based Greedy algorithm for mining top-K influential nodes. The proposed algorithm encompasses two components: 1) an algorithm for detecting communities in a social network by taking into account information diffusion; and 2) a dynamic programming algorithm for selecting communities to find influential nodes. We also provide provable approximation guarantees for our algorithm. Empirical studies on a large real-world mobile social network show that our algorithm is more than an order of magnitudes faster than the state-of-the-art Greedy algorithm for finding top-K influential nodes and the error of our approximate algorithm is small.

#index 1451245
#* Social action tracking via noise tolerant time-varying factor graphs
#@ Chenhao Tan;Jie Tang;Jimeng Sun;Quan Lin;Fengjiao Wang
#t 2010
#c 0
#% 246836
#% 283833
#% 342596
#% 351347
#% 464434
#% 528019
#% 577217
#% 729923
#% 754107
#% 853535
#% 1035581
#% 1055737
#% 1083624
#% 1083641
#% 1083675
#% 1083734
#% 1190127
#% 1214658
#% 1214696
#% 1214699
#% 1214702
#% 1214703
#% 1214722
#% 1355040
#! It is well known that users' behaviors (actions) in a social network are influenced by various factors such as personal interests, social influence, and global trends. However, few publications systematically study how social actions evolve in a dynamic social network and to what extent different factors affect the user actions. In this paper, we propose a Noise Tolerant Time-varying Factor Graph Model (NTT-FGM) for modeling and predicting social actions. NTT-FGM simultaneously models social network structure, user attributes and user action history for better prediction of the users' future actions. More specifically, a user's action at time t is generated by her latent state at t, which is influenced by her attributes, her own latent state at time t-1 and her neighbors' states at time t and t-1. Based on this intuition, we formalize the social action tracking problem using the NTT-FGM model; then present an efficient algorithm to learn the model, by combining the ideas from both continuous linear system and Markov random field. Finally, we present a case study of our model on predicting future social actions. We validate the model on three different types of real-world data sets. Qualitatively, our model can uncover some interesting patterns of the social dynamics. Quantitatively, experimental results show that the proposed method outperforms several baseline methods for action prediction.

#index 1451246
#* Finding effectors in social networks
#@ Theodoros Lappas;Evimaria Terzi;Dimitrios Gunopulos;Heikki Mannila
#t 2010
#c 0
#% 299247
#% 342596
#% 729923
#% 769955
#% 794513
#% 809250
#% 881481
#% 902759
#% 989613
#% 991977
#% 1035589
#% 1181273
#% 1407359
#% 1845595
#! Assume a network (V,E) where a subset of the nodes in V are active. We consider the problem of selecting a set of k active nodes that best explain the observed activation state, under a given information-propagation model. We call these nodes effectors. We formally define the k-Effectors problem and study its complexity for different types of graphs. We show that for arbitrary graphs the problem is not only NP-hard to solve optimally, but also NP-hard to approximate. We also show that, for some special cases, the problem can be solved optimally in polynomial time using a dynamic-programming algorithm. To the best of our knowledge, this is the first work to consider the k-Effectors problem in networks. We experimentally evaluate our algorithms using the DBLP co-authorship graph, where we search for effectors of topics that appear in research papers.

#index 1451247
#* GLS-SOD: a generalized local statistical approach for spatial outlier detection
#@ Feng Chen;Chang-Tien Lu;Arnold P. Boedihardjo
#t 2010
#c 0
#% 574284
#% 727847
#% 757953
#% 785358
#% 998572
#% 1089663
#! Local based approach is a major category of methods for spatial outlier detection (SOD). Currently, there is a lack of systematic analysis on the statistical properties of this framework. For example, most methods assume identical and independent normal distributions (i.i.d. normal) for the calculated local differences, but no justifications for this critical assumption have been presented. The methods' detection performance on geostatistic data with linear or nonlinear trend is also not well studied. In addition, there is a lack of theoretical connections and empirical comparisons between local and global based SOD approaches. This paper discusses all these fundamental issues under the proposed Generalized Local Statistical (GLS) framework. Furthermore, robust estimation and outlier detection methods are designed for the new GLS model. Extensive simulations demonstrated that the SOD method based on the GLS model significantly outperformed all existing approaches when the spatial data exhibits a linear or nonlinear trend.

#index 1451248
#* Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora
#@ Jianwen Zhang;Yangqiu Song;Changshui Zhang;Shixia Liu
#t 2010
#c 0
#% 722904
#% 875959
#% 881498
#% 881514
#% 989586
#% 989650
#% 1073975
#% 1083699
#% 1166524
#% 1176919
#% 1176974
#% 1176975
#% 1214671
#% 1292520
#% 1305548
#% 1424114
#! Mining cluster evolution from multiple correlated time-varying text corpora is important in exploratory text analytics. In this paper, we propose an approach called evolutionary hierarchical Dirichlet processes (EvoHDP) to discover interesting cluster evolution patterns from such text data. We formulate the EvoHDP as a series of hierarchical Dirichlet processes~(HDP) by adding time dependencies to the adjacent epochs, and propose a cascaded Gibbs sampling scheme to infer the model. This approach can discover different evolving patterns of clusters, including emergence, disappearance, evolution within a corpus and across different corpora. Experiments over synthetic and real-world multiple correlated time-varying data sets illustrate the effectiveness of EvoHDP on discovering cluster evolution patterns.

#index 1451249
#* Online discovery and maintenance of time series motifs
#@ Abdullah Mueen;Eamonn Keogh
#t 2010
#c 0
#% 198554
#% 314119
#% 414993
#% 460862
#% 466506
#% 479649
#% 480812
#% 629607
#% 729960
#% 745513
#% 878305
#% 989656
#% 1015280
#% 1064180
#% 1127609
#% 1214752
#% 1220016
#% 1280911
#% 1305491
#% 1387564
#% 1665165
#! The detection of repeated subsequences, time series motifs, is a problem which has been shown to have great utility for several higher-level data mining algorithms, including classification, clustering, segmentation, forecasting, and rule discovery. In recent years there has been significant research effort spent on efficiently discovering these motifs in static offline databases. However, for many domains, the inherent streaming nature of time series demands online discovery and maintenance of time series motifs. In this paper, we develop the first online motif discovery algorithm which monitors and maintains motifs exactly in real time over the most recent history of a stream. Our algorithm has a worst-case update time which is linear to the window size and is extendible to maintain more complex pattern structures. In contrast, the current offline algorithms either need significant update time or require very costly pre-processing steps which online algorithms simply cannot afford. Our core ideas allow useful extensions of our algorithm to deal with arbitrary data rates and discovering multidimensional motifs. We demonstrate the utility of our algorithms with a variety of case studies in the domains of robotics, acoustic monitoring and online compression.

#index 1451250
#* Mining periodic behaviors for moving objects
#@ Zhenhui Li;Bolin Ding;Jiawei Han;Roland Kays;Peter Nye
#t 2010
#c 0
#% 310542
#% 342642
#% 464986
#% 480156
#% 565487
#% 629677
#% 769899
#% 810060
#% 813978
#% 823356
#% 844299
#% 881542
#% 975028
#% 1206625
#! Periodicity is a frequently happening phenomenon for moving objects. Finding periodic behaviors is essential to understanding object movements. However, periodic behaviors could be complicated, involving multiple interleaving periods, partial time span, and spatiotemporal noises and outliers. In this paper, we address the problem of mining periodic behaviors for moving objects. It involves two sub-problems: how to detect the periods in complex movement, and how to mine periodic movement behaviors. Our main assumption is that the observed movement is generated from multiple interleaved periodic behaviors associated with certain reference locations. Based on this assumption, we propose a two-stage algorithm, Periodica, to solve the problem. At the first stage, the notion of observation spot is proposed to capture the reference locations. Through observation spots, multiple periods in the movement can be retrieved using a method that combines Fourier transform and autocorrelation. At the second stage, a probabilistic model is proposed to characterize the periodic behaviors. For a specific period, periodic behaviors are statistically generalized from partial movement sequences through hierarchical clustering. Empirical studies on both synthetic and real data sets demonstrate the effectiveness of our method.

#index 1451251
#* An efficient causal discovery algorithm for linear models
#@ Zhenxing Wang;Laiwan Chan
#t 2010
#c 0
#% 246835
#% 297171
#% 400980
#% 961205
#% 1073925
#% 1399044
#% 1413295
#% 1650794
#! Bayesian network learning algorithms have been widely used for causal discovery since the pioneer work [13,18]. Among all existing algorithms, three-phase dependency analysis algorithm (TPDA) [5] is the most efficient one in the sense that it has polynomial-time complexity. However, there are still some limitations to be improved. First, TPDA depends on mutual information-based conditional independence (CI) tests, and so is not easy to be applied to continuous data. In addition, TPDA uses two phases to get approximate skeletons of Bayesian networks, which is not efficient in practice. In this paper, we propose a two-phase algorithm with partial correlation-based CI tests: the first phase of the algorithm constructs a Markov random field from data, which provides a close approximation to the structure of the true Bayesian network; at the second phase, the algorithm removes redundant edges according to CI tests to get the true Bayesian network. We show that two-phase algorithm with partial correlation-based CI tests can deal with continuous data following arbitrary distributions rather than only Gaussian distribution.

#index 1451252
#* Compressed fisher linear discriminant analysis: classification of randomly projected data
#@ Robert J. Durrant;Ata Kaban
#t 2010
#c 0
#% 3084
#% 450924
#% 527853
#% 593926
#% 643568
#% 729964
#% 866538
#% 1815607
#! We consider random projections in conjunction with classification, specifically the analysis of Fisher's Linear Discriminant (FLD) classifier in randomly projected data spaces. Unlike previous analyses of other classifiers in this setting, we avoid the unnatural effects that arise when one insists that all pairwise distances are approximately preserved under projection. We impose no sparsity or underlying low-dimensional structure constraints on the data; we instead take advantage of the class structure inherent in the problem. We obtain a reasonably tight upper bound on the estimated misclassification error on average over the random choice of the projection, which, in contrast to early distance preserving approaches, tightens in a natural way as the number of training examples increases. It follows that, for good generalisation of FLD, the required projection dimension grows logarithmically with the number of classes. We also show that the error contribution of a covariance misspecification is always no worse in the low-dimensional space than in the initial high-dimensional space. We contrast our findings to previous related work, and discuss our insights.

#index 1451253
#* Scalable similarity search with optimized kernel hashing
#@ Junfeng He;Wei Liu;Shih-Fu Chang
#t 2010
#c 0
#% 190581
#% 249321
#% 317313
#% 424085
#% 479973
#% 760805
#% 762054
#% 784995
#% 874628
#% 883972
#% 915350
#% 1083648
#! Scalable similarity search is the core of many large scale learning or data mining applications. Recently, many research results demonstrate that one promising approach is creating compact and efficient hash codes that preserve data similarity. By efficient, we refer to the low correlation (and thus low redundancy) among generated codes. However, most existing hash methods are designed only for vector data. In this paper, we develop a new hashing algorithm to create efficient codes for large scale data of general formats with any kernel function, including kernels on vectors, graphs, sequences, sets and so on. Starting with the idea analogous to spectral hashing, novel formulations and solutions are proposed such that a kernel based hash function can be explicitly represented and optimized, and directly applied to compute compact hash codes for new samples of general formats. Moreover, we incorporate efficient techniques, such as Nystrom approximation, to further reduce time and space complexity for indexing and search, making our algorithm scalable to huge data sets. Another important advantage of our method is the ability to handle diverse types of similarities according to actual task requirements, including both feature similarities and semantic similarities like label consistency. We evaluate our method using both vector and non-vector data sets at a large scale up to 1 million samples. Our comprehensive results show the proposed method outperforms several state-of-the-art approaches for all the tasks, with a significant gain for most tasks.

#index 1451254
#* Semi-supervised sparse metric learning using alternating linearization optimization
#@ Wei Liu;Shiqian Ma;Dacheng Tao;Jianzhuang Liu;Peng Liu
#t 2010
#c 0
#% 318785
#% 464291
#% 757953
#% 769881
#% 770782
#% 803567
#% 829025
#% 881492
#% 983830
#% 989642
#% 1074353
#% 1083645
#% 1156095
#% 1176946
#% 1211795
#% 1232015
#% 1292880
#% 1299304
#% 1305473
#% 1857498
#! In plenty of scenarios, data can be represented as vectors and then mathematically abstracted as points in a Euclidean space. Because a great number of machine learning and data mining applications need proximity measures over data, a simple and universal distance metric is desirable, and metric learning methods have been explored to produce sensible distance measures consistent with data relationship. However, most existing methods suffer from limited labeled data and expensive training. In this paper, we address these two issues through employing abundant unlabeled data and pursuing sparsity of metrics, resulting in a novel metric learning approach called semi-supervised sparse metric learning. Two important contributions of our approach are: 1) it propagates scarce prior affinities between data to the global scope and incorporates the full affinities into the metric learning; and 2) it uses an efficient alternating linearization method to directly optimize the sparse metric. Compared with conventional methods, ours can effectively take advantage of semi-supervision and automatically discover the sparse metric structure underlying input data patterns. We demonstrate the efficacy of the proposed approach with extensive experiments carried out on six datasets, obtaining clear performance gains over the state-of-the-arts.

#index 1451255
#* Universal multi-dimensional scaling
#@ Arvind Agarwal;Jeff M. Phillips;Suresh Venkatasubramanian
#t 2010
#c 0
#% 163839
#% 201893
#% 280478
#% 402289
#% 450489
#% 519951
#% 722904
#% 748465
#% 760805
#% 812418
#% 875966
#% 898279
#% 959984
#% 1134116
#% 1388244
#% 1720381
#! In this paper, we propose a unified algorithmic framework for solving many known variants of MDS. Our algorithm is a simple iterative scheme with guaranteed convergence, and is modular; by changing the internals of a single subroutine in the algorithm, we can switch cost functions and target spaces easily. In addition to the formal guarantees of convergence, our algorithms are accurate; in most cases, they converge to better quality solutions than existing methods in comparable time. Moreover, they have a small memory footprint and scale effectively for large data sets. We expect that this framework will be useful for a number of MDS variants that have not yet been studied. Our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere, preserving geodesic distances. As a complement to this result, we also extend the Johnson-Lindenstrauss Lemma to this spherical setting, by showing that projecting to a random O((1/µ2) log n)-dimensional sphere causes only an eps-distortion in the geodesic distances.

#index 1451256
#* Unsupervised transfer classification: application to text categorization
#@ Tianbao Yang;Rong Jin;Anil K. Jain;Yang Zhou;Wei Tong
#t 2010
#c 0
#% 747751
#% 770804
#% 771841
#% 818061
#% 818234
#% 818236
#% 839975
#% 840898
#% 884074
#% 983828
#% 1073897
#% 1073969
#% 1083666
#% 1202160
#% 1328303
#% 1674771
#! We study the problem of building the classification model for a target class in the absence of any labeled training example for that class. To address this difficult learning problem, we extend the idea of transfer learning by assuming that the following side information is available: (i) a collection of labeled examples belonging to other classes in the problem domain, called the auxiliary classes; (ii) the class information including the prior of the target class and the correlation between the target class and the auxiliary classes. Our goal is to construct the classification model for the target class by leveraging the above data and information. We refer to this learning problem as unsupervised transfer classification. Our framework is based on the generalized maximum entropy model that is effective in transferring the label information of the auxiliary classes to the target class. A theoretical analysis shows that under certain assumption, the classification model obtained by the proposed approach converges to the optimal model when it is learned from the labeled examples for the target class. Empirical study on text categorization over four different data sets verifies the effectiveness of the proposed approach.

#index 1451257
#* Nonnegative shared subspace learning and its application to social media retrieval
#@ Sunil Kumar Gupta;Dinh Phung;Brett Adams;Truyen Tran;Svetha Venkatesh
#t 2010
#c 0
#% 46803
#% 236497
#% 793248
#% 848221
#% 852098
#% 855563
#% 855601
#% 881054
#% 983899
#% 989651
#% 1038781
#% 1055704
#% 1055887
#% 1190091
#% 1237656
#% 1254853
#% 1338320
#% 1348318
#% 1356643
#! Although tagging has become increasingly popular in online image and video sharing systems, tags are known to be noisy, ambiguous, incomplete and subjective. These factors can seriously affect the precision of a social tag-based web retrieval system. Therefore improving the precision performance of these social tag-based web retrieval systems has become an increasingly important research topic. To this end, we propose a shared subspace learning framework to leverage a secondary source to improve retrieval performance from a primary dataset. This is achieved by learning a shared subspace between the two sources under a joint Nonnegative Matrix Factorization in which the level of subspace sharing can be explicitly controlled. We derive an efficient algorithm for learning the factorization, analyze its complexity, and provide proof of convergence. We validate the framework on image and video retrieval tasks in which tags from the LabelMe dataset are used to improve image retrieval performance from a Flickr dataset and video retrieval performance from a YouTube dataset. This has implications for how to exploit and transfer knowledge from readily available auxiliary tagging resources to improve another social web retrieval system. Our shared subspace learning framework is applicable to a range of problems where one needs to exploit the strengths existing among multiple and heterogeneous datasets.

#index 1451258
#* Learning incoherent sparse and low-rank patterns from multiple tasks
#@ Jianhui Chen;Ji Liu;Jieping Ye
#t 2010
#c 0
#% 209961
#% 236497
#% 465754
#% 577287
#% 723239
#% 757953
#% 770804
#% 829014
#% 840962
#% 916788
#% 961246
#% 1073879
#% 1128929
#% 1211707
#% 1211747
#% 1211772
#% 1232035
#% 1271814
#% 1302843
#% 1302853
#% 1379069
#% 1446818
#% 1654243
#! We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks. Our approach is based on a linear multi-task learning formulation, in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint, respectively. This formulation is non-convex; we convert it into its convex surrogate, which can be routinely solved via semidefinite programming for small-size problems. We propose to employ the general projected gradient scheme to efficiently solve such a convex surrogate; however, in the optimization formulation, the objective function is non-differentiable and the feasible domain is non-trivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. The computation of projected gradient involves a constrained optimization problem; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and an Euclidean projection subproblem. In addition, we present two projected gradient algorithms and discuss their rates of convergence. Experimental results on benchmark data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms.

#index 1451259
#* Multi-task learning for boosting with application to web search ranking
#@ Olivier Chapelle;Pannagadatta Shivaswamy;Srinivas Vadrevu;Kilian Weinberger;Ya Zhang;Belle Tseng
#t 2010
#c 0
#% 236497
#% 309095
#% 769886
#% 771845
#% 983828
#% 1073892
#% 1211829
#% 1268491
#% 1338581
#% 1442578
#! In this paper we propose a novel algorithm for multi-task learning with boosted decision trees. We learn several different learning tasks with a joint model, explicitly addressing the specifics of each learning task with task-specific parameters and the commonalities between them through shared parameters. This enables implicit data sharing and regularization. We evaluate our learning method on web-search ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. Our experiments validate that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.

#index 1451260
#* Transfer metric learning by learning task relationships
#@ Yu Zhang;Dit-Yan Yeung
#t 2010
#c 0
#% 209623
#% 236497
#% 757953
#% 769886
#% 770813
#% 876002
#% 983830
#% 989583
#% 1074017
#% 1083645
#% 1124440
#% 1128929
#% 1211843
#% 1270196
#% 1279813
#% 1305479
#% 1305502
#% 1464068
#% 1861871
#! Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric. However, the labeled data available in many applications is scarce and hence the metrics learned are often unsatisfactory. In this paper, we consider a transfer learning setting in which some related source tasks with labeled data are available to help the learning of the target task. We first propose a convex formulation for multi-task metric learning by modeling the task relationships in the form of a task covariance matrix. Then we regard transfer learning as a special case of multi-task learning and adapt the formulation of multi-task metric learning to the transfer learning setting for our method, called transfer metric learning (TML). In TML, we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem has an efficient solution. Experimental results on some commonly used transfer learning applications demonstrate the effectiveness of our method.

#index 1451261
#* The next generation of transportation systems,greenhouse emissions, and data mining
#@ Hillol Kargupta;Joao Gama;Wei Fan
#t 2010
#c 0
#% 1211988
#% 1451142

#index 1451534
#* DUST: a generalized notion of similarity between uncertain time series
#@ Smruti R. Sarangi;Karin Murthy
#t 2010
#c 0
#% 63223
#% 172949
#% 408396
#% 780253
#% 814194
#% 893102
#% 992857
#% 1127609
#% 1181271
#% 1189215
#% 1206640
#% 1291115
#% 1695788
#! Large-scale sensor deployments and an increased use of privacy-preserving transformations have led to an increasing interest in mining uncertain time series data. Traditional distance measures such as Euclidean distance or dynamic time warping are not always effective for analyzing uncertain time series data. Recently, some measures have been proposed to account for uncertainty in time series data. However, we show in this paper that their applicability is limited. In specific, these approaches do not provide an intuitive way to compare two uncertain time series and do not easily accommodate multiple error functions. In this paper, we provide a theoretical framework that generalizes the notion of similarity between uncertain time series. Secondly, we propose DUST, a novel distance measure that accommodates uncertainty and degenerates to the Euclidean distance when the distance is large compared to the error. We provide an extensive experimental validation of our approach for the following applications: classification, top-k motif search, and top-k nearest-neighbor queries.

#index 1452840
#* Proceedings of the ACM SIGKDD Workshop on Human Computation
#@ 
#t 2010
#c 0
#! Research in data mining and knowledge discovery relies heavily on the availability of datasets. However, compared to the amount of work in the field on techniques for pattern discovery and knowledge extraction, there has been relatively little effort directed at the study of effective methods for collecting and evaluating the quality of data. Human computation is a relatively new research area that studies the process of channeling the vast internet population to perform tasks or provide data towards solving difficult problems that no known efficient computer algorithms can yet solve. There has been a lot of work on games with a purpose (e.g., the ESP Game) that specifically target online gamers who, in the process of playing an enjoyable game, generate useful data (e.g., image tags). There has been considerable interest and research on crowdsourcing marketplaces (e.g. Amazon Mechanical Turk), which are essentially human computation applications that coordinate workers to perform tasks in exchange for monetary rewards. While there have been significant research challenges, increasing business interest and active work in human computation, till last year there was no dedicated forum to discuss these ideas. The first Human Computation Workshop (HComp2009) was held on June 28th, 2009, in Paris, France, collocated with KDD 2009. With HComp2010, we hope to continue to serve the needs of researchers and practitioners interested in this area, integrating work from a number of fields including KDD, information retrieval, gaming, machine learning, game theory, and Human Computer Interaction. Learning from HComp 2009, we expanded the topics of relevance to the workshop. We have also updated the organizing and program committees, bringing new people in, while keeping enough of the last year's team for continuity. This year, our call for papers resulted in 35 submissions (18 long papers, 11 short papers and 6 demos) from a range of perspectives. All submissions were thoroughly reviewed, typically with 3 reviews each, by the organizing and program committees and external reviewers. Even though we had a number of submissions of high quality, given that we had only a half-day workshop, we could accept only 4 long papers, 4 short papers, 5 posters and 5 demos; these are the submissions that appear in the proceedings. The overall themes that emerged from last year's workshop were very clear: on the one hand, there is the experimental side of human computation, with research on new incentives for users to participate, new types of actions, and new modes of interaction. On the more theoretic side, we have research modeling these actions and incentives to examine what theory predicts about these designs. Finally, last year's program focused on how to best handle noise, identify labeler expertise, and use the generated data for data mining purposes. This year's submissions demonstrated the continuation and evolution of these themes with the accepted papers divided into three sessions: "Market Design", "Human Computation in Practice", and "Task and Process Design". In the first session, the authors discuss the economic context of human computation and the practical lessons learned from building a human computation resource. In the second session we see many practical examples of human computation in fields as far-flung as virtual world construction to language translation and beyond. In the final session, the authors focus on how to get the best quality results out of the crowdsourced work. Given the strong submissions for demos both last year and this, we see -- and the community seems to recognize -- the poster and demo session as an integral part of the workshop, where participants can showcase their human computation applications.

#index 1477780
#* New developments in the theory of clustering
#@ Suresh Venkatasubramanian;Sergei Vassilvitskii
#t 2010
#c 0

#index 1477781
#* Temporal pattern mining in symbolic time point and time interval data
#@ Fabian Moerchen
#t 2010
#c 0

#index 1477782
#* Learning through exploration
#@ John Langford;Alina Beygelzimer
#t 2010
#c 0

#index 1477783
#* Geometric tools for graph mining of large social and information networks
#@ Michael Mahoney
#t 2010
#c 0

#index 1477784
#* Mining heterogeneous information networks
#@ Jiawei Han
#t 2010
#c 0

#index 1477785
#* Recommender problems for web applications
#@ Bee-Chung Chen;Deepak Agarwal
#t 2010
#c 0

#index 1477786
#* Indexing and mining time sequences
#@ Lei Li;Christos Faloutsos
#t 2010
#c 0

#index 1478209
#* Mining medical data to improve patient outcomes
#@ R Bharat Rao
#t 2010
#c 0

#index 1478210
#* Modeling with networked data
#@ Françoise Fogelman Soulié
#t 2010
#c 0

#index 1478211
#* Discovering precursors to aviation safety incidents: from massive data to actionable information
#@ Ashok N. Srivastava
#t 2010
#c 0

#index 1478212
#* Text mining to fast-track deserving disability applicants
#@ John Elder
#t 2010
#c 0

#index 1538809
#* ACM SIGKDD Workshop on Intelligence and Security Informatics
#@ Hsinchun Chen;Christopher C. Yang
#t 2010
#c 0

#index 1561547
#* Proceedings of the First Workshop on Social Media Analytics
#@ Prem Melville;Jure Leskovec;Foster Provost
#t 2010
#c 0
#! The explosion of Online Social Media in the form of user-generated content on blogs, microblogs (Twitter), discussion forums, product reviews and multimedia sharing sites presents many new opportunities and challenges to both producers and consumers of information. For producers, this user-generated content provides a rich source of implicit consumer feedback. Tracking the pulse of the ever-expanding social media outlets, enables companies to discern what consumers are saying about their products, which provides useful insight on how to improve and market products better. For consumers, the plethora of information and opinions from diverse sources helps them tap into the wisdom of crowds, to aid in making more informed decisions. Though there is a vast quantity of information available, the consequent challenge is to be able to analyze the large volumes of user-generated content and the implicit (or explicit) links between users, in order to glean meaningful insights therein.

#index 1592317
#* Proceedings of the Third Workshop on Large Scale Data Mining: Theory and Applications
#@ 
#t 2011
#c 0
#! With advances in data collection and storage technologies, large data sources have become ubiquitous. Today, organizations routinely collect terabytes of data on a daily basis with the intent of gleaning non-trivial insights on their business processes. To benefit from these advances, it is imperative that data mining and machine learning techniques scale to such proportions. Such scaling can be achieved through the design of new and faster algorithms and/or through the employment of parallelism. Furthermore, it is important to note that emerging and future processor architectures (like multi-cores) will rely on user-specified parallelism to provide any performance gains. Unfortunately, achieving such scaling is non-trivial and only a handful of research efforts in the data mining and machine learning communities have attempted to address these scales. At the other end of the spectrum, the past few years have witnessed the emergence of several platforms for the implementation and deployment of large-scale analytics. Examples of such platforms include Hadoop (Apache) and Dryad (Microsoft). These platforms have been developed by the large-scale distributed processing community and can not only simplify implementation but also support execution on the cloud making large-scale machine learning and data mining both affordable and available to all. Today, there is a large gap between the data mining/machine learning and the large scale distributed processing communities. To make advances in large-scale analytics it is imperative that both these communities work hand-in-hand. The intent of this workshop is to further research efforts on large-scale data mining and to encourage researchers and practitioners to share their studies and experiences on the implementation and deployment of scalable data mining and machine learning algorithms.

#index 1592664
#* Proceedings of the Tenth International Workshop on Data Mining in Bioinformatics
#@ Mohammed Zaki;Jake Chen;Mohammad Al Hasan;Jun (Luke) Huan
#t 2011
#c 0
#! Average age of the population tends to increase and the number of people requiring care intensive medical monitoring is not small. This increases overall cost of medical care. Therefore, partially replacing the assistance of nursing staff by small health surveillance and communication equipments like sensors, networks, monitoring software could be cost effective and would also increase life standard. The objective is to develop and implement innovative solutions based on information technologies and wireless communication for the benefit of those needing medical permanence. Recent advances in technology have led to the development of small, intelligent, wearable sensors capable of remotely performing critical health monitoring tasks and then transmitting patient's data back to health care centers over wireless medium. Such wireless health monitoring platforms aim to continuously monitor mobile patients needing permanent surveillance.

#index 1592891
#* Proceedings of the Fifth International Workshop on Knowledge Discovery from Sensor Data
#@ Varun Chandola;Olufemi A. Omitaomu;Karsten Steinhaeuser;Auroop R. Ganguly;Joao Gama;Ranga Raju Vatsavai;Nitesh V. Chawla;Mohamed Medhat Gaber
#t 2011
#c 0
#! Wide-area sensor infrastructures, remote sensors, RFIDs, phasor measurements, and wireless sensor networks yield massive volumes of disparate, dynamic, and geographically distributed data. With the recent proliferation of smart-phones and similar GPS enabled mobile devices with several onboard sensors, collection of sensor data is no longer limited to scientific communities, but has reached general public. As such sensors are becoming ubiquitous, a set of broad requirements is beginning to emerge across high-priority applications including adaptability to national or homeland security, critical infrastructures monitoring, smart grids, disaster preparedness and management, greenhouse emissions and climate change, and transportation. The raw data from sensors need to be efficiently managed and transformed to usable information through data fusion, which in turn must be converted to predictive insights via knowledge discovery, ultimately facilitating automated or human-induced tactical decisions or strategic policy based on decision sciences and decision support systems. The challenges for the knowledge discovery community are expected to be immense. On the one hand are dynamic data streams or events that require real-time analysis methodologies and systems, while on the other hand are static data that require high end computing for generating offline predictive insights, which in turn can facilitate real-time analysis. The online and real-time knowledge discovery imply immediate opportunities as well as intriguing short- and long-term challenges for practitioners and researchers in knowledge discovery. The opportunities would be to develop new data mining approaches and adapt traditional and emerging knowledge discovery methodologies to the requirements of the emerging problems. In addition, emerging societal problems require knowledge discovery solutions that are designed to investigate anomalies, rare events, hotspots, changes, extremes and nonlinear processes, and departures from the normal. According to the data mining and domain experts present at the NSF-sponsored Next Generation Data Mining Summit (NGDM '09) held in October 2009, "finding the next generation of solutions to these challenges is critical to sustain our world and civilization." The SensorKDD series of workshops, held in conjunction with the prestigious ACM SIGKDD International Conference of Knowledge Discovery and Data Mining from 2007-2010, have aimed at bringing researchers, from different academic and applied communities, together to address these challenges and moving toward the development of the next generation data mining solutions require to address these challenges. The proposed 5th International Workshop on Knowledge Discovery from Sensor Data (SensorKDD-2011) is the next step in this successful series of workshops with the objective of providing a platform for researchers to present and discuss their research in the area of knowledge discovery from sensor data.

#index 1604416
#* Proceedings of the First International Workshop on Data Mining for Service and Maintenance
#@ Shipeng Yu;Ya Xue;Dragos Margineantu;Nikunj Oza;Michal Rosen-Zvi;R. Bharat Rao
#t 2011
#c 0

#index 1605908
#* Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Chid Apte;Joydeep Ghosh;Padhraic Smyth
#t 2011
#c 0
#! It is my great honor and pleasure to welcome you to ACM SIGKDD's 17th Annual Conference on Knowledge Discovery and Data Mining, KDD2011, being held in San Diego, California, from August 21 through August 24, 2011. This year's KDD Conference continues its tradition of being the premier forum for the sharing of leading edge research results and industrial experiences of KDD theory, technology, and applications, among it's multi-disciplinary participants from academia, government, and industry. The KDD2011 organization committee has worked hard to help organize this year's conference, and to ensure that it maintains its pre-eminent position as the top worldwide conference for dissemination of cutting edge technical progress of this community. The science and business of extracting actionable insights from data has never been more important. We are experiencing an explosion of data, and the consequent interest in analyzing this information in a timely and rigorous fashion. Data and content volumes are expected to grow exponentially in the next decade. Much of the new growth is expected to dominantly come from unstructured content; documents, images, audio and video. These trends are setting up the stage for a fascinating set of new research challenges, and simultaneously driving the need for accelerated deployment of our research advances, ranging from algorithmic advances in core modeling and learning techniques, to scalable methods for parallel and distributed mining of massive scale data, to new paradigms for consumption and delivery of the analytic insights. It is my hope that attendees at this conference will be able to sense and appreciate the impressive progress our community is making in addressing these challenges. We have made a few changes this year to the traditional KDD conference format, to address the growing interest in this technology. The conference will begin as always with workshops and tutorials on Sunday, and then we have a full 3-day program for the main body of the conference. The extended 3-day main program has allowed us to include additional keynote plenary talks and make space for some exciting new content. We are launching a new track this year, called the Industry Practice Expo. The aim of the IPE track is to bring in eminent practitioners of KDD from industry to share with the participants their in depth experiences on what it means to successfully deploy working instances of high impacting KDD solutions.

#index 1605909
#* Convex optimization: from embedded real-time to large-scale distributed
#@ Stephen Boyd
#t 2011
#c 0
#! Convex optimization has emerged as useful tool for applications that include data analysis and model fitting, resource allocation, engineering design, network design and optimization, finance, and control and signal processing. After an overview, the talk will focus on two extremes: real-time embedded convex optimization, and distributed convex optimization. Code generation can be used to generate extremely efficient and reliable solvers for small problems, that can execute in milliseconds or microseconds, and are ideal for embedding in real-time systems. At the other extreme, we describe methods for large-scale distributed optimization, which coordinate many solvers to solve enormous problems.

#index 1605910
#* Internet scale data analysis
#@ Peter Norvig
#t 2011
#c 0
#! This talk covers techniques for analyzing data sets with up to trillions of examples with billions of features, using thousands of computers. To operate at this scale requires an understanding of an increasing complex hardware hierarchy (e.g. cache, memory, SSD, another machine in the rack, disk, a machine in another data center, ...); a model for recovering from inevitable hardware and software failures; a machine learning model that allows for efficient computation over large, continuously updated data sets; and a way to visualize and share the results.

#index 1605911
#* Cancer genomics
#@ David Haussler
#t 2011
#c 0
#! Throughout life, the cells in every individual accumulate many changes in the DNA inherited from his or her parents. Certain combinations of changes lead to cancer. During the last decade, the cost of DNA sequencing has been dropping by a factor of 10 every two years, making it now possible to read most of the three billion base genome from a patient's cancer tumor, and to try to determine all of the thousands of DNA changes in it. Under the auspices of NCI's Cancer Genome Atlas Project, 10,000 tumors will be sequenced in this manner in the next few years. Soon cancer genome sequencing will be a widespread clinical practice, and millions of tumors will be sequenced. A massive computational problem looms in interpreting these data. First, because we can only read short pieces of DNA, we have the enormous problem of assembling a coherent and reliable representation of the tumor genome from massive amounts of incomplete and error-prone evidence. This is the first challenge. Second, every human genome is unique from birth, and every tumor a unique variant. There is no single route to cancer. We must learn to read the varied signatures of cancer within the tumor genome and associate these with optimal treatments. Already there are hundreds of molecularly targeted treatments for cancer available, each known to be more or less effective depending on specific genetic variants. However, targeting a single gene with one treatment rarely works. The second challenge is to tackle the combinatorics of personalized, targeted, combination therapy in cancer.

#index 1605912
#* The mathematics of causal inference
#@ Judea Pearl
#t 2011
#c 0
#! I will review concepts, principles, and mathematical tools that were found useful in applications involving causal and counterfactual relationships. This semantical framework, enriched with a few ideas from logic and graph theory, gives rise to a complete, coherent, and friendly calculus of causation that unifies the graphical and counterfactual approaches to causation and resolves many long-standing problems in several of the sciences. These include questions of causal effect estimation, policy analysis, and the integration of data from diverse studies. Of special interest to KDD researchers would be the following topics: The Mediation Formula, and what it tells us about direct and indirect effects. What mathematics can tell us about "external validity" or "generalizing from experiments" What can graph theory tell us about recovering from sample-selection bias.

#index 1605913
#* CHIRP: a new classifier based on composite hypercubes on iterated random projections
#@ Leland Wilkinson;Anushka Anand;Dang Nhon Tuan
#t 2011
#c 0
#% 81489
#% 136350
#% 248792
#% 333881
#% 449559
#% 482762
#% 520224
#% 722916
#% 726066
#% 770830
#% 806216
#% 824687
#% 833529
#% 881484
#% 915259
#% 961267
#% 1138540
#% 1272365
#% 1318711
#! We introduce a classifier based on the L-infinity norm. This classifier, called CHIRP, is an iterative sequence of three stages (projecting, binning, and covering) that are designed to deal with the curse of dimensionality, computational complexity, and nonlinear separability. CHIRP is not a hybrid or modification of existing classifiers; it employs a new covering algorithm. The accuracy of CHIRP on widely-used benchmark datasets exceeds the accuracy of competitors. Its computational complexity is sub-linear in number of instances and number of variables and subquadratic in number of classes.

#index 1605914
#* Supervised learning for provenance-similarity of binaries
#@ Sagar Chaki;Cory Cohen;Arie Gurfinkel
#t 2011
#c 0
#% 400847
#% 1151981
#% 1216037
#% 1227869
#% 1298831
#% 1407185
#% 1426240
#% 1451148
#! Understanding, measuring, and leveraging the similarity of binaries (executable code) is a foundational challenge in software engineering. We present a notion of similarity based on provenance -- two binaries are similar if they are compiled from the same (or very similar) source code with the same (or similar) compilers. Empirical evidence suggests that provenance-similarity accounts for a significant portion of variation in existing binaries, particularly in malware. We propose and evaluate the applicability of classification to detect provenance-similarity. We evaluate a variety of classifiers, and different types of attributes and similarity labeling schemes, on two benchmarks derived from open-source software and malware respectively. We present encouraging results indicating that classification is a viable approach for automated provenance-similarity detection, and as an aid for malware analysts in particular.

#index 1605915
#* Trading representability for scalability: adaptive multi-hyperplane machine for nonlinear classification
#@ Zhuang Wang;Nemanja Djuric;Koby Crammer;Slobodan Vucetic
#t 2011
#c 0
#% 197394
#% 269218
#% 722816
#% 734919
#% 770754
#% 803575
#% 829021
#% 875970
#% 881477
#% 916781
#% 983905
#% 1073923
#% 1211775
#% 1318710
#% 1385966
#% 1386108
#% 1432713
#% 1451223
#% 1472278
#% 1558464
#% 1759695
#! Support Vector Machines (SVMs) are among the most popular and successful classification algorithms. Kernel SVMs often reach state-of-the-art accuracies, but suffer from the curse of kernelization due to linear model growth with data size on noisy data. Linear SVMs have the ability to efficiently learn from truly large data, but they are applicable to a limited number of domains due to low representational power. To fill the representability and scalability gap between linear and nonlinear SVMs, we propose the Adaptive Multi-hyperplane Machine (AMM) algorithm that accomplishes fast training and prediction and has capability to solve nonlinear classification problems. AMM model consists of a set of hyperplanes (weights), each assigned to one of the multiple classes, and predicts based on the associated class of the weight that provides the largest prediction. The number of weights is automatically determined through an iterative algorithm based on the stochastic gradient descent algorithm which is guaranteed to converge to a local optimum. Since the generalization bound decreases with the number of weights, a weight pruning mechanism is proposed and analyzed. The experiments on several large data sets show that AMM is nearly as fast during training and prediction as the state-of-the-art linear SVM solver and that it can be orders of magnitude faster than kernel SVM. In accuracy, AMM is somewhere between linear and kernel SVMs. For example, on an OCR task with 8 million highly dimensional training examples, AMM trained in 300 seconds on a single-core processor had 0.54% error rate, which was significantly lower than 2.03% error rate of a linear SVM trained in the same time and comparable to 0.43% error rate of a kernel SVM trained in 2 days on 512 processors. The results indicate that AMM could be an attractive option when solving large-scale classification problems. The software is available at www.dabi.temple.edu/~vucetic/AMM.html.

#index 1605916
#* An improved GLMNET for l1-regularized logistic regression
#@ Guo-Xun Yuan;Chia-Hua Ho;Chih-Jen Lin
#t 2011
#c 0
#% 983808
#% 1014657
#% 1077165
#% 1100067
#% 1117675
#% 1117691
#% 1214676
#% 1551204
#% 1565315
#! GLMNET proposed by Friedman et al. is an algorithm for generalized linear models with elastic net. It has been widely applied to solve L1-regularized logistic regression. However, recent experiments indicated that the existing GLMNET implementation may not be stable for large-scale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient regardless of loosely or strictly solving the optimization problem. Experiments demonstrate that the improved GLMNET is more efficient than a state-of-the-art coordinate descent method.

#index 1605917
#* Integrating low-rank and group-sparse structures for robust multi-task learning
#@ Jianhui Chen;Jiayu Zhou;Jieping Ye
#t 2011
#c 0
#% 236497
#% 723239
#% 770804
#% 829014
#% 840962
#% 916788
#% 961246
#% 983942
#% 1128929
#% 1211707
#% 1211747
#% 1214676
#% 1302843
#% 1302853
#% 1417091
#% 1451171
#% 1451258
#% 1504249
#% 1556166
#% 1563695
#! Multi-task learning (MTL) aims at improving the generalization performance by utilizing the intrinsic relationships among multiple related tasks. A key assumption in most MTL algorithms is that all tasks are related, which, however, may not be the case in many real-world applications. In this paper, we propose a robust multi-task learning (RMTL) algorithm which learns multiple tasks simultaneously as well as identifies the irrelevant (outlier) tasks. Specifically, the proposed RMTL algorithm captures the task relationships using a low-rank structure, and simultaneously identifies the outlier tasks using a group-sparse structure. The proposed RMTL algorithm is formulated as a non-smooth convex (unconstrained) optimization problem. We propose to adopt the accelerated proximal method (APM) for solving such an optimization problem. The key component in APM is the computation of the proximal operator, which can be shown to admit an analytic solution. We also theoretically analyze the effectiveness of the RMTL algorithm. In particular, we derive a key property of the optimal solution to RMTL; moreover, based on this key property, we establish a theoretical bound for characterizing the learning performance of RMTL. Our experimental results on benchmark data sets demonstrate the effectiveness and efficiency of the proposed algorithm.

#index 1605918
#* Model order selection for boolean matrix factorization
#@ Pauli Miettinen;Jilles Vreeken
#t 2011
#c 0
#% 144520
#% 769896
#% 798820
#% 850526
#% 878207
#% 915343
#% 978023
#% 985037
#% 991656
#% 1042904
#% 1083656
#% 1117701
#% 1176932
#% 1176968
#% 1194460
#% 1206680
#% 1211811
#% 1294226
#% 1300483
#% 1535419
#% 1565634
#% 1815525
#! Matrix factorizations---where a given data matrix is approximated by a product of two or more factor matrices---are powerful data mining tools. Among other tasks, matrix factorizations are often used to separate global structure from noise. This, however, requires solving the `model order selection problem' of determining where fine-grained structure stops, and noise starts, i.e., what is the proper size of the factor matrices. Boolean matrix factorization (BMF)---where data, factors, and matrix product are Boolean---has received increased attention from the data mining community in recent years. The technique has desirable properties, such as high interpretability and natural sparsity. But so far no method for selecting the correct model order for BMF has been available. In this paper we propose to use the Minimum Description Length (MDL) principle for this task. Besides solving the problem, this well-founded approach has numerous benefits, e.g., it is automatic, does not require a likelihood function, is fast, and, as experiments show, is highly accurate. We formulate the description length function for BMF in general---making it applicable for any BMF algorithm. We extend an existing algorithm for BMF to use MDL to identify the best Boolean matrix factorization, analyze the complexity of the problem, and perform an extensive experimental evaluation to study its behavior.

#index 1605919
#* Rank aggregation via nuclear norm minimization
#@ David F. Gleich;Lek-heng Lim
#t 2011
#c 0
#% 209961
#% 274586
#% 330769
#% 419739
#% 769959
#% 805798
#% 858155
#% 1214694
#% 1309918
#% 1441070
#% 1556166
#% 1563304
#% 1815739
#% 1817647
#! The process of rank aggregation is intimately intertwined with the structure of skew symmetric matrices. We apply recent advances in the theory and algorithms of matrix completion to skew-symmetric matrices. This combination of ideas produces a new method for ranking a set of items. The essence of our idea is that a rank aggregation describes a partially filled skew-symmetric matrix. We extend an algorithm for matrix completion to handle skew-symmetric data and use that to extract ranks for each item. Our algorithm applies to both pairwise comparison and rating data. Because it is based on matrix completion, it is robust to both noise and incomplete data. We show a formal recovery result for the noiseless case and present a detailed study of the algorithm on synthetic data and Netflix ratings.

#index 1605920
#* Large-scale matrix factorization with distributed stochastic gradient descent
#@ Rainer Gemulla;Erik Nijkamp;Peter J. Haas;Yannis Sismanis
#t 2011
#c 0
#% 190265
#% 280819
#% 891559
#% 956521
#% 1102242
#% 1108903
#% 1260273
#% 1400001
#% 1426585
#% 1470631
#! We provide a novel algorithm to approximately factor large matrices with millions of rows, millions of columns, and billions of nonzero elements. Our approach rests on stochastic gradient descent (SGD), an iterative stochastic optimization algorithm. We first develop a novel "stratified" SGD variant (SSGD) that applies to general loss-minimization problems in which the loss function can be expressed as a weighted sum of "stratum losses." We establish sufficient conditions for convergence of SSGD using results from stochastic approximation theory and regenerative process theory. We then specialize SSGD to obtain a new matrix-factorization algorithm, called DSGD, that can be fully distributed and run on web-scale datasets using, e.g., MapReduce. DSGD can handle a wide variety of matrix factorizations. We describe the practical techniques used to optimize performance in our DSGD implementation. Experiments suggest that DSGD converges significantly faster and has better scalability properties than alternative algorithms.

#index 1605921
#* Diversity in ranking via resistive graph centers
#@ Avinava Dubey;Soumen Chakrabarti;Chiranjib Bhattacharyya
#t 2011
#c 0
#% 187651
#% 262112
#% 397133
#% 406493
#% 577329
#% 642975
#% 766472
#% 818266
#% 829043
#% 879618
#% 881496
#% 915344
#% 983805
#% 989628
#% 1055712
#% 1073970
#% 1074025
#% 1083643
#% 1166473
#% 1190062
#% 1214650
#% 1417383
#% 1450988
#% 1451241
#% 1470696
#% 1676017
#! Users can rarely reveal their information need in full detail to a search engine within 1--2 words, so search engines need to "hedge their bets" and present diverse results within the precious 10 response slots. Diversity in ranking is of much recent interest. Most existing solutions estimate the marginal utility of an item given a set of items already in the response, and then use variants of greedy set cover. Others design graphs with the items as nodes and choose diverse items based on visit rates (PageRank). Here we introduce a radically new and natural formulation of diversity as finding centers in resistive graphs. Unlike in PageRank, we do not specify the edge resistances (equivalently, conductances) and ask for node visit rates. Instead, we look for a sparse set of center nodes so that the effective conductance from the center to the rest of the graph has maximum entropy. We give a cogent semantic justification for turning PageRank thus on its head. In marked deviation from prior work, our edge resistances are learnt from training data. Inference and learning are NP-hard, but we give practical solutions. In extensive experiments with subtopic retrieval, social network search, and document summarization, our approach convincingly surpasses recently-published diversity algorithms like subtopic cover, max-marginal relevance (MMR), Grasshopper, DivRank, and SVMdiv.

#index 1605922
#* Collective graph identification
#@ Galileo Mark Namata;Stanley Kok;Lise Getoor
#t 2011
#c 0
#% 310516
#% 722816
#% 730089
#% 818246
#% 833563
#% 850430
#% 915340
#% 937552
#% 961268
#% 961278
#% 1083704
#% 1084483
#% 1166537
#% 1264785
#% 1268069
#% 1269764
#% 1270263
#% 1558464
#! Data describing networks (communication networks, transaction networks, disease transmission networks, collaboration networks, etc.) is becoming increasingly ubiquitous. While this observational data is useful, it often only hints at the actual underlying social or technological structures which give rise to the interactions. For example, an email communication network provides useful insight but is not the same as the "real" social network among individuals. In this paper, we introduce the problem of graph identification, i.e., the discovery of the true graph structure underlying an observed network. We cast the problem as a probabilistic inference task, in which we must infer the nodes, edges, and node labels of a hidden graph, based on evidence provided by the observed network. This in turn corresponds to the problems of performing entity resolution, link prediction, and node labeling to infer the hidden graph. While each of these problems have been studied separately, they have never been considered together as a coherent task. We present a simple yet novel approach to address all three problems simultaneously. Our approach, called C3, consists of Coupled Collective Classifiers that are iteratively applied to propagate information among solutions to the problems. We empirically demonstrate that C3 is superior, in terms of both predictive accuracy and runtime, to state-of-the-art probabilistic approaches on three real-world problems.

#index 1605923
#* Semi-supervised ranking on very large graphs with rich metadata
#@ Bin Gao;Tie-Yan Liu;Wei Wei;Taifeng Wang;Hang Li
#t 2011
#c 0
#% 268079
#% 282905
#% 348173
#% 387427
#% 466891
#% 577338
#% 840965
#% 869492
#% 875948
#% 881457
#% 912202
#% 963669
#% 983805
#% 1018766
#% 1125906
#% 1346154
#% 1410898
#% 1536568
#% 1663624
#! Graph ranking plays an important role in many applications, such as page ranking on web graphs and entity ranking on social networks. In applications, besides graph structure, rich information on nodes and edges and explicit or implicit human supervision are often available. In contrast, conventional algorithms (e.g., PageRank and HITS) compute ranking scores by only resorting to graph structure information. A natural question arises here, that is, how to effectively and efficiently leverage all the information to more accurately calculate graph ranking scores than the conventional algorithms, assuming that the graph is also very large. Previous work only partially tackled the problem, and the proposed solutions are also not satisfying. This paper addresses the problem and proposes a general framework as well as an efficient algorithm for graph ranking. Specifically, we define a semi-supervised learning framework for ranking of nodes on a very large graph and derive within our proposed framework an efficient algorithm called Semi-Supervised PageRank. In the algorithm, the objective function is defined based upon a Markov random walk on the graph. The transition probability and the reset probability of the Markov model are defined as parametric models based on features on nodes and edges. By minimizing the objective function, subject to a number of constraints derived from supervision information, we simultaneously learn the optimal parameters of the model and the optimal ranking scores of the nodes. Finally, we show that it is possible to make the algorithm efficient to handle a billion-node graph by taking advantage of the sparsity of the graph and implement it in the MapReduce logic. Experiments on real data from a commercial search engine show that the proposed algorithm can outperform previous algorithms on several tasks.

#index 1605924
#* Benefits of bias: towards better characterization of network sampling
#@ Arun S. Maiya;Tanya Y. Berger-Wolf
#t 2011
#c 0
#% 256685
#% 268087
#% 309748
#% 330609
#% 341704
#% 755402
#% 805799
#% 823342
#% 878648
#% 881526
#% 989613
#% 992265
#% 1002007
#% 1176911
#% 1210998
#% 1247092
#% 1292553
#% 1400003
#% 1401271
#% 1404182
#% 1446945
#% 1482202
#% 1535470
#! From social networks to P2P systems, network sampling arises in many settings. We present a detailed study on the nature of biases in network sampling strategies to shed light on how best to sample from networks. We investigate connections between specific biases and various measures of structural representativeness. We show that certain biases are, in fact, beneficial for many applications, as they "push" the sampling process towards inclusion of desired properties. Finally, we describe how these sampling biases can be exploited in several, real-world applications including disease outbreak detection and market research.

#index 1605925
#* Scalable distributed inference of dynamic user interests for behavioral targeting
#@ Amr Ahmed;Yucheng Low;Mohamed Aly;Vanja Josifovski;Alexander J. Smola
#t 2011
#c 0
#% 91631
#% 329569
#% 428243
#% 722904
#% 754126
#% 875959
#% 989573
#% 1190251
#% 1214642
#% 1214692
#% 1214715
#% 1229386
#% 1355038
#% 1385969
#% 1396086
#% 1397425
#% 1399989
#% 1399999
#% 1451206
#% 1523858
#! Historical user activity is key for building user profiles to predict the user behavior and affinities in many web applications such as targeting of online advertising, content personalization and social recommendations. User profiles are temporal, and changes in a user's activity patterns are particularly useful for improved prediction and recommendation. For instance, an increased interest in car-related web pages may well suggest that the user might be shopping for a new vehicle.In this paper we present a comprehensive statistical framework for user profiling based on topic models which is able to capture such effects in a fully \emph{unsupervised} fashion. Our method models topical interests of a user dynamically where both the user association with the topics and the topics themselves are allowed to vary over time, thus ensuring that the profiles remain current. We describe a streaming, distributed inference algorithm which is able to handle tens of millions of users. Our results show that our model contributes towards improved behavioral targeting of display advertising relative to baseline models that do not incorporate topical and/or temporal dependencies. As a side-effect our model yields human-understandable results which can be used in an intuitive fashion by advertisers.

#index 1605926
#* Multiple domain user personalization
#@ Yucheng Low;Deepak Agarwal;Alexander J. Smola
#t 2011
#c 0
#% 73441
#% 91631
#% 297171
#% 397155
#% 722904
#% 770816
#% 798509
#% 881537
#% 983883
#% 1073982
#% 1214623
#% 1232035
#% 1260273
#% 1523858
#! Content personalization is a key tool in creating attractive websites. Synergies can be obtained by integrating personalization between several Internet properties. In this paper we propose a hierarchical Bayesian model to address these issues. Our model allows the integration of multiple properties without changing the overall structure, which makes it easily extensible across large Internet portals. It relies at its lowest level on Latent Dirichlet Allocation, while making use of latent side features for cross-property integration. We demonstrate the efficiency of our approach by analyzing data from several properties of a major Internet portal.

#index 1605927
#* Click shaping to optimize multiple objectives
#@ Deepak Agarwal;Bee-Chung Chen;Pradheep Elango;Xuanhui Wang
#t 2011
#c 0
#% 329562
#% 391356
#% 425053
#% 757953
#% 956521
#% 1073938
#% 1190057
#% 1214731
#% 1214754
#% 1227585
#% 1318590
#% 1399999
#% 1426653
#% 1450876
#% 1476450
#% 1536534
#% 1560391
#% 1699611
#! Recommending interesting content to engage users is important for web portals (e.g. AOL, MSN, Yahoo!, and many others). Existing approaches typically recommend articles to optimize for a single objective, i.e., number of clicks. However a click is only the starting point of a user's journey and subsequent downstream utilities such as time-spent and revenue are important. In this paper, we call the problem of recommending links to jointly optimize for clicks and post-click downstream utilities click shaping. We propose a multi-objective programming approach in which multiple objectives are modeled in a constrained optimization framework. Such a formulation can naturally incorporate various application-driven requirements. We study several variants that model different requirements as constraints and discuss some of the subtleties involved. We conduct our experiments on a large dataset from a real system by using a newly proposed unbiased evaluation methodology [17]. Through extensive experiments we quantify the tradeoff between different objectives under various constraints. Our experimental results show interesting characteristics of different formulations and our findings may provide valuable guidance to the design of recommendation engines for web portals.

#index 1605928
#* Response prediction using collaborative filtering with hierarchies and side-information
#@ Aditya Krishna Menon;Krishna-Prasad Chitrapura;Sachin Garg;Deepak Agarwal;Nagaraj Kota
#t 2011
#c 0
#% 783531
#% 956546
#% 989572
#% 1055832
#% 1132535
#% 1176909
#% 1190057
#% 1211829
#% 1214623
#% 1232028
#% 1260273
#% 1287231
#% 1451160
#% 1535441
#% 1560408
#% 1914424
#! In online advertising, response prediction is the problem of estimating the probability that an advertisement is clicked when displayed on a content publisher's webpage. In this paper, we show how response prediction can be viewed as a problem of matrix completion, and propose to solve it using matrix factorization techniques from collaborative filtering (CF). We point out the two crucial differences between standard CF problems and response prediction, namely the requirement of predicting probabilities rather than scores, and the issue of confidence in matrix entries. We address these issues using a matrix factorization analogue of logistic regression, and by applying a principled confidence-weighting scheme to its objective. We show how this factorization can be seamlessly combined with explicit features or side-information for pages and ads, which let us combine the benefits of both approaches. Finally, we combat the extreme sparsity of response prediction data by incorporating hierarchical information about the pages and ads into our factorization model. Experiments on three very large real-world datasets show that our model outperforms current state-of-the-art methods for response prediction.

#index 1605929
#* From bias to opinion: a transfer-learning approach to real-time sentiment analysis
#@ Pedro Henrique Calais Guerra;Adriano Veloso;Wagner Meira, Jr.;Virgílio Almeida
#t 2011
#c 0
#% 815915
#% 835018
#% 868089
#% 961278
#% 975021
#% 1000502
#% 1083652
#% 1127964
#% 1214671
#% 1214741
#% 1214749
#% 1301020
#% 1384224
#% 1399992
#% 1400031
#% 1450879
#% 1455666
#% 1464068
#% 1482447
#% 1506206
#! Real-time interaction, which enables live discussions, has become a key feature of most Web applications. In such an environment, the ability to automatically analyze user opinions and sentiments as discussions develop is a powerful resource known as real time sentiment analysis. However, this task comes with several challenges, including the need to deal with highly dynamic textual content that is characterized by changes in vocabulary and its subjective meaning and the lack of labeled data needed to support supervised classifiers. In this paper, we propose a transfer learning strategy to perform real time sentiment analysis. We identify a task - opinion holder bias prediction - which is strongly related to the sentiment analysis task; however, in constrast to sentiment analysis, it builds accurate models since the underlying relational data follows a stationary distribution. Instead of learning textual models to predict content polarity (i.e., the traditional sentiment analysis approach), we first measure the bias of social media users toward a topic, by solving a relational learning task over a network of users connected by endorsements (e.g., retweets in Twitter). We then analyze sentiments by transferring user biases to textual features. This approach works because while new terms may arise and old terms may change their meaning, user bias tends to be more consistent over time as a basic property of human behavior. Thus, we adopted user bias as the basis for building accurate classification models. We applied our model to posts collected from Twitter on two topics: the 2010 Brazilian Presidential Elections and the 2010 season of Brazilian Soccer League. Our results show that knowing the bias of only 10% of users generates an F1 accuracy level ranging from 80% to 90% in predicting user sentiment in tweets.

#index 1605930
#* User reputation in a comment rating environment
#@ Bee-Chung Chen;Jian Guo;Belle Tseng;Jie Yang
#t 2011
#c 0
#% 400847
#% 424806
#% 722904
#% 754098
#% 846184
#% 943777
#% 956516
#% 963344
#% 987228
#% 1035587
#% 1035589
#% 1055736
#% 1117695
#% 1130857
#% 1190060
#% 1190069
#% 1190129
#% 1227654
#% 1269889
#% 1281893
#% 1281981
#% 1355042
#% 1357698
#% 1370881
#% 1378224
#% 1400002
#% 1401405
#% 1447155
#% 1452857
#% 1472273
#% 1633079
#! Reputable users are valuable assets of a web site. We focus on user reputation in a comment rating environment, where users make comments about content items and rate the comments of one another. Intuitively, a reputable user posts high quality comments and is highly rated by the user community. To our surprise, we find that the quality of a comment judged editorially is almost uncorrelated with the ratings that it receives, but can be predicted using standard text features, achieving accuracy as high as the agreement between two editors! However, extracting a pure reputation signal from ratings is difficult because of data sparseness and several confounding factors in users' voting behavior. To address these issues, we propose a novel bias-smoothed tensor model and empirically show that our model significantly outperforms a number of alternatives based on Yahoo! News, Yahoo! Buzz and Epinions datasets.

#index 1605931
#* Selecting a comprehensive set of reviews
#@ Panayiotis Tsaparas;Alexandros Ntoulas;Evimaria Terzi
#t 2011
#c 0
#% 217812
#% 262112
#% 769892
#% 828958
#% 879618
#% 907490
#% 943810
#% 990210
#% 1055683
#% 1073970
#% 1098282
#% 1166473
#% 1176947
#% 1190068
#% 1190069
#% 1190093
#% 1198257
#% 1206662
#% 1250237
#% 1261574
#% 1312812
#% 1400002
#% 1400011
#% 1426270
#% 1451218
#% 1495595
#! Online user reviews play a central role in the decision-making process of users for a variety of tasks, ranging from entertainment and shopping to medical services. As user-generated reviews proliferate, it becomes critical to have a mechanism for helping the users (information consumers) deal with the information overload, and presenting them with a small comprehensive set of reviews that satisfies their information need. This is particularly important for mobile phone users, who need to make decisions quickly, and have a device with limited screen real-estate for displaying the reviews. Previous approaches have addressed the problem by ranking reviews according to their (estimated) helpfulness. However, such approaches do not account for the fact that the top few high-quality reviews may be highly redundant, repeating the same information, or presenting the same positive (or negative) perspective. In this work, we focus on the problem of selecting a comprehensive set of few high-quality reviews that cover many different aspects of the reviewed item. We formulate the problem as a maximum coverage problem, and we present a generic formalism that can model the different variants of review-set selection. We describe algorithms for the different variants we consider, and, whenever possible, we provide approximation guarantees with respect to the optimal solution. We also perform an experimental evaluation on real data in order to understand the value of coverage for users.

#index 1605932
#* Enabling fast prediction for ensemble models on data streams
#@ Peng Zhang;Jun Li;Peng Wang;Byron J. Gao;Xingquan Zhu;Li Guo
#t 2011
#c 0
#% 86950
#% 136350
#% 310500
#% 342639
#% 427199
#% 435137
#% 464195
#% 466510
#% 479462
#% 480093
#% 481455
#% 482112
#% 729932
#% 765500
#% 823327
#% 840891
#% 918001
#% 961181
#% 1063479
#% 1083714
#% 1117009
#% 1127386
#% 1176955
#% 1211775
#% 1214635
#% 1328112
#% 1451227
#% 1505314
#% 1524819
#% 1535338
#% 1558464
#% 1702194
#! Ensemble learning has become a common tool for data stream classification, being able to handle large volumes of stream data and concept drifting. Previous studies focus on building accurate prediction models from stream data. However, a linear scan of a large number of base classifiers in the ensemble during prediction incurs significant costs in response time, preventing ensemble learning from being practical for many real world time-critical data stream applications, such as Web traffic stream monitoring, spam detection, and intrusion detection. In these applications, data streams usually arrive at a speed of GB/second, and it is necessary to classify each stream record in a timely manner. To address this problem, we propose a novel Ensemble-tree (E-tree for short) indexing structure to organize all base classifiers in an ensemble for fast prediction. On one hand, E-trees treat ensembles as spatial databases and employ an R-tree like height-balanced structure to reduce the expected prediction time from linear to sub-linear complexity. On the other hand, E-trees can automatically update themselves by continuously integrating new classifiers and discarding outdated ones, well adapting to new trends and patterns underneath data streams. Experiments on both synthetic and real-world data streams demonstrate the performance of our approach.

#index 1605933
#* Online active inference and learning
#@ Josh Attenberg;Foster Provost
#t 2011
#c 0
#% 240794
#% 280413
#% 309145
#% 770771
#% 961177
#% 1083628
#% 1083692
#% 1191633
#% 1211696
#% 1211829
#% 1214647
#% 1264829
#% 1279662
#% 1289281
#% 1414128
#% 1451182
#% 1561593
#! We present a generalized framework for active inference, the selective acquisition of labels for cases at prediction time in lieu of using the estimated labels of a predictive model. We develop techniques within this framework for classifying in an online setting, for example, for classifying the stream of web pages where online advertisements are being served. Stream applications present novel complications because (i) at the time of label acquisition, we don't know the set of instances that we will eventually see, (ii) instances repeat based on some unknown (and possibly skewed) distribution. We combine ideas from decision theory, cost-sensitive learning, and online density estimation. We also introduce a method for on-line estimation of the utility distribution, which allows us to manage the budget over the stream. The resulting model tells which instances to label so that by the end of each budget period, the budget is best spent (in expectation). The main results show that: (1) our proposed approach to active inference on streams can indeed reduce error costs substantially over alternative approaches, (2) more sophisticated online estimations achieve larger reductions in error. We next discuss simultaneously conducting active inference and active learning. We show that our expected-utility active inference strategy also selects good examples for learning. We close by pointing out that our utility-distribution estimation strategy can also be applied to convert pool-based active learning techniques into budget-sensitive online active learning techniques.

#index 1605934
#* Unbiased online active learning in data streams
#@ Wei Chu;Martin Zinkevich;Lihong Li;Achint Thomas;Belle Tseng
#t 2011
#c 0
#% 116165
#% 132697
#% 169717
#% 170649
#% 236729
#% 464268
#% 715096
#% 875953
#% 961177
#% 1211696
#% 1264136
#% 1272282
#% 1274885
#! Unlabeled samples can be intelligently selected for labeling to minimize classification error. In many real-world applications, a large number of unlabeled samples arrive in a streaming manner, making it impossible to maintain all the data in a candidate pool. In this work, we focus on binary classification problems and study selective labeling in data streams where a decision is required on each sample sequentially. We consider the unbiasedness property in the sampling process, and design optimal instrumental distributions to minimize the variance in the stochastic process. Meanwhile, Bayesian linear classifiers with weighted maximum likelihood are optimized online to estimate parameters. In empirical evaluation, we collect a data stream of user-generated comments on a commercial news portal in 30 consecutive days, and carry out offline evaluation to compare various sampling strategies, including unbiased active learning, biased variants, and random sampling. Experimental results verify the usefulness of online active learning, especially in the non-stationary situation with concept drift.

#index 1605935
#* Learning to trade off between exploration and exploitation in multiclass bandit prediction
#@ Hamed Valizadegan;Rong Jin;Shijun Wang
#t 2011
#c 0
#% 425053
#% 563266
#% 763708
#% 763718
#% 871302
#% 961172
#% 1073927
#% 1399999
#% 1451141
#% 1558464
#% 1699611
#! We study multi-class bandit prediction, an online learning problem where the learner only receives a partial feedback in each trial indicating whether the predicted class label is correct. The exploration vs. exploitation tradeoff strategy is a well-known technique for online learning with incomplete feedback (i.e., bandit setup). Banditron [8], a multi-class online learning algorithm for bandit setting, maximizes the run-time gain by balancing between exploration and exploitation with a fixed tradeoff parameter. The performance of Banditron can be quite sensitive to the choice of the tradeoff parameter and therefore effective algorithms to automatically tune this parameter is desirable. In this paper, we propose three learning strategies to automatically adjust the tradeoff parameter for Banditron. Our extensive empirical study with multiple real-world data sets verifies the efficacy of the proposed approach in learning the exploration vs. exploitation tradeoff parameter.

#index 1605936
#* Linear scale semantic mining algorithms in microsoft SQL server's semantic platform
#@ Kunal Mukerjee;Todd Porter;Sorin Gherman
#t 2011
#c 0
#% 185287
#% 262045
#% 281480
#% 321635
#% 387427
#% 672613
#% 722904
#% 783642
#% 835018
#% 1275012
#! This paper describes three linear scale, incremental, and fully automatic semantic mining algorithms that are at the foundation of the new Semantic Platform being released in the next version of SQL Server. The target workload is large (10 -- 100 million) Enterprise document corpuses. At these scales, anything short of linear scale and incremental is costly to deploy. These three algorithms give rise to three weighted physical indexes: Tag Index (top keywords in each document); Document Similarity Index (top closely related documents given any document); and Semantic Phrase Similarity Index (top semantically related phrases, given any phrase), which are then query-able through the SQL interface. The need for specifically creating these three indexes was motivated by observing typical stages of document research, and gap analysis, given current tools and technology at the Enterprise. We describe the mining algorithms and architecture, and also outline some compelling user experiences that are enabled by the indexes.

#index 1605937
#* Combining file content and file relations for cloud based malware detection
#@ Yanfang Ye;Tao Li;Shenghuo Zhu;Weiwei Zhuang;Egemen Tas;Umesh Gupta;Melih Abdulhayoglu
#t 2011
#c 0
#% 136350
#% 248810
#% 309142
#% 430761
#% 464267
#% 577242
#% 664717
#% 729940
#% 769923
#% 789080
#% 803575
#% 805849
#% 829254
#% 987253
#% 989681
#% 994157
#% 1055685
#% 1117691
#% 1170857
#% 1387536
#% 1403007
#! Due to their damages to Internet security, malware (such as virus, worms, trojans, spyware, backdoors, and rootkits) detection has caught the attention not only of anti-malware industry but also of researchers for decades. Resting on the analysis of file contents extracted from the file samples, like Application Programming Interface (API) calls, instruction sequences, and binary strings, data mining methods such as Naive Bayes and Support Vector Machines have been used for malware detection. However, besides file contents, relations among file samples, such as a "Downloader" is always associated with many Trojans, can provide invaluable information about the properties of file samples. In this paper, we study how file relations can be used to improve malware detection results and develop a file verdict system (named "Valkyrie") building on a semi-parametric classifier model to combine file content and file relations together for malware detection. To the best of our knowledge, this is the first work of using both file content and file relations for malware detection. A comprehensive experimental study on a large collection of PE files obtained from the clients of anti-malware products of Comodo Security Solutions Incorporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our Valkyrie system outperform other popular anti-malware software tools such as Kaspersky AntiVirus and McAfee VirusScan, as well as other alternative data mining based detection systems.

#index 1605938
#* High-precision phrase-based document classification on a modern scale
#@ Ron Bekkerman;Matan Gavish
#t 2011
#c 0
#% 115608
#% 116149
#% 118731
#% 165115
#% 246243
#% 344447
#% 420507
#% 722930
#% 869500
#% 1074125
#% 1427679
#! We present a document classification system that employs lazy learning from labeled phrases, and argue that the system can be highly effective whenever the following property holds: most of information on document labels is captured in phrases. We call this property near sufficiency. Our research contribution is twofold: (a) we quantify the near sufficiency property using the Information Bottleneck principle and show that it is easy to check on a given dataset; (b) we reveal that in all practical cases---from small-scale to very large-scale---manual labeling of phrases is feasible: the natural language constrains the number of common phrases composed of a vocabulary to grow linearly with the size of the vocabulary. Both these contributions provide firm foundation to applicability of the phrase-based classification (PBC) framework to a variety of large-scale tasks. We deployed the PBC system on the task of job title classification, as a part of LinkedIn's data standardization effort. The system significantly outperforms its predecessor both in terms of precision and coverage. It is currently being used in LinkedIn's ad targeting product, and more applications are being developed. We argue that PBC excels in high explainability of the classification results, as well as in low development and low maintenance costs. We benchmark PBC against existing high-precision document classification algorithms and conclude that it is most useful in multilabel classification.

#index 1605939
#* Activity analysis based on low sample rate smart meters
#@ Feng Chen;Jing Dai;Bingsheng Wang;Sambit Sahu;Milind Naphade;Chang-Tien Lu
#t 2011
#c 0
#% 894552
#% 1132735
#% 1270561
#% 1399009
#% 1558464
#! Activity analysis disaggregates utility consumption from smart meters into specific usage that associates with human activities. It can not only help residents better manage their consumption for sustainable lifestyle, but also allow utility managers to devise conservation programs. Existing research efforts on disaggregating consumption focus on analyzing consumption features with high sample rates (mainly between 1 Hz ~ 1MHz). However, many smart meter deployments support sample rates at most 1/900 Hz, which challenges activity analysis with occurrences of parallel activities, difficulty of aligning events, and lack of consumption features. We propose a novel statistical framework for disaggregation on coarse granular smart meter readings by modeling fixture characteristics, household behavior, and activity correlations. This framework has been implemented into two approaches for different application scenarios, and has been deployed to serve over 300 pilot households in Dubuque, IA. Interesting activity-level consumption patterns have been identified, and the evaluation on both real and synthetic datasets has shown high accuracy on discovering washer and shower.

#index 1605940
#* Estimating the number of users behind ip addresses for combating abusive traffic
#@ Ahmed Metwally;Matt Paduano
#t 2011
#c 0
#% 319832
#% 449101
#% 804738
#% 904961
#% 954300
#% 956518
#% 963504
#% 990375
#% 1015119
#% 1051905
#% 1072118
#% 1127555
#% 1568724
#! This paper addresses estimating the number of the users of a specific application behind IP addresses (IPs). This problem is central to combating abusive traffic, such as DDoS attacks, ad click fraud and email spam. We share our experience building a general framework at Google for estimating the number of users behind IPs, called hereinafter the sizes of the IPs. The primary goal of this framework is combating abusive traffic without violating the user privacy. The estimation techniques produce statistically sound estimates of sizes relying solely on passively mining aggregated application log data, without probing machines or deploying active content like Java applets. This paper also explores using the estimated sizes to detect and filter abusive traffic. The proposed framework was used to build and deploy an ad click fraud filter at Google. The first 50M clicks tagged by the filter had a significant recall of all tagged clicks, and their false positive rate was below 1.4%. For the sake of comparison, we simulated a naive IP-based filter that does not consider the sizes of the IPs. To reach a comparable recall, the naive filter's false positive rate was 37% due to aggressive tagging.

#index 1605941
#* Data-driven multi-touch attribution models
#@ Xuhui Shao;Lexin Li
#t 2011
#c 0
#% 197394
#% 209021
#% 211820
#% 400847
#% 723244
#% 1040858
#% 1214692
#% 1451141
#! In digital advertising, attribution is the problem of assigning credit to one or more advertisements for driving the user to the desirable actions such as making a purchase. Rather than giving all the credit to the last ad a user sees, multi-touch attribution allows more than one ads to get the credit based on their corresponding contributions. Multi-touch attribution is one of the most important problems in digital advertising, especially when multiple media channels, such as search, display, social, mobile and video are involved. Due to the lack of statistical framework and a viable modeling approach, true data-driven methodology does not exist today in the industry. While predictive modeling has been thoroughly researched in recent years in the digital advertising domain, the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification. Traditional classification models fail to achieve those goals. In this paper, we first propose a bivariate metric, one measures the variability of the estimate, and the other measures the accuracy of classifying the positive and negative users. We then develop a bagged logistic regression model, which we show achieves a comparable classification accuracy as a usual logistic regression, but a much more stable estimate of individual advertising channel contributions. We also propose an intuitive and simple probabilistic model to directly quantify the attribution of different advertising channels. We then apply both the bagged logistic model and the probabilistic model to a real-world data set from a multi-channel advertising campaign for a well-known consumer software and services brand. The two models produce consistent general conclusions and thus offer useful cross-validation. The results of our attribution models also shed several important insights that have been validated by the advertising team. We have implemented the probabilistic model in the production advertising platform of the first author's company, and plan to implement the bagged logistic regression in the next product release. We believe availability of such data-driven multi-touch attribution metric and models is a break-through in the digital advertising industry.

#index 1605942
#* Bid landscape forecasting in online ad exchange marketplace
#@ Ying Cui;Ruofei Zhang;Wei Li;Jianchang Mao
#t 2011
#c 0
#% 126894
#% 448194
#% 956546
#% 989572
#% 1190080
#% 1190104
#! Display advertising has been a significant source of revenue for publishers and ad networks in online advertising ecosystem. One important business model in online display advertising is Ad Exchange marketplace, also called non-guaranteed delivery (NGD), in which advertisers buy targeted page views and audiences on a spot market through real-time auction. In this paper, we describe a bid landscape forecasting system in NGD marketplace for any advertiser campaign specified by a variety of targeting attributes. In the system, the impressions that satisfy the campaign targeting attributes are partitioned into multiple mutually exclusive samples. Each sample is one unique combination of quantified attribute values. We develop a divide-and-conquer approach that breaks down the campaign-level forecasting problem. First, utilizing a novel star-tree data structure, we forecast the bid for each sample using non-linear regression by gradient boosting decision trees. Then we employ a mixture-of-log-normal model to generate campaign-level bid distribution based on the sample-level forecasted distributions. The experiment results of a system developed with our approach show that it can accurately forecast the bid distributions for various campaigns running on the world's largest NGD advertising exchange system, outperforming two baseline methods in term of forecasting errors.

#index 1605943
#* Detecting adversarial advertisements in the wild
#@ D. Sculley;Matthew Eric Otey;Michael Pohl;Bridget Spitznagel;John Hainsworth;Yunkai Zhou
#t 2011
#c 0
#% 577224
#% 722797
#% 722904
#% 763708
#% 765519
#% 769885
#% 770754
#% 823397
#% 840882
#% 840941
#% 866696
#% 891559
#% 936754
#% 983905
#% 1014727
#% 1023420
#% 1055713
#% 1073906
#% 1211829
#% 1214754
#% 1232034
#% 1264744
#% 1451182
#% 1860941
#! In a large online advertising system, adversaries may attempt to profit from the creation of low quality or harmful advertisements. In this paper, we present a large scale data mining effort that detects and blocks such adversarial advertisements for the benefit and safety of our users. Because both false positives and false negatives have high cost, our deployed system uses a tiered strategy combining automated and semi-automated methods to ensure reliable classification. We also employ strategies to address the challenges of learning from highly skewed data at scale, allocating the effort of human experts, leveraging domain expert knowledge, and independently assessing the effectiveness of our system.

#index 1605944
#* Applying data mining techniques to address disaster information management challenges on mobile devices
#@ Li Zheng;Chao Shen;Liang Tang;Tao Li;Steve Luis;Shu-Ching Chen
#t 2011
#c 0
#% 711965
#% 915344
#% 1044470
#% 1214718
#% 1260390
#% 1399963
#% 1415735
#% 1451151
#% 1451162
#% 1466386
#! The improvement of Crisis Management and Disaster Recovery techniques are national priorities in the wake of man-made and nature inflicted calamities of the last decade. Our prior work has demonstrated that the efficiency of sharing and managing information plays an important role in business recovery efforts after disaster event. With the proliferation of smart phones and wireless tablets, professionals who have an operational responsibility in disaster situations are relying on such devices to maintain communication. Further, with the rise of social media, technology savvy consumers are also using these devices extensively for situational updates. In this paper, we address several critical tasks which can facilitate information sharing and collaboration between both private and public sector participants for major disaster recovery planning and management. We design and implement an All-Hazard Disaster Situation Browser (ADSB) system that runs on Apple's mobile operating system (iOS) and iPhone and iPad mobile devices. Our proposed techniques create a collaborative solution on a mobile platform using advanced data mining and information retrieval techniques for disaster preparedness and recovery that helps impacted communities better understand the current disaster situation and how the community is recovering. Specifically, hierarchical summarization techniques are used to generate brief reviews from a large collection of reports at different granularities; probabilistic models are proposed to dynamically generate query forms based on user's feedback; and recommendation techniques are adapted to help users identify potential contacts for report sharing and community organization. Furthermore, the developed techniques are designed to be all-hazard capable so that they can be used in earthquake, terrorism, or other unanticipated disaster situations.

#index 1605945
#* Enhancing investment decisions in P2P lending: an investor composition perspective
#@ Chunyu Luo;Hui Xiong;Wenjun Zhou;Yanhong Guo;Guishi Deng
#t 2011
#c 0
#% 574090
#% 1013484
#% 1280726
#% 1345696
#% 1355299
#% 1381504
#% 1788518
#! P2P lending, as a novel economic lending model, has imposed new challenges about how to make effective investment decisions. Indeed, a key challenge along this line is how to align the right information with the right people. For a long time, people have made tremendous efforts in establishing credit records for the borrowers. However, information from investors is still under-explored for improving investment decisions in P2P lending. To that end, we propose a data driven investment decision-making framework, which exploits the investor composition of each investment for enhancing decisions making in P2P lending. Specifically, we first build investor profiles based on quantitative analysis of past performances, risk preferences, and investment experiences of investors. Then, based on investor profiles, we develop an investor composition analysis model, which can be used to select valuable investments and improve the investment decisions. To validate the proposed model, we perform extensive experiments on the real-world data from the world's largest P2P lending marketplace. Experimental results reveal that investor composition can help us evaluate the profit potential of an investment and the decision model based on investor composition can help investors make better investment decisions.

#index 1605946
#* From market baskets to mole rats: using data mining techniques to analyze RFID data describing laboratory animal behavior
#@ Daniel P. McCloskey;Michael E. Kress;Susan P. Imberman;Igor Kushnir;Susan Briffa-Mirabella
#t 2011
#c 0
#% 152934
#% 172386
#% 232136
#% 310494
#% 464873
#% 466664
#% 729933
#% 832571
#% 1090100
#! The use of new technologies, such as RFID sensors, provides scientists with novel ways of doing experimental research. As scientists become more technologically savvy and use these techniques, the traditional approaches to data analysis fail given the huge amounts of data produced by these methods. In this paper we describe an experiment in which colonies of naked mole rats were tagged with RFID transponders. RFID sensors were strategically placed in the mole rat caging system. The goal of this experiment was to document and analyze the interactions between animals. The huge amount of data produced by the sensors was not analyzable using the traditional methods employed by behavioral neuroscience researchers. Computational methods used by data miners, such as cluster analysis, association rule mining, and graphical models, were able to scale to the data and produce knowledge and insight that was previously unknown. This paper describes in detail the experimental setup and the computational methods used.

#index 1605947
#* A pattern discovery approach to retail fraud detection
#@ Prasad Gabbur;Sharath Pankanti;Quanfu Fan;Hoang Trinh
#t 2011
#c 0
#% 96692
#% 190581
#% 425048
#% 479180
#% 891559
#% 1238506
#% 1271973
#% 1484619
#% 1667428
#% 1734409
#! A major source of revenue shrink in retail stores is the intentional or unintentional failure of proper checking out of items by the cashier. More recently, a few automated surveillance systems have been developed to monitor cashier lanes and detect non-compliant activities such as fake item checkouts or scans done with the intention of deriving monetary benefit. These systems use data from surveillance video cameras and transaction logs (TLog) recorded at the Point-of-Sale (POS). In this paper, we present a pattern discovery based approach to detect fraudulent events at the POS. Our approach is based on mining time-ordered text streams, representing retail transactions, formed from a combination of visually detected checkout related activities called primitives and barcodes from TLog data. Patterns representing single item checkouts, i.e. anchored around a single barcode, are discovered from these text streams using an efficient pattern discovery technique called Teiresias. The discovered patterns are used to build models for true and fake item scans by retaining or discarding the anchoring barcodes in those patterns respectively. A pattern matching and classification scheme is designed to robustly detect non-compliant cashier activities in the presence of noise in either the TLog or the video data. Different weighting schemes for quantifying the relative importance of the discovered patterns are explored: Frequency, Support Vector Machine (SVM) and Frequency+SVM. Using a large scale dataset recorded from retail stores, our approach discovers semantically meaningful cashier scan patterns. Our experiments also suggest that different weighting schemes result in varied false and true positive performances on the task of fake scan detection.

#index 1605948
#* Driving with knowledge from the physical world
#@ Jing Yuan;Yu Zheng;Xing Xie;Guangzhong Sun
#t 2011
#c 0
#% 64627
#% 68247
#% 193110
#% 436588
#% 864397
#% 1022268
#% 1044452
#% 1166209
#% 1245094
#% 1250109
#% 1290946
#% 1298896
#% 1393634
#% 1445728
#% 1451230
#% 1480783
#% 1784190
#! This paper presents a Cloud-based system computing customized and practically fast driving routes for an end user using (historical and real-time) traffic conditions and driver behavior. In this system, GPS-equipped taxicabs are employed as mobile sensors constantly probing the traffic rhythm of a city and taxi drivers' intelligence in choosing driving directions in the physical world. Meanwhile, a Cloud aggregates and mines the information from these taxis and other sources from the Internet, like Web maps and weather forecast. The Cloud builds a model incorporating day of the week, time of day, weather conditions, and individual driving strategies (both of the taxi drivers and of the end user for whom the route is being computed). Using this model, our system predicts the traffic conditions of a future time (when the computed route is actually driven) and performs a self-adaptive driving direction service for a particular user. This service gradually learns a user's driving behavior from the user's GPS logs and customizes the fastest route for the user with the help of the Cloud. We evaluate our service using a real-world dataset generated by over 33,000 taxis over a period of 3 months in Beijing. As a result, our service accurately estimates the travel time of a route for a user; hence finding the fastest route customized for the user.

#index 1605949
#* Interactive learning for efficiently detecting errors in insurance claims
#@ Rayid Ghani;Mohit Kumar
#t 2011
#c 0
#% 232102
#% 766436
#% 881477
#% 963244
#% 989676
#% 1197791
#% 1289639
#% 1451145
#% 1451238
#% 1558464
#% 1699589
#! Many practical data mining systems such as those for fraud detection and surveillance deal with building classifiers that are not autonomous but part of a larger interactive system with an expert in the loop. The goal of these systems is not just to maximize the performance of the classifier but to make the experts more efficient at performing their task, thus maximizing the overall Return on Investment of the system. This paper describes an interactive system for detecting payment errors in insurance claims with claim auditors in the loop. We describe an interactive claims prioritization component that uses an online cost-sensitive learning approach (more-like-this) to make the system efficient. Our interactive prioritization component is built on top of a batch classifier that has been trained to detect payment errors in health insurance claims and optimizes the interaction between the classifier and the domain experts who are consuming the results of this system. The goal is to make these auditors more efficient and effective as well as improving the classification performance of the system. The result is both a reduction in time it takes for the auditors to review and label claims as well as improving the precision of the system in finding payment errors. We show results obtained from applying this system at two major US health insurance companies indicating significant reduction in claim audit costs and potential savings of $20-$26 million/year making the insurance providers more efficient and lowering their operating costs. Our system reduces the money being wasted by providers and insurers dealing with incorrectly processed claims and makes the healthcare system more efficient.

#index 1605950
#* NIMBLE: a toolkit for the implementation of parallel data mining and machine learning algorithms on mapreduce
#@ Amol Ghoting;Prabhanjan Kambadur;Edwin Pednault;Ramakrishnan Kannan
#t 2011
#c 0
#% 152934
#% 209021
#% 789002
#% 881469
#% 983467
#% 1051998
#% 1063553
#% 1127463
#% 1299156
#% 1328061
#% 1468421
#% 1558464
#! In the last decade, advances in data collection and storage technologies have led to an increased interest in designing and implementing large-scale parallel algorithms for machine learning and data mining (ML-DM). Existing programming paradigms for expressing large-scale parallelism such as MapReduce (MR) and the Message Passing Interface (MPI) have been the de facto choices for implementing these ML-DM algorithms. The MR programming paradigm has been of particular interest as it gracefully handles large datasets and has built-in resilience against failures. However, the existing parallel programming paradigms are too low-level and ill-suited for implementing ML-DM algorithms. To address this deficiency, we present NIMBLE, a portable infrastructure that has been specifically designed to enable the rapid implementation of parallel ML-DM algorithms. The infrastructure allows one to compose parallel ML-DM algorithms using reusable (serial and parallel) building blocks that can be efficiently executed using MR and other parallel programming models; it currently runs on top of Hadoop, which is an open-source MR implementation. We show how NIMBLE can be used to realize scalable implementations of ML-DM algorithms and present a performance evaluation.

#index 1605951
#* Classification of proxy labeled examples for marketing segment generation
#@ Dean Cerrato;Rosie Jones;Avinash Gupta
#t 2011
#c 0
#% 275360
#% 481290
#% 1151051
#% 1301004
#% 1673593
#! Marketers often rely on a set of descriptive segments, or qualitative subsets of the population, to specify the audiences of targeted advertising campaigns. For example, the descriptive segment "Empty Nesters" might describe a desirable target audience for extended vacation package offers. While some segments may be easily described and generated using demographic data as ground truth, others such as "Soccer Moms" or "Urban Hipsters" reflect a combination of demographic and behavioral attributes. Ideally, these attributes would be available as the basis for ground truth labeling of a classifier training set or even direct member selection from the population. Unfortunately, ground truth attributes are often scarce or unavailable, in which case a proxy labeling scheme is needed. We devise a method for labeling a population according to criteria based on a postulated set of shopping behaviors specific to a descriptive segment. We then perform supervised binary classification on this labeled dataset in order to discover additional identifying patterns of behavior typical of labeled positives in the population. Finally, the resulting classifier is used to perform selection from the population into the segment, extending reach to cookies who may not have exhibited the postulated behaviors but likely belong in the segment. We validate our approach by comparing a descriptive segment trained on ground truth to one trained on behavioral attributes only. We show that our behavior-based approach produces classifiers having performance comparable to that of a classifier trained on the ground truth data.

#index 1605952
#* Ameliorating buyer's remorse
#@ Rakesh Agrawal;Samuel Ieong;Raja Velu
#t 2011
#c 0
#% 729921
#% 891559
#% 1273928
#! Keeping in pace with the increasing importance of commerce conducted over the Web, several e-commerce websites now provide admirable facilities for helping consumers decide what product to buy and where to buy it. However, since the prices of durable and high-tech products generally fall over time, a buyer of such products is often faced with a dilemma: Should she buy the product now or wait for cheaper prices? We present the design and implementation of Prodcast, an experimental system whose goal is to help consumers decide when to buy a product. The system makes use of forecasts of future prices based on price histories of the products, incorporating features such as sales volume, seasonality, and competition in making its recommendation. We describe techniques that are well-suited for this task and present a comprehensive evaluation of their relative merits using retail sales data for electronic products. Our back-testing of the system indicates that the system is capable of helping consumers time their purchase, resulting in significant savings to them.

#index 1605953
#* Experiences with mining temporal event sequences from electronic medical records: initial successes and some challenges
#@ Debprakash Patnaik;Patrick Butler;Naren Ramakrishnan;Laxmi Parida;Benjamin J. Keller;David A. Hanauer
#t 2011
#c 0
#% 310515
#% 420063
#% 463903
#% 544054
#% 988977
#% 1117025
#% 1373433
#% 1451158
#% 1491090
#% 1702457
#! The standardization and wider use of electronic medical records (EMR) creates opportunities for better understanding patterns of illness and care within and across medical systems. Our interest is in the temporal history of event codes embedded in patients' records, specifically investigating frequently occurring sequences of event codes across patients. In studying data from more than 1.6 million patient histories at the University of Michigan Health system we quickly realized that frequent sequences, while providing one level of data reduction, still constitute a serious analytical challenge as many involve alternate serializations of the same sets of codes. To further analyze these sequences, we designed an approach where a partial order is mined from frequent sequences of codes. We demonstrate an EMR mining system called EMRView that enables exploration of the precedence relationships to quickly identify and visualize partial order information encoded in key classes of patients. We demonstrate some important nuggets learned through our approach and also outline key challenges for future research based on our experiences.

#index 1605954
#* Understanding atrophy trajectories in alzheimer's disease using association rules on MRI images
#@ Gyorgy J. Simon;Peter W. Li;Clifford R. Jack, Jr.;Prashanthi Vemuri
#t 2011
#c 0
#% 227919
#% 280458
#% 481290
#% 835018
#! Alzheimer's disease (AD) is associated with progressive cognitive decline leading to dementia. The atrophy/loss of brain structure as seen on Magnetic Resonance Imaging (MRI) is strongly correlated with the severity of the cognitive impairment in AD. In this paper, we set out to find associations between predefined regions of the brain (regions of interest; ROIs) and the severity of the disease. Specifically, we use these associations to address two important issues in AD: (i) typical versus atypical atrophy patterns and (ii) the origin and direction of progression of atrophy, which is currently under debate. We observed that each AD-related ROI is associated with a wide range of severity and that the difference between ROIs is merely a difference in severity distribution. To model differences between the severity distribution of a subpopulation (with significant atrophy in certain ROIs) and the severity distribution of the entire population, we developed the concept of Distributional Association Rules. Using the Distributional Association Rules, we clustered ROIs into disease subsystems. We define a disease subsystem as a contiguous set of ROIs that are collectively implicated in AD. AD is known to be heterogeneous in the sense that multiple sets of ROIs may be related to the disease in a population. We proposed an enhancement to the association rule mining where the algorithm only discovers association rules with ROIs that form an approximately contiguous volume. Next, we applied these association rules to infer the direction of disease progression based on the support measures of the association rules. We also developed a novel statistical test to determine the statistical significance of the discovered direction. We evaluated the proposed method on the Mayo Clinic Alzheimer's Disease Research Center (ADRC) prospective patient cohorts. The key achievements of the methodology is that it accurately identified larger disease subsystems implicated in typical and atypical AD and it successfully mapped the directions of disease progression. The wealth of data available in Radiology gives rise to opportunities for applying this methodology to map out the trajectory of several other diseases, e.g. other neuro-degenerative diseases and cancers, most notably, breast cancer. The applicability of this method is not limited to image data, as associating predictors with severity provides valuable information in most areas of medicine as well as other industries.

#index 1605955
#* A case study in a recommender system based on purchase data
#@ Bruno Pradel;Savaneary Sean;Julien Delporte;Sébastien Guérif;Céline Rouveirol;Nicolas Usunier;Françoise Fogelman-Soulié;Frédéric Dufau-Joel
#t 2011
#c 0
#% 314933
#% 452563
#% 724655
#% 734590
#% 794935
#% 801785
#% 813966
#% 882478
#% 918842
#% 1077627
#% 1083671
#% 1119131
#% 1127488
#% 1127499
#% 1136665
#% 1176909
#% 1214688
#% 1260273
#% 1357698
#% 1358747
#% 1386007
#% 1738881
#! Collaborative filtering has been extensively studied in the context of ratings prediction. However, industrial recommender systems often aim at predicting a few items of immediate interest to the user, typically products that (s)he is likely to buy in the near future. In a collaborative filtering setting, the prediction may be based on the user's purchase history rather than rating information, which may be unreliable or unavailable. In this paper, we present an experimental evaluation of various collaborative filtering algorithms on a real-world dataset of purchase history from customers in a store of a French home improvement and building supplies chain. These experiments are part of the development of a prototype recommender system for salespeople in the store. We show how different settings for training and applying the models, as well as the introduction of domain knowledge may dramatically influence both the absolute and the relative performances of the different algorithms. To the best of our knowledge, the influence of these parameters on the quality of the predictions of recommender systems has rarely been reported in the literature.

#index 1605956
#* Detecting bots via incremental LS-SVM learning with dynamic feature adaptation
#@ Feilong Chen;Supranamaya Ranjan;Pang-Ning Tan
#t 2011
#c 0
#% 92148
#% 292664
#% 732385
#% 789855
#% 963693
#% 963708
#% 987244
#% 1020407
#% 1051907
#% 1211775
#% 1214746
#% 1468416
#% 1531249
#! As botnets continue to proliferate and grow in sophistication, so does the need for more advanced security solutions to effectively detect and defend against such attacks. In particular, botnets such as Conficker have been known to encrypt the communication packets exchanged between bots and their command-and-control server, making it costly for existing botnet detection systems that rely on deep packet inspection (DPI) methods to identify compromised machines. In this paper, we argue that, even in the face of encrypted traffic flows, botnets can still be detected by examining the set of server IP-addresses visited by a client machine in the past. However there are several challenges that must be addressed. First, the set of server IP-addresses visited by client machines may evolve dynamically. Second, the set of client machines used for training and their class labels may also change over time. To overcome these challenges, this paper presents a novel incremental LS-SVM algorithm that is adaptive to both changes in the feature set and class labels of training instances. To evaluate the performance of our algorithm, we have performed experiments on two large-scale datasets, including real-time data collected from peering routers at a large Tier-1 ISP. Experimental results showed that the proposed algorithm produces classification accuracy comparable to its batch counterpart, while consuming significantly less computational resources.

#index 1605957
#* Toward personalized care management of patients at risk: the diabetes case study
#@ Hani Neuvirth;Michal Ozery-Flato;Jianying Hu;Jonathan Laserson;Martin S. Kohn;Shahram Ebadollahi;Michal Rosen-Zvi
#t 2011
#c 0
#% 243728
#% 918507
#% 1072369
#% 1117076
#% 1451157
#% 1556645
#! Chronic diseases constitute the leading cause of mortality in the western world, have a major impact on the patients' quality of life, and comprise the bulk of healthcare costs. Nowadays, healthcare data management systems integrate large amounts of medical information on patients, including diagnoses, medical procedures, lab test results, and more. Sophisticated analysis methods are needed for utilizing these data to assist in patient management and to enhance treatment quality at reduced costs. In this study, we take a first step towards better disease management of diabetic patients by applying state-of-the art methods to anticipate the patient's future health condition and to identify patients at high risk. Two relevant outcome measures are explored: the need for emergency care services and the probability of the treatment producing a sub-optimal result, as defined by domain experts. By identifying the high-risk patients our prediction system can be used by healthcare providers to prepare both financially and logistically for the patient needs. To demonstrate a potential downstream application for the identified high-risk patients, we explore the association between the physician treating these patients and the treatment outcome, and propose a system that can assist healthcare providers in optimizing the match between a patient and a physician. Our work formulates the problem and examines the performance of several learning models on data from several thousands of patients. We further describe a pilot system built on the results of this analysis. We show that the risk for the two considered outcomes can be evaluated from patients' characteristics and that features of the patient-physician match improve the prediction accuracy for the treatment's success. These results suggest that personalized medicine can be valuable for high risk patients and raise interesting questions for future improvements.

#index 1605958
#* Matching unstructured product offers to structured product specifications
#@ Anitha Kannan;Inmar E. Givoni;Rakesh Agrawal;Ariel Fuxman
#t 2011
#c 0
#% 201889
#% 248801
#% 310516
#% 376266
#% 398003
#% 577238
#% 729913
#% 788090
#% 844289
#% 870896
#% 893143
#% 913783
#% 1016172
#% 1127423
#% 1201863
#% 1267781
#% 1272213
#% 1338592
#% 1426566
#! An e-commerce catalog typically comprises of specifications for millions of products. The search engine receives millions of sales offers from thousands of independent merchants that must be matched to the right products. We describe the challenges that a system for matching unstructured offers to structured product descriptions must address, drawing upon our experience from building such a system for Bing Shopping. The heart of our system is a data-driven component that learns the matching function off-line, which is then applied at run-time for matching offers to products. We provide the design of this and other critical components of the system as well as the details of the extensive experiments we performed to assess the readiness of the system. This system is currently deployed in an experimental Commerce Search Engine and is used to match all the offers received by Bing Shopping to the Bing product catalog.

#index 1605959
#* Predictive client-side profiles for personalized advertising
#@ Mikhail Bilenko;Matthew Richardson
#t 2011
#c 0
#% 169803
#% 297675
#% 433964
#% 731615
#% 754126
#% 818259
#% 854636
#% 869501
#% 869536
#% 956552
#% 956553
#% 983808
#% 987579
#% 989613
#% 1074101
#% 1074147
#% 1190077
#% 1190081
#% 1214642
#% 1214692
#% 1214738
#% 1246494
#% 1270275
#% 1355051
#% 1397425
#% 1450845
#% 1536505
#! Personalization is ubiquitous in modern online applications as it provides significant improvements in user experience by adapting it to inferred user preferences. However, there are increasing concerns related to issues of privacy and control of the user data that is aggregated by online systems to power personalized experiences. These concerns are particularly significant for user profile aggregation in online advertising. This paper describes a practical, learning-driven client-side personalization approach for keyword advertising platforms, an emerging application previously not addressed in literature. Our approach relies on storing user-specific information entirely within the user's control (in a browser cookie or browser local storage), thus allowing the user to view, edit or purge it at any time (e.g., via a dedicated webpage). We develop a principled, utility-based formulation for the problem of iteratively updating user profiles stored client-side, which relies on calibrated prediction of future user activity. While optimal profile construction is NP-hard for pay-per-click advertising with bid increments, it can be efficiently solved via a greedy approximation algorithm guaranteed to provide a near-optimal solution due to the fact that keyword profile utility is submodular: it exhibits the property of diminishing returns with increasing profile size. We empirically evaluate client-side keyword profiles for keyword advertising on a large-scale dataset from a major search engine. Experiments demonstrate that predictive client-side personalization allows ad platforms to retain almost all of the revenue gains from personalization even if they give users the freedom to opt out of behavior tracking backed by server-side storage. Additionally, we show that advertisers can potentially increase their return on investment significantly by utilizing bid increments for keyword profiles in their ad campaigns.

#index 1605960
#* Smoothing techniques for adaptive online language models: topic tracking in tweet streams
#@ Jimmy Lin;Rion Snow;William Morgan
#t 2011
#c 0
#% 350859
#% 748738
#% 894646
#% 935763
#% 1270706
#% 1338607
#% 1355042
#% 1399992
#% 1470582
#% 1470583
#% 1536509
#% 1560424
#% 1591965
#! We are interested in the problem of tracking broad topics such as "baseball" and "fashion" in continuous streams of short texts, exemplified by tweets from the microblogging service Twitter. The task is conceived as a language modeling problem where per-topic models are trained using hashtags in the tweet stream, which serve as proxies for topic labels. Simple perplexity-based classifiers are then applied to filter the tweet stream for topics of interest. Within this framework, we evaluate, both intrinsically and extrinsically, smoothing techniques for integrating "foreground" models (to capture recency) and "background" models (to combat sparsity), as well as different techniques for retaining history. Experiments show that unigram language models smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task.

#index 1605961
#* Democrats, republicans and starbucks afficionados: user classification in twitter
#@ Marco Pennacchiotti;Ana-Maria Popescu
#t 2011
#c 0
#% 722904
#% 1019163
#% 1040837
#% 1261563
#% 1275208
#% 1292771
#% 1450894
#% 1470582
#% 1472908
#% 1482215
#% 1482254
#% 1482657
#% 1523858
#% 1544009
#% 1560196
#! More and more technologies are taking advantage of the explosion of social media (Web search, content recommendation services, marketing, ad targeting, etc.). This paper focuses on the problem of automatically constructing user profiles, which can significantly benefit such technologies. We describe a general and robust machine learning framework for large-scale classification of social media users according to dimensions of interest. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business.

#index 1605962
#* Beyond keyword search: discovering relevant scientific literature
#@ Khalid El-Arini;Carlos Guestrin
#t 2011
#c 0
#% 283833
#% 290830
#% 297675
#% 415107
#% 445370
#% 717133
#% 760853
#% 788094
#% 875959
#% 956533
#% 983833
#% 989613
#% 989633
#% 1083647
#% 1214650
#% 1457044
#! In scientific research, it is often difficult to express information needs as simple keyword queries. We present a more natural way of searching for relevant scientific literature. Rather than a string of keywords, we define a query as a small set of papers deemed relevant to the research task at hand. By optimizing an objective function based on a fine-grained notion of influence between documents, our approach efficiently selects a set of highly relevant articles. Moreover, as scientists trust some authors more than others, results are personalized to individual preferences. In a user study, researchers found the papers recommended by our method to be more useful, trustworthy and diverse than those selected by popular alternatives, such as Google Scholar and a state-of-the-art topic modeling approach.

#index 1605963
#* Collaborative topic modeling for recommending scientific articles
#@ Chong Wang;David M. Blei
#t 2011
#c 0
#% 280852
#% 301259
#% 578684
#% 722904
#% 1073982
#% 1176909
#% 1176959
#% 1211838
#% 1214623
#% 1260273
#% 1355025
#% 1535449
#! Researchers have access to large online archives of scientific articles. As a consequence, finding relevant papers has become more difficult. Newly formed online communities of researchers sharing citations provides a new way to solve this problem. In this paper, we develop an algorithm to recommend scientific articles to users of an online community. Our approach combines the merits of traditional collaborative filtering and probabilistic topic modeling. It provides an interpretable latent structure for users and items, and can form recommendations about both existing and newly published articles. We study a large subset of data from CiteULike, a bibliography sharing service, and show that our algorithm provides a more effective recommender system than traditional collaborative filtering.

#index 1605964
#* Partially labeled topic models for interpretable text mining
#@ Daniel Ramage;Christopher D. Manning;Susan Dumais
#t 2011
#c 0
#% 722904
#% 876017
#% 989621
#% 1014678
#% 1035588
#% 1083666
#% 1211848
#% 1310458
#% 1338553
#% 1417055
#% 1455666
#! Abstract Much of the world's electronic text is annotated with human-interpretable labels, such as tags on web pages and subject codes on academic publications. Effective text mining in this setting requires models that can flexibly account for the textual patterns that underlie the observed labels while still discovering unlabeled topics. Neither supervised classification, with its focus on label prediction, nor purely unsupervised learning, which does not model the labels explicitly, is appropriate. In this paper, we present two new partially supervised generative models of labeled text, Partially Labeled Dirichlet Allocation (PLDA) and the Partially Labeled Dirichlet Process (PLDP). These models make use of the unsupervised learning machinery of topic models to discover the hidden topics within each label, as well as unlabeled, corpus-wide latent topics. We explore applications with qualitative case studies of tagged web pages from del.icio.us and PhD dissertation abstracts, demonstrating improved model interpretability over traditional topic models. We use the many tags present in our del.icio.us dataset to quantitatively demonstrate the new models' higher correlation with human relatedness scores over several strong baselines.

#index 1605965
#* Refining causality: who copied from whom?
#@ Tristan Mark Snowsill;Nick Fyson;Tijl De Bie;Nello Cristianini
#t 2011
#c 0
#% 47710
#% 235941
#% 253712
#% 289010
#% 453479
#% 832271
#% 1116726
#% 1214671
#% 1268055
#% 1451242
#% 1587450
#! Inferring causal networks behind observed data is an active area of research with wide applicability to areas such as epidemiology, microbiology and social science. In particular recent research has focused on identifying how information propagates through the Internet. This research has so far only used temporal features of observations, and while reasonable results have been achieved, there is often further information which can be used. In this paper we show that additional features of the observed data can be used very effectively to improve an existing method. Our particular example is one of inferring an underlying network for how text is reused in the Internet, although the general approach is applicable to other inference methods and information sources. We develop a method to identify how a piece of text evolves as it moves through an underlying network and how substring information can be used to narrow down where in the evolutionary process a particular observation at a node lies. Hence we narrow down the number of ways the node could have acquired the infection. Text reuse is detected using a suffix tree which is also used to identify the substring relations between chunks of reused text. We then use a modification of the NetCover method to infer the underlying network. Experimental results -- on both synthetic and real life data -- show that using more information than just timing leads to greater accuracy in the inferred networks.

#index 1605966
#* Conditional topical coding: an efficient topic model conditioned on rich features
#@ Jun Zhu;Ni Lao;Ning Chen;Eric P. Xing
#t 2011
#c 0
#% 280819
#% 299264
#% 464434
#% 643004
#% 722904
#% 768632
#% 1073906
#% 1211730
#% 1211744
#% 1211848
#% 1305467
#% 1385969
#% 1417055
#% 1451218
#% 1523858
#% 1536586
#% 1742154
#! Probabilistic topic models have shown remarkable success in many application domains. However, a probabilistic conditional topic model can be extremely inefficient when considering a rich set of features because it needs to define a normalized distribution, which usually involves a hard-to-compute partition function. This paper presents conditional topical coding (CTC), a novel formulation of conditional topic models which is non-probabilistic. CTC relaxes the normalization constraints as in probabilistic models and learns non-negative document codes and word codes. CTC does not need to define a normalized distribution and can efficiently incorporate a rich set of features for improved topic discovery and prediction tasks. Moreover, CTC can directly control the sparsity of inferred representations by using appropriate regularization. We develop an efficient and easy-to-implement coordinate descent learning algorithm, of which each coding substep has a closed-form solution. Finally, we demonstrate the advantages of CTC on online review analysis datasets. Our results show that conditional topical coding can achieve state-of-the-art prediction performance and is much more efficient in training (one order of magnitude faster) and testing (two orders of magnitude faster) than probabilistic conditional topic models.

#index 1605967
#* Tracking trends: incorporating term volume into temporal topic models
#@ Liangjie Hong;Dawei Yin;Jian Guo;Brian D. Davison
#t 2011
#c 0
#% 339558
#% 722904
#% 823344
#% 869516
#% 875959
#% 879587
#% 881498
#% 989623
#% 989650
#% 1166524
#% 1211848
#% 1292714
#% 1338553
#% 1355025
#% 1400078
#% 1451206
#% 1451248
#! Text corpora with documents from a range of time epochs are natural and ubiquitous in many fields, such as research papers, newspaper articles and a variety of types of recently emerged social media. People not only would like to know what kind of topics can be found from these data sources but also wish to understand the temporal dynamics of these topics and predict certain properties of terms or documents in the future. Topic models are usually utilized to find latent topics from text collections, and recently have been applied to temporal text corpora. However, most proposed models are general purpose models to which no real tasks are explicitly associated. Therefore, current models may be difficult to apply in real-world applications, such as the problems of tracking trends and predicting popularity of keywords. In this paper, we introduce a real-world task, tracking trends of terms, to which temporal topic models can be applied. Rather than building a general-purpose model, we propose a new type of topic model that incorporates the volume of terms into the temporal dynamics of topics and optimizes estimates of term volumes. In existing models, trends are either latent variables or not considered at all which limits the potential for practical use of trend information. In contrast, we combine state-space models with term volumes with a supervised learning model, enabling us to effectively predict the volume in the future, even without new documents. In addition, it is straightforward to obtain the volume of latent topics as a by-product of our model, demonstrating the superiority of utilizing temporal topic models over traditional time-series tools (e.g., autoregressive models) to tackle this kind of problem. The proposed model can be further extended with arbitrary word-level features which are evolving over time. We present the results of applying the model to two datasets with long time periods and show its effectiveness over non-trivial baselines.

#index 1605968
#* Differentially private data release for data mining
#@ Noman Mohammed;Rui Chen;Benjamin C.M. Fung;Philip S. Yu
#t 2011
#c 0
#% 136350
#% 443463
#% 576110
#% 576761
#% 577239
#% 800515
#% 864406
#% 874989
#% 881546
#% 937550
#% 951837
#% 975045
#% 977011
#% 1015140
#% 1022247
#% 1029084
#% 1061644
#% 1070890
#% 1083653
#% 1214684
#% 1214750
#% 1217148
#% 1217156
#% 1328175
#% 1370254
#% 1372692
#% 1372735
#% 1381029
#% 1426322
#% 1426328
#% 1426329
#% 1426454
#% 1426456
#% 1451189
#% 1451190
#% 1478165
#% 1523886
#% 1523888
#% 1584349
#% 1670071
#% 1740518
#! Privacy-preserving data publishing addresses the problem of disclosing sensitive data when mining for useful information. Among the existing privacy models, ∈-differential privacy provides one of the strongest privacy guarantees and has no assumptions about an adversary's background knowledge. Most of the existing solutions that ensure ∈-differential privacy are based on an interactive model, where the data miner is only allowed to pose aggregate queries to the database. In this paper, we propose the first anonymization algorithm for the non-interactive setting based on the generalization technique. The proposed solution first probabilistically generalizes the raw data and then adds noise to guarantee ∈-differential privacy. As a sample application, we show that the anonymized data can be used effectively to build a decision tree induction classifier. Experimental results demonstrate that the proposed non-interactive anonymization algorithm is scalable and performs better than the existing solutions for classification analysis.

#index 1605969
#* k-NN as an implementation of situation testing for discrimination discovery and prevention
#@ Binh Thanh Luong;Salvatore Ruggieri;Franco Turini
#t 2011
#c 0
#% 136350
#% 1384972
#% 1426604
#% 1456835
#! With the support of the legally-grounded methodology of situation testing, we tackle the problems of discrimination discovery and prevention from a dataset of historical decisions by adopting a variant of k-NN classification. A tuple is labeled as discriminated if we can observe a significant difference of treatment among its neighbors belonging to a protected-by-law group and its neighbors not belonging to it. Discrimination discovery boils down to extracting a classification model from the labeled tuples. Discrimination prevention is tackled by changing the decision value for tuples labeled as discriminated before training a classifier. The approach of this paper overcomes legal weaknesses and technical limitations of existing proposals.

#index 1605970
#* Exploiting vulnerability to secure user privacy on a social networking site
#@ Pritam Gundecha;Geoffrey Barbier;Huan Liu
#t 2011
#c 0
#% 728955
#% 840722
#% 949164
#% 982701
#% 1035589
#% 1080356
#% 1083699
#% 1190107
#% 1190108
#% 1214703
#% 1246361
#% 1259854
#% 1314378
#% 1399968
#% 1463608
#% 1471450
#! As (one's) social network expands, a user's privacy protection goes beyond his privacy settings and becomes a social networking problem. In this research, we aim to address some critical issues related to privacy protection: Would the highest privacy settings guarantee a secure protection? Given the open nature of social networking sites, is it possible to manage one's privacy protection? With the diversity of one's social media friends, how can one figure out an effective approach to balance between vulnerability and privacy? We present a novel way to define a vulnerable friend from an individual user's perspective is dependent on whether or not the user's friends' privacy settings protect the friend and the individual's network of friends (which includes the user). As a single vulnerable friend in a user's social network might place all friends at risk, we resort to experiments and observe how much security an individual user can improve by unfriending a vulnerable friend. We also show how privacy weakens if newly accepted friends are unguarded or unprotected. This work provides a large-scale evaluation of new security and privacy indexes using a Facebook dataset. We present and discuss a new perspective for reasoning about social networking security. When a user accepts a new friend, the user should ensure that the new friend is not an increased security risk with the potential of negatively impacting the entire friend network. Additionally, by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends, it is possible to further improve security and privacy without changing the social networking site's existing architecture.

#index 1605971
#* On the semantic annotation of places in location-based social networks
#@ Mao Ye;Dong Shou;Wang-Chien Lee;Peifeng Yin;Krzysztof Janowicz
#t 2011
#c 0
#% 235377
#% 248810
#% 311034
#% 387427
#% 458379
#% 478470
#% 723186
#% 915344
#% 953324
#% 961278
#% 1083652
#% 1132470
#% 1269763
#% 1269764
#% 1289267
#% 1298869
#% 1476139
#% 1558464
#% 1598366
#% 1622260
#! In this paper, we develop a semantic annotation technique for location-based social networks to automatically annotate all places with category tags which are a crucial prerequisite for location search, recommendation services, or data cleaning. Our annotation algorithm learns a binary support vector machine (SVM) classifier for each tag in the tag space to support multi-label classification. Based on the check-in behavior of users, we extract features of places from i) explicit patterns (EP) of individual places and ii) implicit relatedness (IR) among similar places. The features extracted from EP are summarized from all check-ins at a specific place. The features from IR are derived by building a novel network of related places (NRP) where similar places are linked by virtual edges. Upon NRP, we determine the probability of a category tag for each place by exploring the relatedness of places. Finally, we conduct a comprehensive experimental study based on a real dataset collected from a location-based social network, Whrrl. The results demonstrate the suitability of our approach and show the strength of taking both EP and IR into account in feature extraction.

#index 1605972
#* Sparsification of influence networks
#@ Michael Mathioudakis;Francesco Bonchi;Carlos Castillo;Aristides Gionis;Antti Ukkonen
#t 2011
#c 0
#% 342596
#% 468533
#% 577217
#% 729923
#% 754107
#% 832271
#% 949164
#% 989613
#% 1064158
#% 1107420
#% 1130857
#% 1214641
#% 1214671
#% 1313037
#% 1348666
#% 1355040
#% 1451242
#% 1451243
#% 1535254
#% 1535380
#% 1584827
#% 1669913
#% 1726658
#! We present Spine, an efficient algorithm for finding the "backbone" of an influence network. Given a social graph and a log of past propagations, we build an instance of the independent-cascade model that describes the propagations. We aim at reducing the complexity of that model, while preserving most of its accuracy in describing the data. We show that the problem is inapproximable and we present an optimal, dynamic-programming algorithm, whose search space, albeit exponential, is typically much smaller than that of the brute force, exhaustive-search approach. Seeking a practical, scalable approach to sparsification, we devise Spine, a greedy, efficient algorithm with practically little compromise in quality. We claim that sparsification is a fundamental data-reduction operation with many applications, ranging from visualization to exploratory and descriptive data analysis. As a proof of concept, we use Spine on real-world datasets, revealing the backbone of their influence-propagation networks. Moreover, we apply Spine as a pre-processing step for the influence-maximization problem, showing that computations on sparsified models give up little accuracy, but yield significant improvements in terms of scalability.

#index 1605973
#* Leveraging collaborative tagging for web item design
#@ Mahashweta Das;Gautam Das;Vagelis Hristidis
#t 2011
#c 0
#% 333854
#% 404719
#% 768305
#% 855601
#% 869608
#% 1074117
#% 1075132
#% 1246154
#% 1451236
#! The popularity of collaborative tagging sites has created new challenges and opportunities for designers of web items, such as electronics products, travel itineraries, popular blogs, etc. An increasing number of people are turning to online reviews and user-specified tags to choose from among competing items. This creates an opportunity for designers to build items that are likely to attract desirable tags when published. In this paper, we consider a novel optimization problem: given a training dataset of existing items with their user-submitted tags, and a query set of desirable tags, design the k best new items expected to attract the maximum number of desirable tags. We show that this problem is NP-Complete, even if simple Naive Bayes Classifiers are used for tag prediction. We present two principled algorithms for solving this problem: (a) an exact "two-tier" algorithm (based on top-k querying techniques), which performs much better than the naive brute-force algorithm and works well for moderate problem instances, and (b) a novel polynomial-time approximation algorithm with provable error bound for larger problem instances. We conduct detailed experiments on synthetic and real data crawled from the web to evaluate the efficiency and quality of our proposed algorithms.

#index 1605974
#* Stackelberg games for adversarial prediction problems
#@ Michael Brückner;Tobias Scheffer
#t 2011
#c 0
#% 563100
#% 722901
#% 845502
#% 875989
#% 1073899
#% 1318789
#% 1472971
#% 1538192
#! The standard assumption of identically distributed training and test data is violated when test data are generated in response to a predictive model. This becomes apparent, for example, in the context of email spam filtering, where an email service provider employs a spam filter and the spam sender can take this filter into account when generating new emails. We model the interaction between learner and data generator as a Stackelberg competition in which the learner plays the role of the leader and the data generator may react on the leader's move. We derive an optimization problem to determine the solution of this game and present several instances of the Stackelberg prediction game. We show that the Stackelberg prediction game generalizes existing prediction models. Finally, we explore properties of the discussed models empirically in the context of email spam filtering.

#index 1605975
#* Leakage in data mining: formulation, detection, and avoidance
#@ Shachar Kaufman;Saharon Rosset;Claudia Perlich
#t 2011
#c 0
#% 204531
#% 269634
#% 338609
#% 644273
#% 768666
#% 1038332
#% 1200869
#% 1228061
#% 1400712
#% 1467912
#! Deemed "one of the top ten data mining mistakes", leakage is essentially the introduction of information about the data mining target, which should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical i.i.d. assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected.

#index 1605976
#* An information theoretic framework for data mining
#@ Tijl De Bie
#t 2011
#c 0
#% 697
#% 244336
#% 297675
#% 420082
#% 575973
#% 630986
#% 769893
#% 915231
#% 985037
#% 1001365
#% 1100166
#% 1108664
#% 1127267
#% 1214659
#% 1403618
#% 1561605
#% 1618915
#! We formalize the data mining process as a process of information exchange, defined by the following key components. The data miner's state of mind is modeled as a probability distribution, called the background distribution, which represents the uncertainty and misconceptions the data miner has about the data. This model initially incorporates any prior (possibly incorrect) beliefs a data miner has about the data. During the data mining process, properties of the data (to which we refer as patterns) are revealed to the data miner, either in batch, one by one, or even interactively. This acquisition of information in the data mining process is formalized by updates to the background distribution to account for the presence of the found patterns. The proposed framework can be motivated using concepts from information theory and game theory. Understanding it from this perspective, it is easy to see how it can be extended to more sophisticated settings, e.g. where patterns are probabilistic functions of the data (thus allowing one to account for noise and errors in the data mining process, and allowing one to study data mining techniques based on subsampling the data). The framework then models the data mining process using concepts from information geometry, and I-projections in particular. The framework can be used to help in designing new data mining algorithms that maximize the efficiency of the information exchange from the algorithm to the data miner.

#index 1605977
#* Tell me what i need to know: succinctly summarizing data with itemsets
#@ Michael Mampaey;Nikolaj Tatti;Jilles Vreeken
#t 2011
#c 0
#% 227919
#% 248012
#% 464873
#% 577214
#% 769893
#% 823356
#% 823361
#% 881542
#% 948087
#% 1001365
#% 1083656
#% 1100166
#% 1108910
#% 1127267
#% 1214659
#% 1214686
#% 1291602
#% 1348648
#% 1451168
#% 1456837
#% 1495603
#! Data analysis is an inherently iterative process. That is, what we know about the data greatly determines our expectations, and hence, what result we would find the most interesting. With this in mind, we introduce a well-founded approach for succinctly summarizing data with a collection of itemsets; using a probabilistic maximum entropy model, we iteratively find the most interesting itemset, and in turn update our model of the data accordingly. As we only include itemsets that are surprising with regard to the current model, the summary is guaranteed to be both descriptive and non-redundant. The algorithm that we present can either mine the top-k most interesting itemsets, or use the Bayesian Information Criterion to automatically identify the model containing only the itemsets most important for describing the data. Or, in other words, it will 'tell you what you need to know'. Experiments on synthetic and benchmark data show that the discovered summaries are succinct, and correctly identify the key patterns in the data. The models they form attain high likelihoods, and inspection shows that they summarize the data well with increasingly specific, yet non-redundant itemsets.

#index 1605978
#* Direct local pattern sampling by efficient two-step random procedures
#@ Mario Boley;Claudio Lucchese;Daniel Paurat;Thomas Gärtner
#t 2011
#c 0
#% 58608
#% 139183
#% 232136
#% 280409
#% 299985
#% 420126
#% 464714
#% 477497
#% 546694
#% 614619
#% 722920
#% 729418
#% 955321
#% 961134
#% 1072518
#% 1083649
#% 1108863
#% 1206650
#% 1311697
#% 1328170
#% 1405146
#! We present several exact and highly scalable local pattern sampling algorithms. They can be used as an alternative to exhaustive local pattern discovery methods (e.g, frequent set mining or optimistic-estimator-based subgroup discovery) and can substantially improve efficiency as well as controllability of pattern discovery processes. While previous sampling approaches mainly rely on the Markov chain Monte Carlo method, our procedures are direct, i.e., non process-simulating, sampling algorithms. The advantages of these direct methods are an almost optimal time complexity per pattern as well as an exactly controlled distribution of the produced patterns. Namely, the proposed algorithms can sample (item-)sets according to frequency, area, squared frequency, and a class discriminativity measure. Experiments demonstrate that these procedures can improve the accuracy of pattern-based models similar to frequent sets and often also lead to substantial gains in terms of scalability.

#index 1605979
#* Mining frequent closed graphs on evolving data streams
#@ Albert Bifet;Geoff Holmes;Bernhard Pfahringer;Ricard Gavaldà
#t 2011
#c 0
#% 414993
#% 629603
#% 629708
#% 729938
#% 745515
#% 785339
#% 789011
#% 798044
#% 867873
#% 881520
#% 905703
#% 960305
#% 985041
#% 1035127
#% 1041263
#% 1077173
#% 1083627
#% 1131675
#% 1214635
#% 1312802
#% 1372657
#% 1472282
#% 1523882
#% 1538536
#% 1718448
#! Graph mining is a challenging task by itself, and even more so when processing data streams which evolve in real-time. Data stream mining faces hard constraints regarding time and space for processing, and also needs to provide for concept drift detection. In this paper we present a framework for studying graph pattern mining on time-varying streams. Three new methods for mining frequent closed subgraphs are presented. All methods work on coresets of closed subgraphs, compressed representations of graph sets, and maintain these sets in a batch-incremental manner, but use different approaches to address potential concept drift. An evaluation study on datasets comprising up to four million graphs explores the strength and limitations of the proposed methods. To the best of our knowledge this is the first work on mining frequent closed subgraphs in non-stationary data streams.

#index 1605980
#* Latent topic feedback for information retrieval
#@ David Andrzejewski;David Buttler
#t 2011
#c 0
#% 452641
#% 722904
#% 789959
#% 838540
#% 857180
#% 876017
#% 879587
#% 943042
#% 967300
#% 1055719
#% 1083679
#% 1152447
#% 1181094
#% 1195831
#% 1211693
#% 1267766
#% 1434145
#% 1450874
#% 1451024
#% 1470574
#% 1523858
#% 1544073
#% 1565542
#% 1650387
#! We consider the problem of a user navigating an unfamiliar corpus of text documents where document metadata is limited or unavailable, the domain is specialized, and the user base is small. These challenging conditions may hold, for example, within an organization such as a business or government agency. We propose to augment standard keyword search with user feedback on latent topics. These topics are automatically learned from the corpus in an unsupervised manner and presented alongside search results. User feedback is then used to reformulate the original query, resulting in improved information retrieval performance in our experiments.

#index 1605981
#* Localized factor models for multi-context recommendation
#@ Deepak Agarwal;Bee-Chung Chen;Bo Long
#t 2011
#c 0
#% 252011
#% 271060
#% 280852
#% 769886
#% 770804
#% 815211
#% 840898
#% 876081
#% 939332
#% 983814
#% 983828
#% 983865
#% 983899
#% 989592
#% 1083678
#% 1083696
#% 1190066
#% 1214623
#% 1260273
#% 1261539
#% 1264788
#% 1269766
#% 1272110
#% 1358747
#! Combining correlated information from multiple contexts can significantly improve predictive accuracy in recommender problems. Such information from multiple contexts is often available in the form of several incomplete matrices spanning a set of entities like users, items, features, and so on. Existing methods simultaneously factorize these matrices by sharing a single set of factors for entities across all contexts. We show that such a strategy may introduce significant bias in estimates and propose a new model that ameliorates this issue by positing local, context-specific factors for entities. To avoid over-fitting in contexts with sparse data, the local factors are connected through a shared global model. This sharing of parameters allows information to flow across contexts through multivariate regressions among local factors, instead of enforcing exactly the same factors for an entity, everywhere. Model fitting is done in an EM framework, we show that the E-step can be fitted through a fast multi-resolution Kalman filter algorithm that ensures scalability. Experiments on benchmark and real-world Yahoo! datasets clearly illustrate the usefulness of our approach. Our model significantly improves predictive accuracy, especially in cold-start scenarios.

#index 1605982
#* Latent aspect rating analysis without aspect keyword supervision
#@ Hongning Wang;Yue Lu;ChengXiang Zhai
#t 2011
#c 0
#% 303620
#% 577246
#% 722904
#% 769892
#% 854646
#% 879595
#% 907489
#% 939346
#% 939848
#% 939896
#% 956510
#% 1055682
#% 1055683
#% 1190068
#% 1214734
#% 1250237
#% 1292503
#% 1451218
#% 1481541
#% 1536586
#! Mining detailed opinions buried in the vast amount of review text data is an important, yet quite challenging task with widespread applications in multiple domains. Latent Aspect Rating Analysis (LARA) refers to the task of inferring both opinion ratings on topical aspects (e.g., location, service of a hotel) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings. A major limitation of previous work on LARA is the assumption of pre-specified aspects by keywords. However, the aspect information is not always available, and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews. In this paper, we propose a unified generative model for LARA, which does not need pre-specified aspect keywords and simultaneously mines 1) latent topical aspects, 2) ratings on each identified aspect, and 3) weights placed on different aspects by a reviewer. Experiment results on two different review data sets demonstrate that the proposed model can effectively perform the Latent Aspect Rating Analysis task without the supervision of aspect keywords. Because of its generality, the proposed model can be applied to explore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interesting application tasks, such as aspect-based opinion summarization, personalized entity ranking and recommendation, and reviewer behavior analysis.

#index 1605983
#* Density estimation trees
#@ Parikshit Ram;Alexander G. Gray
#t 2011
#c 0
#% 197394
#% 361100
#% 769938
#% 771624
#% 789003
#% 875957
#% 891559
#% 1011200
#% 1181239
#% 1549990
#% 1673573
#! In this paper we develop density estimation trees (DETs), the natural analog of classification trees and regression trees, for the task of density estimation. We consider the estimation of a joint probability density function of a d-dimensional random vector X and define a piecewise constant estimator structured as a decision tree. The integrated squared error is minimized to learn the tree. We show that the method is nonparametric: under standard conditions of nonparametric density estimation, DETs are shown to be asymptotically consistent. In addition, being decision trees, DETs perform automatic feature selection. They empirically exhibit the interpretability, adaptability and feature selection properties of supervised decision trees while incurring slight loss in accuracy over other nonparametric density estimators. Hence they might be able to avoid the curse of dimensionality if the true density is sparse in dimensions. We believe that density estimation trees provide a new tool for exploratory data analysis with unique capabilities.

#index 1605984
#* Unsupervised clustering of multidimensional distributions using earth mover distance
#@ David Applegate;Tamraparni Dasu;Shankar Krishnan;Simon Urbanek
#t 2011
#c 0
#% 325683
#% 420123
#% 436569
#% 718437
#% 840964
#% 975141
#% 1221417
#% 1816287
#! Multidimensional distributions are often used in data mining to describe and summarize different features of large datasets. It is natural to look for distinct classes in such datasets by clustering the data. A common approach entails the use of methods like k-means clustering. However, the k-means method inherently relies on the Euclidean metric in the embedded space and does not account for additional topology underlying the distribution. In this paper, we propose using Earth Mover Distance (EMD) to compare multidimensional distributions. For a n-bin histogram, the EMD is based on a solution to the transportation problem with time complexity O(n3 log n). To mitigate the high computational cost of EMD, we propose an approximation that reduces the cost to linear time. Given the large size of our dataset a fast approximation is crucial for this application. Other notions of distances such as the information theoretic Kullback-Leibler divergence and statistical χ2 distance, account only for the correspondence between bins with the same index, and do not use information across bins, and are sensitive to bin size. A cross-bin distance measure like EMD is not affected by binning differences and meaningfully matches the perceptual notion of "nearness". Our technique is simple, efficient and practical for clustering distributions. We demonstrate the use of EMD on a real-world application of analyzing 411,550 anonymous mobility usage patterns which are defined as distributions over a manifold. EMD allows us to represent inherent relationships in this space, and enables us to successfully cluster even sparse signatures.

#index 1605985
#* Online heterogeneous mixture modeling with marginal and copula selection
#@ Ryohei Fujimaki;Yasuhiro Sogawa;Satoshi Morinaga
#t 2011
#c 0
#% 169358
#% 277483
#% 300136
#% 331916
#% 749406
#% 771924
#% 855602
#% 855610
#% 925382
#% 1176944
#% 1333052
#% 1558464
#% 1816709
#! This paper proposes an online mixture modeling methodology in which individual components can have different marginal distributions and dependency structures. Mixture models have been widely studied and applied to various application areas, including density estimation, fraud/failure detection, image segmentation, etc. Previous research has been almost exclusively focused on mixture models having components of a single type (e.g., a Gaussian mixture model.) However, recent growing needs for complicated data modeling necessitate the use of more flexible mixture models (e.g., a mixture of a lognormal distribution for medical costs and a Gaussian distribution for blood pressure, for medical analytics.) Our key ideas include: 1) separating marginal distributions and their dependencies using copulas and 2) online extension of a recently-developed "expectation minimization of description length," which enable us to efficiently learn types of both marginal distributions and copulas as well as their parameters. The proposed method provides not only good performance in applications, but also scalable, automatic model selection, which greatly reduces the intensive modeling costs in data mining processes. We show that the proposed method outperforms state-of-the-art methods in application to density estimation and to anomaly detection.

#index 1605986
#* Dual active feature and sample selection for graph classification
#@ Xiangnan Kong;Wei Fan;Philip S. Yu
#t 2011
#c 0
#% 116165
#% 236729
#% 466644
#% 466887
#% 478274
#% 629603
#% 629708
#% 727845
#% 769891
#% 769951
#% 770771
#% 850729
#% 876080
#% 1063502
#% 1073898
#% 1083649
#% 1100053
#% 1214713
#% 1227871
#% 1396658
#% 1426575
#% 1451219
#% 1558464
#% 1673681
#! Graph classification has become an important and active research topic in the last decade. Current research on graph classification focuses on mining discriminative subgraph features under supervised settings. The basic assumption is that a large number of labeled graphs are available. However, labeling graph data is quite expensive and time consuming for many real-world applications. In order to reduce the labeling cost for graph data, we address the problem of how to select the most important graph to query for the label. This problem is challenging and different from conventional active learning problems because there is no predefined feature vector. Moreover, the subgraph enumeration problem is NP-hard. The active sample selection problem and the feature selection problem are correlated for graph data. Before we can solve the active sample selection problem, we need to find a set of optimal subgraph features. To address this challenge, we demonstrate how one can simultaneously estimate the usefulness of a query graph and a set of subgraph features. The idea is to maximize the dependency between subgraph features and graph labels using an active learning framework. We propose a branch-and-bound algorithm to search for the optimal query graph and optimal features simultaneously. Empirical studies on nine real-world tasks demonstrate that the proposed method can obtain better accuracy on graph data than alternative approaches.

#index 1605987
#* It's who you know: graph mining using recursive structural features
#@ Keith Henderson;Brian Gallagher;Lei Li;Leman Akoglu;Tina Eliassi-Rad;Hanghang Tong;Christos Faloutsos
#t 2011
#c 0
#% 283833
#% 438553
#% 729983
#% 823342
#% 838412
#% 926881
#% 983828
#% 983865
#% 1083652
#% 1083655
#% 1130907
#% 1292561
#% 1394202
#% 1451155
#% 1451163
#% 1481634
#% 1491558
#% 1535421
#% 1535470
#% 1536542
#% 1710593
#! Given a graph, how can we extract good features for the nodes? For example, given two large graphs from the same domain, how can we use information in one to do classification in the other (i.e., perform across-network classification or transfer learning on graphs)? Also, if one of the graphs is anonymized, how can we use information in one to de-anonymize the other? The key step in all such graph mining tasks is to find effective node features. We propose ReFeX (Recursive Feature eXtraction), a novel algorithm, that recursively combines local (node-based) features with neighborhood (egonet-based) features; and outputs regional features -- capturing "behavioral" information. We demonstrate how these powerful regional features can be used in within-network and across-network classification and de-anonymization tasks -- without relying on homophily, or the availability of class labels. The contributions of our work are as follows: (a) ReFeX is scalable and (b) it is effective, capturing regional ("behavioral") information in large graphs. We report experiments on real graphs from various domains with over 1M edges, where ReFeX outperforms its competitors on typical graph mining tasks like network classification and de-anonymization.

#index 1605988
#* Triangle listing in massive networks and its applications
#@ Shumo Chu;James Cheng
#t 2011
#c 0
#% 41684
#% 247009
#% 278835
#% 291940
#% 300118
#% 379485
#% 547286
#% 594021
#% 599542
#% 605157
#% 749449
#% 765682
#% 874902
#% 1083625
#% 1124590
#% 1214705
#% 1256460
#% 1426539
#% 1504829
#% 1523970
#% 1560415
#% 1594586
#% 1625106
#% 1719564
#! Triangle listing is one of the fundamental algorithmic problems whose solution has numerous applications especially in the analysis of complex networks, such as the computation of clustering coefficient, transitivity, triangular connectivity, etc. Existing algorithms for triangle listing are mainly in-memory algorithms, whose performance cannot scale with the massive volume of today's fast growing networks. When the input graph cannot fit into main memory, triangle listing requires random disk accesses that can incur prohibitively large I/O cost. Some streaming and sampling algorithms have been proposed but these are approximation algorithms. We propose an I/O-efficient algorithm for triangle listing. Our algorithm is exact and avoids random disk access. Our results show that our algorithm is scalable and outperforms the state-of-the-art local triangle estimation algorithm.

#index 1605989
#* Fast clustering using MapReduce
#@ Alina Ene;Sungjin Im;Benjamin Moseley
#t 2011
#c 0
#% 249177
#% 249183
#% 347263
#% 449321
#% 578388
#% 593913
#% 656745
#% 743962
#% 777988
#% 795445
#% 801683
#% 847165
#% 963669
#% 991230
#% 1039668
#% 1106986
#% 1354118
#% 1399956
#% 1426513
#% 1429325
#% 1442067
#% 1467704
#% 1484141
#! Clustering problems have numerous applications and are becoming more challenging as the size of the data increases. In this paper, we consider designing clustering algorithms that can be used in MapReduce, the most popular programming environment for processing large datasets. We focus on the practical and popular clustering problems, k-center and k-median. We develop fast clustering algorithms with constant factor approximation guarantees. From a theoretical perspective, we give the first analysis that shows several clustering algorithms are in MRC0, a theoretical MapReduce class introduced by Karloff et al. [26]. Our algorithms use sampling to decrease the data size and they run a time consuming clustering algorithm such as local search or Lloyd's algorithm on the resulting data set. Our algorithms have sufficient flexibility to be used in practice since they run in a constant number of MapReduce rounds. We complement these results by performing experiments using our algorithms. We compare the empirical performance of our algorithms to several sequential and parallel algorithms for the k-median problem. The experiments show that our algorithms' solutions are similar to or better than the other algorithms' solutions. Furthermore, on data sets that are sufficiently large, our algorithms are faster than the other parallel algorithms that we tested.

#index 1605990
#* Clustering very large multi-dimensional datasets with MapReduce
#@ Robson Leonardo Ferreira Cordeiro;Caetano Traina, Junior;Agma Juci Machado Traina;Julio López;U. Kang;Christos Faloutsos
#t 2011
#c 0
#% 248792
#% 273891
#% 443480
#% 765439
#% 789010
#% 801683
#% 810047
#% 844313
#% 963669
#% 1021533
#% 1047783
#% 1083683
#% 1127359
#% 1165480
#% 1318636
#! Given a very large moderate-to-high dimensionality dataset, how could one cluster its points? For datasets that don't fit even on a single disk, parallelism is a first class option. In this paper we explore MapReduce for clustering this kind of data. The main questions are (a) how to minimize the I/O cost, taking into account the already existing data partition (e.g., on disks), and (b) how to minimize the network cost among processing nodes. Either of them may be a bottleneck. Thus, we propose the Best of both Worlds -- BoW method, that automatically spots the bottleneck and chooses a good strategy. Our main contributions are: (1) We propose BoW and carefully derive its cost functions, which dynamically choose the best strategy; (2) We show that BoW has numerous desirable features: it can work with most serial clustering methods as a plugged-in clustering subroutine, it balances the cost for disk accesses and network accesses, achieving a very good tradeoff between the two, it uses no user-defined parameters (thanks to our reasonable defaults), it matches the clustering quality of the serial algorithm, and it has near-linear scale-up; and finally, (3) We report experiments on real and synthetic data with billions of points, using up to 1,024 cores in parallel. To the best of our knowledge, our Yahoo! web is the largest real dataset ever reported in the database subspace clustering literature. Spanning 0.2 TB of multi-dimensional data, it took only 8 minutes to be clustered, using 128 cores.

#index 1605991
#* Selective block minimization for faster convergence of limited memory large-scale linear models
#@ Kai-Wei Chang;Dan Roth
#t 2011
#c 0
#% 209021
#% 232728
#% 269217
#% 543892
#% 562950
#% 722797
#% 729940
#% 881477
#% 916781
#% 938667
#% 961152
#% 961213
#% 983905
#% 1042610
#% 1073905
#% 1073923
#% 1073988
#% 1083669
#% 1117691
#% 1232034
#% 1305483
#% 1338580
#% 1385974
#% 1451223
#% 1481629
#% 1621276
#! As the size of data sets used to build classifiers steadily increases, training a linear model efficiently with limited memory becomes essential. Several techniques deal with this problem by loading blocks of data from disk one at a time, but usually take a considerable number of iterations to converge to a reasonable model. Even the best block minimization techniques [1] require many block loads since they treat all training examples uniformly. As disk I/O is expensive, reducing the amount of disk access can dramatically decrease the training time. This paper introduces a selective block minimization (SBM) algorithm, a block minimization method that makes use of selective sampling. At each step, SBM updates the model using data consisting of two parts: (1) new data loaded from disk and (2) a set of informative samples already in memory from previous steps. We prove that, by updating the linear model in the dual form, the proposed method fully utilizes the data in memory and converges to a globally optimal solution on the entire data. Experiments show that the SBM algorithm dramatically reduces the number of blocks loaded from disk and consequently obtains an accurate and stable model quickly on both binary and multi-class classification.

#index 1605992
#* Bounded coordinate-descent for biological sequence classification in high dimensional predictor space
#@ Georgiana Ifrim;Carsten Wiuf
#t 2011
#c 0
#% 397654
#% 469390
#% 722803
#% 771845
#% 773682
#% 793247
#% 830744
#% 833913
#% 881477
#% 906397
#% 1073923
#% 1074360
#% 1077165
#% 1083663
#% 1117675
#% 1214676
#% 1385997
#% 1472290
#! We present a framework for discriminative sequence classification where linear classifiers work directly in the explicit high-dimensional predictor space of all subsequences in the training set (as opposed to kernel-induced spaces). This is made feasible by employing a gradient-bounded coordinate-descent algorithm for efficiently selecting discriminative subsequences without having to expand the whole space. Our framework can be applied to a wide range of loss functions, including binomial log-likelihood loss of logistic regression and squared hinge loss of support vector machines. When applied to protein remote homology detection and remote fold recognition, our framework achieves comparable performance to the state-of-the-art (e.g., kernel support vector machines). In contrast to state-of-the-art sequence classifiers, our models are simply lists of weighted discriminative subsequences and can thus be interpreted and related to the biological problem -- a crucial requirement for the bioinformatics and medical communities.

#index 1605993
#* Multi-source domain adaptation and its application to early detection of fatigue
#@ Rita Chattopadhyay;Jieping Ye;Sethuraman Panchanathan;Wei Fan;Ian Davidson
#t 2011
#c 0
#% 906248
#% 947546
#% 961218
#% 1083655
#% 1130817
#% 1147703
#% 1211726
#% 1214724
#% 1267778
#% 1270196
#% 1305479
#% 1385982
#% 1464068
#! We consider the characterization of muscle fatigue through noninvasive sensing mechanism such as surface electromyography (SEMG). While changes in the properties of SEMG signals with respect to muscle fatigue have been reported in the literature, the large variation in these signals across different individuals makes the task of modeling and classification of SEMG signals challenging. Indeed, the variation in SEMG parameters from subject to subject creates differences in the data distribution. In this paper, we propose a transfer learning framework based on the multi-source domain adaptation methodology for detecting different stages of fatigue using SEMG signals, that addresses the distribution differences. In the proposed framework, the SEMG data of a subject represent a domain; data from multiple subjects in the training set form the multiple source domains and the test subject data form the target domain. SEMG signals are predominantly different in conditional probability distribution across subjects. The key feature of the proposed framework is a novel weighting scheme that addresses the conditional probability distribution differences across multiple domains (subjects). We have validated the proposed framework on Surface Electromyogram signals collected from 8 people during a fatigue-causing repetitive gripping activity. Comprehensive experiments on the SEMG data set demonstrate that the proposed method improves the classification accuracy by 20% to 30% over the cases without any domain adaptation method and by 13% to 30% over the existing state-of-the-art domain adaptation methods.

#index 1605994
#* Two-locus association mapping in subquadratic time
#@ Panagiotis Achlioptas;Bernhard Schölkopf;Karsten Borgwardt
#t 2011
#c 0
#% 82154
#% 347225
#% 1083715
#% 1447022
#! Genome-wide association studies (GWAS) have not been able to discover strong associations between many complex human diseases and single genetic loci. Mapping these phenotypes to pairs of genetic loci is hindered by the huge number of candidates leading to enormous computational and statistical problems. In GWAS on single nucleotide polymorphisms (SNPs), one has to consider in the order of 1010 to 1014 pairs, which is infeasible in practice. In this article, we give the first algorithm for 2-locus genome-wide association studies that is subquadratic in the number, n, of SNPs. The running time of our algorithm is data-dependent, but large experiments over real genomic data suggest that it scales empirically as n3/2. As a result, our algorithm can easily cope with n ~ 107, i.e., it can efficiently search all pairs of SNPs in the human genome.

#index 1605995
#* A taxi business intelligence system
#@ Yong Ge;Chuanren Liu;Hui Xiong;Jian Chen
#t 2011
#c 0
#% 1083673
#% 1451230
#% 1482422
#% 1781298
#! The increasing availability of large-scale location traces creates unprecedent opportunities to change the paradigm for knowledge discovery in transportation systems. A particularly promising area is to extract useful business intelligence, which can be used as guidance for reducing inefficiencies in energy consumption of transportation sectors, improving customer experiences, and increasing business performances. However, extracting business intelligence from location traces is not a trivial task. Conventional data analytic tools are usually not customized for handling large, complex, dynamic, and distributed nature of location traces. To that end, we develop a taxi business intelligence system to explore the massive taxi location traces from different business perspectives with various data mining functions. Since we implement the system using the real-world taxi GPS data, this demonstration will help taxi companies to improve their business performances by understanding the behaviors of both drivers and customers. In addition, several identified technical challenges also motivate data mining people to develop more sophisticate techniques in the future.

#index 1605996
#* Apolo: interactive large graph sensemaking by combining machine learning and visualization
#@ Duen Horng Chau;Aniket Kittur;Jason I. Hong;Christos Faloutsos
#t 2011
#c 0
#% 152097
#% 268079
#% 290830
#% 580307
#% 662790
#% 1047327
#% 1183138
#% 1214748
#% 1286743
#% 1573362
#! We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO's usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.

#index 1605997
#* Article clipper: a system for web article extraction
#@ Jian Fan;Ping Luo;Suk Hwan Lim;Sam Liu;Parag Joshi;Jerry Liu
#t 2011
#c 0
#% 259465
#% 818233
#% 1190152
#% 1214756
#% 1252649
#! Many people use the Web as the main source of information in their daily lives. However, most web pages contain non-informative components such as side bars, footers, headers, and advertisements, which are undesirable for certain applications like printing. We demonstrate a system that automatically extracts the informative contents from news- and blog-like web pages. In contrast to many existing methods that are limited to identifying only the text or the bounding rectangular region, our system not only identifies the content but also the structural roles of various content components such as title, paragraphs, images and captions. The structural information enables re-layout of the content in a pleasing way. Besides the article text extraction, our system includes the following components: 1) print-link detection to identify the URL link for printing, and to use it for more reliable analysis and recognition; 2) title detection incorporating both visual cues and HTML tags; 3) image and caption detection utilizing extensive visual cues; 4) multiple-page and next page URL detection. The performance of our system has been thoroughly evaluated using a human labeled ground truth dataset consisting of 2000 web pages from 100 major web sites. We show accurate results using such a dataset.

#index 1605998
#* Data intensive analysis on the gordon high performance data and compute system
#@ Robert S. Sinkovits;Pietro Cicotti;Shawn Strande;Mahidhar Tatineni;Paul Rodriguez;Nicole Wolter;Natasha Balac
#t 2011
#c 0
#% 881575
#% 928968
#% 1017256
#% 1453394
#% 1453398
#% 1602771
#! The Gordon data intensive computing system was designed to handle problems with large memory requirements that cannot easily be solved using standard workstations or distributed memory supercomputers. We describe the unique features of Gordon that make it ideally suited for data mining and knowledge discovery applications: memory aggregation using the vSMP software solution from ScaleMP, I/O nodes containing 4 TB of low-latency flash memory, and a high performance parallel file system with 4 PB capacity. We also demonstrate how a number of standard data mining tools (e.g. Matlab, WEKA, R) can be used effectively on Dash, an early prototype of the full Gordon system.

#index 1605999
#* Frontex real-time news event extraction framework
#@ Jakub Piskorski;Martin Atkinson
#t 2011
#c 0
#% 936822
#% 943805
#% 995485
#% 1102264
#% 1190211
#% 1220923
#% 1624746
#! An ever-growing amount of information relevant for early detection of certain threats can be extracted from on-line news. This led to an emergence of news mining tools to help analysts to digest the overflow of information and to extract valuable knowledge from on line news sources. This paper gives an overview of the fully operational Real-time News Event Extraction Framework developed for Frontex, the EU Border Agency, to facilitate the process of extracting structured information on border security-related events from on-line news. In particular, a hybrid event extraction system has been constructed, which is applied to the stream of news articles continuously gathered and pre-processed by the Europe Media Monitor - a large-scale multilingual news aggregation engine. The framework consists also of an earth browser, in which events are visualized and an event moderation tool, which allows to access the database of automatically extracted event descriptions and to clean, validate, group, enhance and export them into other knowledge repositories.

#index 1606000
#* LikeMiner: a system for mining the power of 'like' in social media networks
#@ Xin Jin;Chi Wang;Jiebo Luo;Xiao Yu;Jiawei Han
#t 2011
#c 0
#% 328355
#% 589927
#% 592155
#% 635689
#% 664855
#% 722904
#% 726267
#% 729923
#% 1040539
#% 1041734
#% 1214702
#% 1400136
#% 1451243
#% 1550601
#% 1560408
#! Social media is becoming increasingly ubiquitous and popular on the Internet. Due to the huge popularity of social media websites, such as Facebook, Twitter, YouTube and Flickr, many companies or public figures are now active in maintaining pages on those websites to interact with online users, attracting a large number of fans/followers by posting interesting objects, e.g., (product) photos/videos and text messages. 'Like' has now become a very popular social function by allowing users to express their like of certain objects. It provides an accurate way of estimating user interests and an effective way of sharing/promoting information in social media. In this demo, we propose a system called LikeMiner to mine the power of 'like' in social media networks. We introduce a heterogeneous network model for social media with 'likes', and propose 'like' mining algorithms to estimate representativeness and influence of objects. The implemented prototype system demonstrates the effectiveness of the proposed approach using the large scale Facebook data.

#index 1606001
#* MIME: a framework for interactive visual pattern mining
#@ Bart Goethals;Sandy Moens;Jilles Vreeken
#t 2011
#c 0
#% 310517
#% 434613
#% 577214
#% 867057
#% 1083743
#% 1428404
#% 1428407
#% 1541786
#% 1618915
#% 1705413
#! We present a framework for interactive visual pattern mining. Our system enables the user to browse through the data and patterns easily and intuitively, using a toolbox consisting of interestingness measures, mining algorithms and post-processing algorithms to assist in identifying interesting patterns. By mining interactively, we enable the user to combine their subjective interestingness measure and background knowledge with a wide variety of objective measures to easily and quickly mine the most important and interesting patterns. Basically, we enable the user to become an essential part of the mining algorithm. Our demo currently applies to mining interesting itemsets and association rules, and its extension to episodes and decision trees is ongoing.

#index 1606002
#* SIGKDD demo: sensors and software to allow computational entomology, an emerging application of data mining
#@ Gustavo E. Batista;Eamonn J. Keogh;Agenor Mafra-Neto;Edgar Rowton
#t 2011
#c 0
#% 345829
#% 1481526
#! The history of humankind is intimately connected to insects. Insect borne diseases kill a million people and destroy tens of billions of dollars worth of crops annually. However, at the same time, beneficial insects pollinate the majority of crop species, and it has been estimated that approximately one third of all food consumed by humans is directly pollinated by bees alone. Given the importance of insects in human affairs, it is somewhat surprising that computer science has not had a larger impact in entomology. We believe that recent advances in sensor technology are beginning change this, and a new field of Computational Entomology will emerge. We will demonstrate an inexpensive sensor that allows us to capture data from flying insects, and the software that allows us to analyze the data. Moreover, we will distribute both the sensors and software for free, to parties willing to take part in a crowdsourcing project on insect classification.

#index 1606003
#* Social flocks: a crowd simulation framework for social network generation, community detection, and collective behavior modeling
#@ Cheng-Te Li;Shou-De Lin
#t 2011
#c 0
#% 31686
#% 729923
#% 781507
#% 823342
#% 867050
#% 983230
#% 1268028
#% 1399996
#! This work combines the central ideas from two different areas, crowd simulation and social network analysis, to tackle some existing problems in both areas from a new angle. We present a novel spatio-temporal social crowd simulation framework, Social Flocks, to revisit three essential research problems, (a) generation of social networks, (b) community detection in social networks, (c) modeling collective social behaviors in crowd simulation. Our framework produces social networks that satisfy the properties of high clustering coefficient, low average path length, and power-law degree distribution. It can also be exploited as a novel dynamic model for community detection. Finally our framework can be used to produce real-life collective social behaviors over crowds, including community-guided flocking, leader following, and spatio-social information propagation. Social Flocks can serve as visualization of simulated crowds for domain experts to explore the dynamic effects of the spatial, temporal, and social factors on social networks. In addition, it provides an experimental platform of collective social behaviors for social gaming and movie animations. Social Flocks demo is at http://mslab.csie.ntu.edu.tw/socialflocks/ .

#index 1606004
#* Topic-level social network search
#@ Jie Tang;Sen Wu;Bo Gao;Yang Wan
#t 2011
#c 0
#% 729923
#% 769887
#% 1083734
#% 1214641
#% 1214702
#% 1481049
#% 1538534
#% 1696329
#% 1755323
#% 1810385
#! We study the problem of topic-level social network search, which aims to find who are the most influential users in a network on a specific topic and how the influential users connect with each other. We employ a topic model to find topical aspects of each user and a retrieval method to identify influential users by combining the language model and the topic model. An influence maximization algorithm is then presented to find the sub network that closely connects the influential users. Two demonstration systems have been developed and are online available. Empirical analysis based on the user's viewing time and the number of clicks validates the proposed methodologies.

#index 1606005
#* Video analytics solution for tracking customer locations in retail shopping malls
#@ Harikrishna G.N. Rai;Kishore Jonna;P. Radha Krishna
#t 2011
#c 0
#% 908795
#% 945190
#% 1038792
#% 1072257
#! Due to increased adoption of digital inclusion in various businesses, location based services are gaining importance to provide value-added services for their customers. In this work, we present a computer vision based system for tracking customer locations by recognizing individual shopping carts inside shopping malls in order to facilitate location based services. We provide an efficient approach for cart recognition that consists of two stages: cart detection and then cart recognition. A binary pattern is placed between two pre-defined color markers and attached to each cart for recognition. The system takes live video feed as input from the cameras mounted on the aisles of the shopping mall and processes frames in real-time. In the cart detection stage, color segmentation, feature extraction and classification are used for detection of binary pattern along with color markers. In recognition stage, segmented binary strip is processed using spatial image processing techniques to decode the cart identification number.

#index 1606006
#* "Which half Is wasted?": controlled experiments to measure online-advertising effectiveness
#@ David Reiley
#t 2011
#c 0
#! The department-store retailer John Wanamaker famously stated, "Half the money I spend on advertising is wasted--I just don't know which half." Compared with the measurement of advertising effectiveness in traditional media, online advertisers and publishers have considerable data advantages, including individual-level data on advertising exposures, clicks, searches, and other online user behaviors. However, as I shall discuss in this talk, the science of advertising effectiveness requires more than just quantity of data - even more important is the quality of the data. In particular, in many cases, using various statistical techniques with observational data leads to incorrect measurements. To measure the true causal effects, we run controlled experiments that suppress advertising to a control group, much like the placebo in a drug trial. With experiments to determine the ground truth, we can show that in many circumstances, observational-data techniques rely on identifying assumptions that prove to be incorrect, and they produce estimates differing wildly from the truth. Despite increases in data availability, Wanamaker's complaint remains just as true for online advertising as it was for print advertising a century ago. In this talk, I will discuss recent advances in running randomized experiments online, measuring the impact of online display advertising on consumer behavior. Interesting results include the measurable effects of online advertising on offline transactions, the impact on viewers who do not click the ads, the surprisingly large effects of frequency of exposure, and the heterogeneity of advertising effectiveness across users in different demographic groups or geographic locations. I also show that sample sizes of a million or more customers may be necessary to get enough precision for statistical significance of economically important effects - so we have just reached the cusp of being able to measure effects precisely with present technology. (By comparison, previous controlled experiments using split-cable TV systems, with sample sizes in the mere thousands, have lacked statistical power to measure precise effects for a given campaign.) As I show with several examples that establish the ground truth using controlled experiments, the bias in observational studies can be extremely large, over-or-underestimating the true causal effects by an order of magnitude. I will discuss the (implicit or explicit) modeling assumptions made by researchers using observational data, and identify several reasons why these assumptions are violated in practice. I will also discuss future directions in using experiments to measure advertising effectiveness.

#index 1606007
#* Accelerating large-scale data mining using in-database analytics
#@ Mario E. Inchiosa
#t 2011
#c 0
#! In more and more industries, competitive advantage hinges on exploiting the largest quantity of data in the shortest possible time - and doing so cost-effectively. Data volumes are growing exponentially, while businesses are striving to deploy sophisticated and computationally intensive predictive analytics. Often, massive data is stored in a data warehouse running on dedicated parallel hardware, but advanced analytics is performed on a separate compute platform. Moving data from the data warehouse to the compute environment can constitute a significant bottleneck. Organizations resort to considering only a fraction of their data or refreshing their analyses infrequently. To address the data movement bottleneck and take full advantage of parallel data warehouse platforms, vendors are offering new in-database analytics capabilities. They are opening up their platforms, allowing users to run their own user-defined functions and statistical models as well as vendor- and partner-supplied advanced analytics on the database platform, close to the data, in parallel, without transporting the data through a host node or corporate network. In this talk, we will present the need for in-database analytics and discuss a number of the new solutions available, highlighting case studies where solution times have been reduced from hours to minutes or seconds.

#index 1606008
#* Applications of data mining and machine learning in online customer care
#@ Ravi Vijayaraghavan;P V Kannan
#t 2011
#c 0
#! With the coming of age of web as a mainstream customer service channel, B2C companies have invested substantial resources in enhancing their web presence. Today customers can interact with a company through channels such as phone, chat, email, social media or web self-service. With the availability of web logs, CRM data and text transcripts these online channels are rich with data and they track several aspects of customer behavior and intent. 24/7 Customer Innovation Labs has developed a series of data mining and statistics driven solutions to improve customer experience in each of these online channels. This talk will focus on solutions to enhance performance of web chat as a customer service channel. 2 stages of customer life-cycle will be considered -- new customer acquisition (or sales) and service of existing customers. In customer acquisition the key objective is to maximize "incremental" revenues via chat. While in customer service the objective is to drive up the quality of customer experience (measured by customer satisfaction surveys or mined customer sentiments) through chat. The solution based on machine learning methods involves: Real-time targeting of the right visitors to chat Predicting customer needs Routing customer to the right customer service agent Mining chat transcripts and Social Media Portals to identify key customer issues and customer sentiments Mining agents' responses for performance improvement Feeding back learning from 4 and 5 to 1 (better targeting) Real-life case studies will be presented to show how that this closed loop solution can quickly improve key metrics.

#index 1606009
#* Broad scale predictive modeling and marketing optimization in retail sales
#@ Dan Steinberg;Felipe Fernandez Martinez
#t 2011
#c 0
#! The challenge of predicting retail sales on a product-by-product basis throughout a network of retail stores has been researched intensively by applied econometricians and statisticians for decades. The principal tools of analysis have been linear regression with Bayesian inspired adjustments to stabilize demand curve estimates. The scale of such analytics can be challenging as retailers often work with more than 100,000 products (SKUs) and typically operate networks of hundreds of brick and mortar stores. Department and grocery stores are excellent examples but fast food restaurants also require such detailed predictive modeling systems. Depending on the objectives of the company, predictions may be required for blocks of time spanning a week or more, or, as in the case of fast food operators, predictions are required for each 15-minute time interval of the operating day. The authors have modernized industry standard approaches to such predictive modeling by leveraging advanced data mining techniques. These techniques are more adept in detecting nonlinear response and accommodating interactions and automatically sifting through hundreds if not thousands of potential factors influencing sales outcomes. Results show that conventional statistical models miss a substantial fraction of the explainable variance while the new methods dominate in terms of performance and speed of model development. Accurate prediction is required for reliable planning and logistics, and optimization. Optimization with respect to pricing, promotion and assortment can be asked for relative to a variety of objectives (e.g. revenue, profits) and short term and long-term optimization may result in different decisions being taken. A unique challenge for retailers is the large number of constraints to which complex retail organizations are subject. Contracts and special understandings with valued suppliers severely constrain a retailer's flexibility. For example, certain products may not be promotable (or discounted) in isolation, and others (say from competitors) may not be promoted jointly, and the costs of goods sold may well depend on the quantities contracted. We discuss how we have resolved such challenges via a cycle of prediction and simulation to develop a flexible high-speed system for handling arbitrary constraints, arbitrary objectives, and achieve new levels of predictive accuracy and reliability.

#index 1606010
#* Knowledge discovery and data mining in pharmaceutical cancer research
#@ Paul Rejto
#t 2011
#c 0
#! Biased and unbiased approaches to develop predictive biomarkers of response to drug treatment will be introduced and their utility demonstrated for cell cycle inhibitors. Opportunities to leverage the growing knowledge of tumors characterized by modern methods to measure DNA and RNA will be shown, including the use of appropriate preclinical models and selection of patients. Furthermore, techniques to identify mechanisms of resistance prior to clinical treatment will be discussed. Prospects for systematic data mining and current barriers to the application of precision medicine in cancer will be reviewed along with potential solutions.

#index 1606011
#* Operational security analytics: doing more with less
#@ Colleen McCue
#t 2011
#c 0
#! Why just count crime when you can anticipate, prevent and respond more effectively? Companies in the commercial sector have long understood the importance of being able to anticipate or predict future behavior and demand in order to respond efficiently and effectively. Embracing the promise of predictive analytics, the public safety community is moving from a focus on "what happened," to a system that enables the ability to anticipate future events and effectively deploy resources in front of crime; thereby, changing outcomes. While we have become familiar with the use of advanced analytics in support of fraud detection and prevention, techniques similar to those used to support customer loyalty programs and supply chain management have been used to prevent and solve violent crimes, enhance investigative pace and efficacy, support information-based risk and threat assessment, and deploy public safety resources more efficiently. As public safety agencies increasingly are asked to do more with less, the ability to anticipate crime represents a game changing paradigm shift; enabling information-based tactics, strategy and policy in support of prevention and response. Reporting, collecting and compiling data are necessary but not sufficient to increasing public safety. Ultimately, the ability to anticipate, prevent and respond more effectively will enable us to do more with less and change public safety outcomes.

#index 1606012
#* Real-time risk control system for CNP (card not present)
#@ Tai Hsu
#t 2011
#c 0
#! AliExpress is an online e-commerce platform for wholesale products. Credit card is one of its various payment methods. An online transaction using credit cards is called a "card not present" (CNP) transaction where the physical card has not been swiped into a reader. It's also the major type of credit card frauds causing a great overhead of the online operation, sellers, and buyers. To protect customers on our platform, we developed a real-time credit card fraud detection system, using the machine learning technologies which allows us to achieve a precision of 97%, at a recall of 80%. With the system, we can provide the best online shopping experience for our customers, without the high risk of online transactions which always result a high operational cost. We will briefly share our experience and practice in the expo.

#index 1606013
#* The power of analysis and data
#@ David Norton
#t 2011
#c 0
#! Caesars Entertainment, the largest provider of branded casino entertainment, captures a wealth of data for 40 million+ customers through its Total Rewards program. In-depth data analysis has helped Caesars weather the economic downturn by prioritizing marketing spend, expense savings targets and identifying new revenue opportunities. This talk will describe how closed-loop marketing, state-of-the-art user segmentation, and ongoing experimentation via test and control groups have enabled Caesars Entertainment to achieve all-time high customer satisfaction scores and outperform the competition in a challenging economic climate. The lessons learned are generic and apply across multiple industries. Insights will also be provided on the next wave of challenges to be answered analytically.

#index 1606014
#* The practitioner's viewpoint to data mining: key lessons learned in the trenches and case studies
#@ Richard Boire
#t 2011
#c 0
#! In many data mining exercises, we see information that appears on the surface to demonstrate a particular conclusion. But closer examination of the data reveals that these results are indeed misleading. In this session, we will examine this notion of misleading results in three areas: Statistical Issues Statistical issues such as multicollinearity and outliers can impact results dramatically. We will first outline how these statistical issues can provide misleading results. At the same time, we will demonstrate how the data mining practitioner overcomes these issues through data analysis approaches that provide both more meaningful and non-misleading results to the business community. Overstating of Results From a business standpoint, we will also look at results that appear to be too good to be true. In other words, there appears to be some overstating of results within a given data mining solution. Initially, we will discuss how to identify these situations. Secondly, we will outline what causes this overstatement of results and detail our approach on how we would overcome this predicament. Overfitting Another topic for discussion is overfitting of results. This is particularly the case when building predictive models. In this section of the seminar, we will define what overfitting is and why it is becoming more relevant for understanding by the business community. Once again, analytical approaches will be discussed in terms of how to best handle this issue. We present two case studies that demonstrate how our principled 4-step approach can be used to solve challenging data mining problems. These 4 steps are as follows: How to identify the problem How we construct the right data environment to conduct our analytics What kind of analytics are employed which include techniques such as correlation analysis, EDA reports, logistic regression, and gains charts. More importantly, we discuss how to interpret the output in terms of the actual impact to the business (i.e. increased response rate and ultimately increased ROI.) How do we apply the learning to a future initiative and what were the actual results

#index 1606015
#* Thriving as a data miner in the real world
#@ John F. Elder, IV
#t 2011
#c 0
#! Meaningful work is a deep human need. We all yearn to contribute to something greater than ourselves, be listened to, and work alongside friendly peers. Data mining consulting is a powerful way to use technical skills and gain these great side benefits. The power of analytics and its high return on investment makes one's expertise welcome virtually everywhere. And the variety of projects and domains encountered leads to continual learning as new problems are met and solved. Teaching and writing are possible, and there is great satisfaction in seeing one's work actually implemented and used, potentially touching millions. Still, in industry, one has the joy and hazards of working closely with other humans, where final success can depend as much on others as oneself, and on social as well as technical issues. In my experience, business risk strongly outweighs technical risk in whether a solution is used. I will share some hard-won lessons learned on how to best succeed, both technically and socially, in the results-oriented world of industry.

#index 1606016
#* 2D-interval predictions for time series
#@ Luis Torgo;Orlando Ohashi
#t 2011
#c 0
#% 309208
#% 400847
#% 961168
#% 989670
#% 1289281
#% 1550123
#! Research on time series forecasting is mostly focused on point predictions - models are obtained to estimate the expected value of the target variable for a certain point in future. However, for several relevant applications this type of forecasts has limited utility (e.g. costumer wallet value estimation, wind and electricity power production, control of water quality, etc.). For these domains it is frequently more important to be able to forecast a range of plausible future values of the target variable. A typical example is wind power production, where it is of high relevance to predict the future wind variability in order to ensure that supply and demand are balanced. This type of predictions will allow timely actions to be taken in order to cope with the expected values of the target variable on a certain future time horizon. In this paper we study this type of predictions - the prediction of a range of expected values for a future time interval. We describe some possible approaches to this task and propose an alternative procedure that our extensive experiments on both artificial and real world domains show to have clear advantages.

#index 1606017
#* A game theoretic framework for heterogenous information network clustering
#@ Faris Alqadah;Raj Bhatnagar
#t 2011
#c 0
#% 342621
#% 384416
#% 397384
#% 469422
#% 729918
#% 769967
#% 778215
#% 823359
#% 830275
#% 840840
#% 857161
#% 876018
#% 881487
#% 893124
#% 915288
#% 938978
#% 1013604
#% 1036692
#% 1130919
#% 1181261
#% 1213625
#% 1214701
#% 1250223
#% 1656532
#! Heterogeneous information networks are pervasive in applications ranging from bioinformatics to e-commerce. As a result, unsupervised learning and clustering methods pertaining to such networks have gained significant attention recently. Nodes in a heterogeneous information network are regarded as objects derived from distinct domains such as 'authors' and 'papers'. In many cases, feature sets characterizing the objects are not available, hence, clustering of the objects depends solely on the links and relationships amongst objects. Although several previous studies have addressed information network clustering, shortcomings remain. First, the definition of what constitutes an information network cluster varies drastically from study to study. Second, previous algorithms have generally focused on non-overlapping clusters, while many algorithms are also limited to specific network topologies. In this paper we introduce a game theoretic framework (GHIN) for defining and mining clusters in heterogeneous information networks. The clustering problem is modeled as a game wherein each domain represents a player and clusters are defined as the Nash equilibrium points of the game. Adopting the abstraction of Nash equilibrium points as clusters allows for flexible definition of reward functions that characterize clusters without any modification to the underlying algorithm. We prove that well-established definitions of clusters in 2-domain information networks such as formal concepts, maximal bi-cliques, and noisy binary tiles can always be represented as Nash equilibrium points. Moreover, experimental results employing a variety of reward functions and several real world information networks illustrate that the GHIN framework produces more accurate and informative clusters than the recently proposed NetClus and state of the art MDC algorithms.

#index 1606018
#* A GPU-tailored approach for training kernelized SVMs
#@ Andrew Cotter;Nathan Srebro;Joseph Keshet
#t 2011
#c 0
#% 269217
#% 269218
#% 393059
#% 592108
#% 722816
#% 916781
#% 916790
#% 983905
#% 1073885
#% 1073923
#% 1105774
#% 1558464
#% 1563305
#! We present a method for efficiently training binary and multiclass kernelized SVMs on a Graphics Processing Unit (GPU). Our methods apply to a broad range of kernels, including the popular Gaus- sian kernel, on datasets as large as the amount of available memory on the graphics card. Our approach is distinguished from earlier work in that it cleanly and efficiently handles sparse datasets through the use of a novel clustering technique. Our optimization algorithm is also specifically designed to take advantage of the graphics hardware. This leads to different algorithmic choices then those preferred in serial implementations. Our easy-to-use library is orders of magnitude faster then existing CPU libraries, and several times faster than prior GPU approaches.

#index 1606019
#* A multi-task learning formulation for predicting disease progression
#@ Jiayu Zhou;Lei Yuan;Jun Liu;Jieping Ye
#t 2011
#c 0
#% 317525
#! Alzheimer's Disease (AD), the most common type of dementia, is a severe neurodegenerative disorder. Identifying markers that can track the progress of the disease has recently received increasing attentions in AD research. A definitive diagnosis of AD requires autopsy confirmation, thus many clinical/cognitive measures including Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-Cog) have been designed to evaluate the cognitive status of the patients and used as important criteria for clinical diagnosis of probable AD. In this paper, we propose a multi-task learning formulation for predicting the disease progression measured by the cognitive scores and selecting markers predictive of the progression. Specifically, we formulate the prediction problem as a multi-task regression problem by considering the prediction at each time point as a task. We capture the intrinsic relatedness among different tasks by a temporal group Lasso regularizer. The regularizer consists of two components including an L2,1-norm penalty on the regression weight vectors, which ensures that a small subset of features will be selected for the regression models at all time points, and a temporal smoothness term which ensures a small deviation between two regression models at successive time points. We have performed extensive evaluations using various types of data at the baseline from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database for predicting the future MMSE and ADAS-Cog scores. Our experimental studies demonstrate the effectiveness of the proposed algorithm for capturing the progression trend and the cross-sectional group differences of AD severity. Results also show that most markers selected by the proposed algorithm are consistent with findings from existing cross-sectional studies.

#index 1606020
#* A simple statistical model and association rule filtering for classification
#@ Gyorgy J. Simon;Vipin Kumar;Peter W. Li
#t 2011
#c 0
#% 227919
#% 283138
#% 310539
#% 400847
#% 466483
#% 478133
#% 481290
#% 546047
#% 796210
#% 835018
#% 1120982
#% 1497336
#! Associative classification is a predictive modeling technique that constructs a classifier based on class association rules (also known as predictive association rules; PARs). PARs are association rules where the consequence of the rule is a class label. Associative classification has gained substantial research attention because it successfully joins the benefits of association rule mining with classification. These benefits include the inherent ability of association rule mining to extract high-order interactions among the predictors--an ability that many modern classifiers lack--and also the natural interpretability of the individual PARs. Associative classification is not without its caveats. Association rule mining often discovers a combinatorially large number of association rules, eroding the interpretability of the rule set. Extensive effort has been directed towards developing interestingness measures, which filter (predictive) association rules after they have been generated. These interestingness measures, albeit very successful at selecting interesting rules, lack two features that are highly valuable in the context of classification. First, only few of the interestingness measures are rooted in a statistical model. Given the distinction between a training and a test data set in the classification setting, the ability to make statistical inferences about the performance of the predictive classification rules on the test set is highly desirable. Second, the unfiltered set of predictive assocation rules (PARs) are often redundant, we can prove that certain PARs will not be used to construct a classification model given the presence of other PARs. In this paper, we propose a simple statistical model towards making inferences on the test set about the various performance metrics of predictive association rules. We also derive three filtering criteria based on hypothesis testing, which are very selective (reduce the number of PARs to be considered by the classifier by several orders of magnitude), yet do not effect the performance of the classification adversely. In the case, where the classification model is constructed as a logistic model on top of the PARs, we can mathematically prove, that the filtering criteria do not significantly effect the classifier's performance. We also demonstrate empirically on three publicly available data sets that the vast reduction in the number of PARs indeed did not come at the cost of reducing the predictive performance.

#index 1606021
#* A time-dependent topic model for multiple text streams
#@ Liangjie Hong;Byron Dom;Siva Gurumurthy;Kostas Tsioutsiouliklis
#t 2011
#c 0
#% 73441
#% 329569
#% 643520
#% 722904
#% 769967
#% 875959
#% 879587
#% 881498
#% 989623
#% 989650
#% 1040837
#% 1166524
#% 1200156
#% 1211725
#% 1214671
#% 1268037
#% 1292714
#% 1338675
#% 1385969
#% 1424114
#% 1451206
#% 1451248
#% 1536522
#% 1587367
#% 1736245
#! In recent years social media have become indispensable tools for information dissemination, operating in tandem with traditional media outlets such as newspapers, and it has become critical to understand the interaction between the new and old sources of news. Although social media as well as traditional media have attracted attention from several research communities, most of the prior work has been limited to a single medium. In addition temporal analysis of these sources can provide an understanding of how information spreads and evolves. Modeling temporal dynamics while considering multiple sources is a challenging research problem. In this paper we address the problem of modeling text streams from two news sources - Twitter and Yahoo! News. Our analysis addresses both their individual properties (including temporal dynamics) and their inter-relationships. This work extends standard topic models by allowing each text stream to have both local topics and shared topics. For temporal modeling we associate each topic with a time-dependent function that characterizes its popularity over time. By integrating the two models, we effectively model the temporal dynamics of multiple correlated text streams in a unified framework. We evaluate our model on a large-scale dataset, consisting of text streams from both Twitter and news feeds from Yahoo! News. Besides overcoming the limitations of existing models, we show that our work achieves better perplexity on unseen data and identifies more coherent topics. We also provide analysis of finding real-world events from the topics obtained by our model.

#index 1606022
#* Active learning for node classification in assortative and disassortative networks
#@ Cristopher Moore;Xiaoran Yan;Yaojia Zhu;Jean-Baptiste Rouquier;Terran Lane
#t 2011
#c 0
#% 132697
#% 344033
#% 464268
#% 851888
#% 868089
#% 1117695
#% 1269773
#% 1274885
#! In many real-world networks, nodes have class labels or variables that affect the network's topology. If the topology of the network is known but the labels of the nodes are hidden, we would like to select a small subset of nodes such that, if we knew their labels, we could accurately predict the labels of all the other nodes. We develop an active learning algorithm for this problem which uses information-theoretic techniques to choose which nodes to explore. We test our algorithm on networks from three different domains: a social network, a network of English words that appear adjacently in a novel, and a marine food web. Our algorithm makes no initial assumptions about how the groups connect, and performs well even when faced with quite general types of network structure. In particular, we do not assume that nodes of the same class are more likely to be connected to each other - only that they connect to the rest of the network in similar ways.

#index 1606023
#* Active learning using on-line algorithms
#@ Chris Mesterharm;Michael J. Pazzani
#t 2011
#c 0
#% 67056
#% 82156
#% 169717
#% 197394
#% 236729
#% 252034
#% 257877
#% 269217
#% 451055
#% 722814
#% 749132
#% 875953
#% 961177
#% 1138681
#% 1211696
#% 1478821
#% 1815223
#! This paper describes a new technique and analysis for using on-line learning algorithms to solve active learning problems. Our algorithm is called Active Vote, and it works by actively selecting instances that force several perturbed copies of an on-line algorithm to make mistakes. The main intuition for our result is based on the fact that the number of mistakes made by the optimal on-line algorithm is a lower bound on the number of labels needed for active learning. We provide performance bounds for Active Vote in both a batch and on-line model of active learning. These performance bounds depend on the algorithm having a set of unlabeled instances in which the various perturbed on-line algorithms disagree. The motivating application for Active Vote is an Internet advertisement rating program. We conduct experiments using data collected for this advertisement problem along with experiments using standard datasets. We show Active Vote can achieve an order of magnitude decrease in the number of labeled instances over various passive learning algorithms such as Support Vector Machines.

#index 1606024
#* Algorithms for speeding up distance-based outlier detection
#@ Kanishka Bhaduri;Bryan L. Matthews;Chris R. Giannella
#t 2011
#c 0
#% 300136
#% 300183
#% 321455
#% 427199
#% 430430
#% 478624
#% 479791
#% 481956
#% 729912
#% 844395
#% 866326
#% 1051998
#% 1165483
#% 1202160
#% 1495062
#! The problem of distance-based outlier detection is difficult to solve efficiently in very large datasets because of potential quadratic time complexity. We address this problem and develop sequential and distributed algorithms that are significantly more efficient than state-of-the-art methods while still guaranteeing the same outliers. By combining simple but effective indexing and disk block accessing techniques, we have developed a sequential algorithm iOrca that is up to an order-of-magnitude faster than the state-of-the-art. The indexing scheme is based on sorting the data points in order of increasing distance from a fixed reference point and then accessing those points based on this sorted order. To speed up the basic outlier detection technique, we develop two distributed algorithms (DOoR and iDOoR) for modern distributed multi-core clusters of machines, connected on a ring topology. The first algorithm passes data blocks from each machine around the ring, incrementally updating the nearest neighbors of the points passed. By maintaining a cutoff threshold, it is able to prune a large number of points in a distributed fashion. The second distributed algorithm extends this basic idea with the indexing scheme discussed earlier. In our experiments, both distributed algorithms exhibit significant improvements compared to the state-of-the-art distributed method [13].

#index 1606025
#* An effective evaluation measure for clustering on evolving data streams
#@ Hardy Kremer;Philipp Kranen;Timm Jansen;Thomas Seidl;Albert Bifet;Geoff Holmes;Bernhard Pfahringer
#t 2011
#c 0
#% 36672
#% 296738
#% 310537
#% 316709
#% 375017
#% 375388
#% 492952
#% 548116
#% 659972
#% 755463
#% 840907
#% 878207
#% 891162
#% 907498
#% 939129
#% 989584
#% 1015261
#% 1042225
#% 1165130
#% 1176965
#% 1214635
#% 1214709
#% 1318644
#% 1472282
#% 1489123
#% 1535182
#% 1535416
#% 1737760
#! Due to the ever growing presence of data streams, there has been a considerable amount of research on stream mining algorithms. While many algorithms have been introduced that tackle the problem of clustering on evolving data streams, hardly any attention has been paid to appropriate evaluation measures. Measures developed for static scenarios, namely structural measures and ground-truth-based measures, cannot correctly reflect errors attributable to emerging, splitting, or moving clusters. These situations are inherent to the streaming context due to the dynamic changes in the data distribution. In this paper we develop a novel evaluation measure for stream clustering called Cluster Mapping Measure (CMM). CMM effectively indicates different types of errors by taking the important properties of evolving data streams into account. We show in extensive experiments on real and synthetic data that CMM is a robust measure for stream clustering evaluation.

#index 1606026
#* An iterated graph laplacian approach for ranking on manifolds
#@ Xueyuan Zhou;Mikhail Belkin;Nathan Srebro
#t 2011
#c 0
#% 316228
#% 424085
#% 593047
#% 995140
#% 1014651
#% 1133457
#% 1183871
#! Ranking is one of the key problems in information retrieval. Recently, there has been significant interest in a class of ranking algorithms based on the assumption that data is sampled from a low dimensional manifold embedded in a higher dimensional Euclidean space. In this paper, we study a popular graph Laplacian based ranking algorithm [23] using an analytical method, which provides theoretical insights into the ranking algorithm going beyond the intuitive idea of "diffusion." Our analysis shows that the algorithm is sensitive to a commonly used parameter due to the use of symmetric normalized graph Laplacian. We also show that the ranking function may diverge to infinity at the query point in the limit of infinite samples. To address these issues, we propose an improved ranking algorithm on manifolds using Green's function of an iterated unnormalized graph Laplacian, which is more robust and density adaptive, as well as pointwise continuous in the limit of infinite samples. We also for the first time in the ranking literature empirically explore two variants from a family of twice normalized graph Laplacians. Experimental results on text and image data support our analysis, which also suggest the potential value of twice normalized graph Laplacians in practice.

#index 1606027
#* Anomaly localization for network data streams with graph joint sparse PCA
#@ Ruoyi Jiang;Hongliang Fei;Jun Huan
#t 2011
#c 0
#% 757953
#% 770890
#% 783722
#% 821933
#% 844310
#% 967005
#% 1117054
#% 1202160
#% 1206639
#% 1206850
#% 1211747
#% 1214740
#% 1217252
#% 1291615
#% 1318585
#% 1318717
#% 1401372
#% 1417091
#% 1451204
#% 1661361
#! Determining anomalies in data streams that are collected and transformed from various types of networks has recently attracted significant research interest. Principal Component Analysis (PCA) has been extensively applied to detecting anomalies in network data streams. However, none of existing PCA based approaches addresses the problem of identifying the sources that contribute most to the observed anomaly, or anomaly localization. In this paper, we propose novel sparse PCA methods to perform anomaly detection and localization for network data streams. Our key observation is that we can localize anomalies by identifying a sparse low dimensional space that captures the abnormal events in data streams. To better capture the sources of anomalies, we incorporate the structure information of the network stream data in our anomaly localization framework. We have performed comprehensive experimental studies of the proposed methods, and have compared our methods with the state-ofthe-art using three real-world data sets from different application domains. Our experimental studies demonstrate the utility of the proposed methods.

#index 1606028
#* Approximate kernel k-means: solution to large scale kernel clustering
#@ Radha Chitta;Rong Jin;Timothy C. Havens;Anil K. Jain
#t 2011
#c 0
#% 119916
#% 190190
#% 193458
#% 210173
#% 256620
#% 266426
#% 313959
#% 330327
#% 443531
#% 471558
#% 563100
#% 578388
#% 633222
#% 643008
#% 732552
#% 743284
#% 765282
#% 775647
#% 778728
#% 883972
#% 916799
#% 956521
#% 1015261
#% 1214733
#% 1254273
#% 1378293
#% 1484662
#% 1495861
#% 1860974
#! Digital data explosion mandates the development of scalable tools to organize the data in a meaningful and easily accessible form. Clustering is a commonly used tool for data organization. However, many clustering algorithms designed to handle large data sets assume linear separability of data and hence do not perform well on real world data sets. While kernel-based clustering algorithms can capture the non-linear structure in data, they do not scale well in terms of speed and memory requirements when the number of objects to be clustered exceeds tens of thousands. We propose an approximation scheme for kernel k-means, termed approximate kernel k-means, that reduces both the computational complexity and the memory requirements by employing a randomized approach. We show both analytically and empirically that the performance of approximate kernel k-means is similar to that of the kernel k-means algorithm, but with dramatically reduced run-time complexity and memory requirements.

#index 1606029
#* Ask me better questions: active learning queries based on rule induction
#@ Parisa Rashidi;Diane J. Cook
#t 2011
#c 0
#% 116165
#% 136350
#% 169717
#% 224755
#% 236729
#% 449566
#% 466887
#% 479973
#% 529191
#% 565531
#% 770771
#% 840004
#% 843651
#% 875997
#% 881575
#% 1074125
#% 1100053
#% 1220998
#% 1264829
#% 1387560
#% 1392451
#% 1415435
#% 1424132
#% 1558464
#! Active learning methods are used to improve the classification accuracy when little labeled data is available. Most traditional active learning methods pose a very specific query to the oracle, i.e. they ask for the label of an unlabeled example. This paper proposes a novel active learning method called RIQY (Rule Induced active learning QuerY). It can construct generic active learning queries based on rule induction from multiple unlabeled instances. These queries are shorter and more readable for the oracle and encompass many similar cases. Also the learning algorithm can achieve higher accuracy rates by asking fewer queries. We evaluate our algorithm on 12 different real datasets. Our results show that we can achieve higher accuracy rates using fewer queries compared to the traditional active learning methods.

#index 1606030
#* Automatically tagging email by leveraging other users' folders
#@ Yehuda Koren;Edo Liberty;Yoelle Maarek;Roman Sandler
#t 2011
#c 0
#% 466564
#% 501668
#% 581658
#% 722935
#% 844425
#% 991230
#% 1355036
#% 1457039
#% 1488024
#% 1535379
#! Most email applications devote a significant part of their real estate to organization mechanisms such as folders. Yet, we verified on the Yahoo! Mail service that 70% of email users have never defined a single folder. This implies that one of the most well known email features is underexploited. We propose here to revive the feature by providing a method for generating a lighter form of folders, or tags, benefiting even the most passive users. The method automatically associates, whenever possible, an appropriate semantic tag with a given email. This gives rise to an alternate mechanism for organizing and searching email. We advocate a novel modeling approach that exploits the overall population of users, thereby learning from the wisdom-of-crowds how to categorize messages. Given our massive user base, it is enough to learn from a minority of the users who label certain messages in order to label that kind of messages for the general population. We design a novel cascade classification approach, which copes with the severe scalability and accuracy constraints we are facing. Significant efficiency gains are achieved by working within a low dimensional latent space, and by using a novel hierarchical classifier. Precision level is controlled by separating the task into a two-phase classification process. We performed an extensive empirical study covering three different time periods, over 100 million messages, and thousands of candidate tags per message. The results are encouraging and compare favorably with alternative approaches. Our method successfully tags 72% of incoming email traffic. Performance-wise, the computational overhead, even on surge large traffic, is sufficiently low for our approach to be applicable in production on any large Web mail service.

#index 1606031
#* Axiomatic ranking of network role similarity
#@ Ruoming Jin;Victor E. Lee;Hui Hong
#t 2011
#c 0
#% 577273
#% 581661
#% 641979
#% 805904
#% 818218
#% 893124
#% 1055853
#% 1055861
#% 1115354
#% 1131498
#% 1195980
#% 1292521
#% 1292660
#! A key task in analyzing social networks and other complex networks is role analysis: describing and categorizing nodes by how they interact with other nodes. Two nodes have the same role if they interact with equivalent sets of neighbors. The most fundamental role equivalence is automorphic equivalence. Unfortunately, the fastest algorithm known for graph automorphism is nonpolynomial. Moreover, since exact equivalence is rare, a more meaningful task is measuring the role similarity between any two nodes. This task is closely related to the link-based similarity problem that SimRank addresses. However, SimRank and other existing simliarity measures are not sufficient because they do not guarantee to recognize automorphically or structurally equivalent nodes. This paper makes two contributions. First, we present and justify several axiomatic properties necessary for a role similarity measure or metric. Second, we present RoleSim, a role similarity metric which satisfies these axioms and which can be computed with a simple iterative algorithm. We rigorously prove that RoleSim satisfies all the axiomatic properties and demonstrate its superior interpretative power on both synthetic and real datasets.

#index 1606032
#* Brain effective connectivity modeling for alzheimer's disease by sparse gaussian bayesian network
#@ Shuai Huang;Jing Li;Jieping Ye;Adam Fleisher;Kewei Chen;Teresa Wu;Eric Reiman
#t 2011
#c 0
#% 643688
#% 893460
#% 1117673
#% 1238676
#% 1269873
#% 1650289
#% 1650673
#! Recent studies have shown that Alzheimer's disease (AD) is related to alteration in brain connectivity networks. One type of connectivity, called effective connectivity, defined as the directional relationship between brain regions, is essential to brain function. However, there have been few studies on modeling the effective connectivity of AD and characterizing its difference from normal controls (NC). In this paper, we investigate the sparse Bayesian Network (BN) for effective connectivity modeling. Specifically, we propose a novel formulation for the structure learning of BNs, which involves one L1-norm penalty term to impose sparsity and another penalty to ensure the learned BN to be a directed acyclic graph - a required property of BNs. We show, through both theoretical analysis and extensive experiments on eleven moderate and large benchmark networks with various sample sizes, that the proposed method has much improved learning accuracy and scalability compared with ten competing algorithms. We apply the proposed method to FDG-PET images of 42 AD and 67 NC subjects, and identify the effective connectivity models for AD and NC, respectively. Our study reveals that the effective connectivity of AD is different from that of NC in many ways, including the global-scale effective connectivity, intra-lobe, inter-lobe, and inter-hemispheric effective connectivity distributions, as well as the effective connectivity associated with specific brain regions. These findings are consistent with known pathology and clinical progression of AD, and will contribute to AD knowledge discovery.

#index 1606033
#* Classification of functional magnetic resonance imaging data using informative pattern features
#@ Francisco Pereira;Matthew Botvinick
#t 2011
#c 0
#% 768668
#! The canonical technique for analyzing functional magnetic resonance imaging (fMRI) data, statistical parametric mapping, produces maps of brain locations that are more active during performance of a task than during a control condition. In recent years, there has been increasing awareness of the fact that there is information in the entire pattern of brain activation and not just in saliently active locations. Classifiers have been the tool of choice for capturing this information and used to make predictions ranging from what kind of object a subject is thinking about to what decision they will make. Such classifiers are usually trained on a selection of voxels from the 3D grid that makes up the activation pattern; often this means the best accuracy is obtained using few voxels, from all across the brain, and that different voxels will be chosen in different cross-validation folds, making the classifiers hard to interpret. The increasing commonality of datasets with tens to hundreds of classes makes this problem even more acute. In this paper we introduce a method for identifying informative subsets of adjacent voxels, corresponding to brain patches that distinguish subsets of classes. These patches can then be used to train classifiers for the distinctions they support and used as "pattern features" for a meta-classifier. We show that this method permits classification at a higher accuracy than that obtained with traditional voxel selection, and that the sets of voxels used are more reproducible across cross-validation folds than those identified with voxel selection, and lie in plausible brain locations.

#index 1606034
#* Clustering with relative constraints
#@ Eric Yi Liu;Zhaojun Zhang;Wei Wang
#t 2011
#c 0
#% 201269
#% 208882
#% 269217
#% 316034
#% 342382
#% 464291
#% 464608
#% 464631
#% 466890
#% 503213
#% 769881
#% 770782
#% 846095
#% 870980
#% 907282
#% 961603
#% 1038720
#% 1085668
#% 1171459
#% 1183429
#% 1232364
#% 1663626
#! Recent studies have suggested using relative distance comparisons as constraints to represent domain knowledge. A natural extension to relative comparisons is the combination of two comparisons defined on the same set of three instances. Constraints in this form, termed Relative Constraints, provide a unified knowledge representation for both partitional and hierarchical clusterings. But many key properties of relative constraints remain unknown. In this paper, we answer the following important questions that enable the broader application of relative constraints in general clustering problems: " Feasibility: Does there exist a clustering that satisfies a given set of relative constraints? (consistency of constraints) "Completeness: Given a set of consistent relative constraints, how can one derive a complete clustering without running into dead-ends? " Informativeness: How can one extract the most informative relative constraints from given knowledge sources? We show that any hierarchical domain knowledge can be easily represented by relative constraints. We further present a hierarchical algorithm that finds a clustering satisfying all given constraints in polynomial time. Experiments showed that our algorithm achieves significantly higher accuracy than the existing metric learning approach based on relative comparisons.

#index 1606035
#* Common component analysis for multiple covariance matrices
#@ Huahua Wang;Arindam Banerjee;Daniel Boley
#t 2011
#c 0
#% 3084
#% 65393
#% 80995
#% 224113
#% 316143
#% 316150
#% 333881
#% 415756
#% 415767
#% 527853
#% 844312
#% 846431
#% 875946
#% 881493
#% 940956
#% 1300087
#! We consider the problem of finding a suitable common low dimensional subspace for accurately representing a given set of covariance matrices. With one covariance matrix, this is principal component analysis (PCA). For multiple covariance matrices, we term the problem Common Component Analysis (CCA). While CCA can be posed as a tensor decomposition problem, standard approaches to tensor decompositions have two critical issues: (i) tensor decomposition methods are iterative and rely on the initialization; (ii) for a given level of approximation error, it is difficult to choose a suitable low dimensionality. In this paper, we present a detailed analysis of CCA that yields an effective initialization and iterative algorithms for the problem. The proposed methodology has provable approximation guarantees w.r.t. the global maximum and also allows one to choose the dimensionality for a given level of approximation error. We also establish conditions under which the methodology will achieve the global maximum. We illustrate the effectiveness of the proposed method through extensive experiments on synthetic data as well as on two real stock market datasets, where major financial events can be visualized in low dimensions.

#index 1606036
#* Compression of weighted graphs
#@ Hannu Toivonen;Fang Zhou;Aleksi Hartikainen;Atte Hinkka
#t 2011
#c 0
#% 656281
#% 754117
#% 769887
#% 1063501
#% 1063512
#% 1083509
#% 1176876
#% 1318605
#% 1328171
#% 1726658
#! We propose to compress weighted graphs (networks), motivated by the observation that large networks of social, biological, or other relations can be complex to handle and visualize. In the process also known as graph simplification, nodes and (unweighted) edges are grouped to supernodes and superedges, respectively, to obtain a smaller graph. We propose models and algorithms for weighted graphs. The interpretation (i.e. decompression) of a compressed, weighted graph is that a pair of original nodes is connected by an edge if their supernodes are connected by one, and that the weight of an edge is approximated to be the weight of the superedge. The compression problem now consists of choosing supernodes, superedges, and superedge weights so that the approximation error is minimized while the amount of compression is maximized. In this paper, we formulate this task as the 'simple weighted graph compression problem'. We then propose a much wider class of tasks under the name of 'generalized weighted graph compression problem'. The generalized task extends the optimization to preserve longer-range connectivities between nodes, not just individual edge weights. We study the properties of these problems and propose a range of algorithms to solve them, with different balances between complexity and quality of the result. We evaluate the problems and algorithms experimentally on real networks. The results indicate that weighted graphs can be compressed efficiently with relatively little compression error.

#index 1606037
#* Content-driven trust propagation framework
#@ V.G. Vinod Vydiswaran;ChengXiang Zhai;Dan Roth
#t 2011
#c 0
#% 268079
#% 280819
#% 290830
#% 348173
#% 411762
#% 722904
#% 769967
#% 956520
#% 1036759
#% 1081580
#% 1310399
#% 1328156
#% 1355029
#% 1450880
#% 1481078
#% 1484339
#% 1523915
#! Existing fact-finding models assume availability of structured data or accurate information extraction. However, as online data gets more unstructured, these assumptions are no longer valid. To overcome this, we propose a novel, content-based, trust propagation framework that relies on signals from the textual content to ascertain veracity of free-text claims and compute trustworthiness of their sources. We incorporate the quality of relevant content into the framework and present an iterative algorithm for propagation of trust scores. We show that existing fact finders on structured data can be modeled as specific instances of this framework. Using a retrieval-based approach to find relevant articles, we instantiate the framework to compute trustworthiness of news sources and articles. We show that the proposed framework helps ascertain trustworthiness of sources better. We also show that ranking news articles based on trustworthiness learned from the content-driven framework is significantly better than baselines that ignore either the content quality or the trust framework.

#index 1606038
#* Cost-aware travel tour recommendation
#@ Yong Ge;Qi Liu;Hui Xiong;Alexander Tuzhilin;Jian Chen
#t 2011
#c 0
#% 734592
#% 734594
#% 813966
#% 818216
#% 888957
#% 939312
#% 1020829
#% 1083671
#% 1116993
#% 1214623
#% 1214666
#% 1227602
#% 1260273
#% 1287221
#% 1288794
#% 1298163
#% 1396096
#% 1399973
#% 1451230
#% 1482413
#% 1543056
#! Advances in tourism economics have enabled us to collect massive amounts of travel tour data. If properly analyzed, this data can be a source of rich intelligence for providing real-time decision making and for the provision of travel tour recommendations. However, tour recommendation is quite different from traditional recommendations, because the tourist's choice is directly affected by the travel cost, which includes the financial cost and the time. To that end, in this paper, we provide a focused study of cost-aware tour recommendation. Along this line, we develop two cost-aware latent factor models to recommend travel packages by considering both the travel cost and the tourist's interests. Specifically, we first design a cPMF model, which models the tourist's cost with a 2-dimensional vector. Also, in this cPMF model, the tourist's interests and the travel cost are learnt by exploring travel tour data. Furthermore, in order to model the uncertainty in the travel cost, we further introduce a Gaussian prior into the cPMF model and develop the GcPMF model, where the Gaussian prior is used to express the uncertainty of the travel cost. Finally, experiments on real-world travel tour data show that the cost-aware recommendation models outperform state-of-the-art latent factor models with a significant margin. Also, the GcPMF model with the Gaussian prior can better capture the impact of the uncertainty of the travel cost, and thus performs better than the cPMF model.

#index 1606039
#* Discovering highly reliable subgraphs in uncertain graphs
#@ Ruoming Jin;Lin Liu;Charu C. Aggarwal
#t 2011
#c 0
#% 165487
#% 237380
#% 277018
#% 322536
#% 370988
#% 729923
#% 754098
#% 818916
#% 823357
#% 985041
#% 1080074
#% 1083509
#% 1100170
#% 1117041
#% 1117057
#% 1179162
#% 1217126
#% 1451203
#% 1464049
#% 1506189
#% 1523884
#% 1592313
#% 1697228
#% 1710570
#! In this paper, we investigate the highly reliable subgraph problem, which arises in the context of uncertain graphs. This problem attempts to identify all induced subgraphs for which the probability of connectivity being maintained under uncertainty is higher than a given threshold. This problem arises in a wide range of network applications, such as protein-complex discovery, network routing, and social network analysis. Since exact discovery may be computationally intractable, we introduce a novel sampling scheme which enables approximate discovery of highly reliable subgraphs with high probability. Furthermore, we transform the core mining task into a new frequent cohesive set problem in deterministic graphs. Such transformation enables the development of an efficient two-stage approach which combines novel peeling techniques for maximal set discovery with depth-first search for further enumeration. We demonstrate the effectiveness and efficiency of the proposed algorithms on real and synthetic data sets.

#index 1606040
#* Discovering shakers from evolving entities via cascading graph inference
#@ Xiaoxiao Shi;Wei Fan;Jianping Zhang;Philip Yu
#t 2011
#c 0
#% 197387
#% 268079
#% 313959
#% 632090
#% 729923
#% 824709
#% 868469
#% 1005548
#% 1035589
#% 1269888
#% 1394202
#% 1451242
#% 1451244
#% 1451246
#% 1451249
#% 1676017
#% 1697242
#! In an interconnected and dynamic world, the evolution of one entity may cause a series of significant value changes for some others. For example, the currency inflation of Thailand caused the currency slump of other Asian countries, which eventually led to the financial crisis of 1997. We call such high impact entities shakers. To discover shakers, we first introduce the concept of a cascading graph to capture the causality relationships among evolving entities over some period of time, and then infer shakers from the graph. In a cascading graph, nodes represent entities and weighted links represent the causality effects. In order to find hidden shakers in such a graph, two scoring functions are proposed, each of which estimates how much the target entity can affect the values of some others. The idea is to artificially inject a significant change on the target entity, and estimate its direct and indirect influence on the others, by following an inference rule under the Markovian assumption. Both scoring functions are proven to be only dependent on the structure of a cascading graph and can be calculated in polynomial time. Experiments included three datasets in social sciences. Without directly applicable previous methods, we modified three graphical models as baselines. The two proposed scoring functions can effectively capture those high impact entities. For example, in the experiment to discover stock market shakers, the proposed models outperform the three baselines by as much as 50% in accuracy with the ground truth obtained from Yahoo!~Finance.

#index 1606041
#* Discovering spatio-temporal causal interactions in traffic data streams
#@ Wei Liu;Yu Zheng;Sanjay Chawla;Jing Yuan;Xie Xing
#t 2011
#c 0
#% 288660
#% 770890
#% 785358
#% 810049
#% 813978
#% 844292
#% 844299
#% 960283
#% 963521
#% 967005
#% 975028
#% 1206639
#% 1214637
#% 1214680
#% 1214740
#% 1254221
#% 1451247
#% 1451250
#% 1480783
#% 1495599
#% 1496772
#% 1605948
#% 1735404
#! The detection of outliers in spatio-temporal traffic data is an important research problem in the data mining and knowledge discovery community. However to the best of our knowledge, the discovery of relationships, especially causal interactions, among detected traffic outliers has not been investigated before. In this paper we propose algorithms which construct outlier causality trees based on temporal and spatial properties of detected outliers. Frequent substructures of these causality trees reveal not only recurring interactions among spatio-temporal outliers, but potential flaws in the design of existing traffic networks. The effectiveness and strength of our algorithms are validated by experiments on a very large volume of real taxi trajectories in an urban road network.

#index 1606042
#* Display advertising impact: search lift and social influence
#@ Panagiotis Papadimitriou;Hector Garcia-Molina;Prabhakar Krishnamurthy;Randall A. Lewis;David H. Reiley
#t 2011
#c 0
#% 342596
#% 729923
#% 949164
#% 1055677
#% 1159217
#% 1451139
#% 1560370
#% 1581646
#% 1584788
#! We study the impact of display advertising on user search behavior using a field experiment. In such an experiment, the treatment group users are exposed to some display advertising campaign, while the control group users are not. During the campaign and the post-campaign period we monitor the user search queries and we label them as relevant or irrelevant to the campaign using techniques that leverage the bipartite query-URL click graph. Our results indicate that users who are exposed to the advertising campaign submit 5% to 25% more queries that are relevant to it compared to the unexposed users. Using the social graph of the experiment users, we also explore how users are affected by their friends who are exposed to ads. Our results indicate that a user with exposed friends is more likely to submit queries relevant to the campaign, as compared to a user without exposed friends. The result is surprising given that the display advertising campaign that we study does not include any incentive for social action, e.g., discount for recommending friends.

#index 1606043
#* Diversified ranking on large graphs: an optimization viewpoint
#@ Hanghang Tong;Jingrui He;Zhen Wen;Ravi Konuru;Ching-Yung Lin
#t 2011
#c 0
#% 262112
#% 278500
#% 290830
#% 309749
#% 348173
#% 528152
#% 729983
#% 730089
#% 805841
#% 823359
#% 824710
#% 881480
#% 915344
#% 989572
#% 989613
#% 1016175
#% 1074025
#% 1181290
#% 1190062
#% 1214650
#% 1214666
#% 1214668
#% 1214675
#% 1214695
#% 1312812
#% 1318636
#% 1318676
#% 1400003
#% 1451163
#% 1451191
#% 1451193
#% 1451230
#% 1451241
#% 1451245
#% 1535449
#% 1536568
#! Diversified ranking on graphs is a fundamental mining task and has a variety of high-impact applications. There are two important open questions here. The first challenge is the measure - how to quantify the goodness of a given top-k ranking list that captures both the relevance and the diversity? The second challenge lies in the algorithmic aspect - how to find an optimal, or near-optimal, top-k ranking list that maximizes the measure we defined in a scalable way? In this paper, we address these challenges from an optimization point of view. Firstly, we propose a goodness measure for a given top-k ranking list. The proposed goodness measure intuitively captures both (a) the relevance between each individual node in the ranking list and the query; and (b) the diversity among different nodes in the ranking list. Moreover, we propose a scalable algorithm (linear wrt the size of the graph) that generates a provably near-optimal solution. The experimental evaluations on real graphs demonstrate its effectiveness and efficiency.

#index 1606044
#* Entity disambiguation with hierarchical topic models
#@ Saurabh S. Kataria;Krishnan S. Kumar;Rajeev R. Rastogi;Prithviraj Sen;Srinivasan H. Sengamedu
#t 2011
#c 0
#% 266292
#% 722904
#% 876017
#% 983883
#% 1019082
#% 1055680
#% 1055682
#% 1130858
#% 1206818
#% 1214667
#% 1338553
#! Disambiguating entity references by annotating them with unique ids from a catalog is a critical step in the enrichment of unstructured content. In this paper, we show that topic models, such as Latent Dirichlet Allocation (LDA) and its hierarchical variants, form a natural class of models for learning accurate entity disambiguation models from crowd-sourced knowledge bases such as Wikipedia. Our main contribution is a semi-supervised hierarchical model called Wikipedia-based Pachinko Allocation Model} (WPAM) that exploits: (1) All words in the Wikipedia corpus to learn word-entity associations (unlike existing approaches that only use words in a small fixed window around annotated entity references in Wikipedia pages), (2) Wikipedia annotations to appropriately bias the assignment of entity labels to annotated (and co-occurring unannotated) words during model learning, and (3) Wikipedia's category hierarchy to capture co-occurrence patterns among entities. We also propose a scheme for pruning spurious nodes from Wikipedia's crowd-sourced category hierarchy. In our experiments with multiple real-life datasets, we show that WPAM outperforms state-of-the-art baselines by as much as 16% in terms of disambiguation accuracy.

#index 1606045
#* Exploiting place features in link prediction on location-based social networks
#@ Salvatore Scellato;Anastasios Noulas;Cecilia Mascolo
#t 2011
#c 0
#% 136350
#% 292240
#% 400847
#% 406493
#% 420065
#% 466086
#% 730089
#% 858102
#% 926881
#% 1083675
#% 1287262
#% 1399939
#% 1451163
#% 1451178
#% 1475162
#% 1476153
#! Link prediction systems have been largely adopted to recommend new friends in online social networks using data about social interactions. With the soaring adoption of location-based social services it becomes possible to take advantage of an additional source of information: the places people visit. In this paper we study the problem of designing a link prediction system for online location-based social networks. We have gathered extensive data about one of these services, Gowalla, with periodic snapshots to capture its temporal evolution. We study the link prediction space, finding that about 30% of new links are added among "place-friends", i.e., among users who visit the same places. We show how this prediction space can be made 15 times smaller, while still 66% of future connections can be discovered. Thus, we define new prediction features based on the properties of the places visited by users which are able to discriminate potential future links among them. Building on these findings, we describe a supervised learning framework which exploits these prediction features to predict new links among friends-of-friends and place-friends. Our evaluation shows how the inclusion of information about places and related user activity offers high link prediction performance. These results open new directions for real-world link recommendation systems on location-based social networks.

#index 1606046
#* Fast approximate similarity search based on degree-reduced neighborhood graphs
#@ Kazuo Aoyama;Kazumi Saito;Hiroshi Sawada;Naonori Ueda
#t 2011
#c 0
#% 1722
#% 249321
#% 342827
#% 347264
#% 627909
#% 731409
#% 749529
#% 829310
#% 898309
#% 1217189
#% 1289399
#% 1350729
#% 1385976
#! This paper presents a fast approximate similarity search method for finding the most similar object to a given query object from an object set with a dissimilarity with a success probability exceeding a given value. As a search index, the proposed method utilizes a degree-reduced k-nearest neighbor (k-DR) graph constructed from the object set with the dissimilarity, and explores the k-DR graph along its edges using a greedy search (GS) algorithm starting from multiple initial vertices with parallel processing. In the graph-construction stage, the structural parameter k of the k-DR graph is determined so that the probability with which at least one search trial of those with multiple initial vertices succeeds is more than the given success probability. To estimate the greedy-search success probability, we introduce the concept of a basin in the k-DR graph. The experimental results on a real data set verify the approximation scheme and high search performance of the proposed method and demonstrate that it is superior to E2LSH in terms of the expected search cost.

#index 1606047
#* Fast coordinate descent methods with variable selection for non-negative matrix factorization
#@ Cho-Jui Hsieh;Inderjit S. Dhillon
#t 2011
#c 0
#% 51999
#% 722937
#% 763708
#% 793248
#% 818291
#% 995168
#% 1133918
#% 1176925
#% 1733138
#% 1845372
#! Nonnegative Matrix Factorization (NMF) is an effective dimension reduction method for non-negative dyadic data, and has proven to be useful in many areas, such as text mining, bioinformatics and image processing. NMF is usually formulated as a constrained non-convex optimization problem, and many algorithms have been developed for solving it. Recently, a coordinate descent method, called FastHals, has been proposed to solve least squares NMF and is regarded as one of the state-of-the-art techniques for the problem. In this paper, we first show that FastHals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus, performs unneeded descent steps on unimportant variables. We then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method. Our new method is considerably faster in practice and we show that it has theoretical convergence guarantees. Moreover when the solution is sparse, as is often the case in real applications, our new method benefits by selecting important variables to update more often, thus resulting in higher speed. As an example, on a text dataset RCV1, our method is 7 times faster than FastHals, and more than 15 times faster when the sparsity is increased by adding an L1 penalty. We also develop new coordinate descent methods when error in NMF is measured by KL-divergence by applying the Newton method to solve the one-variable sub-problems. Experiments indicate that our algorithm for minimizing the KL-divergence is faster than the Lee & Seung multiplicative rule by a factor of 10 on the CBCL image dataset.

#index 1606048
#* Fast locality-sensitive hashing
#@ Anirban Dasgupta;Ravi Kumar;Tamas Sarlos
#t 2011
#c 0
#% 3084
#% 205305
#% 232767
#% 249321
#% 255137
#% 311808
#% 347225
#% 479649
#% 479973
#% 762054
#% 805839
#% 824787
#% 847166
#% 879600
#% 956506
#% 956507
#% 1022281
#% 1023422
#% 1038911
#% 1083648
#% 1107013
#% 1133921
#% 1299706
#% 1310347
#% 1451253
#! Locality-sensitive hashing (LSH) is a basic primitive in several large-scale data processing applications, including nearest-neighbor search, de-duplication, clustering, etc. In this paper we propose a new and simple method to speed up the widely-used Euclidean realization of LSH. At the heart of our method is a fast way to estimate the Euclidean distance between two d-dimensional vectors; this is achieved by the use of randomized Hadamard transforms in a non-linear setting. This decreases the running time of a (k, L)-parameterized LSH from O(dkL) to O(dlog d + kL). Our experiments show that using the new LSH in nearest-neighbor applications can improve their running times by significant amounts. To the best of our knowledge, this is the first running time improvement to LSH that is both provable and practical.

#index 1606049
#* Friendship and mobility: user movement in location-based social networks
#@ Eunjoon Cho;Seth A. Myers;Jure Leskovec
#t 2011
#c 0
#% 1055763
#% 1090046
#% 1137671
#% 1190131
#% 1190134
#% 1300556
#% 1399939
#% 1399973
#% 1400036
#% 1451250
#% 1476153
#% 1525070
#! Even though human movement and mobility patterns have a high degree of freedom and variation, they also exhibit structural patterns due to geographic and social constraints. Using cell phone location data, as well as data from two online location-based social networks, we aim to understand what basic laws govern human motion and dynamics. We find that humans experience a combination of periodic movement that is geographically limited and seemingly random jumps correlated with their social networks. Short-ranged travel is periodic both spatially and temporally and not effected by the social network structure, while long-distance travel is more influenced by social network ties. We show that social relationships can explain about 10% to 30% of all human movement, while periodic behavior explains 50% to 70%. Based on our findings, we develop a model of human mobility that combines periodic short range movements with travel due to the social network structure. We show that our model reliably predicts the locations and dynamics of future human movement and gives an order of magnitude better performance than present models of human mobility.

#index 1606050
#* GBASE: a scalable and general graph management system
#@ U. Kang;Hanghang Tong;Jimeng Sun;Ching-Yung Lin;Christos Faloutsos
#t 2011
#c 0
#% 278500
#% 291940
#% 754117
#% 765462
#% 824697
#% 824710
#% 844334
#% 915344
#% 954300
#% 960304
#% 963669
#% 1022280
#% 1063502
#% 1063542
#% 1063553
#% 1083726
#% 1127559
#% 1176961
#% 1206841
#% 1214643
#% 1214675
#% 1217170
#% 1217232
#% 1318636
#% 1328108
#% 1328181
#% 1382887
#% 1404186
#% 1426513
#% 1426547
#% 1451191
#% 1451193
#% 1673564
#% 1710593
#! Graphs appear in numerous applications including cyber-security, the Internet, social networks, protein networks, recommendation systems, and many more. Graphs with millions or even billions of nodes and edges are common-place. How to store such large graphs efficiently? What are the core operations/queries on those graph? How to answer the graph queries quickly? We propose GBASE, a scalable and general graph management and mining system. The key novelties lie in 1) our storage and compression scheme for a parallel setting and 2) the carefully chosen graph operations and their efficient implementation. We designed and implemented an instance of GBASE using MapReduce/Hadoop. GBASE provides a parallel indexing mechanism for graph mining operations that both saves storage space, as well as accelerates queries. We ran numerous experiments on real graphs, spanning billions of nodes and edges, and we show that our proposed GBASE is indeed fast, scalable and nimble, with significant savings in space and time.

#index 1606051
#* Human mobility, social ties, and link prediction
#@ Dashun Wang;Dino Pedreschi;Chaoming Song;Fosca Giannotti;Albert-Laszlo Barabasi
#t 2011
#c 0
#% 730089
#% 809424
#% 827132
#% 989604
#% 1083675
#% 1099023
#% 1108850
#% 1117026
#% 1206625
#% 1214685
#% 1399939
#% 1425621
#% 1451163
#% 1476153
#% 1663073
#! Our understanding of how individual mobility patterns shape and impact the social network is limited, but is essential for a deeper understanding of network dynamics and evolution. This question is largely unexplored, partly due to the difficulty in obtaining large-scale society-wide data that simultaneously capture the dynamical information on individual movements and social interactions. Here we address this challenge for the first time by tracking the trajectories and communication records of 6 Million mobile phone users. We find that the similarity between two individuals' movements strongly correlates with their proximity in the social network. We further investigate how the predictive power hidden in such correlations can be exploited to address a challenging problem: which new links will develop in a social network. We show that mobility measures alone yield surprising predictive power, comparable to traditional network-based measures. Furthermore, the prediction accuracy can be significantly improved by learning a supervised classifier based on combined mobility and network measures. We believe our findings on the interplay of mobility patterns and social ties offer new perspectives on not only link prediction but also network dynamics.

#index 1606052
#* I want to answer; who has a question?: Yahoo! answers recommender system
#@ Gideon Dror;Yehuda Koren;Yoelle Maarek;Idan Szpektor
#t 2011
#c 0
#% 124010
#% 280817
#% 280852
#% 330687
#% 448194
#% 452563
#% 907525
#% 956521
#% 1035587
#% 1179994
#% 1190060
#% 1214623
#% 1260273
#% 1270283
#% 1280725
#% 1287234
#% 1292541
#% 1399976
#% 1450880
#% 1450883
#% 1541728
#! Yahoo! Answers is currently one of the most popular question answering systems. We claim however that its user experience could be significantly improved if it could route the "right question" to the "right user." Indeed, while some users would rush answering a question such as "what should I wear at the prom?," others would be upset simply being exposed to it. We argue here that Community Question Answering sites in general and Yahoo! Answers in particular, need a mechanism that would expose users to questions they can relate to and possibly answer. We propose here to address this need via a multi-channel recommender system technology for associating questions with potential answerers on Yahoo! Answers. One novel aspect of our approach is exploiting a wide variety of content and social signals users regularly provide to the system and organizing them into channels. Content signals relate mostly to the text and categories of questions and associated answers, while social signals capture the various user interactions with questions, such as asking, answering, voting, etc. We fuse and generalize known recommendation approaches within a single symmetric framework, which incorporates and properly balances multiple types of signals according to channels. Tested on a large scale dataset, our model exhibits good performance, clearly outperforming standard baselines.

#index 1606053
#* Improving predictions using aggregate information
#@ Amit Dhurandhar
#t 2011
#c 0
#% 272995
#% 424081
#% 989577
#% 1214744
#% 1495349
#% 1495498
#% 1535126
#! In domains such as consumer products or manufacturing amongst others, we have problems that warrant the prediction of a continuous target. Besides the usual set of explanatory attributes we may also have exact (or approximate) estimates of aggregated targets, which are the sums of disjoint sets of individual targets that we are trying to predict. Hence, the question now becomes can we use these aggregated targets, which are a coarser piece of information, to improve the quality of predictions of the individual targets? In this paper, we provide a simple yet provable way of accomplishing this. In particular, given predictions from any regression model of the target on the test data, we elucidate a provable method for improving these predictions in terms of mean squared error, given exact (or accurate enough) information of the aggregated targets. These estimates of the aggregated targets may be readily available or obtained -- through multilevel regression -- at different levels of granularity. Based on the proof of our method we suggest a criterion for choosing the appropriate level. Moreover, in addition to estimates of the aggregated targets, if we have exact (or approximate) estimates of the mean and variance of the target distribution, then based on our general strategy we provide an optimal way of incorporating this information so as to further improve the quality of predictions of the individual targets. We then validate the results and our claims by conducting experiments on synthetic and real industrial data obtained from diverse domains.

#index 1606054
#* INCONCO: interpretable clustering of numerical and categorical objects
#@ Claudia Plant;Christian Böhm
#t 2011
#c 0
#% 210173
#% 300131
#% 375388
#% 420081
#% 466425
#% 570885
#% 729918
#% 765439
#% 810047
#% 881456
#% 918017
#% 938978
#% 949578
#% 986227
#% 1063483
#% 1078626
#% 1083683
#% 1083750
#% 1165480
#% 1211824
#% 1318668
#% 1671475
#% 1737762
#! The integrative mining of heterogeneous data and the interpretability of the data mining result are two of the most important challenges of today's data mining. It is commonly agreed in the community that, particularly in the research area of clustering, both challenges have not yet received the due attention. Only few approaches for clustering of objects with mixed-type attributes exist and those few approaches do not consider cluster-specific dependencies between numerical and categorical attributes. Likewise, only a few clustering papers address the problem of interpretability: to explain why a certain set of objects have been grouped into a cluster and what a particular cluster distinguishes from another. In this paper, we approach both challenges by constructing a relationship to the concept of data compression using the Minimum Description Length principle: a detected cluster structure is the better the more efficient it can be exploited for data compression. Following this idea, we can learn, during the run of a clustering algorithm, the optimal trade-off for attribute weights and distinguish relevant attribute dependencies from coincidental ones. We extend the efficient Cholesky decomposition to model dependencies in heterogeneous data and to ensure interpretability. Our proposed algorithm, INCONCO, successfully finds clusters in mixed type data sets, identifies the relevant attribute dependencies, and explains them using linear models and case-by-case analysis. Thereby, it outperforms existing approaches in effectiveness, as our extensive experimental evaluation demonstrates.

#index 1606055
#* Incorporating SAT solvers into hierarchical clustering algorithms: an efficient and flexible approach
#@ Sean Gilpin;Ian Davidson
#t 2011
#c 0
#% 288952
#% 466890
#% 576214
#% 601159
#% 961603
#% 1038344
#% 1085668
#% 1183429
#% 1673558
#% 1727637
#! The area of constrained clustering has been actively pursued for the last decade. A more recent extension that will be the focus of this paper is constrained hierarchical clustering which allows building user-constrained dendrograms/trees. Like all forms of constrained clustering, previous work on hierarchical constrained clustering uses simple constraints that are typically implemented in a procedural language. However, there exists mature results and packages in the fields of constraint satisfaction languages and solvers that the constrained clustering field has yet to explore. This work marks the first steps towards introducing constraints satisfaction languages/solvers into hierarchical constrained clustering. We make several significant contributions. We show how many existing and new constraints for hierarchical clustering, can be modeled as a Horn-SAT problem that is easily solvable in polynomial time and which allows their implementation in any number of declarative languages or efficient solvers. We implement our own solver for efficiency reasons. We then show how to formulate constrained hierarchical clustering in a flexible manner so that any number of algorithms, whose output is a dendrogram, can make use of the constraints.

#index 1606056
#* Latent graphical models for quantifying and predicting patent quality
#@ Yan Liu;Pei-yun Hseuh;Rick Lawrence;Steve Meliksetian;Claudia Perlich;Alejandro Veen
#t 2011
#c 0
#% 881530
#% 1093383
#% 1211770
#% 1214739
#! The number of patents filed each year has increased dramatically in recent years, raising concerns that patents of questionable validity are restricting the issuance of truly innovative patents. For this reason, there is a strong demand to develop an objective model to quantify patent quality and characterize the attributes that lead to higher-quality patents. In this paper, we develop a latent graphical model to infer patent quality from related measurements. In addition, we extract advanced lexical features via natural language processing techniques to capture the quality measures such as clarity of claims, originality, and importance of cited prior art. We demonstrate the effectiveness of our approach by validating its predictions with previous court decisions of litigated patents.

#index 1606057
#* Logical-shapelets: an expressive primitive for time series classification
#@ Abdullah Mueen;Eamonn Keogh;Neal Young
#t 2011
#c 0
#% 643518
#% 810058
#% 993965
#% 1127609
#% 1214716
#% 1273832
#% 1307196
#% 1426516
#% 1538184
#! Time series shapelets are small, local patterns in a time series that are highly predictive of a class and are thus very useful features for building classifiers and for certain visualization and summarization tasks. While shapelets were introduced only recently, they have already seen significant adoption and extension in the community. Despite their immense potential as a data mining primitive, there are two important limitations of shapelets. First, their expressiveness is limited to simple binary presence/absence questions. Second, even though shapelets are computed offline, the time taken to compute them is significant. In this work, we address the latter problem by introducing a novel algorithm that finds shapelets in less time than current methods by an order of magnitude. Our algorithm is based on intelligent caching and reuse of computations, and the admissible pruning of the search space. Because our algorithm is so fast, it creates an opportunity to consider more expressive shapelet queries. In particular, we show for the first time an augmented shapelet representation that distinguishes the data based on conjunctions or disjunctions of shapelets. We call our novel representation Logical-Shapelets. We demonstrate the efficiency of our approach on the classic benchmark datasets used for these problems, and show several case studies where logical shapelets significantly outperform the original shapelet representation and other time series classification techniques. We demonstrate the utility of our ideas in domains as diverse as gesture recognition, robotics, and biometrics.

#index 1606058
#* Meta optimization and its application to portfolio selection
#@ Puja Das;Arindam Banerjee
#t 2011
#c 0
#% 165663
#% 209021
#% 227736
#% 232319
#% 235377
#% 240809
#% 400847
#% 722907
#% 835018
#% 850011
#% 871302
#% 875946
#% 875955
#% 1000325
#% 1073274
#% 1272037
#! Several data mining algorithms use iterative optimization methods for learning predictive models. It is not easy to determine upfront which optimization method will perform best or converge fast for such tasks. In this paper, we analyze Meta Algorithms (MAs) which work by adaptively combining iterates from a pool of base optimization algorithms. We show that the performance of MAs are competitive with the best convex combination of the iterates from the base algorithms for online as well as batch convex optimization problems. We illustrate the effectiveness of MAs on the problem of portfolio selection in the stock market and use several existing ideas for portfolio selection as base algorithms. Using daily S\&P500 data for the past 21 years and a benchmark NYSE dataset, we show that MAs outperform existing portfolio selection algorithms with provable guarantees by several orders of magnitude, and match the performance of the best heuristics in the pool.

#index 1606059
#* Mining closed episodes with simultaneous events
#@ Nikolaj Tatti;Boris Cule
#t 2011
#c 0
#% 172892
#% 420063
#% 443502
#% 463903
#% 466496
#% 481290
#% 727913
#% 745515
#% 799764
#% 805093
#% 844359
#% 902449
#% 989612
#% 1117002
#% 1176861
#% 1318693
#% 1535472
#% 1737789
#! Sequential pattern discovery is a well-studied field in data mining. Episodes are sequential patterns describing events that often occur in the vicinity of each other. Episodes can impose restrictions to the order of the events, which makes them a versatile technique for describing complex patterns in the sequence. Most of the research on episodes deals with special cases such as serial, parallel, and injective episodes, while discovering general episodes is understudied. In this paper we extend the definition of an episode in order to be able to represent cases where events often occur simultaneously. We present an efficient and novel miner for discovering frequent and closed general episodes. Such a task presents unique challenges. Firstly, we cannot define closure based on frequency. We solve this by computing a more conservative closure that we use to reduce the search space and discover the closed episodes as a postprocessing step. Secondly, episodes are traditionally presented as directed acyclic graphs. We argue that this representation has drawbacks leading to redundancy in the output. We solve these drawbacks by defining a subset relationship in such a way that allows us to remove the redundant episodes. We demonstrate the efficiency of our algorithm and the need for using closed episodes empirically on synthetic and real-world datasets.

#index 1606060
#* Mining mobility data to minimise travellers' spending on public transport
#@ Neal Lathia;Licia Capra
#t 2011
#c 0
#% 327432
#% 437902
#% 729921
#% 1272280
#% 1291463
#% 1295868
#% 1301004
#% 1384295
#% 1535435
#% 1541728
#! As the public transport infrastructure of large cities expands, transport operators are diversifying the range and prices of tickets that can be purchased for travel. However, selecting the best fare for each individual traveller's needs is a complex process that is left almost completely unaided. By examining the relation between urban mobility and fare purchasing habits in large datasets from London, England's public transport network, we estimate that travellers in the city cumulatively spend, per year, up to approximately GBP 200 million more than they need to, as a result of purchasing the incorrect fares. We propose to address these incorrect purchases by leveraging the huge volumes of data that travellers create as they move about the city, by providing, to each of them, personalised ticket recommendations based on their estimated future travel patterns. In this work, we explore the viability of building a fare-recommendation system for public transport networks by (a) formalising the problem as two separate prediction problems and (b) evaluating a number of algorithms that aim to match travellers to the best fare. We find that applying data mining techniques to public transport data has the potential to provide travellers with substantial savings.

#index 1606061
#* Mining mobility user profiles for car pooling
#@ Roberto Trasarti;Fabio Pinelli;Mirco Nanni;Fosca Giannotti
#t 2011
#c 0
#% 280416
#% 989604
#% 1046207
#% 1207116
#% 1480826
#% 1495262
#% 1496793
#% 1531117
#% 1720762
#% 1953174
#! In this paper we introduce a methodology for extracting mobility profiles of individuals from raw digital traces (in particular, GPS traces), and study criteria to match individuals based on profiles. We instantiate the profile matching problem to a specific application context, namely proactive car pooling services, and therefore develop a matching criterion that satisfies various basic constraints obtained from the background knowledge of the application domain. In order to evaluate the impact and robustness of the methods introduced, two experiments are reported, which were performed on a massive dataset containing GPS traces of private cars: (i) the impact of the car pooling application based on profile matching is measured, in terms of percentage shareable traffic; (ii) the approach is adapted to coarser-grained mobility data sources that are nowadays commonly available from telecom operators. In addition the ensuing loss in precision and coverage of profile matches is measured.

#index 1606062
#* Mining partially annotated images
#@ Zhongang Qi;Ming Yang;Zhongfei (Mark) Zhang;Zhengyou Zhang
#t 2011
#c 0
#% 464641
#% 642989
#% 642990
#% 722904
#% 722927
#% 760805
#% 836778
#% 989605
#% 1040539
#% 1174988
#% 1292880
#% 1378799
#% 1484605
#% 1502531
#% 1727359
#! In this paper, we study the problem of mining partially annotated images. We first define what the problem of mining partially annotated images is, and argue that in many real-world applications annotated images are typically partially annotated and thus that the problem of mining partially annotated images exists in many situations. We then propose an effective solution to this problem based on a statistical model we have developed called the Semi-Supervised Correspondence Hierarchical Dirichlet Process (SSCHDP). The main idea of this model lies in exploiting the information pertaining to partially annotated images or even unannotated images to achieve semi-supervised learning under the HDP structure. We apply this model to completing the annotations appropriately for partially annotated images in the training data and then to predicting the annotations appropriately and completely for all the unannotated images either in the training data or in any unseen data beyond the training process. Experiments show that SSC-HDP is superior to the peer models from the recent literature when they are applied to solving the problem of mining partially annotated images.

#index 1606063
#* Multi-view transfer learning with a large margin approach
#@ Dan Zhang;Jingrui He;Yan Liu;Luo Si;Richard Lawrence
#t 2011
#c 0
#% 252011
#% 280819
#% 316509
#% 397147
#% 464267
#% 743284
#% 770804
#% 770847
#% 881477
#% 881557
#% 983828
#% 983899
#% 987253
#% 989592
#% 1073897
#% 1073994
#% 1083678
#% 1117687
#% 1211717
#% 1237657
#% 1269766
#% 1270196
#% 1270208
#% 1292600
#% 1386108
#% 1417094
#! Transfer learning has been proposed to address the problem of scarcity of labeled data in the target domain by leveraging the data from the source domain. In many real world applications, data is often represented from different perspectives, which correspond to multiple views. For example, a web page can be described by its contents and its associated links. However, most existing transfer learning methods fail to capture the multi-view {nature}, and might not be best suited for such applications. To better leverage both the labeled data from the source domain and the features from different views, {this paper proposes} a general framework: Multi-View Transfer Learning with a Large Margin Approach (MVTL-LM). On one hand, labeled data from the source domain is effectively utilized to construct a large margin classifier; on the other hand, the data from both domains is employed to impose consistencies among multiple views. As an instantiation of this framework, we propose an efficient optimization method, which is guaranteed to converge to ε precision in O(1/ε) steps. Furthermore, we analyze its error bound, which improves over existing results of related methods. An extensive set of experiments are conducted to demonstrate the advantages of our proposed method over state-of-the-art techniques.

#index 1606064
#* MultiRank: co-ranking for objects and relations in multi-relational data
#@ Michaek Kwok-Po Ng;Xutao Li;Yunming Ye
#t 2011
#c 0
#% 290830
#% 397169
#% 754089
#% 805877
#% 805896
#% 844312
#% 881493
#% 918685
#% 927768
#% 1089780
#% 1116996
#% 1214645
#% 1214674
#% 1719430
#! The main aim of this paper is to design a co-ranking scheme for objects and relations in multi-relational data. It has many important applications in data mining and information retrieval. However, in the literature, there is a lack of a general framework to deal with multi-relational data for co-ranking. The main contribution of this paper is to (i) propose a framework (MultiRank) to determine the importance of both objects and relations simultaneously based on a probability distribution computed from multi-relational data; (ii) show the existence and uniqueness of such probability distribution so that it can be used for co-ranking for objects and relations very effectively; and (iii) develop an efficient iterative algorithm to solve a set of tensor (multivariate polynomial) equations to obtain such probability distribution. Extensive experiments on real-world data suggest that the proposed framework is able to provide a co-ranking scheme for objects and relations successfully. Experimental results have also shown that our algorithm is computationally efficient, and effective for identification of interesting and explainable co-ranking results.

#index 1606065
#* On dynamic data-driven selection of sensor streams
#@ Charu C. Aggarwal;Yan Xie;Philip S. Yu
#t 2011
#c 0
#% 36672
#% 481281
#% 632090
#% 654488
#% 765445
#% 801695
#% 810058
#% 918001
#% 938511
#% 990806
#% 993961
#% 1016178
#% 1269936
#% 1414128
#% 1491381
#! Sensor nodes have limited local storage, computational power, and battery life, as a result of which it is desirable to minimize the storage, processing and communication from these nodes during data collection. The problem is further magnified by the large volumes of data collected. In real applications, sensor streams are often highly correlated with one another or may have other kinds of functional dependencies. For example, a group of sound sensors in a given geographical proximity may pick almost the same set of signals. Clearly, since there are considerable functional dependencies between different sensors, there are huge redundancies in the data collected by sensors. These redundancies may also change as the data evolve over time. In this paper, we discuss real time algorithms for reducing the volume of the data collected in sensor networks. The broad idea is to determine the functional dependencies between sensor streams efficiently in real time, and actively collect the data only from a minimal set of sensors. The remaining sensors collect the data passively at low sampling rates in order to detect any changing trends in the underlying data. We present real time algorithms in order to minimize the power consumption in reducing the data collected and show that the resulting data retains almost the same amount of information at a much lower cost.

#index 1606066
#* On the privacy of anonymized networks
#@ Pedram Pedarsani;Matthias Grossglauser
#t 2011
#c 0
#% 288990
#% 300079
#% 309748
#% 316784
#% 344551
#% 528369
#% 772884
#% 814008
#% 823342
#% 840722
#% 956511
#% 975090
#% 1002007
#% 1062485
#% 1080078
#% 1080080
#% 1080082
#% 1080356
#% 1083672
#% 1130836
#% 1206703
#% 1206763
#% 1210998
#% 1214671
#% 1259854
#% 1386131
#% 1399997
#% 1463608
#% 1524388
#! The proliferation of online social networks, and the concomitant accumulation of user data, give rise to hotly debated issues of privacy, security, and control. One specific challenge is the sharing or public release of anonymized data without accidentally leaking personally identifiable information (PII). Unfortunately, it is often difficult to ascertain that sophisticated statistical techniques, potentially employing additional external data sources, are unable to break anonymity. In this paper, we consider an instance of this problem, where the object of interest is the structure of a social network, i.e., a graph describing users and their links. Recent work demonstrates that anonymizing node identities may not be sufficient to keep the network private: the availability of node and link data from another domain, which is correlated with the anonymized network, has been used to re-identify the anonymized nodes. This paper is about conditions under which such a de-anonymization process is possible. We attempt to shed light on the following question: can we assume that a sufficiently sparse network is inherently anonymous, in the sense that even with unlimited computational power, de-anonymization is impossible? Our approach is to introduce a random graph model for a version of the de-anonymization problem, which is parameterized by the expected node degree and a similarity parameter that controls the correlation between two graphs over the same vertex set. We find simple conditions on these parameters delineating the boundary of privacy, and show that the mean node degree need only grow slightly faster than log n with network size n for nodes to be identifiable. Our results have policy implications for sharing of anonymized network information.

#index 1606067
#* Ontology enhancement and concept granularity learning: keeping yourself current and adaptive
#@ Shan Jiang;Lidong Bing;Bai Sun;Yan Zhang;Wai Lam
#t 2011
#c 0
#% 152968
#% 280817
#% 280819
#% 722904
#% 881552
#% 956564
#% 1002315
#% 1292559
#% 1305622
#% 1318763
#% 1481390
#% 1482417
#% 1707940
#! As a well-known semantic repository, WordNet is widely used in many applications. However, due to costly edit and maintenance, WordNet's capability of keeping up with the emergence of new concepts is poor compared with on-line encyclopedias such as Wikipedia. To keep WordNet current with folk wisdom, we propose a method to enhance WordNet automatically by merging Wikipedia entities into WordNet, and construct an enriched ontology, named as WorkiNet. WorkiNet keeps the desirable structure of WordNet. At the same time, it captures abundant information from Wikipedia. We also propose a learning approach which is able to generate a tailor-made semantic concept collection for a given document collection. The learning process takes the characteristics of the given document collection into consideration and the semantic concepts in the tailor-made collection can be used as new features for document representation. The experimental results show that the adaptively generated feature space can outperform a static one significantly in text mining tasks, and WorkiNet dominates WordNet most of the time due to its high coverage.

#index 1606068
#* Personal privacy vs population privacy: learning to attack anonymization
#@ Graham Cormode
#t 2011
#c 0
#% 576761
#% 864412
#% 893100
#% 904304
#% 977011
#% 1022247
#% 1083631
#% 1083653
#% 1141473
#% 1198224
#% 1214684
#% 1217156
#% 1414540
#% 1426329
#% 1451189
#% 1523888
#% 1581862
#% 1670071
#! Over the last decade great strides have been made in developing techniques to compute functions privately. In particular, Differential Privacy gives strong promises about conclusions that can be drawn about an individual. In contrast, various syntactic methods for providing privacy (criteria such as k-anonymity and l-diversity) have been criticized for still allowing private information of an individual to be inferred. In this paper, we consider the ability of an attacker to use data meeting privacy definitions to build an accurate classifier. We demonstrate that even under Differential Privacy, such classifiers can be used to infer "private" attributes accurately in realistic data. We compare this to similar approaches for inference-based attacks on other forms of anonymized data. We show how the efficacy of all these attacks can be measured on the same scale, based on the probability of successfully inferring a private attribute. We observe that the accuracy of inference of private attributes for differentially private data and $l$-diverse data can be quite similar.

#index 1606069
#* Privacy-preserving social network publication against friendship attacks
#@ Chih-Hua Tai;Philip S. Yu;De-Nian Yang;Ming-Syan Chen
#t 2011
#c 0
#% 248030
#% 576761
#% 956511
#% 1063476
#% 1127360
#% 1200862
#% 1206763
#% 1280746
#% 1281958
#% 1328188
#% 1381029
#% 1415851
#% 1426540
#! Due to the rich information in graph data, the technique for privacy protection in published social networks is still in its infancy, as compared to the protection in relational databases. In this paper we identify a new type of attack called a friendship attack. In a friendship attack, an adversary utilizes the degrees of two vertices connected by an edge to re-identify related victims in a published social network data set. To protect against such attacks, we introduce the concept of k2-degree anonymity, which limits the probability of a vertex being re-identified to 1/k. For the k2-degree anonymization problem, we propose an Integer Programming formulation to find optimal solutions in small-scale networks. We also present an efficient heuristic approach for anonymizing large-scale social networks against friendship attacks. The experimental results demonstrate that the proposed approaches can preserve much of the characteristics of social networks.

#index 1606070
#* Probabilistic topic models with biased propagation on heterogeneous information networks
#@ Hongbo Deng;Jiawei Han;Bo Zhao;Yintao Yu;Cindy Xide Lin
#t 2011
#c 0
#% 132779
#% 268079
#% 277483
#% 280819
#% 290830
#% 643008
#% 722904
#% 769906
#% 799636
#% 818266
#% 875959
#% 879587
#% 1055681
#% 1055743
#% 1083684
#% 1083734
#% 1130899
#% 1166526
#% 1211703
#% 1214645
#% 1214701
#% 1318691
#% 1451205
#% 1560379
#% 1598455
#! With the development of Web applications, textual documents are not only getting richer, but also ubiquitously interconnected with users and other objects in various ways, which brings about text-rich heterogeneous information networks. Topic models have been proposed and shown to be useful for document analysis, and the interactions among multi-typed objects play a key role at disclosing the rich semantics of the network. However, most of topic models only consider the textual information while ignore the network structures or can merely integrate with homogeneous networks. None of them can handle heterogeneous information network well. In this paper, we propose a novel topic model with biased propagation (TMBP) algorithm to directly incorporate heterogeneous information network with topic modeling in a unified way. The underlying intuition is that multi-typed objects should be treated differently along with their inherent textual information and the rich semantics of the heterogeneous information network. A simple and unbiased topic propagation across such a heterogeneous network does not make much sense. Consequently, we investigate and develop two biased propagation frameworks, the biased random walk framework and the biased regularization framework, for the TMBP algorithm from different perspectives, which can discover latent topics and identify clusters of multi-typed objects simultaneously. We extensively evaluate the proposed approach and compare to the state-of-the-art techniques on several datasets. Experimental results demonstrate that the improvement in our proposed approach is consistent and promising.

#index 1606071
#* Prominent streak discovery in sequence data
#@ Xiao Jiang;Chengkai Li;Ping Luo;Min Wang;Yong Yu
#t 2011
#c 0
#% 172949
#% 288976
#% 329537
#% 459006
#% 460862
#% 462231
#% 463903
#% 465167
#% 480671
#% 481609
#% 777338
#% 778732
#% 806212
#% 875011
#% 912241
#% 993954
#% 1206852
#% 1274922
#% 1378458
#% 1703161
#! This paper studies the problem of prominent streak discovery in sequence data. Given a sequence of values, a prominent streak is a long consecutive subsequence consisting of only large (small) values. For finding prominent streaks, we make the observation that prominent streaks are skyline points in two dimensions- streak interval length and minimum value in the interval. Our solution thus hinges upon the idea to separate the two steps in prominent streak discovery' candidate streak generation and skyline operation over candidate streaks. For candidate generation, we propose the concept of local prominent streak (LPS). We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence, in comparison with the quadratic number of candidates produced by a brute-force baseline method. We develop efficient algorithms based on the concept of LPS. The non-linear LPS-based method (NLPS) considers a superset of LPSs as candidates, and the linear LPS-based method (LLPS) further guarantees to consider only LPSs. The results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method.

#index 1606072
#* Protecting location privacy using location semantics
#@ Byoungyoung Lee;Jinoh Oh;Hwanjo Yu;Jong Kim
#t 2011
#c 0
#% 325683
#% 421124
#% 576761
#% 812799
#% 818916
#% 893151
#% 907397
#% 911803
#% 937550
#% 956531
#% 975141
#% 1013611
#% 1055695
#% 1063478
#% 1135166
#% 1147432
#% 1190134
#% 1206712
#% 1206881
#% 1208170
#% 1217302
#% 1245120
#% 1298807
#% 1328196
#% 1400036
#% 1441736
#% 1729021
#! As the use of mobile devices increases, a location-based service (LBS) becomes increasingly popular because it provides more convenient context-aware services. However, LBS introduces problematic issues for location privacy due to the nature of the service. Location privacy protection methods based on k-anonymity and l-diversity have been proposed to provide anonymized use of LBS. However, the k-anonymity and l-diversity methods still can endanger the user's privacy because location semantic information could easily be breached while using LBS. This paper presents a novel location privacy protection technique, which protects the location semantics from an adversary. In our scheme, location semantics are first learned from location data. Then, the trusted-anonymization server performs the anonymization using the location semantic information by cloaking with semantically heterogeneous locations. Thus, the location semantic information is kept secure as the cloaking is done with semantically heterogeneous locations and the true location information is not delivered to the LBS applications. This paper proposes algorithms for learning location semantics and achieving semantically secure cloaking.

#index 1606073
#* Ranking-based classification of heterogeneous information networks
#@ Ming Ji;Jiawei Han;Marina Danilevsky
#t 2011
#c 0
#% 118771
#% 235377
#% 268079
#% 281186
#% 290830
#% 805896
#% 818266
#% 876018
#% 961268
#% 961278
#% 1083698
#% 1214701
#% 1214714
#% 1292715
#% 1305507
#% 1495579
#! It has been recently recognized that heterogeneous information networks composed of multiple types of nodes and links are prevalent in the real world. Both classification and ranking of the nodes (or data objects) in such networks are essential for network analysis. However, so far these approaches have generally been performed separately. In this paper, we combine ranking and classification in order to perform more accurate analysis of a heterogeneous information network. Our intuition is that highly ranked objects within a class should play more important roles in classification. On the other hand, class membership information is important for determining a quality ranking over a dataset. We believe it is therefore beneficial to integrate classification and ranking in a simultaneous, mutually enhancing process, and to this end, propose a novel ranking-based iterative classification framework, called RankClass. Specifically, we build a graph-based ranking model to iteratively compute the ranking distribution of the objects within each class. At each iteration, according to the current ranking results, the graph structure used in the ranking algorithm is adjusted so that the sub-network corresponding to the specific class is emphasized, while the rest of the network is weakened. As our experiments show, integrating ranking with classification not only generates more accurate classes than the state-of-art classification methods on networked data, but also provides meaningful ranking of objects within each class, serving as a more informative view of the data than traditional classification.

#index 1606074
#* Real-time bidding algorithms for performance-based display ad allocation
#@ Ye Chen;Pavel Berkhin;Bo Anderson;Nikhil R. Devanur
#t 2011
#c 0
#% 412400
#% 927472
#% 1055688
#% 1190080
#% 1214642
#% 1336429
#% 1426655
#% 1434948
#% 1451160
#% 1656756
#! We describe a real-time bidding algorithm for performance-based display ad allocation. A central issue in performance display advertising is matching campaigns to ad impressions, which can be formulated as a constrained optimization problem that maximizes revenue subject to constraints such as budget limits and inventory availability. The current practice is to solve the optimization problem offline at a tractable level of impression granularity (e.g., the page level), and to serve ads online based on the precomputed static delivery scheme. Although this offline approach takes a global view to achieve optimality, it fails to scale to ad allocation at the individual impression level. Therefore, we propose a real-time bidding algorithm that enables fine-grained impression valuation (e.g., targeting users with real-time conversion data), and adjusts value-based bids according to real-time constraint snapshots (e.g., budget consumption levels). Theoretically, we show that under a linear programming (LP) primal-dual formulation, the simple real-time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem as input. In other words, the online algorithm guarantees the offline optimality given the same level of knowledge an offline optimization would have. Empirically, we develop and experiment with two real-time bid adjustment approaches to adapting to the non-stationary nature of the marketplace: one adjusts bids against real-time constraint satisfaction levels using control-theoretic methods, and the other adjusts bids also based on the statistically modeled historical bidding landscape. Finally, we show experimental results with real-world ad delivery data that support our theoretical conclusions.

#index 1606075
#* Revisiting sequential pattern hiding to enhance utility
#@ Aris Gkoulalas-Divanis;Grigorios Loukides
#t 2011
#c 0
#% 256685
#% 463903
#% 464996
#% 577256
#% 586838
#% 727815
#% 740764
#% 844360
#% 907563
#% 958757
#% 1046207
#% 1066737
#% 1074831
#% 1127361
#% 1207156
#% 1233857
#% 1262947
#% 1447131
#% 1512988
#% 1523887
#% 1537150
#% 1698994
#! Sequence datasets are encountered in a plethora of applications spanning from web usage analysis to healthcare studies and ubiquitous computing. Disseminating such datasets offers remarkable opportunities for discovering interesting knowledge patterns, but may lead to serious privacy violations if sensitive patterns, such as business secrets, are disclosed. In this work, we consider how to sanitize data to prevent the disclosure of sensitive patterns during sequential pattern mining, while ensuring that the nonsensitive patterns can still be discovered. First, we re-define the problem of sequential pattern hiding to capture the information loss incurred by sanitization in terms of both events' modification (distortion) and lost nonsensitive knowledge patterns (side-effects). Second, we model sequences as graphs and propose two algorithms to solve the problem by operating on the graphs. The first algorithm attempts to sanitize data with minimal distortion, whereas the second focuses on reducing the side-effects. Extensive experiments show that our algorithms outperform the existing solution in terms of data distortion and side-effects and are more efficient.

#index 1606076
#* Sampling hidden objects using nearest-neighbor oracles
#@ Nilesh Dalvi;Ravi Kumar;Ashwin Machanavajjhala;Vibhor Rastogi
#t 2011
#c 0
#% 90740
#% 268114
#% 298221
#% 309748
#% 340146
#% 480328
#% 807320
#% 877912
#% 907547
#% 960286
#% 1091268
#% 1131022
#% 1206906
#% 1387553
#% 1426573
#% 1433109
#% 1535432
#! Given an unknown set of objects embedded in the Euclidean plane and a nearest-neighbor oracle, how to estimate the set size and other properties of the objects? In this paper we address this problem. We propose an efficient method that uses the Voronoi partitioning of the space by the objects and a nearest-neighbor oracle. Our method can be used in the hidden web/databases context where the goal is to estimate the number of certain objects of interest. Here, we assume that each object has a geographic location and the nearest-neighbor oracle can be realized by applications such as maps, local, or store-locator APIs. We illustrate the performance of our method on several real-world datasets.

#index 1606077
#* Scalable kNN search on vertically stored time series
#@ Shrikant Kashyap;Panagiotis Karras
#t 2011
#c 0
#% 86950
#% 172949
#% 201876
#% 214595
#% 227924
#% 227939
#% 237204
#% 248796
#% 248797
#% 248798
#% 249321
#% 273903
#% 287466
#% 342828
#% 397376
#% 399763
#% 427199
#% 435141
#% 460862
#% 478460
#% 479462
#% 479649
#% 479973
#% 480093
#% 480133
#% 480146
#% 480632
#% 481947
#% 481956
#% 717452
#% 765403
#% 765451
#% 796201
#% 810049
#% 814646
#% 824686
#% 992857
#% 1022238
#% 1044456
#% 1083693
#% 1127351
#% 1127397
#% 1127609
#% 1177867
#% 1217189
#% 1264692
#% 1378172
#! Nearest-neighbor search over time series has received vast research attention as a basic data mining task. Still, none of the hitherto proposed methods scales well with increasing time-series length. This is due to the fact that all methods provide an one-off pruning capacity only. In particular, traditional methods utilize an index to search in a reduced-dimensionality feature space; however, for high time-series length, search with such an index yields many false hits that need to be eliminated by accessing the full records. An attempt to reduce false hits by indexing more features exacerbates the curse of dimensionality, and vice versa. A recently proposed alternative, iSAX, uses symbolic approximate representations accessed by a simple file-system directory as an index. Still, iSAX also encounters false hits, which are again eliminated by accessing records in full: once a false hit is generated by the index, there is no second chance to prune it; thus, the pruning capacity iSAX provides is also one-off. This paper proposes an alternative approach to time series kNN search, following a nontraditional pruning style. Instead of navigating through candidate records via an index, we access their features, obtained by a multi-resolution transform, in a stepwise sequential-scan manner, one level of resolution at a time, over a vertical representation. Most candidates are progressively eliminated after a few of their terms are accessed, using pre-computed information and an unprecedentedly tight double-bounding scheme, involving not only lower, but also upper distance bounds. Our experimental study with large, high-length time-series data confirms the advantage of our approach over both the current state-of-the-art method, iSAX, and classical index-based methods.

#index 1606078
#* Serendipitous learning: learning beyond the predefined label space
#@ Dan Zhang;Yan Liu;Luo Si
#t 2011
#c 0
#% 304876
#% 350859
#% 397147
#% 466263
#% 577297
#% 729437
#% 763708
#% 1269502
#% 1302114
#% 1386108
#% 1451222
#% 1451256
#% 1674795
#% 1862068
#! Most traditional supervised learning methods are developed to learn a model from labeled examples and use this model to classify the unlabeled ones into the same label space predefined by the models. However, in many real world applications, the label spaces for both the labeled/training and unlabeled/testing examples can be different. To solve this problem, this paper proposes a novel notion of Serendipitous Learning (SL), which is defined to address the learning scenarios in which the label space can be enlarged during the testing phase. In particular, a large margin approach is proposed to solve SL. The basic idea is to leverage the knowledge in the labeled examples to help identify novel/unknown classes, and the large margin formulation is proposed to incorporate both the classification loss on the examples within the known categories, as well as the clustering loss on the examples in unknown categories. An efficient optimization algorithm based on CCCP and the bundle method is proposed to solve the optimization problem of the large margin formulation of SL. Moreover, an efficient online learning method is proposed to address the issue of large scale data in online learning scenario, which has been shown to have a guaranteed learning regret. An extensive set of experimental results on two synthetic datasets and two datasets from real world applications demonstrate the advantages of the proposed method over several other baseline algorithms. One limitation of the proposed method is that the number of unknown classes is given in advance. It may be possible to remove this constraint if we model it by using a non-parametric way. We also plan to do experiments on more real world applications in the future.

#index 1606079
#* Spatially regularized logistic regression for disease mapping on large moving populations
#@ Vuk Malbasa;Slobodan Vucetic
#t 2011
#c 0
#% 576761
#% 769901
#% 1073973
#% 1080161
#% 1170188
#! Spatial analysis of disease risk, or disease mapping, typically relies on information about the residence and health status of individuals from population under study. However, residence information has its limitations because people are exposed to numerous disease risks as they spend time outside of their residences. Thanks to the wide-spread use of mobile phones and GPS-enabled devices, it is becoming possible to obtain a detailed record about the movement of human populations. Availability of movement information opens up an opportunity to improve the accuracy of disease mapping. Starting with an assumption that an individual's disease risk is a weighted average of risks at the locations which were visited, we show that disease mapping can be accomplished by spatially regularized logistic regression. Due to the inherent sparsity of movement data, the proposed approach can be applied to large populations and over large spatial grids. In our experiments, we were able to map disease for a simulated population with 1.6 million people and a spatial grid with 65 thousand locations in several minutes. The results indicate that movement information can improve the accuracy of disease mapping as compared to residential data only. We also studied a privacy-preserving scenario in which only the aggregate statistics are available about the movement of the overall population, while detailed movement information is available only for individuals with disease. The results indicate that the accuracy of disease mapping remains satisfactory when learning from movement data sanitized in this way.

#index 1606080
#* Temporal multi-hierarchy smoothing for estimating rates of rare events
#@ Nagaraj Kota;Deepak Agarwal
#t 2011
#c 0
#% 342597
#% 342611
#% 425067
#% 577264
#% 729437
#% 956521
#% 963669
#% 983835
#% 989572
#% 989647
#% 1039685
#% 1074344
#% 1127449
#% 1211829
#% 1214642
#% 1328061
#% 1451160
#! We consider the problem of estimating rates of rare events obtained through interactions among several categorical variables that are heavy-tailed and hierarchical. In our previous work, we proposed a scalable log-linear model called LMMH (Log-Linear Models for Multiple Hierarchies) that combats data sparsity at granular levels through small sample size corrections that borrow strength from rate estimates at coarser resolutions. This paper extends our previous work in two directions. First, we model excess heterogeneity by fitting local LMMH models to relatively homogeneous subsets of the data. To ensure scalable computation, these subsets are induced through a decision tree, we call this Treed-LMMH. Second, the Treed-LMMH method is coupled with temporal smoothing procedure based on a fast Kalman filter style algorithm. We show that simultaneously performing hierarchical and temporal smoothing leads to significant improvement in predictive accuracy. Our methods are illustrated on a large scale computational advertising dataset consisting of billions of observations and hundreds of millions of attribute combinations(cells).

#index 1606081
#* ThermoCast: a cyber-physical forecasting model for datacenters
#@ Lei Li;Chieh-Jan Mike Liang;Jie Liu;Suman Nath;Andreas Terzis;Christos Faloutsos
#t 2011
#c 0
#% 13453
#% 172949
#% 333941
#% 765452
#% 810030
#% 896764
#% 960127
#% 963640
#% 1053136
#% 1072067
#% 1089193
#% 1116362
#% 1142424
#% 1206639
#% 1214672
#% 1214752
#% 1290941
#% 1307193
#% 1468407
#% 1523829
#! Efficient thermal management is important in modern data centers as cooling consumes up to 50% of the total energy. Unlike previous work, we consider proactive thermal management, whereby servers can predict potential overheating events due to dynamics in data center configuration and workload, giving operators enough time to react. However, such forecasting is very challenging due to data center scales and complexity. Moreover, such a physical system is influenced by cyber effects, including workload scheduling in servers. We propose ThermoCast, a novel thermal forecasting model to predict the temperatures surrounding the servers in a data center, based on continuous streams of temperature and airflow measurements. Our approach is (a) capable of capturing cyberphysical interactions and automatically learning them from data; (b) computationally and physically scalable to data center scales; (c) able to provide online prediction with real-time sensor measurements. The paper's main contributions are: (i) We provide a systematic approach to integrate physical laws and sensor observations in a data center; (ii) We provide an algorithm that uses sensor data to learn the parameters of a data center's cyber-physical system. In turn, this ability enables us to reduce model complexity compared to full-fledged fluid dynamics models, while maintaining forecast accuracy; (iii) Unlike previous simulation-based studies, we perform experiments in a production data center. Using real data traces, we show that ThermoCast forecasts temperature better than a machine learning approach solely driven by data, and can successfully predict thermal alarms 4.2 minutes ahead of time.

#index 1606082
#* Towards bounding sequential patterns
#@ Chedy Raïssi;Jian Pei
#t 2011
#c 0
#% 152934
#% 329537
#% 459006
#% 463903
#% 464996
#% 466487
#% 477791
#% 576118
#% 577256
#% 589384
#% 763551
#% 789589
#% 805094
#% 839173
#% 864470
#% 925598
#% 949146
#% 1035590
#% 1063518
#% 1172640
#% 1214761
#% 1440242
#% 1738861
#! Given a sequence database, can we have a non-trivial upper bound on the number of sequential patterns? The problem of bounding sequential patterns is very challenging in theory due to the combinatorial complexity of sequences, even given some inspiring results on bounding itemsets in frequent itemset mining. Moreover, the problem is highly meaningful in practice, since the upper bound can be used in many applications such as space allocation in building sequence data warehouses. In this paper, we tackle the problem of bounding sequential patterns by presenting, for the first time in the field of sequential pattern mining, strong combinatorial results on computing the number of possible sequential patterns that can be generated at a given length k. We introduce, as a case study, two novel techniques to estimate the number of candidate sequences. An extensive empirical study on both real data and synthetic data verifies the effectiveness of our methods.

#index 1606083
#* User-click modeling for understanding and predicting search-behavior
#@ Yuchen Zhang;Weizhu Chen;Dong Wang;Qiang Yang
#t 2011
#c 0
#% 309095
#% 766472
#% 818207
#% 840846
#% 956546
#% 1035578
#% 1074092
#% 1083721
#% 1166521
#% 1190055
#% 1190056
#% 1214675
#% 1227577
#% 1355034
#% 1355048
#% 1450885
#% 1451161
#% 1482222
#% 1482279
#% 1560356
#! Recent advances in search users' click modeling consider both users' search queries and click/skip behavior on documents to infer the user's perceived relevance. Most of these models, including dynamic Bayesian networks (DBN) and user browsing models (UBM), use probabilistic models to understand user click behavior based on individual queries. The user behavior is more complex when her actions to satisfy her information needs form a search session, which may include multiple queries and subsequent click behaviors on various items on search result pages. Previous research is limited to treating each query within a search session in isolation, without paying attention to their dynamic interactions with other queries in a search session. Investigating this problem, we consider the sequence of queries and their clicks in a search session as a task and propose a task-centric click model~(TCM). TCM characterizes user behavior related to a task as a collective whole. Specifically, we identify and consider two new biases in TCM as the basis for user modeling. The first indicates that users tend to express their information needs incrementally in a task, and thus perform more clicks as their needs become clearer. The other illustrates that users tend to click fresh documents that are not included in the results of previous queries. Using these biases, TCM is more accurately able to capture user search behavior. Extensive experimental results demonstrate that by considering all the task information collectively, TCM can better interpret user click behavior and achieve significant improvements in terms of ranking metrics of NDCG and perplexity.

#index 1606084
#* User-level sentiment analysis incorporating social networks
#@ Chenhao Tan;Lillian Lee;Jie Tang;Long Jiang;Ming Zhou;Ping Li
#t 2011
#c 0
#% 577356
#% 938687
#% 939346
#% 1127964
#% 1176920
#% 1261563
#% 1277969
#% 1299754
#% 1310464
#% 1346155
#% 1470673
#% 1535408
#% 1544009
#% 1544032
#% 1591944
#! We show that information about social relationships can be used to improve user-level sentiment analysis. The main motivation behind our approach is that users that are somehow "connected" may be more likely to hold similar opinions; therefore, relationship information can complement what we can extract about a user's viewpoints from their utterances. Employing Twitter as a source for our experimental data, and working within a semi-supervised framework, we propose models that are induced either from the Twitter follower/followee network or from the network in Twitter formed by users referring to each other using "@" mentions. Our transductive learning results reveal that incorporating social-network information can indeed lead to statistically significant sentiment classification improvements over the performance of an approach based on Support Vector Machines having access only to textual features.

#index 1606085
#* Web information extraction using markov logic networks
#@ Sandeepkumar Satpal;Sahely Bhadra;Sundararajan Sellamanickam;Rajeev Rastogi;Prithviraj Sen
#t 2011
#c 0
#% 333943
#% 431536
#% 480824
#% 722903
#% 769877
#% 805846
#% 850430
#% 881505
#% 889107
#% 1166537
#% 1190073
#% 1190153
#% 1250584
#% 1270258
#% 1328133
#! In this paper, we consider the problem of extracting structured data from web pages taking into account both the content of individual attributes as well as the structure of pages and sites. We use Markov Logic Networks (MLNs) to capture both content and structural features in a single unified framework, and this enables us to perform more accurate inference. MLNs allow us to model a wide range of rich structural features like proximity, precedence, alignment, and contiguity, using first-order clauses. We show that inference in our information extraction scenario reduces to solving an instance of the maximum weight subgraph problem. We develop specialized procedures for solving the maximum subgraph variants that are far more efficient than previously proposed inference methods for MLNs that solve variants of MAX-SAT. Experiments with real-life datasets demonstrate the effectiveness of our MLN-based approach compared to existing state-of-the-art extraction methods.

#index 1608547
#* Proceedings of the 2011 workshop on Knowledge discovery, modeling and simulation
#@ Chris Augeri;Amy Henninger;David Jensen;Amy McGovern;David Pratt;William Rand;Julie Rosen;Ashok Srivastava
#t 2011
#c 0

#index 1608557
#* Proceedings of the 2011 workshop on Data mining for medicine and healthcare
#@ Nitesh Chawla;Rayid Ghani;Jianying Hu;Balaji Krishnapuram;Mohit Kumar;David Madigan;Jonathan Silverstein;Jimeng Sun;K. P. Unnikrishnan;Ramasamy Uthurusamy;Fei Wang;John Younger
#t 2011
#c 0
#! Healthcare systems around the world are struggling to keep up with patient needs, and improve quality of care while reducing costs at the same time. At the same time, more and more data is being captured around healthcare processes in the form of Electronic Health Records (EHR), health insurance claims, medical imaging databases, disease registries, spontaneous reporting sites, and clinical trials. As this data gets collected, government regulations are requiring healthcare providers to not only store it in an electronic format but also use it in meaningful ways. Using this data in an effective way to improve quality of care and reduce costs requires innovation in data mining as well as academic, industry and government partnerships. The goals of this workshop are to: Bring together the KDD community and the medical researchers & practitioners to discuss and explore mutual benefits of applying KDD to the right medical challenges and to collaborate on identifying and developing promising new techniques and methodologies. Bring together researchers (from both academia and industry) as well as practitioners from all three different groups in medicine and healthcare (payers, providers, and pharmaceuticals) to talk about their different perspectives and to share their latest problems and ideas. Attract healthcare professionals who have access to interesting sources of data and problems but not the expertise in data mining to solve them effectively. This group would otherwise not attend KDD and we believe through our personal experiences that it is essential for KDD research community to interact with them. This workshop serves as a bridge between the traditional KDD community and professionals in medicine and healthcare - two groups of participants that have a lot to learn from and share with each other. We aim to emphasize the following aspects: Addressing the fundamental challenges in improving healthcare and how data mining technologies will help Presenting recent advances in data mining algorithms and methods for healthcare transformation Identifying the next step of healthcare solutions and the possible data driven solutions Fostering interactions and collaborations among researchers and practitioners (from different backgrounds), and healthcare professionals, to promote cross-fertilization of ideas. Exploring unified platforms and data for better evaluation of the techniques Deployed healthcare applications of data mining New classes of research problems motivated by real-world business problems Data mining applications as components of healthcare business processes How data mining is useful for various participants in the healthcare system Providers (hospitals, labs, clinics) Payers (Insurance companies) Pharmaceuticals Topic areas for the workshop include (but are not limited to) the following: Statistical analysis and characterization of healthcare data Meaningful use of healthcare data for improved patient care and cost-reduction Data quality assessment and improvement: preprocessing, cleaning, missing data treatment etc. Pattern detection and hypothesis generation from observational data Comparative effectiveness research Medical information retrieval Cloud-computing models and scalability Privacy and security issues in healthcare Information visualization for healthcare data Information fusion and knowledge transfer in healthcare Evolutionary and longitudinal patient and disease models Mining knowledge from medical imaging data Medical fraud detection Clinical decision support Bio-surveillance Intelligent payment models Collaborative care delivery models Post-market surveillance of medical interventions Text mining - mining free text in electronic medical records Help with ICD 9 to ICD 10 conversions Improving Clinical trial management and design Pay for performance models in healthcare Feasibility of Health Information exchanges

#index 1608571
#* Proceedings of the 2011 workshop on Predictive markup language modeling
#@ Rick Pechter;Robert Grossman;Christoph Lingenfelder;Ashok Savasere;Michael Zeller
#t 2011
#c 0
#! Over the past decade, the Predictive Model Mark-up Language (PMML) has emerged as the de facto standard for predictive model specification that is vendor neutral, application agnostic and a true enabler for cross-platform interoperability. As PMML has matured, a broader variety of applications and deployment mechanisms are being used. These create opportunities for future growth but they also create challenges for the standard to remain flexible enough to meet the demands of new users and new solutions. Related fields such as data grids, web services, and the semantic web have also developed standards-based infrastructures and services relevant to KDD. These standards and standards-based services and platforms have the potential to change the way data mining is being used. Based on the structure of similar standards workshops that were part of past KDD conferences, this workshop aims to bring together practitioners from industry and academia to discuss various applications and foster a dialog about data mining models, model interoperability, data transformation and post-processing workflows and related topics. In addition, the workshop will include talks on future requirements and the future direction for data mining services and platforms. As part of the KDD-2011 conference, we hope to expose the broader community to this area, and conversely, to collect requirements for new versions of the PMML standard. Included in this year's workshop are eight papers that cover a variety of topics related to PMML, not only how it is used, but also how it can be extended while remaining the lingua franca for predictive models. The workshop will also include a panel discussion on PMML where participants address questions and debate future directions for the standard. As predictive analytics and data mining continue to gain momentum in industry and business applications, open standards like PMML provide an essential foundation to share predictive models across vendors and to disseminate model-related information within organizations. It is only through the contributions of many people from industry and academia that PMML standard continues to grow in adoption and evolve in capability. The workshop organizers would like to thank all those who give generously of their skills and expertise in order to make PMML the standard that it is today.

#index 1620896
#* Lessons learned from contests in data mining
#@ Charles Elkan;Jeremy Howard;Yehuda Koren;Tie-Yan Liu;Claudia Perlich
#t 2011
#c 0

#index 1679100
#* Proceedings of the 17th ACM SIGKDD International Conference Tutorials
#@ 
#t 2011
#c 0

#index 1790755
#* Proceedings of the Eleventh International Workshop on Multimedia Data Mining
#@ Aaron Baughman;Jiang (John) Gao;Jia-Yu Pan
#t 2011
#c 0

#index 1865339
#* Proceedings of the ACM SIGKDD Workshop on Intelligence and Security Informatics
#@ Christopher C. Yang;Hsinchun Chen;Wenji Mao;Fei-Yue Wang
#t 2012
#c 0

#index 1872224
#* Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Qiang Yang;Deepak Agarwal;Jian Pei
#t 2012
#c 0
#! The KDD conference has seen remarkable growth since its origins as an IJCAI workshop in Detroit in 1989, evolving into a full-fledged research conference in 1995, underscoring the important role data mining as a field has played in extracting knowledge and actionable insights from vast troves of data that is being generated in the digital world around us. This year we received a record 755 submissions to the research program, from which 133 papers were accepted, for an aggregate acceptance rate of 17.6% (quite similar to recent years). Among the academic conferences, the KDD conference has typically more of an emphasis on research motivated by real-world applications. It is important to keep in mind that it is this synergy of research in areas like algorithms, computational geometry, database, graph theory, machine learning, natural language processing, statistics, visualization and many others when applied to problems arising in diverse fields such as web, medicine, climatology, marketing that drives our field forward, makes it vibrant and fun - who would know that ideas in computational geometry can be adapted to construct fast algorithms to improve online advertising and movie recommendations? The breadth of topics covered in this year's research program is truly comprehensive, including social networks, privacy, text mining, predictive modeling, time-series forecasting, spatial data analysis, geometry, and more. We are very fortunate to have 4 world-class keynote speakers this year spanning industry and academia, providing inspirational talks on cutting-edge techniques and issues in web mining, information networks, statistical inference for big data, and social computing. The process of whittling down the initial 734 submissions to the final set of 133 accepted papers required the coordination and time of a large number of willing volunteers. The program committee (PC) consisted of over 350 reviewers (PC members) and 50 senior PC members. In the first phase each submitted paper was automatically assigned to 3 reviewers (after a bidding process). Once the reviews from each of the 3 reviewers were completed, the program chairs rejected papers that did not receive much support from any of the reviewers. We rejected 259 papers at this stage. Special care was taken to minimize the error of rejecting a potentially good paper at this stage. The papers that survived the first phase were assigned to the senior PC members based on their bids, they had the option of initiating a discussion for any of their papers, e.g., if there was significant divergence in scores among reviewers, or if a paper was on the borderline of being accepted. Following the discussion phase, the senior PC members provided a recommendation score and a detailed meta-review for each paper. In the final phase, we (the program chairs) analyzed all of this information, starting with the obvious accept and reject decisions, and then gradually focusing in more detail on the papers near the borderline, seeking additional reviews and input from the PC and senior PC members where appropriate. We also initiated a shepherding phase with 15 papers having the opportunity of fixing mild issues we thought would be possible to address before they can be accepted. 13 of them were accepted after thorough revisions. Finally, it is quite likely that in hindsight some worthy papers may have been rejected as part of this process - these errors are an unfortunate reality of modern computer science conferences, and hard to avoid when a very large number of decisions have to be made over a short time span based on a subjective reviewing process. Nevertheless, we, the PC chairs, are responsible for those unfortunate errors and welcome suggestions on the matter.

#index 1872225
#* Nine real hard problems we'd like you to solve
#@ Robin Li
#t 2012
#c 0
#! Since 2000, Baidu has set its mission as providing the best way for people to find what they're looking for. Today, the company has become the world's largest Chinese search engine. Everyday, we process billions of search queries and serve hundreds of millions of internet users. The huge volume of online text and multimedia content, as well as user log data, provide us unprecedented opportunities and challenges for further accomplishing our mission. In addition, we have seen several megatrends, cloud computing is becoming a pervasive service infrastructure, mobile internet will surpass traditional internet in user time spend, and social platform has demonstrated its great power. In response to all these opportunities, challenges, and megatrends, we must think ahead on the major technology focuses that may help Baidu to serve our users better. In this talk, I would like to share with the audience the nine areas that are the most important and interesting in my mind. For each of the areas, I will describe the challenges and explain why it is important and interesting. I hope the research community can get excited, and help us provide better services for users.

#index 1872226
#* Mining heterogeneous information networks: the next frontier
#@ Jaiwei Han
#t 2012
#c 0
#% 1181261
#% 1214701
#% 1451159
#% 1495579
#% 1606073
#% 1635098
#% 1707456
#% 1898007
#! Real world physical and abstract data objects are interconnected, forming gigantic, interconnected networks. By structuring these data objects into multiple types, such networks become semi-structured heterogeneous information networks. Most real world applications that handle big data, including interconnected social media and social networks, scientific, engineering, or medical information systems, online e-commerce systems, and most database systems, can be structured into heterogeneous information networks. For example, in a medical care network, objects of multiple types, such as patients, doctors, diseases, medication, and links such as visits, diagnosis, and treatments are intertwined together, providing rich information and forming heterogeneous information networks. Effective analysis of large-scale heterogeneous information networks poses an interesting but critical challenge. In this talk, we present a set of data mining scenarios in heterogeneous information networks and show that mining heterogeneous information networks is a new and promising research frontier in data mining research. Departing from many existing network models that view data as homogeneous graphs or networks, the semi-structured heterogeneous information network model leverages the rich semantics of typed nodes and links in a network and can uncover surprisingly rich knowledge from interconnected data. This heterogeneous network modeling will lead to the discovery of a set of new principles and methodologies for mining interconnected data. The examples to be used in this discussion include (1) meta path-based similarity search, (2) rank-based clustering, (3) rank-based classification, (4) meta path-based link/relationship prediction, (5) relation strength-aware mining, as well as a few other recent developments. We will also point out some promising research directions and provide convincing arguments on that mining heterogeneous information networks is the next frontier in data mining.

#index 1872227
#* Divide-and-conquer and statistical inference for big data
#@ Michael I. Jordan
#t 2012
#c 0
#! I present some recent work on statistical inference for Big Data. Divide-and-conquer is a natural computational paradigm for approaching Big Data problems, particularly given recent developments in distributed and parallel computing, but some interesting challenges arise when applying divide-and-conquer algorithms to statistical inference problems. One interesting issue is that of obtaining confidence intervals in massive datasets. The bootstrap principle suggests resampling data to obtain fluctuations in the values of estimators, and thereby confidence intervals, but this is infeasible with massive data. Subsampling the data yields fluctuations on the wrong scale, which have to be corrected to provide calibrated statistical inferences. I present a new procedure, the "bag of little bootstraps," which circumvents this problem, inheriting the favorable theoretical properties of the bootstrap but also having a much more favorable computational profile. Another issue that I discuss is the problem of large-scale matrix completion. Here divide-and-conquer is a natural heuristic that works well in practice, but new theoretical problems arise when attempting to characterize the statistical performance of divide-and-conquer algorithms. Here the theoretical support is provided by concentration theorems for random matrices, and I present a new approach to this problem based on Stein's method1.

#index 1872228
#* Experiments in social computation: (and the data they generate)
#@ Michael Kearns
#t 2012
#c 0
#! For a number of years we have been conducting controlled human-subject experiments in distributed social computation in networks with only limited and local communication. These experiments cast a number of traditional computational, economic and sociological problems (including graph coloring, consensus, independent set, networked bargaining, biased voting and network formation) as games of strategic interaction in which subjects have financial incentives to collectively "compute" global solutions. I will overview and summarize the many behavioral findings from this line of experimentation. I will give particular emphasis to the novel data the experiments have generated, and the analyses this data has permitted, including quantitative studies of subject "personality" traits such as stubbornness, altruism, and patience, and whether those traits seem helpful or harmful to individual and collective performance.

#index 1872229
#* Rise and fall patterns of information diffusion: model and implications
#@ Yasuko Matsubara;Yasushi Sakurai;B. Aditya Prakash;Lei Li;Christos Faloutsos
#t 2012
#c 0
#% 172949
#% 394984
#% 413548
#% 480482
#% 480628
#% 577220
#% 729923
#% 729943
#% 754098
#% 765402
#% 769922
#% 794513
#% 810058
#% 949164
#% 1016194
#% 1083732
#% 1214671
#% 1214672
#% 1318665
#% 1451195
#% 1451246
#% 1535333
#% 1535470
#% 1536522
#% 1590537
#% 1606081
#% 1688538
#% 1746901
#% 1865569
#! The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated faster propagation of news and rumors. How quickly does a piece of news spread over these media? How does its popularity diminish over time? Does the rising and falling pattern follow a simple universal law? In this paper, we propose SpikeM, a concise yet flexible analytical model for the rise and fall patterns of influence propagation. Our model has the following advantages: (a) unification power: it generalizes and explains earlier theoretical models and empirical observations; (b) practicality: it matches the observed behavior of diverse sets of real data; (c) parsimony: it requires only a handful of parameters; and (d) usefulness: it enables further analytics tasks such as fore- casting, spotting anomalies, and interpretation by reverse- engineering the system parameters of interest (e.g. quality of news, count of interested bloggers, etc.). Using SpikeM, we analyzed 7.2GB of real data, most of which were collected from the public domain. We have shown that our SpikeM model accurately and succinctly describes all the patterns of the rise-and-fall spikes in these real datasets.

#index 1872230
#* Efficient personalized pagerank with accuracy assurance
#@ Yasuhiro Fujiwara;Makoto Nakatsuji;Takeshi Yamamuro;Hiroaki Shiokawa;Makoto Onizuka
#t 2012
#c 0
#% 258598
#% 280852
#% 577273
#% 577329
#% 722904
#% 729936
#% 730089
#% 783528
#% 805897
#% 844334
#% 871315
#% 898311
#% 915344
#% 975021
#% 1016176
#% 1055877
#% 1074231
#% 1083652
#% 1085164
#% 1265149
#% 1275197
#% 1482233
#% 1482413
#% 1531275
#% 1536522
#% 1541777
#% 1588228
#% 1605922
#% 1605923
#% 1607321
#% 1707460
#! Personalize PageRank (PPR) is an effective relevance (proximity) measure in graph mining. The goal of this paper is to efficiently compute single node relevance and top-k/highly relevant nodes without iteratively computing the relevances of all nodes. Based on a "random surfer model", PPR iteratively computes the relevances of all nodes in a graph until convergence for a given user preference distribution. The problem with this iterative approach is that it cannot compute the relevance of just one or a few nodes. The heart of our solution is to compute single node relevance accurately in non-iterative manner based on sparse matrix representation, and to compute top-k/highly relevant nodes exactly by pruning unnecessary relevance computations based on upper/lower relevance estimations. Our experiments show that our approach is up to seven orders of magnitude faster than the existing alternatives.

#index 1872231
#* PageRank on an evolving graph
#@ Bahman Bahmani;Ravi Kumar;Mohammad Mahdian;Eli Upfal
#t 2012
#c 0
#% 348137
#% 577328
#% 769460
#% 783528
#% 805879
#% 805897
#% 853940
#% 881467
#% 917283
#% 956536
#% 983330
#% 1035570
#% 1055715
#% 1063716
#% 1166536
#% 1231060
#% 1291642
#% 1369418
#% 1531275
#% 1581927
#% 1663928
#! One of the most important features of the Web graph and social networks is that they are constantly evolving. The classical computational paradigm, which assumes a fixed data set as an input to an algorithm that terminates, is inadequate for such settings. In this paper we study the problem of computing PageRank on an evolving graph. We propose an algorithm that, at any moment in the time and by crawling a small portion of the graph, provides an estimate of the PageRank that is close to the true PageRank of the graph at that moment. We will also evaluate our algorithm experimentally on real data sets and on randomly generated inputs. Under a stylized model of graph evolution, we show that our algorithm achieves a provable performance guarantee that is significantly better than the naive algorithm that crawls the nodes in a round-robin fashion.

#index 1872232
#* Information diffusion and external influence in networks
#@ Seth A. Myers;Chenguang Zhu;Jure Leskovec
#t 2012
#c 0
#% 324817
#% 729923
#% 832271
#% 1083624
#% 1214671
#% 1399992
#% 1536509
#% 1560425
#% 1560426
#! Social networks play a fundamental role in the diffusion of information. However, there are two different ways of how information reaches a person in a network. Information reaches us through connections in our social networks, as well as through the influence external out-of-network sources, like the mainstream media. While most present models of information adoption in networks assume information only passes from a node to node via the edges of the underlying network, the recent availability of massive online social media data allows us to study this process in more detail. We present a model in which information can reach a node via the links of the social network or through the influence of external sources. We then develop an efficient model parameter fitting technique and apply the model to the emergence of URL mentions in the Twitter network. Using a complete one month trace of Twitter we study how information reaches the nodes of the network. We quantify the external influences over time and describe how these influences affect the information adoption. We discover that the information tends to "jump" across the network, which can only be explained as an effect of an unobservable external influence on the network. We find that only about 71% of the information volume in Twitter can be attributed to network diffusion, and the remaining 29% is due to external events and factors outside the network.

#index 1872233
#* The missing models: a data-driven approach for learning how networks grow
#@ Robert Patro;Geet Duggal;Emre Sefer;Hao Wang;Darya Filippova;Carl Kingsford
#t 2012
#c 0
#% 823342
#% 881523
#% 1066469
#% 1214701
#% 1263959
#% 1300556
#% 1386131
#% 1556687
#% 1777209
#! Probabilistic models of network growth have been extensively studied as idealized representations of network evolution. Models, such as the Kronecker model, duplication-based models, and preferential attachment models, have been used for tasks such as representing null models, detecting anomalies, algorithm testing, and developing an understanding of various mechanistic growth processes. However, developing a new growth model to fit observed properties of a network is a difficult task, and as new networks are studied, new models must constantly be developed. Here, we present a framework, called GrowCode, for the automatic discovery of novel growth models that match user-specified topological features in undirected graphs. GrowCode introduces a set of basic commands that are general enough to encode several previously developed models. Coupling this formal representation with an optimization approach, we show that GrowCode is able to discover models for protein interaction networks, autonomous systems networks, and scientific collaboration networks that closely match properties such as the degree distribution, the clustering coefficient, and assortativity that are observed in real networks of these classes. Additional tests on simulated networks show that the models learned by GrowCode generate distributions of graphs with similar variance as existing models for these classes.

#index 1872234
#* Finding minimum representative pattern sets
#@ Guimei Liu;Haojun Zhang;Limsoon Wong
#t 2012
#c 0
#% 152934
#% 248791
#% 322412
#% 333877
#% 431033
#% 464873
#% 478770
#% 536291
#% 729933
#% 769876
#% 800181
#% 823356
#% 824710
#% 826265
#% 881500
#% 881542
#% 941039
#% 1083668
#% 1214690
#% 1605977
#! Frequent pattern mining often produces an enormous number of frequent patterns, which imposes a great challenge on understanding and further analysis of the generated patterns. This calls for finding a small number of representative patterns to best approximate all other patterns. An ideal approach should 1) produce a minimum number of representative patterns; 2) restore the support of all patterns with error guarantee; and 3) have good efficiency. Few existing approaches can satisfy all the three requirements. In this paper, we develop two algorithms, MinRPset and FlexRPset, for finding minimum representative pattern sets. Both algorithms provide error guarantee. MinRPset produces the smallest solution that we can possibly have in practice under the given problem setting, and it takes a reasonable amount of time to finish. FlexRPset is developed based on MinRPset. It provides one extra parameter K to allow users to make a trade-off between result size and efficiency. Our experiment results show that MinRPset and FlexRPset produce fewer representative patterns than RPlocal---an efficient algorithm that is developed for solving the same problem. FlexRPset can be slightly faster than RPlocal when K is small.

#index 1872235
#* Mining emerging patterns by streaming feature selection
#@ Kui Yu;Wei Ding;Dan A. Simovici;Xindong Wu
#t 2012
#c 0
#% 243728
#% 248791
#% 280409
#% 310550
#% 466483
#% 478133
#% 501540
#% 501982
#% 546047
#% 564401
#% 727835
#% 793239
#% 865731
#% 881486
#% 961200
#% 1214677
#% 1688457
#% 1692331
#! Building an accurate emerging pattern classifier with a high-dimensional dataset is a challenging issue. The problem becomes even more difficult if the whole feature space is unavailable before learning starts. This paper presents a new technique on mining emerging patterns using streaming feature selection. We model high feature dimensions with streaming features, that is, features arrive and are processed one at a time. As features flow in one by one, we online evaluate each coming feature to determine whether it is useful for mining predictive emerging patterns (EPs) by exploiting the relationship between feature relevance and EP discriminability (the predictive ability of an EP). We employ this relationship to guide an online EP mining process. This new approach can mine EPs from a high-dimensional dataset, even when its entire feature set is unavailable before learning. The experiments on a broad range of datasets validate the effectiveness of the proposed approach against other well-established methods, in terms of predictive accuracy, pattern numbers and running time.

#index 1872236
#* Linear space direct pattern sampling using coupling from the past
#@ Mario Boley;Sandy Moens;Thomas Gärtner
#t 2012
#c 0
#% 212147
#% 280409
#% 299985
#% 546694
#% 1072518
#% 1108863
#% 1328170
#% 1405146
#% 1565634
#% 1605978
#! This paper shows how coupling from the past (CFTP) can be used to avoid time and memory bottlenecks in direct local pattern sampling procedures. Such procedures draw controlled amounts of suitably biased samples directly from the pattern space of a given dataset in polynomial time. Previous direct pattern sampling methods can produce patterns in rapid succession after some initial preprocessing phase. This preprocessing phase, however, turns out to be prohibitive in terms of time and memory for many datasets. We show how CFTP can be used to avoid any super-linear preprocessing and memory requirements. This allows to simulate more complex distributions, which previously were intractable. We show for a large number of public real-world datasets that these new algorithms are fast to execute and their pattern collections outperform previous approaches both in unsupervised as well as supervised contexts.

#index 1872237
#* Mining top-K high utility itemsets
#@ Cheng Wei Wu;Bai-En Shie;Vincent S. Tseng;Philip S. Yu
#t 2012
#c 0
#% 300120
#% 481290
#% 498445
#% 629644
#% 727894
#% 772831
#% 800181
#% 829993
#% 1019450
#% 1044004
#% 1046743
#% 1072635
#% 1176880
#% 1327654
#% 1331657
#% 1401374
#% 1411078
#% 1451164
#% 1693339
#% 1914469
#! Mining high utility itemsets from databases is an emerging topic in data mining, which refers to the discovery of itemsets with utilities higher than a user-specified minimum utility threshold min_util. Although several studies have been carried out on this topic, setting an appropriate minimum utility threshold is a difficult problem for users. If min_util is set too low, too many high utility itemsets will be generated, which may cause the mining algorithms to become inefficient or even run out of memory. On the other hand, if min_util is set too high, no high utility itemset will be found. Setting appropriate minimum utility thresholds by trial and error is a tedious process for users. In this paper, we address this problem by proposing a new framework named top-k high utility itemset mining, where k is the desired number of high utility itemsets to be mined. An efficient algorithm named TKU (Top-K Utility itemsets mining) is proposed for mining such itemsets without setting min_util. Several features were designed in TKU to solve the new challenges raised in this problem, like the absence of anti-monotone property and the requirement of lossless results. Moreover, TKU incorporates several novel strategies for pruning the search space to achieve high efficiency. Results on real and synthetic datasets show that TKU has excellent performance and scalability.

#index 1872238
#* Sampling minimal frequent boolean (DNF) patterns
#@ Geng Li;Mohammed J. Zaki
#t 2012
#c 0
#% 232136
#% 338594
#% 342717
#% 464714
#% 765529
#% 796210
#% 823361
#% 833572
#% 881486
#% 881558
#% 1072518
#% 1311697
#% 1328170
#% 1558464
#% 1605978
#% 1716938
#! We tackle the challenging problem of mining the simplest Boolean patterns from categorical datasets. Instead of complete enumeration, which is typically infeasible for this class of patterns, we develop effective sampling methods to extract a representative subset of the minimal Boolean patterns (in disjunctive normal form - DNF). We make both theoretical and practical contributions, which allow us to prune the search space based on provable properties. Our approach can provide a near-uniform sample of the minimal DNF patterns. We also show that the mined minimal DNF patterns are very effective when used as features for classification.

#index 1872239
#* The contextual focused topic model
#@ Xu Chen;Mingyuan Zhou;Lawrence Carin
#t 2012
#c 0
#% 268079
#% 280819
#% 290830
#% 722904
#% 739899
#% 769906
#% 818266
#% 1055681
#% 1083684
#% 1083734
#% 1130899
#% 1166526
#% 1211703
#% 1214645
#% 1214701
#% 1551224
#% 1606070
#! A nonparametric Bayesian contextual focused topic model (cFTM) is proposed. The cFTM infers a sparse ("focused") set of topics for each document, while also leveraging contextual information about the author(s) and document venue. The hierarchical beta process, coupled with a Bernoulli process, is employed to infer the focused set of topics associated with each author and venue; the same construction is also employed to infer those topics associated with a given document that are unusual (termed "random effects"), relative to topics that are inferred as probable for the associated author(s) and venue. To leverage statistical strength and infer latent interrelationships between authors and venues, the Dirichlet process is utilized to cluster authors and venues. The cFTM automatically infers the number of topics needed to represent the corpus, the number of author and venue clusters, and the probabilistic importance of the author, venue and random-effect information on word assignment for a given document. Efficient MCMC inference is presented. Example results and interpretations are presented for two real datasets, demonstrating promising performance, with comparison to other state-of-the-art methods.

#index 1872240
#* Practical collapsed variational bayes inference for hierarchical dirichlet process
#@ Issei Sato;Kenichi Kurihara;Hiroshi Nakagawa
#t 2012
#c 0
#% 722904
#% 769906
#% 788094
#% 823342
#% 875959
#% 875970
#% 875981
#% 879587
#% 881534
#% 937549
#% 1083684
#% 1214625
#% 1214638
#% 1214650
#% 1214715
#% 1417055
#% 1451206
#% 1451207
#% 1605963
#% 1605964
#% 1605966
#% 1605967
#% 1605980
#% 1606070
#! We propose a novel collapsed variational Bayes (CVB) inference for the hierarchical Dirichlet process (HDP). While the existing CVB inference for the HDP variant of latent Dirichlet allocation (LDA) is more complicated and harder to implement than that for LDA, the proposed algorithm is simple to implement, does not require variance counts to be maintained, does not need to set hyper-parameters, and has good predictive performance.

#index 1872241
#* Overlapping decomposition for causal graphical modeling
#@ Lei Han;Guojie Song;Gao Cong;Kunqing Xie
#t 2012
#c 0
#% 989577
#% 1100067
#% 1214680
#% 1214744
#% 1318686
#% 1417383
#! Causal graphical models are developed to detect the dependence relationships between random variables and provide intuitive explanations for the relationships in complex systems. Most of existing work focuses on learning a single graphical model for all the variables. However, a single graphical model cannot accurately characterize the complicated causal relationships for a relatively large graph. In this paper, we propose the problem of estimating an overlapping decomposition for Gaussian graphical models of a large scale to generate overlapping sub-graphical models. Specifically, we formulate an objective function for the overlapping decomposition problem and propose an approximate algorithm for it. A key theory of the algorithm is that the problem of solving a κ+1 node graphical model can be reduced to the problem of solving a one-step regularization based on a solved κ node graphical model. Based on this theory, a greedy expansion algorithm is proposed to generate the overlapping subgraphs. We evaluate the effectiveness of our model on both synthetic datasets and real traffic dataset, and the experimental results show the superiority of our method.

#index 1872242
#* TM-LDA: efficient online modeling of latent topic transitions in social media
#@ Yu Wang;Eugene Agichtein;Michele Benzi
#t 2012
#c 0
#% 722904
#% 729943
#% 769906
#% 800496
#% 875959
#% 881498
#% 1400018
#% 1411585
#% 1451233
#% 1517899
#% 1536522
#% 1536536
#% 1607052
#% 1642000
#% 1650298
#% 1688506
#% 1693930
#! Latent topic analysis has emerged as one of the most effective methods for classifying, clustering and retrieving textual data. However, existing models such as Latent Dirichlet Allocation (LDA) were developed for static corpora of relatively large documents. In contrast, much of the textual content on the web, and especially social media, is temporally sequenced, and comes in short fragments, including microblog posts on sites such as Twitter and Weibo, status updates on social networking sites such as Facebook and LinkedIn, or comments on content sharing sites such as YouTube. In this paper we propose a novel topic model, Temporal-LDA or TM-LDA, for efficiently mining text streams such as a sequence of posts from the same author, by modeling the topic transitions that naturally arise in these data. TM-LDA learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent postings. After training, TM-LDA is thus able to accurately predict the expected topic distribution in future posts. To make these predictions more efficient for a realistic online setting, we develop an efficient updating algorithm to adjust the topic transition parameters, as new documents stream in. Our empirical results, over a corpus of over 30 million microblog posts, show that TM-LDA significantly outperforms state-of-the-art static LDA models for estimating the topic distribution of new documents over time. We also demonstrate that TM-LDA is able to highlight interesting variations of common topic transitions, such as the differences in the work-life rhythm of cities, and factors associated with area-specific problems and complaints.

#index 1872243
#* Multi-view clustering using mixture models in subspace projections
#@ Stephan Günnemann;Ines Färber;Thomas Seidl
#t 2012
#c 0
#% 823379
#% 891559
#% 915231
#% 1117008
#% 1137062
#% 1165480
#% 1176995
#% 1214693
#% 1214709
#% 1292599
#% 1451197
#% 1451231
#% 1642075
#! Detecting multiple clustering solutions is an emerging research field. While data is often multi-faceted in its very nature, traditional clustering methods are restricted to find just a single grouping. To overcome this limitation, methods aiming at the detection of alternative and multiple clustering solutions have been proposed. In this work, we present a Bayesian framework to tackle the problem of multi-view clustering. We provide multiple generalizations of the data by using multiple mixture models. Each mixture describes a specific view on the data by using a mixture of Beta distributions in subspace projections. Since a mixture summarizes the clusters located in similar subspace projections, each view highlights specific aspects of the data. In addition, our model handles overlapping views, where the mixture components compete against each other in the data generation process. For efficiently learning the distributions, we propose the algorithm MVGen that exploits the ICM principle and uses Bayesian model selection to trade-off the cluster model's complexity against its goodness of fit. With experiments on various real-world data sets, we demonstrate the high potential of MVGen to detect multiple, overlapping clustering views in subspace projections of the data.

#index 1872244
#* A simple methodology for soft cost-sensitive classification
#@ Te-Kang Jan;Da-Wei Wang;Chi-Hung Lin;Hsuan-Tien Lin
#t 2012
#c 0
#% 160852
#% 190581
#% 280437
#% 423630
#% 443790
#% 458218
#% 714684
#% 727925
#% 769875
#% 796213
#% 840841
#% 990437
#% 1074347
#% 1250597
#% 1289281
#% 1301004
#% 1451238
#% 1558464
#% 1567974
#% 1685519
#% 1705511
#% 1765235
#% 1777209
#% 1860941
#% 1880357
#! Many real-world data mining applications need varying cost for different types of classification errors and thus call for cost-sensitive classification algorithms. Existing algorithms for cost-sensitive classification are successful in terms of minimizing the cost, but can result in a high error rate as the trade-off. The high error rate holds back the practical use of those algorithms. In this paper, we propose a novel cost-sensitive classification methodology that takes both the cost and the error rate into account. The methodology, called soft cost-sensitive classification, is established from a multicriteria optimization problem of the cost and the error rate, and can be viewed as regularizing cost-sensitive classification with the error rate. The simple methodology allows immediate improvements of existing cost-sensitive classification algorithms. Experiments on the benchmark and the real-world data sets show that our proposed methodology indeed achieves lower test error rates and similar (sometimes lower) test costs than existing cost-sensitive classification algorithms.

#index 1872245
#* Intelligible models for classification and regression
#@ Yin Lou;Rich Caruana;Johannes Gehrke
#t 2012
#c 0
#% 400847
#% 424997
#% 448194
#% 875965
#% 928422
#% 1023330
#% 1100070
#% 1214651
#% 1328061
#% 1560393
#! Complex models for regression and classification have high accuracy, but are unfortunately no longer interpretable by users. We study the performance of generalized additive models (GAMs), which combine single-feature models called shape functions through a linear function. Since the shape functions can be arbitrarily complex, GAMs are more accurate than simple linear models. But since they do not contain any interactions between features, they can be easily interpreted by users. We present the first large-scale empirical comparison of existing methods for learning GAMs. Our study includes existing spline and tree-based methods for shape functions and penalized least squares, gradient boosting, and backfitting for learning GAMs. We also present a new method based on tree ensembles with an adaptive number of leaves that consistently outperforms previous work. We complement our experimental results with a bias-variance analysis that explains how different shape models influence the additive model. Our experiments show that shallow bagged trees with gradient boosting distinguish itself as the best method on low- to medium-dimensional datasets.

#index 1872246
#* NASA: achieving lower regrets and faster rates via adaptive stepsizes
#@ Hua Ouyang;Alexander Gray
#t 2012
#c 0
#% 1299294
#% 1674795
#! The classic Stochastic Approximation (SA) method achieves optimal rates under the black-box model. This optimality does not rule out better algorithms when more information about functions and data is available. We present a family of Noise Adaptive Stochastic Approximation (NASA) algorithms for online convex optimization and stochastic convex optimization. NASA is an adaptive variant of Mirror Descent Stochastic Approximation. It is novel in its practical variation-dependent stepsizes and better theoretical guarantees. We show that comparing with state-of-the-art adaptive and non-adaptive SA methods, lower regrets and faster rates can be achieved under low-variation assumptions.

#index 1872247
#* Learning in non-stationary environments with class imbalance
#@ Thomas Ryan Hoens;Nitesh V. Chawla
#t 2012
#c 0
#% 136350
#% 451036
#% 580510
#% 727880
#% 765519
#% 1168771
#% 1214635
#% 1301004
#% 1343213
#% 1406871
#% 1472282
#% 1491570
#% 1688475
#! Learning in non-stationary environments is an increasingly important problem in a wide variety of real-world applications. In non-stationary environments data arrives incrementally, however the underlying generating function may change over time. In addition to the environments being non-stationary, they also often exhibit class imbalance. That is one class (the majority class) vastly outnumbers the other class (the minority class). This combination of class imbalance with non-stationary environments poses significant and interesting practical problems for classification. To overcome these issues, we introduce a novel instance selection mechanism, as well as provide a modification to the Heuristic Updatable Weighted Random Subspaces (HUWRS) method for the class imbalance problem. We then compare our modifications of HUWRS (called HUWRS.IP) to other state of the art algorithms, concluding that HUWRS. IP often achieves vastly superior performance.

#index 1872248
#* Linear support vector machines via dual cached loops
#@ Shin Matsushima;S.V.N. Vishwanathan;Alexander J. Smola
#t 2012
#c 0
#% 131165
#% 197394
#% 227736
#% 269217
#% 269218
#% 269222
#% 757953
#% 881477
#% 916781
#% 961152
#% 961187
#% 983905
#% 1073923
#% 1117691
#% 1211829
#% 1386108
#% 1451223
#% 1523858
#% 1558464
#% 1605991
#% 1693873
#% 1761178
#% 1815826
#% 1862488
#! Modern computer hardware offers an elaborate hierarchy of storage subsystems with different speeds, capacities, and costs associated with them. Furthermore, processors are now inherently parallel offering the execution of several diverse threads simultaneously. This paper proposes StreamSVM, the first algorithm for training linear Support Vector Machines (SVMs) which takes advantage of these properties by integrating caching with optimization. StreamSVM works by performing updates in the dual, thus obviating the need to rebalance frequently visited examples. Furthermore we trade off file I/O with data expansion on the fly by generating features on demand. This significantly increases throughput. Experiments show that StreamSVM outperforms other linear SVM solvers, including the award winning work of [38], by orders of magnitude and produces more accurate solutions within a shorter amount of time.

#index 1872249
#* Discovering regions of different functions in a city using human mobility and POIs
#@ Jing Yuan;Yu Zheng;Xing Xie
#t 2012
#c 0
#% 32926
#% 123940
#% 391298
#% 722904
#% 1451230
#% 1560379
#% 1589340
#% 1605948
#% 1605995
#% 1606041
#% 1613884
#% 1613886
#% 1641648
#% 1693962
#% 1701756
#! The development of a city gradually fosters different functional regions, such as educational areas and business districts. In this paper, we propose a framework (titled DRoF) that Discovers Regions of different Functions in a city using both human mobility among regions and points of interests (POIs) located in a region. Specifically, we segment a city into disjointed regions according to major roads, such as highways and urban express ways. We infer the functions of each region using a topic-based inference model, which regards a region as a document, a function as a topic, categories of POIs (e.g., restaurants and shopping malls) as metadata (like authors, affiliations, and key words), and human mobility patterns (when people reach/leave a region and where people come from and leave for) as words. As a result, a region is represented by a distribution of functions, and a function is featured by a distribution of mobility patterns. We further identify the intensity of each function in different locations. The results generated by our framework can benefit a variety of applications, including urban planning, location choosing for a business, and social recommendations. We evaluated our method using large-scale and real-world datasets, consisting of two POI datasets of Beijing (in 2010 and 2011) and two 3-month GPS trajectory datasets (representing human mobility) generated by over 12,000 taxicabs in Beijing in 2010 and 2011 respectively. The results justify the advantages of our approach over baseline methods solely using POIs or human mobility.

#index 1872250
#* Constructing popular routes from uncertain trajectories
#@ Ling-Yin Wei;Yu Zheng;Wen-Chih Peng
#t 2012
#c 0
#% 201876
#% 989604
#% 1019077
#% 1190134
#% 1230824
#% 1318678
#% 1426523
#% 1445701
#% 1451250
#% 1480783
#% 1482236
#% 1523885
#% 1594578
#% 1594591
#% 1688509
#% 1693962
#% 1846708
#! The advances in location-acquisition technologies have led to a myriad of spatial trajectories. These trajectories are usually generated at a low or an irregular frequency due to applications' characteristics or energy saving, leaving the routes between two consecutive points of a single trajectory uncertain (called an uncertain trajectory). In this paper, we present a Route Inference framework based on Collective Knowledge (abbreviated as RICK) to construct the popular routes from uncertain trajectories. Explicitly, given a location sequence and a time span, the RICK is able to construct the top-k routes which sequentially pass through the locations within the specified time span, by aggregating such uncertain trajectories in a mutual reinforcement way (i.e., uncertain + uncertain → certain). Our work can benefit trip planning, traffic management, and animal movement studies. The RICK comprises two components: routable graph construction and route inference. First, we explore the spatial and temporal characteristics of uncertain trajectories and construct a routable graph by collaborative learning among the uncertain trajectories. Second, in light of the routable graph, we propose a routing algorithm to construct the top-k routes according to a user-specified query. We have conducted extensive experiments on two real datasets, consisting of Foursquare check-in datasets and taxi trajectories. The results show that RICK is both effective and efficient.

#index 1872251
#* GetJar mobile application recommendations with very sparse datasets
#@ Kent Shi;Kamal Ali
#t 2012
#c 0
#% 124010
#% 330687
#% 420515
#% 452563
#% 734594
#% 860672
#% 918842
#% 1022958
#% 1083671
#% 1476448
#! The Netflix competition of 2006 [2] has spurred significant activity in the recommendations field, particularly in approaches using latent factor models [3,5,8,12] However, the near ubiquity of the Netflix and the similar MovieLens datasets1 may be narrowing the generality of lessons learned in this field. At GetJar, our goal is to make appealing recommendations of mobile applications (apps). For app usage, we observe a distribution that has higher kurtosis (heavier head and longer tail) than that for the aforementioned movie datasets. This happens primarily because of the large disparity in resources available to app developers and the low cost of app publication relative to movies. In this paper we compare a latent factor (PureSVD) and a memory-based model with our novel PCA-based model, which we call Eigenapp. We use both accuracy and variety as evaluation metrics. PureSVD did not perform well due to its reliance on explicit feedback such as ratings, which we do not have. Memory-based approaches that perform vector operations in the original high dimensional space over-predict popular apps because they fail to capture the neighborhood of less popular apps. They have high accuracy due to the concentration of mass in the head, but did poorly in terms of variety of apps exposed. Eigenapp, which exploits neighborhood information in low dimensional spaces, did well both on precision and variety, underscoring the importance of dimensionality reduction to form quality neighborhoods in high kurtosis distributions.

#index 1872252
#* Differentially private transit data publication: a case study on the montreal transportation system
#@ Rui Chen;Benjamin C.M. Fung;Bipin C. Desai;Nériah M. Sossou
#t 2012
#c 0
#% 463903
#% 464996
#% 576761
#% 751578
#% 1029084
#% 1061644
#% 1080161
#% 1083653
#% 1181219
#% 1192445
#% 1198224
#% 1198227
#% 1206713
#% 1217148
#% 1217156
#% 1426415
#% 1441735
#% 1478165
#% 1523886
#% 1581862
#% 1581865
#% 1584349
#% 1605968
#% 1740518
#% 1818428
#% 1954684
#! With the wide deployment of smart card automated fare collection (SCAFC) systems, public transit agencies have been benefiting from huge volume of transit data, a kind of sequential data, collected every day. Yet, improper publishing and use of transit data could jeopardize passengers' privacy. In this paper, we present our solution to transit data publication under the rigorous differential privacy model for the Société de transport de Montréal (STM). We propose an efficient data-dependent yet differentially private transit data sanitization approach based on a hybrid-granularity prefix tree structure. Moreover, as a post-processing step, we make use of the inherent consistency constraints of a prefix tree to conduct constrained inferences, which lead to better utility. Our solution not only applies to general sequential data, but also can be seamlessly extended to trajectory data. To our best knowledge, this is the first paper to introduce a practical solution for publishing large volume of sequential data under differential privacy. We examine data utility in terms of two popular data analysis tasks conducted at the STM, namely count queries and frequent sequential pattern mining. Extensive experiments on real-life STM datasets confirm that our approach maintains high utility and is scalable to large datasets.

#index 1872253
#* Interaction and collective intelligence in internet computing
#@ Deyi Li
#t 2012
#c 0
#! Network interconnection, information interoperability, and crowds interaction on the Internet could inspire better computation models than Turing machine, since that human plays an important factor in Internet computing, so that the human-machine and machine-machine interactions have evolved to be the kernel of Internet computing. Internet has not been simply equivalent to a virtual huge computer, or a set of computers. On the Internet, human's behaviors are uncertain, the interactions and influence among people are also uncertain. These uncertainties cannot be described by Turing machine and traditional interaction machine. As a new computation platform, Internet computing requires new theories and methods. By combining topology in mathematics with the field theory in physics, we propose the topological potential approach, which set up a virtual field by the topological space to reflect individual activities, local effects and preferential attachment. This approach can be used to research the emergence of collective intelligence. Here, we introduce three case studies to illustrate the analysis on the collective intelligence on the Internet and discuss some potential applications of the topological potential approach.

#index 1872254
#* Building an engine for big data
#@ Masaru Kitsuregawa
#t 2012
#c 0
#! IT program in Japan to build powerful engine for big data was launched. Quite recently the initial version is commercialized. This presentation will give a brief overview of the project. Also some of the potential applications will be introduced.

#index 1872255
#* A new challenge of information processing under the 21st century
#@ Bo Zhang
#t 2012
#c 0
#! In web era, we are confronted with a huge amount of raw data and a tremendous change of man-machine interaction modes. We have to deal with the content (semantics) of data rather than their form alone. Traditional information processing approaches face a new challenge since they cannot deal with the semantic meaning or content of information. But humans can handle such a problem easily. So it's needed a new information processing strategy that correlated with the content of information by learning some mechanisms from human beings. Therefore, we need (1) a set of robust detectors for detecting semantically meaningful features such as boundaries, shapes, etc. in images, words, sentences, etc. in text, and (2) a set of methods that can effectively analyze and exploit the information structures that encode the content of information. During the past 40 years the probability theory has made a great progress. It has provided a set of mathematical tools for representing and analyzing information structures. In the talk we will discuss what difficulty we face, what we can do, and how we should do in the content-based information processing.

#index 1872256
#* Developing data mining applications
#@ Geoff Holmes
#t 2012
#c 0
#! In this talk I will review several real-world applications developed at the University of Waikato over the past 15 years. These include the use of near infrared spectroscopy coupled with data mining as an alternate laboratory technique for predicting compound concentrations in soil and plant samples, and the analysis of gas chromatography mass spectrometry (GCMS) data, a technique used to determine in environmental applications, for example, the petroleum content in soil and water samples. I will then briefly discuss how experience with these applications has led to the development of an open-source framework for application development.

#index 1872257
#* Learning from crowds in the presence of schools of thought
#@ Yuandong Tian;Jun Zhu
#t 2012
#c 0
#% 147960
#% 1047347
#% 1214647
#% 1264744
#% 1384503
#% 1452857
#% 1472273
#! Crowdsourcing has recently become popular among machine learning researchers and social scientists as an effective way to collect large-scale experimental data from distributed workers. To extract useful information from the cheap but potentially unreliable answers to tasks, a key problem is to identify reliable workers as well as unambiguous tasks. Although for objective tasks that have one correct answer per task, previous works can estimate worker reliability and task clarity based on the single gold standard assumption, for tasks that are subjective and accept multiple reasonable answers that workers may be grouped into, a phenomenon called schools of thought, existing models cannot be trivially applied. In this work, we present a statistical model to estimate worker reliability and task clarity without resorting to the single gold standard assumption. This is instantiated by explicitly characterizing the grouping behavior to form schools of thought with a rank-1 factorization of a worker-task groupsize matrix. Instead of performing an intermediate inference step, which can be expensive and unstable, we present an algorithm to analytically compute the sizes of different groups. We perform extensive empirical studies on real data collected from Amazon Mechanical Turk. Our method discovers the schools of thought, shows reasonable estimation of worker reliability and task clarity, and is robust to hyperparameter changes. Furthermore, our estimated worker reliability can be used to improve the gold standard prediction for objective tasks.

#index 1872258
#* Social sampling
#@ Anirban Dasgupta;Ravi Kumar;D. Sivakumar
#t 2012
#c 0
#% 183094
#% 252078
#% 338394
#% 881526
#% 1292544
#% 1449326
#% 1487839
#% 1560414
#% 1560416
#% 1612128
#% 1746900
#! We investigate a class of methods that we call "social sampling," where participants in a poll respond with a summary of their friends' putative responses to the poll. Social sampling leads to a novel trade-off question: the savings in the number of samples(roughly the average degree of the network of participants) vs. the systematic bias in the poll due to the network structure. We provide precise analyses of estimators that result from this idea. With non-uniform sampling of nodes and non-uniform weighting of neighbors' responses, we devise an ideal unbiased estimator. We show that the variance of this estimator is controlled by the second eigenvalue of the normalized Laplacian of the network (the network structure penalty) and the correlation between node degrees and the property being measured (the effective savings factor). In addition, we present a sequence of approximate estimators that are simpler or more realistic or both, and analyze their performance. Experiments on large real-world networks show that social sampling is a powerful paradigm in obtaining accurate estimates with very few samples. At the same time, our results urge caution in interpreting recent results about "expectation vs. intent polling".

#index 1872259
#* From user comments to on-line conversations
#@ Chunyan Wang;Mao Ye;Bernardo A. Huberman
#t 2012
#c 0
#% 351376
#% 823332
#% 1035587
#% 1055736
#% 1183221
#% 1190088
#% 1214671
#% 1292698
#% 1300556
#% 1301020
#% 1399995
#% 1451195
#% 1586591
#! We present an analysis of user conversations in on-line social media and their evolution over time. We propose a dynamic model that predicts the growth dynamics and structural properties of conversation threads. The model reconciles the differing observations that have been reported in existing studies. By separating artificial factors from user behavior, we show that there are actually underlying rules in common for on-line conversations in different social media websites. Results of our model are supported by empirical measurements throughout a number of different social media websites.

#index 1872260
#* eTrust: understanding trust evolution in an online world
#@ Jiliang Tang;Huiji Gao;Huan Liu;Atish Das Sarma
#t 2012
#c 0
#% 17631
#% 577217
#% 754098
#% 838504
#% 943767
#% 955712
#% 1001279
#% 1071523
#% 1083671
#% 1130901
#% 1190130
#% 1214661
#% 1214666
#% 1233312
#% 1247796
#% 1292559
#% 1399997
#% 1400002
#% 1400031
#% 1536554
#% 1560411
#% 1595763
#% 1603140
#% 1693870
#! Most existing research about online trust assumes static trust relations between users. As we are informed by social sciences, trust evolves as humans interact. Little work exists studying trust evolution in an online world. Researching online trust evolution faces unique challenges because more often than not, available data is from passive observation. In this paper, we leverage social science theories to develop a methodology that enables the study of online trust evolution. In particular, we propose a framework of evolution trust, eTrust, which exploits the dynamics of user preferences in the context of online product review. We present technical details about modeling trust evolution, and perform experiments to show how the exploitation of trust evolution can help improve the performance of online applications such as rating and trust prediction.

#index 1872261
#* Searching and mining trillions of time series subsequences under dynamic time warping
#@ Thanawin Rakthanmanon;Bilson Campana;Abdullah Mueen;Gustavo Batista;Brandon Westover;Qiang Zhu;Jesin Zakaria;Eamonn Keogh
#t 2012
#c 0
#% 91091
#% 462231
#% 564263
#% 643518
#% 740761
#% 809264
#% 998465
#% 998813
#% 1044456
#% 1066734
#% 1083693
#% 1127609
#% 1132575
#% 1173744
#% 1206865
#% 1211645
#% 1214716
#% 1246209
#% 1246943
#% 1362520
#% 1451249
#% 1538191
#% 1590537
#% 1702249
#% 1754971
#! Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms. The difficulty of scaling search to large datasets largely explains why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine truly massive time series for the first time. We demonstrate the following extremely unintuitive fact; in large datasets we can exactly search under DTW much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We show that our ideas allow us to solve higher-level time series data mining problem such as motif discovery and clustering at scales that would otherwise be untenable. In addition to mining massive datasets, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.

#index 1872262
#* Fast mining and forecasting of complex time-stamped events
#@ Yasuko Matsubara;Yasushi Sakurai;Christos Faloutsos;Tomoharu Iwata;Masatoshi Yoshikawa
#t 2012
#c 0
#% 280819
#% 316143
#% 722904
#% 810058
#% 844312
#% 874985
#% 875024
#% 875959
#% 881498
#% 1015301
#% 1073989
#% 1074027
#% 1083687
#% 1176853
#% 1190057
#% 1214694
#% 1214715
#% 1275221
#% 1305518
#% 1318665
#% 1328115
#% 1451206
#% 1523829
#% 1605967
#% 1705530
#! Given huge collections of time-evolving events such as web-click logs, which consist of multiple attributes (e.g., URL, userID, times- tamp), how do we find patterns and trends? How do we go about capturing daily patterns and forecasting future events? We need two properties: (a) effectiveness, that is, the patterns should help us understand the data, discover groups, and enable forecasting, and (b) scalability, that is, the method should be linear with the data size. We introduce TriMine, which performs three-way mining for all three attributes, namely, URLs, users, and time. Specifically TriMine discovers hidden topics, groups of URLs, and groups of users, simultaneously. Thanks to its concise but effective summarization, it makes it possible to accomplish the most challenging and important task, namely, to forecast future events. Extensive experiments on real datasets demonstrate that TriMine discovers meaningful topics and makes long-range forecasts, which are notoriously difficult to achieve. In fact, TriMine consistently outperforms the best state-of-the-art existing methods in terms of accuracy and execution speed (up to 74x faster).

#index 1872263
#* Mining recent temporal patterns for event detection in multivariate time series data
#@ Iyad Batal;Dmitriy Fradkin;James Harrison;Fabian Moerchen;Milos Hauskrecht
#t 2012
#c 0
#% 399
#% 224476
#% 329537
#% 443350
#% 463903
#% 464996
#% 481290
#% 487661
#% 784569
#% 824709
#% 844326
#% 876074
#% 881532
#% 949146
#% 975048
#% 980988
#% 1024911
#% 1037611
#% 1081949
#% 1362601
#% 1523829
#% 1605992
#% 1685514
#% 1826253
#! Improving the performance of classifiers using pattern mining techniques has been an active topic of data mining research. In this work we introduce the recent temporal pattern mining framework for finding predictive patterns for monitoring and event detection problems in complex multivariate time series data. This framework first converts time series into time-interval sequences of temporal abstractions. It then constructs more complex temporal patterns backwards in time using temporal operators. We apply our framework to health care data of 13,558 diabetic patients and show its benefits by efficiently finding useful patterns for detecting and diagnosing adverse medical conditions that are associated with diabetes.

#index 1872264
#* A shapelet transform for time series classification
#@ Jason Lines;Luke M. Davis;Jon Hills;Anthony Bagnall
#t 2012
#c 0
#% 643518
#% 881568
#% 961134
#% 1127609
#% 1214716
#% 1374686
#% 1538189
#% 1585310
#% 1606057
#% 1619685
#! The problem of time series classification (TSC), where we consider any real-valued ordered data a time series, presents a specific machine learning challenge as the ordering of variables is often crucial in finding the best discriminating features. One of the most promising recent approaches is to find shapelets within a data set. A shapelet is a time series subsequence that is identified as being representative of class membership. The original research in this field embedded the procedure of finding shapelets within a decision tree. We propose disconnecting the process of finding shapelets from the classification algorithm by proposing a shapelet transformation. We describe a means of extracting the k best shapelets from a data set in a single pass, and then use these shapelets to transform data by calculating the distances from a series to each shapelet. We demonstrate that transformation into this new data space can improve classification accuracy, whilst retaining the explanatory power provided by shapelets.

#index 1872265
#* Accelerated singular value thresholding for matrix completion
#@ Yao Hu;Debing Zhang;Jun Liu;Jieping Ye;Xiaofei He
#t 2012
#c 0
#% 987198
#% 996872
#% 1013661
#% 1083671
#% 1214666
#% 1214676
#% 1309918
#% 1441070
#% 1442717
#% 1451211
#% 1476448
#% 1504249
#% 1556166
#% 1605981
#% 1858962
#! Recovering a large matrix from a small subset of its entries is a challenging problem arising in many real world applications, such as recommender system and image in-painting. These problems can be formulated as a general matrix completion problem. The Singular Value Thresholding (SVT) algorithm is a simple and efficient first-order matrix completion method to recover the missing values when the original data matrix is of low rank. SVT has been applied successfully in many applications. However, SVT is computationally expensive when the size of the data matrix is large, which significantly limits its applicability. In this paper, we propose an Accelerated Singular Value Thresholding (ASVT) algorithm which improves the convergence rate from O(1/N) for SVT to O(1/N2), where N is the number of iterations during optimization. Specifically, the dual problem of the nuclear norm minimization problem is derived and an adaptive line search scheme is introduced to solve this dual problem. Consequently, the optimal solution of the primary problem can be readily obtained from that of the dual problem. We have conducted a series of experiments on a synthetic dataset, a distance matrix dataset and a large movie rating dataset. The experimental results have demonstrated the efficiency and effectiveness of the proposed algorithm.

#index 1872266
#* Fast bregman divergence NMF using taylor expansion and coordinate descent
#@ Liangda Li;Guy Lebanon;Haesun Park
#t 2012
#c 0
#% 280819
#% 816173
#% 916785
#% 995168
#% 1038899
#% 1050550
#% 1108903
#% 1133918
#% 1176925
#% 1178006
#% 1440394
#% 1565315
#% 1606047
#% 1745107
#% 1815505
#% 1873023
#! Non-negative matrix factorization (NMF) provides a lower rank approximation of a matrix. Due to nonnegativity imposed on the factors, it gives a latent structure that is often more physically meaningful than other lower rank approximations such as singular value decomposition (SVD). Most of the algorithms proposed in literature for NMF have been based on minimizing the Frobenius norm. This is partly due to the fact that the minimization problem based on the Frobenius norm provides much more flexibility in algebraic manipulation than other divergences. In this paper we propose a fast NMF algorithm that is applicable to general Bregman divergences. Through Taylor series expansion of the Bregman divergences, we reveal a relationship between Bregman divergences and Euclidean distance. This key relationship provides a new direction for NMF algorithms with general Bregman divergences when combined with the scalar block coordinate descent method. The proposed algorithm generalizes several recently proposed methods for computation of NMF with Bregman divergences and is computationally faster than existing alternatives. We demonstrate the effectiveness of our approach with experiments conducted on artificial as well as real world data.

#index 1872267
#* GigaTensor: scaling tensor analysis up by 100 times - algorithms and discoveries
#@ U. Kang;Evangelos Papalexakis;Abhay Harpale;Christos Faloutsos
#t 2012
#c 0
#% 290830
#% 757953
#% 805877
#% 915297
#% 963669
#% 989585
#% 996872
#% 1021533
#% 1038978
#% 1042588
#% 1063553
#% 1176933
#% 1300087
#% 1318636
#% 1400001
#% 1594624
#% 1606050
#% 1607936
#% 1635120
#% 1758232
#% 1858962
#! Many data are modeled as tensors, or multi dimensional arrays. Examples include the predicates (subject, verb, object) in knowledge bases, hyperlinks and anchor texts in the Web graphs, sensor streams (time, location, and type), social networks over time, and DBLP conference-author-keyword relations. Tensor decomposition is an important data mining tool with various applications including clustering, trend detection, and anomaly detection. However, current tensor decomposition algorithms are not scalable for large tensors with billions of sizes and hundreds millions of nonzeros: the largest tensor in the literature remains thousands of sizes and hundreds thousands of nonzeros. Consider a knowledge base tensor consisting of about 26 million noun-phrases. The intermediate data explosion problem, associated with naive implementations of tensor decomposition algorithms, would require the materialization and the storage of a matrix whose largest dimension would be ≈7 x 1014; this amounts to ~10 Petabytes, or equivalently a few data centers worth of storage, thereby rendering the tensor analysis of this knowledge base, in the naive way, practically impossible. In this paper, we propose GIGATENSOR, a scalable distributed algorithm for large scale tensor decomposition. GIGATENSOR exploits the sparseness of the real world tensors, and avoids the intermediate data explosion problem by carefully redesigning the tensor decomposition algorithm. Extensive experiments show that our proposed GIGATENSOR solves 100 times bigger problems than existing methods. Furthermore, we employ GIGATENSOR in order to analyze a very large real world, knowledge base tensor and present our astounding findings which include discovery of potential synonyms among millions of noun-phrases (e.g. the noun 'pollutant' and the noun-phrase 'greenhouse gases').

#index 1872268
#* Active learning for online bayesian matrix factorization
#@ Jorge Silva;Lawrence Carin
#t 2012
#c 0
#% 855610
#% 961250
#% 1038334
#% 1073982
#% 1074346
#% 1083671
#% 1100108
#% 1386100
#% 1441070
#! The problem of large-scale online matrix completion is addressed via a Bayesian approach. The proposed method learns a factor analysis (FA) model for large matrices, based on a small number of observed matrix elements, and leverages the statistical model to actively select which new matrix entries/observations would be most informative if they could be acquired, to improve the model; the model inference and active learning are performed in an online setting. In the context of online learning, a greedy, fast and provably near-optimal algorithm is employed to sequentially maximize the mutual information between past and future observations, taking advantage of submodularity properties. Additionally, a simpler procedure, which directly uses the posterior parameters learned by the Bayesian approach, is shown to achieve slightly lower estimation quality, with far less computational effort. Inference is performed using a computationally efficient online variational Bayes (VB) procedure. Competitive results are obtained in a very large collaborative filtering problem, namely the Yahoo! Music ratings dataset.

#index 1872269
#* A sparsity-inducing formulation for evolutionary co-clustering
#@ Shuiwang Ji;Wenlu Zhang;Jun Liu
#t 2012
#c 0
#% 342621
#% 342659
#% 397854
#% 469422
#% 729918
#% 778215
#% 881468
#% 881514
#% 1036692
#% 1121275
#% 1179992
#% 1279659
#% 1448345
#% 1451171
#% 1706503
#% 1866320
#! Traditional co-clustering methods identify block structures from static data matrices. However, the data matrices in many applications are dynamic; that is, they evolve smoothly over time. Consequently, the hidden block structures embedded into the matrices are also expected to vary smoothly along the temporal dimension. It is therefore desirable to encourage smoothness between the block structures identified from temporally adjacent data matrices. In this paper, we propose an evolutionary co-clustering formulation for identifying co-cluster structures from time-varying data. The proposed formulation encourages smoothness between temporally adjacent blocks by employing the fused Lasso type of regularization. Our formulation is very flexible and allows for imposing smoothness constraints over only one dimension of the data matrices, thereby enabling its applicability to a large variety of settings. The optimization problem for the proposed formulation is non-convex, non-smooth, and non-separable. We develop an iterative procedure to compute the solution. Each step of the iterative procedure involves a convex, but non-smooth and non-separable problem. We propose to solve this problem in its dual form, which is convex and smooth. This leads to a simple gradient descent algorithm for computing the dual optimal solution. We evaluate the proposed formulation using the Allen Developing Mouse Brain Atlas data. Results show that our formulation consistently outperforms methods without the temporal smoothness constraints.

#index 1872270
#* Detecting changes of clustering structures using normalized maximum likelihood coding
#@ So Hirai;Kenji Yamanishi
#t 2012
#c 0
#% 369349
#% 823375
#% 855610
#% 881514
#% 984507
#% 989640
#% 1617314
#% 1813697
#% 1816709
#! We are concerned with the issue of detecting changes of clustering structures from multivariate time series. From the viewpoint of the minimum description length(MDL) principle, we propose an algorithm that tracks changes of clustering structures so that the sum of the code-length for data and that for clustering changes is minimum. Here we employ a Gaussian mixture model(GMM) as representation of clustering, and compute the code-length for data sequences using the normalized maximum likelihood (NML) coding. The proposed algorithm enables us to deal with clustering dynamics including merging, splitting, emergence, disappearance of clusters from a unifying view of the MDL principle. We empirically demonstrate using artificial data sets that our proposed method is able to detect cluster changes significantly more accurately than an existing statistical-test based method and AIC/BIC-based methods. We further use real customers' transaction data sets to demonstrate the validity of our algorithm in market analysis. We show that it is able to detect changes of customer groups, which correspond to changes of real market environments.

#index 1872271
#* Subspace correlation clustering: finding locally correlated dimensions in subspace projections of the data
#@ Stephan Günnemann;Ines Färber;Kittipat Virochsiri;Thomas Seidl
#t 2012
#c 0
#% 248792
#% 300131
#% 464888
#% 659967
#% 765439
#% 765548
#% 871026
#% 881456
#% 982551
#% 1130904
#% 1137067
#% 1165480
#% 1176982
#% 1206638
#% 1292599
#% 1737761
#! The necessity to analyze subspace projections of complex data is a well-known fact in the clustering community. While the full space may be obfuscated by overlapping patterns and irrelevant dimensions, only certain subspaces are able to reveal the clustering structure. Subspace clustering discards irrelevant dimensions and allows objects to belong to multiple, overlapping clusters due to individual subspace projections for each set of objects. As we will demonstrate, the observations, which originate the need to consider subspace projections for traditional clustering, also apply for the task of correlation analysis. In this work, we introduce the novel paradigm of subspace correlation clustering: we analyze subspace projections to find subsets of objects showing linear correlations among this subset of dimensions. In contrast to existing techniques, which determine correlations based on the full-space, our method is able to exclude locally irrelevant dimensions, enabling more precise detection of the correlated features. Since we analyze subspace projections, each object can contribute to several correlations. Our model allows multiple overlapping clusters in general but simultaneously avoids redundant clusters deducible from already known correlations. We introduce the algorithm SSCC that exploits different pruning techniques to efficiently generate a subspace correlation clustering. In thorough experiments we demonstrate the strength of our novel paradigm in comparison to existing methods.

#index 1872272
#* Dependency clustering across measurement scales
#@ Claudia Plant
#t 2012
#c 0
#% 300131
#% 420081
#% 765439
#% 810047
#% 938978
#% 986227
#% 995140
#% 1078626
#% 1211824
#% 1606054
#% 1671475
#% 1737762
#! How to automatically spot the major trends in large amounts of heterogeneous data? Clustering can help. However, most existing techniques suffer from one or more of the following drawbacks: 1) Many techniques support only one particular data type, most commonly numerical attributes. 2) Other techniques do not support attribute dependencies which are prevalent in real data. 3) Some approaches require input parameters which are difficult to estimate. 4) Most clustering approaches lack in interpretability. To address these challenges, we present the algorithm Scenic for dependency clustering across measurement scales. Our approach seamlessly integrates heterogenous data types measured at different scales, most importantly continuous numerical and discrete categorical data. Scenic clusters by arranging objects and attributes in a cluster-specific low-dimensional space. The embedding serves as a compact cluster model allowing to reconstruct the original heterogenous attributes with high accuracy. Thereby embedding reveals the major cluster-specific mixed-type attribute dependencies. Following the Minimum Description Length (MDL) principle, the cluster-specific embedding serves as a codebook for effective data compression. This compression-based view automatically balances goodness-of-fit and model complexity, making input parameters redundant. Finally, the embedding serves as a visualization enhancing the interpretability of the clustering result. Extensive experiments demonstrate the benefits of Scenic.

#index 1872273
#* A framework for summarizing and analyzing twitter feeds
#@ Xintian Yang;Amol Ghoting;Yiye Ruan;Srinivasan Parthasarathy
#t 2012
#c 0
#% 722904
#% 745513
#% 824710
#% 881542
#% 989663
#% 993960
#% 1015261
#% 1083708
#% 1084210
#% 1108878
#% 1214671
#% 1536522
#% 1605960
#% 1631439
#! The firehose of data generated by users on social networking and microblogging sites such as Facebook and Twitter is enormous. Real-time analytics on such data is challenging with most current efforts largely focusing on the efficient querying and retrieval of data produced recently. In this paper, we present a dynamic pattern driven approach to summarize data produced by Twitter feeds. We develop a novel approach to maintain an in-memory summary while retaining sufficient information to facilitate a range of user-specific and topic-specific temporal analytics. We empirically compare our approach with several state-of-the-art pattern summarization approaches along the axes of storage cost, query accuracy, query flexibility, and efficiency using real data from Twitter. We find that the proposed approach is not only scalable but also outperforms existing approaches by a large margin.

#index 1872274
#* Entity-centric topic-oriented opinion summarization in twitter
#@ Xinfan Meng;Furu Wei;Xiaohua Liu;Ming Zhou;Sujian Li;Houfeng Wang
#t 2012
#c 0
#% 742345
#% 769892
#% 854191
#% 907489
#% 956510
#% 1055682
#% 1077150
#% 1127964
#% 1250237
#% 1264738
#% 1264800
#% 1338553
#% 1346171
#% 1470682
#% 1544009
#% 1544032
#% 1558464
#% 1591944
#% 1592152
#% 1642034
#! Microblogging services, such as Twitter, have become popular channels for people to express their opinions towards a broad range of topics. Twitter generates a huge volume of instant messages (i.e. tweets) carrying users' sentiments and attitudes every minute, which both necessitates automatic opinion summarization and poses great challenges to the summarization system. In this paper, we study the problem of opinion summarization for entities, such as celebrities and brands, in Twitter. We propose an entity-centric topic-based opinion summarization framework, which aims to produce opinion summaries in accordance with topics and remarkably emphasizing the insight behind the opinions. To this end, we first mine topics from #hashtags, the human-annotated semantic tags in tweets. We integrate the #hashtags as weakly supervised information into topic modeling algorithms to obtain better interpretation and representation for calculating the similarity among them, and adopt Affinity Propagation algorithm to group #hashtags into coherent topics. Subsequently, we use templates generalized from paraphrasing to identify tweets with deep insights, which reveal reasons, express demands or reflect viewpoints. Afterwards, we develop a target (i.e. entity) dependent sentiment classification approach to identifying the opinion towards a given target (i.e. entity) of tweets. Finally, the opinion summary is generated through integrating information from dimensions of topic, opinion and insight, as well as other factors (e.g. topic relevancy, redundancy and language styles) in an unified optimization framework. We conduct extensive experiments on a real-life data set to evaluate the performance of individual opinion summarization modules as well as the quality of the produced summary. The promising experiment results show the effectiveness of the proposed framework and algorithms.

#index 1872275
#* Community discovery and profiling with social messages
#@ Wenjun Zhou;Hongxia Jin;Yan Liu
#t 2012
#c 0
#% 722904
#% 788094
#% 869480
#% 869649
#% 907491
#% 1211773
#% 1214625
#% 1214638
#% 1214668
#% 1214722
#% 1272187
#% 1289476
#% 1650298
#% 1688509
#! Discovering communities from social media and collaboration systems has been of great interest in recent years. Existing work show prospects of modeling contents and social links, aiming at discovering social communities, whose definition varies by application. We believe that a community depends not only on the group of people who actively participate, but also the topics they communicate about or collaborate on. This is especially true for workplace email communications. Within an organization, it is not uncommon that employees multifunction, and groups of employees collaborate on multiple projects at the same time. In this paper, we aim to automatically discovering and profiling users' communities by taking into account both the contacts and the topics. More specifically, we propose a community profiling model called COCOMP, where the communities labels are latent, and each social document corresponds to an information sharing activity among the most probable community members regarding the most relevant community issues. Experiment results on several social communication datasets, including emails and Twitter messages, demonstrate that the model can discover users' communities effectively, and provide concrete semantics.

#index 1872276
#* Finding trending local topics in search queries for personalization of a recommendation system
#@ Ziad Al Bawab;George H. Mills;Jean-Francois Crespo
#t 2012
#c 0
#% 115608
#% 310567
#% 342961
#% 818256
#% 1035574
#% 1074114
#% 1166523
#% 1166534
#% 1190057
#% 1355017
#% 1451210
#% 1598347
#% 1642196
#% 1712595
#! In this paper, we present our approach for geographic personalization of a content recommendation system. More specifically, our work focuses on recommending query topics to users. We do this by mining the search query logs to detect trending local topics. For a set of queries we compute their counts and what we call buzz scores, which is a metric for detecting trending behavior. We also compute the entropy of the geographic distribution of the queries as means of detecting their location affinity. We cluster the queries into trending topics and assign the topics to their corresponding location. Human editors then select a subset of these local topics and enter them into a recommendation system. In turn the recommendation system optimizes a pool of trending local and global topics by exploiting user feedback. We present some editorial evaluation of the technique and results of a live experiment. Inclusion of local topics in selected locations into the global pool of topics resulted in more than 6% relative increase in user engagement with the recommendation system compared to using the global topics exclusively.

#index 1872277
#* China's national personal credit scoring system: a real-life intelligent knowledge application
#@ Yong Shi
#t 2012
#c 0
#! Credit Reference Centre (CRC) of People's Bank of China (PBC) has built a big data: the largest personal credit database in the world with 800 million people's accounts collected from all commercial banks in China since 2003. From June 2006 to Sept 2009, Research Centre on Fictitious Economy and Data Science, Chinese Academy of Sciences (CASFEDS) and CRC jointly developed China's National Personal Credit Scoring System, known as "China Score", which is a unique and advanced KDD application under intelligent knowledge management on this big data. The system will be eventually serving all 1.3 billion population of China for their daily financial activities, such as bank accounts, credit card application, mortgage, personal loans, etc. It can become one of the most influential events of KDD techniques to human kind. This talk will introduce the key components of China Score project that includes objectives, modeling process, KDD techniques used in the projects, intelligent knowledge management and experience of the project development. In addition, the talk will also outline a number of policy recommendations based on China Score project which has been potentially impacting Chinese Government on its strategic decision making for China's economic developments.

#index 1872278
#* Maximizing return and minimizing cost with the right decision management systems
#@ Rich Holada
#t 2012
#c 0
#! The ability to achieve operational efficiency, product leadership, and customer intimacy still eludes many organizations due, in large part, to the chaos of business. Inconsistent prioritization and decision making; poor visibility between systems; processes that are not well controlled; and individual front-line decisions that seem small but, in totality, have a huge impact make it difficult for organizations to link strategy to execution and back. During this presentation, we will demonstrate how automating and optimizing decisions (operational efficiency) with business rules and predictive models enables better data driven results across the enterprise, and how this is implemented at the point of impact (customer intimacy) to transform an organization and support market leadership.

#index 1872279
#* Efficient and domain-invariant competitor mining
#@ Theodoros Lappas;George Valkanas;Dimitrios Gunopulos
#t 2012
#c 0
#% 289148
#% 654480
#% 915250
#% 993954
#% 1035591
#% 1117697
#% 1133032
#% 1328118
#% 1372687
#% 1523827
#% 1531995
#% 1573141
#% 1594611
#% 1614697
#% 1693378
#! In any competitive business, success is based on the ability to make an item more appealing to customers than the competition. A number of questions arise in the context of this task: how do we formalize and quantify the competitiveness relationship between two items? Who are the true competitors of a given item? What are the features of an item that most affect its competitiveness? Despite the impact and relevance of this problem to many domains, only a limited amount of work has been devoted toward an effective solution. In this paper, we present a formal definition of the competitiveness between two items. We present efficient methods for evaluating competitiveness in large datasets and address the natural problem of finding the top-k competitors of a given item. Our methodology is evaluated against strong baselines via a user study and experiments on multiple datasets from different domains.

#index 1872280
#* Discriminative clustering for market segmentation
#@ Peter Haider;Luca Chiarandini;Ulf Brefeld
#t 2012
#c 0
#% 169358
#% 458705
#% 464291
#% 765548
#% 785334
#% 875975
#% 937943
#% 1042787
#% 1074028
#% 1083112
#% 1318706
#% 1366388
#% 1813854
#! We study discriminative clustering for market segmentation tasks. The underlying problem setting resembles discriminative clustering, however, existing approaches focus on the prediction of univariate cluster labels. By contrast, market segments encode complex (future) behavior of the individuals which cannot be represented by a single variable. In this paper, we generalize discriminative clustering to structured and complex output variables that can be represented as graphical models. We devise two novel methods to jointly learn the classifier and the clustering using alternating optimization and collapsed inference, respectively. The two approaches jointly learn a discriminative segmentation of the input space and a generative output prediction model for each segment. We evaluate our methods on segmenting user navigation sequences from Yahoo! News. The proposed collapsed algorithm is observed to outperform baseline approaches such as mixture of experts. We showcase exemplary projections of the resulting segments to display the interpretability of the solutions.

#index 1872281
#* Interacting viruses in networks: can both survive?
#@ Alex Beutel;B. Aditya Prakash;Roni Rosenfeld;Christos Faloutsos
#t 2012
#c 0
#% 205209
#% 309656
#% 324817
#% 342596
#% 577217
#% 577360
#% 729923
#% 754107
#% 868469
#% 991977
#% 1102550
#% 1354567
#% 1407359
#% 1451246
#% 1496777
#% 1535434
#% 1535470
#% 1688538
#% 1746901
#% 1865569
#! Suppose we have two competing ideas/products/viruses, that propagate over a social or other network. Suppose that they are strong/virulent enough, so that each, if left alone, could lead to an epidemic. What will happen when both operate on the network? Earlier models assume that there is perfect competition: if a user buys product 'A' (or gets infected with virus 'X'), she will never buy product 'B' (or virus 'Y'). This is not always true: for example, a user could install and use both Firefox and Google Chrome as browsers. Similarly, one type of flu may give partial immunity against some other similar disease. In the case of full competition, it is known that 'winner takes all,' that is the weaker virus/product will become extinct. In the case of no competition, both viruses survive, ignoring each other. What happens in-between these two extremes? We show that there is a phase transition: if the competition is harsher than a critical level, then 'winner takes all;' otherwise, the weaker virus survives. These are the contributions of this paper (a) the problem definition, which is novel even in epidemiology literature (b) the phase-transition result and (c) experiments on real data, illustrating the suitability of our results.

#index 1872282
#* Aggregating web offers to determine product prices
#@ Rakesh Agrawal;Samuel Ieong
#t 2012
#c 0
#% 266616
#% 278011
#% 577238
#% 724212
#% 788090
#% 857094
#% 870896
#% 891559
#% 913783
#% 1201863
#% 1267781
#% 1272213
#% 1605952
#% 1605958
#% 1641996
#% 1650666
#! Historical prices are important information that can help consumers decide whether the time is right to buy a product. They provide both a context to the users, and facilitate the use of prediction algorithms for forecasting future prices. To produce a representative price history, one needs to consider all offers for the product. However, matching offers to a product is a challenging problem, and mismatches could lead to glaring errors in price history. We propose a principled approach to filter out erroneous matches based on a probabilistic model of prices. We give an efficient algorithm for performing inference that takes advantage of the structure of the problem. We evaluate our results empirically using merchant offers collected from a search engine, and measure the proximity of the price history generated by our approach to the true price history. Our method outperforms alternatives based on robust statistics both in tracking the true price levels and the true price trends.

#index 1872283
#* Mining event periodicity from incomplete observations
#@ Zhenhui Li;Jingjing Wang;Jiawei Han
#t 2012
#c 0
#% 310542
#% 464986
#% 480156
#% 813978
#% 844299
#% 905945
#% 1089789
#% 1451250
#! Advanced technology in GPS and sensors enables us to track physical events, such as human movements and facility usage. Periodicity analysis from the recorded data is an important data mining task which provides useful insights into the physical events and enables us to report outliers and predict future behaviors. To mine periodicity in an event, we have to face real-world challenges of inherently complicated periodic behaviors and imperfect data collection problem. Specifically, the hidden temporal periodic behaviors could be oscillating and noisy, and the observations of the event could be incomplete. In this paper, we propose a novel probabilistic measure for periodicity and design a practical method to detect periods. Our method has thoroughly considered the uncertainties and noises in periodic behaviors and is provably robust to incomplete observations. Comprehensive experiments on both synthetic and real datasets demonstrate the effectiveness of our method.

#index 1872284
#* Towards heterogeneous temporal clinical event pattern discovery: a convolutional approach
#@ Fei Wang;Noah Lee;Jianying Hu;Jimeng Sun;Shahram Ebadollahi
#t 2012
#c 0
#% 329537
#% 662750
#% 729418
#% 778732
#% 793248
#% 958195
#% 992858
#% 1047348
#% 1214735
#% 1275185
#% 1386100
#% 1400711
#% 1403249
#% 1775592
#! Large collections of electronic clinical records today provide us with a vast source of information on medical practice. However, the utilization of those data for exploratory analysis to support clinical decisions is still limited. Extracting useful patterns from such data is particularly challenging because it is longitudinal, sparse and heterogeneous. In this paper, we propose a Nonnegative Matrix Factorization (NMF) based framework using a convolutional approach for open-ended temporal pattern discovery over large collections of clinical records. We call the method One-Sided Convolutional NMF (OSC-NMF). Our framework can mine common as well as individual shift-invariant temporal patterns from heterogeneous events over different patient groups, and handle sparsity as well as scalability problems well. Furthermore, we use an event matrix based representation that can encode quantitatively all key temporal concepts including order, concurrency and synchronicity. We derive efficient multiplicative update rules for OSC-NMF, and also prove theoretically its convergence. Finally, the experimental results on both synthetic and real world electronic patient data are presented to demonstrate the effectiveness of the proposed method.

#index 1872285
#* The long and the short of it: summarising event sequences with serial episodes
#@ Nikolaj Tatti;Jilles Vreeken
#t 2012
#c 0
#% 144520
#% 310515
#% 420063
#% 745515
#% 805093
#% 878207
#% 902449
#% 972338
#% 989612
#% 1176932
#% 1181315
#% 1318693
#% 1346099
#% 1541786
#% 1565634
#% 1606059
#% 1763761
#% 1763763
#% 1815365
#! An ideal outcome of pattern mining is a small set of informative patterns, containing no redundancy or noise, that identifies the key structure of the data at hand. Standard frequent pattern miners do not achieve this goal, as due to the pattern explosion typically very large numbers of highly redundant patterns are returned. We pursue the ideal for sequential data, by employing a pattern set mining approach - an approach where, instead of ranking patterns individually, we consider results as a whole. Pattern set mining has been successfully applied to transactional data, but has been surprisingly understudied for sequential data. In this paper, we employ the MDL principle to identify the set of sequential patterns that summarises the data best. In particular, we formalise how to encode sequential data using sets of serial episodes, and use the encoded length as a quality score. As search strategy, we propose two approaches: the first algorithm selects a good pattern set from a large candidate set, while the second is a parameter-free any-time algorithm that mines pattern sets directly from the data. Experimentation on synthetic and real data demonstrates we efficiently discover small sets of informative patterns.

#index 1872286
#* Efficient event pattern matching with match windows
#@ Bruno Cadonna;Johann Gamper;Michael H. Böhlen
#t 2012
#c 0
#% 654510
#% 726621
#% 763881
#% 838512
#% 993949
#% 1063480
#% 1206571
#% 1217161
#% 1217239
#% 1549839
#% 1688281
#! In event pattern matching a sequence of input events is matched against a complex query pattern that specifies constraints on extent, order, values, and quantification of matching events. In this paper we propose a general pattern matching strategy that consists of a pre-processing step and a pattern matching step. Instead of eagerly matching incoming events, the pre-processing step buffers events in a match window to apply different pruning techniques (filtering, partitioning, and testing for necessary match conditions). In the second step, an event pattern matching algorithm, A, is called only for match windows that satisfy the necessary match conditions. This two-phase strategy with a lazy call of the matching algorithm significantly reduces the number of events that need to be processed by A as well as the number of calls to A. This is important since pattern matching algorithms tend to be expensive in terms of runtime and memory complexity, whereas the pre-processing can be done very efficiently. We conduct extensive experiments using real-world data with pattern matching algorithms for, respectively, automata and join trees. The experimental results confirm the effectiveness of our strategy for both types of pattern matching algorithms.

#index 1872287
#* Optimal exact least squares rank minimization
#@ Shuo Xiang;Yunzhang Zhu;Xiaotong Shen;Jieping Ye
#t 2012
#c 0
#% 187651
#% 983806
#% 1024450
#% 1211747
#% 1309918
#% 1451258
#% 1504249
#% 1556166
#% 1654243
#% 1760863
#! In multivariate analysis, rank minimization emerges when a low-rank structure of matrices is desired as well as a small estimation error. Rank minimization is nonconvex and generally NP-hard, imposing one major challenge. In this paper, we consider a nonconvex least squares formulation, which seeks to minimize the least squares loss function with the rank constraint. Computationally, we develop efficient algorithms to compute a global solution as well as an entire regularization solution path. Theoretically, we show that our method reconstructs the oracle estimator exactly from noisy data. As a result, it recovers the true rank optimally against any method and leads to sharper parameter estimation over its counterpart. Finally, the utility of the proposed method is demonstrated by simulations and image reconstruction from noisy background.

#index 1872288
#* Large-scale distributed non-negative sparse coding and sparse dictionary learning
#@ Vikas Sindhwani;Amol Ghoting
#t 2012
#c 0
#% 722904
#% 793248
#% 963669
#% 1073906
#% 1385969
#% 1386100
#% 1400001
#% 1501267
#% 1523858
#% 1594623
#% 1605950
#% 1615039
#% 1770443
#! We consider the problem of building compact, unsupervised representations of large, high-dimensional, non-negative data using sparse coding and dictionary learning schemes, with an emphasis on executing the algorithm in a Map-Reduce environment. The proposed algorithms may be seen as parallel optimization procedures for constructing sparse non-negative factorizations of large, sparse matrices. Our approach alternates between a parallel sparse coding phase implemented using greedy or convex (l1) regularized risk minimization procedures, and a sequential dictionary learning phase where we solve a set of l0 optimization problems exactly. These two-fold sparsity constraints lead to better statistical performance on text analysis tasks and at the same time make it possible to implement each iteration in a single Map-Reduce job. We detail our implementations and optimizations that lead to the ability to factor matrices with more than 100 million rows and billions of non-zero entries in just a few hours on a small commodity cluster.

#index 1872289
#* Learning binary codes for collaborative filtering
#@ Ke Zhou;Hongyuan Zha
#t 2012
#c 0
#% 249321
#% 251365
#% 724227
#% 724290
#% 762054
#% 813966
#% 956521
#% 1215859
#% 1291600
#% 1305611
#% 1348075
#% 1358747
#% 1400014
#% 1450831
#% 1451210
#% 1451253
#% 1529930
#% 1697462
#! This paper tackles the efficiency problem of making recommendations in the context of large user and item spaces. In particular, we address the problem of learning binary codes for collaborative filtering, which enables us to efficiently make recommendations with time complexity that is independent of the total number of items. We propose to construct binary codes for users and items such that the preference of users over items can be accurately preserved by the Hamming distance between their respective binary codes. By using two loss functions measuring the degree of divergence between the training and predicted ratings, we formulate the problem of learning binary codes as a discrete optimization problem. Although this optimization problem is intractable in general, we develop effective relaxations that can be efficiently solved by existing methods. Moreover, we investigate two methods to obtain the binary codes from the relaxed solutions. Evaluations are conducted on three public-domain data sets and the results suggest that our proposed method outperforms several baseline alternatives.

#index 1872290
#* Low rank modeling of signed networks
#@ Cho-Jui Hsieh;Kai-Yang Chiang;Inderjit S. Dhillon
#t 2012
#c 0
#% 754098
#% 1013626
#% 1013696
#% 1190129
#% 1260273
#% 1309918
#% 1399997
#% 1441070
#% 1504249
#% 1556166
#% 1642048
#! Trust networks, where people leave trust and distrust feedback, are becoming increasingly common. These networks may be regarded as signed graphs, where a positive edge weight captures the degree of trust while a negative edge weight captures the degree of distrust. Analysis of such signed networks has become an increasingly important research topic. One important analysis task is that of sign inference, i.e., infer unknown (or future) trust or distrust relationships given a partially observed signed network. Most state-of-the-art approaches consider the notion of structural balance in signed networks, building inference algorithms based on information about links, triads, and cycles in the network. In this paper, we first show that the notion of weak structural balance in signed networks naturally leads to a global low-rank model for the network. Under such a model, the sign inference problem can be formulated as a low-rank matrix completion problem. We show that we can perfectly recover missing relationships, under certain conditions, using state-of-the-art matrix completion algorithms. We also propose the use of a low-rank matrix factorization approach with generalized loss functions as a practical method for sign inference - this approach yields high accuracy while being scalable to large signed networks, for instance, we show that this analysis can be performed on a synthetic graph with 1.1 million nodes and 120 million edges in 10 minutes. We further show that the low-rank model can be used for other analysis tasks on signed networks, such as user segmentation through signed graph clustering, with theoretical guarantees. Experiments on synthetic as well as real data show that our low rank model substantially improves accuracy of sign inference as well as clustering. As an example, on the largest real dataset available to us (Epinions data with 130K nodes and 840K edges), our matrix factorization approach yields 94.6% accuracy on the sign inference task as compared to 90.8% accuracy using a state-of-the-art cycle-based method - moreover, our method runs in 40 seconds as compared to 10,000 seconds for the cycle-based method.

#index 1872291
#* A structural cluster kernel for learning on graphs
#@ Madeleine Seeland;Andreas Karwath;Stefan Kramer
#t 2012
#c 0
#% 550576
#% 580511
#% 629708
#% 833913
#% 840908
#% 905703
#% 1041263
#% 1472270
#% 1496784
#% 1529085
#% 1617357
#! In recent years, graph kernels have received considerable interest within the machine learning and data mining community. Here, we introduce a novel approach enabling kernel methods to utilize additional information hidden in the structural neighborhood of the graphs under consideration. Our novel structural cluster kernel (SCK) incorporates similarities induced by a structural clustering algorithm to improve state-of-the-art graph kernels. The approach taken is based on the idea that graph similarity can not only be described by the similarity between the graphs themselves, but also by the similarity they possess with respect to their structural neighborhood. We applied our novel kernel in a supervised and a semi-supervised setting to regression and classification problems on a number of real-world datasets of molecular graphs. Our results show that the structural cluster similarity information can indeed leverage the prediction performance of the base kernel, particularly when the dataset is structurally sparse and consequently structurally diverse. By additionally taking into account a large number of unlabeled instances the performance of the structural cluster kernel can further be improved.

#index 1872292
#* Multi-label hypothesis reuse
#@ Sheng-Jun Huang;Yang Yu;Zhi-Hua Zhou
#t 2012
#c 0
#% 235377
#% 302391
#% 311034
#% 344447
#% 783478
#% 837668
#% 838412
#% 840928
#% 851951
#% 875967
#% 950571
#% 989655
#% 1083666
#% 1083698
#% 1274865
#% 1301004
#% 1388992
#% 1451240
#% 1647350
#% 1647889
#! Multi-label learning arises in many real-world tasks where an object is naturally associated with multiple concepts. It is well-accepted that, in order to achieve a good performance, the relationship among labels should be exploited. Most existing approaches require the label relationship as prior knowledge, or exploit by counting the label co-occurrence. In this paper, we propose the MAHR approach, which is able to automatically discover and exploit label relationship. Our basic idea is that, if two labels are related, the hypothesis generated for one label can be helpful for the other label. MAHR implements the idea as a boosting approach with a hypothesis reuse mechanism. In each boosting round, the base learner for a label is generated by not only learning on its own task but also reusing the hypotheses from other labels, and the amount of reuse across labels provides an estimate of the label relationship. Extensive experimental results validate that MAHR is able to achieve superior performance and discover reasonable label relationship. Moreover, we disclose that the label relationship is usually asymmetric.

#index 1872293
#* Rank-loss support instance machines for MIML instance annotation
#@ Forrest Briggs;Xiaoli Z. Fern;Raviv Raich
#t 2012
#c 0
#% 92546
#% 224755
#% 757953
#% 836905
#% 983905
#% 1117691
#% 1176978
#% 1261427
#% 1305521
#% 1535443
#% 1558464
#% 1606372
#% 1647350
#! Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose Rank-Loss Support Instance Machines, which optimize a regularized rank-loss objective and can be instantiated with different aggregation models connecting instance-level predictions with bag-level predictions. The aggregation models that we consider are equivalent to defining a "support instance" for each bag, which allows efficient optimization of the rank-loss objective using primal sub-gradient descent. Experiments on artificial and real-world datasets show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification.

#index 1872294
#* Inductive multi-task learning with multiple view data
#@ Jintao Zhang;Jun Huan
#t 2012
#c 0
#% 252011
#% 316509
#% 420495
#% 464457
#% 769886
#% 815908
#% 916788
#% 1073994
#% 1100081
#% 1292880
#% 1451258
#% 1551196
#% 1705508
#! In many real-world applications, it is becoming common to have data extracted from multiple diverse sources, known as "multi-view" data. Multi-view learning (MVL) has been widely studied in many applications, but existing MVL methods learn a single task individually. In this paper, we study a new direction of multi-view learning where there are multiple related tasks with multi-view data (i.e. multi-view multi-task learning, or MVMT Learning). In our MVMT learning methods, we learn a linear mapping for each view in each task. In a single task, we use co-regularization to obtain functions that are in-agreement with each other on the unlabeled samples and achieve low classification errors on the labeled samples simultaneously. Cross different tasks, additional regularization functions are utilized to ensure the functions that we learn in each view are similar. We also developed two extensions of the MVMT learning algorithm. One extension handles missing views and the other handles non-uniformly related tasks. Experimental studies on three real-world data sets demonstrate that our MVMT methods significantly outperform the existing state-of-the-art methods.

#index 1872295
#* Scalable misbehavior detection in online video chat services
#@ Xinyu Xing;Yu-li Liang;Sui Huang;Hanqiang Cheng;Richard Han;Qin Lv;Xue Liu;Shivakant Mishra;Yi Zhu
#t 2012
#c 0
#% 429731
#% 481290
#% 635689
#% 760805
#% 845226
#% 939122
#% 954849
#% 975161
#% 1279843
#% 1464094
#% 1560423
#! The need for highly scalable and accurate detection and filtering of misbehaving users and obscene content in online video chat services has grown as the popularity of these services has exploded in popularity. This is a challenging problem because processing large amounts of video is compute intensive, decisions about whether a user is misbehaving or not must be made online and quickly, and moreover these video chats are characterized by low quality video, poorly lit scenes, diversity of users and their behaviors, diversity of the content, and typically short sessions. This paper presents EMeralD, a highly scalable system for accurately detecting and filtering misbehaving users in online video chat applications. EMeralD substantially improves upon the state-of-the-art filtering mechanisms by achieving much lower computational cost and higher accuracy. We demonstrate EMeralD's improvement via experimental evaluations on real-world data sets obtained from Chatroulette.com.

#index 1872296
#* Keyword-propagation-based information enriching and noise removal for web news videos
#@ Jun Zhang;Xiaoming Fan;Jianyong Wang;Lizhu Zhou
#t 2012
#c 0
#% 262042
#% 262043
#% 307247
#% 445316
#% 577328
#% 643016
#% 765552
#% 780688
#% 801611
#% 839839
#% 876068
#% 961278
#% 989601
#% 990300
#% 997240
#% 1055717
#% 1055773
#% 1130845
#! The volume of Web videos have increased sharply through the past several years because of the evolvement of Web video sites.Enhanced algorithms on retrieval, classification and TDT (abbreviation of Topic Detection and Tracking) can bring lots of convenience to Web users as well as release tedious work from the administrators. Nevertheless, due to the the insufficiency of annotation keywords and the gap between video features and semantic concepts, it is still far away from satisfactory to implement them based on initial keywords and visual features. In this paper we utilize a keyword propagation algorithm based on manifold structure to enrich the keyword information and remove the noise for videos. Both text similarity and temporal similarity are employed to explore the relationship between any pair of videos and to construct the propagation model. We explore three applications, i.e., TDT, Retrieval and Classification based on a Web news video dataset obtained from a famous online video-distributing website, YouKu, and evaluate our approach. Experimental results demonstrate that they achieve satisfactory performance and always outperform the baseline methods.

#index 1872297
#* Harnessing the wisdom of the crowds for accurate web page clipping
#@ Lei Zhang;Linpeng Tang;Ping Luo;Enhong Chen;Limei Jiao;Min Wang;Guiquan Liu
#t 2012
#c 0
#% 152934
#% 300120
#% 464873
#% 464989
#% 465003
#% 729418
#% 769889
#% 800181
#% 818233
#% 1183079
#% 1190152
#% 1214756
#% 1252649
#% 1472858
#% 1605997
#! Clipping Web pages, namely extracting the informative clips (areas) from Web pages, has many applications, such as Web printing and e-reading on small handheld devices. Although many existing methods attempt to address this task, most of them can either work only on certain types of Web pages (e.g., news- and blog-like web pages), or perform semi-automatically where extra user efforts are required in adjusting the outputs. The problem of clipping any types of Web pages accurately in a totally automatic way remains pretty much open. To this end in this study we harness the wisdom of the crowds to provide accurate recommendation of informative clips on any given Web pages. Specifically, we leverage the knowledge on how previous users clip similar Web pages, and this knowledge repository can be represented as a transaction database where each transaction contains the clips selected by a user on a certain Web page. Then, we formulate a new pattern mining problem, mining top-1 qualified pattern, on transaction database for this recommendation. Here, the recommendation considers not only the pattern support but also the pattern occupancy (proposed in this work). High support requires that patterns appear frequently in the database, while high occupancy requires that patterns occupy a large portion of the transactions they appear in. Thus, it leads to both precise and complete recommendation. Additionally, we explore the properties on occupancy to further prune the search space for high-efficient pattern mining. Finally, we show the effectiveness of the proposed algorithm on a human-labeled ground truth dataset consisting of 2000 web pages from 100 major Web sites, and demonstrate its efficiency on large synthetic datasets.

#index 1872298
#* Bootstrapped language identification for multi-site internet domains
#@ Uwe F. Mayer
#t 2012
#c 0
#% 1192961
#! We present an algorithm for language identification, in particular of short documents, for the case of an Internet domain with sites in multiple countries with differing languages. The algorithm is significantly faster than standard language identification methods, while providing state-of-the-art identification. We bootstrap the algorithm based on the language identification based on the site alone, a methodology suitable for any supervised language identification algorithm. We demonstrate the bootstrapping and algorithm on eBay email data and on Twitter status updates data. The algorithm is deployed at eBay as part of the back-office development data repository.

#index 1872299
#* Semantic search and a new moore's law effect in knowledge engineering
#@ Wei-Ying Ma
#t 2012
#c 0
#! In history, the Moore's law effect has been used to describe phenomena of exponential improvement in technology when it has a virtuous cycle that makes technology improvement proportional to technology itself. For example, chip performance had doubled every 18-24 months because better processors support the development of better layout tools that support the development of even better processors. I will describe a new Moore's law effect that is being created in knowledge engineering and is driven by the self-reinforcing nature of three trends and technical advancements: big data, machine learning, and Internet economics. I will explain how we can take advantage of this new effect to develop a new generation of semantic and knowledge-based search engines. Specifically, my presentation will cover the following three areas: Knowledge acquisition - our goal is to build a comprehensive entity graph and knowledge graph to complement the web and social graphs. I will introduce techniques for entity extraction and knowledgebase construction through automatic and interactive mining and crowdsourcing. Managing knowledge - our goal is to support advanced analytical queries by combining probabilistic knowledge with a distributed platform. I will focus on technology for both online knowledge serving and offline knowledge inference. Knowledge-empowered search and applications - the knowledge we have acquired and curated enables applications like query understanding, entity-centric search experiences, and answers to natural language queries.

#index 1872300
#* Key lessons learned building recommender systems for large-scale social networks
#@ Christian Posse
#t 2012
#c 0
#! By helping members to connect, discover and share relevant content or find a new career opportunity, recommender systems have become a critical component of user growth and engagement for social networks. The multidimensional nature of engagement and diversity of members on large-scale social networks have generated new infrastructure and modeling challenges and opportunities in the development, deployment and operation of recommender systems. This presentation will address some of these issues, focusing on the modeling side for which new research is much needed while describing a recommendation platform that enables real-time recommendation updates at scale as well as batch computations, and cross-leverage between different product recommendations. Topics covered on the modeling side will include optimizing for multiple competing objectives, solving contradicting business goals, modeling user intent and interest to maximize placement and timeliness of the recommendations, utility metrics beyond CTR that leverage both real-time tracking of explicit and implicit user feedback, gathering training data for new product recommendations, virality preserving online testing and virtual profiling.

#index 1872301
#* Magnet community identification on social networks
#@ Guan Wang;Yuchen Zhao;Xiaoxiao Shi;Philip S. Yu
#t 2012
#c 0
#% 290830
#% 411762
#% 466891
#% 577224
#% 577338
#% 757953
#% 875948
#% 881457
#% 989640
#% 989643
#% 1055740
#% 1098204
#% 1214629
#% 1280760
#% 1536563
#% 1538537
#% 1605923
#! Social communities connect people of similar interests together and play essential roles in social network applications. Examples of such communities include people who like the same objects on Facebook, follow common subjects on Twitter, or join similar groups on LinkedIn. Among communities, we notice that some of them are {\em magnetic} to people. A {\em magnet community} is such a community that attracts significantly more people's interests and attentions than other communities of similar topics. With the explosive number of self-formed communities in social networks, one important demand is to identify magnet communities for users. This can not only track attractive communities, but also help improve user experiences and increase their engagements, e.g., the login frequencies and user-generated-content qualities. In this paper, we initiate the study of magnet community identification problem. First we observe several properties of magnet communities, such as attention flow, attention qualify, and attention persistence. Second, we formalize these properties with the combination of community feature extraction into a graph ranking formulation based on constraint quadratic programming. In details, we treat communities of a network as super nodes, and their interactions as links among those super nodes. Therefore, a network of communities is defined. We extract community's magnet features from heterogeneous sources, i.e., a community's standalone features and its dependency features with other communities. A graph ranking model is formulated given these features. Furthermore, we define constraints reflecting communities' magnet properties to regularize the model. We demonstrate the effectiveness of our framework on real world social network data.

#index 1872302
#* Vertex neighborhoods, low conductance cuts, and good seeds for local community methods
#@ David F. Gleich;C. Seshadhri
#t 2012
#c 0
#% 148021
#% 274612
#% 283833
#% 313959
#% 433981
#% 754117
#% 755402
#% 869485
#% 881460
#% 898311
#% 937549
#% 1055741
#% 1071520
#% 1183359
#% 1245882
#% 1399993
#% 1399996
#% 1506251
#% 1560413
#% 1710593
#% 1835483
#! The communities of a social network are sets of vertices with more connections inside the set than outside. We theoretically demonstrate that two commonly observed properties of social networks, heavy-tailed degree distributions and large clustering coefficients, imply the existence of vertex neighborhoods (also known as egonets) that are themselves good communities. We evaluate these neighborhood communities on a range of graphs. What we find is that the neighborhood communities can exhibit conductance scores that are as good as the Fiedler cut. Also, the conductance of neighborhood communities shows similar behavior as the network community profile computed with a personalized PageRank community detection method. Neighborhood communities give us a simple and powerful heuristic for speeding up local partitioning methods. Since finding good seeds for the PageRank clustering method is difficult, most approaches involve an expensive sweep over a great many starting vertices. We show how to use neighborhood communities to quickly generate a small set of seeds.

#index 1872303
#* Overlapping community detection via bounded nonnegative matrix tri-factorization
#@ Yu Zhang;Dit-Yan Yeung
#t 2012
#c 0
#% 881468
#% 891559
#% 995168
#% 1565432
#% 1606047
#! Complex networks are ubiquitous in our daily life, with the World Wide Web, social networks, and academic citation networks being some of the common examples. It is well understood that modeling and understanding the network structure is of crucial importance to revealing the network functions. One important problem, known as community detection, is to detect and extract the community structure of networks. More recently, the focus in this research topic has been switched to the detection of overlapping communities. In this paper, based on the matrix factorization approach, we propose a method called bounded nonnegative matrix tri-factorization (BNMTF). Using three factors in the factorization, we can explicitly model and learn the community membership of each node as well as the interaction among communities. Based on a unified formulation for both directed and undirected networks, the optimization problem underlying BNMTF can use either the squared loss or the generalized KL-divergence as its loss function. In addition, to address the sparsity problem as a result of missing edges, we also propose another setting in which the loss function is defined only on the observed edges. We report some experiments on real-world datasets to demonstrate the superiority of BNMTF over other related matrix factorization methods.

#index 1872304
#* DEMON: a local-first discovery method for overlapping communities
#@ Michele Coscia;Giulio Rossetti;Fosca Giannotti;Dino Pedreschi
#t 2012
#c 0
#% 949164
#% 1023420
#% 1108891
#% 1117074
#% 1206838
#% 1560413
#% 1560414
#% 1560428
#% 1628678
#! Community discovery in complex networks is an interesting problem with a number of applications, especially in the knowledge extraction task in social and information networks. However, many large networks often lack a particular community organization at a global level. In these cases, traditional graph partitioning algorithms fail to let the latent knowledge embedded in modular structure emerge, because they impose a top-down global view of a network. We propose here a simple local-first approach to community discovery, able to unveil the modular organization of real complex networks. This is achieved by democratically letting each node vote for the communities it sees surrounding it in its limited view of the global system, i.e. its ego neighborhood, using a label propagation algorithm; finally, the local communities are merged into a global collection. We tested this intuition against the state-of-the-art overlapping and non-overlapping community discovery methods, and found that our new method clearly outperforms the others in the quality of the obtained communities, evaluated by using the extracted communities to predict the metadata about the nodes of several real world networks. We also show how our method is deterministic, fully incremental, and has a limited time complexity, so that it can be used on web-scale real networks.

#index 1872305
#* On the separability of structural classes of communities
#@ Bruno Abrahao;Sucheta Soundarajan;John Hopcroft;Robert Kleinberg
#t 2012
#c 0
#% 92533
#% 274612
#% 278253
#% 868469
#% 881460
#% 1055741
#% 1085750
#% 1130291
#% 1355041
#% 1399996
#% 1860542
#! Three major factors govern the intricacies of community extraction in networks: (1) the application domain includes a wide variety of networks of fundamentally different natures, (2) the literature offers a multitude of disparate community detection algorithms, and (3) there is no consensus characterizing how to discriminate communities from non-communities. In this paper, we present a comprehensive analysis of community properties through a class separability framework. Our approach enables the assessement of the structural dissimilarity among the output of multiple community detection algorithms and between the output of algorithms and communities that arise in practice. To demostrate this concept, we furnish our method with a large set of structural properties and multiple community detection algorithms. Applied to a diverse collection of large scale network datasets, the analysis reveals that (1) the different detection algorithms extract fundamentally different structures; (2) the structure of communities that arise in practice is closest to that of communities that random-walk-based algorithms extract, although still siginificantly different from that of the output of all the algorithms; and (3) a small subset of the properties are nearly as discriminative as the full set, while making explicit the ways in which the algorithms produce biases. Our framework enables an informed choice of the most suitable community detection method for a given purpose and network and allows for a comparison of existing community detection algorithms while guiding the design of new ones.

#index 1872306
#* Discovering lag intervals for temporal dependencies
#@ Liang Tang;Tao Li;Larisa Shwartz
#t 2012
#c 0
#% 194756
#% 252206
#% 310559
#% 420063
#% 459006
#% 464986
#% 464996
#% 466476
#% 481290
#% 559688
#% 577256
#% 577275
#% 727902
#% 785405
#% 799764
#% 823418
#% 832572
#% 835018
#% 881532
#% 989612
#% 1328157
#% 1428694
#% 1451250
#% 1468578
#% 1535223
#! Time lag is a key feature of hidden temporal dependencies within sequential data. In many real-world applications, time lag plays an essential role in interpreting the cause of discovered temporal dependencies. Traditional temporal mining methods either use a predefined time window to analyze the item sequence, or employ statistical techniques to simply derive the time dependencies among items. Such paradigms cannot effectively handle varied data with special properties, e.g., the interleaved temporal dependencies. In this paper, we study the problem of finding lag intervals for temporal dependency analysis. We first investigate the correlations between the temporal dependencies and other temporal patterns, and then propose a generalized framework to resolve the problem. By utilizing the sorted table in representing time lags among items, the proposed algorithm achieves an elegant balance between the time cost and the space cost. Extensive empirical evaluation on both synthetic and real data sets demonstrates the efficiency and effectiveness of our proposed algorithm in finding the temporal dependencies with lag intervals in sequential data.

#index 1872307
#* Testing the significance of spatio-temporal teleconnection patterns
#@ Jaya Kawale;Snigdhansu Chatterjee;Dominick Ormsby;Karsten Steinhaeuser;Stefan Liess;Vipin Kumar
#t 2012
#c 0
#% 14749
#% 629629
#% 729954
#% 1001365
#% 1663691
#! Dipoles represent long distance connections between the pressure anomalies of two distant regions that are negatively correlated with each other. Such dipoles have proven important for understanding and explaining the variability in climate in many regions of the world, e.g., the El Nino climate phenomenon is known to be responsible for precipitation and temperature anomalies over large parts of the world. Systematic approaches for dipole detection generate a large number of candidate dipoles, but there exists no method to evaluate the significance of the candidate teleconnections. In this paper, we present a novel method for testing the statistical significance of the class of spatio-temporal teleconnection patterns called as dipoles. One of the most important challenges in addressing significance testing in a spatio-temporal context is how to address the spatial and temporal dependencies that show up as high autocorrelation. We present a novel approach that uses the wild bootstrap to capture the spatio-temporal dependencies, in the special use case of teleconnections in climate data. Our approach to find the statistical significance takes into account the autocorrelation, the seasonality and the trend in the time series over a period of time. This framework is applicable to other problems in spatio-temporal data mining to assess the significance of the patterns.

#index 1872308
#* SeqiBloc: mining multi-time spanning blockmodels in dynamic graphs
#@ Jeffrey Chan;Wei Liu;Christopher Leckie;James Bailey;Kotagiri Ramamohanarao
#t 2012
#c 0
#% 848218
#% 858102
#% 878207
#% 881514
#% 989586
#% 989640
#% 1117695
#% 1506205
#% 1538537
#% 1787250
#! Blockmodelling is an important technique for decomposing graphs into sets of roles. Vertices playing the same role have similar patterns of interactions with vertices in other roles. These roles, along with the role to role interactions, can succinctly summarise the underlying structure of the studied graphs. As the underlying graphs evolve with time, it is important to study how their blockmodels evolve too. This will enable us to detect role changes across time, detect different patterns of interactions, for example, weekday and weekend behaviour, and allow us to study how the structure in the underlying dynamic graph evolves. To date, there has been limited research on studying dynamic blockmodels. They focus on smoothing role changes between adjacent time instances. However, this approach can overfit during stationary periods where the underling structure does not change but there is random noise in the graph. Therefore, an approach to a) find blockmodels across spans of time and b) to find the stationary periods is needed. In this paper, we propose an information theoretic framework, SeqiBloc, combined with a change point detection approach to achieve a) and b). In addition, we propose new vertex equivalence definitions that include time, and show how they relate back to our information theoretic approach. We demonstrate their usefulness and superior accuracy over existing work on synthetic and real datasets.

#index 1872309
#* USpan: an efficient algorithm for mining high utility sequential patterns
#@ Junfu Yin;Zhigang Zheng;Longbing Cao
#t 2012
#c 0
#% 329537
#% 463903
#% 464996
#% 577256
#% 945869
#% 1019450
#% 1327654
#% 1368909
#% 1442076
#% 1451164
#% 1463509
#% 1587711
#% 1707858
#! Sequential pattern mining plays an important role in many applications, such as bioinformatics and consumer behavior analysis. However, the classic frequency-based framework often leads to many patterns being identified, most of which are not informative enough for business decision-making. In frequent pattern mining, a recent effort has been to incorporate utility into the pattern selection framework, so that high utility (frequent or infrequent) patterns are mined which address typical business concerns such as dollar value associated with each pattern. In this paper, we incorporate utility into sequential pattern mining, and a generic framework for high utility sequence mining is defined. An efficient algorithm, USpan, is presented to mine for high utility sequential patterns. In USpan, we introduce the lexicographic quantitative sequence tree to extract the complete set of high utility sequences and design concatenation mechanisms for calculating the utility of a node and its children with two effective pruning strategies. Substantial experiments on both synthetic and real datasets show that USpan efficiently identifies high utility sequences from large scale data with very low minimum utility.

#index 1872310
#* Mining large-scale, sparse GPS traces for map inference: comparison of approaches
#@ Xuemei Liu;James Biagioni;Jakob Eriksson;Yin Wang;George Forman;Yanmin Zhu
#t 2012
#c 0
#% 574305
#% 647499
#% 754413
#% 902370
#% 1065024
#% 1236216
#% 1245609
#% 1298860
#% 1298894
#% 1298896
#% 1426598
#% 1484170
#% 1505162
#% 1589884
#% 1774974
#! We address the problem of inferring road maps from large-scale GPS traces that have relatively low resolution and sampling frequency. Unlike past published work that requires high-resolution traces with dense sampling, we focus on situations with coarse granularity data, such as that obtained from thousands of taxis in Shanghai, which transmit their location as seldom as once per minute. Such data sources can be made available inexpensively as byproducts of existing processes, rather than having to drive every road with high-quality GPS instrumentation just for map building - and having to re-drive roads for periodic updates. Although the challenges in using opportunistic probe data are significant, successful mining algorithms could potentially enable the creation of continuously updated maps at very low cost. In this paper, we compare representative algorithms from two approaches: working with individual reported locations vs. segments between consecutive locations. We assess their trade-offs and effectiveness in both qualitative and quantitative comparisons for regions of Shanghai and Chicago.

#index 1872311
#* Transparent user models for personalization
#@ Khalid El-Arini;Ulrich Paquet;Ralf Herbrich;Jurgen Van Gael;Blaise Agüera y Arcas
#t 2012
#c 0
#% 722904
#% 1260273
#% 1338553
#% 1523858
#% 1769265
#% 1826250
#% 1873672
#! Personalization is a ubiquitous phenomenon in our daily online experience. While such technology is critical for helping us combat the overload of information we face, in many cases, we may not even realize that our results are being tailored to our personal tastes and preferences. Worse yet, when such a system makes a mistake, we have little recourse to correct it. In this work, we propose a framework for addressing this problem by developing a new user-interpretable feature set upon which to base personalized recommendations. These features, which we call badges, represent fundamental traits of users (e.g., "vegetarian" or "Apple fanboy") inferred by modeling the interplay between a user's behavior and self-reported identity. Specifically, we consider the microblogging site Twitter, where users provide short descriptions of themselves in their profiles, as well as perform actions such as tweeting and retweeting. Our approach is based on the insight that we can define badges using high precision, low recall rules (e.g., "Twitter profile contains the phrase 'Apple fanboy'"), and with enough data, generalize to other users by observing shared behavior. We develop a fully Bayesian, generative model that describes this interaction, while allowing us to avoid the pitfalls associated with having positive-only data. Experiments on real Twitter data demonstrate the effectiveness of our model at capturing rich and interpretable user traits that can be used to provide transparency for personalization.

#index 1872312
#* Estimating entity importance via counting set covers
#@ Aristides Gionis;Theodoros Lappas;Evimaria Terzi
#t 2012
#c 0
#% 190611
#% 197754
#% 341672
#% 727835
#% 729923
#% 756494
#% 769876
#% 873496
#% 891559
#% 917863
#% 1054889
#% 1178476
#% 1214668
#% 1250237
#% 1278844
#% 1482238
#% 1495595
#% 1605931
#% 1617316
#! The data-mining literature is rich in problems asking to assess the importance of entities in a given dataset. At a high level, existing work identifies important entities either by ranking or by selection. Ranking methods assign a score to every entity in the population, and then use the assigned scores to create a ranked list. The major shortcoming of such approaches is that they ignore the redundancy between high-ranked entities, which may in fact be very similar or even identical. Therefore, in scenarios where diversity is desirable, such methods perform poorly. Selection methods overcome this drawback by evaluating the importance of a group of entities collectively. To achieve this, they typically adopt a set-cover formulation, which identifies the entities in the minimum set cover as the important ones. However, this dichotomy of entities conceals the fact that, even though an entity may not be in the reported cover, it may still participate in many other optimal or near-optimal solutions. In this paper, we propose a framework that overcomes the above drawbacks by integrating the ranking and selection paradigms. Our approach assigns importance scores to entities based on both the number and the quality of set-cover solutions that they participate. Our algorithmic contribution lies with the design of an efficient algorithm for approximating the number of high-quality set covers that each entity participates. Our methodology applies to a wide range of applications. In a user study and an experimental evaluation on real data, we demonstrate that our framework is efficient and provides useful and intuitive results.

#index 1872313
#* ComSoc: adaptive transfer of user behaviors over composite social network
#@ Erheng Zhong;Wei Fan;Junwei Wang;Lei Xiao;Yong Li
#t 2012
#c 0
#% 330687
#% 722760
#% 722904
#% 730089
#% 1023420
#% 1055741
#% 1083671
#% 1083684
#% 1083687
#% 1214641
#% 1214724
#% 1399996
#% 1400031
#% 1464068
#% 1495568
#% 1496805
#% 1523858
#% 1536533
#% 1605963
#% 1676017
#! Accurate prediction of user behaviors is important for many social media applications, including social marketing, personalization and recommendation, etc. A major challenge lies in that, the available behavior data or interactions between users and items in a given social network are usually very limited and sparse (e.g., = 99.9% empty). Many previous works model user behavior from only historical user logs. We observe that many people are members of several social networks in the same time, such as Facebook, Twitter and Tencent's QQ. Importantly, their behaviors and interests in different networks influence one another. This gives us an opportunity to leverage the knowledge of user behaviors in different networks, in order to alleviate the data sparsity problem, and enhance the predictive performance of user modeling. Combining different networks "simply and naively" does not work well. Instead, we formulate the problem to model multiple networks as "composite network knowledge transfer". We first select the most suitable networks inside a composite social network via a hierarchical Bayesian model, parameterized for individual users, and then build topic models for user behavior prediction using both the relationships in the selected networks and related behavior data. To handle big data, we have implemented the algorithm using Map/Reduce. We demonstrate that the proposed composite network-based user behavior model significantly improve the predictive accuracy over a number of existing approaches on several real world applications, such as a very large social-networking dataset from Tencent Inc.

#index 1872314
#* Online learning to diversify from implicit feedback
#@ Karthik Raman;Pannaga Shivaswamy;Thorsten Joachims
#t 2012
#c 0
#% 262112
#% 642975
#% 763708
#% 879618
#% 946521
#% 1073970
#% 1074025
#% 1166473
#% 1263586
#% 1312812
#% 1482296
#% 1605962
#% 1641948
#! In order to minimize redundancy and optimize coverage of multiple user interests, search engines and recommender systems aim to diversify their set of results. To date, these diversification mechanisms are largely hand-coded or relied on expensive training data provided by experts. To overcome this problem, we propose an online learning model and algorithms for learning diversified recommendations and retrieval functions from implicit feedback. In our model, the learning algorithm presents a ranking to the user at each step, and uses the set of documents from the presented ranking, which the user reads, as feedback. Even for imperfect and noisy feedback, we show that the algorithms admit theoretical guarantees for maximizing any submodular utility measure under approximately rational user behavior. In addition to the theoretical results, we find that the algorithm learns quickly, accurately, and robustly in empirical evaluations on two datasets.

#index 1872315
#* Playlist prediction via metric embedding
#@ Shuo Chen;Josh L. Moore;Douglas Turnbull;Thorsten Joachims
#t 2012
#c 0
#% 935763
#% 1074025
#% 1260273
#% 1375780
#% 1400014
#% 1400035
#% 1605963
#% 1746798
#% 1792875
#! Digital storage of personal music collections and cloud-based music services (e.g. Pandora, Spotify) have fundamentally changed how music is consumed. In particular, automatically generated playlists have become an important mode of accessing large music collections. The key goal of automated playlist generation is to provide the user with a coherent listening experience. In this paper, we present Latent Markov Embedding (LME), a machine learning algorithm for generating such playlists. In analogy to matrix factorization methods for collaborative filtering, the algorithm does not require songs to be described by features a priori, but it learns a representation from example playlists. We formulate this problem as a regularized maximum-likelihood embedding of Markov chains in Euclidian space, and show how the resulting optimization problem can be solved efficiently. An empirical evaluation shows that the LME is substantially more accurate than adaptations of smoothed n-gram models commonly used in natural language processing.

#index 1872316
#* Parallel field ranking
#@ Ming Ji;Binbin Lin;Xiaofei He;Deng Cai;Jiawei Han
#t 2012
#c 0
#% 8153
#% 290830
#% 341269
#% 442110
#% 732522
#% 780688
#% 875948
#% 905203
#% 1077150
#% 1164191
#% 1190090
#% 1227644
#% 1275220
#% 1558464
#% 1598386
#% 1605923
#% 1606026
#% 1775495
#! Recently, ranking data with respect to the intrinsic geometric structure (manifold ranking) has received considerable attentions, with encouraging performance in many applications in pattern recognition, information retrieval and recommendation systems. Most of the existing manifold ranking methods focus on learning a ranking function that varies smoothly along the data manifold. However, beyond smoothness, a desirable ranking function should vary monotonically along the geodesics of the data manifold, such that the ranking order along the geodesics is preserved. In this paper, we aim to learn a ranking function that varies linearly and therefore monotonically along the geodesics of the data manifold. Recent theoretical work shows that the gradient field of a linear function on the manifold has to be a parallel vector field. Therefore, we propose a novel ranking algorithm on the data manifolds, called Parallel Field Ranking. Specifically, we try to learn a ranking function and a vector field simultaneously. We require the vector field to be close to the gradient field of the ranking function, and the vector field to be as parallel as possible. Moreover, we require the value of the ranking function at the query point to be the highest, and then decrease linearly along the manifold. Experimental results on both synthetic data and real data demonstrate the effectiveness of our proposed algorithm.

#index 1872317
#* Semi-supervised learning with mixed knowledge information
#@ Fanhua Shang;L.C. Jiao;Fei Wang
#t 2012
#c 0
#% 224113
#% 336073
#% 464608
#% 466263
#% 466890
#% 769881
#% 770782
#% 840892
#% 852097
#% 881474
#% 961218
#% 983830
#% 983849
#% 1034714
#% 1073944
#% 1305473
#% 1309918
#% 1355177
#% 1455666
#% 1504249
#% 1525164
#% 1535296
#% 1558464
#% 1581622
#% 1585056
#% 1606361
#% 1606366
#% 1688549
#! Integrating new knowledge sources into various learning tasks to improve their performance has recently become an interesting topic. In this paper we propose a novel semi-supervised learning (SSL) approach, called semi-supervised learning with Mixed Knowledge Information (SSL-MKI) which can simultaneously handle both sparse labeled data and additional pairwise constraints together with unlabeled data. Specifically, we first construct a unified SSL framework to combine the manifold assumption and the pairwise constraints assumption for classification tasks. Then we present a Modified Fixed Point Continuation (MFPC) algorithm with an eigenvalue thresholding (EVT) operator to learn the enhanced kernel matrix. Finally, we develop a two-stage optimization strategy and provide an efficient SSL approach that takes advantage of Laplacian spectral regularization: semi-supervised learning with Enhanced Spectral Kernel (ESK). Experimental results on a variety of synthetic and real-world datasets demonstrate the effectiveness of the proposed ESK approach.

#index 1872318
#* Batch mode active sampling based on marginal probability distribution matching
#@ Rita Chattopadhyay;Zheng Wang;Wei Fan;Ian Davidson;Sethuraman Panchanathan;Jieping Ye
#t 2012
#c 0
#% 116165
#% 236729
#% 290482
#% 466419
#% 466576
#% 722797
#% 722798
#% 875997
#% 876080
#% 906248
#% 1305479
#% 1318754
#% 1377382
#% 1385982
#% 1464068
#% 1472280
#% 1855337
#! Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data; it is especially useful when there are large amount of unlabeled data and labeling them is expensive. Recently, batch-mode active learning, where a set of samples are selected concurrently for labeling, based on their collective merit, has attracted a lot of attention. The objective of batch-mode active learning is to select a set of informative samples so that a classifier learned on these samples has good generalization performance on the unlabeled data. Most of the existing batch-mode active learning methodologies try to achieve this by selecting samples based on varied criteria. In this paper we propose a novel criterion which achieves good generalization performance of a classifier by specifically selecting a set of query samples that minimizes the difference in distribution between the labeled and the unlabeled data, after annotation. We explicitly measure this difference based on all candidate subsets of the unlabeled data and select the best subset. The proposed objective is an NP-hard integer programming optimization problem. We provide two optimization techniques to solve this problem. In the first one, the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem. Our empirical studies using publicly available UCI datasets and a biomedical image dataset demonstrate the effectiveness of the proposed approach in comparison with the state-of-the-art batch-mode active learning methods. We also present two extensions of the proposed approach, which incorporate uncertainty of the predicted labels of the unlabeled data and transfer learning in the proposed formulation. Our empirical studies on UCI datasets show that incorporation of uncertainty information improves performance at later iterations while our studies on 20 Newsgroups dataset show that transfer learning improves the performance of the classifier during initial iterations.

#index 1872319
#* SPF-GMKL: generalized multiple kernel learning with a million kernels
#@ Ashesh Jain;S.V.N. Vishwanathan;Manik Varma
#t 2012
#c 0
#% 9795
#% 416695
#% 416706
#% 425040
#% 761311
#% 763697
#% 770846
#% 829029
#% 961190
#% 983953
#% 1073916
#% 1074363
#% 1083635
#% 1211823
#% 1551234
#% 1558464
#% 1861629
#! Multiple Kernel Learning (MKL) aims to learn the kernel in an SVM from training data. Many MKL formulations have been proposed and some have proved effective in certain applications. Nevertheless, as MKL is a nascent field, many more formulations need to be developed to generalize across domains and meet the challenges of real world applications. However, each MKL formulation typically necessitates the development of a specialized optimization algorithm. The lack of an efficient, general purpose optimizer capable of handling a wide range of formulations presents a significant challenge to those looking to take MKL out of the lab and into the real world. This problem was somewhat alleviated by the development of the Generalized Multiple Kernel Learning (GMKL) formulation which admits fairly general kernel parameterizations and regularizers subject to mild constraints. However, the projected gradient descent GMKL optimizer is inefficient as the computation of the step size and a reasonably accurate objective function value or gradient direction are all expensive. We overcome these limitations by developing a Spectral Projected Gradient (SPG) descent optimizer which: a) takes into account second order information in selecting step sizes; b) employs a non-monotone step size selection criterion requiring fewer function evaluations; c) is robust to gradient noise, and d) can take quick steps when far away from the optimum. We show that our proposed SPG-GMKL optimizer can be an order of magnitude faster than projected gradient descent on even small and medium sized datasets. In some cases, SPG-GMKL can even outperform state-of-the-art specialized optimization algorithms developed for a single MKL formulation. Furthermore, we demonstrate that SPG-GMKL can scale well beyond gradient descent to large problems involving a million kernels or half a million data points. Our code and implementation are available publically.

#index 1872320
#* Efficient evaluation of large sequence kernels
#@ Pavel P. Kuksa;Vladimir Pavlovic
#t 2012
#c 0
#% 397654
#% 643010
#% 722803
#% 743284
#% 773682
#% 830744
#% 833913
#% 840876
#% 840941
#% 906397
#% 1089344
#% 1511248
#% 1605992
#% 1860542
#! Classification of sequences drawn from a finite alphabet using a family of string kernels with inexact matching (e.g., spectrum or mismatch) has shown great success in machine learning. However, selection of optimal mismatch kernels for a particular task is severely limited by inability to compute such kernels for long substrings (k-mers) with potentially many mismatches (m). In this work we introduce a new method that allows us to exactly evaluate kernels for large k, m and arbitrary alphabet size. The task can be accomplished by first solving the more tractable problem for small alphabets, and then trivially generalizing to any alphabet using a small linear system of equations. This makes it possible to explore a larger set of kernels with a wide range of kernel parameters, opening a possibility to better model selection and improved performance of the string kernels. To investigate the utility of large (k,m) string kernels, we consider several sequence classification problems, including protein remote homology detection, fold prediction, and music classification. Our results show that increased k-mer lengths with larger substitutions can improve classification performance.

#index 1872321
#* Estimating conversion rate in display advertising from past erformance data
#@ Kuang-chih Lee;Burkay Orten;Ali Dasdan;Wentong Li
#t 2012
#c 0
#% 17144
#% 722904
#% 765520
#% 956546
#% 975104
#% 989572
#% 1287220
#% 1451160
#% 1504249
#% 1605925
#% 1605928
#% 1605951
#% 1606074
#! In targeted display advertising, the goal is to identify the best opportunities to display a banner ad to an online user who is most likely to take a desired action such as purchasing a product or signing up for a newsletter. Finding the best ad impression, i.e., the opportunity to show an ad to a user, requires the ability to estimate the probability that the user who sees the ad on his or her browser will take an action, i.e., the user will convert. However, conversion probability estimation is a challenging task since there is extreme data sparsity across different data dimensions and the conversion event occurs rarely. In this paper, we present our approach to conversion rate estimation which relies on utilizing past performance observations along user, publisher and advertiser data hierarchies. More specifically, we model the conversion event at different select hierarchical levels with separate binomial distributions and estimate the distribution parameters individually. Then we demonstrate how we can combine these individual estimators using logistic regression to accurately identify conversion events. In our presentation, we also discuss main practical considerations such as data imbalance, missing data, and output probability calibration, which render this estimation problem more difficult but yet need solving for a real-world implementation of the approach. We provide results from real advertising campaigns to demonstrate the effectiveness of our proposed approach.

#index 1872322
#* Multimedia features for click prediction of new ads in display advertising
#@ Haibin Cheng;Roelof van Zwol;Javad Azimi;Eren Manavoglu;Ruofei Zhang;Yang Zhou;Vidhya Navalpakkam
#t 2012
#c 0
#% 162505
#% 211044
#% 391298
#% 589935
#% 603451
#% 635689
#% 812619
#% 874603
#% 879772
#% 1029264
#% 1055713
#% 1071117
#% 1148278
#% 1211829
#% 1355051
#% 1450842
#% 1451020
#% 1451160
#% 1484562
#% 1650414
#% 1746995
#! Non-guaranteed display advertising (NGD) is a multi-billion dollar business that has been growing rapidly in recent years. Advertisers in NGD sell a large portion of their ad campaigns using performance dependent pricing models such as cost-per-click (CPC) and cost-per-action (CPA). An accurate prediction of the probability that users click on ads is a crucial task in NGD advertising because this value is required to compute the expected revenue. State-of-the-art prediction algorithms rely heavily on historical information collected for advertisers, users and publishers. Click prediction of new ads in the system is a challenging task due to the lack of such historical data. The objective of this paper is to mitigate this problem by integrating multimedia features extracted from display ads into the click prediction models. Multimedia features can help us capture the attractiveness of the ads with similar contents or aesthetics. In this paper we evaluate the use of numerous multimedia features (in addition to commonly used user, advertiser and publisher features) for the purposes of improving click prediction in ads with no history. We provide analytical results generated over billions of samples and demonstrate that adding multimedia features can significantly improve the accuracy of click prediction for new ads, compared to a state-of-the-art baseline model.

#index 1872323
#* Trustworthy online controlled experiments: five puzzling outcomes explained
#@ Ron Kohavi;Alex Deng;Brian Frasca;Roger Longbotham;Toby Walker;Ya Xu
#t 2012
#c 0
#% 989668
#% 1087056
#% 1154062
#% 1214732
#% 1451140
#! Online controlled experiments are often utilized to make data-driven decisions at Amazon, Microsoft, eBay, Facebook, Google, Yahoo, Zynga, and at many other companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and mining of online controlled experiments at scale--thousands of experiments now--has taught us many lessons. These exemplify the proverb that the difference between theory and practice is greater in practice than in theory. We present our learnings as they happened: puzzling outcomes of controlled experiments that we analyzed deeply to understand and explain. Each of these took multiple-person weeks to months to properly analyze and get to the often surprising root cause. The root causes behind these puzzling results are not isolated incidents; these issues generalized to multiple experiments. The heightened awareness should help readers increase the trustworthiness of the results coming out of controlled experiments. At Microsoft's Bing, it is not uncommon to see experiments that impact annual revenue by millions of dollars, thus getting trustworthy results is critical and investing in understanding anomalies has tremendous payoff: reversing a single incorrect decision based on the results of an experiment can fund a whole team of analysts. The topics we cover include: the OEC (Overall Evaluation Criterion), click tracking, effect trends, experiment length and power, and carryover effects.

#index 1872324
#* Position-normalized click prediction in search advertising
#@ Ye Chen;Tak W. Yan
#t 2012
#c 0
#% 956546
#% 1035578
#% 1190055
#% 1451160
#% 1584778
#% 1587856
#! Click-through rate (CTR) prediction plays a central role in search advertising. One needs CTR estimates unbiased by positional effect in order for ad ranking, allocation, and pricing to be based upon ad relevance or quality in terms of click propensity. However, the observed click-through data has been confounded by positional bias, that is, users tend to click more on ads shown in higher positions than lower ones, regardless of the ad relevance. We describe a probabilistic factor model as a general principled approach to studying these exogenous and often overwhelming phenomena. The model is simple and linear in nature, while empirically justified by the advertising domain. Our experimental results with artificial and real-world sponsored search data show the soundness of the underlying model assumption, which in turn yields superior prediction accuracy.

#index 1872325
#* Bid optimizing and inventory scoring in targeted online advertising
#@ Claudia Perlich;Brian Dalessandro;Rod Hook;Ori Stitelman;Troy Raeder;Foster Provost
#t 2012
#c 0
#% 868445
#% 1214692
#% 1605941
#% 1693911
#% 1872392
#% 1881297
#! Billions of online display advertising spots are purchased on a daily basis through real time bidding exchanges (RTBs). Advertising companies bid for these spots on behalf of a company or brand in order to purchase these spots to display banner advertisements. These bidding decisions must be made in fractions of a second after the potential purchaser is informed of what location (Internet site) has a spot available and who would see the advertisement. The entire transaction must be completed in near real-time to avoid delays loading the page and maintain a good users experience. This paper presents a bid-optimization approach that is implemented in production at Media6Degrees for bidding on these advertising opportunities at an appropriate price. The approach combines several supervised learning algorithms, as well as second price auction theory, to determine the correct price to ensure that the right message is delivered to the right person, at the right time.

#index 1872326
#* Algorithms for mining uncertain graph data
#@ Jianzhong Li
#t 2012
#c 0
#! With the rapid development of advanced data acquisition techniques such as high-throughput biological experiments and wireless sensor networks, large amount of graph-structured data, graph data for short, have been collected in a wide range of applications. Discovering knowledge from graph data has witnessed a number of applications and received a lot of research attentions. Recently, it is observed that uncertainties are inherent in the structures of some graph data. For example, protein-protein interaction (PPI) data can be represented as a graph, where vertices represent proteins, and edges represent PPI's. Due to the limits of PPI detection methods, it is uncertain that a detected PPI exist in practice. Other examples of uncertain graph data include topologies of wireless sensor networks, social networks and so on. Managing and mining such large-scale uncertain graph data is of both theoretical and practical significance. Many solid works have been conducted on uncertain graph mining from the aspects of models, semantics, methodology and algorithms in last few years. A number of research papers on managing and mining uncertain graph data have been published in the database and data mining conferences such as VLDB, ICDE, KDD, CIKM and EDBT. This talk focuses on the data model, semantics, computational complexity and algorithms of uncertain graph mining. In the talk, some typical research work in the field of uncertain graph mining will also be introduced, including frequent subgraph pattern mining, dense subgraph detection, reliable subgraph discovery, and clustering on uncertain graph data.

#index 1872327
#* Experience with discovering knowledge by acquiring it
#@ Paul Compton
#t 2012
#c 0
#! Machines and people have complementary skills in Knowledge Discovery. Automated techniques can process enormous amounts of data to find new relationships, but generally these are represented by fairly simple models. On the other hand people are endlessly inventive in creating models to explain data at hand, but have problems developing consistent overall models to explain all the data that might occur in a domain; and the larger the model, the more difficult it becomes to maintain consistency. Ripple-Down Rules is a technique that has been developed to allow people to make real-time updates to a model whenever they notice some data that the model does not yet explain, while at the same time maintaining consistency. This allows an entire knowledge base to be built while it is already in use by making updates. There are now 100s of Ripple-Down-Rule knowledge bases in use and this paper presents some observations from log files tracking how people build these systems, and also outlines some recent research on how such techniques can be used to add greater specificity to the simpler models developed by automated techniques.

#index 1872328
#* Bayesian relational data analysis
#@ Naonori Ueda
#t 2012
#c 0
#! Recently there have been many collections of relational data in diverse areas such as the internet, social networks, customer shopping records, bioinformatics, etc. The main goal of the relational data analysis is to discover latent structure from the data. The conventional data mining algorithms based on exhaustive enumeration have an inherent limitation for this purpose because of the combinatorial nature of the methods. In contrast, in machine learning a lot of statistical models have been proposed for the relational data analysis. In this talk, first I will review the statistical approach, especially Bayesian approach, for the relational data analysis with recent advancements in machine learning literature. Then, as a future research I will also talk about a statistical approach for combining multiple relational data.

#index 1872329
#* Cross-media knowledge discovery
#@ Zhongzhi Shi
#t 2012
#c 0
#! In this talk I introduce cloud computing based cross-media knowledge discovery. We propose a framework for cross-media semantic understanding which contains discriminative modeling, generative modeling and cognitive modeling. In cognitive modeling a new model entitled CAM is proposed which is suitable for cross-media semantic understanding. We develop an agent-aid model for load balance in cloud computing environment. For quality of service we present a utility function to evaluate the cloud performance. A Cross-Media Intelligent Retrieval System (CMIRS), which is managed by ontology-based knowledge system KMSphere, will be illustrated. Finally, the directions for further researches on cloud computing based cross-media knowledge discovery will be pointed out and discussed.

#index 1872330
#* Review spam detection via temporal pattern discovery
#@ Sihong Xie;Guan Wang;Shuyang Lin;Philip S. Yu
#t 2012
#c 0
#% 765412
#% 1022239
#% 1035590
#% 1476479
#% 1482272
#% 1482375
#% 1560192
#% 1591960
#% 1746817
#! Online reviews play a crucial role in today's electronic commerce. It is desirable for a customer to read reviews of products or stores before making the decision of what or from where to buy. Due to the pervasive spam reviews, customers can be misled to buy low-quality products, while decent stores can be defamed by malicious reviews. We observe that, in reality, a great portion ( 90% in the data we study) of the reviewers write only one review (singleton review). These reviews are so enormous in number that they can almost determine a store's rating and impression. However, existing methods did not examine this larger part of the reviews. Are most of these singleton reviews truthful ones? If not, how to detect spam reviews in singleton reviews? We call this problem singleton review spam detection. To address this problem, we observe that the normal reviewers' arrival pattern is stable and uncorrelated to their rating pattern temporally. In contrast, spam attacks are usually bursty and either positively or negatively correlated to the rating. Thus, we propose to detect such attacks via unusually correlated temporal patterns. We identify and construct multidimensional time series based on aggregate statistics, in order to depict and mine such correlations. In this way, the singleton review spam detection problem is mapped to a abnormally correlated pattern detection problem. We propose a hierarchical algorithm to robustly detect the time windows where such attacks are likely to have happened. The algorithm also pinpoints such windows in different time resolutions to facilitate faster human inspection. Experimental results show that the proposed method is effective in detecting singleton review attacks. We discover that singleton review is a significant source of spam reviews and largely affects the ratings of online stores.

#index 1872331
#* Selecting a characteristic set of reviews
#@ Theodoros Lappas;Mark Crovella;Evimaria Terzi
#t 2012
#c 0
#% 408396
#% 769892
#% 907489
#% 907490
#% 990210
#% 1035590
#% 1035591
#% 1136473
#% 1176947
#% 1188997
#% 1195867
#% 1261574
#% 1310429
#% 1400002
#% 1495595
#% 1605931
#! Online reviews provide consumers with valuable information that guides their decisions on a variety of fronts: from entertainment and shopping to medical services. Although the proliferation of online reviews gives insights about different aspects of a product, it can also prove a serious drawback: consumers cannot and will not read thousands of reviews before making a purchase decision. This need to extract useful information from large review corpora has spawned considerable prior work, but so far all have drawbacks. Review summarization (generating statistical descriptions of review sets) sacrifices the immediacy and narrative structure of reviews. Likewise, review selection (identifying a subset of 'helpful' or 'important' reviews) leads to redundant or non-representative summaries. In this paper, we fill the gap between existing review-summarization and review-selection methods by selecting a small subset of reviews that together preserve the statistical properties of the entire review corpus. We formalize this task as a combinatorial optimization problem and show that it NP-hard both tosolve and approximate. We also design effective algorithms that prove to work well in practice. Our experiments with real review corpora on different types of products demonstrate the utility of our methods, and our user studies indicate that our methods provide a better summary than prior approaches.

#index 1872332
#* Mining contentions from discussions and debates
#@ Arjun Mukherjee;Bing Liu
#t 2012
#c 0
#% 458379
#% 577356
#% 722904
#% 788094
#% 876067
#% 938737
#% 956510
#% 989621
#% 1055681
#% 1055682
#% 1083684
#% 1117083
#% 1127964
#% 1211693
#% 1211773
#% 1261563
#% 1264719
#% 1264771
#% 1292503
#% 1310458
#% 1328328
#% 1338553
#% 1425621
#% 1450945
#% 1481541
#% 1535392
#% 1536586
#% 1544065
#% 1544104
#% 1591964
#% 1591967
#% 1592079
#% 1650298
#% 1732740
#! Social media has become a major source of information for many applications. Numerous techniques have been proposed to analyze network structures and text contents. In this paper, we focus on fine-grained mining of contentions in discussion/debate forums. Contentions are perhaps the most important feature of forums that discuss social, political and religious issues. Our goal is to discover contention and agreement indicator expressions, and contention points or topics both at the discussion collection level and also at each individual post level. To the best of our knowledge, limited work has been done on such detailed analysis. This paper proposes three models to solve the problem, which not only model both contention/agreement expressions and discussion topics, but also, more importantly, model the intrinsic nature of discussions/debates, i.e., interactions among discussants or debaters and topic sharing among posts through quoting and replying relations. Evaluation results using real-life discussion/debate posts from several domains demonstrate the effectiveness of the proposed models.

#index 1872333
#* Discovering value from community activity on focused question answering sites: a case study of stack overflow
#@ Ashton Anderson;Daniel Huttenlocher;Jon Kleinberg;Jure Leskovec
#t 2012
#c 0
#% 754098
#% 879593
#% 956516
#% 1019165
#% 1047396
#% 1055738
#% 1071516
#% 1074111
#% 1179994
#% 1183154
#% 1190069
#% 1355047
#% 1384246
#% 1399997
#% 1411585
#% 1450880
#% 1554240
#% 1561548
#% 1573559
#% 1598375
#% 1693931
#! Question answering (Q&A) websites are now large repositories of valuable knowledge. While most Q&A sites were initially aimed at providing useful answers to the question asker, there has been a marked shift towards question answering as a community-driven knowledge creation process whose end product can be of enduring value to a broad audience. As part of this shift, specific expertise and deep knowledge of the subject at hand have become increasingly important, and many Q&A sites employ voting and reputation mechanisms as centerpieces of their design to help users identify the trustworthiness and accuracy of the content. To better understand this shift in focus from one-off answers to a group knowledge-creation process, we consider a question together with its entire set of corresponding answers as our fundamental unit of analysis, in contrast with the focus on individual question-answer pairs that characterized previous work. Our investigation considers the dynamics of the community activity that shapes the set of answers, both how answers and voters arrive over time and how this influences the eventual outcome. For example, we observe significant assortativity in the reputations of co-answerers, relationships between reputation and answer speed, and that the probability of an answer being chosen as the best one strongly depends on temporal characteristics of answer arrivals. We then show that our understanding of such properties is naturally applicable to predicting several important quantities, including the long-term value of the question and its answers, as well as whether a question requires a better answer. Finally, we discuss the implications of these results for the design of Q&A sites.

#index 1872334
#* Integrating community matching and outlier detection for mining evolutionary community outliers
#@ Manish Gupta;Jing Gao;Yizhou Sun;Jiawei Han
#t 2012
#c 0
#% 300136
#% 300183
#% 333929
#% 443853
#% 466745
#% 479791
#% 494396
#% 570886
#% 577263
#% 781774
#% 785389
#% 844317
#% 1041180
#% 1202160
#% 1206639
#% 1232040
#% 1292669
#% 1318691
#% 1451221
#% 1482422
#% 1562549
#% 1594652
#! Temporal datasets, in which data evolves continuously, exist in a wide variety of applications, and identifying anomalous or outlying objects from temporal datasets is an important and challenging task. Different from traditional outlier detection, which detects objects that have quite different behavior compared with the other objects, temporal outlier detection tries to identify objects that have different evolutionary behavior compared with other objects. Usually objects form multiple communities, and most of the objects belonging to the same community follow similar patterns of evolution. However, there are some objects which evolve in a very different way relative to other community members, and we define such objects as evolutionary community outliers. This definition represents a novel type of outliers considering both temporal dimension and community patterns. We investigate the problem of identifying evolutionary community outliers given the discovered communities from two snapshots of an evolving dataset. To tackle the challenges of community evolution and outlier detection, we propose an integrated optimization framework which conducts outlier-aware community matching across snapshots and identification of evolutionary outliers in a tightly coupled way. A coordinate descent algorithm is proposed to improve community matching and outlier detection performance iteratively. Experimental results on both synthetic and real datasets show that the proposed approach is highly effective in discovering interesting evolutionary community outliers.

#index 1872335
#* Different slopes for different folks: mining for exceptional regression models with cook's distance
#@ Wouter Duivesteijn;Ad Feelders;Arno Knobbe
#t 2012
#c 0
#% 424759
#% 1108880
#% 1263962
#% 1535398
#! Exceptional Model Mining (EMM) is an exploratory data analysis technique that can be regarded as a generalization of subgroup discovery. In EMM we look for subgroups of the data for which a model fitted to the subgroup differs substantially from the same model fitted to the entire dataset. In this paper we develop methods to mine for exceptional regression models. We propose a measure for the exceptionality of regression models (Cook's distance), and explore the possibilities to avoid having to fit the regression model to each candidate subgroup. The algorithm is evaluated on a number of real life datasets. These datasets are also used to illustrate the results of the algorithm. We find interesting subgroups with deviating models on datasets from several different domains. We also show that under certain circumstances one can forego fitting regression models on up to 40% of the subgroups, and these 40% are the relatively expensive regression models to compute.

#index 1872336
#* A near-linear time approximation algorithm for angle-based outlier detection in high-dimensional data
#@ Ninh Pham;Rasmus Pagh
#t 2012
#c 0
#% 205305
#% 278835
#% 300136
#% 300183
#% 333929
#% 347225
#% 465031
#% 479791
#% 729912
#% 1039658
#% 1051998
#% 1083673
#% 1224602
#% 1594653
#% 1594655
#! Outlier mining in d-dimensional point sets is a fundamental and well studied data mining task due to its variety of applications. Most such applications arise in high-dimensional domains. A bottleneck of existing approaches is that implicit or explicit assessments on concepts of distance or nearest neighbor are deteriorated in high-dimensional data. Following up on the work of Kriegel et al. (KDD '08), we investigate the use of angle-based outlier factor in mining high-dimensional outliers. While their algorithm runs in cubic time (with a quadratic time heuristic), we propose a novel random projection-based technique that is able to estimate the angle-based outlier factor for all data points in time near-linear in the size of the data. Also, our approach is suitable to be performed in parallel environment to achieve a parallel speedup. We introduce a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm. The empirical experiments on synthetic and real world data sets demonstrate that our approach is efficient and scalable to very large high-dimensional data sets.

#index 1872337
#* Intrusion as (anti)social communication: characterization and detection
#@ Qi Ding;Natallia Katenka;Paul Barford;Eric Kolaczyk;Mark Crovella
#t 2012
#c 0
#% 302479
#% 580965
#% 729983
#% 758132
#% 789054
#% 790040
#% 853537
#% 967005
#% 1206652
#% 1213374
#% 1247092
#! A reasonable definition of intrusion is: entering a community to which one does not belong. This suggests that in a network, intrusion attempts may be detected by looking for communication that does not respect community boundaries. In this paper, we examine the utility of this concept for identifying malicious network sources. In particular, our goal is to explore whether this concept allows a core-network operator using flow data to augment signature-based systems located at network edges. We show that simple measures of communities can be defined for flow data that allow a remarkably effective level of intrusion detection simply by looking for flows that do not respect those communities. We validate our approach using labeled intrusion attempt data collected at a large number of edge networks. Our results suggest that community-based methods can offer an important additional dimension for intrusion detection systems.

#index 1872338
#* Robust multi-task feature learning
#@ Pinghua Gong;Jieping Ye;Changshui Zhang
#t 2012
#c 0
#% 3084
#% 236497
#% 723239
#% 769886
#% 770804
#% 840962
#% 916788
#% 961246
#% 1128929
#% 1271814
#% 1302843
#% 1302853
#% 1417091
#% 1451171
#% 1451258
#% 1451260
#% 1605917
#% 1606019
#% 1654243
#% 1765634
#% 1842749
#! Multi-task learning (MTL) aims to improve the performance of multiple related tasks by exploiting the intrinsic relationships among them. Recently, multi-task feature learning algorithms have received increasing attention and they have been successfully applied to many applications involving high dimensional data. However, they assume that all tasks share a common set of features, which is too restrictive and may not hold in real-world applications, since outlier tasks often exist. In this paper, we propose a Robust Multi-Task Feature Learning algorithm (rMTFL) which simultaneously captures a common set of features among relevant tasks and identifies outlier tasks. Specifically, we decompose the weight (model) matrix for all tasks into two components. We impose the well-known group Lasso penalty on row groups of the first component for capturing the shared features among relevant tasks. To simultaneously identify the outlier tasks, we impose the same group Lasso penalty but on column groups of the second component. We propose to employ the accelerated gradient descent to efficiently solve the optimization problem in rMTFL, and show that the proposed algorithm is scalable to large-size problems. In addition, we provide a detailed theoretical analysis on the proposed rMTFL formulation. Specifically, we present a theoretical bound to measure how well our proposed rMTFL approximates the true evaluation, and provide bounds to measure the error between the estimated weights of rMTFL and the underlying true weights. Moreover, by assuming that the underlying true weights are above the noise level, we present a sound theoretical result to show how to obtain the underlying true shared features and outlier tasks (sparsity patterns). Empirical studies on both synthetic and real-world data demonstrate that our proposed rMTFL is capable of simultaneously capturing shared features among tasks and identifying outlier tasks.

#index 1872339
#* Unsupervised feature selection for linked social media data
#@ Jiliang Tang;Huan Liu
#t 2012
#c 0
#% 3084
#% 310560
#% 310561
#% 425048
#% 452881
#% 466410
#% 466414
#% 729437
#% 757953
#% 771842
#% 796212
#% 814023
#% 865332
#% 875980
#% 916789
#% 928386
#% 983948
#% 995140
#% 1214703
#% 1400031
#% 1417091
#% 1451172
#% 1535324
#% 1693870
#% 1826317
#! The prevalent use of social media produces mountains of unlabeled, high-dimensional data. Feature selection has been shown effective in dealing with high-dimensional data for efficient data mining. Feature selection for unlabeled data remains a challenging task due to the absence of label information by which the feature relevance can be assessed. The unique characteristics of social media data further complicate the already challenging problem of unsupervised feature selection, (e.g., part of social media data is linked, which makes invalid the independent and identically distributed assumption), bringing about new challenges to traditional unsupervised feature selection algorithms. In this paper, we study the differences between social media data and traditional attribute-value data, investigate if the relations revealed in linked data can be used to help select relevant features, and propose a novel unsupervised feature selection framework, LUFS, for linked social media data. We perform experiments with real-world social media datasets to evaluate the effectiveness of the proposed framework and probe the working of its key components.

#index 1872340
#* Model mining for robust feature selection
#@ Adam Woznica;Phong Nguyen;Alexandros Kalousis
#t 2012
#c 0
#% 310494
#% 425048
#% 465003
#% 720010
#% 729437
#% 977991
#% 1083713
#% 1108900
#% 1214678
#% 1374741
#! A common problem with most of the feature selection methods is that they often produce feature sets--models--that are not stable with respect to slight variations in the training data. Different authors tried to improve the feature selection stability using ensemble methods which aggregate different feature sets into a single model. However, the existing ensemble feature selection methods suffer from two main shortcomings: (i) the aggregation treats the features independently and does not account for their interactions, and (ii) a single feature set is returned, nevertheless, in various applications there might be more than one feature sets, potentially redundant, with similar information content. In this work we address these two limitations. We present a general framework in which we mine over different feature models produced from a given dataset in order to extract patterns over the models. We use these patterns to derive more complex feature model aggregation strategies that account for feature interactions, and identify core and distinct feature models. We conduct an extensive experimental evaluation of the proposed framework where we demonstrate its effectiveness over a number of high-dimensional problems from the fields of biology and text-mining.

#index 1872341
#* Feature grouping and selection over an undirected graph
#@ Sen Yang;Lei Yuan;Ying-Cheng Lai;Xiaotong Shen;Peter Wonka;Jieping Ye
#t 2012
#c 0
#% 770846
#% 1060798
#% 1211744
#% 1482462
#! High-dimensional regression/classification continues to be an important and challenging problem, especially when features are highly correlated. Feature selection, combined with additional structure information on the features has been considered to be promising in promoting regression/classification performance. Graph-guided fused lasso (GFlasso) has recently been proposed to facilitate feature selection and graph structure exploitation, when features exhibit certain graph structures. However, the formulation in GFlasso relies on pairwise sample correlations to perform feature grouping, which could introduce additional estimation bias. In this paper, we propose three new feature grouping and selection methods to resolve this issue. The first method employs a convex function to penalize the pairwise l∞ norm of connected regression/classification coefficients, achieving simultaneous feature grouping and selection. The second method improves the first one by utilizing a non-convex function to reduce the estimation bias. The third one is the extension of the second method using a truncated l1 regularization to further reduce the estimation bias. The proposed methods combine feature grouping and feature selection to enhance estimation accuracy. We employ the alternating direction method of multipliers (ADMM) and difference of convex functions (DC) programming to solve the proposed formulations. Our experimental results on synthetic data and two real datasets demonstrate the effectiveness of the proposed methods.

#index 1872342
#* Maximum inner-product search using cone trees
#@ Parikshit Ram;Alexander G. Gray
#t 2012
#c 0
#% 2115
#% 249321
#% 317313
#% 347225
#% 479973
#% 632011
#% 875957
#% 956506
#% 1038334
#% 1061636
#% 1260273
#! The problem of efficiently finding the best match for a query in a given set with respect to the Euclidean distance or the cosine similarity has been extensively studied. However, the closely related problem of efficiently finding the best match with respect to the inner-product has never been explored in the general setting to the best of our knowledge. In this paper we consider this problem and contrast it with the previous problems considered. First, we propose a general branch-and-bound algorithm based on a (single) tree data structure. Subsequently, we present a dual-tree algorithm for the case where there are multiple queries. Our proposed branch-and-bound algorithms are based on novel inner-product bounds. Finally we present a new data structure, the cone tree, for increasing the efficiency of the dual-tree algorithm. We evaluate our proposed algorithms on a variety of data sets from various applications, and exhibit up to five orders of magnitude improvement in query time over the naive search technique in some cases.

#index 1872343
#* A probabilistic model for multimodal hash function learning
#@ Yi Zhen;Dit-Yan Yeung
#t 2012
#c 0
#% 249238
#% 249321
#% 264161
#% 317313
#% 347225
#% 479973
#% 722904
#% 724290
#% 760805
#% 917232
#% 919460
#% 1023422
#% 1083648
#% 1273928
#% 1292880
#% 1450831
#% 1451253
#% 1484424
#% 1598356
#% 1598428
#% 1606048
#% 1826280
#% 1945137
#! In recent years, both hashing-based similarity search and multimodal similarity search have aroused much research interest in the data mining and other communities. While hashing-based similarity search seeks to address the scalability issue, multimodal similarity search deals with applications in which data of multiple modalities are available. In this paper, our goal is to address both issues simultaneously. We propose a probabilistic model, called multimodal latent binary embedding (MLBE), to learn hash functions from multimodal data automatically. MLBE regards the binary latent factors as hash codes in a common Hamming space. Given data from multiple modalities, we devise an efficient algorithm for the learning of binary latent factors which corresponds to hash function learning. Experimental validation of MLBE has been conducted using both synthetic data and two realistic data sets. Experimental results show that MLBE compares favorably with two state-of-the-art models.

#index 1872344
#* On socio-spatial group query for location-based social networks
#@ De-Nian Yang;Chih-Ya Shen;Wang-Chien Lee;Ming-Syan Chen
#t 2012
#c 0
#% 201876
#% 287466
#% 745464
#% 993955
#% 1181225
#% 1214668
#% 1217190
#% 1451234
#% 1512399
#% 1573239
#% 1598366
#! Challenges faced in organizing impromptu activities are the requirements of making timely invitations in accordance with the locations of candidate attendees and the social relationship among them. It is desirable to find a group of attendees close to a rally point and ensure that the selected attendees have a good social relationship to create a good atmosphere in the activity. Therefore, this paper proposes Socio-Spatial Group Query (SSGQ) to select a group of nearby attendees with tight social relation. Efficient processing of SSGQ is very challenging due to the tradeoff in the spatial and social domains. We show that the problem is NP-hard via a proof and design an efficient algorithm SSGSelect, which includes effective pruning techniques to reduce the running time for finding the optimal solution. We also propose a new index structure, Social R-Tree to further improve the efficiency. User study and experimental results demonstrate that SSGSelect significantly outperforms manual coordination in both solution quality and efficiency.

#index 1872345
#* Random forests for metric learning with implicit pairwise position dependence
#@ Caiming Xiong;David Johnson;Ran Xu;Jason J. Corso
#t 2012
#c 0
#% 236656
#% 400847
#% 770798
#% 812372
#% 852092
#% 875965
#% 884027
#% 983830
#% 1074017
#% 1108888
#% 1211843
#% 1232015
#% 1450963
#% 1474682
#% 1481330
#% 1502472
#% 1562701
#! Metric learning makes it plausible to learn semantically meaningful distances for complex distributions of data using label or pairwise constraint information. However, to date, most metric learning methods are based on a single Mahalanobis metric, which cannot handle heterogeneous data well. Those that learn multiple metrics throughout the feature space have demonstrated superior accuracy, but at a severe cost to computational efficiency. Here, we adopt a new angle on the metric learning problem and learn a single metric that is able to implicitly adapt its distance function throughout the feature space. This metric adaptation is accomplished by using a random forest-based classifier to underpin the distance function and incorporate both absolute pairwise position and standard relative position into the representation. We have implemented and tested our method against state of the art global and multi-metric methods on a variety of data sets. Overall, the proposed method outperforms both types of method in terms of accuracy (consistently ranked first) and is an order of magnitude faster than state of the art multi-metric methods (16x faster in the worst case).

#index 1872346
#* Empowering authors to diagnose comprehension burden in textbooks
#@ Rakesh Agrawal;Sunandan Chakraborty;Sreenivas Gollapudi;Anitha Kannan;Krishnaram Kenthapadi
#t 2012
#c 0
#% 9197
#% 352869
#% 747647
#% 772517
#% 816186
#% 1434144
#% 1468142
#% 1484302
#% 1528118
#% 1560308
#% 1723951
#! Good textbooks are organized in a systematically progressive fashion so that students acquire new knowledge and learn new concepts based on known items of information. We provide a diagnostic tool for quantitatively assessing the comprehension burden that a textbook imposes on the reader due to non-sequential presentation of concepts. We present a formal definition of comprehension burden and propose an algorithmic approach for computing it. We apply the tool to a corpus of high school textbooks from India and empirically examine its effectiveness in helping authors identify sections of textbooks that can benefit from reorganizing the material presented.

#index 1872347
#* Coupled behavior analysis for capturing coupling relationships in group-based market manipulations
#@ Yin Song;Longbing Cao;Xindong Wu;Gang Wei;Wu Ye;Wei Ding
#t 2012
#c 0
#% 592062
#% 677512
#% 980078
#% 1000502
#% 1209716
#% 1417854
#% 1451147
#% 1650403
#% 1761822
#% 1848056
#! In stock markets, an emerging challenge for surveillance is that a group of hidden manipulators collaborate with each other to manipulate the price movement of securities. Recently, the coupled hidden Markov model (CHMM)-based coupled behavior analysis (CBA) has been proposed to consider the coupling relationships in the above group-based behaviors for manipulation detection. From the modeling perspective, however, this requires overall aggregation of the behavioral data to cater for the CHMM modeling, which does not differentiate the coupling relationships presented in different forms within the aggregated behaviors and degrade the capability for further anomaly detection. Thus, this paper suggests a general CBA framework for detecting group-based market manipulation by capturing more comprehensive couplings and proposes two variant implementations, which are hybrid coupling (HC)-based and hierarchical grouping (HG)-based respectively. The proposed framework consists of three stages. The first stage, qualitative analysis, generates possible qualitative coupling relationships between behaviors with or without domain knowledge. In the second stage, quantitative representation of coupled behaviors is learned via proper methods. For the third stage, anomaly detection algorithms are proposed to cater for different application scenarios. Experimental results on data from a major Asian stock market show that the proposed framework outperforms the CHMM-based analysis in terms of detecting abnormal collaborative market manipulations. Additionally, the two different implementations are compared with their effectiveness for different application scenarios.

#index 1872348
#* HySAD: a semi-supervised hybrid shilling attack detector for trustworthy product recommendation
#@ Zhiang Wu;Junjie Wu;Jie Cao;Dacheng Tao
#t 2012
#c 0
#% 192878
#% 311027
#% 564259
#% 734590
#% 754097
#% 783438
#% 807349
#% 835018
#% 836153
#% 844357
#% 881512
#% 881555
#% 987671
#% 1166755
#% 1287238
#% 1541728
#% 1728733
#% 1868022
#! Shilling attackers apply biased rating profiles to recommender systems for manipulating online product recommendations. Although many studies have been devoted to shilling attack detection, few of them can handle the hybrid shilling attacks that usually happen in practice, and the studies for real-life applications are rarely seen. Moreover, little attention has yet been paid to modeling both labeled and unlabeled user profiles, although there are often a few labeled but numerous unlabeled users available in practice. This paper presents a Hybrid Shilling Attack Detector, or HySAD for short, to tackle these problems. In particular, HySAD introduces MC-Relief to select effective detection metrics, and Semi-supervised Naive Bayes (SNB_lambda) to precisely separate Random-Filler model attackers and Average-Filler model attackers from normal users. Thorough experiments on MovieLens and Netflix datasets demonstrate the effectiveness of HySAD in detecting hybrid shilling attacks, and its robustness for various obfuscated strategies. A real-life case study on product reviews of Amazon.cn is also provided, which further demonstrates that HySAD can effectively improve the accuracy of a collaborative-filtering based recommender system, and provide interesting opportunities for in-depth analysis of attacker behaviors. These, in turn, justify the value of HySAD for real-world applications.

#index 1872349
#* Following the electrons: methods for power management in commercial buildings
#@ Gowtham Bellala;Manish Marwah;Martin Arlitt;Geoff Lyon;Cullen Bash
#t 2012
#c 0
#% 115608
#% 729437
#% 862540
#% 989613
#% 1074346
#% 1269936
#% 1429119
#% 1429120
#% 1487205
#% 1487207
#% 1487212
#% 1487216
#% 1751792
#% 1948630
#! Commercial buildings are significant consumers of electricity. The first step towards better energy management in commercial buildings is monitoring consumption. However, instrumenting every electrical panel in a large commercial building is expensive and wasteful. In this paper, we propose a greedy meter (sensor) placement algorithm based on maximization of information gained, subject to a cost constraint. The algorithm provides a near-optimal solution guarantee. Furthermore, to identify power saving opportunities, we use an unsupervised anomaly detection technique based on a low-dimensional embedding. Further, to better manage resources such as lighting and HVAC, we propose a semi-supervised approach combining hidden Markov models (HMM) and a standard classifier to model occupancy based on readily available port-level network statistics.

#index 1872350
#* Ensembles and model delivery for tax compliance
#@ Graham Williams
#t 2012
#c 0
#! Revenue authorities characteristically have large stores of historic audit data, with outcomes, ready for analysis. The Australian Taxation Office established one of the largest data mining teams in Australia in 2004 as a foundation to becoming a knowledge-based organization. Today, every tax return lodged in Australia is risk assessed by one or more models developed through data mining, generally based on historic data. We observe that any of the traditional modeling approaches, particularly including random forests, generally deliver similar models in terms of accuracy. We take advantage of combining different model types and modeling approaches for risk scoring, and in particular report on recent research that increases the diversity of trees that make up a random forest. We also review, in a practical context, how such models are evaluated and delivered.

#index 1872351
#* Leveraging predictive modeling to reduce signal theft in a multi-service organization environment
#@ Seymour Douglas
#t 2012
#c 0
#! Signal theft can be defined as the interdiction, consumption or usage of carrier signal from a provider's network without payment or payment of an amount less than the level of service consumed. High levels of signal theft can potentially reflect open technical network issues, failure of electronic countermeasures or operational gaps that are estimated to cost the cable industry providers more than $5 billion annually. This session will discuss the business challenges associated with the quantification of signal theft-related losses, outline some of the countermeasures taken by MSOs, and then provide views on the development of predictive models to help identify the potential likelihood of signal theft in a given environment. We will examine the performance of certain machine learning algorithms as well as data challenges associated with both the architecture construction and analytical efforts, and conclude with a lessons-learned discussion and views on future approaches.

#index 1872352
#* Capacitated team formation problem on social networks
#@ Anirban Majumder;Samik Datta;K.V.M. Naidu
#t 2012
#c 0
#% 39703
#% 452665
#% 1075875
#% 1164112
#% 1178476
#% 1214668
#% 1399997
#% 1400031
#% 1482238
#% 1561923
#% 1746882
#! In a team formation problem, one is required to find a group of users that can match the requirements of a collaborative task. Example of such collaborative tasks abound, ranging from software product development to various participatory sensing tasks in knowledge creation. Due to the nature of the task, team members are often required to work on a co-operative basis. Previous studies [1, 2] have indicated that co-operation becomes effective in presence of social connections. Therefore, effective team selection requires the team members to be socially close as well as a division of the task among team members so that no user is overloaded by the assignment. In this work, we investigate how such teams can be formed on a social network. Since our team formation problems are proven to be NP-hard, we design efficient approximate algorithms for finding near optimum teams with provable guarantees. As traditional data-sets from on-line social networks (e.g. Twitter, Facebook etc) typically do not contain instances of large scale collaboration, we have crawled millions of software repositories spanning a period of four years and hundreds of thousands of developers from GitHub, a popular open-source social coding network. We perform large scale experiments on this data-set to evaluate the accuracy and efficiency of our algorithms. Experimental results suggest that our algorithms achieve significant improvement in finding effective teams, as compared to naive strategies and scale well with the size of the data. Finally, we provide a validation of our techniques by comparing with existing software teams.

#index 1872353
#* Finding trendsetters in information networks
#@ Diego Saez-Trumper;Giovanni Comarela;Virgílio Almeida;Ricardo Baeza-Yates;Fabrício Benevenuto
#t 2012
#c 0
#% 290830
#% 342596
#% 641979
#% 729923
#% 769428
#% 881460
#% 956516
#% 1083624
#% 1083672
#% 1130857
#% 1214641
#% 1222654
#% 1355042
#% 1399992
#% 1451242
#% 1482198
#% 1536507
#% 1536522
#% 1560424
#% 1617342
#! Influential people have an important role in the process of information diffusion. However, there are several ways to be influential, for example, to be the most popular or the first that adopts a new idea. In this paper we present a methodology to find trendsetters in information networks according to a specific topic of interest. Trendsetters are people that adopt and spread new ideas influencing other people before these ideas become popular. At the same time, not all early adopters are trendsetters because only few of them have the ability of propagating their ideas by their social contacts through word-of-mouth. Differently from other influence measures, a trendsetter is not necessarily popular or famous, but the one whose ideas spread over the graph successfully. Other metrics such as node in-degree or even standard Pagerank focus only in the static topology of the network. We propose a ranking strategy that focuses on the ability of some users to push new ideas that will be successful in the future. To that end, we combine temporal attributes of nodes and edges of the network with a Pagerank based algorithm to find the trendsetters for a given topic. To test our algorithm we conduct innovative experiments over a large Twitter dataset. We show that nodes with high in-degree tend to arrive late for new trends, while users in the top of our ranking tend to be early adopters that also influence their social contacts to adopt the new trend.

#index 1872354
#* Towards social user profiling: unified and discriminative influence model for inferring home locations
#@ Rui Li;Shengjie Wang;Hongbo Deng;Rui Wang;Kevin Chen-Chuan Chang
#t 2012
#c 0
#% 766441
#% 869536
#% 920792
#% 987205
#% 1055707
#% 1074070
#% 1190131
#% 1214692
#% 1214717
#% 1227622
#% 1355041
#% 1355044
#% 1399939
#% 1482254
#% 1536509
#% 1560379
#% 1560408
#% 1605925
#! Users' locations are important to many applications such as targeted advertisement and news recommendation. In this paper, we focus on the problem of profiling users' home locations in the context of social network (Twitter). The problem is nontrivial, because signals, which may help to identify a user's location, are scarce and noisy. We propose a unified discriminative influence model, named as UDI, to solve the problem. To overcome the challenge of scarce signals, UDI integrates signals observed from both social network (friends) and user-centric data (tweets) in a unified probabilistic framework. To overcome the challenge of noisy signals, UDI captures how likely a user connects to a signal with respect to 1) the distance between the user and the signal, and 2) the influence scope of the signal. Based on the model, we develop local and global location prediction methods. The experiments on a large scale data set show that our methods improve the state-of-the-art methods by 13%, and achieve the best performance.

#index 1872355
#* Event-based social networks: linking the online and offline social worlds
#@ Xingjie Liu;Qi He;Yuanyuan Tian;Wang-Chien Lee;John McPherson;Jiawei Han
#t 2012
#c 0
#% 36672
#% 313959
#% 330687
#% 769935
#% 823328
#% 956578
#% 1002007
#% 1198232
#% 1300556
#% 1425621
#% 1451163
#% 1606045
#% 1606049
#% 1633202
#! Newly emerged event-based online social services, such as Meetup and Plancast, have experienced increased popularity and rapid growth. From these services, we observed a new type of social network - event-based social network (EBSN). An EBSN does not only contain online social interactions as in other conventional online social networks, but also includes valuable offline social interactions captured in offline activities. By analyzing real data collected from Meetup, we investigated EBSN properties and discovered many unique and interesting characteristics, such as heavy-tailed degree distributions and strong locality of social interactions. We subsequently studied the heterogeneous nature (co-existence of both online and offline social interactions) of EBSNs on two challenging problems: community detection and information flow. We found that communities detected in EBSNs are more cohesive than those in other types of social networks (e.g. location-based social networks). In the context of information flow, we studied the event recommendation problem. By experimenting various information diffusion patterns, we found that a community-based diffusion model that takes into account of both online and offline interactions provides the best prediction power. This paper is the first research to study EBSNs at scale and paves the way for future studies on this new type of social network. A sample dataset of this study can be downloaded from http://www.largenetwork.org/ebsn.

#index 1872356
#* Differential identifiability
#@ Jaewoo Lee;Chris Clifton
#t 2012
#c 0
#% 443463
#% 576761
#% 937550
#% 960289
#% 1074831
#% 1214684
#% 1429646
#% 1451189
#% 1451190
#% 1581862
#% 1581864
#% 1595893
#% 1605968
#% 1606068
#% 1631260
#% 1670071
#% 1740518
#! A key challenge in privacy-preserving data mining is ensuring that a data mining result does not inherently violate privacy. ε-Differential Privacy appears to provide a solution to this problem. However, there are no clear guidelines on how to set ε to satisfy a privacy policy. We give an alternate formulation, Differential Identifiability, parameterized by the probability of individual identification. This provides the strong privacy guarantees of differential privacy, while letting policy makers set parameters based on the established privacy concept of individual identifiability.

#index 1872357
#* Anonymizing set-valued data by nonreciprocal recoding
#@ Mingqiang Xue;Panagiotis Karras;Chedy Raïssi;Jaideep Vaidya;Kian-Lee Tan
#t 2012
#c 0
#% 428404
#% 443463
#% 539744
#% 576111
#% 577233
#% 937550
#% 993988
#% 1070890
#% 1083631
#% 1083709
#% 1154063
#% 1200329
#% 1206581
#% 1206584
#% 1217156
#% 1292623
#% 1328187
#% 1426564
#% 1523887
#% 1523888
#% 1537150
#% 1538422
#! Today there is a strong interest in publishing set-valued data in a privacy-preserving manner. Such data associate individuals to sets of values (e.g., preferences, shopping items, symptoms, query logs). In addition, an individual can be associated with a sensitive label (e.g., marital status, religious or political conviction). Anonymizing such data implies ensuring that an adversary should not be able to (1) identify an individual's record, and (2) infer a sensitive label, if such exists. Existing research on this problem either perturbs the data, publishes them in disjoint groups disassociated from their sensitive labels, or generalizes their values by assuming the availability of a generalization hierarchy. In this paper, we propose a novel alternative. Our publication method also puts data in a generalized form, but does not require that published records form disjoint groups and does not assume a hierarchy either; instead, it employs generalized bitmaps and recasts data values in a nonreciprocal manner; formally, the bipartite graph from original to anonymized records does not have to be composed of disjoint complete subgraphs. We configure our schemes to provide popular privacy guarantees while resisting attacks proposed in recent research, and demonstrate experimentally that we gain a clear utility advantage over the previous state of the art.

#index 1872358
#* Adversarial support vector machine learning
#@ Yan Zhou;Murat Kantarcioglu;Bhavani Thuraisingham;Bowei Xi
#t 2012
#c 0
#% 145156
#% 722901
#% 769885
#% 804739
#% 823397
#% 863446
#% 864873
#% 864874
#% 875989
#% 904964
#% 1073899
#% 1301004
#% 1457045
#% 1472971
#% 1538192
#% 1605974
#% 1728891
#! Many learning tasks such as spam filtering and credit card fraud detection face an active adversary that tries to avoid detection. For learning problems that deal with an active adversary, it is important to model the adversary's attack strategy and develop robust learning models to mitigate the attack. These are the two objectives of this paper. We consider two attack models: a free-range attack model that permits arbitrary data corruption and a restrained attack model that anticipates more realistic attacks that a reasonable adversary would devise under penalties. We then develop optimal SVM learning strategies against the two attack models. The learning algorithms minimize the hinge loss while assuming the adversary is modifying data to maximize the loss. Experiments are performed on both artificial and real data sets. We demonstrate that optimal solutions may be overly pessimistic when the actual attacks are much weaker than expected. More important, we demonstrate that it is possible to develop a much more resilient SVM learning model while making loose assumptions on the data corruption models. When derived under the restrained attack model, our optimal SVM learning strategy provides more robust overall performance under a wide range of attack parameters.

#index 1872359
#* Web image prediction using multivariate point processes
#@ Gunhee Kim;Li Fei-Fei;Eric P. Xing
#t 2012
#c 0
#% 229931
#% 268079
#% 788094
#% 881498
#% 956521
#% 986037
#% 1119142
#% 1130999
#% 1131921
#% 1484459
#% 1484617
#% 1495442
#% 1536521
#% 1746858
#% 1750408
#% 1813671
#! In this paper, we investigate a problem of predicting what images are likely to appear on the Web at a future time point, given a query word and a database of historical image streams that potentiates learning of uploading patterns of previous user images and associated metadata. We address such a Web image prediction problem at both a collective group level and an individual user level. We develop a predictive framework based on the multivariate point process, which employs a stochastic parametric model to solve the relations between image occurrence and the covariates that influence it, in a flexible, scalable, and globally optimal way. Using Flickr datasets of more than ten million images of 40 topics, our empirical results show that the proposed algorithm is more successful in predicting unseen Web images than other candidate methods, including forecasting on semantic meanings only, a PageRank-based image retrieval, and a generative author-time topic model.

#index 1872360
#* Transductive multi-label ensemble classification for protein function prediction
#@ Guoxian Yu;Carlotta Domeniconi;Huzefa Rangwala;Guoji Zhang;Zhiwen Yu
#t 2012
#c 0
#% 451221
#% 830744
#% 832903
#% 833913
#% 905823
#% 906025
#% 916945
#% 961218
#% 1047785
#% 1136500
#% 1229209
#% 1414236
#% 1447814
#% 1606394
#% 1622019
#% 1748455
#% 1750425
#% 1779363
#! Advances in biotechnology have made available multitudes of heterogeneous proteomic and genomic data. Integrating these heterogeneous data sources, to automatically infer the function of proteins, is a fundamental challenge in computational biology. Several approaches represent each data source with a kernel (similarity) function. The resulting kernels are then integrated to determine a composite kernel, which is used for developing a function prediction model. Proteins are also found to have multiple roles and functions. As such, several approaches cast the protein function prediction problem within a multi-label learning framework. In our work we develop an approach that takes advantage of several unlabeled proteins, along with multiple data sources and multiple functions of proteins. We develop a graph-based transductive multi-label classifier (TMC) that is evaluated on a composite kernel, and also propose a method for data integration using the ensemble framework, called transductive multi-label ensemble classifier (TMEC). The TMEC approach trains a graph-based multi-label classifier for each individual kernel, and then combines the predictions of the individual models. Our contribution is the use of a bi-relational directed graph that captures relationships between pairs of proteins, between pairs of functions, and between proteins and functions. We evaluate the ability of TMC and TMEC to predict the functions of proteins by using two yeast datasets. We show that our approach performs better than recently proposed protein function prediction methods on composite and multiple kernels.

#index 1872361
#* Multi-domain active learning for text classification
#@ Lianghao Li;Xiaoming Jin;Sinno Jialin Pan;Jian-Tao Sun
#t 2012
#c 0
#% 116165
#% 169717
#% 458379
#% 464268
#% 714351
#% 722797
#% 769886
#% 916788
#% 983828
#% 1108902
#% 1117691
#% 1130870
#% 1190064
#% 1211696
#% 1261539
#% 1270196
#% 1272126
#% 1377374
#% 1400008
#% 1472892
#% 1482394
#! Active learning has been proven to be effective in reducing labeling efforts for supervised learning. However, existing active learning work has mainly focused on training models for a single domain. In practical applications, it is common to simultaneously train classifiers for multiple domains. For example, some merchant web sites (like Amazon.com) may need a set of classifiers to predict the sentiment polarity of product reviews collected from various domains (e.g., electronics, books, shoes). Though different domains have their own unique features, they may share some common latent features. If we apply active learning on each domain separately, some data instances selected from different domains may contain duplicate knowledge due to the common features. Therefore, how to choose the data from multiple domains to label is crucial to further reducing the human labeling efforts in multi-domain learning. In this paper, we propose a novel multi-domain active learning framework to jointly select data instances from all domains with duplicate information considered. In our solution, a shared subspace is first learned to represent common latent features of different domains. By considering the common and the domain-specific features together, the model loss reduction induced by each data instance can be decomposed into a common part and a domain-specific part. In this way, the duplicate information across domains can be encoded into the common part of model loss reduction and taken into account when querying. We compare our method with the state-of-the-art active learning approaches on several text classification tasks: sentiment classification, newsgroup classification and email spam filtering. The experiment results show that our method reduces the human labeling efforts by 33.2%, 42.9% and 68.7% on the three tasks, respectively.

#index 1872362
#* Modeling disease progression via fused sparse group lasso
#@ Jiayu Zhou;Jun Liu;Vaibhav A. Narayan;Jieping Ye
#t 2012
#c 0
#% 267046
#% 299012
#% 576520
#% 723239
#% 823420
#% 829014
#% 875970
#% 916788
#% 1128929
#% 1211707
#% 1211836
#% 1357157
#% 1451171
#% 1606019
#! Alzheimer's Disease (AD) is the most common neurodegenerative disorder associated with aging. Understanding how the disease progresses and identifying related pathological biomarkers for the progression is of primary importance in Alzheimer's disease research. In this paper, we develop novel multi-task learning techniques to predict the disease progression measured by cognitive scores and select biomarkers predictive of the progression. In multi-task learning, the prediction of cognitive scores at each time point is considered as a task, and multiple prediction tasks at different time points are performed simultaneously to capture the temporal smoothness of the prediction models across different time points. Specifically, we propose a novel convex fused sparse group Lasso (cFSGL) formulation that allows the simultaneous selection of a common set of biomarkers for multiple time points and specific sets of biomarkers for different time points using the sparse group Lasso penalty and in the meantime incorporates the temporal smoothness using the fused Lasso penalty. The proposed formulation is challenging to solve due to the use of several non-smooth penalties. We show that the proximal operator associated with the proposed formulation exhibits a certain decomposition property and can be computed efficiently; thus cFSGL can be solved efficiently using the accelerated gradient method. To further improve the model, we propose two non-convex formulations to reduce the shrinkage bias inherent in the convex formulation. We employ the difference of convex programming technique to solve the non-convex formulations. Our extensive experiments using data from the Alzheimer's Disease Neuroimaging Initiative demonstrate the effectiveness of the proposed progression models in comparison with existing methods for disease progression. We also perform longitudinal stability selection to identify and analyze the temporal patterns of biomarkers in disease progression.

#index 1872363
#* Open domain event extraction from twitter
#@ Alan Ritter; Mausam;Oren Etzioni;Sam Clark
#t 2012
#c 0
#% 262042
#% 262043
#% 464434
#% 740900
#% 754106
#% 757350
#% 766444
#% 817550
#% 939376
#% 939595
#% 939941
#% 1183376
#% 1214671
#% 1214715
#% 1215367
#% 1261542
#% 1275182
#% 1400018
#% 1451233
#% 1470574
#% 1470582
#% 1470583
#% 1471222
#% 1471328
#% 1477267
#% 1481659
#% 1536525
#% 1560198
#% 1560429
#% 1591965
#% 1591968
#% 1592026
#% 1592065
#% 1592082
#% 1592152
#% 1604661
#% 1605960
#% 1701368
#% 1711778
#% 1711785
#% 1711814
#% 1711858
#% 1711864
#% 1711865
#! Tweets are the most up-to-date and inclusive stream of in- formation and commentary on current events, but they are also fragmented and noisy, motivating the need for systems that can extract, aggregate and categorize important events. Previous work on extracting structured representations of events has focused largely on newswire text; Twitter's unique characteristics present new challenges and opportunities for open-domain event extraction. This paper describes TwiCal-- the first open-domain event-extraction and categorization system for Twitter. We demonstrate that accurately extracting an open-domain calendar of significant events from Twitter is indeed feasible. In addition, we present a novel approach for discovering important event categories and classifying extracted events based on latent variable models. By leveraging large volumes of unlabeled data, our approach achieves a 14% increase in maximum F1 over a supervised baseline. A continuously updating demonstration of our system can be viewed at http://statuscalendar.com; Our NLP tools are available at http://github.com/aritter/ twitter_nlp.

#index 1872364
#* Stratified k-means clustering over a deep web data source
#@ Tantan Liu;Gagan Agrawal
#t 2012
#c 0
#% 36672
#% 236729
#% 466083
#% 481779
#% 577261
#% 629652
#% 721137
#% 722802
#% 777930
#% 785342
#% 881506
#% 881548
#% 948115
#% 960286
#% 1127353
#% 1127356
#% 1127395
#% 1127413
#% 1127557
#% 1176857
#% 1206906
#% 1396657
#% 1535432
#% 1594622
#% 1813854
#! This paper focuses on the problem of clustering data from a {\em hidden} or a deep web data source. A key characteristic of deep web data sources is that data can only be accessed through the limited query interface they support. Because the underlying data set cannot be accessed directly, data mining must be performed based on sampling of the datasets. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs. We have developed a new stratified clustering method addressing this problem for a deep web data source. Specifically, we have developed a stratified k-means clustering method. In our approach, the space of input attributes of a deep web data source is stratified for capturing the relationship between the input and the output attributes. The space of output attributes of a deep web data source is partitioned into sub-spaces. Three representative sampling methods are developed in this paper, with the goal of achieving a good estimation of the statistics, including proportions and centers, within the sub-spaces of the output attributes. We have evaluated our methods using two synthetic and two real datasets. Our comparison shows significant gains in estimation accuracy from both the novel aspects of our work, i.e., the use of stratification(5%-55%), and our and representative sampling methods(up to 54%).

#index 1872365
#* Metro maps of science
#@ Dafna Shahaf;Carlos Guestrin;Eric Horvitz
#t 2012
#c 0
#% 340883
#% 775988
#% 836516
#% 1451202
#% 1586082
#% 1598408
#% 1605962
#% 1746887
#! As the number of scientific publications soars, even the most enthusiastic reader can have trouble staying on top of the evolving literature. It is easy to focus on a narrow aspect of one's field and lose track of the big picture. Information overload is indeed a major challenge for scientists today, and is especially daunting for new investigators attempting to master a discipline and scientists who seek to cross disciplinary borders. In this paper, we propose metrics of influence, coverage and connectivity for scientific literature. We use these metrics to create structured summaries of information, which we call metro maps. Most importantly, metro maps explicitly show the relations between papers in a way which captures developments in the field. Pilot user studies demonstrate that our method helps researchers acquire new knowledge efficiently: map users achieved better precision and recall scores and found more seminal papers while performing fewer searches.

#index 1872366
#* Active sampling for entity matching
#@ Kedar Bellare;Suresh Iyengar;Aditya G. Parameswaran;Vibhor Rastogi
#t 2012
#c 0
#% 2115
#% 170649
#% 310516
#% 350103
#% 577238
#% 729913
#% 913783
#% 983848
#% 1022229
#% 1073898
#% 1136226
#% 1176916
#% 1211696
#% 1211872
#% 1217163
#% 1314445
#% 1396658
#% 1426567
#% 1675764
#! In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or non-duplicates is the one of selecting informative training examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0-1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more non-duplicate pairs than duplicate pairs). To address this, a recent paper [1] proposes to maximize recall of the classifier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst-case. Our main result is an active learning algorithm that approximately maximizes recall of the classifier while respecting a precision constraint with provably sub-linear label complexity (under certain distributional assumptions). Our algorithm uses as a black-box any active learning module that minimizes 0-1 loss. We show that label complexity of our algorithm is at most log n times the label complexity of the black-box, and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint. We provide an empirical evaluation of our algorithm on several real-world matching data sets that demonstrates the effectiveness of our approach.

#index 1872367
#* An integrated data mining approach to real-time clinical monitoring and deterioration warning
#@ Yi Mao;Wenlin Chen;Yixin Chen;Chenyang Lu;Marin Kollef;Thomas Bailey
#t 2012
#c 0
#% 485797
#% 915253
#% 929722
#% 1041316
#% 1083738
#% 1132743
#% 1451157
#% 1606019
#! Clinical study found that early detection and intervention are essential for preventing clinical deterioration in patients, for patients both in intensive care units (ICU) as well as in general wards but under real-time data sensing (RDS). In this paper, we develop an integrated data mining approach to give early deterioration warnings for patients under real-time monitoring in ICU and RDS. Existing work on mining real-time clinical data often focus on certain single vital sign and specific disease. In this paper, we consider an integrated data mining approach for general sudden deterioration warning. We synthesize a large feature set that includes first and second order time-series features, detrended fluctuation analysis (DFA), spectral analysis, approximative entropy, and cross-signal features. We then systematically apply and evaluate a series of established data mining methods, including forward feature selection, linear and nonlinear classification algorithms, and exploratory undersampling for class imbalance. An extensive empirical study is conducted on real patient data collected between 2001 and 2008 from a variety of ICUs. Results show the benefit of each of the proposed techniques, and the final integrated approach significantly improves the prediction quality. The proposed clinical warning system is currently under integration with the electronic medical record system at Barnes-Jewish Hospital in preparation for a clinical trial. This work represents a promising step toward general early clinical warning which has the potential to significantly improve the quality of patient care in hospitals.

#index 1872368
#* Multi-source learning for joint analysis of incomplete multi-modality neuroimaging data
#@ Lei Yuan;Yalin Wang;Paul M. Thompson;Vaibhav A. Narayan;Jieping Ye
#t 2012
#c 0
#% 551723
#% 916788
#% 1083738
#% 1128929
#% 1417091
#% 1441070
#% 1451171
#% 1556148
#! Incomplete data present serious problems when integrating large-scale brain imaging data sets from different imaging modalities. In the Alzheimer's Disease Neuroimaging Initiative (ADNI), for example, over half of the subjects lack cerebrospinal fluid (CSF) measurements; an independent half of the subjects do not have fluorodeoxyglucose positron emission tomography (FDG-PET) scans; many lack proteomics measurements. Traditionally, subjects with missing measures are discarded, resulting in a severe loss of available information. We address this problem by proposing two novel learning methods where all the samples (with at least one available data source) can be used. In the first method, we divide our samples according to the availability of data sources, and we learn shared sets of features with state-of-the-art sparse learning methods. Our second method learns a base classifier for each data source independently, based on which we represent each source using a single column of prediction scores; we then estimate the missing prediction scores, which, combined with the existing prediction scores, are used to build a multi-source fusion model. To illustrate the proposed approaches, we classify patients from the ADNI study into groups with Alzheimer's disease (AD), mild cognitive impairment (MCI) and normal controls, based on the multi-modality data. At baseline, ADNI's 780 participants (172 AD, 397 MCI, 211 Normal), have at least one of four data types: magnetic resonance imaging (MRI), FDG-PET, CSF and proteomics. These data are used to test our algorithms. Comprehensive experiments show that our proposed methods yield stable and promising results.

#index 1872369
#* RainMon: an integrated approach to mining bursty timeseries monitoring data
#@ Ilari Shafer;Kai Ren;Vishnu Naresh Boddeti;Yoshihisa Abe;Gregory R. Ganger;Christos Faloutsos
#t 2012
#c 0
#% 149237
#% 278011
#% 280408
#% 300136
#% 729943
#% 729952
#% 770890
#% 824709
#% 882221
#% 992857
#% 1188393
#% 1202160
#% 1213038
#% 1214672
#% 1214740
#% 1214752
#% 1290542
#% 1328117
#% 1468194
#% 1523829
#% 1523923
#% 1606081
#% 1620959
#% 1669951
#% 1689735
#% 1862798
#! Metrics like disk activity and network traffic are widespread sources of diagnosis and monitoring information in datacenters and networks. However, as the scale of these systems increases, examining the raw data yields diminishing insight. We present RainMon, a novel end-to-end approach for mining timeseries monitoring data designed to handle its size and unique characteristics. Our system is able to (a) mine large, bursty, real-world monitoring data, (b) find significant trends and anomalies in the data, (c) compress the raw data effectively, and (d) estimate trends to make forecasts. Furthermore, RainMon integrates the full analysis process from data storage to the user interface to provide accessible long-term diagnosis. We apply RainMon to three real-world datasets from production systems and show its utility in discovering anomalous machines and time periods.

#index 1872370
#* SympGraph: a framework for mining clinical notes through symptom relation graphs
#@ Parikshit Sondhi;Jimeng Sun;Hanghang Tong;ChengXiang Zhai
#t 2012
#c 0
#% 80854
#% 290830
#% 348173
#% 1047785
#% 1127384
#% 1132477
#% 1206709
#% 1214701
#% 1310635
#% 1372721
#% 1544034
#% 1566266
#! As an integral part of Electronic Health Records (EHRs), clinical notes pose special challenges for analyzing EHRs due to their unstructured nature. In this paper, we present a general mining framework SympGraph for modeling and analyzing symptom relationships in clinical notes. A SympGraph has symptoms as nodes and co-occurrence relations between symptoms as edges, and can be constructed automatically through extracting symptoms over sequences of clinical notes for a large number of patients. We present an important clinical application of SympGraph: symptom expansion, which can expand a given set of symptoms to other related symptoms by analyzing the underlying SympGraph structure. We further propose a matrix update algorithm which provides a significant computational saving for dynamic updates to the graph. Comprehensive evaluation on 1 million longitudinal clinical notes over 13K patients shows that static symptom expansion can successfully expand a set of known symptoms to a disease with high agreement rate with physician input (average precision 0.46), a 31% improvement over baseline co-occurrence based methods. The experimental results also show that the expanded symptoms can serve as useful features for improving AUC measure for disease diagnosis prediction, thus confirming the potential clinical value of our work.

#index 1872371
#* Experiences and lessons in developing industry-strength machine learning and data mining software
#@ Chih-Jen Lin
#t 2012
#c 0
#! Traditionally academic machine learning and data mining researchers focus on proposing new algorithms. The task of implementing these methods is often left to companies that are developing software packages. However, the gap between the two sides has caused some problems. First, the practical deployment of new algorithms still involves some challenging issues that need to be studied by researchers. Second, without further investigation after publishing their papers, researchers have neither the opportunity to work with real problems nor see how their methods are used. We discuss the experiences in developing two machine learning packages LIBSVM and LIBLINEAR, that are widely used in both academia and industry. We demonstrate that the interaction with users leads us to identify some important research problems. For example, the decision to study and then support multi-class SVM was essential in the early stage of developing LIBSVM. The birth of LIBLINEAR was driven by the need to classify large-scale documents in Internet companies. For fast training of large-scale problems, we had to create new algorithms other than those used in LIBSVM for kernel SVM. We present some practical use of LIBLINEAR for Internet applications. Finally, we give lessons learned and future perspectives for developing industry-strength machine learning and data mining software.

#index 1872372
#* Joint optimization of bid and budget allocation in sponsored search
#@ Weinan Zhang;Ying Zhang;Bin Gao;Yong Yu;Xiaojie Yuan;Tie-Yan Liu
#t 2012
#c 0
#% 725236
#% 868445
#% 956547
#% 963332
#% 1055886
#% 1074101
#% 1190078
#% 1355052
#% 1536556
#! This paper is concerned with the joint allocation of bid price and campaign budget in sponsored search. In this application, an advertiser can create a number of campaigns and set a budget for each of them. In a campaign, he/she can further create several ad groups with bid keywords and bid prices. Data analysis shows that many advertisers are dealing with a very large number of campaigns, bid keywords, and bid prices at the same time, which poses a great challenge to the optimality of their campaign management. As a result, the budgets of some campaigns might be too low to achieve the desired performance goals while those of some other campaigns might be wasted; the bid prices for some keywords may be too low to win competitive auctions while those of some other keywords may be unnecessarily high. In this paper, we propose a novel algorithm to automatically address this issue. In particular, we model the problem as a constrained optimization problem, which maximizes the expected advertiser revenue subject to the constraints of the total budget of the advertiser and the ranges of bid price change. By solving this optimization problem, we can obtain an optimal budget allocation plan as well as an optimal bid price setting. Our simulation results based on the sponsored search log of a commercial search engine have shown that by employing the proposed method, we can effectively improve the performances of the advertisers while at the same time we also see an increase in the revenue of the search engine. In addition, the results indicate that this method is robust to the second-order effects caused by the bid fluctuations from other advertisers.

#index 1872373
#* The untold story of the clones: content-agnostic factors that impact YouTube video popularity
#@ Youmna Borghol;Sebastien Ardon;Niklas Carlsson;Derek Eager;Anirban Mahanti
#t 2012
#c 0
#% 79312
#% 1002006
#% 1309092
#% 1411585
#% 1487840
#% 1536579
#% 1558841
#% 1621790
#! Video dissemination through sites such as YouTube can have widespread impacts on opinions, thoughts, and cultures. Not all videos will reach the same popularity and have the same impact. Popularity differences arise not only because of differences in video content, but also because of other "content-agnostic" factors. The latter factors are of considerable interest but it has been difficult to accurately study them. For example, videos uploaded by users with large social networks may tend to be more popular because they tend to have more interesting content, not because social network size has a substantial direct impact on popularity. In this paper, we develop and apply a methodology that is able to accurately assess, both qualitatively and quantitatively, the impacts of various content-agnostic factors on video popularity. When controlling for video content, we observe a strong linear "rich-get-richer" behavior, with the total number of previous views as the most important factor except for very young videos. The second most important factor is found to be video age. We analyze a number of phenomena that may contribute to rich-get-richer, including the first-mover advantage, and search bias towards popular videos. For young videos we find that factors other than the total number of previous views, such as uploader characteristics and number of keywords, become relatively more important. Our findings also confirm that inaccurate conclusions can be reached when not controlling for content.

#index 1872374
#* SHALE: an efficient algorithm for allocation of guaranteed display advertising
#@ Vijay Bharadwaj;Peiji Chen;Wenjing Ma;Chandrashekhar Nagarajan;John Tomlin;Sergei Vassilvitskii;Erik Vee;Jian Yang
#t 2012
#c 0
#% 1222625
#% 1336448
#% 1379556
#% 1426653
#% 1496079
#% 1606074
#% 1668301
#! Motivated by the problem of optimizing allocation in guaranteed display advertising, we develop an efficient, lightweight method of generating a compact allocation plan that can be used to guide ad server decisions. The plan itself uses just O(1) state per guaranteed contract, is robust to noise, and allows us to serve (provably) nearly optimally. The optimization method we develop is scalable, with a small in-memory footprint, and working in linear time per iteration. It is also "stop-anytime", meaning that time-critical applications can stop early and still get a good serving solution. Thus, it is particularly useful for optimizing the large problems arising in the context of display advertising. We demonstrate the effectiveness of our algorithm using actual Yahoo! data.

#index 1872375
#* Factoring past exposure in display advertising targeting
#@ Neha Gupta;Abhimanyu Das;Sandeep Pandey;Vijay K. Narayanan
#t 2012
#c 0
#% 722904
#% 1023420
#% 1190081
#% 1214642
#% 1214692
#% 1399936
#% 1407385
#% 1450847
#% 1451141
#% 1451161
#% 1535253
#! Online advertising is becoming more and more performance oriented where the decision to show an advertisement to a user is made based on the user's propensity to respond to the ad in a positive manner, (e.g., purchasing a product, subscribing to an email list). The user response depends on how well the ad campaign matches to the user's interest, as well as the amount of user's past exposure to the campaign - a factor shown to be impactful in controlled experimental studies. Past exposure builds brand-awareness and familiarity with the user, which in turn leads to a higher propensity of the user to buy/convert on the ad impression. In this paper we propose a model of the user response to an ad campaign as a function of both the interest match and the past exposure, where the interest match is estimated using historical search/browse activities of the user. The goal of this paper is two-fold. First, we demonstrate the role played by the user interest and the past exposure in modeling user response by jointly estimating the parameters of these factors. We test this response model over hundreds of real ad campaigns. Second, we use the findings from this joint model to identify more relevant target users for ad campaigns. In particular, we show that on real advertising data this model combines past exposure together with the user profile to identify better target users over the conventional targeting models.

#index 1872376
#* Online allocation of display ads with smooth delivery
#@ Anand Bhalgat;Jon Feldman;Vahab Mirrokni
#t 2012
#c 0
#% 836518
#% 1222625
#% 1336462
#% 1404809
#% 1426653
#% 1451192
#% 1496079
#% 1606074
#% 1668301
#! Display ads on the Internet are often sold in bundles of thousands or millions of impressions over a particular time period, typically weeks or months. Ad serving systems that assign ads to pages on behalf of publishers must satisfy these contracts, but at the same time try to maximize overall quality of placement. This is usually modeled in the literature as an online allocation problem, where contracts are represented by overall delivery constraints over a finite time horizon. However this model misses an important aspect of ad delivery: time homogeneity. Advertisers who buy these packages expect their ad to be shown smoothly throughout the purchased time period, in order to reach a wider audience, to have a sustained impact, and to support the ads they are running on other media (e.g., television). In this paper we formalize this problem using several nested packing constraints, and develop a tight (1-1/e)-competitive online algorithm for this problem. Our algorithms and analysis require novel techniques as they involve online computation of multiple dual variables per ad. We then show the effectiveness of our algorithms through exhaustive simulation studies on real data sets.

#index 1872377
#* Streaming graph partitioning for large distributed graphs
#@ Isabelle Stanton;Gabriel Kliot
#t 2012
#c 0
#% 202285
#% 202286
#% 290262
#% 469887
#% 898311
#% 925577
#% 1002007
#% 1169663
#% 1198212
#% 1231059
#% 1237170
#% 1386131
#% 1399992
#% 1475077
#% 1523711
#% 1531275
#% 1566266
#% 1621140
#% 1783374
#! Extracting knowledge by performing computations on graphs is becoming increasingly challenging as graphs grow in size. A standard approach distributes the graph over a cluster of nodes, but performing computations on a distributed graph is expensive if large amount of data have to be moved. Without partitioning the graph, communication quickly becomes a limiting factor in scaling the system up. Existing graph partitioning heuristics incur high computation and communication cost on large graphs, sometimes as high as the future computation itself. Observing that the graph has to be loaded into the cluster, we ask if the partitioning can be done at the same time with a lightweight streaming algorithm. We propose natural, simple heuristics and compare their performance to hashing and METIS, a fast, offline heuristic. We show on a large collection of graph datasets that our heuristics are a significant improvement, with the best obtaining an average gain of 76%. The heuristics are scalable in the size of the graphs and the number of partitions. Using our streaming partitioning methods, we are able to speed up PageRank computations on Spark, a distributed computation system, by 18% to 39% for large social networks.

#index 1872378
#* RolX: structural role extraction & mining in large graphs
#@ Keith Henderson;Brian Gallagher;Tina Eliassi-Rad;Hanghang Tong;Sugato Basu;Leman Akoglu;Danai Koutra;Christos Faloutsos;Lei Li
#t 2012
#c 0
#% 833913
#% 995168
#% 1083652
#% 1110367
#% 1202419
#% 1211714
#% 1260689
#% 1272187
#% 1305496
#% 1318599
#% 1451155
#% 1451163
#% 1451172
#% 1451231
#% 1491558
#% 1605987
#% 1710593
#% 1787250
#% 1813854
#! Given a network, intuitively two nodes belong to the same role if they have similar structural behavior. Roles should be automatically determined from the data, and could be, for example, "clique-members," "periphery-nodes," etc. Roles enable numerous novel and useful network-mining tasks, such as sense-making, searching for similar nodes, and node classification. This paper addresses the question: Given a graph, how can we automatically discover roles for nodes? We propose RolX (Role eXtraction), a scalable (linear in the number of edges), unsupervised learning approach for automatically extracting structural roles from general network data. We demonstrate the effectiveness of RolX on several network-mining tasks: from exploratory data analysis to network transfer learning. Moreover, we compare network role discovery with network community discovery. We highlight fundamental differences between the two (e.g., roles generalize across disconnected networks, communities do not); and show that the two approaches are complimentary in nature.

#index 1872379
#* Fast algorithms for maximal clique enumeration with limited memory
#@ James Cheng;Linhong Zhu;Yiping Ke;Shumo Chu
#t 2012
#c 0
#% 41684
#% 318330
#% 322619
#% 466664
#% 732477
#% 765682
#% 937814
#% 1124599
#% 1130969
#% 1179877
#% 1192926
#% 1360738
#% 1426539
#% 1594586
#% 1597269
#% 1605988
#% 1625106
#% 1693358
#% 1848109
#% 1907284
#! Maximal clique enumeration (MCE) is a long-standing problem in graph theory and has numerous important applications. Though extensively studied, most existing algorithms become impractical when the input graph is too large and is disk-resident. We first propose an efficient partition-based algorithm for MCE that addresses the problem of processing large graphs with limited memory. We then further reduce the high cost of CPU computation of MCE by a careful nested partition based on a cost model. Finally, we parallelize our algorithm to further reduce the overall running time. We verified the efficiency of our algorithms by experiments in large real-world graphs.

#index 1872380
#* Summarization-based mining bipartite graphs
#@ Jing Feng;Xiao He;Bettina Konte;Christian Böhm;Claudia Plant
#t 2012
#c 0
#% 342621
#% 466850
#% 476708
#% 729918
#% 730089
#% 769883
#% 823396
#% 844369
#% 881487
#% 1063501
#% 1063512
#% 1078626
#% 1176960
#% 1211824
#% 1451163
#! How to extract the truly relevant information from a large relational data set? The answer of this paper is a technique integrating graph summarization, graph clustering, link prediction and the discovery of the hidden structure on the basis of data compression. Our novel algorithm SCMiner (for Summarization-Compression Miner) reduces a large bipartite input graph to a highly compact representation which is very useful for different data mining tasks: 1) Clustering: The compact summary graph contains the truly relevant clusters of both types of nodes of a bipartite graph. 2) Link prediction: The compression scheme of SCMiner reveals suspicious edges which are probably erroneous as well as missing edges, i.e. pairs of nodes which should be connected by an edge. 3) Discovery of the hidden structure: Unlike traditional co-clustering methods, the result of SCMiner is not limited to row- and column-clusters. Besides the clusters, the summary graph also contains the essential relationships between both types of clusters and thus reveals the hidden structure of the data. Extensive experiments on synthetic and real data demonstrate that SCMiner outperforms state-of-the-art techniques for clustering and link prediction. Moreover, SCMiner discovers the hidden structure and reports it in an interpretable way to the user. Based on data compression, our technique does not rely on any input parameters which are difficult to estimate.

#index 1872381
#* Mining coherent subgraphs in multi-layer graphs with edge labels
#@ Brigitte Boden;Stephan Günnemann;Holger Hoffmann;Thomas Seidl
#t 2012
#c 0
#% 241
#% 464888
#% 823347
#% 864460
#% 881553
#% 989636
#% 1108882
#% 1165480
#% 1267707
#% 1318668
#% 1372657
#% 1535395
#% 1617292
#% 1642075
#! Mining dense subgraphs such as cliques or quasi-cliques is an important graph mining problem and closely related to the notion of graph clustering. In various applications, graphs are enriched by additional information. For example, we can observe graphs representing different types of relations between the vertices. These multiple edge types can also be viewed as different "layers" of the same graph, which is denoted as a "multi-layer graph" in this work. Additionally, each edge might be annotated by a label characterizing the given relation in more detail. By exploiting all these different kinds of information, the detection of more interesting clusters in the graph can be supported. In this work, we introduce the multi-layer coherent subgraph (MLCS) model, which defines clusters of vertices that are densely connected by edges with similar labels in a subset of the graph layers. We avoid redundancy in the result by selecting only the most interesting, non-redundant clusters for the output. Based on this model, we introduce the best-first search algorithm MiMAG. In thorough experiments we demonstrate the strengths of MiMAG in comparison with related approaches on synthetic as well as real-world datasets.

#index 1872382
#* Circle-based recommendation in online social networks
#@ Xiwang Yang;Harald Steck;Yong Liu
#t 2012
#c 0
#% 577217
#% 790459
#% 813966
#% 818216
#% 879627
#% 983903
#% 989580
#% 1001279
#% 1083671
#% 1130901
#% 1214666
#% 1227602
#% 1275183
#% 1385585
#% 1451209
#% 1472299
#% 1476461
#% 1536533
#% 1625376
#! Online social network information promises to increase recommendation accuracy beyond the capabilities of purely rating/feedback-driven recommender systems (RS). As to better serve users' activities across different domains, many online social networks now support a new feature of "Friends Circles", which refines the domain-oblivious "Friends" concept. RS should also benefit from domain-specific "Trust Circles". Intuitively, a user may trust different subsets of friends regarding different domains. Unfortunately, in most existing multi-category rating datasets, a user's social connections from all categories are mixed together. This paper presents an effort to develop circle-based RS. We focus on inferring category-specific social trust circles from available rating data combined with social network data. We outline several variants of weighting friends within circles based on their inferred expertise levels. Through experiments on publicly available data, we demonstrate that the proposed circle-based recommendation models can better utilize user's social trust information, resulting in increased recommendation accuracy.

#index 1872383
#* Incorporating heterogeneous information for personalized tag recommendation in social tagging systems
#@ Wei Feng;Jianyong Wang
#t 2012
#c 0
#% 1055704
#% 1100174
#% 1127455
#% 1127466
#% 1214694
#% 1227601
#% 1227644
#% 1287228
#% 1327635
#% 1355024
#% 1429408
#% 1482259
#% 1482399
#% 1536568
#% 1558473
#% 1605923
#! A social tagging system provides users an effective way to collaboratively annotate and organize items with their own tags. A social tagging system contains heterogeneous information like users' tagging behaviors, social networks, tag semantics and item profiles. All the heterogeneous information helps alleviate the cold start problem due to data sparsity. In this paper, we model a social tagging system as a multi-type graph. To learn the weights of different types of nodes and edges, we propose an optimization framework, called OptRank. OptRank can be characterized as follows:(1) Edges and nodes are represented by features. Different types of edges and nodes have different set of features. (2) OptRank learns the best feature weights by maximizing the average AUC (Area Under the ROC Curve) of the tag recommender. We conducted experiments on two publicly available datasets, i.e., Delicious and Last.fm. Experimental results show that: (1) OptRank outperforms the existing graph based methods when only (user, tag, item) relation is available. (2) OptRank successfully improves the results by incorporating social network, tag semantics and item profiles.

#index 1872384
#* Cross-domain collaboration recommendation
#@ Jie Tang;Sen Wu;Jimeng Sun;Hang Su
#t 2012
#c 0
#% 220708
#% 220709
#% 280819
#% 283833
#% 387427
#% 722904
#% 734594
#% 769906
#% 788043
#% 844334
#% 867050
#% 879570
#% 881526
#% 955712
#% 956521
#% 989621
#% 1083734
#% 1214703
#% 1227601
#% 1399997
#% 1451163
#% 1451238
#% 1538534
#% 1588381
#% 1598567
#% 1605963
#% 1625374
#% 1642411
#% 1650437
#! Interdisciplinary collaborations have generated huge impact to society. However, it is often hard for researchers to establish such cross-domain collaborations. What are the patterns of cross-domain collaborations? How do those collaborations form? Can we predict this type of collaborations? Cross-domain collaborations exhibit very different patterns compared to traditional collaborations in the same domain: 1) sparse connection: cross-domain collaborations are rare; 2) complementary expertise: cross-domain collaborators often have different expertise and interest; 3) topic skewness: cross-domain collaboration topics are focused on a subset of topics. All these patterns violate fundamental assumptions of traditional recommendation systems. In this paper, we analyze the cross-domain collaboration data from research publications and confirm the above patterns. We propose the Cross-domain Topic Learning (CTL) model to address these challenges. For handling sparse connections, CTL consolidates the existing cross-domain collaborations through topic layers instead of at author layers, which alleviates the sparseness issue. For handling complementary expertise, CTL models topic distributions from source and target domains separately, as well as the correlation across domains. For handling topic skewness, CTL only models relevant topics to the cross-domain collaboration. We compare CTL with several baseline approaches on large publication datasets from different domains. CTL outperforms baselines significantly on multiple recommendation metrics. Beyond accurate recommendation performance, CTL is also insensitive to parameter tuning as confirmed in the sensitivity analysis.

#index 1872385
#* RecMax: exploiting recommender systems for fun and profit
#@ Amit Goyal;Laks V.S. Lakshmanan
#t 2012
#c 0
#% 280852
#% 330687
#% 342596
#% 420515
#% 656708
#% 729923
#% 813966
#% 879627
#% 987671
#% 1044003
#% 1055690
#% 1214641
#% 1541728
#% 1625369
#% 1693922
#! In recent times, collaborative filtering based Recommender Systems (RS) have become extremely popular. While research in recommender systems has mostly focused on improving the accuracy of recommendations, in this paper, we look at the "flip" side of a RS. That is, instead of improving existing recommender algorithms, we ask whether we can use an existing operational RS to launch a targeted marketing campaign. To this end, we propose a novel problem called RecMax that aims to select a set of "seed" users for a marketing campaign for a new product, such that if they endorse the product by providing relatively high ratings, the number of other users to whom the product is recommended by the underlying RS algorithm is maximum. We motivate RecMax with real world applications. We show that seeding can make a substantial difference, if done carefully. We prove that RecMax is not only NP-hard to solve optimally, it is NP-hard to even approximate within any reasonable factor. Given this hardness, we explore several natural heuristics on 3 real world datasets - Movielens, Yahoo! Music and Jester Joke and report our findings. We show that even though RecMax is hard to approximate, simple natural heuristics may provide impressive gains, for targeted marketing using RS.

#index 1872386
#* Learning personal + social latent factor model for social recommendation
#@ Yelong Shen;Ruoming Jin
#t 2012
#c 0
#% 26568
#% 330687
#% 790459
#% 813966
#% 840924
#% 1083671
#% 1117695
#% 1227602
#% 1260273
#% 1275183
#% 1385585
#% 1536533
#% 1560408
#! Social recommendation, which aims to systematically leverage the social relationships between users as well as their past behaviors for automatic recommendation, attract much attention recently. The belief is that users linked with each other in social networks tend to share certain common interests or have similar tastes (homophily principle); such similarity is expected to help improve the recommendation accuracy and quality. There have been a few studies on social recommendations; however, they almost completely ignored the heterogeneity and diversity of the social relationship. In this paper, we develop a joint personal and social latent factor (PSLF) model for social recommendation. Specifically, it combines the state-of-the-art collaborative filtering and the social network modeling approaches for social recommendation. Especially, the PSLF extracts the social factor vectors for each user based on the state-of-the-art mixture membership stochastic blockmodel, which can explicitly express the varieties of the social relationship. To optimize the PSLF model, we develop a scalable expectation-maximization (EM) algorithm, which utilizes a novel approximate mean-field technique for fast expectation computation. We compare our approach with the latest social recommendation approaches on two real datasets, Flixter and Douban (both with large social networks). With similar training cost, our approach has shown a significant improvement in terms of prediction accuracy criteria over the existing approaches.

#index 1872387
#* Two approaches to understanding when constraints help clustering
#@ Ian Davidson
#t 2012
#c 0
#% 217824
#% 464291
#% 466890
#% 593790
#% 715529
#% 769881
#% 829025
#% 948091
#% 1275122
#% 1663626
#! Most algorithm work in data mining focuses on designing algorithms to address a learning problem. Here we focus our attention on designing algorithms to determine the ease or difficulty of a problem instance. The area of clustering under constraints has recently received much attention in the data mining community. We can view the constraints as restricting (either directly or indirectly) the search space of a clustering algorithm to just feasible clusterings. However, to our knowledge no work explores methods to count the feasible clusterings or other measures of difficulty nor the importance of these measures. We present two approaches to efficiently characterize the difficulty of satisfying must-link (ML) and cannot-link (CL) constraints: calculating the fractional chromatic polynomial of the constraint graph using LP and approximately counting the number of feasible clusterings using MCMC samplers. We show that these measures are correlated to the classical performance measures of constrained clustering algorithms. From these insights and our algorithms we construct new methods of generating and pruning constraints and empirically demonstrate their usefulness.

#index 1872388
#* Chromatic correlation clustering
#@ Francesco Bonchi;Aristides Gionis;Francesco Gullo;Antti Ukkonen
#t 2012
#c 0
#% 722904
#% 765548
#% 847164
#% 1091267
#% 1232243
#% 1318692
#% 1426512
#% 1523971
#% 1594585
#% 1607320
#% 1635163
#% 1642103
#% 1688435
#% 1763764
#! We study a novel clustering problem in which the pairwise relations between objects are categorical. This problem can be viewed as clustering the vertices of a graph whose edges are of different types (colors). We introduce an objective function that aims at partitioning the graph such that the edges within each cluster have, as much as possible, the same color. We show that the problem is NP-hard and propose a randomized algorithm with approximation guarantee proportional to the maximum degree of the input graph. The algorithm iteratively picks a random edge as pivot, builds a cluster around it, and removes the cluster from the graph. Although being fast, easy-to-implement, and parameter free, this algorithm tends to produce a relatively large number of clusters. To overcome this issue we introduce a variant algorithm, which modifies how the pivot is chosen and and how the cluster is built around the pivot. Finally, to address the case where a fixed number of output clusters is required, we devise a third algorithm that directly optimizes the objective function via a strategy based on the alternating minimization paradigm. We test our algorithms on synthetic and real data from the domains of protein-interaction networks, social media, and bibliometrics. Experimental evidence show that our algorithms outperform a baseline algorithm both in the task of reconstructing a ground-truth clustering and in terms of objective function value.

#index 1872389
#* Locally-scaled spectral clustering using empty region graphs
#@ Carlos D. Correa;Peter Lindstrom
#t 2012
#c 0
#% 124409
#% 257461
#% 313959
#% 342621
#% 363808
#% 420078
#% 718494
#% 755402
#% 1096306
#% 1108904
#% 1133179
#% 1297641
#% 1308039
#% 1914464
#! This paper introduces a new method for estimating the local neighborhood and scale of data points to improve the robustness of spectral clustering algorithms. We employ a subset of empty region graphs - the β-skeleton - and non-linear diffusion to define a locally-adapted affinity matrix, which, as we demonstrate, provides higher quality clustering than conventional approaches based on κ nearest neighbors or global scale parameters. Moreover, we show that the clustering quality is far less sensitive to the choice of β and other algorithm parameters, and to transformations such as geometric distortion and random perturbation. We summarize the results of an empirical study that applies our method to a number of 2D synthetic data sets, consisting of clusters of arbitrary shape and scale, and to real multi-dimensional classification examples from benchmarks, including image segmentation.

#index 1872390
#* Active spectral clustering via iterative uncertainty reduction
#@ Fabian L. Wauthier;Nebojsa Jojic;Michael I. Jordan
#t 2012
#c 0
#% 313959
#% 453490
#% 466576
#% 722797
#% 731607
#% 732552
#% 743284
#% 789800
#% 870225
#% 916799
#% 938793
#% 995140
#% 1254273
#% 1358071
#% 1535323
#% 1718535
#! Spectral clustering is a widely used method for organizing data that only relies on pairwise similarity measurements. This makes its application to non-vectorial data straight-forward in principle, as long as all pairwise similarities are available. However, in recent years, numerous examples have emerged in which the cost of assessing similarities is substantial or prohibitive. We propose an active learning algorithm for spectral clustering that incrementally measures only those similarities that are most likely to remove uncertainty in an intermediate clustering solution. In many applications, similarities are not only costly to compute, but also noisy. We extend our algorithm to maintain running estimates of the true similarities, as well as estimates of their accuracy. Using this information, the algorithm updates only those estimates which are relatively inaccurate and whose update would most likely remove clustering uncertainty. We compare our methods on several datasets, including a realistic example where similarities are expensive and noisy. The results show a significant improvement in performance compared to the alternatives.

#index 1872391
#* Integrating meta-path selection with user-guided object clustering in heterogeneous information networks
#@ Yizhou Sun;Brandon Norick;Jiawei Han;Xifeng Yan;Philip S. Yu;Xiao Yu
#t 2012
#c 0
#% 280819
#% 313959
#% 464631
#% 466574
#% 722902
#% 722929
#% 722934
#% 769881
#% 770782
#% 829025
#% 840892
#% 876018
#% 916785
#% 989618
#% 989654
#% 995140
#% 1002279
#% 1063503
#% 1117695
#% 1125382
#% 1181261
#% 1214701
#% 1474171
#% 1565432
#! Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is its potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path, a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples ("seeds") than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weights for each meta-path that are consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus, is proposed to learn the model, where the clustering quality and the meta-path weights are mutually enhancing each other. Our experiments with several clustering tasks in two real networks demonstrate the power of the algorithm in comparison with the baselines.

#index 1872392
#* Design principles of massive, robust prediction systems
#@ Troy Raeder;Ori Stitelman;Brian Dalessandro;Claudia Perlich;Foster Provost
#t 2012
#c 0
#% 420064
#% 770847
#% 840913
#% 976824
#% 989676
#% 1200869
#% 1264136
#% 1272062
#% 1451139
#% 1581646
#% 1605943
#% 1605949
#! Most data mining research is concerned with building high-quality classification models in isolation. In massive production systems, however, the ability to monitor and maintain performance over time while growing in size and scope is equally important. Many external factors may degrade classification performance including changes in data distribution, noise or bias in the source data, and the evolution of the system itself. A well-functioning system must gracefully handle all of these. This paper lays out a set of design principles for large-scale autonomous data mining systems and then demonstrates our application of these principles within the m6d automated ad targeting system. We demonstrate a comprehensive set of quality control processes that allow us monitor and maintain thousands of distinct classification models automatically, and to add new models, take on new data, and correct poorly-performing models without manual intervention or system disruption.

#index 1872393
#* PatentMiner: topic-driven patent analysis and mining
#@ Jie Tang;Bo Wang;Yang Yang;Po Hu;Yanting Zhao;Xinyu Yan;Bo Gao;Minlie Huang;Peng Xu;Weichang Li;Adam K. Usadi
#t 2012
#c 0
#% 262112
#% 280819
#% 312701
#% 340948
#% 375017
#% 466574
#% 722904
#% 766409
#% 769906
#% 840967
#% 955502
#% 956510
#% 963669
#% 1083734
#% 1176930
#% 1214702
#% 1481049
#% 1538534
#! Patenting is one of the most important ways to protect company's core business concepts and proprietary technologies. Analyzing large volume of patent data can uncover the potential competitive or collaborative relations among companies in certain areas, which can provide valuable information to develop strategies for intellectual property (IP), R&D, and marketing. In this paper, we present a novel topic-driven patent analysis and mining system. Instead of merely searching over patent content, we focus on studying the heterogeneous patent network derived from the patent database, which is represented by several types of objects (companies, inventors, and technical content) jointly evolving over time. We design and implement a general topic-driven framework for analyzing and mining the heterogeneous patent network. Specifically, we propose a dynamic probabilistic model to characterize the topical evolution of these objects within the patent network. Based on this modeling framework, we derive several patent analytics tools that can be directly used for IP and R&D strategy planning, including a heterogeneous network co-ranking method, a topic-level competitor evolution analysis algorithm, and a method to summarize the search results. We evaluate the proposed methods on a real-world patent database. The experimental results show that the proposed techniques clearly outperform the corresponding baseline methods.

#index 1872394
#* Storytelling in entity networks to support intelligence analysts
#@ M. Shahriar Hossain;Patrick Butler;Arnold P. Boedihardjo;Naren Ramakrishnan
#t 2012
#c 0
#% 109215
#% 598098
#% 740266
#% 769887
#% 791736
#% 823361
#% 838441
#% 857483
#% 858459
#% 875957
#% 915477
#% 1015205
#% 1024955
#% 1081575
#% 1190298
#% 1209111
#% 1213000
#% 1251359
#% 1364995
#% 1451202
#% 1654055
#% 1678477
#! Intelligence analysts grapple with many challenges, chief among them is the need for software support in storytelling, i.e., automatically 'connecting the dots' between disparate entities (e.g., people, organizations) in an effort to form hypotheses and suggest non-obvious relationships. We present a system to automatically construct stories in entity networks that can help form directed chains of relationships, with support for co-referencing, evidence marshaling, and imposing syntactic constraints on the story generation process. A novel optimization technique based on concept lattice mining enables us to rapidly construct stories on massive datasets. Using several public domain datasets, we illustrate how our approach overcomes many limitations of current systems and enables the analyst to efficiently narrow down to hypotheses of interest and reason about alternative explanations.

#index 1872395
#* A framework for robust discovery of entity synonyms
#@ Kaushik Chakrabarti;Surajit Chaudhuri;Tao Cheng;Dong Xin
#t 2012
#c 0
#% 54453
#% 279755
#% 333679
#% 458630
#% 810014
#% 869501
#% 983467
#% 987222
#% 1127426
#% 1130854
#% 1190070
#% 1328142
#% 1338626
#% 1484373
#! Entity synonyms are critical for many applications like information retrieval and named entity recognition in documents. The current trend is to automatically discover entity synonyms using statistical techniques on web data. Prior techniques suffer from several limitations like click log sparsity and inability to distinguish between entities of different concept classes. In this paper, we propose a general framework for robustly discovering entity synonym with two novel similarity functions that overcome the limitations of prior techniques. We develop efficient and scalable techniques leveraging the MapReduce framework to discover synonyms at large scale. To handle long entity names with extraneous tokens, we propose techniques to effectively map long entity names to short queries in query log. Our experiments on real data from different entity domains demonstrate the superior quality of our synonyms as well as the efficiency of our algorithms. The entity synonyms produced by our system is in production in Bing Shopping and Video search, with experiments showing the significance it brings in improving search experience.

#index 1872396
#* SmartDispatch: enabling efficient ticket dispatch in an IT service environment
#@ Shivali Agarwal;Renuka Sindhgatta;Bikram Sengupta
#t 2012
#c 0
#% 248218
#% 660710
#% 816186
#% 1083691
#% 1083725
#% 1127581
#% 1176922
#% 1586513
#% 1601958
#% 1642138
#! In an IT service delivery environment, the speedy dispatch of a ticket to the correct resolution group is the crucial first step in the problem resolution process. The size and complexity of such environments make the dispatch decision challenging, and incorrect routing by a human dispatcher can lead to significant delays that degrade customer satisfaction, and also have adverse financial implications for both the customer and the IT vendor. In this paper, we present SmartDispatch, a learning-based tool that seeks to automate the process of ticket dispatch while maintaining high accuracy levels. SmartDispatch comes with two classification approaches - the well-known SVM method, and a discriminative term-based approach that we designed to address some of the issues in SVM classification that were empirically observed. Using a combination of these approaches, SmartDispatch is able to automate the dispatch of a ticket to the correct resolution group for a large share of the tickets, while for the rest, it is able to suggest a short list of 3-5 groups that contain the correct resolution group with a high probability. Empirical evaluation of SmartDispatch on data from 3 large service engagement projects in IBM demonstrate the efficacy and practical utility of the approach.

#index 1872397
#* Social media data analysis for revealing collective behaviors
#@ Aoying Zhou;Weining Qian;Haixin Ma
#t 2012
#c 0
#! Along with the development of Web 2.0 applications, social media services has attracted many users and become their hands-on toolkits for recording life, sharing ideas, and social networking. Though social media services are essentially web or mobile applications and services, they combine user-generated content and social networks together, so that information can be created, transmitted, transformed, and consumed in the cyberspace. Thus, social media somehow could be regarded as a kind of sensor to the real life of its users. In general, the data from social media is of low quality. Pieces of information in social media are usually short, with informal presentation, and in some specific context that is highly related to the physical world. Therefore, it is challenging to extract semantics from social media data. However, we argue that given sufficient social media data, users' collective behaviors could be sensed, studied, and even predicted in a certain circumstance. Our study is conducted on data from two services, i.e. Twitter, and Sina Weibo, the most popular microblogging services all over the world and in China, respectively. Collective behaviors are actions of a large amount of various people, which are neither conforming nor deviant. Various collective behaviors are studied in the context of social media. Our studies show that there are various information flow patterns in social media, some of which are similar to traditional media such as newspapers, while others are embedded deep in the social network structure. The evolution of hotspots is highly affected by external stimulation, the social network structure, and individual user's activities. Furthermore, social media tends to be immune to some repeated similar external stimulations. Last but not the least, there is considerable difference in users' behavior between Twitter and Sina Weibo.

#index 1872398
#* Information processing in social networks
#@ Ming-Syan Chen
#t 2012
#c 0
#! In the current social network, a user may have hundreds of friends and find it very time consuming to categorize and tag every friend manually. When a user is going to initiate an activity by issuing a corresponding query, he/she needs to consider the relationship among candidate attendees to find a group of mutually close friends. Meanwhile, he/she also needs to consider the schedule of candidate attendees to find an activity period available for all attendees. It would certainly be desirable if the efficiency of such process is improved. In this talk, information processing in social networks will first be reviewed in three phrases, namely (i) from content to social relationship, (ii) mining on social relationship, and (iii) from social relationship to content organization. In addition, we shall present an effective procedure which helps a user to organize an event with proper attendees with minimum total social distance and commonly available time. Moreover, it is noted that the information retrieved from the social networks is also able to facilitate those user-dependent and human-centric services. In light of this, we shall explore the quality of recommendation through incorporating the notion of social filtering and collaborative filtering. Finally, it is recognized that the cloud computing has offered many new capabilities of storing and processing huge amounts of heterogeneous data in social networks. In view of this, we shall also examine how this paradigm shift will affect the information processing in social networks.

#index 1872399
#* Understanding users' satisfaction for search engine evaluation
#@ Gordon Sun
#t 2012
#c 0
#! To fulfill users' search needs, the search engine must have good performance, easy-to-use functionalities, and good search result quality. Search quality evaluation becomes challenging when users' satisfaction may not be able to judge by a single search and even within a single search judgments from various sources are not consistent. In this talk, I will discuss how user's satisfaction is decomposed into different components in general, and how we measure them with various means - human judgment, automatic computation with query log, and outsourcing, and their pros and cons with operational implications. For an outlook, I will postulate potential evaluation approaches for a better user's satisfaction.

#index 1872400
#* Similarity search in real world networks
#@ Cuiping Li
#t 2012
#c 0
#! Recently there has been a lot of interest in graph-based analysis. One of the most important aspects of graph-based analysis is to measure similarity between nodes and to do similarity search in a graph. For example, in social networks such as Facebook, system may want to recommend potential friends to a particular user based on connections between users. In custom-product networks such as eBay, one may wish to recommend products to others based on purchases history. In this talk, I will introduce some methods on vertex similarities computations and their applications on similarity search in real world networks.

#index 1872401
#* Large-scale learning of word relatedness with constraints
#@ Guy Halawi;Gideon Dror;Evgeniy Gabrilovich;Yehuda Koren
#t 2012
#c 0
#% 228088
#% 278099
#% 342963
#% 576218
#% 722904
#% 786511
#% 840583
#% 896031
#% 1215322
#% 1264744
#% 1270225
#% 1272267
#% 1346151
#% 1369940
#% 1471208
#% 1560388
#% 1693885
#! Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.

#index 1872402
#* Latent association analysis of document pairs
#@ Gengxin Miao;Ziyu Guan;Louise E. Moser;Xifeng Yan;Shu Tao;Nikos Anerousis;Jimeng Sun
#t 2012
#c 0
#% 280819
#% 340899
#% 722904
#% 769906
#% 983644
#% 987287
#% 1055681
#% 1055743
#% 1074110
#% 1083684
#% 1211794
#% 1338620
#% 1417061
#% 1432248
#% 1482402
#% 1598401
#% 1697450
#! This paper presents Latent Association Analysis (LAA), a generative model that analyzes the topics within two document sets simultaneously, as well as the correlations between the two topic structures, by considering the semantic associations among document pairs. LAA defines a correlation factor that represents the connection between two documents, and considers the topic proportion of paired documents based on this factor. Words in the documents are assumed to be randomly generated by particular topic assignments and topic-to-word probability distributions. The paper also presents a new ranking algorithm, based on LAA, that can be used to retrieve target documents that are potentially associated with a given source document. The ranking algorithm uses the latent factor in LAA to rank target documents by the strength of their semantic associations with the source document. We evaluate the LAA algorithm with real datasets, specifically, the IT-Change and the IT-Solution document sets from the IBM IT service environment and the Symptom-Treatment document sets from Google Health. Experimental results demonstrate that the LAA algorithm significantly outperforms existing algorithms.

#index 1872403
#* LIEGE:: link entities in web lists with knowledge base
#@ Wei Shen;Jianyong Wang;Ping Luo;Min Wang
#t 2012
#c 0
#% 404719
#% 465914
#% 654469
#% 754068
#% 956564
#% 1019061
#% 1055735
#% 1092530
#% 1127393
#% 1127557
#% 1214667
#% 1328133
#% 1328199
#% 1409954
#% 1484272
#% 1523913
#% 1592023
#% 1592311
#% 1746843
#! A critical step in bridging the knowledge base with the huge corpus of semi-structured Web list data is to link the entity mentions that appear in the Web lists with the corresponding real world entities in the knowledge base, which we call list linking task. This task can facilitate many different tasks such as knowledge base population, entity search and table annotation. However, the list linking task is challenging because a Web list has almost no textual context, and the only input for this task is a list of entity mentions extracted from the Web pages. In this paper, we propose LIEGE, the first general framework to Link the entities in web lists with the knowledge base to the best of our knowledge. Our assumption is that entities mentioned in a Web list can be any collection of entities that have the same conceptual type that people have in mind. To annotate the list items in a Web list with entities that they likely mention, we leverage the prior probability of an entity being mentioned and the global coherence between the types of entities in the Web list. The interdependence between different entity assignments in a Web list makes the optimization of this list linking problem NP-hard. Accordingly, we propose a practical solution based on the iterative substitution to jointly optimize the identification of the mapping entities for the Web list items. We extensively evaluated the performance of our proposed framework over both manually annotated real Web lists extracted from the Web pages and two public data sets, and the experimental results show that our framework significantly outperforms the baseline method in terms of accuracy.

#index 1872404
#* Automatic taxonomy construction from keywords
#@ Xueqing Liu;Yangqiu Song;Shixia Liu;Haixun Wang
#t 2012
#c 0
#% 296738
#% 321455
#% 479973
#% 629672
#% 756964
#% 840903
#% 1269723
#% 1400017
#% 1471209
#% 1482279
#% 1537112
#% 1590535
#% 1770359
#% 1826362
#% 1826433
#! Taxonomies, especially the ones in specific domains, are becoming indispensable to a growing number of applications. State-of-the-art approaches assume there exists a text corpus to accurately characterize the domain of interest, and that a taxonomy can be derived from the text corpus using information extraction techniques. In reality, neither assumption is valid, especially for highly focused or fast-changing domains. In this paper, we study a challenging problem: Deriving a taxonomy from a set of keyword phrases. A solution can benefit many real life applications because i) keywords give users the flexibility and ease to characterize a specific domain; and ii) in many applications, such as online advertisements, the domain of interest is already represented by a set of keywords. However, it is impossible to create a taxonomy out of a keyword set itself. We argue that additional knowledge and contexts are needed. To this end, we first use a general purpose knowledgebase and keyword search to supply the required knowledge and context. Then we develop a Bayesian approach to build a hierarchical taxonomy for a given set of keywords. We reduce the complexity of previous hierarchical clustering approaches from O(n2 log n) to O(n log n), so that we can derive a domain specific taxonomy from one million keyword phrases in less than an hour. Finally, we conduct comprehensive large scale experiments to show the effectiveness and efficiency of our approach. A real life example of building an insurance-related query taxonomy illustrates the usefulness of our approach for specific domains.

#index 1872405
#* An enhanced relevance criterion for more concise supervised pattern discovery
#@ Henrik Großkreutz;Daniel Paurat;Stefan Rüping
#t 2012
#c 0
#% 232126
#% 279120
#% 299985
#% 420126
#% 477497
#% 629645
#% 763701
#% 1074357
#% 1108863
#% 1116995
#% 1214686
#% 1232020
#% 1268044
#% 1291602
#% 1605977
#% 1617290
#% 1617370
#% 1663669
#% 1710149
#% 1710157
#! Supervised local pattern discovery aims to find subsets of a database with a high statistical unusualness in the distribution of a target attribute. Local pattern discovery is often used to generate a human-understandable representation of the most interesting dependencies in a data set. Hence, the more crisp and concise the output is, the better. Unfortunately, standard algorithm often produce very large and redundant outputs. In this paper, we introduce delta-relevance, a definition of a more strict criterion of relevance. It will allow us to significantly reduce the output space, while being able to guarantee that every local pattern has a delta-relevant representative which is almost as good in a clearly defined sense. We show empirically that delta-relevance leads to a considerable reduction of the amount of returned patterns. We also demonstrate that in a top-k setting, the removal of not delta-relevant patterns improves the quality of the result set.

#index 1872406
#* Efficient frequent item counting in multi-core hardware
#@ Pratanu Roy;Jens Teubner;Gustavo Alonso
#t 2012
#c 0
#% 411680
#% 443513
#% 444341
#% 481290
#% 566122
#% 864446
#% 894443
#% 1038146
#% 1127608
#% 1142229
#% 1179657
#% 1278375
#% 1285685
#% 1328057
#% 1328127
#% 1328141
#% 1581849
#% 1581898
#% 1586197
#% 1628656
#! The increasing number of cores and the rich instruction sets of modern hardware are opening up new opportunities for optimizing many traditional data mining tasks. In this paper we demonstrate how to speed up the performance of the computation of frequent items by almost one order of magnitude over the best published results by matching the algorithm to the underlying hardware architecture. We start with the observation that frequent item counting, like other data mining tasks, assumes certain amount of skew in the data. We exploit this skew to design a new algorithm that uses a pre-filtering stage that can be implemented in a highly efficient manner through SIMD instructions. Using pipelining, we then combine this pre-filtering stage with a conventional frequent item algorithm (Space-Saving) that will process the remainder of the data. The resulting operator can be parallelized with a small number of cores, leading to a parallel implementation that does not suffer any of the overheads of existing parallel solutions when querying the results and offers significantly higher throughput.

#index 1872407
#* On nested palindromes in clickstream data
#@ Michel Speiser;Gianluca Antonini;Abderrahim Labbi;Juliana Sutanto
#t 2012
#c 0
#% 235941
#% 310515
#% 451536
#% 463903
#% 478770
#% 481290
#% 729938
#% 778732
#% 798044
#% 902449
#% 936239
#% 985041
#% 1102110
#% 1300556
#% 1605977
#% 1688560
#% 1738861
#! In this paper we discuss an interesting and useful property of clickstream data. Often a visit includes repeated views of the same page. We show that in three real datasets, sampled from the websites of technology and consulting groups and a news broadcaster, page repetitions occur for the majority as a very specific structure, namely in the form of nested palindromes. This can be explained by the widespread use of features which are available in any web browser: the "refresh" and "back" buttons. Among the types of patterns which can be mined from sequence data, many either stumble if symbol repetitions are involved, or else fail to capture interesting aspects related to symbol repetitions. In an attempt to remedy this, we characterize the palindromic structures, and discuss possible ways of making use of them. One way is to pre-process the sequence data by explicitly inserting these structures, in order to obtain a richer output from conventional mining algorithms. Another application we discuss is to use the information directly, in order to analyze certain aspects of the website under study. We also provide the simple linear-time algorithm which we developed to identify and extract the structures from our data.

#index 1872408
#* Mining discriminative components with low-rank and sparsity constraints for face recognition
#@ Qiang Zhang;Baoxin Li
#t 2012
#c 0
#% 323243
#% 336073
#% 415197
#% 656665
#% 800190
#% 992335
#% 1164188
#% 1298976
#% 1563695
#! This paper introduces a novel image decomposition approach for an ensemble of correlated images, using low-rank and sparsity constraints. Each image is decomposed as a combination of three components: one common component, one condition component, which is assumed to be a low-rank matrix, and a sparse residual. For a set of face images of Nsubjects, the decomposition finds N common components, one for each subject, K low-rank components, each capturing a different global condition of the set (e.g., different illumination conditions), and a sparse residual for each input image. Through this decomposition, the proposed approach recovers a clean face image (the common component) for each subject and discovers the conditions (the condition components and the sparse residuals) of the images in the set. The set of N+K images containing only the common and the low-rank components form a compact and discriminative representation for the original images. We design a classifier using only these N+K images. Experiments on commonly-used face data sets demonstrate the effectiveness of the approach for face recognition through comparing with the leading state-of-the-art in the literature. The experiments further show good accuracy in classifying the condition of an input image, suggesting that the components from the proposed decomposition indeed capture physically meaningful features of the input.

#index 1872409
#* Fast algorithms for comprehensive n-point correlation estimates
#@ William B. March;Andrew J. Connolly;Alexander G. Gray
#t 2012
#c 0
#% 2115
#% 317313
#! The n-point correlation functions (npcf) are powerful spatial statistics capable of fully characterizing any set of multidimensional points. These functions are critical in key data analyses in astronomy and materials science, among other fields, for example to test whether two point sets come from the same distribution and to validate physical models and theories. For example, the npcf has been used to study the phenomenon of dark energy, considered one of the major breakthroughs in recent scientific discoveries. Unfortunately, directly estimating the continuous npcf at a single value requires O(Nn) time for $N$ points, and n may be 2, 3, 4 or even higher, depending on the sensitivity required. In order to draw useful conclusions about real scientific problems, we must repeat this expensive computation both for many different scales in order to derive a smooth estimate and over many different subsamples of our data in order to bound the variance. We present the first comprehensive approach to the entire n-point correlation function estimation problem, including fast algorithms for the computation at multiple scales and for many subsamples. We extend the current state-of-the-art tree-based approach with these two algorithms. We show an order-of-magnitude speedup over the current best approach with each of our new algorithms and show that they can be used together to obtain over 500x speedups over the state-of-the-art in order to enable much larger datasets and more accurate scientific analyses than were possible previously.

#index 1872410
#* On "one of the few" objects
#@ You Wu;Pankaj K. Agarwal;Chengkai Li;Jun Yang;Cong Yu
#t 2012
#c 0
#% 273916
#% 289148
#% 330769
#% 480671
#% 806212
#% 824670
#% 864452
#% 866331
#% 912241
#% 993954
#% 1022242
#% 1206656
#% 1269491
#% 1393861
#% 1590905
#% 1606071
#! Objects with multiple numeric attributes can be compared within any "subspace" (subset of attributes). In applications such as computational journalism, users are interested in claims of the form: Karl Malone is one of the only two players in NBA history with at least 25,000 points, 12,000 rebounds, and 5,000 assists in one's career. One challenge in identifying such "one-of-the-k" claims (k = 2 above) is ensuring their "interestingness". A small k is not a good indicator for interestingness, as one can often make such claims for many objects by increasing the dimensionality of the subspace considered. We propose a uniqueness-based interestingness measure for one-of-the-few claims that is intuitive for non-technical users, and we design algorithms for finding all interesting claims (across all subspaces) from a dataset. Sometimes, users are interested primarily in the objects appearing in these claims. Building on our notion of interesting claims, we propose a scheme for ranking objects and an algorithm for computing the top-ranked objects. Using real-world datasets, we evaluate the efficiency of our algorithms as well as the advantage of our object-ranking scheme over popular methods such as Kemeny optimal rank aggregation and weighted-sum ranking.

#index 1872411
#* BC-PDM: data mining, social network analysis and text mining system based on cloud computing
#@ Le Yu;Jian Zheng;Wei Chong Shen;Bin Wu;Bai Wang;Long Qian;Bo Ren Zhang
#t 2012
#c 0
#% 69503
#% 1318636
#% 1335084
#% 1605950
#% 1701031
#! Telecom BI(Business Intelligence) system consists of a set of application programs and technologies for gathering, storing, analyzing and providing access to data, which contribute to manage business information and make decision precisely. However, traditional analysis algorithms meet new challenges as the continued exponential growth in both the volume and the complexity of telecom data. With the Cloud Computing development, some parallel data analysis systems have been emerging. However, existing systems have rarely comprehensive function, either providing data analysis service or providing social network analysis. We need a comprehensive tool to store and analysis large scale data efficiently. In response to the challenge, the SaaS (Software-as-a-Service) BI system, BC-PDM (Big Cloud-Parallel Data Mining), are proposed. BC-PDM supports parallel ETL process, statistical analysis, data mining, text mining and social network analysis which are based on Hadoop. This demo introduces three tasks: business recommendation, customer community detection and user preference classification by employing a real telecom data set. Experimental results show BC-PDM is very efficient and effective for intelligence data analysis.

#index 1872412
#* Query-driven discovery of semantically similar substructures in heterogeneous networks
#@ Xiao Yu;Yizhou Sun;Peixiang Zhao;Jiawei Han
#t 2012
#c 0
#% 288990
#% 1022280
#% 1214701
#% 1451228
#% 1523825
#% 1581921
#% 1606073
#% 1635098
#% 1635140
#! Heterogeneous information networks that contain multiple types of objects and links are ubiquitous in the real world, such as bibliographic networks, cyber-physical networks, and social media networks. Although researchers have studied various data mining tasks in information networks, interactive query-based network exploration techniques have not been addressed systematically, which, in fact, are highly desirable for exploring large-scale information networks. In this demo, we introduce and demonstrate our recent research project on query-driven discovery of semantically similar substructures in heterogeneous networks. Given a subgraph query, our system searches a given large information network and finds efficiently a list of subgraphs that are structurally identical and semantically similar. Since data mining methods are used to obtain semantically similar entities (nodes), we use discovery as a term to describe this process. In order to achieve high efficiency and scalability, we design and implement a filter-and verification search framework, which can first generate promising subgraph candidates using off line indices built by data mining results, and then verify candidates with a recursive pruning matching process. The proposed system demonstrates the effectiveness of our query-driven semantic similarity search framework and the efficiency of the proposed methodology on multiple real-world heterogeneous information networks.

#index 1872413
#* DAGger: clustering correlated uncertain data (to predict asset failure in energy networks)
#@ Dan Olteanu;Sebastiaan J. van Schaik
#t 2012
#c 0
#% 731478
#% 850430
#% 891559
#% 1016178
#% 1111126
#% 1179162
#% 1451166
#% 1535186
#% 1594635
#% 1615075
#% 1880436
#! DAGger is a clustering algorithm for uncertain data. In contrast to prior work, DAGger can work on arbitrarily correlated data and can compute both exact and approximate clusterings with error guarantees. We demonstrate DAGger using a real-world scenario in which partial discharge data from UK Power Networks is clustered to predict asset failure in the energy network.

#index 1872414
#* UFIMT: an uncertain frequent itemset mining toolbox
#@ Yongxin Tong;Lei Chen;Philip S. Yu
#t 2012
#c 0
#% 1214624
#% 1214633
#% 1393138
#% 1411089
#% 1451166
#% 1482221
#% 1535367
#% 1846710
#% 1880477
#! In recent years, mining frequent itemsets over uncertain data has attracted much attention in the data mining community. Unlike the corresponding problem in deterministic data, the frequent itemset under uncertain data has two different definitions: the expected support-based frequent itemset and the probabilistic frequent itemset. Most existing works only focus on one of the definitions and no comprehensive study is conducted to compare the two different definitions. Moreover, due to lacking the uniform implementation platform, existing solutions for the same definition even generate inconsistent results. In this demo, we present a demonstration called as UFIMT (underline Uncertain Frequent Itemset Mining Toolbox) which not only discovers frequent itemsets over uncertain data but also compares the performance of different algorithms and demonstrates the relationship between different definitions. In this demo, we firstly present important techniques and implementation skills of the mining problem, secondly, we show the system architecture of UFIMT, thirdly, we report an empirical analysis on extensive both real and synthetic benchmark data sets, which are used to compare different algorithms and to show the close relationship between two different frequent itemset definitions, and finally we discuss some existing challenges and new findings.

#index 1872415
#* Visual exploration of collaboration networks based on graph degeneracy
#@ Christos Giatsidis;Klaus Berberich;Dimitrios M. Thilikos;Michalis Vazirgiannis
#t 2012
#c 0
#% 101409
#% 211041
#% 1635210
#! We demonstrate a system that supports the visual exploration of collaboration networks. The system leverages the notion of fractional cores introduced in earlier work to rank vertices in a collaboration network and filter vertices' neighborhoods. Fractional cores build on the idea of graph degeneracy as captured by the notion of k-cores in graph theory and extend it to undirected edge-weighted graphs. In a co-authorship network, for instance, the fractional core index of an author intuitively reflects the degree of collaboration with equally or higher-ranked authors. Our system has been deployed on a real-world co-authorship network derived from DBLP, demonstrating that the idea of fractional cores can be applied even to large-scale networks. The system provides an easy-to-use interface to query for the fractional core index of an author, to see who the closest equally or higher-ranked co-authors are, and explore the entire co-authorship network in an incremental manner.

#index 1872416
#* TourViz: interactive visualization of connection pathways in large graphs
#@ Duen Horng Chau;Leman Akoglu;Jilles Vreeken;Hanghang Tong;Christos Faloutsos
#t 2012
#c 0
#% 769887
#% 853538
#% 881480
#% 881496
#% 893200
#% 1159232
#% 1573362
#! We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a "close-by" group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes. We demonstrate TourViz's usage and benefits using the DBLP graph, consisting of authors and their co-authorship relations, while our system is designed generally to work with any kind of graph data. We will invite the audience to experiment with our system and comment on its usability, usefulness, and how our system can help with their research and improve the understanding of data in other domains.

#index 1872417
#* D-INDEX: a web environment for analyzing dependences among scientific collaborators
#@ Claudio Schifanella;Luigi Di Caro;Mario Cataldi;Marie-Aude Aufaure
#t 2012
#c 0
#! In this work, we demonstrate a web application, available at http://d-index.di.unito.it, that permits to analyze the scientific profiles of all the researchers indexed by DBLP by focusing on the collaborations that contributed to define their curricula. The presented application allows the user to analyze the profile of a researcher, her dependence degrees on all the co-authors (along her entire scientific publication history) and to make comparisons among them in terms of dependence patterns. In particular, it is possible to estimate and visualize how much a researcher has benefited from collaboration with another researcher as well as the communities in which she has been involved. Moreover, the application permits to compare, in a single chart, each researcher with all the scientists indexed in DBLP by focusing on their dependences with respect to many other parameters like the total number of papers, the number of collaborations and the length of the scientific careers.

#index 1872418
#* Information propagation game: a tool to acquire humanplaying data for multiplayer influence maximization on social networks
#@ Hung-Hsuan Chen;Yan-Bin Ciou;Shou-De Lin
#t 2012
#c 0
#% 729923
#% 989613
#% 990216
#% 1214641
#% 1407359
#% 1560421
#! With the popularity of online social network services, influence maximization on social networks has drawn much attention in recent years. Most of these studies approximate a greedy based sub-optimal solution by proving the submodular nature of the utility function. Instead of using the analytical techniques, we are interested in solving the diffusion competition and influence maximization problem by a data-driven approach. We propose Information Propagation Game (IPG), a framework that can collect a large number of seed picking strategies for analysis. Through the IPG framework, human players are not only having fun but also helping contributing the seed picking strategies. Preliminary experiment suggests that centrality based heuristics are too simple for seed selection in a multiple player environment.

#index 1872419
#* MoodLens: an emoticon-based sentiment analysis system for chinese tweets
#@ Jichang Zhao;Li Dong;Junjie Wu;Ke Xu
#t 2012
#c 0
#% 1117691
#% 1562301
#! Recent years have witnessed the explosive growth of online social media. Weibo, a Twitter-like online social network in China, has attracted more than 300 million users in less than three years, with more than 1000 tweets generated in every second. These tweets not only convey the factual information, but also reflect the emotional states of the authors, which are very important for understanding user behaviors. However, a tweet in Weibo is extremely short and the words it contains evolve extraordinarily fast. Moreover, the Chinese corpus of sentiments is still very small, which prevents the conventional keyword-based methods from being used. In light of this, we build a system called MoodLens, which to our best knowledge is the first system for sentiment analysis of Chinese tweets in Weibo. In MoodLens, 95 emoticons are mapped into four categories of sentiments, i.e. angry, disgusting, joyful, and sad, which serve as the class labels of tweets. We then collect over 3.5 million labeled tweets as the corpus and train a fast Naive Bayes classifier, with an empirical precision of 64.3%. MoodLens also implements an incremental learning method to tackle the problem of the sentiment shift and the generation of new words. Using MoodLens for real-time tweets obtained from Weibo, several interesting temporal and spatial patterns are observed. Also, sentiment variations are well captured by MoodLens to effectively detect abnormal events in China. Finally, by using the highly efficient Naive Bayes classifier, MoodLens is capable of online real-time sentiment monitoring. The demo of MoodLens can be found at http://goo.gl/8DQ65.

#index 1872420
#* Intelligent advertising framework for digital signage
#@ Phil Tian;Addicam V. Sanjay;Kunapareddy Chiranjeevi;Shahzad Malik Malik
#t 2012
#c 0
#% 1217270
#% 1223245
#% 1434832
#! How to realize targeted advertising in digital signage is an interesting question. This paper proposed an Intelligent Advertising Framework (IAF), which pioneers the integration of Anonymous Viewer Analytics (AVA) and Data Mining technologies to achieve Targeted and interactive Advertising. IAF correlates AVA viewership information with point-of-sale (POS) data, and establishes a link between the response time to an ad by a certain demographic group and the effect on the sale of the advertised product. With the advertising models learned based on this correlation, IAF can provide advertisers and retailers with intelligence to show the right ads to right audience in right location at right time. Preliminary results indicate that IAF will greatly improve the effect and utility of advertising and maximize the Return on Investment (ROI) of advertisers and retailers. The demo shows Intel's leadership regarding intelligent advertising in the Digital Signage industry.

#index 1872421
#* AssocExplorer: an association rule visualization system for exploratory data analysis
#@ Guimei Liu;Andre Suchitra;Haojun Zhang;Mengling Feng;See-Kiong Ng;Limsoon Wong
#t 2012
#c 0
#% 152934
#% 619859
#% 913787
#% 941039
#! We present a system called AssocExplorer to support exploratory data analysis via association rule visualization and exploration. AssocExplorer is designed by following the visual information-seeking mantra: overview first, zoom and filter, then details on demand. It effectively uses coloring to deliver information so that users can easily detect things that are interesting to them. If users find a rule interesting, they can explore related rules for further analysis, which allows users to find interesting phenomenon that are difficult to detect when rules are examined separately. Our system also allows users to compare rules and inspect rules with similar item composition but different statistics so that the key factors that contribute to the difference can be isolated.

#index 1872422
#* GeoSearch: georeferenced video retrieval system
#@ Youngwoo Kim;Jinha Kim;Hwanjo Yu
#t 2012
#c 0
#% 153722
#% 427199
#% 480473
#% 527323
#% 730144
#% 885377
#% 1131863
#% 1406035
#% 1446669
#% 1480802
#% 1854917
#! Conventional video search systems, to find relevant videos, rely on textual data such as video titles, annotations, and text around the video. Nowadays, video recording devices such as ameras, smartphones and car blackboxes are equipped with GPS sensors and able to capture videos with spatiotemporal information such as time, location and camera direction. We call such videos georeferenced videos. This paper presents a georeferenced video retrieval system, geosearch, which efficiently retrieves videos containing a certain point or range in the map. To enable a fast search of georeferenced videos, geosearch adopts a novel data structure MBTR (Minimum Bounding Tilted Rectangle) in the leaf nodes of R-Tree. New algorithms are developed to build MBTRs from georeferenced videos and to efficiently process point and range queries on MBTRs. We demonstrate our system on real georeferenced videos, and show that, compared to previous methods, geosearch substantially reduces the index size and also improves the search speed for georeferenced video data. Our online demo is available at "http://dm.hwanjoyu.org/geosearch".

#index 1872423
#* Siren: an interactive tool for mining and visualizing geospatial redescriptions
#@ Esther Galbrun;Pauli Miettinen
#t 2012
#c 0
#% 769902
#% 1056760
#% 1232020
#% 1269491
#% 1876102
#! We present SIREN, an interactive tool for mining and visualizing geospatial redescriptions. Redescription mining is a powerful data analysis tool that aims at finding alternative descriptions of the same entities. For example, in biology, an important task is to identify the bioclimatic constraints that allow some species to survive, that is, to describe geographical regions in terms of both the fauna that inhabits them and their bioclimatic conditions. Using SIREN, users can explore geospatial data of their interest by visualizing the redescriptions on a map, interactively edit, extend and filter them. To demonstrate the use of the tool, we focus on climatic niche-finding over Europe, as an example task. Yet, SIREN is by no means limited to a particular dataset or application.

#index 1872424
#* Navigating information facets on twitter (NIF-T)
#@ Shamanth Kumar;Fred Morstatter;Grant Marshall;Huan Liu;Ullas Nambiar
#t 2012
#c 0
#% 989652
#% 1130808
#% 1183211
#% 1426611
#% 1561558
#% 1826333
#! Recent years have seen an exponential increase in the number of users of social media sites. As the number of users of these sites continues to grow at an extraordinary rate, the amount of data produced follows in magnitude. With this deluge of social media data, the need for comprehensive tools to analyze user interactions is ever increasing. In this paper, we present a novel tool, Navigating Information Facets on Twitter (NIF-T), which helps users to explore data generated on social media sites. Using the three dimensions or facets: time, location, and topic as an example of the many possible facets, we enable the users to explore large social media datasets. With the help of a large corpus of tweets collected from the Occupy Wall Street movement on the Twitter platform we show how our system can be used to identify important aspects of the event along these facets.

#index 1872425
#* HeteRecom: a semantic-based recommendation system in heterogeneous networks
#@ Chuan Shi;Chong Zhou;Xiangnan Kong;Philip S. Yu;Gang Liu;Bai Wang
#t 2012
#c 0
#% 220709
#% 1332120
#% 1451228
#% 1650569
#% 1798390
#! Making accurate recommendations for users has become an important function of e-commerce system with the rapid growth of WWW. Conventional recommendation systems usually recommend similar objects, which are of the same type with the query object without exploring the semantics of different similarity measures. In this paper, we organize objects in the recommendation system as a heterogeneous network. Through employing a path-based relevance measure to evaluate the relatedness between any-typed objects and capture the subtle semantic containing in each path, we implement a prototype system (called HeteRecom) for semantic based recommendation. HeteRecom has the following unique properties: (1) It provides the semantic-based recommendation function according to the path specified by users. (2) It recommends the similar objects of the same type as well as related objects of different types. We demonstrate the effectiveness of our system with a real-world movie data set.

#index 1872426
#* VOXSUP: a social engagement framework
#@ Yusheng Xie;Daniel Honbo;Alok Choudhary;Kunpeng Zhang;Yu Cheng;Ankit Agrawal
#t 2012
#c 0
#% 875959
#% 1127608
#% 1642194
#% 1689729
#! Social media websites are currently central hubs on the Internet. Major online social media platforms are not only places for individual users to socialize but are increasingly more important as channels for companies to advertise, public figures to engage, etc. In order to optimize such advertising and engaging efforts, there is an emerging challenge for knowledge discovery on today's Internet. The goal of knowledge discovery is to understand the entire online social landscape instead of merely summarizing the statistics. To answer this challenge, we have created VOXSUP as a unified social engagement framework. Unlike most existing tools, VOXSUP not only aggregates and filters social data from the Internet, but also provides what we call Voxsupian Knowledge Discovery (VKD). VKD consists of an almost human-level understanding of social conversations at any level of granularity from a single comment sentiment to multi-lingual inter-platform user demographics. Here we describe the technologies that are crucial to VKD, and subsequently go beyond experimental verification and present case studies from our live VOXSUP system.

#index 1872427
#* A system for extracting top-K lists from the web
#@ Zhixian Zhang;Kenny Qili Zhu;Haixun Wang
#t 2012
#c 0
#% 464434
#% 729978
#% 956500
#% 1024194
#% 1127393
#% 1190153
#% 1560405
#% 1561591
#% 1610177
#% 1770359
#% 1826433
#! List data is an important source of structured data on the web. This paper is concerned with "top-k" pages, which are web pages that describe a list of k instances of a particular topic or concept. Examples include "the 10 tallest persons in the world" and "the 50 hits of 2010 you don't want to miss". Compared to normal web list data, "top-k" lists contain richer information and are easier to understand. Therefore the extraction of such lists can help enrich existing knowledge bases about general concepts, or act as a preprocessing step to produce facts for a fact answering engine. We present an efficient system that extracts the target lists from web pages with high accuracy. We have used the system to process up to 160 million, or 1/10 of a high-frequency web snapshot from Bing, and obtained over 140,000 lists with 90.4% precision.

#index 1872428
#* EventSearch: a system for event discovery and retrieval on multi-type historical data
#@ Dongdong Shan;Wayne Xin Zhao;Rishan Chen;Baihan Shu;Ziqi Wang;Junjie Yao;Hongfei Yan;Xiaoming Li
#t 2012
#c 0
#% 262042
#% 577220
#% 839926
#% 989601
#% 1426611
#% 1913364
#! We present EventSearch, a system for event extraction and retrieval on four types of news-related historical data, i.e., Web news articles, newspapers, TV news program, and micro-blog short messages. The system incorporates over 11 million web pages extracted from "Web InfoMall", the Chinese Web Archive since 2001. The newspaper and TV news video clips also span from 2001 to 2011. The system, upon a user query, returns a list of event snippets from multiple data sources. A novel burst model is used to discover events from time-stamped texts. In addition to offline event extraction, our system also provides online event extraction to further meet the user needs. EventSearch provides meaningful analytics that synthesize an accurate description of events. Users interact with the system by ranking the identified events using different criteria (scale, recency and relevance) and submitting their own information needs in different input fields.

#index 1872429
#* EvaPlanner: an evacuation planner with social-based flocking kinetics
#@ Cheng-Te Li;Shou-De Lin
#t 2012
#c 0
#% 31686
#% 729923
#% 781507
#% 867050
#% 915344
#% 983230
#! This paper demonstrates a system that exploits graph mining, social network analysis, and agent-based crowd simulation techniques to investigate the evacuation dynamics during fire emergency. We create a novel evacuation planning system, EvaPlanner, to deal with three tasks. First, the system identifies the preferable locations to establish the exits to facilitate efficient evacuation from the dangerous areas. Second, it determines the most effective positions to place the emergency signs such that panic crowd can quickly find the exits. Third, it faithfully simulates the evacuation dynamics of crowd considering not only the individual movement kinetics but also the social connections between people. EvaPlanner provides a flexible experimental platform for investigating the evacuation dynamics under a variety of settings, and can further be utilized for animation and movie production. In addition, it can serve as a tool to assist architects address the safety concern during the planning phase. The demo system can be found in the link: http://mslab.csie.ntu.edu.tw/evaplanner/

#index 1872430
#* PubMed search and exploration with real-time semantic network construction
#@ Jinoh Oh;Taehoon Kim;Sun Park;Hwanjo Yu
#t 2012
#c 0
#% 280849
#% 340951
#% 754124
#% 1202162
#% 1388155
#% 1482180
#! Exploring PubMed to find relevant information is challenging and time-consuming because PubMed typically returns a long list of articles as a result of query. Semantic network helps users to explore a large document collection and to capture key concepts and relationships among the concepts. The semantic network also serves to broaden the user's knowledge and extend query keyword by detecting and visualizing new related concepts or relations hidden in the retrieved documents. The problem of existing semantic network techniques is that they typically produce many redundant relationships, which prevents users from quickly capturing the underlying relationships among concepts. This paper develops an online PubMed search system, which displays semantic networks having no redundant relationships in real-time as a result of query. To do so, we propose an efficient semantic network construction algorithm, which prevents producing redundant relationships during the network construction. Our extensive experiments on actual PubMed data show that the proposed method is significantly faster than the method removing redundant relationships afterward. Our method is implemented and integrated into a relevance feedback PubMed search engine, called RefMed, "http://dm.postech.ac.kr/refmed", and will be demonstrated through the website.

#index 1872508
#* Information propagation game: a tool to acquire humanplaying data for multiplayer influence maximization on social networks
#@ Hung-Hsuan Chen;Yan-Bin Ciou;Shou-De Lin
#t 2012
#c 0
#% 205305
#% 356600
#% 416577
#% 729923
#% 989613
#% 990216
#% 1214641
#% 1407359
#% 1560421
#% 1759761
#% 1760986
#% 1809587
#% 1809670
#% 1818337
#% 1849469
#! With the popularity of online social network services, influence maximization on social networks has drawn much attention in recent years. Most of these studies approximate a greedy based sub-optimal solution by proving the submodular nature of the utility function. Instead of using the analytical techniques, we are interested in solving the diffusion competition and influence maximization problem by a data-driven approach. We propose Information Propagation Game (IPG), a framework that can collect a large number of seed picking strategies for analysis. Through the IPG framework, human players are not only having fun but also helping contributing the seed picking strategies. Preliminary experiment suggests that centrality based heuristics are too simple for seed selection in a multiple player environment.

#index 1875825
#* Proceedings of the Twelfth International Workshop on Multimedia Data Mining
#@ Aaron Baughman;Jiang (John) Gao;Jia-Yu Pan;Fang Chu;Yizhou Wang
#t 2012
#c 0

#index 1877532
#* Proceedings of the ACM SIGKDD International Workshop on Urban Computing
#@ Ouri E. Wolfson;Yu Zheng
#t 2012
#c 0
#! With the rapid progress of urbanization and civilization on earth, urban computing is emerging as a concept where every sensor, device, person, vehicle, building, and street in the urban areas can be used as a component to probe city dynamics to further enable city-wide computing for serving people and their cities. Urban computing aims to enhance both human life and urban environment smartly through a recurrent process of sensing, mining, understanding, and improving. Urban computing also aims to deeply understand the nature and sciences behind the phenomenon occurring in urban spaces, using a variety of heterogeneous data sources, such as traffic flows, human mobility, geographic and map data, environment, energy consumption, populations, and economics, etc.

#index 1877623
#* Proceedings of the 1st International Workshop on Context Discovery and Data Mining
#@ Jilei Tian;Alvin Chin;Enhong Chen
#t 2012
#c 0
#! Welcome to the 1st International Workshop on Context Discovery and Data Mining (ContextDD 2012). The workshop was held in Beijing, China, on August 2012, in conjunction with the 18th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2012). ContextDD 2012 is a forum to discuss the research issues and challenges within rich context mobile computing from both an academic and industrial research perspective. Some of these research issues and challenges include: context data collection, rich context data definition, context sensing, context recognition, context aggregation, context segmentation, context management and inference, context fusion, context awareness methods, context frameworks, context datasets and algorithms for mining user behavior and other intelligence. Applications of this includes context recommendation, context-aware personalized smart applications and services, location based services, etc. The workshop aims to attract and bring together researchers working in data mining and retrieval, mobile computing and pervasive computing to attend, as well as students, professors, and industry researchers from leading universities and research labs internationally that are doing work in the context, sensing, mobile, social and pervasive computing areas.

#index 1877684
#* Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining
#@ Erik Cambria;Yongzheng Zhang;Yunqing Xia;Newton Howard
#t 2012
#c 0
#! The exponential growth of the Social Web is virally infecting more and more critical business processes such as customer support and satisfaction, brand and reputation management, product design and marketing. Because of this global trend, web users already evolved from the era of social relationships, in which they began to get connected and started to share contents, to the era of social functionality, in which they started using social networks as the main platform for communication and dissemination of information. Today, web users are going through the era of social colonization, in which every experience on the Web can be social (e.g., Facebook Like button), and are getting ready for the era of social context, in which web contents will be highly targeted and personalized. The final stage of such Social Web evolution is the so called era of social commerce, in which communities will define future products and services. In such context, the research field of sentiment analysis, which has already been rapidly growing in the last decade, is destined to become more and more important for Web and business dynamics. To this end, the First International Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM 2012: http://sentic.net/wisdom/) aims to explore how the wisdom of the crowds is affecting (and will affect) the evolution of the Web and of businesses gravitating around it. In particular, the workshop explores two different stages of sentiment analysis: the former focusing on the identification of opinionated text over the Web, the latter focusing on the classification of such text either in terms of polarity detection or emotion recognition.

#index 1880383
#* Proceedings of the 11th International Workshop on Data Mining in Bioinformatics
#@ Jake Chen;Mohammed J. Zaki;Tamer Kahveci;Saeed Salem;Mehmet Koyutürk
#t 2012
#c 0
#! The past two decades have witnessed rapid technological advances in biological data collection and acquisition. These advances in biotechnology enabled interrogation of cellular systems at various levels, leading to generation and collection of large-scale biological data (mostly in public databases) at an exponential rate. The explosion of biological data is leading to a paradigm shift in research methods in life sciences; from hypothesis-driven research to data driven research. In the last decade, sophisticated algorithms for knowledge discovery and data mining have demonstrated great promise in extracting novel biological information from complex, heterogeneous, and very high-dimensional biological datasets. The International Workshop on Data Mining in Bioinformatics (BIOKDD) has successfully established a tradition in providing a platform for the presentation and discussion of advances in data mining techniques that primarily target biological data in the last ten years. This year's BIOKDD continued the tradition of bringing together data mining researchers and life scientists, emphasizing novel problems with various types of biological data, emerging types of biological data (e.g., next generation sequencing, RNA-seq, electronic medical records), integration of heterogenous biological datasets, and emerging applications in the new era of life and medical sciences (e.g., systems biology, genome-wide association studies, translational science, personalized genomics).

#index 1880389
#* Proceedings of the Sixth International Workshop on Knowledge Discovery from Sensor Data
#@ Debasish Das;Auroop R. Ganguly;Varun Chandola;Olufemi A. Omitaomu;Karsten Steinhaeuser;Joao Gama;Ranga Raju Vatsavai;Mohamed Medhat Gaber;Nitesh V. Chawla
#t 2012
#c 0
#! Wide-area sensor infrastructures, remote sensors, RFIDs, phasor measurements, and wireless sensor networks yield massive volumes of disparate, dynamic, and geographically distributed data. With the recent proliferation of smart-phones and similar GPS enabled mobile devices with several onboard sensors, collection of sensor data is no longer limited to scientific communities, but has reached general public. As such sensors are becoming ubiquitous, a set of broad requirements is beginning to emerge across high-priority applications including adaptability to national or homeland security, critical infrastructures monitoring, smart grids, disaster preparedness and management, greenhouse emissions and climate change, and transportation. The raw data from sensors need to be efficiently managed and transformed to usable information through data fusion, which in turn must be converted to predictive insights via knowledge discovery, ultimately facilitating automated or human-induced tactical decisions or strategic policy based on decision sciences and decision support systems. The challenges for the knowledge discovery community are expected to be immense. On the one hand are dynamic data streams or events that require real-time analysis methodologies and systems, while on the other hand are static data that require high end computing for generating offline predictive insights, which in turn can facilitate real-time analysis. The online and real-time knowledge discovery imply immediate opportunities as well as intriguing short- and long-term challenges for practitioners and researchers in knowledge discovery. The opportunities would be to develop new data mining approaches and adapt traditional and emerging knowledge discovery methodologies to the requirements of the emerging problems. In addition, emerging societal problems require knowledge discovery solutions that are designed to investigate anomalies, rare events, hotspots, changes, extremes and nonlinear processes, and departures from the normal.

#index 1880397
#* Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics
#@ Ying Ding;Jiawei Han;Jie Tang;Philip S. Yu
#t 2012
#c 0

#index 1881268
#* Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications
#@ Wei Fan;Albert Bifet;Qiang Yang;Philip Yu
#t 2012
#c 0
#! Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.

#index 1881285
#* Proceedings of the 1st International Workshop on Cross Domain Knowledge Discovery in Web and Social Network Mining
#@ Bo Long;Yi Chang;Hang Li
#t 2012
#c 0
#! In the field of Web and social network mining, more and more learning tasks can easily acquire multiple data sets from various domains. For example, a modern search engine system often conducts ranking learning tasks in various domains with different languages (e.g., English text search, Spanish text search, etc.), or different verticals/topics (e.g., news search, product search, etc.); and recently recommendation systems start to leverage multiple types of user data from different domains, such as user browsing history data, user shopping record data, and user social network data. At the same time, the need for knowledge transfer is increasingly evident as many new datasets, or parts of data, are only very sparsely annotated. Different from traditional single-domain learning problems based on the assumption that training and test data are drawn from identical distribution, cross domain learning problems are built on multiple domain data that may have different degrees of relatedness to target tasks, offering an opportunity to help one another. To better leverage multiple domain data, mining and transferring of shared knowledge across multiple domains is likely to become a crucial step in Web and social network mining in the future.

#index 1881290
#* Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy
#@ 
#t 2012
#c 0

#index 1904111
#* Panel on Mining the Big Data
#@ Michael I. Jordan;Christos Faloutsos;Wen Gao;Jiawei Han;Zijian Zheng;Usuama Fayyad
#t 2012
#c 0

#index 1904112
#* Modeling Content and Users: Structured Probabilistic Representation and Scalable Inference Algorithms
#@ Amr Ahmed
#t 2012
#c 0

#index 1904113
#* Exact Primitives for Time Series Data Mining
#@ Sbdullah Al Mueen
#t 2012
#c 0

#index 1904114
#* Efficient Algorithms for Detecting Genetic Interactions in Genome-Wide Association Study
#@ Xiang Zhang
#t 2012
#c 0

#index 1908658
#* Proceedings of the First International Workshop on Software Mining
#@ Ming Li;Hongyu Zhang;David Lo
#t 2012
#c 0
#! Software systems have been playing important roles in business, scientific research, and our everyday lives. It is critical to improve both software productivity and quality, which are major challenges to software engineering researchers and practitioners. In recent years, software mining has emerged as a promising means to address these challenges. It has been successfully applied to discover knowledge from software artifacts (e.g., specifications, source code, documentations, execution logs, and bug reports) to improve software quality and development process (e.g., to obtain the insights for the causes leading to poor software quality, to help software engineers locate and identify problems quickly, and to help the managers optimize the resources for better productivity). Software mining has attracted much attention in both software engineering and data mining communities. The first International Workshop on Software Mining (SoftwareMining-2012) aims to bridge research in the data mining community and software engineering community by providing an open and interactive forum for researchers who are interested in software mining to discuss the methodologies and technical foundations of software mining, approaches and techniques for mining various types of software-related data, applications of data mining to facilitate specialized tasks in software engineering. The participants of diverse background in either data mining or software engineering can benefit from this workshop by sharing their expertise, exchanging ideas and discussing new research results.

#index 1915002
#* Proceedings of the First ACM International Workshop on Hot Topics on Interdisciplinary Social Networks Research
#@ Xiaoming Fu;Peter Gloor;Jie Tang
#t 2012
#c 0
#! With the blessing of information technology, we are living in an increasingly networked world. People, information and other entities are connected via World Wide Web, email networks, instant messaging networks, mobile communication networks, online social networks, etc. These online networks grow fast and possess huge amount of recorded information, which presents great opportunities in understanding the science of these networks, and in developing new applications from these networks and for these networks. The increasingly networked society has fundamentally changed our way of thinking, individual behaviors and social activities. It is foreseen that the public health relating to epidemic diseases is greatly impacted by this emerging connectivity as they are by nature mediated by direct or indirect human interactions and mobility. However, new challenges have to be met --- the networks are huge and information is noisy, and they demand new methodologies in accessing and analyzing these networks, and in developing theories and applications for the networks. To meet with these challenges, researchers from a wide range of academic fields, including theory and algorithms, data mining and machine learning, computer systems and networks, statistical physics and complex systems, sociology, social psychology, economics and managerial science, etc. are all actively studying various aspects concerning social and information networks. However, we lack the proper opportunities for people from these diverse backgrounds to directly interact with each other. The diversity of approaches and methodologies to study various social networks has raised the need for an interdisciplinary effort to create the required expertise to address the fundamental open questions in this field. This workshop is intended to present such an opportunity and serve as a forum to bring together people from various fields to exchange their latest research results and to sparkle new ideas and directions to properly understand these networks. It will be held on August 12, 2012, in conjunction with ACM KDD 2012, August 12-16, Beijing, China.

#index 1955025
#* Proceedings of the First International Workshop on Crowdsourcing and Data Mining
#@ 
#t 2012
#c 0

#index 1991775
#* Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining
#@ Inderjit S. Dhillon;Yehuda Koren;Rayid Ghani;Ted E. Senator;Paul Bradley;Rajesh Parekh;Jingrui He;Robert L. Grossman;Ramasamy Uthurusamy
#t 2013
#c 0
#! It is our great pleasure to welcome you to the 19th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). The annual ACM SIGKDD conference is the premier international forum for data mining, knowledge discovery and big data. It brings together researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. KDD-2013 features plenary presentations, paper presentations, poster sessions, workshops, tutorials, exhibits, demonstrations, and the KDD Cup competition. Today, you hear a lot about big data, data science and data intensive computing. The core of this work is extracting knowledge and useful information from data, which for science leads to beautiful insights, and for applications leads to actions, alerts and decisions. The KDD community has always been at the center of this activity and it is clear from this conference that it will continue to drive this broader field of big data. This year there were 726 submissions to the KDD Research Track, and 125 papers were accepted. There were 136 submissions to the KDD Industry and Government Track, and 34 papers were accepted. KDD also has a history of inviting talks that are of broad interest to the KDD community. This year we chose to have 4 plenary talks. A program committee also selected 8 talks to present at the Industry Practice Exposition. A strength of the KDD conference is the number of workshops and tutorials that are co-located with it. This year there were 10 full-day workshops, 5 half-day workshops, and 6 tutorials. We thank all sponsors, who are a very important part of the conference, and the members of the Organizing Committee and our other colleagues who volunteered their time during the past year to make this conference a success. Special thanks goes to the Research Track Co-Chairs and the Industry and Government Track Co-Chairs. Also special thanks are due to the Local Arrangements Chair, the Treasurer, the Proceedings Co-Chairs, and the KDD Cup Committee. We are grateful to the several program committees that provided the advice necessary to put together a quality program - the Research Track Program Committee, the Research Track Senior Program Committee, the Industry and Government Track Program Committee, the Industry Practice Expo Program Committee, the Workshop Program Committee, the Tutorial Program Committee, and the Demo Program Committee. We know that you will find this year's exhibits and demonstrations exciting and remind you that some of the most interesting discussions can be found there. Please join us for KDD-2013 to gain new knowledge and to exchange exciting new research results, leading practices, and high impact applications in big data, knowledge discovery and data mining. We hope that you will find this program interesting and thought-provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.

#index 1991776
#* Who, where, when and what: discover spatio-temporal topics for twitter users
#@ Quan Yuan;Gao Cong;Zongyang Ma;Aixin Sun;Nadia Magnenat- Thalmann
#t 2013
#c 0
#% 86950
#% 869516
#% 1016371
#% 1355044
#% 1399973
#% 1400018
#% 1481659
#% 1560379
#% 1592024
#% 1598366
#% 1606049
#% 1642282
#% 1643161
#% 1746875
#% 1872363
#% 1918350
#% 1930597
#! Micro-blogging services, such as Twitter, and location-based social network applications have generated short text messages associated with geographic information, posting time, and user ids. The availability of such data received from users offers a good opportunity to study the user's spatial-temporal behavior and preference. In this paper, we propose a probabilistic model W4 (short for Who+Where+When+What) to exploit such data to discover individual users' mobility behaviors from spatial, temporal and activity aspects. To the best of our knowledge, our work offers the first solution to jointly model individual user's mobility behavior from the three aspects. Our model has a variety of applications, such as user profiling and location prediction; it can be employed to answer questions such as ``Can we infer the location of a user given a tweet posted by the user and the posting time?" Experimental results on two real-world datasets show that the proposed model is effective in discovering users' spatial-temporal topics, and outperforms state-of-the-art baselines significantly for the task of location prediction for tweets.

#index 1991777
#* Multi-label classification by mining label and instance correlations from heterogeneous information networks
#@ Xiangnan Kong;Bokai Cao;Philip S. Yu
#t 2013
#c 0
#% 266215
#% 823370
#% 838412
#% 884074
#% 950571
#% 1100077
#% 1176915
#% 1176950
#% 1181261
#% 1214701
#% 1250573
#% 1264044
#% 1267771
#% 1451240
#% 1457044
#% 1535421
#% 1606073
#% 1872391
#% 1919776
#% 1974020
#! Multi-label classification is prevalent in many real-world applications, where each example can be associated with a set of multiple labels simultaneously. The key challenge of multi-label classification comes from the large space of all possible label sets, which is exponential to the number of candidate labels. Most previous work focuses on exploiting correlations among different labels to facilitate the learning process. It is usually assumed that the label correlations are given beforehand or can be derived directly from data samples by counting their label co-occurrences. However, in many real-world multi-label classification tasks, the label correlations are not given and can be hard to learn directly from data samples within a moderate-sized training set. Heterogeneous information networks can provide abundant knowledge about relationships among different types of entities including data samples and class labels. In this paper, we propose to use heterogeneous information networks to facilitate the multi-label classification process. By mining the linkage structure of heterogeneous information networks, multiple types of relationships among different class labels and data samples can be extracted. Then we can use these relationships to effectively infer the correlations among different class labels in general, as well as the dependencies among the label sets of data examples inter-connected in the network. Empirical studies on real-world tasks demonstrate that the performance of multi-label classification can be effectively boosted using heterogeneous information net- works.

#index 1991778
#* Fast structure learning in generalized stochastic processes with latent factors
#@ Mohammad Taha Bahadori;Yan Liu;Eric P. Xing
#t 2013
#c 0
#% 1556166
#% 1872232
#% 1872359
#! Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multivariate time series data. In this paper, we analyze a flexible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood. In particular, we examine three examples, including two popular models for count data, i.e, Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful model for extreme value data, i.e., Gumbel vector auto-regressive processes. We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. Experiments on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models.

#index 1991779
#* Accurate intelligible models with pairwise interactions
#@ Yin Lou;Rich Caruana;Johannes Gehrke;Giles Hooker
#t 2013
#c 0
#% 424997
#% 722929
#% 769939
#% 928422
#% 1073997
#% 1077150
#% 1100070
#% 1272177
#% 1872245
#! Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs can be interpreted by users, their accuracy is significantly less than more complex models that permit interactions. In this paper, we suggest adding selected terms of interacting pairs of features to standard GAMs. The resulting models, which we call GA2{M}$-models, for Generalized Additive Models plus Interactions, consist of univariate terms and a small number of pairwise interaction terms. Since these models only include one- and two-dimensional components, the components of GA2M-models can be visualized and interpreted by users. To explore the huge (quadratic) number of pairs of features, we develop a novel, computationally efficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model. In a large-scale empirical study, we show the effectiveness of FAST in ranking candidate pairs of features. In addition, we show the surprising result that GA2M-models have almost the same performance as the best full-complexity models on a number of real datasets. Thus this paper postulates that for many problems, GA2M-models can yield models that are both intelligible and accurate.

#index 1991780
#* Spotting opinion spammers using behavioral footprints
#@ Arjun Mukherjee;Abhinav Kumar;Bing Liu;Junhui Wang;Meichun Hsu;Malu Castellanos;Riddhiman Ghosh
#t 2013
#c 0
#% 251365
#% 269217
#% 420464
#% 458379
#% 577224
#% 734915
#% 838490
#% 869469
#% 891559
#% 912202
#% 956513
#% 1035590
#% 1100099
#% 1250370
#% 1268491
#% 1338553
#% 1399983
#% 1432762
#% 1482272
#% 1482375
#% 1560192
#% 1591960
#% 1688551
#% 1746804
#% 1746817
#% 1746818
#% 1764757
#% 1826458
#% 1872330
#% 1913389
#! Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or to demote some target products. In recent years, fake review detection has attracted significant attention from both the business and research communities. However, due to the difficulty of human labeling needed for supervised learning and evaluation, the problem remains to be highly challenging. This work proposes a novel angle to the problem by modeling spamicity as latent. An unsupervised model, called Author Spamicity Model (ASM), is proposed. It works in the Bayesian setting, which facilitates modeling spamicity of authors as latent and allows us to exploit various observed behavioral footprints of reviewers. The intuition is that opinion spammers have different behavioral distributions than non-spammers. This creates a distributional divergence between the latent population distributions of two clusters: spammers and non-spammers. Model inference results in learning the population distributions of the two clusters. Several extensions of ASM are also considered leveraging from different priors. Experiments on a real-life Amazon review dataset demonstrate the effectiveness of the proposed models which significantly outperform the state-of-the-art competitors.

#index 1991781
#* TurboGraph: a fast parallel graph engine handling billion-scale graphs in a single PC
#@ Wook-Shin Han;Sangyeon Lee;Kyungyeol Park;Jeong-Hoon Lee;Min-Soo Kim;Jinha Kim;Hwanjo Yu
#t 2013
#c 0
#% 53368
#% 291940
#% 765429
#% 824697
#% 881460
#% 983467
#% 1318636
#% 1382887
#% 1399992
#% 1426513
#% 1451193
#% 1523825
#% 1523835
#% 1606050
#% 1716273
#% 1769265
#% 1910905
#% 1911310
#% 1911311
#% 1959792
#% 1972772
#! Graphs are used to model many real objects such as social networks and web graphs. Many real applications in various fields require efficient and effective management of large-scale graph structured data. Although distributed graph engines such as GBase and Pregel handle billion-scale graphs, the user needs to be skilled at managing and tuning a distributed system in a cluster, which is a nontrivial job for the ordinary user. Furthermore, these distributed systems need many machines in a cluster in order to provide reasonable performance. In order to address this problem, a disk-based parallel graph engine called Graph-Chi, has been recently proposed. Although Graph-Chi significantly outperforms all representative (disk-based) distributed graph engines, we observe that Graph-Chi still has serious performance problems for many important types of graph queries due to 1) limited parallelism and 2) separate steps for I/O processing and CPU processing. In this paper, we propose a general, disk-based graph engine called TurboGraph to process billion-scale graphs very efficiently by using modern hardware on a single PC. TurboGraph is the first truly parallel graph engine that exploits 1) full parallelism including multi-core parallelism and FlashSSD IO parallelism and 2) full overlap of CPU processing and I/O processing as much as possible. Specifically, we propose a novel parallel execution model, called pin-and-slide. TurboGraph also provides engine-level operators such as BFS which are implemented under the pin-and-slide model. Extensive experimental results with large real datasets show that TurboGraph consistently and significantly outperforms Graph-Chi by up to four orders of magnitude! Our implementation of TurboGraph is available at ``http://wshan.net/turbograph}" as executable files.

#index 1991782
#* Flexible and robust co-regularized multi-domain graph clustering
#@ Wei Cheng;Xiang Zhang;Zhishan Guo;Yubao Wu;Patrick F. Sullivan;Wei Wang
#t 2013
#c 0
#% 372188
#% 643008
#% 722902
#% 757953
#% 770836
#% 785334
#% 881468
#% 983949
#% 989613
#% 1038986
#% 1073897
#% 1211706
#% 1318663
#% 1447022
#% 1451196
#% 1763764
#% 1898007
#% 1907561
#! Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach.

#index 1991783
#* Density-based logistic regression
#@ Wenlin Chen;Yixin Chen;Yi Mao;Baolong Guo
#t 2013
#c 0
#% 304917
#% 425040
#% 451933
#% 469390
#% 731607
#% 846432
#% 891559
#% 1215322
#% 1872367
#! This paper introduces a nonlinear logistic regression model for classification. The main idea is to map the data to a feature space based on kernel density estimation. A discriminative model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya-Watson kernel density estimator. We then propose a hierarchical optimization algorithm for learning the coefficients and kernel bandwidths in an integrated way. Compared to other nonlinear models such as kernel logistic regression (KLR) and SVM, our approach is far more efficient since it solves an optimization problem with a much smaller size. Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi-class problems. Moveover, our approach inherits from logistic regression good interpretability of the model, which is important for clinical applications but not offered by KLR and SVM. Extensive results on real datasets, including a clinical prediction application currently under deployment in a major hospital, show that our approach not only achieves superior classification accuracy, but also drastically reduces the computing time as compared to other leading methods.

#index 1991784
#* Extracting social events for learning better information diffusion models
#@ Shuyang Lin;Fengjiao Wang;Qingbo Hu;Philip S. Yu
#t 2013
#c 0
#% 729923
#% 989613
#% 1083624
#% 1083672
#% 1107420
#% 1159229
#% 1190127
#% 1214641
#% 1222654
#% 1333069
#% 1355040
#% 1451243
#% 1451246
#% 1846762
#% 1872232
#! Learning of the information diffusion model is a fundamental problem in the study of information diffusion in social networks. Existing approaches learn the diffusion models from events in social networks. However, events in social networks may have different underlying reasons. Some of them may be caused by the social influence inside the network, while others may reflect external trends in the ``real world''. Most existing work on the learning of diffusion models does not distinguish the events caused by the social influence from those caused by external trends. In this paper, we extract social events from data streams in social networks, and then use the extracted social events to improve the learning of information diffusion models. We propose a LADP (Latent Action Diffusion Path) model to incorporate the information diffusion model with the model of external trends, and then design an EM-based algorithm to infer the diffusion probabilities, the external trends and the sources of events efficiently.

#index 1991785
#* Mining lines in the sand: on trajectory discovery from untrustworthy data in cyber-physical system
#@ Lu-An Tang;Xiao Yu;Quanquan Gu;Jiawei Han;Alice Leung;Thomas La Porta
#t 2013
#c 0
#% 793871
#% 837835
#% 889115
#% 1080143
#% 1298480
#% 1535468
#% 1582169
#% 1693962
#% 1758331
#! A Cyber-Physical System (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance and traffic control. One key research problem of CPS is called "mining lines in the sand". With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all the trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy; (2) the intruders do not send out any identification information. The system needs to distinguish multiple intruders and track their movements. In this study, we propose a method called LiSM (Line-in-the-Sand Miner) to discover trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone-model from the historical trajectories and tracks multiple intruders based on this model. Finally the system validates the mining results and updates the sensor's reliability in a feedback process. Extensive experiments on big datasets demonstrate the feasibility and applicability of the proposed methods.

#index 1991786
#* An efficient ADMM algorithm for multidimensional anisotropic total variation regularization problems
#@ Sen Yang;Jie Wang;Wei Fan;Xiatian Zhang;Peter Wonka;Jieping Ye
#t 2013
#c 0
#% 127154
#% 137448
#% 735330
#% 757953
#% 1299937
#% 1302843
#% 1302853
#% 1302871
#% 1357036
#% 1451171
#% 1745124
#% 1854626
#% 1872595
#! Total variation (TV) regularization has important applications in signal processing including image denoising, image deblurring, and image reconstruction. A significant challenge in the practical use of TV regularization lies in the nondifferentiable convex optimization, which is difficult to solve especially for large-scale problems. In this paper, we propose an efficient alternating augmented Lagrangian method (ADMM) to solve total variation regularization problems. The proposed algorithm is applicable for tensors, thus it can solve multidimensional total variation regularization problems. One appealing feature of the proposed algorithm is that it does not need to solve a linear system of equations, which is often the most expensive part in previous ADMM-based methods. In addition, each step of the proposed algorithm involves a set of independent and smaller problems, which can be solved in parallel. Thus, the proposed algorithm scales to large size problems. Furthermore, the global convergence of the proposed algorithm is guaranteed, and the time complexity of the proposed algorithm is O(dN/ε) on a d-mode tensor with N entries for achieving an ε-optimal solution. Extensive experimental results demonstrate the superior performance of the proposed algorithm in comparison with current state-of-the-art methods.

#index 1991787
#* Speeding up large-scale learning with a social prior
#@ Deepayan Chakrabarti;Ralf Herbrich
#t 2013
#c 0
#% 300079
#% 715096
#% 856762
#% 1227601
#% 1399940
#% 1400002
#% 1451176
#% 1523670
#% 1536533
#% 1598399
#% 1765219
#% 1783934
#% 1872341
#! Slow convergence and poor initial accuracy are two problems that plague efforts to use very large feature sets in online learning. This is especially true when only a few features are "active" in any training example, and the frequency of activations of different features is skewed. We show how these problems can be mitigated if a graph of relationships between features is known. We study this problem in a fully Bayesian setting, focusing on the problem of using Facebook user-IDs as features, with the social network giving the relationship structure. Our analysis uncovers significant problems with the obvious regularizations, and motivates a two-component mixture-model "social prior" that is provably better. Empirical results on large-scale click prediction problems show that our algorithm can learn as well as the baseline with 12M fewer training examples, and continuously outperforms it for over 60M examples. On a second problem using binned features, our model outperforms the baseline even after the latter sees 5x as much data.

#index 1991788
#* Beyond myopic inference in big data pipelines
#@ Karthik Raman;Adith Swaminathan;Johannes Gehrke;Thorsten Joachims
#t 2013
#c 0
#% 466908
#% 579947
#% 748609
#% 787549
#% 817472
#% 939898
#% 1127378
#% 1127964
#% 1206717
#% 1261597
#% 1264787
#% 1275591
#% 1338672
#% 1367652
#% 1481634
#% 1511306
#% 1604587
#% 1769265
#% 1996205
#! Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy. We propose a novel model for reasoning across components of Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model. Different message passing schemes on this graphical model provide various inference algorithms to trade-off end-to-end performance and computational cost. We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.

#index 1991789
#* FISM: factored item similarity models for top-N recommender systems
#@ Santosh Kabbur;Xia Ning;George Karypis
#t 2013
#c 0
#% 274189
#% 297171
#% 301259
#% 734594
#% 1083671
#% 1396094
#% 1417104
#% 1476448
#% 1605920
#% 1688529
#! The effectiveness of existing top-N recommendation methods decreases as the sparsity of the datasets increases. To alleviate this problem, we present an item-based method for generating top-N recommendations that learns the item-item similarity matrix as the product of two low dimensional latent factor matrices. These matrices are learned using a structural equation modeling approach, wherein the value being estimated is not used for its own estimation. A comprehensive set of experiments on multiple datasets at three different sparsity levels indicate that the proposed methods can handle sparse datasets effectively and outperforms other state-of-the-art top-N recommendation methods. The experimental results also show that the relative performance gains compared to competing methods increase as the data gets sparser.

#index 1991790
#* Nonparametric hierarchal bayesian modeling in non-contractual heterogeneous survival data
#@ Shouichi Nagano;Yusuke Ichikawa;Noriko Takaya;Tadasu Uchiyama;Makoto Abe
#t 2013
#c 0
#% 28673
#% 810855
#% 951249
#% 951254
#% 1212305
#% 1250567
#! An important problem in the non-contractual marketing domain is discovering the customer lifetime and assessing the impact of customer's characteristic variables on the lifetime. Unfortunately, the conventional hierarchical Bayes model cannot discern the impact of customer's characteristic variables for each customer. To overcome this problem, we present a new survival model using a non-parametric Bayes paradigm with MCMC. The assumption of a conventional model, logarithm of purchase rate and dropout rate with linear regression, is extended to include our assumption of the Dirichlet Process Mixture of regression. The extension assumes that each customer belongs probabilistically to different mixtures of regression, thereby permitting us to estimate a different impact of customer characteristic variables for each customer. Our model creates several customer groups to mirror the structure of the target data set. The effectiveness of our proposal is confirmed by a comparison involving a real e-commerce transaction dataset and an artificial dataset; it generally achieves higher predictive performance. In addition, we show that preselecting the actual number of customer groups does not always lead to higher predictive performance.

#index 1991791
#* Fast and scalable polynomial kernels via explicit feature maps
#@ Ninh Pham;Rasmus Pagh
#t 2013
#c 0
#% 393059
#% 466597
#% 492912
#% 722815
#% 881477
#% 916799
#% 983905
#% 1117691
#% 1211829
#% 1558464
#% 1584820
#% 1606028
#% 1663950
#% 1978830
#! Approximation of non-linear kernels using random feature mapping has been successfully employed in large-scale data analysis applications, accelerating the training of kernel machines. While previous random feature mappings run in O(ndD) time for $n$ training samples in d-dimensional space and D random feature maps, we propose a novel randomized tensor product technique, called Tensor Sketching, for approximating any polynomial kernel in O(n(d+D \log{D})) time. Also, we introduce both absolute and relative error bounds for our approximation to guarantee the reliability of our estimation algorithm. Empirically, Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state-of-the-art approach for large-scale real-world datasets.

#index 1991792
#* SIGMa: simple greedy matching for aligning large knowledge bases
#@ Simon Lacoste-Julien;Konstantina Palla;Alex Davies;Gjergji Kasneci;Thore Graepel;Zoubin Ghahramani
#t 2013
#c 0
#% 579944
#% 742769
#% 896024
#% 924747
#% 937552
#% 940005
#% 956564
#% 1055897
#% 1116176
#% 1153043
#% 1190116
#% 1206834
#% 1246170
#% 1409974
#% 1538763
#% 1560363
#% 1654048
#% 1890006
#! The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm that leverages both the structural information from the relationship graph and flexible similarity measures between entity properties in a greedy local search, which makes it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high accuracy. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency.

#index 1991793
#* Cross-task crowdsourcing
#@ Kaixiang Mo;Erheng Zhong;Qiang Yang
#t 2013
#c 0
#% 1264744
#% 1272110
#% 1328303
#% 1464068
#% 1741662
#% 1746896
#% 1747273
#% 1872313
#! Crowdsourcing is an effective method for collecting labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually. In this case, each single task needs large amounts of data to provide accurate estimations. However, in practice, budgets provided by customers for a given target task may be limited, and hence each question can be presented to only a few users where each user can answer only a few questions. This data sparsity problem can cause previous approaches to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity. Fortunately, in real-world applications, users can answer questions from multiple historical tasks. For example, one can annotate images as well as label the sentiment of a given title. In this paper, we employ transfer learning, which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC (Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impact, TLC introduces task-specific factors to model task differences. The experimental results show that TLC significantly improves the accuracy over several state-of-the-art non-transfer-learning approaches under very limited budget in various labeling tasks.

#index 1991794
#* Multi-source learning with block-wise missing data for Alzheimer's disease prediction
#@ Shuo Xiang;Lei Yuan;Wei Fan;Yalin Wang;Paul M. Thompson;Jieping Ye
#t 2013
#c 0
#% 729437
#% 757953
#% 832903
#% 844199
#% 956633
#% 983807
#% 1083738
#% 1117687
#% 1128929
#% 1211797
#% 1302843
#% 1302853
#% 1357157
#% 1417091
#% 1447006
#% 1472310
#% 1745125
#! With the advances and increasing sophistication in data collection techniques, we are facing with large amounts of data collected from multiple heterogeneous sources in many applications. For example, in the study of Alzheimer's Disease (AD), different types of measurements such as neuroimages, gene/protein expression data, genetic data etc. are often collected and analyzed together for improved predictive power. It is believed that a joint learning of multiple data sources is beneficial as different data sources may contain complementary information, and feature-pruning and data source selection are critical for learning interpretable models from high-dimensional data. Very often the collected data comes with block-wise missing entries; for example, a patient without the MRI scan will have no information in the MRI data block, making his/her overall record incomplete. There has been a growing interest in the data mining community on expanding traditional techniques for single-source complete data analysis to the study of multi-source incomplete data. The key challenge is how to effectively integrate information from multiple heterogeneous sources in the presence of block-wise missing data. In this paper we first investigate the situation of complete data and present a unified ``bi-level" learning model for multi-source data. Then we give a natural extension of this model to the more challenging case with incomplete data. Our major contributions are threefold: (1) the proposed models handle both feature-level and source-level analysis in a unified formulation and include several existing feature learning approaches as special cases; (2) the model for incomplete data avoids direct imputation of the missing elements and thus provides superior performances. Moreover, it can be easily generalized to other applications with block-wise missing data sources; (3) efficient optimization algorithms are presented for both the complete and incomplete models. We have performed comprehensive evaluations of the proposed models on the application of AD diagnosis. Our proposed models compare favorably against existing approaches.

#index 1991795
#* Evaluating the crowd with confidence
#@ Manas Joglekar;Hector Garcia-Molina;Aditya Parameswaran
#t 2013
#c 0
#% 1083692
#% 1150163
#% 1211801
#% 1214647
#% 1264744
#% 1338533
#% 1531270
#% 1550748
#% 1565816
#% 1628171
#% 1747273
#% 1770349
#% 1770351
#% 1869838
#% 1872257
#% 1872366
#% 1880463
#% 1928024
#! Worker quality control is a crucial aspect of crowdsourcing systems; typically occupying a large fraction of the time and money invested on crowdsourcing. In this work, we devise techniques to generate confidence intervals for worker error rate estimates, thereby enabling a better evaluation of worker quality. We show that our techniques generate correct confidence intervals on a range of real-world datasets, and demonstrate wide applicability by using them to evict poorly performing workers, and provide confidence intervals on the accuracy of the answers.

#index 1991796
#* Representing documents through their readers
#@ Khalid El-Arini;Min Xu;Emily B. Fox;Carlos Guestrin
#t 2013
#c 0
#% 722904
#% 754126
#% 875959
#% 1077150
#% 1214484
#% 1214650
#% 1338553
#% 1399999
#% 1441070
#% 1693876
#% 1711748
#% 1872311
#% 1911310
#! From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers? To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users describe themselves in a short profile, labeling themselves with phrases such as "vegetarian" or "liberal." By assuming that a user's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty. Finally, we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the "politics" label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is effective.

#index 1991797
#* Inferring social roles and statuses in social networks
#@ Yuchen Zhao;Guan Wang;Philip S. Yu;Shaobo Liu;Simon Zhang
#t 2013
#c 0
#% 730089
#% 766484
#% 771846
#% 949164
#% 992948
#% 1055737
#% 1190108
#% 1214702
#% 1272187
#% 1355041
#% 1399963
#% 1417383
#% 1425621
#% 1536568
#% 1540712
#% 1560425
#% 1567510
#% 1584788
#% 1594652
#% 1606084
#% 1688467
#% 1693935
#% 1747032
#% 1846813
#% 1872301
#% 1872378
#% 1992522
#! Users in online social networks play a variety of social roles and statuses. For example, users in Twitter can be represented as advertiser, content contributor, information receiver, etc; users in Linkedin can be in different professional roles, such as engineer, salesperson and recruiter. Previous research work mainly focuses on using categorical and textual information to predict the attributes of users. However, it cannot be applied to a large number of users in real social networks, since much of such information is missing, outdated and non-standard. In this paper, we investigate the social roles and statuses that people act in online social networks in the perspective of network structures, since the uniqueness of social networks is connecting people. We quantitatively analyze a number of key social principles and theories that correlate with social roles and statuses. We systematically study how the network characteristics reflect the social situations of users in an online society. We discover patterns of homophily, the tendency of users to connect with users with similar social roles and statuses. In addition, we observe that different factors in social theories influence the social role/status of an individual user to various extent, since these social principles represent different aspects of the network. We then introduce an optimization framework based on Factor Conditioning Symmetry, and we propose a probabilistic model to integrate the optimization framework on local structural information as well as network influence to infer the unknown social roles and statuses of online users. We will present experiment results to show the effectiveness of the inference.

#index 1991798
#* Adaptive collective routing using gaussian process dynamic congestion models
#@ Siyuan Liu;Yisong Yue;Ramayya Krishnan
#t 2013
#c 0
#% 105870
#% 765257
#% 891549
#% 960389
#% 989613
#% 1039650
#% 1098282
#% 1110802
#% 1290265
#% 1368060
#% 1409360
#% 1451232
#% 1602947
#% 1605948
#% 1606041
#% 1613884
#% 1650463
#% 1743757
#% 1894910
#! We consider the problem of adaptively routing a fleet of cooperative vehicles within a road network in the presence of uncertain and dynamic congestion conditions. To tackle this problem, we first propose a Gaussian Process Dynamic Congestion Model that can effectively characterize both the dynamics and the uncertainty of congestion conditions. Our model is efficient and thus facilitates real-time adaptive routing in the face of uncertainty. Using this congestion model, we develop an efficient algorithm for non-myopic adaptive routing to minimize the collective travel time of all vehicles in the system. A key property of our approach is the ability to efficiently reason about the long-term value of exploration, which enables collectively balancing the exploration/exploitation trade-off for entire fleets of vehicles. We validate our approach based on traffic data from two large Asian cities. We show that our congestion model is effective in modeling dynamic congestion conditions. We also show that our routing algorithm generates significantly faster routes compared to standard baselines, and achieves near-optimal performance compared to an omniscient routing algorithm. We also present the results from a preliminary field study, which showcases the efficacy of our approach.

#index 1991799
#* Maximizing acceptance probability for active friending in online social networks
#@ De-Nian Yang;Hui-Ju Hung;Wang-Chien Lee;Wei Chen
#t 2013
#c 0
#% 379482
#% 729923
#% 823403
#% 960305
#% 989613
#% 1002007
#% 1107420
#% 1181262
#% 1183090
#% 1246431
#% 1287226
#% 1355040
#% 1399993
#% 1451176
#% 1451243
#% 1523857
#% 1523898
#% 1535380
#% 1610256
#% 1628176
#% 1746851
#% 1879048
#% 1948128
#! Friending recommendation has successfully contributed to the explosive growth of online social networks. Most friending recommendation services today aim to support passive friending, where a user passively selects friending targets from the recommended candidates. In this paper, we advocate a recommendation support for active friending, where a user actively specifies a friending target. To the best of our knowledge, a recommendation designed to provide guidance for a user to systematically approach his friending target has not been explored for existing online social networking services. To maximize the probability that the friending target would accept an invitation from the user, we formulate a new optimization problem, namely, Acceptance Probability Maximization (APM), and develop a polynomial time algorithm, called Selective Invitation with Tree and In-Node Aggregation (SITINA), to find the optimal solution. We implement an active friending service with SITINA on Facebook to validate our idea. Our user study and experimental results reveal that SITINA outperforms manual selection and the baseline approach in solution quality efficiently.

#index 1991800
#* Statistical quality estimation for general crowdsourcing tasks
#@ Yukino Baba;Hisashi Kashima
#t 2013
#c 0
#% 1083692
#% 1214647
#% 1264744
#% 1428411
#% 1477559
#% 1477589
#% 1477591
#% 1480225
#% 1592050
#% 1598354
#% 1948142
#! One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose an unsupervised statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure; multiple workers are first requested to work on the same tasks in the creation stage, and then another set of workers review and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, which implies that our method can deliver high quality results with lower costs.

#index 1991801
#* Mining frequent graph patterns with differential privacy
#@ Entong Shen;Ting Yu
#t 2013
#c 0
#% 478274
#% 629708
#% 772884
#% 813989
#% 907530
#% 1029084
#% 1041838
#% 1074831
#% 1214684
#% 1328170
#% 1449326
#% 1451190
#% 1581409
#% 1581864
#% 1595893
#% 1605968
#% 1646490
#% 1648675
#% 1732708
#% 1740518
#% 1846816
#% 1880451
#% 1882460
#! Discovering frequent graph patterns in a graph database offers valuable information in a variety of applications. However, if the graph dataset contains sensitive data of individuals such as mobile phone-call graphs and web-click graphs, releasing discovered frequent patterns may present a threat to the privacy of individuals. Differential privacy has recently emerged as the de facto standard for private data analysis due to its provable privacy guarantee. In this paper we propose the first differentially private algorithm for mining frequent graph patterns. We first show that previous techniques on differentially private discovery of frequent itemsets cannot apply in mining frequent graph patterns due to the inherent complexity of handling structural information in graphs. We then address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling based algorithm. Unlike previous work on frequent itemset mining, our techniques do not rely on the output of a non-private mining algorithm. Instead, we observe that both frequent graph pattern mining and the guarantee of differential privacy can be unified into an MCMC sampling framework. In addition, we establish the privacy and utility guarantee of our algorithm and propose an efficient neighboring pattern counting technique as well. Experimental results show that the proposed algorithm is able to output frequent patterns with good precision.

#index 1991802
#* Approximate graph mining with label costs
#@ Pranay Anchuri;Mohammed J. Zaki;Omer Barkol;Shahar Golan;Moshe Shamy
#t 2013
#c 0
#% 431105
#% 466644
#% 629708
#% 727845
#% 814008
#% 841960
#% 1072518
#% 1102991
#% 1117041
#% 1206703
#% 1328170
#% 1411112
#% 1668371
#! Many real-world graphs have complex labels on the nodes and edges. Mining only exact patterns yields limited insights, since it may be hard to find exact matches. However, in many domains it is relatively easy to define a cost (or distance) between different labels. Using this information, it becomes possible to mine a much richer set of approximate subgraph patterns, which preserve the topology but allow bounded label mismatches. We present novel and scalable methods to efficiently solve the approximate isomorphism problem. We show that approximate mining yields interesting patterns in several real-world graphs ranging from IT and protein interaction networks to protein structures.

#index 1991803
#* Mining evolutionary multi-branch trees from text streams
#@ Xiting Wang;Shixia Liu;Yangqiu Song;Baining Guo
#t 2013
#c 0
#% 208882
#% 722902
#% 769881
#% 840903
#% 844386
#% 875959
#% 881514
#% 989650
#% 1166524
#% 1176974
#% 1176975
#% 1183429
#% 1314743
#% 1380802
#% 1451248
#% 1560381
#% 1606034
#% 1646372
#% 1688469
#% 1688490
#% 1779354
#% 1872404
#! Understanding topic hierarchies in text streams and their evolution patterns over time is very important in many applications. In this paper, we propose an evolutionary multi-branch tree clustering method for streaming text data. We build evolutionary trees in a Bayesian online filtering framework. The tree construction is formulated as an online posterior estimation problem, which considers both the likelihood of the current tree and conditional prior given the previous tree. We also introduce a constraint model to compute the conditional prior of a tree in the multi-branch setting. Experiments on real world news data demonstrate that our algorithm can better incorporate historical tree information and is more efficient and effective than the traditional evolutionary hierarchical clustering algorithm.

#index 1991804
#* Robust principal component analysis via capped norms
#@ Qian Sun;Shuo Xiang;Jieping Ye
#t 2013
#c 0
#% 336073
#% 1386133
#% 1504249
#% 1555961
#% 1556166
#% 1566264
#% 1653966
#% 1654409
#% 1661309
#% 1745124
#% 1818266
#% 1855689
#% 1927987
#! In many applications such as image and video processing, the data matrix often possesses simultaneously a low-rank structure capturing the global information and a sparse component capturing the local information. How to accurately extract the low-rank and sparse components is a major challenge. Robust Principal Component Analysis (RPCA) is a general framework to extract such structures. It is well studied that under certain assumptions, convex optimization using the trace norm and l1-norm can be an effective computation surrogate of the difficult RPCA problem. However, such convex formulation is based on a strong assumption which may not hold in real-world applications, and the approximation error in these convex relaxations often cannot be neglected. In this paper, we present a novel non-convex formulation for the RPCA problem using the capped trace norm and the capped l1-norm. In addition, we present two algorithms to solve the non-convex optimization: one is based on the Difference of Convex functions (DC) framework and the other attempts to solve the sub-problems via a greedy approach. Our empirical evaluations on synthetic and real-world data show that both of the proposed algorithms achieve higher accuracy than existing convex formulations. Furthermore, between the two proposed algorithms, the greedy algorithm is more efficient than the DC programming, while they achieve comparable accuracy.

#index 1991805
#* Active search on graphs
#@ Xuezhi Wang;Roman Garnett;Jeff Schneider
#t 2013
#c 0
#% 425053
#% 1073894
#! Active search is an increasingly important learning problem in which we use a limited budget of label queries to discover as many members of a certain class as possible. Numerous real-world applications may be approached in this manner, including fraud detection, product recommendation, and drug discovery. Active search has model learning and exploration/exploitation features similar to those encountered in active learning and bandit problems, but algorithms for those problems do not fit active search. Previous work on the active search problem [5] showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selections to be made and proposed a truncated lookahead heuristic. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs. We suggest selecting points by maximizing a score considering the potential impact of selecting a node, meant to emulate lookahead while avoiding exponential search. We test the proposed algorithm empirically on real-world graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps.

#index 1991806
#* Fast rank-2 nonnegative matrix factorization for hierarchical document clustering
#@ Da Kuang;Haesun Park
#t 2013
#c 0
#% 46809
#% 411762
#% 420495
#% 430958
#% 629653
#% 643008
#% 722904
#% 763708
#% 995168
#% 1038899
#% 1077150
#% 1133918
#% 1176925
#% 1711748
#% 1845372
#! Nonnegative matrix factorization (NMF) has been successfully used as a clustering method especially for flat partitioning of documents. In this paper, we propose an efficient hierarchical document clustering method based on a new algorithm for rank-2 NMF. When the two block coordinate descent framework of nonnegative least squares is applied to computing rank-2 NMF, each subproblem requires a solution for nonnegative least squares with only two columns in the matrix. We design the algorithm for rank-2 NMF by exploiting the fact that an exhaustive search for the optimal active set can be performed extremely fast when solving these NNLS problems. In addition, we design a measure based on the results of rank-2 NMF for determining which leaf node should be further split. On a number of text data sets, our proposed method produces high-quality tree structures in significantly less time compared to other methods such as hierarchical K-means, standard NMF, and latent Dirichlet allocation.

#index 1991807
#* The role of information diffusion in the evolution of social networks
#@ Lilian Weng;Jacob Ratkiewicz;Nicola Perra;Bruno Gonçalves;Carlos Castillo;Francesco Bonchi;Rossano Schifanella;Filippo Menczer;Alessandro Flammini
#t 2013
#c 0
#% 131258
#% 593994
#% 881523
#% 955712
#% 1083675
#% 1185580
#% 1222654
#% 1355043
#% 1394202
#% 1396209
#% 1428692
#% 1536509
#% 1536568
#% 1560424
#% 1740995
#% 1930576
#% 1948126
#! Every day millions of users are connected through online social networks, generating a rich trove of data that allows us to study the mechanisms behind human interactions. Triadic closure has been treated as the major mechanism for creating social links: if Alice follows Bob and Bob follows Charlie, Alice will follow Charlie. Here we present an analysis of longitudinal micro-blogging data, revealing a more nuanced view of the strategies employed by users when expanding their social circles. While the network structure affects the spread of information among users, the network is in turn shaped by this communication activity. This suggests a link creation mechanism whereby Alice is more likely to follow Charlie after seeing many messages by Charlie. We characterize users with a set of parameters associated with different link creation strategies, estimated by a Maximum-Likelihood approach. Triadic closure does have a strong effect on link formation, but shortcuts based on traffic are another key factor in interpreting network evolution. However, individual strategies for following other users are highly heterogeneous. Link creation behaviors can be summarized by classifying users in different categories with distinct structural and behavioral characteristics. Users who are popular, active, and influential tend to create traffic-based shortcuts, making the information diffusion process more efficient in the network.

#index 1991808
#* LCARS: a location-content-aware recommender system
#@ Hongzhi Yin;Yizhou Sun;Bin Cui;Zhiting Hu;Ling Chen
#t 2013
#c 0
#% 330687
#% 333854
#% 452563
#% 770816
#% 813966
#% 823392
#% 849492
#% 883129
#% 1083671
#% 1083734
#% 1190123
#% 1214685
#% 1400036
#% 1476448
#% 1480830
#% 1541728
#% 1550750
#% 1598366
#% 1606045
#% 1606049
#% 1606051
#% 1625379
#% 1650545
#% 1695791
#% 1730808
#% 1846747
#% 1848116
#% 1872355
#% 1872384
#% 1893801
#% 1919779
#% 1941002
#! Newly emerging location-based and event-based social network services provide us with a new platform to understand users' preferences based on their activity history. A user can only visit a limited number of venues/events and most of them are within a limited distance range, so the user-item matrix is very sparse, which creates a big challenge for traditional collaborative filtering-based recommender systems. The problem becomes more challenging when people travel to a new city where they have no activity history. In this paper, we propose LCARS, a location-content-aware recommender system that offers a particular user a set of venues (e.g., restaurants) or events (e.g., concerts and exhibitions) by giving consideration to both personal interest and local preference. This recommender system can facilitate people's travel not only near the area in which they live, but also in a city that is new to them. Specifically, LCARS consists of two components: offline modeling and online recommendation. The offline modeling part, called LCA-LDA, is designed to learn the interest of each individual user and the local preference of each individual city by capturing item co-occurrence patterns and exploiting item contents. The online recommendation part automatically combines the learnt interest of the querying user and the local preference of the querying city to produce the top-k recommendations. To speed up this online process, a scalable query processing technique is developed by extending the classic Threshold Algorithm (TA). We evaluate the performance of our recommender system on two large-scale real data sets, DoubanEvent and Foursquare. The results show the superiority of LCARS in recommending spatial items for users, especially when traveling to new cities, in terms of both effectiveness and efficiency.

#index 1991809
#* A “semi-lazy” approach to probabilistic path prediction in dynamic environments
#@ Jingbo Zhou;Anthony K.H. Tung;Wei Wu;Wee Siong Ng
#t 2013
#c 0
#% 300174
#% 321455
#% 421124
#% 729866
#% 765452
#% 772835
#% 827132
#% 1099023
#% 1206625
#% 1207011
#% 1214637
#% 1214685
#% 1456850
#% 1663073
#% 1667194
#% 1728806
#% 1760888
#% 1846700
#% 1846833
#% 1884414
#% 1897313
#% 1930422
#! Path prediction is useful in a wide range of applications. Most of the existing solutions, however, are based on eager learning methods where models and patterns are extracted from historical trajectories and then used for future prediction. Since such approaches are committed to a set of statistically significant models or patterns, problems can arise in dynamic environments where the underlying models change quickly or where the regions are not covered with statistically significant models or patterns. We propose a "semi-lazy" approach to path prediction that builds prediction models on the fly using dynamically selected reference trajectories. Such an approach has several advantages. First, the target trajectories to be predicted are known before the models are built, which allows us to construct models that are deemed relevant to the target trajectories. Second, unlike the lazy learning approaches, we use sophisticated learning algorithms to derive accurate prediction models with acceptable delay based on a small number of selected reference trajectories. Finally, our approach can be continuously self-correcting since we can dynamically re-construct new models if the predicted movements do not match the actual ones. Our prediction model can construct a probabilistic path whose probability of occurrence is larger than a threshold and which is furthest ahead in term of time. Users can control the confidence of the path prediction by setting a probability threshold. We conducted a comprehensive experimental study on real-world and synthetic datasets to show the effectiveness and efficiency of our approach.

#index 1991810
#* Multi-label relational neighbor classification using social context features
#@ Xi Wang;Gita Sukthankar
#t 2013
#c 0
#% 248810
#% 729982
#% 763708
#% 961278
#% 1000502
#% 1214703
#% 1292578
#% 1344744
#% 1417071
#% 1428692
#% 1606073
#% 1618917
#% 1650403
#% 1715951
#% 1815596
#% 1826270
#! Networked data, extracted from social media, web pages, and bibliographic databases, can contain entities of multiple classes, interconnected through different types of links. In this paper, we focus on the problem of performing multi-label classification on networked data, where the instances in the network can be assigned multiple labels. In contrast to traditional content-only classification methods, relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances. However, instances in a network can be linked for various causal reasons, hence treating all links in a homogeneous way can limit the performance of relational classifiers. In this paper, we propose a multi-label iterative relational neighbor classifier that employs social context features (SCRN). Our classifier incorporates a class propagation probability distribution obtained from instances' social features, which are in turn extracted from the network topology. This class-propagation probability captures the node's intrinsic likelihood of belonging to each class, and serves as a prior weight for each class when aggregating the neighbors' class labels in the collective inference procedure. Experiments on several real-world datasets demonstrate that our proposed classifier boosts classification performance over common benchmarks on networked multi-label data.

#index 1991811
#* Optimizing parallel belief propagation in junction treesusing regression
#@ Lu Zheng;Ole Mengshoel
#t 2013
#c 0
#% 5179
#% 427411
#% 887531
#% 1062558
#% 1428981
#% 1442146
#% 1510392
#% 1693957
#% 1881282
#! The junction tree approach, with applications in artificial intelligence, computer vision, machine learning, and statistics, is often used for computing posterior distributions in probabilistic graphical models. One of the key challenges associated with junction trees is computational, and several parallel computing technologies - including many-core processors - have been investigated to meet this challenge. Many-core processors (including GPUs) are now programmable, unfortunately their complexities make it hard to manually tune their parameters in order to optimize software performance. In this paper, we investigate a machine learning approach to minimize the execution time of parallel junction tree algorithms implemented on a GPU. By carefully allocating a GPU's threads to different parallel computing opportunities in a junction tree, and treating this thread allocation problem as a machine learning problem, we find in experiments that regression - specifically support vector regression - can substantially outperform manual optimization.

#index 1991812
#* Multi-source deep learning for information trustworthiness estimation
#@ Liang Ge;Jing Gao;Xiaoyi Li;Aidong Zhang
#t 2013
#c 0
#% 450888
#% 754097
#% 785334
#% 891060
#% 956513
#% 983903
#% 1035590
#% 1081580
#% 1269893
#% 1302905
#% 1358747
#% 1450883
#% 1482272
#% 1617306
#% 1826432
#% 1959789
#% 1978817
#! In recent years, information trustworthiness has become a serious issue when user-generated contents prevail in our information world. In this paper, we investigate the important problem of estimating information trustworthiness from the perspective of correlating and comparing multiple data sources. To a certain extent, the consistency degree is an indicator of information reliability--Information unanimously agreed by all the sources is more likely to be reliable. Based on this principle, we develop an effective computational approach to identify consistent information from multiple data sources. Particularly, we analyze vast amounts of information collected from multiple review platforms (multiple sources) in which people can rate and review the items they have purchased. The major challenge is that different platforms attract diverse sets of users, and thus information cannot be compared directly at the surface. However, latent reasons hidden in user ratings are mostly shared by multiple sources, and thus inconsistency about an item only appears when some source provides ratings deviating from the common latent reasons. Therefore, we propose a novel two-step procedure to calculate information consistency degrees for a set of items which are rated by multiple sets of users on different platforms. We first build a Multi-Source Deep Belief Network (MSDBN) to identify the common reasons hidden in multi-source rating data, and then calculate a consistency score for each item by comparing individual sources with the reconstructed data derived from the latent reasons. We conduct experiments on real user ratings collected from Orbitz, Priceline and TripAdvisor on all the hotels in Las Vegas and New York City. Experimental results demonstrate that the proposed approach successfully finds the hotels that receive inconsistent, and possibly unreliable, ratings.

#index 1991813
#* Model selection in markovian processes
#@ Assaf Hallak;Dotan Di-Castro;Shie Mannor
#t 2013
#c 0
#% 75936
#% 363744
#% 577237
#% 655325
#% 702594
#% 729437
#% 878207
#% 959214
#% 1289471
#% 1354564
#% 1647890
#% 1787250
#! When analyzing data that originated from a dynamical system, a common practice is to encompass the problem in the well known frameworks of Markov Decision Processes (MDPs) and Reinforcement Learning (RL). The state space in these solutions is usually chosen in some heuristic fashion and the formed MDP can then be used to simulate and predict data, as well as indicate the best possible action in each state. The model chosen to characterize the data affects the complexity and accuracy of any further action we may wish to apply, yet few methods that rely on the dynamic structure to select such a model were suggested. In this work we address the problem of how to use time series data to choose from a finite set of candidate discrete state spaces, where these spaces are constructed by a domain expert. We formalize the notion of model selection consistency in the proposed setup. We then discuss the difference between our proposed framework and the classical Maximum Likelihood (ML) framework, and give an example where ML fails. Afterwards, we suggest alternative selection criteria and show them to be weakly consistent. We then define weak consistency for a model construction algorithm and show a simple algorithm that is weakly consistent. Finally, we test the performance of the suggested criteria and algorithm on both simulated and real world data.

#index 1991814
#* Unsupervised link prediction using aggregative statistics on heterogeneous social networks
#@ Tsung-Ting Kuo;Rui Yan;Yu-Yang Huang;Perng-Hwa Kung;Shou-De Lin
#t 2013
#c 0
#% 44876
#% 411762
#% 503213
#% 729936
#% 829043
#% 875974
#% 955712
#% 1083734
#% 1399997
#% 1451159
#% 1451178
#% 1475157
#% 1560174
#% 1560645
#% 1598366
#% 1635130
#% 1642046
#% 1691742
#% 1693935
#% 1746844
#% 1810385
#% 1913422
#% 1966744
#% 1978832
#! The concern of privacy has become an important issue for online social networks. In services such as Foursquare.com, whether a person likes an article is considered private and therefore not disclosed; only the aggregative statistics of articles (i.e., how many people like this article) is revealed. This paper tries to answer a question: can we predict the opinion holder in a heterogeneous social network without any labeled data? This question can be generalized to a link prediction with aggregative statistics problem. This paper devises a novel unsupervised framework to solve this problem, including two main components: (1) a three-layer factor graph model and three types of potential functions; (2) a ranked-margin learning and inference algorithm. Finally, we evaluate our method on four diverse prediction scenarios using four datasets: preference (Foursquare), repost (Twitter), response (Plurk), and citation (DBLP). We further exploit nine unsupervised models to solve this problem as baselines. Our approach not only wins out in all scenarios, but on the average achieves 9.90% AUC and 12.59% NDCG improvement over the best competitors. The resources are available at http://www.csie.ntu.edu.tw/~d97944007/aggregative/

#index 1991815
#* Link prediction with social vector clocks
#@ Conrad Lee;Bobo Nick;Ulrik Brandes;Pádraig Cunningham
#t 2013
#c 0
#% 99833
#% 104402
#% 121506
#% 300078
#% 320187
#% 730089
#% 787244
#% 806990
#% 1014037
#% 1083672
#% 1113174
#% 1203761
#% 1254832
#% 1366213
#% 1451163
#% 1536509
#% 1616923
#% 1653960
#% 1653970
#% 1693927
#% 1966744
#! State-of-the-art link prediction utilizes combinations of complex features derived from network panel data. We here show that computationally less expensive features can achieve the same performance in the common scenario in which the data is available as a sequence of interactions. Our features are based on social vector clocks, an adaptation of the vector-clock concept introduced in distributed computing to social interaction networks. In fact, our experiments suggest that by taking into account the order and spacing of interactions, social vector clocks exploit different aspects of link formation so that their combination with previous approaches yields the most accurate predictor to date.

#index 1991816
#* Geo-spotting: mining online location-based services for optimal retail store placement
#@ Dmytro Karamshuk;Anastasios Noulas;Salvatore Scellato;Vincenzo Nicosia;Cecilia Mascolo
#t 2013
#c 0
#% 115608
#% 411762
#% 428965
#% 445243
#% 840846
#% 1267720
#% 1480891
#% 1535435
#% 1567948
#% 1606060
#% 1872249
#% 1894912
#% 1941002
#! The problem of identifying the optimal location for a new retail store has been the focus of past research, especially in the field of land economy, due to its importance in the success of a business. Traditional approaches to the problem have factored in demographics, revenue and aggregated human flow statistics from nearby or remote areas. However, the acquisition of relevant data is usually expensive. With the growth of location-based social networks, fine grained data describing user mobility and popularity of places has recently become attainable. In this paper we study the predictive power of various machine learning features on the popularity of retail stores in the city through the use of a dataset collected from Foursquare in New York. The features we mine are based on two general signals: geographic, where features are formulated according to the types and density of nearby places, and user mobility, which includes transitions between venues or the incoming flow of mobile users from distant areas. Our evaluation suggests that the best performing features are common across the three different commercial chains considered in the analysis, although variations may exist too, as explained by heterogeneities in the way retail facilities attract users. We also show that performance improves significantly when combining multiple features in supervised learning algorithms, suggesting that the retail success of a business may depend on multiple factors.

#index 1991817
#* Location-aware publish/subscribe
#@ Guoliang Li;Yang Wang;Ting Wang;Jianhua Feng
#t 2013
#c 0
#% 124009
#% 158911
#% 271199
#% 333938
#% 463734
#% 480296
#% 736381
#% 813189
#% 818938
#% 837356
#% 838407
#% 874993
#% 982560
#% 1015276
#% 1206801
#% 1206997
#% 1328137
#% 1523828
#% 1581875
#% 1581876
#% 1581877
#% 1594674
#% 1846749
#% 1848110
#% 1918373
#% 1918428
#! Location-based services have become widely available on mobile devices. Existing methods employ a pull model or user-initiated model, where a user issues a query to a server which replies with location-aware answers. To provide users with instant replies, a push model or server-initiated model is becoming an inevitable computing model in the next-generation location-based services. In the push model, subscribers register spatio-textual subscriptions to capture their interests, and publishers post spatio-textual messages. This calls for a high-performance location-aware publish/subscribe system to deliver publishers' messages to relevant subscribers.In this paper, we address the research challenges that arise in designing a location-aware publish/subscribe system. We propose an rtree based index structure by integrating textual descriptions into rtree nodes. We devise efficient filtering algorithms and develop effective pruning techniques to improve filtering efficiency. Experimental results show that our method achieves high performance. For example, our method can filter 500 tweets in a second for 10 million registered subscriptions on a commodity computer.

#index 1991818
#* Summarizing probabilistic frequent patterns: a fast approach
#@ Chunyang Liu;Ling Chen;Chengqi Zhang
#t 2013
#c 0
#% 248791
#% 464873
#% 478770
#% 823356
#% 824710
#% 1083668
#% 1189215
#% 1214624
#% 1214633
#% 1214690
#% 1393138
#% 1411089
#% 1451166
#% 1482221
#% 1535367
#% 1602175
#% 1744114
#% 1846710
#% 1872234
#% 1880477
#! Mining probabilistic frequent patterns from uncertain data has received a great deal of attention in recent years due to the wide applications. However, probabilistic frequent pattern mining suffers from the problem that an exponential number of result patterns are generated, which seriously hinders further evaluation and analysis. In this paper, we focus on the problem of mining probabilistic representative frequent patterns (P-RFP), which is the minimal set of patterns with adequately high probability to represent all frequent patterns. Observing the bottleneck in checking whether a pattern can probabilistically represent another, which involves the computation of a joint probability of the supports of two patterns, we introduce a novel approximation of the joint probability with both theoretical and empirical proofs. Based on the approximation, we propose an Approximate P-RFP Mining (APM) algorithm, which effectively and efficiently compresses the set of probabilistic frequent patterns. To our knowledge, this is the first attempt to analyze the relationship between two probabilistic frequent patterns through an approximate approach. Our experiments on both synthetic and real-world datasets demonstrate that the APM algorithm accelerates P-RFP mining dramatically, orders of magnitudes faster than an exact solution. Moreover, the error rate of APM is guaranteed to be very small when the database contains hundreds transactions, which further affirms APM is a practical solution for summarizing probabilistic frequent patterns.

#index 1991819
#* Network discovery via constrained tensor analysis of fMRI data
#@ Ian Davidson;Sean Gilpin;Owen Carmichael;Peter Walker
#t 2013
#c 0
#% 1214755
#% 1300087
#% 1451196
#! We pose the problem of network discovery which involves simplifying spatio-temporal data into cohesive regions (nodes) and relationships between those regions (edges). Such problems naturally exist in fMRI scans of human subjects. These scans consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easy to implement. We show this formulation works well in controlled experiments where supervision is incomplete, superfluous and noisy and is able to recover the underlying ground truth network. We then show that for real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting-state healthy and Alzheimer affected individuals. Finally, we show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with clinical information.

#index 1991820
#* Guided learning for role discovery (GLRD): framework, algorithms, and applications
#@ Sean Gilpin;Tina Eliassi-Rad;Ian Davidson
#t 2013
#c 0
#% 757953
#% 793248
#% 915294
#% 1002007
#% 1073906
#% 1085668
#% 1176992
#% 1211772
#% 1214693
#% 1260689
#% 1451231
#% 1565432
#% 1605987
#% 1780377
#% 1845612
#% 1872378
#! Role discovery in graphs is an emerging area that allows analysis of complex graphs in an intuitive way. In contrast to community discovery, which finds groups of highly connected nodes, role discovery finds groups of nodes that share similar topological structure in the graph, and hence a common role (or function) such as being a broker or a periphery node. However, existing work so far is completely unsupervised, which is undesirable for a number of reasons. We provide an alternating least squares framework that allows convex constraints to be placed on the role discovery problem, which can provide useful supervision. In particular we explore supervision to enforce i) sparsity, ii) diversity, and iii) alternativeness in the roles. We illustrate the usefulness of this supervision on various data sets and applications.

#index 1991821
#* Quadratic optimization to identify highly heritable quantitative traits from complex phenotypic features
#@ Jiangwen Sun;Jinbo Bi;Henry R. Kranzler
#t 2013
#c 0
#% 743284
#! Identifying genetic variation underlying a complex disease is important. Many complex diseases have heterogeneous phenotypes and are products of a variety of genetic and environmental factors acting in concert. Deriving highly heritable quantitative traits of a complex disease can improve the identification of genetic risk of the disease. The most sophisticated methods so far perform unsupervised cluster analysis on phenotypic features; and then a quantitative trait is derived based on each resultant cluster. Heritability is estimated to assess the validity of the derived quantitative traits. However, none of these methods explicitly maximize the heritability of the derived traits. We propose a quadratic optimization approach that directly utilizes heritability as an objective during the derivation of quantitative traits of a disease. This method maximizes an objective function that is formulated by decomposing the traditional maximum likelihood method for estimating heritability of a quantitative trait. We demonstrate the effectiveness of the proposed method on both synthetic data and real-world problems. We apply our algorithm to identify highly heritable traits of complex human-behavior disorders including opioid and cocaine use disorders, and highly heritable traits of dairy cattle that are economically important. Our approach outperforms standard cluster analysis and several previous methods.

#index 1991822
#* Repetition-aware content placement in navigational networks
#@ Dora Erdos;Vatche Ishakian;Azer Bestavros;Evimaria Terzi
#t 2013
#c 0
#% 268079
#% 271153
#% 342596
#% 408396
#% 577217
#% 729923
#% 989613
#% 1039690
#% 1190057
#% 1407355
#% 1657966
#! Arguably, the most effective technique to ensure wide adoption of a concept (or product) is by repeatedly exposing individuals to messages that reinforce the concept (or promote the product). Recognizing the role of repeated exposure to a message, in this paper we propose a novel framework for the effective placement of content: Given the navigational patterns of users in a network, e.g., web graph, hyperlinked corpus, or road network, and given a model of the relationship between content-adoption and frequency of exposition, we define the repetition-aware content-placement (RACP) problem as that of identifying the set of B nodes on which content should be placed so that the expected number of users adopting that content is maximized. The key contribution of our work is the introduction of memory into the navigation process, by making user conversion dependent on the number of her exposures to that content. This dependency is captured using a conversion model that is general enough to capture arbitrary dependencies. Our solution to this general problem builds upon the notion of absorbing random walks, which we extend appropriately in order to address the technicalities of our definitions. Although we show the RACP problem to be NP-hard, we propose a general and efficient algorithmic solution. Our experimental results demonstrate the efficacy and the efficiency of our methods in multiple real-world datasets obtained from different application domains.

#index 1991823
#* Simple and deterministic matrix sketching
#@ Edo Liberty
#t 2013
#c 0
#% 248027
#% 333881
#% 453490
#% 548479
#% 569754
#% 593842
#% 649540
#% 898279
#% 938793
#% 967461
#% 1133926
#% 1164922
#% 1198209
#% 1211829
#% 1426284
#% 1537383
#% 1542355
#% 1658035
#% 1668261
#% 1727910
#% 1727912
#% 1810736
#% 1992591
#! A sketch of a matrix A is another matrix B which is significantly smaller than A but still approximates it well. Finding such sketches efficiently is an important building block in modern algorithms for approximating, for example, the PCA of massive matrices. This task is made more challenging in the streaming model, where each row of the input matrix can only be processed once and storage is severely limited. In this paper we adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives n rows of a large matrix A ε ℜ n x m one after the other in a streaming fashion. It maintains a sketch B ℜ l x m containing only l n rows but still guarantees that ATA BTB. More accurately, ∀x || x,||=1 0≤||Ax||2 - ||Bx||2 ≤ 2||A||_f 2 l Or BTB prec ATA and ||ATA - BTB|| ≤ 2 ||A||f2 l. This gives a streaming algorithm whose error decays proportional to 1/l using O(ml) space. For comparison, random-projection, hashing or sampling based algorithms produce convergence bounds proportional to 1/√l. Sketch updates per row in A require amortized O(ml) operations and the algorithm is perfectly parallelizable. Our experiments corroborate the algorithm's scalability and improved convergence rate. The presented algorithm also stands out in that it is deterministic, simple to implement and elementary to prove.

#index 1991824
#* Discovering latent influence in online social activities via shared cascade poisson processes
#@ Tomoharu Iwata;Amar Shah;Zoubin Ghahramani
#t 2013
#c 0
#% 342596
#% 577217
#% 722904
#% 754107
#% 879628
#% 949164
#% 956512
#% 1083641
#% 1092318
#% 1219220
#% 1287276
#% 1333069
#% 1429433
#% 1535333
#% 1743316
#% 1872232
#% 1948125
#! Many people share their activities with others through online communities. These shared activities have an impact on other users' activities. For example, users are likely to become interested in items that are adopted (e.g. liked, bought and shared) by their friends. In this paper, we propose a probabilistic model for discovering latent influence from sequences of item adoption events. An inhomogeneous Poisson process is used for modeling a sequence, in which adoption by a user triggers the subsequent adoption of the same item by other users. For modeling adoption of multiple items, we employ multiple inhomogeneous Poisson processes, which share parameters, such as influence for each user and relations between users. The proposed model can be used for finding influential users, discovering relations between users and predicting item popularity in the future. We present an efficient Bayesian inference procedure of the proposed model based on the stochastic EM algorithm. The effectiveness of the proposed model is demonstrated by using real data sets in a social bookmark sharing service.

#index 1991825
#* Scalable all-pairs similarity search in metric spaces
#@ Ye Wang;Ahmed Metwally;Srinivasan Parthasarathy
#t 2013
#c 0
#% 221298
#% 325683
#% 754117
#% 765463
#% 823403
#% 824711
#% 864392
#% 864398
#% 879600
#% 956506
#% 956518
#% 991230
#% 1013086
#% 1023420
#% 1054481
#% 1055684
#% 1215321
#% 1357698
#% 1426543
#% 1535356
#% 1581925
#% 1605990
#% 1707459
#% 1769264
#% 1770396
#% 1872261
#% 1948143
#! Given a set of entities, the all-pairs similarity search aims at identifying all pairs of entities that have similarity greater than (or distance smaller than) some user-defined threshold. In this article, we propose a parallel framework for solving this problem in metric spaces. Novel elements of our solution include: i) flexible support for multiple metrics of interest; ii) an autonomic approach to partition the input dataset with minimal redundancy to achieve good load-balance in the presence of limited computing resources; iii) an on-the- fly lossless compression strategy to reduce both the running time and the final output size. We validate the utility, scalability and the effectiveness of the approach on hundreds of machines using real and synthetic datasets.

#index 1991826
#* Indexed block coordinate descent for large-scale linear classification with limited memory
#@ Ian En-Hsu Yen;Chun-Fu Chang;Ting-Wei Lin;Shan-Wei Lin;Shou-De Lin
#t 2013
#c 0
#% 131165
#% 450263
#% 576520
#% 592073
#% 734919
#% 875970
#% 881477
#% 983905
#% 1022281
#% 1073923
#% 1077165
#% 1083126
#% 1117691
#% 1318699
#% 1451223
#% 1605991
#% 1872342
#! Linear Classification has achieved complexity linear to the data size. However, in many applications, data contain large amount of samples that does not help improve the quality of model, but still cost much I/O and memory to process. In this paper, we show how a Block Coordinate Descent method based on Nearest-Neighbor Index can significantly reduce such cost when learning a dual-sparse model. In particular, we employ truncated loss function to induce a series of convex programs with superior dual sparsity, and solve each dual using Indexed Block Coordinate Descent, which makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrelevant samples. We prove that, despite the bias and weak guarantee from ANN query, the proposed algorithm has global convergence to the solution defined on entire dataset, with sublinear complexity each iteration. Experiments in both sufficient and limited memory conditions show that the proposed approach learns many times faster than other state-of-the-art solvers without sacrificing accuracy.

#index 1991827
#* Active learning and search on low-rank matrices
#@ Dougal J. Sutherland;Barnabás Póczos;Jeff Schneider
#t 2013
#c 0
#% 722797
#% 788069
#% 840924
#% 1073982
#% 1273828
#% 1535449
#% 1541728
#% 1659463
#% 1672989
#% 1673052
#% 1872268
#% 1893817
#% 1893869
#! Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Low-rank matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. Using variational approximations or Markov chain Monte Carlo sampling to estimate the posterior distribution over models, we can choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many "positive" data points as possible. We evaluate our methods on simulated data, and also show their applicability to movie ratings prediction and the discovery of drug-target interactions.

#index 1991828
#* Massively parallel expectation maximization using graphics processing units
#@ Muzaffer Can Altinigneli;Claudia Plant;Christian Böhm
#t 2013
#c 0
#% 269195
#% 1023380
#% 1236646
#% 1270717
#% 1589296
#% 1606054
#% 1756872
#% 1775742
#% 1862161
#! Composed of several hundreds of processors, the Graphics Processing Unit (GPU) has become a very interesting platform for computationally demanding tasks on massive data. A special hierarchy of processors and fast memory units allow very powerful and efficient parallelization but also demands novel parallel algorithms. Expectation Maximization (EM) is a widely used technique for maximum likelihood estimation. In this paper, we propose an innovative EM clustering algorithm particularly suited for the GPU platform on NVIDIA's Fermi architecture. The central idea of our algorithm is to allow the parallel threads exchanging their local information in an asynchronous way and thus updating their cluster representatives on demand by a technique called Asynchronous Model Updates (Async-EM). Async-EM enables our algorithm not only to accelerate convergence but also to reduce the overhead induced by memory bandwidth limitations and synchronization requirements. We demonstrate (1) how to reformulate the EM algorithm to be able to exchange information using Async-EM and (2) how to exploit the special memory and processor architecture of a modern GPU in order to share this information among threads in an optimal way. As a perspective Async-EM is not limited to EM but can be applied to a variety of algorithms.

#index 1991829
#* Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms
#@ Chris Thornton;Frank Hutter;Holger H. Hoos;Kevin Leyton-Brown
#t 2013
#c 0
#% 422182
#% 425056
#% 429833
#% 431102
#% 466722
#% 718510
#% 856942
#% 961223
#% 1091776
#% 1248573
#% 1290045
#% 1301004
#% 1369579
#% 1386122
#% 1453676
#% 1738047
#% 1747267
#% 1887935
#% 1943421
#! Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.

#index 1991830
#* Direct optimization of ranking measures for learning to rank models
#@ Ming Tan;Tian Xia;Lily Guo;Shaojun Wang
#t 2013
#c 0
#% 734915
#% 757953
#% 840846
#% 976952
#% 983820
#% 987226
#% 987240
#% 987241
#% 1035577
#% 1039843
#% 1074021
#% 1074064
#% 1083633
#% 1272396
#% 1292549
#% 1385985
#% 1442575
#% 1442577
#% 1456843
#% 1456844
#% 1560393
#% 1765231
#! We present a novel learning algorithm, DirectRank, which directly and exactly optimizes ranking measures without resorting to any upper bounds or approximations. Our approach is essentially an iterative coordinate ascent method. In each iteration, we choose one coordinate and only update the corresponding parameter, with all others remaining fixed. Since the ranking measure is a stepwise function of a single parameter, we propose a novel line search algorithm that can locate the interval with the best ranking measure along this coordinate quite efficiently. In order to stabilize our system in small datasets, we construct a probabilistic framework for document-query pairs to maximize the likelihood of the objective permutation of top-$\tau$ documents. This iterative procedure ensures convergence. Furthermore, we integrate regression trees as our weak learners in order to consider the correlation between the different features. Experiments on LETOR datasets and two large datasets, Yahoo challenge data and Microsoft 30K web data, show an improvement over state-of-the-art systems.

#index 1991831
#* A phrase mining framework for recursive construction of a topical hierarchy
#@ Chi Wang;Marina Danilevsky;Nihit Desai;Yinan Zhang;Phuong Nguyen;Thrivikrama Taula;Jiawei Han
#t 2013
#c 0
#% 481290
#% 534271
#% 722904
#% 729418
#% 771571
#% 783483
#% 855200
#% 876017
#% 983883
#% 989620
#% 1026937
#% 1083745
#% 1117083
#% 1190121
#% 1337374
#% 1481571
#% 1567974
#% 1578116
#% 1591967
#% 1826362
#% 1866572
#% 1872404
#% 1919944
#% 1978715
#! A high quality hierarchical organization of the concepts in a dataset at different levels of granularity has many valuable applications such as search, summarization, and content browsing. In this paper we propose an algorithm for recursively constructing a hierarchy of topics from a collection of content-representative documents. We characterize each topic in the hierarchy by an integrated ranked list of mixed-length phrases. Our mining framework is based on a phrase-centric view for clustering, extracting, and ranking topical phrases. Experiments with datasets from three different domains illustrate our ability to generate hierarchies of high quality topics represented by meaningful phrases.

#index 1991832
#* Multi-space probabilistic sequence modeling
#@ Shuo Chen;Jiexun Xu;Thorsten Joachims
#t 2013
#c 0
#% 274612
#% 798967
#% 983884
#% 1014679
#% 1055685
#% 1400014
#% 1417104
#% 1476454
#% 1747263
#% 1872315
#% 1913335
#! Learning algorithms that embed objects into Euclidean space have become the methods of choice for a wide range of problems, ranging from recommendation and image search to playlist prediction and language modeling. Probabilistic embedding methods provide elegant approaches to these problems, but can be expensive to train and store as a large monolithic model. In this paper, we propose a method that trains not one monolithic model, but multiple local embeddings for a class of pairwise conditional models especially suited for sequence and co-occurrence modeling. We show that computation and memory for training these multi-space models can be efficiently parallelized over many nodes of a cluster. Focusing on sequence modeling for music playlists, we show that the method substantially speeds up training while maintaining high model quality.

#index 1991833
#* DTW-D: time series semi-supervised learning from a single example
#@ Yanping Chen;Bing Hu;Eamonn Keogh;Gustavo E.A.P.A Batista
#t 2013
#c 0
#% 375017
#% 792603
#% 814194
#% 876074
#% 881545
#% 893161
#% 940010
#% 1073971
#% 1127609
#% 1279298
#% 1314744
#% 1497106
#% 1544141
#% 1606002
#% 1826290
#% 1987095
#! Classification of time series data is an important problem with applications in virtually every scientific endeavor. The large research community working on time series classification has typically used the UCR Archive to test their algorithms. In this work we argue that the availability of this resource has isolated much of the research community from the following reality, labeled time series data is often very difficult to obtain. The obvious solution to this problem is the application of semi-supervised learning; however, as we shall show, direct applications of off-the-shelf semi-supervised learning algorithms do not typically work well for time series. In this work we explain why semi-supervised learning algorithms typically fail for time series problems, and we introduce a simple but very effective fix. We demonstrate our ideas on diverse real word problems.

#index 1991834
#* Towards never-ending learning from time series streams
#@ Yuan Hao;Yanping Chen;Jesin Zakaria;Bing Hu;Thanawin Rakthanmanon;Eamonn Keogh
#t 2013
#c 0
#% 375017
#% 765440
#% 844424
#% 883878
#% 1083647
#% 1127609
#% 1366462
#% 1606002
#% 1606057
#% 1681284
#% 1897300
#! Time series classification has been an active area of research in the data mining community for over a decade, and significant progress has been made in the tractability and accuracy of learning. However, virtually all work assumes a one-time training session in which labeled examples of all the concepts to be learned are provided. This assumption may be valid in a handful of situations, but it does not hold in most medical and scientific applications where we initially may have only the vaguest understanding of what concepts can be learned. Based on this observation, we propose a never-ending learning framework for time series in which an agent examines an unbounded stream of data and occasionally asks a teacher (which may be a human or an algorithm) for a label. We demonstrate the utility of our ideas with experiments in domains as diverse as medicine, entomology, wildlife monitoring, and human behavior analyses.

#index 1991835
#* Constrained stochastic gradient descent for large-scale least squares problem
#@ Yang Mu;Wei Ding;Tianyi Zhou;Dacheng Tao
#t 2013
#c 0
#% 116794
#% 217580
#% 292664
#% 576218
#% 770754
#% 891559
#% 1073988
#% 1232034
#% 1299294
#% 1551187
#% 1606391
#% 1814206
#! The least squares problem is one of the most important regression problems in statistics, machine learning and data mining. In this paper, we present the Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by imposing a provable constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound $O(\log{T})$, and fastest convergence speed among all first order approaches. Empirical studies justify the effectiveness of CSGD by comparing it with SGD and other state-of-the-art approaches. An example is also given to show how to use CSGD to optimize SGD based least squares problems to achieve a better performance.

#index 1991836
#* Diversity maximization under matroid constraints
#@ Zeinab Abbassi;Vahab S. Mirrokni;Mayur Thakur
#t 2013
#c 0
#% 256685
#% 262112
#% 325409
#% 347225
#% 754124
#% 860672
#% 864456
#% 879686
#% 881500
#% 960287
#% 1055701
#% 1190093
#% 1206662
#% 1207001
#% 1214650
#% 1399955
#% 1400011
#% 1400021
#% 1560386
#% 1581911
#% 1594636
#% 1693888
#% 1845364
#% 1880433
#! Aggregator websites typically present documents in the form of representative clusters. In order for users to get a broader perspective, it is important to deliver a diversified set of representative documents in those clusters. One approach to diversification is to maximize the average dissimilarity among documents. Another way to capture diversity is to avoid showing several documents from the same category (e.g. from the same news channel). We combine the above two diversification concepts by modeling the latter approach as a (partition) matroid constraint, and study diversity maximization problems under matroid constraints. We present the first constant-factor approximation algorithm for this problem, using a new technique. Our local search 0.5-approximation algorithm is also the first constant-factor approximation for the max-dispersion problem under matroid constraints. Our combinatorial proof technique for maximizing diversity under matroid constraints uses the existence of a family of Latin squares which may also be of independent interest. In order to apply these diversity maximization algorithms in the context of aggregator websites and as a preprocessing step for our diversity maximization tool, we develop greedy clustering algorithms that maximize weighted coverage of a predefined set of topics. Our algorithms are based on computing a set of cluster centers, where clusters are formed around them. We show the better performance of our algorithms for diversity and coverage maximization by running experiments on real (Twitter) and synthetic data in the context of real-time search over micro-posts. Finally we perform a user study validating our algorithms and diversity metrics.

#index 1991837
#* Succinct interval-splitting tree for scalable similarity search of compound-protein pairs with property constraints
#@ Yasuo Tabei;Akihiro Kishimoto;Masaaki Kotera;Yoshihiro Yamanishi
#t 2013
#c 0
#% 249238
#% 379390
#% 453572
#% 905703
#% 1153123
#% 1206665
#% 1213000
#% 1267118
#% 1400000
#% 1412873
#% 1486247
#% 1573188
#% 1590535
#% 1933514
#! Analyzing functional interactions between small compounds and proteins is indispensable in genomic drug discovery. Since rich information on various compound-protein inter- actions is available in recent molecular databases, strong demands for making best use of such databases require to in- vent powerful methods to help us find new functional compound-protein pairs on a large scale. We present the succinct interval-splitting tree algorithm (SITA) that efficiently per- forms similarity search in databases for compound-protein pairs with respect to both binary fingerprints and real-valued properties. SITA achieves both time and space efficiency by developing the data structure called interval-splitting trees, which enables to efficiently prune the useless portions of search space, and by incorporating the ideas behind wavelet tree, a succinct data structure to compactly represent trees. We experimentally test SITA on the ability to retrieve similar compound-protein pairs/substrate-product pairs for a query from large databases with over 200 million compound- protein pairs/substrate-product pairs and show that SITA performs better than other possible approaches.

#index 1991838
#* Making recommendations from multiple domains
#@ Wei Chen;Wynne Hsu;Mong Li Lee
#t 2013
#c 0
#% 173879
#% 316143
#% 734594
#% 818216
#% 842605
#% 1083671
#% 1211767
#% 1214661
#% 1214694
#% 1327635
#% 1400001
#% 1536533
#% 1598436
#% 1607077
#% 1826431
#% 1826483
#% 1872386
#! Given the vast amount of information on the World Wide Web, recommender systems are increasingly being used to help filter irrelevant data and suggest information that would interest users. Traditional systems make recommendations based on a single domain e.g., movie or book domain. Recent work has examined the correlations in different domains and designed models that exploit user preferences on a source domain to predict user preferences on a target domain. However, these methods are based on matrix factorization and can only be applied to two-dimensional data. Transferring high dimensional data from one domain to another requires decomposing the high dimensional data to binary relations which results in information loss. Furthermore, this decomposition creates a large number of matrices that need to be transferred and combining them in the target domain is non-trivial. Separately, researchers have looked into using social network information to improve recommendation. However, this social network information has not been explored in cross domain collaborative filtering. In this work, we propose a generalized cross domain collaborative filtering framework that integrates social network information seamlessly with cross domain data. This is achieved by utilizing tensor factorization with topic based social regularization. This framework is able to transfer high dimensional data without the need for decomposition by finding shared implicit cluster-level tensor from multiple domains. Extensive experiments conducted on real world datasets indicate that the proposed framework outperforms state-of-art algorithms for item recommendation, user recommendation and tag recommendation.

#index 1991839
#* Cascading outbreak prediction in networks: a data-driven approach
#@ Peng Cui;Shifei Jin;Linyun Yu;Fei Wang;Wenwu Zhu;Shiqiang Yang
#t 2013
#c 0
#% 342596
#% 389155
#% 722942
#% 729923
#% 794513
#% 797693
#% 814023
#% 949164
#% 989613
#% 1184909
#% 1214641
#% 1214671
#% 1260274
#% 1536522
#% 1605928
#% 1617261
#% 1646511
#% 1872229
#% 1872232
#% 1918359
#% 1919907
#% 1948125
#% 1978799
#! Cascades are ubiquitous in various network environments such as epidemic networks, traffic networks, water distribution networks and social networks. The outbreaks of cascades will often bring bad or even devastating effects. How to accurately predict the cascading outbreaks in early stage is of paramount importance for people to avoid these bad effects. Although there have been some pioneering works on cascading outbreaks detection, how to predict, rather than detect, the cascading outbreaks is still an open problem. In this paper, we attempt harnessing historical cascade data, propose a novel data driven approach to select important nodes as sensors, and predict the outbreaks based on the cascading behaviors of these sensors. In particular, we propose Orthogonal Sparse LOgistic Regression (OSLOR) method to jointly optimize node selection and outbreak prediction, where the prediction loss are combined with an orthogonal regularizer and L1 regularizer to guarantee good prediction accuracy, as well as the sparsity and low-redundancy of selected sensors. We evaluate the proposed method on a real online social network dataset including 182.7 million information cascades. The experimental results show that the proposed OSLOR significantly and consistently outperform topological measure based method and other data driven methods in prediction performances.

#index 1991840
#* Social influence based clustering of heterogeneous information networks
#@ Yang Zhou;Ling Liu
#t 2013
#c 0
#% 313959
#% 342596
#% 729923
#% 989636
#% 989654
#% 1063512
#% 1130830
#% 1181261
#% 1214695
#% 1214714
#% 1289267
#% 1328169
#% 1451162
#% 1523857
#% 1535346
#% 1606073
#% 1673591
#% 1688510
#% 1707456
#% 1770361
#% 1872232
#% 1872391
#% 1872412
#! Social networks continue to grow in size and the type of information hosted. We witness a growing interest in clustering a social network of people based on both their social relationships and their participations in activity based information networks. In this paper, we present a social influence based clustering framework for analyzing heterogeneous information networks with three unique features. First, we introduce a novel social influence based vertex similarity metric in terms of both self-influence similarity and co-influence similarity. We compute self-influence and co-influence based similarity based on social graph and its associated activity graphs and influence graphs respectively. Second, we compute the combined social influence based similarity between each pair of vertices by unifying the self-similarity and multiple co-influence similarity scores through a weight function with an iterative update method. Third, we design an iterative learning algorithm, SI-Cluster, to dynamically refine the K clusters by continuously quantifying and adjusting the weights on self-influence similarity and on multiple co-influence similarity scores towards the clustering convergence. To make SI-Cluster converge fast, we transformed a sophisticated nonlinear fractional programming problem of multiple weights into a straightforward nonlinear parametric programming problem of single variable. Our experiment results show that SI-Cluster not only achieves a better balance between self-influence and co-influence similarities but also scales extremely well for large graph clustering.

#index 1991841
#* Selective sampling on graphs for classification
#@ Quanquan Gu;Charu Aggarwal;Jialu Liu;Jiawei Han
#t 2013
#c 0
#% 170649
#% 224113
#% 236729
#% 451055
#% 466887
#% 757953
#% 801566
#% 840873
#% 871302
#% 961177
#% 1083628
#% 1211705
#% 1495579
#% 1745126
#% 1978818
#! Selective sampling is an active variant of online learning in which the learner is allowed to adaptively query the label of an observed example. The goal of selective sampling is to achieve a good trade-off between prediction performance and the number of queried labels. Existing selective sampling algorithms are designed for vector-based data. In this paper, motivated by the ubiquity of graph representations in real-world applications, we propose to study selective sampling on graphs. We first present an online version of the well-known Learning with Local and Global Consistency method (OLLGC). It is essentially a second-order online learning algorithm, and can be seen as an online ridge regression in the Hilbert space of functions defined on graphs. We prove its regret bound in terms of the structural property (cut size) of a graph. Based on OLLGC, we present a selective sampling algorithm, namely Selective Sampling with Local and Global Consistency (SSLGC), which queries the label of each node based on the confidence of the linear function on graphs. Its bound on the label complexity is also derived. We analyze the low-rank approximation of graph kernels, which enables the online algorithms scale to large graphs. Experiments on benchmark graph datasets show that OLLGC outperforms the state-of-the-art first-order algorithm significantly, and SSLGC achieves comparable or even better results than OLLGC while querying substantially fewer nodes. Moreover, SSLGC is overwhelmingly better than random sampling.

#index 1991842
#* WiseMarket: a new paradigm for managing wisdom of online social users
#@ Caleb Chen Cao;Yongxin Tong;Lei Chen;H. V. Jagadish
#t 2013
#c 0
#% 1343306
#% 1400018
#% 1425621
#% 1560406
#% 1573506
#% 1581851
#% 1628171
#% 1711868
#% 1746896
#% 1826300
#% 1880463
#% 1880464
#% 1962368
#! The benefits of crowdsourcing are well-recognized today for an increasingly broad range of problems. Meanwhile, the rapid development of social media makes it possible to seek the wisdom of a crowd of targeted users. However, it is not trivial to implement the crowdsourcing platform on social media, specifically to make social media users as workers, we need to address the following two challenges: 1) how to motivate users to participate in tasks, and 2) how to choose users for a task. In this paper, we present Wise Market as an effective framework for crowdsourcing on social media that motivates users to participate in a task with care and correctly aggregates their opinions on pairwise choice problems. The Wise Market consists of a set of investors each with an associated individual confidence in his/her prediction, and after the investment, only the ones whose choices are the same as the whole market are granted rewards. Therefore, a social media user has to give his/her ``best'' answer in order to get rewards, as a consequence, careless answers from sloppy users are discouraged. Under the Wise Market framework, we define an optimization problem to minimize expected cost of paying out rewards while guaranteeing a minimum confidence level, called the Effective Market Problem (EMP). We propose exact algorithms for calculating the market confidence and the expected cost with O(nlog2n) time cost in a Wise Market with n investors. To deal with the enormous number of users on social media, we design a Central Limit Theorem-based approximation algorithm to compute the market confidence with O(n) time cost, as well as a bounded approximation algorithm to calculate the expected cost with O(n) time cost. Finally, we have conducted extensive experiments to validate effectiveness of the proposed algorithms on real and synthetic data.

#index 1991843
#* Querying discriminative and representative samples for batch mode active learning
#@ Zheng Wang;Jieping Ye
#t 2013
#c 0
#% 116165
#% 236729
#% 420077
#% 464268
#% 466576
#% 722797
#% 722909
#% 735256
#% 770771
#% 875997
#% 876080
#% 881506
#% 906248
#% 961139
#% 1083671
#% 1211696
#% 1272282
#% 1387560
#% 1472280
#% 1557618
#% 1558464
#% 1588933
#% 1745124
#% 1747282
#% 1872318
#! Empirical risk minimization (ERM) provides a useful guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires that the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label and these data may not follow the source distribution. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. The proposed formulation involves a non-convex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying the most uncertain and representative queries. Experiments on benchmark data sets and real-world applications demonstrate the superior performance of our proposed method in comparison with the state-of-the-art methods.

#index 1991844
#* Recursive regularization for large-scale classification with hierarchical and graphical dependencies
#@ Siddharth Gopal;Yiming Yang
#t 2013
#c 0
#% 73441
#% 131165
#% 219052
#% 309141
#% 318412
#% 340904
#% 397854
#% 420507
#% 466078
#% 642986
#% 763708
#% 769886
#% 770796
#% 783478
#% 829043
#% 829975
#% 881557
#% 961135
#% 961192
#% 983905
#% 1073923
#% 1074128
#% 1128929
#% 1227578
#% 1450868
#% 1715026
#! The two key challenges in hierarchical classification are to leverage the hierarchical dependencies between the class-labels for improving performance, and, at the same time maintaining scalability across large hierarchies. In this paper we propose a regularization framework for large-scale hierarchical classification that addresses both the problems. Specifically, we incorporate the hierarchical dependencies between the class-labels into the regularization structure of the parameters thereby encouraging classes nearby in the hierarchy to share similar model parameters. Furthermore, we extend our approach to scenarios where the dependencies between the class-labels are encoded in the form of a graph rather than a hierarchy. To enable large-scale training, we develop a parallel-iterative optimization scheme that can handle datasets with hundreds of thousands of classes and millions of instances and learning terabytes of parameters. Our experiments showed a consistent improvement over other competing approaches and achieved state-of-the-art results on benchmark datasets.

#index 1991845
#* Denser than the densest subgraph: extracting optimal quasi-cliques with quality guarantees
#@ Charalampos Tsourakakis;Francesco Bonchi;Aristides Gionis;Francesco Gullo;Maria Tsiarli
#t 2013
#c 0
#% 55327
#% 203148
#% 303305
#% 322619
#% 346788
#% 447718
#% 498852
#% 511151
#% 674497
#% 781868
#% 824711
#% 906292
#% 1023840
#% 1035579
#% 1172001
#% 1173140
#% 1214735
#% 1217208
#% 1232289
#% 1349835
#% 1451234
#% 1730736
#! Finding dense subgraphs is an important graph-mining task with many applications. Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density, research has focused on optimizing alternative density functions. A very popular among such functions is the average degree, whose maximization leads to the well-known densest-subgraph notion. Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density and large diameter. In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter. We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties. To optimize the proposed function we provide an additive approximation algorithm and a local-search heuristic. Both algorithms are very efficient and scale well to large graphs. We evaluate our algorithms on real and synthetic datasets, and we also devise several application studies as variants of our original problem. When compared with the method that finds the subgraph of the largest average degree, our algorithms return denser subgraphs with smaller diameter. Finally, we discuss new interesting research directions that our problem leaves open.

#index 1991846
#* Combining latent factor model with location features for event-based group recommendation
#@ Wei Zhang;Jianyong Wang;Wei Feng
#t 2013
#c 0
#% 280819
#% 722904
#% 1050550
#% 1073982
#% 1083636
#% 1190123
#% 1260273
#% 1268491
#% 1287275
#% 1400036
#% 1417104
#% 1450940
#% 1476457
#% 1598366
#% 1605963
#% 1606045
#% 1693877
#% 1693933
#% 1746798
#% 1853756
#% 1872355
#% 1872401
#% 1879058
#% 1918402
#% 1941002
#! Groups play an essential role in many social websites which promote users' interactions and accelerate the diffusion of information. Recommending groups that users are really interested to join is significant for both users and social media. While traditional group recommendation problem has been extensively studied, we focus on a new type of the problem, i.e., event-based group recommendation. Unlike the other forms of groups, users join this type of groups mainly for participating offline events organized by group members or inviting other users to attend events sponsored by them. These characteristics determine that previously proposed approaches for group recommendation cannot be adapted to the new problem easily as they ignore the geographical influence and other explicit features of groups and users. In this paper, we propose a method called Pairwise Tag enhAnced and featuRe-based Matrix factorIzation for Group recommendAtioN (PTARMIGAN), which considers location features, social features, and implicit patterns simultaneously in a unified model. More specifically, we exploit matrix factorization to model interactions between users and groups. Meanwhile, we incorporate their profile information into pairwise enhanced latent factors respectively. We also utilize the linear model to capture explicit features. Due to the reinforcement between explicit features and implicit patterns, our approach can provide better group recommendations. We conducted a comprehensive performance evaluation on real word data sets and the experimental results demonstrate the effectiveness of our method.

#index 1991847
#* Cost-sensitive online active learning with application to malicious URL detection
#@ Peilin Zhao;Steven C.H. Hoi
#t 2013
#c 0
#% 236729
#% 240794
#% 302390
#% 464612
#% 722797
#% 722814
#% 722903
#% 838469
#% 869471
#% 875997
#% 956558
#% 961152
#% 961177
#% 1084472
#% 1164781
#% 1174739
#% 1190112
#% 1190189
#% 1211775
#% 1214746
#% 1232017
#% 1396658
#% 1558467
#% 1591695
#% 1606374
#% 1659418
#% 1815223
#% 1815581
#% 1978752
#! Malicious Uniform Resource Locator (URL) detection is an important problem in web search and mining, which plays a critical role in internet security. In literature, many existing studies have attempted to formulate the problem as a regular supervised binary classification task, which typically aims to optimize the prediction accuracy. However, in a real-world malicious URL detection task, the ratio between the number of malicious URLs and legitimate URLs is highly imbalanced, making it very inappropriate for simply optimizing the prediction accuracy. Besides, another key limitation of the existing work is to assume a large amount of training data is available, which is impractical as the human labeling cost could be potentially quite expensive. To solve these issues, in this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL), which only queries a small fraction of training data for labeling and directly optimizes two cost-sensitive measures to address the class-imbalance issue. In particular, we propose two CSOAL algorithms and analyze their theoretical performance in terms of cost-sensitive bounds. We conduct an extensive set of experiments to examine the empirical performance of the proposed algorithms for a large-scale challenging malicious URL detection task, in which the encouraging results showed that the proposed technique by querying an extremely small-sized labeled data (about 0.5% out of 1-million instances) can achieve better or highly comparable classification performance in comparison to the state-of-the-art cost-insensitive and cost-sensitive online classification algorithms using a huge amount of labeled data.

#index 1991848
#* Connecting users across social media sites: a behavioral-modeling approach
#@ Reza Zafarani;Huan Liu
#t 2013
#c 0
#% 428405
#% 748738
#% 852013
#% 878207
#% 956511
#% 1080356
#% 1190110
#% 1615634
#% 1810980
#% 1815525
#! People use various social media for different purposes. The information on an individual site is often incomplete. When sources of complementary information are integrated, a better profile of a user can be built to improve online services such as verifying online information. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We introduce a methodology (MOBIUS) for finding a mapping among identities of individuals across social media sites. It consists of three key components: the first component identifies users' unique behavioral patterns that lead to information redundancies across sites; the second component constructs features that exploit information redundancies due to these behavioral patterns; and the third component employs machine learning for effective user identification. We formally define the cross-media user identification problem and show that MOBIUS is effective in identifying users across social media sites. This study paves the way for analysis and mining across social media sites, and facilitates the creation of novel online services across sites.

#index 1991849
#* The bang for the buck: fair competitive viral marketing from the host perspective
#@ Wei Lu;Francesco Bonchi;Amit Goyal;Laks V.S. Lakshmanan
#t 2013
#c 0
#% 408396
#% 729923
#% 989613
#% 990216
#% 1102550
#% 1355040
#% 1407359
#% 1451243
#% 1535380
#% 1535434
#% 1540249
#% 1560421
#% 1628176
#% 1688456
#! The key algorithmic problem in viral marketing is to identify a set of influential users (called seeds) in a social network, who, when convinced to adopt a product, shall influence other users in the network, leading to a large number of adoptions. When two or more players compete with similar products on the same network we talk about competitive viral marketing, which so far has been studied exclusively from the perspective of one of the competing players. In this paper we propose and study the novel problem of competitive viral marketing from the perspective of the host, i.e., the owner of the social network platform. The host sells viral marketing campaigns as a service to its customers, keeping control of the selection of seeds. Each company specifies its budget and the host allocates the seeds accordingly. From the host's perspective, it is important not only to choose the seeds to maximize the collective expected spread, but also to assign seeds to companies so that it guarantees the "bang for the buck" for all companies is nearly identical, which we formalize as the fair seed allocation problem. We propose a new propagation model capturing the competitive nature of viral marketing. Our model is intuitive and retains the desired properties of monotonicity and submodularity. We show that the fair seed allocation problem is NP-hard, and develop an efficient algorithm called Needy Greedy. We run experiments on three real-world social networks, showing that our algorithm is effective and scalable.

#index 1991850
#* A general bootstrap performance diagnostic
#@ Ariel Kleiner;Ameet Talwalkar;Sameer Agarwal;Ion Stoica;Michael I. Jordan
#t 2013
#c 0
#% 1869837
#! As datasets become larger, more complex, and more available to diverse groups of analysts, it would be quite useful to be able to automatically and generically assess the quality of estimates, much as we are able to automatically train and evaluate predictive models such as classifiers. However, despite the fundamental importance of estimator quality assessment in data analysis, this task has eluded highly automatic solutions. While the bootstrap provides perhaps the most promising step in this direction, its level of automation is limited by the difficulty of evaluating its finite sample performance and even its asymptotic consistency. Thus, we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap's outputs, determining whether or not the bootstrap is performing satisfactorily when applied to a given dataset and estimator. We show that our proposed diagnostic is effective via an extensive empirical evaluation on a variety of estimators and simulated and real datasets, including a real-world query workload from Conviva, Inc. involving 1.7TB of data (i.e., approximately 0.5 billion data points).

#index 1991851
#* MI2LS: multi-instance learning from multiple informationsources
#@ Dan Zhang;Jingrui He;Richard Lawrence
#t 2013
#c 0
#% 224755
#% 252011
#% 272527
#% 280819
#% 464267
#% 464633
#% 465916
#% 565537
#% 635689
#% 741317
#% 743284
#% 771844
#% 876033
#% 881477
#% 881557
#% 902511
#% 983817
#% 1077150
#% 1119137
#% 1176978
#% 1270208
#% 1274898
#% 1327713
#% 1386108
#% 1563305
#% 1595865
#% 1606063
#% 1653809
#% 1755422
#! In Multiple Instance Learning (MIL), each entity is normally expressed as a set of instances. Most of the current MIL methods only deal with the case when each instance is represented by one type of features. However, in many real world applications, entities are often described from several different information sources/views. For example, when applying MIL to image categorization, the characteristics of each image can be derived from both its RGB features and SIFT features. Previous research work has shown that, in traditional learning methods, leveraging the consistencies between different information sources could improve the classification performance drastically. Out of a similar motivation, to incorporate the consistencies between different information sources into MIL, we propose a novel research framework -- Multi-Instance Learning from Multiple Information Sources (MI2LS). Based on this framework, an algorithm -- Fast MI2LS (FMI2LS) is designed, which combines Concave-Convex Constraint Programming (CCCP) method and an adapte- d Stoachastic Gradient Descent (SGD) method. Some theoretical analysis on the optimality of the adapted SGD method and the generalized error bound of the formulation are given based on the proposed method. Experimental results on document classification and a novel application -- Insider Threat Detection (ITD), clearly demonstrate the superior performance of the proposed method over state-of-the-art MIL methods.

#index 1991852
#* Modeling the dynamics of composite social networks
#@ Erheng Zhong;Wei Fan;Yin Zhu;Qiang Yang
#t 2013
#c 0
#% 730089
#% 1083675
#% 1117695
#% 1211731
#% 1310058
#% 1366213
#% 1399996
#% 1400031
#% 1495568
#% 1605926
#% 1614276
#% 1872313
#! Modeling the dynamics of online social networks over time not only helps us understand the evolution of network structures and user behaviors, but also improves the performance of other analysis tasks, such as link prediction and community detection. Nowadays, users engage in multiple networks and form a "composite social network" by considering common users as the bridge. State-of-the-art network-dynamics analysis is performed in isolation for individual networks, but users' interactions in one network can influence their behaviors in other networks, and in an individual network, different types of user interactions also affect each other. Without considering the influences across networks, one may not be able to model the dynamics in a given network correctly due to the lack of information. In this paper, we study the problem of modeling the dynamics of composite networks, where the evolution processes of different networks are jointly considered. However, due to the difference in network properties, simply merging multiple networks into a single one is not ideal because individual evolution patterns may be ignored and network differences may bring negative impacts. The proposed solution is a nonparametric Bayesian model, which models each user's common latent features to extract the cross-network influences, and use network-specific factors to describe different networks' evolution patterns. Empirical studies on large-scale dynamic composite social networks demonstrate that the proposed approach improves the performance of link prediction over several state-of-the-art baselines and unfolds the network evolution accurately.

#index 1991853
#* Learning to question: leveraging user preferences for shopping advice
#@ Mahashweta Das;Gianmarco De Francisci Morales;Aristides Gionis;Ingmar Weber
#t 2013
#c 0
#% 173879
#% 301590
#% 319705
#% 344112
#% 424007
#% 452563
#% 734915
#% 768133
#% 768664
#% 813966
#% 836876
#% 881477
#% 1272133
#% 1272396
#% 1312124
#% 1541748
#% 1650569
#% 1693875
#! We present ShoppingAdvisor, a novel recommender system that helps users in shopping for technical products. ShoppingAdvisor leverages both user preferences and technical product attributes in order to generate its suggestions. The system elicits user preferences via a tree-shaped flowchart, where each node is a question to the user. At each node, ShoppingAdvisor suggests a ranking of products matching the preferences of the user, and that gets progressively refined along the path from the tree's root to one of its leafs. In this paper we show (i) how to learn the structure of the tree, i.e., which questions to ask at each node, and (ii) how to produce a suitable ranking at each node. First, we adapt the classical top-down strategy for building decision trees in order to find the best user attribute to ask at each node. Differently from decision trees, ShoppingAdvisor partitions the user space rather than the product space. Second, we show how to employ a learning-to-rank approach in order to learn, for each node of the tree, a ranking of products appropriate to the users who reach that node. We experiment with two real-world datasets for cars and cameras, and a synthetic one. We use mean reciprocal rank to evaluate ShoppingAdvisor, and show how the performance increases by more than 50% along the path from root to leaf. We also show how collaborative recommendation algorithms such as k-nearest neighbor benefits from feature selection done by the ShoppingAdvisor tree. Our experiments show that ShoppingAdvisor produces good quality interpretable recommendations, while requiring less input from users and being able to handle the cold-start problem.

#index 1991854
#* Mining high utility episodes in complex event sequences
#@ Cheng-Wei Wu;Yu-Feng Lin;Philip S. Yu;Vincent S. Tseng
#t 2013
#c 0
#% 300120
#% 329537
#% 420063
#% 463903
#% 464996
#% 481290
#% 577256
#% 727894
#% 785410
#% 805093
#% 829993
#% 989612
#% 1015844
#% 1019450
#% 1176880
#% 1254215
#% 1327654
#% 1331657
#% 1390145
#% 1451164
#% 1587711
#% 1605953
#% 1606059
#% 1688439
#% 1872237
#% 1872285
#% 1872309
#% 1918340
#% 1978700
#! Frequent episode mining (FEM) is an interesting research topic in data mining with wide range of applications. However, the traditional framework of FEM treats all events as having the same importance/utility and assumes that a same type of event appears at most once at any time point. These simplifying assumptions do not reflect the characteristics of scenarios in real applications and thus the useful information of episodes in terms of utilities such as profits is lost. Furthermore, most studies on FEM focused on mining episodes in simple event sequences and few considered the scenario of complex event sequences, where different events can occur simultaneously. To address these issues, in this paper, we incorporate the concept of utility into episode mining and address a new problem of mining high utility episodes from complex event sequences, which has not been explored so far. In the proposed framework, the importance/utility of different events is considered and multiple events can appear simultaneously. Several novel features are incorporated into the proposed framework to resolve the challenges raised by this new problem, such as the absence of anti-monotone property and the huge set of candidate episodes. Moreover, an efficient algorithm named UP-Span (Utility ePisodes mining by Spanning prefixes) is proposed for mining high utility episodes with several strategies incorporated for pruning the search space to achieve high efficiency. Experimental results on real and synthetic datasets show that UP-Span has excellent performance and serves as an effective solution to the new problem of mining high utility episodes from complex event sequences.

#index 1991855
#* A time-dependent enhanced support vector machine for time series regression
#@ Goce Ristanoski;Wei Liu;James Bailey
#t 2013
#c 0
#% 435290
#% 494955
#% 527499
#% 889089
#% 907582
#% 987218
#% 1066734
#% 1248081
#% 1386108
#% 1403607
#% 1451249
#% 1710594
#% 1861267
#! Support Vector Machines (SVMs) are a leading tool in machine learning and have been used with considerable success for the task of time series forecasting. However, a key challenge when using SVMs for time series is the question of how to deeply integrate time elements into the learning process. To address this challenge, we investigated the distribution of errors in the forecasts delivered by standard SVMs. Once we identified the samples that produced the largest errors, we observed their correlation with distribution shifts that occur in the time series. This motivated us to propose a time-dependent loss function which allows the inclusion of the information about the distribution shifts in the series directly into the SVM learning process. We present experimental results which indicate that using a time-dependent loss function is highly promising, reducing the overall variance of the errors, as well as delivering more accurate predictions.

#index 1991856
#* A new collaborative filtering approach for increasing the aggregate diversity of recommender systems
#@ Katja Niemann;Martin Wolpers
#t 2013
#c 0
#% 342687
#% 414514
#% 734590
#% 740900
#% 805841
#% 813966
#% 860672
#% 867053
#% 918842
#% 963350
#% 1077150
#% 1127451
#% 1127465
#% 1127481
#% 1287293
#% 1355036
#% 1411849
#% 1605855
#% 1625357
#% 1625387
#% 1755308
#! In order to satisfy and positively surprise the users, a recommender system needs to recommend items the users will like and most probably would not have found on their own. This requires the recommender system to recommend a broader range of items including niche items as well. Such an approach also support online-stores that often offer more items than traditional stores and need recommender systems to enable users to find the not so popular items as well. However, popular items that hold a lot of usage data are more easy to recommend and, thus, niche items are often excluded from the recommendations. In this paper, we propose a new collaborative filtering approach that is based on the items' usage contexts. The approach increases the rating predictions for niche items with fewer usage data available and improves the aggragate diversity of the recommendations.

#index 1991857
#* STRIP: stream learning of influence probabilities
#@ Konstantin Kutzkov;Albert Bifet;Francesco Bonchi;Aristides Gionis
#t 2013
#c 0
#% 125830
#% 190611
#% 311808
#% 342596
#% 414993
#% 443393
#% 519953
#% 729923
#% 751684
#% 823342
#% 989613
#% 1107420
#% 1355040
#% 1373450
#% 1451243
#% 1549697
#% 1628176
#% 1776166
#! Influence-driven diffusion of information is a fundamental process in social networks. Learning the latent variables of such process, i.e., the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cascades, and developing applications such as viral marketing. Motivated by modern microblogging platforms, such as twitter, in this paper we study the problem of learning influence probabilities in a data-stream scenario, in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweets using a small amount of time and memory. Our contribution is a number of randomized approximation algorithms, categorized according to the available space (superlinear, linear, and sublinear in the number of nodes n) and according to different models (landmark and sliding window). Among several results, we show that we can learn influence probabilities with one pass over the data, using O(nlog n) space, in both the landmark model and the sliding-window model, and we further show that our algorithm is within a logarithmic factor of optimal. For truly large graphs, when one needs to operate with sublinear space, we show that we can still learn influence probabilities in one pass, assuming that we restrict our attention to the most active users. Our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithms agrees with that predicted by the theory.

#index 1991858
#* Scalable inference in max-margin topic models
#@ Jun Zhu;Xun Zheng;Li Zhou;Bo Zhang
#t 2013
#c 0
#% 431293
#% 722904
#% 763699
#% 763708
#% 916788
#% 1083687
#% 1211734
#% 1385969
#% 1523858
#% 1693873
#% 2004118
#! Topic models have played a pivotal role in analyzing large collections of complex data. Besides discovering latent semantics, supervised topic models (STMs) can make predictions on unseen test data. By marrying with advanced learning techniques, the predictive strengths of STMs have been dramatically enhanced, such as max-margin supervised topic models, state-of-the-art methods that integrate max-margin learning with topic models. Though powerful, max-margin STMs have a hard non-smooth learning problem. Existing algorithms rely on solving multiple latent SVM subproblems in an EM-type procedure, which can be too slow to be applicable to large-scale categorization tasks. In this paper, we present a highly scalable approach to building max-margin supervised topic models. Our approach builds on three key innovations: 1) a new formulation of Gibbs max-margin supervised topic models for both multi-class and multi-label classification; 2) a simple ``augment-and-collapse" Gibbs sampling algorithm without making restricting assumptions on the posterior distributions; 3) an efficient parallel implementation that can easily tackle data sets with hundreds of categories and millions of documents. Furthermore, our algorithm does not need to solve SVM subproblems. Though performing the two tasks of topic discovery and learning predictive models jointly, which significantly improves the classification performance, our methods have comparable scalability as the state-of-the-art parallel algorithms for the standard LDA topic models which perform the single task of topic discovery only. Finally, an open-source implementation is also provided at: http://www.ml-thu.net/~jun/medlda.

#index 1991859
#* Automatic selection of social media responses to news
#@ Tadej Štajner;Bart Thomee;Ana-Maria Popescu;Marco Pennacchiotti;Alejandro Jaimes
#t 2013
#c 0
#% 768632
#% 967452
#% 1035587
#% 1190088
#% 1292502
#% 1355297
#% 1399966
#% 1399992
#% 1482457
#% 1512403
#% 1530857
#% 1536509
#% 1545596
#% 1563712
#% 1598359
#% 1606437
#% 1711785
#% 1729391
#% 1746840
#% 1746885
#! Social media responses to news have increasingly gained in importance as they can enhance a consumer's news reading experience, promote information sharing and aid journalists in assessing their readership's response to a story. Given that the number of responses to an online news article may be huge, a common challenge is that of selecting only the most interesting responses for display. This paper addresses this challenge by casting message selection as an optimization problem. We define an objective function which jointly models the messages' utility scores and their entropy. We propose a near-optimal solution to the underlying optimization problem, which leverages the submodularity property of the objective function. Our solution first learns the utility of individual messages in isolation and then produces a diverse selection of interesting messages by maximizing the defined objective function. The intuitions behind our work are that an interesting selection of messages contains diverse, informative, opinionated and popular messages referring to the news article, written mostly by users that have authority on the topic. Our intuitions are embodied by a rich set of content, social and user features capturing the aforementioned aspects. We evaluate our approach through both human and automatic experiments, and demonstrate it outperforms the state of the art. Additionally, we perform an in-depth analysis of the annotated ``interesting'' responses, shedding light on the subjectivity around the selection process and the perception of interestingness.

#index 1991860
#* A data-driven method for in-game decision making in MLB: when to pull a starting pitcher
#@ Ganeshapillai Gartheeban;John Guttag
#t 2013
#c 0
#% 769886
#% 1116993
#! Professional sports is a roughly $500 billion dollar industry that is increasingly data-driven. In this paper we show how machine learning can be applied to generate a model that could lead to better on-field decisions by managers of professional baseball teams. Specifically we show how to use regularized linear regression to learn pitcher-specific predictive models that can be used to help decide when a starting pitcher should be replaced. A key step in the process is our method of converting categorical variables (e.g., the venue in which a game is played) into continuous variables suitable for the regression. Another key step is dealing with situations in which there is an insufficient amount of data to compute measures such as the effectiveness of a pitcher against specific batters. For each season we trained on the first 80% of the games, and tested on the rest. The results suggest that using our model could have led to better decisions than those made by major league managers. Applying our model would have led to a different decision 48% of the time. For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up performing poorly 60% of the time.

#index 1991861
#* Collaborative boosting for activity classification in microblogs
#@ Yangqiu Song;Zhengdong Lu;Cane Wing-ki Leung;Qiang Yang
#t 2013
#c 0
#% 235377
#% 236495
#% 236497
#% 248810
#% 466263
#% 577240
#% 577258
#% 769886
#% 961218
#% 961278
#% 1400018
#% 1451259
#% 1535479
#% 1588252
#% 1606084
#% 1641654
#% 1699613
#% 1872363
#% 1925443
#% 1948178
#! Users' daily activities, such as dining and shopping, inherently reflect their habits, intents and preferences, thus provide invaluable information for services such as personalized information recommendation and targeted advertising. Users' activity information, although ubiquitous on social media, has largely been unexploited. This paper addresses the task of user activity classification in microblogs, where users can publish short messages and maintain social networks online. We identify the importance of modeling a user's individuality, and that of exploiting opinions of the user's friends for accurate activity classification. In this light, we propose a novel collaborative boosting framework comprising a text-to-activity classifier for each user, and a mechanism for collaboration between classifiers of users having social connections. The collaboration between two classifiers includes exchanging their own training instances and their dynamically changing labeling decisions. We propose an iterative learning procedure that is formulated as gradient descent in learning function space, while opinion exchange between classifiers is implemented with a weighted voting in each learning iteration. We show through experiments that on real-world data from Sina Weibo, our method outperforms existing off-the-shelf algorithms that do not take users' individuality or social connections into account.

#index 1991862
#* Exploiting user clicks for automatic seed set generation for entity matching
#@ Xiao Bai;Flavio P. Junqueira;Srinivasan H. Sengamedu
#t 2013
#c 0
#% 162955
#% 240955
#% 342621
#% 350103
#% 410276
#% 729913
#% 769952
#% 905209
#% 915242
#% 915344
#% 987222
#% 989578
#% 1083721
#% 1190162
#% 1314445
#% 1399954
#% 1426513
#% 1495113
#% 1538763
#% 1560179
#% 1563057
#% 1895104
#% 1919802
#! Matching entities from different information sources is a very important problem in data analysis and data integration. It is, however, challenging due to the number and diversity of information sources involved, and the significant editorial efforts required to collect sufficient training data. In this paper, we present an approach that leverages user clicks during Web search to automatically generate training data for entity matching. The key insight of our approach is that Web pages clicked for a given query are likely to be about the same entity. We use random walk with restart to reduce data sparseness, rely on co-clustering to group queries and Web pages, and exploit page similarity to improve matching precision. Experimental results show that: (i) With 360K pages from 6 major travel websites, we obtain 84K matchings (of 179K pages) that refer to the same entities, with an average precision of 0.826; (ii) The quality of matching obtained from a classifier trained on the resulted seed data is promising: the performance matches that of editorial data at small size and improves with size.

#index 1991863
#* Silence is also evidence: interpreting dwell time for recommendation from psychological perspective
#@ Peifeng Yin;Ping Luo;Wang-Chien Lee;Min Wang
#t 2013
#c 0
#% 280852
#% 397155
#% 578684
#% 766454
#% 879567
#% 907516
#% 1000869
#% 1039535
#% 1055676
#% 1074107
#% 1083671
#% 1130901
#% 1176909
#% 1190055
#% 1227602
#% 1260273
#% 1287252
#% 1355025
#% 1450876
#% 1480505
#% 1482349
#% 1535399
#% 1598366
#% 1598396
#% 1625371
#% 1625387
#% 1679166
#% 1719270
#% 1764853
#! Social media is a platform for people to share and vote content. From the analysis of the social media data we found that users are quite inactive in rating/voting. For example, a user on average only votes 2 out of 100 accessed items. Traditional recommendation methods are mostly based on users' votes and thus can not cope with this situation. Based on the observation that the dwell time on an item may reflect the opinion of a user, we aim to enrich the user-vote matrix by converting the dwell time on items into users' ``pseudo votes'' and then help improve recommendation performance. However, it is challenging to correctly interpret the dwell time since many subjective human factors, e.g. user expectation, sensitivity to various item qualities, reading speed, are involved into the casual behavior of online reading. In psychology, it is assumed that people have choice threshold in decision making. The time spent on making decision reflects the decision maker's threshold. This idea inspires us to develop a View-Voting model, which can estimate how much the user likes the viewed item according to her dwell time, and thus make recommendations even if there is no voting data available. Finally, our experimental evaluation shows that the traditional rate-based recommendation's performance is greatly improved with the support of VV model.

#index 1991864
#* Trace complexity of network inference
#@ Bruno Abrahao;Flavio Chierichetti;Robert Kleinberg;Alessandro Panconesi
#t 2013
#c 0
#% 115608
#% 283833
#% 729923
#% 754107
#% 832271
#% 1355041
#% 1451242
#% 1480884
#% 1536509
#% 1800689
#! The network inference problem consists of reconstructing the edge set of a network given traces representing the chronology of infection times as epidemics spread through the network. This problem is a paradigmatic representative of prediction tasks in machine learning that require deducing a latent structure from observed patterns of activity in a network, which often require an unrealistically large number of resources (e.g., amount of available data, or computational time). A fundamental question is to understand which properties we can predict with a reasonable degree of accuracy with the available resources, and which we cannot. We define the trace complexity as the number of distinct traces required to achieve high fidelity in reconstructing the topology of the unobserved network or, more generally, some of its properties. We give algorithms that are competitive with, while being simpler and more efficient than, existing network inference approaches. Moreover, we prove that our algorithms are nearly optimal, by proving an information-theoretic lower bound on the number of traces that an optimal inference algorithm requires for performing this task in the general case. Given these strong lower bounds, we turn our attention to special cases, such as trees and bounded-degree graphs, and to property recovery tasks, such as reconstructing the degree distribution without inferring the network. We show that these problems require a much smaller (and more realistic) number of traces, making them potentially solvable in practice.

#index 1991865
#* Efficient single-source shortest path and distance queries on large graphs
#@ Andy Diwen Zhu;Xiaokui Xiao;Sibo Wang;Wenqing Lin
#t 2013
#c 0
#% 280394
#% 282771
#% 617131
#% 847101
#% 1214671
#% 1230568
#% 1263166
#% 1404186
#% 1412885
#% 1449326
#% 1581881
#% 1597258
#% 1642053
#% 1676469
#% 1770357
#% 1784039
#% 1972751
#! This paper investigates two types of graph queries: single source distance (SSD) queries and single source shortest path (SSSP) queries. Given a node v in a graph G, an SSD query from v asks for the distance from $v$ to any other node in G, while an SSSP query retrieves the shortest path from v to any other node. These two types of queries find important applications in graph analysis, especially in the computation of graph measures. Most of the existing solutions for SSD and SSSP queries, however, require that the input graph fits in the main memory, which renders them inapplicable for the massive disk-resident graphs commonly used in web and social applications. There are several techniques that are designed to be I/O efficient, but they all focus on undirected and/or unweighted graphs, and they only offer sub-optimal query efficiency. To address the deficiency of existing work, this paper presents Highways-on-Disk (HoD), a disk-based index that supports both SSD and SSSP queries on directed and weighted graphs. The key idea of HoD is to augment the input graph with a set of auxiliary edges, and exploit them during query processing to reduce I/O and computation costs. We experimentally evaluate HoD on both directed and undirected real-world graphs with up to billions of nodes and edges, and we demonstrate that HoD significantly outperforms alternative solutions in terms of query efficiency.

#index 1991866
#* On community detection in real-world networks and the importance of degree assortativity
#@ Marek Ciglan;Michal Laclavík;Kjetil Nørvåg
#t 2013
#c 0
#% 1108861
#% 1108891
#% 1130858
#% 1214714
#% 1214721
#% 1399996
#% 1583582
#% 1746811
#% 1872305
#% 1978787
#! Graph clustering, often addressed as community detection, is a prominent task in the domain of graph data mining with dozens of algorithms proposed in recent years. In this paper, we focus on several popular community detection algorithms with low computational complexity and with decent performance on the artificial benchmarks, and we study their behaviour on real-world networks. Motivated by the observation that there is a class of networks for which the community detection methods fail to deliver good community structure, we examine the assortativity coefficient of ground-truth communities and show that assortativity of a community structure can be very different from the assortativity of the original network. We then examine the possibility of exploiting the latter by weighting edges of a network with the aim to improve the community detection outputs for networks with assortative community structure. The evaluation shows that the proposed weighting can significantly improve the results of community detection methods on networks with assortative community structure.

#index 1991867
#* Robust sparse estimation of multiresponse regression and inverse covariance matrix via the L2 distance
#@ Aurelie C. Lozano;Huijing Jiang;Xinwei Deng
#t 2013
#c 0
#% 1074353
#% 1864263
#! We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among responses while adjusting for their predictors. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. In this work, we propose instead to minimize a regularized distance criterion, which is motivated by the minimum distance functionals used in nonparametric methods for their excellent robustness properties. The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm. We provide theoretical justification such as estimation consistency for the proposed estimator. Additionally, we shed light on the robustness of our estimator through its linearization, which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness. We demonstrate the merits of our framework through simulation study and the analysis of real financial and genetics data.

#index 1991868
#* Comparing apples to oranges: a scalable solution with heterogeneous hashing
#@ Mingdong Ou;Peng Cui;Fei Wang;Jun Wang;Wenwu Zhu;Shiqiang Yang
#t 2013
#c 0
#% 249238
#% 249321
#% 635689
#% 722904
#% 898309
#% 1286845
#% 1292880
#% 1649056
#% 1750268
#% 1826280
#% 1872343
#% 1884017
#% 1884343
#% 1919751
#% 1931623
#! Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogeneous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of ``comparing apples to oranges'' under the large scale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing (RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogeneous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets; one is crawled from a popular social media sites, Tencent Weibo, and the other is an open dataset of Flickr(NUS-WIDE). The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.

#index 1991869
#* Trial and error in influential social networks
#@ Xiaohui Bei;Ning Chen;Liyu Dou;Xiangru Huang;Ruixin Qiang
#t 2013
#c 0
#% 342596
#% 577217
#% 729923
#% 989613
#% 1000451
#% 1039690
#% 1214641
#% 1227295
#% 1425621
#% 1451243
#% 1992586
#! In this paper, we introduce a trial-and-error model to study information diffusion in a social network. Specifically, in every discrete period, all individuals in the network concurrently try a new technology or product with certain respective probabilities. If it turns out that an individual observes a better utility, he will then adopt the trial; otherwise, the individual continues to choose his prior selection. We first demonstrate that the trial and error behavior of individuals characterizes certain global community structures of a social network, from which we are able to detect macro-communities through the observation of micro-behavior of individuals. We run simulations on classic benchmark testing graphs, and quite surprisingly, the results show that the trial and error dynamics even outperforms the Louvain method (a popular modularity maximization approach) if individuals have dense connections within communities. This gives a solid justification of the model. We then study the influence maximization problem in the trial-and-error dynamics. We give a heuristic algorithm based on community detection and provide experiments on both testing and large scale collaboration networks. Simulation results show that our algorithm significantly outperforms several well-studied heuristics including degree centrality and distance centrality in almost all of the scenarios. Our results reveal the relation between the budget that an advertiser invests and marketing strategies, and indicate that the mixing parameter, a benchmark evaluating network community structures, plays a critical role for information diffusion.

#index 1991870
#* Collaborative matrix factorization with multiple similarities for predicting drug-target interactions
#@ Xiaodong Zheng;Hao Ding;Hiroshi Mamitsuka;Shanfeng Zhu
#t 2013
#c 0
#% 465914
#% 846431
#% 875974
#% 1041217
#% 1072340
#% 1126420
#% 1130901
#% 1176959
#% 1214688
#% 1264367
#% 1318698
#% 1327693
#% 1382144
#% 1482274
#% 1558464
#% 1639820
#% 1920712
#! We address the problem of predicting new drug-target interactions from three inputs: known interactions, similarities over drugs and those over targets. This setting has been considered by many methods, which however have a common problem of allowing to have only one similarity matrix over drugs and that over targets. The key idea of our approach is to use more than one similarity matrices over drugs as well as those over targets, where weights over the multiple similarity matrices are estimated from data to automatically select similarities, which are effective for improving the performance of predicting drug-target interactions. We propose a factor model, named Multiple Similarities Collaborative Matrix Factorization(MSCMF), which projects drugs and targets into a common low-rank feature space, which is further consistent with weighted similarity matrices over drugs and those over targets. These two low-rank matrices and weights over similarity matrices are estimated by an alternating least squares algorithm. Our approach allows to predict drug-target interactions by the two low-rank matrices collaboratively and to detect similarities which are important for predicting drug-target interactions. This approach is general and applicable to any binary relations with similarities over elements, being found in many applications, such as recommender systems. In fact, MSCMF is an extension of weighted low-rank approximation for one-class collaborative filtering. We extensively evaluated the performance of MSCMF by using both synthetic and real datasets. Experimental results showed nice properties of MSCMF on selecting similarities useful in improving the predictive performance and the performance advantage of MSCMF over six state-of-the-art methods for predicting drug-target interactions.

#index 1991871
#* FeaFiner: biomarker identification from medical data through feature generalization and selection
#@ Jiayu Zhou;Zhaosong Lu;Jimeng Sun;Lei Yuan;Fei Wang;Jieping Ye
#t 2013
#c 0
#% 771626
#% 961223
#% 1077165
#% 1211744
#% 1211772
#% 1214676
#% 1298490
#% 1417091
#% 1606019
#% 1615039
#% 1872362
#! Traditionally, feature construction and feature selection are two important but separate processes in data mining. However, many real world applications require an integrated approach for creating, refining and selecting features. To address this problem, we propose FeaFiner (short for Feature Refiner), an efficient formulation that simultaneously generalizes low-level features into higher level concepts and then selects relevant concepts based on the target variable. Specifically, we formulate a double sparsity optimization problem that identifies groups in the low-level features, generalizes higher level features using the groups and performs feature selection. Since in many clinical researches non- overlapping groups are preferred for better interpretability, we further improve the formulation to generalize features using mutually exclusive feature groups. The proposed formulation is challenging to solve due to the orthogonality constraints, non-convexity objective and non-smoothness penal- ties. We apply a recently developed augmented Lagrangian method to solve this formulation in which each subproblem is solved by a non-monotone spectral projected gradient method. Our numerical experiments show that this approach is computationally efficient and also capable of producing solutions of high quality. We also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation. Finally, the proposed FeaFiner method is validated on Alzheimer's Disease Neuroimaging Initiative dataset, where low-level biomarkers are automatically generalized into robust higher level concepts which are then selected for predicting the disease status measured by Mini Mental State Examination and Alzheimer's Disease Assessment Scale cognitive subscore. Compared to existing predictive modeling methods, FeaFiner provides intuitive and robust feature concepts and competitive predictive accuracy.

#index 1991872
#* Text-based measures of document diversity
#@ Kevin Bache;David Newman;Padhraic Smyth
#t 2013
#c 0
#% 1560378
#% 1913634
#! Quantitative notions of diversity have been explored across a variety of disciplines ranging from conservation biology to economics. However, there has been relatively little work on measuring the diversity of text documents via their content. In this paper we present a text-based framework for quantifying how diverse a document is in terms of its content. The proposed approach learns a topic model over a corpus of documents, and computes a distance matrix between pairs of topics using measures such as topic co-occurrence. These pairwise distance measures are then combined with the distribution of topics within a document to estimate each document's diversity relative to the rest of the corpus. The method provides several advantages over existing methods. It is fully data-driven, requiring only the text from a corpus of documents as input, it produces human-readable explanations, and it can be generalized to score diversity of other entities such as authors, academic departments, or journals. We describe experimental results on several large data sets which suggest that the approach is effective and accurate in quantifying how diverse a document is relative to other documents in a corpus.

#index 1991873
#* Learning geographical preferences for point-of-interest recommendation
#@ Bin Liu;Yanjie Fu;Zijun Yao;Hui Xiong
#t 2013
#c 0
#% 722904
#% 1176909
#% 1190134
#% 1194460
#% 1214623
#% 1260273
#% 1355025
#% 1355044
#% 1400036
#% 1451230
#% 1560379
#% 1598360
#% 1598366
#% 1605963
#% 1606049
#% 1642006
#% 1688509
#% 1746875
#% 1941002
#% 1948163
#! The problem of point of interest (POI) recommendation is to provide personalized recommendations of places of interests, such as restaurants, for mobile users. Due to its complexity and its connection to location based social networks (LBSNs), the decision process of a user choose a POI is complex and can be influenced by various factors, such as user preferences, geographical influences, and user mobility behaviors. While there are some studies on POI recommendations, it lacks of integrated analysis of the joint effect of multiple factors. To this end, in this paper, we propose a novel geographical probabilistic factor analysis framework which strategically takes various factors into consideration. Specifically, this framework allows to capture the geographical influences on a user's check-in behavior. Also, the user mobility behaviors can be effectively exploited in the recommendation model. Moreover, the recommendation model can effectively make use of user check-in count data as implicity user feedback for modeling user preferences. Finally, experimental results on real-world LBSNs data show that the proposed recommendation method outperforms state-of-the-art latent factor models with a significant margin.

#index 1991874
#* SVMpAUCtight: a new support vector method for optimizing partial AUC based on a tight convex upper bound
#@ Harikrishna Narasimhan;Shivani Agarwal
#t 2013
#c 0
#% 829008
#% 840882
#% 881477
#% 1073906
#% 1083707
#% 1200868
#% 1214726
#% 1264133
#% 1385985
#% 1588927
#% 1904839
#% 1978810
#% 1991874
#! The area under the ROC curve (AUC) is a well known performance measure in machine learning and data mining. In an increasing number of applications, however, ranging from ranking applications to a variety of important bioinformatics applications, performance is measured in terms of the partial area under the ROC curve between two specified false positive rates. In recent work, we proposed a structural SVM based approach for optimizing this performance measure (Narasimhan and Agarwal, 2013). In this paper, we develop a new support vector method, SVMpAUCtight, that optimizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced computational complexity. In particular, by rewriting the empirical partial AUC risk as a maximum over subsets of negative instances, we derive a new formulation, where a modified form of the earlier optimization objective is evaluated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the resulting optimization problem can be solved using a cutting-plane algorithm, but the new method has better run time guarantees. We also discuss a projected subgradient method for solving this problem, which offers additional computational savings in certain settings. We demonstrate on a wide variety of bioinformatics tasks, ranging from protein-protein interaction prediction to drug discovery tasks, that the proposed method does, in many cases, perform significantly better on the partial AUC measure than the previous structural SVM approach. In addition, we also develop extensions of our method to learn sparse and group sparse models, often of interest in biological applications.

#index 1991875
#* Learning mixed kronecker product graph models with simulated method of moments
#@ Sebastian I. Moreno;Jennifer Neville;Sergey Kirshner
#t 2013
#c 0
#% 593994
#% 878224
#% 1246431
#% 1386131
#% 1673564
#% 1688548
#! There has recently been a great deal of work focused on developing statistical models of graph structure---with the goal of modeling probability distributions over graphs from which new, similar graphs can be generated by sampling from the estimated distributions. Although current graph models can capture several important characteristics of social network graphs (e.g., degree, path lengths), many of them do not generate graphs with sufficient variation to reflect the natural variability in real world graph domains. One exception is the mixed Kronecker Product Graph Model (mKPGM), a generalization of the Kronecker Product Graph Model, which uses parameter tying to capture variance in the underlying distribution [10]. The enhanced representation of mKPGMs enables them to match both the mean graph statistics and their spread as observed in real network populations, but unfortunately to date, the only method to estimate mKPGMs involves an exhaustive search over the parameters. In this work, we present the first learning algorithm for mKPGMs. The O(|E|) algorithm searches over the continuous parameter space using constrained line search and is based on simulated method of moments, where the objective function minimizes the distance between the observed moments in the training graph and the empirically estimated moments of the model. We evaluate the mKPGM learning algorithm by comparing it to several different graph models, including KPGMs. We use multi-dimensional KS distance to compare the generated graphs to the observed graphs and the results show mKPGMs are able to produce a closer match to real-world graphs (10-90% reduction in KS distance), while still providing natural variation in the generated graphs.

#index 1991876
#* Subsampling for efficient and effective unsupervised outlier detection ensembles
#@ Arthur Zimek;Matthew Gaudet;Ricardo J.G.B. Campello;Jörg Sander
#t 2013
#c 0
#% 300136
#% 300183
#% 333933
#% 342625
#% 443616
#% 478624
#% 482502
#% 551723
#% 721137
#% 722902
#% 729912
#% 823340
#% 837616
#% 881506
#% 915267
#% 940282
#% 1083673
#% 1083710
#% 1165483
#% 1196028
#% 1267765
#% 1292669
#% 1523925
#% 1669937
#% 1672436
#% 1697243
#% 1846695
#% 1846754
#% 1911392
#% 1912090
#% 1971509
#! Outlier detection and ensemble learning are well established research directions in data mining yet the application of ensemble techniques to outlier detection has been rarely studied. Here, we propose and study subsampling as a technique to induce diversity among individual outlier detectors. We show analytically and experimentally that an outlier detector based on a subsample per se, besides inducing diversity, can, under certain conditions, already improve upon the results of the same outlier detector on the complete dataset. Building an ensemble on top of several subsamples is further improving the results. While in the literature so far the intuition that ensembles improve over single outlier detectors has just been transferred from the classification literature, here we also justify analytically why ensembles are also expected to work in the unsupervised area of outlier detection. As a side effect, running an ensemble of several outlier detectors on subsamples of the dataset is more efficient than ensembles based on other means of introducing diversity and, depending on the sample rate and the size of the ensemble, can be even more efficient than just the single outlier detector on the complete data.

#index 1991877
#* Big data analytics with small footprint: squaring the cloud
#@ John Canny;Huasha Zhao
#t 2013
#c 0
#% 397153
#% 766422
#% 983905
#% 1260273
#% 1464950
#% 1523858
#% 1566972
#% 1783374
#% 1895052
#% 1911310
#! This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported cluster implementations for common benchmark problems. A key design criterion is rapid exploration of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes very efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.

#index 1991878
#* A space efficient streaming algorithm for triangle counting using the birthday paradox
#@ Madhav Jha;C. Seshadhri;Ali Pinar
#t 2013
#c 0
#% 1331
#% 1604
#% 281655
#% 379443
#% 847067
#% 874902
#% 1083625
#% 1124590
#% 1176970
#% 1214705
#% 1245882
#% 1254809
#% 1384712
#% 1560415
#% 1605988
#% 1626355
#% 1682599
#% 1701869
#% 1719564
#% 1748646
#% 1770116
#% 1888912
#% 1934554

#index 1991879
#* Measuring spontaneous devaluations in user preferences
#@ Komal Kapoor;Nisheeth Srivastava;Jaideep Srivastava;Paul Schrater
#t 2013
#c 0
#% 572968
#% 805841
#% 838504
#% 860672
#% 1357698
#% 1450855
#% 1480899
#% 1625364
#! Spontaneous devaluation in preferences is ubiquitous, where yesterday's hit is today's affliction. Despite technological advances facilitating access to a wide range of media commodities, finding engaging content is a major enterprise with few principled solutions. Systems tracking spontaneous devaluation in user preferences can allow prediction of the onset of boredom in users potentially catering to their changed needs. In this work, we study the music listening histories of Last.fm users focusing on the changes in their preferences based on their choices for different artists at different points in time. A hazard function, commonly used in statistics for survival analysis, is used to capture the rate at which a user returns to an artist as a function of exposure to the artist. The analysis provides the first evidence of spontaneous devaluation in preferences of music listeners. Better understanding of the temporal dynamics of this phenomenon can inform solutions to the similarity-diversity dilemma of recommender systems.

#index 1991880
#* Mining evidences for named entity disambiguation
#@ Yang Li;Chi Wang;Fangqiu Han;Jiawei Han;Dan Roth;Xifeng Yan
#t 2013
#c 0
#% 722904
#% 956564
#% 975019
#% 1063570
#% 1130858
#% 1270689
#% 1338553
#% 1409954
#% 1482395
#% 1484272
#% 1592023
#% 1592043
#% 1592066
#% 1598410
#% 1606044
#% 1711795
#% 1711796
#% 1711798
#% 1711865
#% 1746843
#% 1746871
#% 1826368
#% 1913579
#! Named entity disambiguation is the task of disambiguating named entity mentions in natural language text and link them to their corresponding entries in a knowledge base such as Wikipedia. Such disambiguation can help enhance readability and add semantics to plain text. It is also a central step in constructing high-quality information network or knowledge graph from unstructured text. Previous research has tackled this problem by making use of various textual and structural features from a knowledge base. Most of the proposed algorithms assume that a knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity. However, the existing knowledge bases are rarely complete (likely will never be), thus leading to poor performance on short queries with not well-known contexts. In such cases, we need to collect additional evidences scattered in internal and external corpus to augment the knowledge bases and enhance their disambiguation power. In this work, we propose a generative model and an incremental algorithm to automatically mine useful evidences across documents. With a specific modeling of "background topic" and "unknown entities", our model is able to harvest useful evidences out of noisy information. Experimental results show that our proposed method outperforms the state-of-the-art approaches significantly: boosting the disambiguation accuracy from 43% (baseline) to 86% on short queries derived from tweets.

#index 1991881
#* One theme in all views: modeling consensus topics in multiple contexts
#@ Jian Tang;Ming Zhang;Qiaozhu Mei
#t 2013
#c 0
#% 280819
#% 722904
#% 785334
#% 788094
#% 879587
#% 881498
#% 1055681
#% 1055682
#% 1055683
#% 1292503
#% 1355042
#% 1470574
#% 1481646
#% 1536586
#% 1560379
#% 1561559
#% 1587367
#% 1688451
#% 1711748
#% 1746875
#! New challenges have been presented to classical topic models when applied to social media, as user-generated content suffers from significant problems of data sparseness. A variety of heuristic adjustments to these models have been proposed, many of which are based on the use of context information to improve the performance of topic modeling. Existing contextualized topic models rely on arbitrary manipulation of the model structure, by incorporating various context variables into the generative process of classical topic models in an ad hoc manner. Such manipulations usually result in much more complicated model structures, sophisticated inference procedures, and low generalizability to accommodate arbitrary types or combinations of contexts. In this paper we explore a different direction. We propose a general solution that is able to exploit multiple types of contexts without arbitrary manipulation of the structure of classical topic models. We formulate different types of contexts as multiple views of the partition of the corpus. A co-regularization framework is proposed to let these views collaborate with each other, vote for the consensus topics, and distinguish them from view-specific topics. Experiments with real-world datasets prove that the proposed method is both effective and flexible to handle arbitrary types of contexts.

#index 1991882
#* Information cascade at group scale
#@ Milad Eftekhar;Yashar Ganjali;Nick Koudas
#t 2013
#c 0
#% 342596
#% 577217
#% 631985
#% 729923
#% 754107
#% 786841
#% 963337
#% 989613
#% 1000451
#% 1055690
#% 1214641
#% 1222657
#% 1399997
#% 1425621
#% 1451242
#% 1451243
#% 1482198
#% 1657956
#% 1746849
#! Identifying the k most influential individuals in a social network is a well-studied problem. The objective is to detect k individuals in a (social) network who will influence the maximum number of people, if they are independently convinced of adopting a new strategy (product, idea, etc). There are cases in real life, however, where we aim to instigate groups instead of individuals to trigger network diffusion. Such cases abound, e.g., billboards, TV commercials and newspaper ads are utilized extensively to boost the popularity and raise awareness. In this paper, we generalize the "influential nodes" problem. Namely we are interested to locate the most "influential groups" in a network. As the first paper to address this problem: we (1) propose a fine-grained model of information diffusion for the group-based problem, (2) show that the process is submodular and present an algorithm to determine the influential groups under this model (with a precise approximation bound), (3) propose a coarse-grained model that inspects the network at group level (not individuals) significantly speeding up calculations for large networks, (4) show that the diffusion function we design here is submodular in general case, and propose an approximation algorithm for this coarse-grained model, and finally by conducting experiments on real datasets, (5) demonstrate that seeding members of selected groups to be the first adopters can broaden diffusion (when compared to the influential individuals case). Moreover, we can identify these influential groups much faster (up to 12 million times speedup), delivering a practical solution to this problem.

#index 1991883
#* Debiasing social wisdom
#@ Abhimanyu Das;Sreenivas Gollapudi;Rina Panigrahy;Mahyar Salek
#t 2013
#c 0
#% 190611
#% 342596
#% 729923
#% 835045
#% 1657994
#% 1770349
#% 1872258
#% 1895096
#% 1948124
#% 1992587
#! With the explosive growth of social networks, many applications are increasingly harnessing the pulse of online crowds for a variety of tasks such as marketing, advertising, and opinion mining. An important example is the wisdom of crowd effect that has been well studied for such tasks when the crowd is non-interacting. However, these studies don't explicitly address the network effects in social networks. A key difference in this setting is the presence of social influences that arise from these interactions and can undermine the wisdom of the crowd [17]. Using a natural model of opinion formation, we analyze the effect of these interactions on an individual's opinion and estimate her propensity to conform. We then propose efficient sampling algorithms incorporating these conformity values to arrive at a debiased estimate of the wisdom of a crowd. We analyze the trade-off between the sample size and estimation error and validate our algorithms using both real data obtained from online user experiments and synthetic data.

#index 1991884
#* Estimating sharer reputation via social data calibration
#@ Jaewon Yang;Bee-Chung Chen;Deepak Agarwal
#t 2013
#c 0
#% 268079
#% 290830
#% 729923
#% 1083641
#% 1117691
#% 1318590
#% 1355042
#% 1482198
#% 1536507
#% 1536523
#% 1560429
#% 1605930
#% 1607968
#% 1688496
#% 1693931
#% 1711565
#% 1737767
#% 1746831
#% 1872301
#% 1872311
#% 1872333
#% 1872353
#% 1879070
#% 1895107
#% 1919937
#! Online social networks have become important channels for users to share content with their connections and diffuse information. Although much work has been done to identify socially influential users, the problem of finding "reputable" sharers, who share good content, has received relatively little attention. Availability of such reputation scores can be useful or various applications like recommending people to follow, procuring high quality content in a scalable way, creating a content reputation economy to incentivize high quality sharing, and many more. To estimate sharer reputation, it is intuitive to leverage data that records how recipients respond (through clicking, liking, etc.) to content items shared by a sharer. However, such data is usually biased --- it has a selection bias since the shared items can only be seen and responded to by users connected to the sharer in most social networks, and it has a response bias since the response is usually influenced by the relationship between the sharer and the recipient (which may not indicate whether the shared content is good). To correct for such biases, we propose to utilize an additional data source that provides unbiased goodness estimates for a small set of shared items, and calibrate biased social data through a novel multi-level hierarchical model that describes how the unbiased data and biased data are jointly generated according to sharer reputation scores. The unbiased data also provides the ground truth for quantitative evaluation of different methods. Experiments based on such ground-truth data show that our proposed model significantly outperforms existing methods that estimate social influence using biased social data.

#index 1991885
#* Linking named entities in Tweets with knowledge base via user interest modeling
#@ Wei Shen;Jianyong Wang;Ping Luo;Min Wang
#t 2013
#c 0
#% 348173
#% 956564
#% 1063570
#% 1214667
#% 1355042
#% 1384223
#% 1409954
#% 1482547
#% 1591965
#% 1598410
#% 1632490
#% 1693865
#% 1693918
#% 1711796
#% 1746843
#% 1770359
#% 1872403
#% 1879058
#% 1879064
#% 1918369
#! Twitter has become an increasingly important source of information, with more than 400 million tweets posted per day. The task to link the named entity mentions detected from tweets with the corresponding real world entities in the knowledge base is called tweet entity linking. This task is of practical importance and can facilitate many different tasks, such as personalized recommendation and user interest discovery. The tweet entity linking task is challenging due to the noisy, short, and informal nature of tweets. Previous methods focus on linking entities in Web documents, and largely rely on the context around the entity mention and the topical coherence between entities in the document. However, these methods cannot be effectively applied to the tweet entity linking task due to the insufficient context information contained in a tweet. In this paper, we propose KAURI, a graph-based framework to collectively link all the named entity mentions in all tweets posted by a user via modeling the user's topics of interest. Our assumption is that each user has an underlying topic interest distribution over various named entities. KAURI integrates the intra-tweet local information with the inter-tweet user interest information into a unified graph-based framework. We extensively evaluated the performance of KAURI over manually annotated tweet corpus, and the experimental results show that KAURI significantly outperforms the baseline methods in terms of accuracy, and KAURI is efficient and scales well to tweet stream.

#index 1991886
#* Privacy-preserving data exploration in genome-wide association studies
#@ Aaron Johnson;Vitaly Shmatikov
#t 2013
#c 0
#% 576762
#% 833558
#% 937550
#% 963241
#% 1029084
#% 1190072
#% 1198224
#% 1198226
#% 1214684
#% 1298824
#% 1318624
#% 1318815
#% 1426328
#% 1451190
#% 1464628
#% 1521655
#% 1689683
#% 1740518
#! Genome-wide association studies (GWAS) have become a popular method for analyzing sets of DNA sequences in order to discover the genetic basis of disease. Unfortunately, statistics published as the result of GWAS can be used to identify individuals participating in the study. To prevent privacy breaches, even previously published results have been removed from public databases, impeding researchers' access to the data and hindering collaborative research. Existing techniques for privacy-preserving GWAS focus on answering specific questions, such as correlations between a given pair of SNPs (DNA sequence variations). This does not fit the typical GWAS process, where the analyst may not know in advance which SNPs to consider and which statistical tests to use, how many SNPs are significant for a given dataset, etc. We present a set of practical, privacy-preserving data mining algorithms for GWAS datasets. Our framework supports exploratory data analysis, where the analyst does not know a priori how many and which SNPs to consider. We develop privacy-preserving algorithms for computing the number and location of SNPs that are significantly associated with the disease, the significance of any statistical test between a given SNP and the disease, any measure of correlation between SNPs, and the block structure of correlations. We evaluate our algorithms on real-world datasets and demonstrate that they produce significantly more accurate results than prior techniques while guaranteeing differential privacy.

#index 1991887
#* Synthetic review spamming and defense
#@ Huan Sun;Alex Morales;Xifeng Yan
#t 2013
#c 0
#% 544011
#% 722904
#% 818223
#% 1024557
#% 1035590
#% 1166531
#% 1176947
#% 1289524
#% 1301004
#% 1482272
#% 1549085
#% 1591960
#% 1592028
#% 1711831
#! Online reviews have been popularly adopted in many applications. Since they can either promote or harm the reputation of a product or a service, buying and selling fake reviews becomes a profitable business and a big threat. In this paper, we introduce a very simple, but powerful review spamming technique that could fail the existing feature-based detection algorithms easily. It uses one truthful review as a template, and replaces its sentences with those from other reviews in a repository. Fake reviews generated by this mechanism are extremely hard to detect: Both the state-of-the-art computational approaches and human readers acquire an error rate of 35%-48%, just slightly better than a random guess. While it is challenging to detect such fake reviews, we have made solid progress in suppressing them. A novel defense method that leverages the difference of semantic flows between synthetic and truthful reviews is developed, which is able to reduce the detection error rate to approximately 22%, a significant improvement over the performance of existing approaches. Nevertheless, it is still a challenging research task to further decrease the error rate. Synthetic Review Spamming Demo: www.cs.ucsb.edu/~alex_morales/reviewspam/

#index 1991888
#* Redundancy-aware maximal cliques
#@ Jia Wang;James Cheng;Ada Wai-Chee Fu
#t 2013
#c 0
#% 268040
#% 322619
#% 498852
#% 769876
#% 823347
#% 823356
#% 881500
#% 937814
#% 1108882
#% 1124599
#% 1166473
#% 1206662
#% 1211648
#% 1426539
#% 1594586
#% 1605988
#% 1625106
#% 1848109
#% 1872379
#% 1907284
#! Recent research efforts have made notable progress in improving the performance of (exhaustive) maximal clique enumeration (MCE). However, existing algorithms still suffer from exploring the huge search space of MCE. Furthermore, their results are often undesirable as many of the returned maximal cliques have large overlapping parts. This redundancy leads to problems in both computational efficiency and usefulness of MCE. In this paper, we aim at providing a concise and complete summary of the set of maximal cliques, which is useful to many applications. We propose the notion of τ-visible MCE to achieve this goal and design algorithms to realize the notion. Based on the refined output space, we further consider applications including an efficient computation of the top-k results with diversity and an interactive clique exploration process. Our experimental results demonstrate that our approach is capable of producing output of high usability and our algorithms achieve superior efficiency over classic MCE algorithms.

#index 1991889
#* Information cartography: creating zoomable, large-scale maps of information
#@ Dafna Shahaf;Jaewon Yang;Caroline Suen;Jeff Jacobs;Heidi Wang;Jure Leskovec
#t 2013
#c 0
#% 340883
#% 722904
#% 769887
#% 823344
#% 989613
#% 995168
#% 1029072
#% 1131010
#% 1214650
#% 1451202
#% 1598408
#% 1746887
#% 1948183
#! In an era of information overload, many people struggle to make sense of complex stories, such as presidential elections or economic reforms. We propose a methodology for creating structured summaries of information, which we call zoomable metro maps. Just as cartographic maps have been relied upon for centuries to help us understand our surroundings, metro maps can help us understand the information landscape. Given large collection of news documents our proposed algorithm generates a map of connections that explicitly captures story development. As different users might be interested in different levels of granularity, the maps are zoomable, with each level of zoom showing finer details and interactions. In this paper, we formalize characteristics of good zoomable maps and formulate their construction as an optimization problem. We provide efficient, scalable methods with theoretical guarantees for generating maps. Pilot user studies over real-world datasets demonstrate that our method helps users comprehend complex stories better than prior work.

#index 1991890
#* Confluence: conformity influence in large social networks
#@ Jie Tang;Sen Wu;Jimeng Sun
#t 2013
#c 0
#% 342596
#% 464434
#% 577217
#% 729923
#% 1019178
#% 1083624
#% 1083641
#% 1083734
#% 1214641
#% 1214702
#% 1355040
#% 1399993
#% 1425621
#% 1451242
#% 1451245
#% 1606049
#% 1642030
#% 1650318
#% 1650488
#% 1693935
#% 1783934
#% 1810385
#% 1992461
#! Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Employing several social networks as the source for our experimental data, we study how the effect of conformity plays a role in changing users' online behavior. We formally define several major types of conformity in individual, peer, and group levels. We propose Confluence model to formalize the effects of social conformity into a probabilistic model. Confluence can distinguish and quantify the effects of the different types of conformities. To scale up to large social networks, we propose a distributed learning method that can construct the Confluence model efficiently with near-linear speedup. Our experimental results on four different types of large social networks, i.e., Flickr, Gowalla, Weibo and Co-Author, verify the existence of the conformity phenomena. Leveraging the conformity information, Confluence can accurately predict actions of users. Our experiments show that Confluence significantly improves the prediction accuracy by up to 5-10% compared with several alternative methods.

#index 1991891
#* Mining discriminative subgraphs from global-state networks
#@ Sayan Ranu;Minh Hoang;Ambuj Singh
#t 2013
#c 0
#% 1044007
#% 1060798
#% 1063502
#% 1184831
#% 1207028
#% 1268040
#% 1318642
#% 1328170
#% 1426575
#% 1688444
#! Global-state networks provide a powerful mechanism to model the increasing heterogeneity in data generated by current systems. Such a network comprises of a series of network snapshots with dynamic local states at nodes, and a global network state indicating the occurrence of an event. Mining discriminative subgraphs from global-state networks allows us to identify the influential sub-networks that have maximum impact on the global state and unearth the complex relationships between the local entities of a network and their collective behavior. In this paper, we explore this problem and design a technique called MINDS to mine minimally discriminative subgraphs from large global-state networks. To combat the exponential subgraph search space, we derive the concept of an edit map and perform Metropolis Hastings sampling on it to compute the answer set. Furthermore, we formulate the idea of network-constrained decision trees to learn prediction models that adhere to the underlying network structure. Extensive experiments on real datasets demonstrate excellent accuracy in terms of prediction quality. Additionally, MINDS achieves a speed-up of at least four orders of magnitude over baseline techniques.

#index 1991892
#* Scalable text and link analysis with mixed-topic link models
#@ Yaojia Zhu;Xiaoran Yan;Lise Getoor;Cristopher Moore
#t 2013
#c 0
#% 280819
#% 466574
#% 722904
#% 722914
#% 859289
#% 1083684
#% 1117695
#% 1264771
#% 1417123
#% 1541098
#% 1606022
#! Many data sets contain rich information about objects, as well as pairwise relations between them. For instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community. The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classification and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.

#index 1991893
#* Exact sparse recovery with L0 projections
#@ Ping Li;Cun-Hui Zhang
#t 2013
#c 0
#% 56600
#% 274586
#% 816392
#% 879397
#% 894646
#% 989615
#% 1039579
#% 1299246
#% 1417090
#% 1761822
#% 1810658
#% 1815246
#% 1815896
#% 1815965
#% 1865671
#% 1874624
#! Many applications (e.g., anomaly detection) concern sparse signals. This paper focuses on the problem of recovering a K-sparse signal x ∈ R/1×N, i.e., K N and ∑N/i=1 1{xi ≠ 0} = K. In the mainstream framework of compressed sensing (CS), × is recovered from M linear measurements y = xS ∈ R/1×M, where S ∈ RN×M is often a Gaussian (or Gaussian-like) design matrix. In our proposed method, the design matrix S is generated from an α-stable distribution with α ≈ 0. Our decoding algorithm mainly requires one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are "undetermined" in the previous iteration. Our practical algorithm consists of two estimators. In the first iteration, the (absolute) minimum estimator is able to filter out a majority of the zero coordinates. The gap estimator, which is applied in each iteration, can accurately recover the magnitudes of the nonzero coordinates. Comparisons with linear programming (LP) and orthogonal matching pursuit (OMP) demonstrate that our algorithm can be significantly faster in decoding speed and more accurate in recovery quality, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements, our algorithm can still reliably recover a significant portion of the nonzero coordinates.

#index 1991894
#* Graph cluster randomization: network exposure to multiple universes
#@ Johan Ugander;Brian Karrer;Lars Backstrom;Jon Kleinberg
#t 2013
#c 0
#% 347264
#% 723894
#% 1560416
#% 1746900
#% 1872323
#% 1948175
#! A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how graph cluster randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified. Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a restricted-growth condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference.

#index 1991895
#* Restreaming graph partitioning: simple versatile algorithms for advanced balancing
#@ Joel Nishimura;Johan Ugander
#t 2013
#c 0
#% 274612
#% 291940
#% 414944
#% 656714
#% 1002007
#% 1055741
#% 1214643
#% 1715951
#% 1770116
#% 1872377
#% 1911311
#% 1948175
#% 1991894
#! Partitioning large graphs is difficult, especially when performed in the limited models of computation afforded to modern large scale computing systems. In this work we introduce restreaming graph partitioning and develop algorithms that scale similarly to streaming partitioning algorithms yet empirically perform as well as fully offline algorithms. In streaming partitioning, graphs are partitioned serially in a single pass. Restreaming partitioning is motivated by scenarios where approximately the same dataset is routinely streamed, making it possible to transform streaming partitioning algorithms into an iterative procedure. This combination of simplicity and powerful performance allows restreaming algorithms to be easily adapted to efficiently tackle more challenging partitioning objectives. In particular, we consider the problem of stratified graph partitioning, where each of many node attribute strata are balanced simultaneously. As such, stratified partitioning is well suited for the study of network effects on social networks, where it is desirable to isolate disjoint dense subgraphs with representative user demographics. To demonstrate, we partition a large social network such that each partition exhibits the same degree distribution in the original graph --- a novel achievement for non-regular graphs. As part of our results, we also observe a fundamental difference in the ease with which social graphs are partitioned when compared to web graphs. Namely, the modular structure of web graphs appears to motivate full offline optimization, whereas the locally dense structure of social graphs precludes significant gains from global manipulations.

#index 1991896
#* Stochastic collapsed variational Bayesian inference for latent Dirichlet allocation
#@ James Foulds;Levi Boyles;Christopher DuBois;Padhraic Smyth;Max Welling
#t 2013
#c 0
#% 722904
#% 1083687
#% 1214715
#% 1385969
#% 1417055
#% 1523858
#% 1723624
#! There has been an explosion in the amount of digital text information available in recent years, leading to challenges of scale for traditional inference algorithms for topic models. Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on very large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model. We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method. In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than previous methods. Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software.

#index 1991897
#* Understanding evolution of research themes: a probabilistic generative model for citations
#@ Xiaolong Wang;Chengxiang Zhai;Dan Roth
#t 2013
#c 0
#% 329569
#% 584932
#% 722904
#% 823344
#% 875959
#% 881498
#% 1083684
#% 1192430
#% 1251672
#% 1338740
#% 1560380
#% 1663619
#% 1689553
#! Understanding how research themes evolve over time in a research community is useful in many ways (e.g., revealing important milestones and discovering emerging major research trends). In this paper, we propose a novel way of analyzing literature citation to explore the research topics and the theme evolution by modeling article citation relations with a probabilistic generative model. The key idea is to represent a research paper by a ``bag of citations'' and model such a ``citation document'' with a probabilistic topic model. We explore the extension of a particular topic model, i.e., Latent Dirichlet Allocation~(LDA), for citation analysis, and show that such a Citation-LDA can facilitate discovering of individual research topics as well as the theme evolution from multiple related topics, both of which in turn lead to the construction of evolution graphs for characterizing research themes. We test the proposed citation-LDA on two datasets: the ACL Anthology Network(AAN) of natural language research literatures and PubMed Central(PMC) archive of biomedical and life sciences literatures, and demonstrate that Citation-LDA can effectively discover the evolution of research themes, with better formed topics than (conventional) Content-LDA.

#index 1991898
#* Psychological advertising: exploring user psychology for click prediction in sponsored search
#@ Taifeng Wang;Jiang Bian;Shusen Liu;Yuyu Zhang;Tie-Yan Liu
#t 2013
#c 0
#% 211044
#% 956546
#% 987209
#% 987361
#% 990182
#% 1055694
#% 1074101
#% 1214728
#% 1246499
#% 1355051
#% 1587856
#% 1642194
#% 1872324
#! Precise click prediction is one of the key components in the sponsored search system. Previous studies usually took advantage of two major kinds of information for click prediction, i.e., relevance information representing the similarity between ads and queries and historical click-through information representing users' previous preferences on the ads. These existing works mainly focused on interpreting ad clicks in terms of what users seek (i.e., relevance information) and how users choose to click (historically clicked-through information). However, few of them attempted to understand why users click the ads. In this paper, we aim at answering this ``why'' question. In our opinion, users click those ads that can convince them to take further actions, and the critical factor is if those ads can trigger users' desires in their hearts. Our data analysis on a commercial search engine reveals that specific text patterns, e.g., ``official site'', ``$x\%$ off'', and ``guaranteed return in $x$ days'', are very effective in triggering users' desires, and therefore lead to significant differences in terms of click-through rate (CTR). These observations motivate us to systematically model user psychological desire in order for a precise prediction on ad clicks. To this end, we propose modeling user psychological desire in sponsored search according to Maslow's desire theory, which categorizes psychological desire into five levels and each one is represented by a set of textual patterns automatically mined from ad texts. We then construct novel features for both ads and users based on our definition on psychological desire and incorporate them into the learning framework of click prediction. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that this approach can result in significant improvement in terms of click prediction accuracy, for both the ads with rich historical data and those with rare one. Further analysis reveals that specific pattern combinations are especially effective in driving click-through rates, which provides a good guideline for advertisers to improve their ad textual descriptions.

#index 1991899
#* Model-based kernel for efficient time series analysis
#@ Huanhuan Chen;Fengzhen Tang;Peter Tino;Xin Yao
#t 2013
#c 0
#% 345829
#% 469390
#% 743284
#% 771841
#% 812413
#% 902656
#% 1044339
#% 1558464
#% 1806620
#% 1835465
#! We present novel, efficient, model based kernels for time series data rooted in the reservoir computation framework. The kernels are implemented by fitting reservoir models sharing the same fixed deterministically constructed state transition part to individual time series. The proposed kernels can naturally handle time series of different length without the need to specify a parametric model class for the time series. Compared with most time series kernels, our kernels are computationally efficient. We show how the model distances used in the kernel can be calculated analytically or efficiently estimated. The experimental results on synthetic and benchmark time series classification tasks confirm the efficiency of the proposed kernel in terms of both generalization accuracy and computational speed. This paper also investigates on-line reservoir kernel construction for extremely long time series.

#index 1991900
#* On the equivalent of low-rank linear regressions and linear discriminant analysis based regressions
#@ Xiao Cai;Chris Ding;Feiping Nie;Heng Huang
#t 2013
#c 0
#% 80995
#% 235342
#% 593593
#% 875980
#% 961223
#% 983940
#% 1214755
#% 1688445
#% 1872287
#% 1878648
#% 1885652
#! The low-rank regression model has been studied and applied to capture the underlying classes/tasks correlation patterns, such that the regression/classification results can be enhanced. In this paper, we will prove that the low-rank regression model is equivalent to doing linear regression in the linear discriminant analysis (LDA) subspace. Our new theory reveals the learning mechanism of low-rank regression, and shows that the low-rank structures exacted from classes/tasks are connected to the LDA projection results. Thus, the low-rank regression efficiently works for the high-dimensional data. Moreover, we will propose new discriminant low-rank ridge regression and sparse low-rank regression methods. Both of them are equivalent to doing regularized regression in the regularized LDA subspace. These new regularized objectives provide better data mining results than existing low-rank regression in both theoretical and empirical validations. We evaluate our discriminant low-rank regression methods by six benchmark datasets. In all empirical results, our discriminant low-rank models consistently show better results than the corresponding full-rank methods.

#index 1991901
#* LAFT-Explorer: inferring, visualizing and predicting how your social network expands
#@ Jun Zhang;Chaokun Wang;Yuanchi Ning;Yichi Liu;Jianmin Wang;Philip S. Yu
#t 2013
#c 0
#% 730089
#% 769952
#% 1083675
#% 1482199
#% 1948184
#% 1988789
#! The study of social network evolution has attracted many attentions from both the industry and academia. In this paper we demonstrate LaFT-Explorer, a general toolkit for explaining and reproducing the network growth process based on the friendship propagation. LaFT-Explorer presents multiple perspectives for analyzing the network evolution process and structure, including LaFT-Tree, LaFT-Trace and LaFT-Flow. Upon that we build LaFT-Rec, a new visualized interactive friend recommendation service based on the friendship propagation. LaFT-Rec not only shows whom one may make friends with, but also tells the user that why you should make friends with him and how you can reach him. We demonstrate our system built upon the academic social network of DBLP.

#index 1991902
#* Understanding Twitter data with TweetXplorer
#@ Fred Morstatter;Shamanth Kumar;Huan Liu;Ross Maciejewski
#t 2013
#c 0
#% 122797
#% 436620
#% 1384210
#% 1512931
#! In the era of big data it is increasingly difficult for an analyst to extract meaningful knowledge from a sea of information. We present TweetXplorer, a system for analysts with little information about an event to gain knowledge through the use of effective visualization techniques. Using tweets collected during Hurricane Sandy as an example, we will lead the reader through a workflow that exhibits the functionality of the system.

#index 1991903
#* JobMiner: a real-time system for mining job-related patterns from social media
#@ Yu Cheng;Yusheng Xie;Zhengzhang Chen;Ankit Agrawal;Alok Choudhary;Songtao Guo
#t 2013
#c 0
#% 1482198
#% 1538537
#% 1689750
#% 1877985
#% 1881271
#! The various kinds of booming social media not only provide a platform where people can communicate with each other, but also spread useful domain information, such as career and job market information. For example, LinkedIn publishes a large amount of messages either about people who want to seek jobs or companies who want to recruit new members. By collecting information, we can have a better understanding of the job market and provide insights to job-seekers, companies and even decision makers. In this paper, we analyze the job information from the social network point of view. We first collect the job-related information from various social media sources. Then we construct an inter-company job-hopping network, with the vertices denoting companies and the edges denoting flow of personnel between companies. We subsequently employ graphmining techniques to mine influential companies and related company groups based on the job-hopping network model. Demonstration on LinkedIn data shows that our system JobMiner can provide a better understanding of the dynamic processes and a more accurate identification of important entities in the job market.

#index 1991904
#* LAICOS: an open source platform for personalized social web search
#@ Mohamed Reda Bouadjenek;Hakim Hacid;Mokrane Bouzeghoub
#t 2013
#c 0
#% 399057
#% 590523
#% 869548
#% 956544
#% 1074070
#% 1227647
#% 1292590
#% 1409929
#% 1418196
#% 1499606
#% 1598457
#% 1697448
#% 1988890
#% 1988891
#! In this paper, we introduce LAICOS, a social Web search engine as a contribution to the growing area of Social Information Retrieval (SIR). Social information and personalization are at the heart of LAICOS. On the one hand, the social context of documents is added as a layer to their textual content traditionally used for indexing to provide Personalized Social Document Representations. On the other hand, the social context of users is used for the query expansion process using the Personalized Social Query Expansion framework (PSQE) proposed in our earlier works. We describe the different components of the system while relying on social bookmarking systems as a source of social information for personalizing and enhancing the IR process. We show how the internal structure of indexes as well as the query expansion process operated using social information.

#index 1991905
#* When TEDDY meets GrizzLY: temporal dependency discovery for triggering road deicing operations
#@ Céline Robardet;Vasile-Marian Scuturici;Marc Plantevit;Antoine Fraboulet
#t 2013
#c 0
#! Temporal dependencies between multiple sensor data sources link two types of events if the occurrence of one is repeatedly followed by the appearance of the other in a certain time interval. TEDDY algorithm aims at discovering such dependencies, identifying the statically significant time intervals with a chi2 test. We present how these dependencies can be used within the GrizzLY project to tackle an environmental and technical issue: the deicing of the roads. This project aims to wisely organize the deicing operations of an urban area, based on several sensor network measures of local atmospheric phenomena. A spatial and temporal dependency-based model is built from these data to predict freezing alerts.

#index 1991906
#* Inferring distant-time location in low-sampling-rate trajectories
#@ Meng-Fen Chiang;Yung-Hsiang Lin;Wen-Chih Peng;Philip S. Yu
#t 2013
#c 0
#% 273890
#% 1206625
#% 1214685
#% 1456850
#% 1459153
#% 1480783
#% 1846708
#% 1927869
#! With the growth of location-based services and social services, low- sampling-rate trajectories from check-in data or photos with geo- tag information becomes ubiquitous. In general, most detailed mov- ing information in low-sampling-rate trajectories are lost. Prior works have elaborated on distant-time location prediction in high- sampling-rate trajectories. However, existing prediction models are pattern-based and thus not applicable due to the sparsity of data points in low-sampling-rate trajectories. To address the sparsity in low-sampling-rate trajectories, we develop a Reachability-based prediction model on Time-constrained Mobility Graph (RTMG) to predict locations for distant-time queries. Specifically, we de- sign an adaptive temporal exploration approach to extract effective supporting trajectories that are temporally close to the query time. Based on the supporting trajectories, a Time-constrained mobility Graph (TG) is constructed to capture mobility information at the given query time. In light of TG, we further derive the reacha- bility probabilities among locations in TG. Thus, a location with maximum reachability from the current location among all possi- ble locations in supporting trajectories is considered as the predic- tion result. To efficiently process queries, we proposed the index structure Sorted Interval-Tree (SOIT) to organize location records. Extensive experiments with real data demonstrated the effective- ness and efficiency of RTMG. First, RTMG with adaptive tempo- ral exploration significantly outperforms the existing pattern-based prediction model HPM [2] over varying data sparsity in terms of higher accuracy and higher coverage. Also, the proposed index structure SOIT can efficiently speedup RTMG in large-scale trajec- tory dataset. In the future, we could extend RTMG by considering more factors (e.g., staying durations in locations, application us- ages in smart phones) to further improve the prediction accuracy.

#index 1991907
#* A transfer learning based framework of crowd-selection on twitter
#@ Zhou Zhao;Da Yan;Wilfred Ng;Shi Gao
#t 2013
#c 0
#% 1083692
#% 1269755
#% 1472273
#% 1746898
#% 1869838
#% 1872257
#% 1880464
#% 1962380
#! Crowd selection is essential to crowd sourcing applications, since choosing the right workers with particular expertise to carry out crowdsourced tasks is extremely important. The central problem is simple but tricky: given a crowdsourced task, who are the most knowledgable users to ask? In this demo, we show our framework that tackles the problem of crowdsourced task assignment on Twitter according to the social activities of its users. Since user profiles on Twitter do not reveal user interests and skills, we transfer the knowledge from categorized Yahoo! Answers datasets for learning user expertise. Then, we select the right crowd for certain tasks based on user expertise. We study the effectiveness of our system using extensive user evaluation. We further engage the attendees to participate a game called--Whom to Ask on Twitter?. This helps understand our ideas in an interactive manner. Our crowd selection can be accessed by the following url http://webproject2.cse.ust.hk:8034/tcrowd/.

#index 1991908
#* Real-time disease surveillance using Twitter data: demonstration on flu and cancer
#@ Kathy Lee;Ankit Agrawal;Alok Choudhary
#t 2013
#c 0
#! Social media is producing massive amounts of data on an unprecedented scale. Here people share their experiences and opinions on various topics, including personal health issues, symptoms, treatments, side-effects, and so on. This makes publicly available social media data an invaluable resource for mining interesting and actionable healthcare insights. In this paper, we describe a novel real-time flu and cancer surveillance system that uses spatial, temporal, and text mining on Twitter data. The real-time analysis results are reported visually in terms of US disease surveillance maps, distribution and timelines of disease types, symptoms, and treatments, in addition to overall disease activity timelines on our project website. Our surveillance system can be very useful not only for early prediction of seasonal disease outbreaks such as flu, but also for monitoring distribution of cancer patients with different cancer types and symptoms in each state and the popularity of treatments used. The resulting insights are expected to help facilitate faster response to and preparation for epidemics and also be very useful for both patients and doctors to make more informed decisions.

#index 1991909
#* Forex-foreteller: currency trend modeling using news articles
#@ Fang Jin;Nathan Self;Parang Saraf;Patrick Butler;Wei Wang;Naren Ramakrishnan
#t 2013
#c 0
#% 722904
#% 1482254
#! Financial markets are quite sensitive to unanticipated news and events. Identifying the effect of news on the market is a challenging task. In this demo, we present Forex-foreteller (FF) which mines news articles and makes forecasts about the movement of foreign currency markets. The system uses a combination of language models, topic clustering, and sentiment analysis to identify relevant news articles. These articles along with the historical stock index and currency exchange values are used in a linear regression model to make forecasts. The system has an interactive visualizer designed specifically for touch-sensitive devices which depicts forecasts along with the chronological news events and financial data used for making the forecasts.

#index 1991910
#* KeySee: supporting keyword search on evolving events in social streams
#@ Pei Lee;Laks V.S. Lakshmanan;Evangelos Milios
#t 2013
#c 0
#% 1567974
#% 1573368
#% 1581900
#% 1581966
#% 1598538
#% 1872363
#! Online social streams such as Twitter/Facebook timelines and forum discussions have emerged as prevalent channels for information dissemination. As these social streams surge quickly, information overload has become a huge problem. Existing keyword search engines on social streams like Twitter Search are not successful in overcoming the problem, because they merely return an overwhelming list of posts, with little aggregation or semantics. In this demo, we provide a new solution called \keysee by grouping posts into events, and track the evolution patterns of events as new posts stream in and old posts fade out. Noise and redundancy problems are effectively addressed in our system. Our demo supports refined keyword query on evolving events by allowing users to specify the time span and designated evolution pattern. For each event result, we provide various analytic views such as frequency curves, word clouds and GPS distributions. We deploy \keysee on real Twitter streams and the results show that our demo outperforms existing keyword search engines on both quality and usability.

#index 1991911
#* STED: semi-supervised targeted-interest event detectionin in twitter
#@ Ting Hua;Feng Chen;Liang Zhao;Chang-Tien Lu;Naren Ramakrishnan
#t 2013
#c 0
#! Social microblogs such as Twitter and Weibo are experiencing an explosive growth with billions of global users sharing their daily observations and thoughts. Beyond public interests (e.g., sports, music), microblogs can provide highly detailed information for those interested in public health, homeland security, and financial analysis. However, the language used in Twitter is heavily informal, ungrammatical, and dynamic. Existing data mining algorithms require extensive manually labeling to build and maintain a supervised system. This paper presents STED, a semi-supervised system that helps users to automatically detect and interactively visualize events of a targeted type from twitter, such as crimes, civil unrests, and disease outbreaks. Our model first applies transfer learning and label propagation to automatically generate labeled data, then learns a customized text classifier based on mini-clustering, and finally applies fast spatial scan statistics to estimate the locations of events. We demonstrate STED's usage and benefits using twitter data collected from Latin America countries, and show how our system helps to detect and track example events such as civil unrests and crimes.

#index 1991912
#* A tool for collecting provenance data in social media
#@ Pritam Gundecha;Suhas Ranganath;Zhuo Feng;Huan Liu
#t 2013
#c 0
#% 1080356
#% 1190108
#% 1605970
#% 1917435
#! In recent years, social media sites have provided a large amount of information. Recipients of such information need mechanisms to know more about the received information, including the provenance. Previous research has shown that some attributes related to the received information provide additional context, so that a recipient can assess the amount of value, trust, and validity to be placed in the received information. Personal attributes of a user, including name, location, education, ethnicity, gender, and political and religious affiliations, can be found in social media sites. In this paper, we present a novel web-based tool for collecting the attributes of interest associated with a particular social media user related to the received information. This tool provides a way to combine different attributes available at different social media sites into a single user profile. Using different types of Twitter users, we also evaluate the performance of the tool in terms of number of attribute values collected, validity of these values, and total amount of retrieval time.

#index 1991913
#* FIU-Miner: a fast, integrated, and user-friendly system for data mining in distributed environment
#@ Chunqiu Zeng;Yexi Jiang;Li Zheng;Jingxuan Li;Lei Li;Hongtai Li;Chao Shen;Wubai Zhou;Tao Li;Bing Duan;Ming Lei;Pengnian Wang
#t 2013
#c 0
#% 814023
#% 835018
#% 928386
#% 1301004
#% 1701031
#% 1872340
#% 1872411
#! The advent of Big Data era drives data analysts from different domains to use data mining techniques for data analysis. However, performing data analysis in a specific domain is not trivial; it often requires complex task configuration, onerous integration of algorithms, and efficient execution in distributed environments.Few efforts have been paid on developing effective tools to facilitate data analysts in conducting complex data analysis tasks. In this paper, we design and implement FIU-Miner, a Fast, Integrated, and User-friendly system to ease data analysis. FIU-Miner allows users to rapidly configure a complex data analysis task without writing a single line of code. It also helps users conveniently import and integrate different analysis programs. Further, it significantly balances resource utilization and task execution in heterogeneous environments. A case study of a real-world application demonstrates the efficacy and effectiveness of our proposed system.

#index 1991914
#* An online system with end-user services: mining novelty concepts from tv broadcast subtitles
#@ Mika Rautiainen;Jouni Sarvanko;Arto Heikkinen;Mika Ylianttila;Vassilis Kostakos
#t 2013
#c 0
#% 731721
#% 939867
#% 1256692
#! Better tools for content-based access of video are needed to improve access to time-continuous video data. Particularly information about linear TV broadcast programs has been available in a form limited to program guides that provide short manually described overviews of the program content. Recent development in digitalization of TV broadcasting and emergence of web-based services for catch-up and on-demand viewing bring out new possibilities to access data. In this paper we introduce our data mining system and accompanying services for summarizing Finnish DVB broadcast streams from seven national channels. We describe how data mining of novelty concepts can be extracted from DVB subtitles to augment web-based "Catch-Up TV Guide" and "Novelty Cloud" TV services. Furthermore, our system allows accessing media fragments as Picture Quotes via generated word lists and provides content-based recommendations to find new programs that have content similar to the user selected programs. Our index consists of over 180 000 programs that are used to recommend relevant programs. The service has been under development and available online since 2010. It has registered over 5000 user sessions.

#index 1991915
#* AMETHYST: a system for mining and exploring topical hierarchies of heterogeneous data
#@ Marina Danilevsky;Chi Wang;Fangbo Tao;Son Nguyen;Gong Chen;Nihit Desai;Lidan Wang;Jiawei Han
#t 2013
#c 0
#% 1055681
#% 1083734
#% 1606070
#% 1872226
#% 1872239
#% 1991831
#! In this demo we present AMETHYST, a system for exploring and analyzing a topical hierarchy constructed from a heterogeneous information network (HIN). HINs, composed of multiple types of entities and links are very common in the real world. Many have a text component, and thus can benefit from a high quality hierarchical organization of the topics in the network dataset. By organizing the topics into a hierarchy, AMETHYST helps understand search results in the context of an ontology, and explain entity relatedness at different granularities. The automatically constructed topical hierarchy reflects a domain-specific ontology, interacts with multiple types of linked entities, and can be tailored for both free text and OLAP queries.

#index 1991916
#* Risk-O-Meter: an intelligent clinical risk calculator
#@ Kiyana Zolfaghar;Jayshree Agarwal;Deepthi Sistla;Si-Chi Chin;Senjuti Basu Roy;Nele Verbiest
#t 2013
#c 0
#% 316709
#% 1418196
#! We present a system called Risk-O-Meter to predict and an- alyze clinical risk via data imputation, visualization, predic- tive modeling, and association rule exploration. Clinical risk calculators provide information about a person's chance of having a disease or encountering a clinical event. Such tools could be highly useful to educate patients to understand and monitor their health conditions. Unlike existing risk calcu- lators that are primarily designed for domain experts, Risk- O-Meter is useful to patients who are unfamiliar with medi- cal terminologies, or providers who have limited information about a patient. Risk-O-Meter is designed in a way such that it is flexible enough to accept limited or incomplete data in- puts, and still manages to predict the clinical risk efficiently and effectively. Current version of Risk-O-Meter evaluates 30-day risk of hospital readmission. However, the proposed system framework is applicable to general clinical risk pre- dictions. In this demonstration paper, we describe different components of Risk-O-Meter and the intelligent algorithms associated with each of these components to evaluate risk of readmission using incomplete patient data inputs.

#index 1991917
#* EventCube: multi-dimensional search and mining of structured and text data
#@ Fangbo Tao;Kin Hou Lei;Jiawei Han;Chengxiang Zhai;Xiao Cheng;Marina Danilevsky;Nihit Desai;Bolin Ding;Jing Ge Ge;Heng Ji;Rucha Kanade;Anne Kao;Qi Li;Yanen Li;Cindy Lin;Jialu Liu;Nikunj Oza;Ashok Srivastava;Rod Tjoelker;Chi Wang;Duo Zhang;Bo Zhao
#t 2013
#c 0
#% 881529
#% 1176884
#% 1314743
#% 1642116
#% 1646322
#% 1991831
#! A large portion of real world data is either text or structured (e.g., relational) data. Moreover, such data objects are often linked together (e.g., structured specification of products linking with the corresponding product descriptions and customer comments). Even for text data such as news data, typed entities can be extracted with entity extraction tools. The EventCube project constructs TextCube and TopicCube from interconnected structured and text data (or from text data via entity extraction and dimension building), and performs multidimensional search and analysis on such datasets, in an informative, powerful, and user-friendly manner. This proposed EventCube demo will show the power of the system not only on the originally designed ASRS (Aviation Safety Report System) data sets, but also on news datasets collected from multiple news agencies, and academic datasets constructed from the DBLP and web data. The system has high potential to be extended in many powerful ways and serve as a general platform for search, OLAP (online analytical processing) and data mining on integrated text and structured data. After the system demo in the conference, the system will be put on the web for public access and evaluation.

#index 1991918
#* SEA: a system for event analysis on chinese tweets
#@ Yaqiong Wang;Hongfu Liu;Hao Lin;Junjie Wu;Zhiang Wu;Jie Cao
#t 2013
#c 0
#! Recent years have witnessed the explosive growth of online social media. Weibo, a famous "Chinese Twitter", has attracted over 0.5 billion users in less than four years, with more than 1000 tweets generated in every second. These tweets are informative but very fragmented, and thus would be better archived from an event perspective, as done by Weibo itself in the "Micro-Topic" program. This effort, however, is yet far from satisfaction for not providing enough analytical power to events. In light of this, in this demo paper, we propose SEA, a System for Event Analysis on Chinese tweets. In general, SEA is an event-centric, multi-functional platform that conducts panoramic analysis on Weibo events from various aspects, including the semantic information of the events, the temporal and spatial trends, the public sentiments, the hidden sub-events, the key users in the event diffusion and their preferences, etc. These functions are enabled by the integration of various analytical models and by the noSQL techniques adopted purposefully for massive tweets management. Finally, a case study on the "Spring Festival" event demonstrates the effectiveness of SEA. To our best knowledge, SEA is the first third-party system that provides panoramic analysis to Weibo events.

#index 1991919
#* SAE: social analytic engine for large networks
#@ Yang Yang;Jianfei Wang;Yutao Zhang;Wei Chen;Jing Zhang;Honglei Zhuang;Zhilin Yang;Bo Ma;Zhanpeng Fang;Sen Wu;Xiaoxiao Li;Debing Liu;Jie Tang
#t 2013
#c 0
#% 1083734
#% 1214702
#% 1451159
#% 1482198
#% 1617365
#% 1642046
#% 1693935
#% 1872393
#% 1919752
#% 1992461
#! Online social networks become a bridge to connect our physical daily life and the virtual Web space, which not only provides rich data for mining, but also brings many new challenges. In this paper, we present a novel Social Analytic Engine (SAE) for large online social networks. The key issues we pursue in the analytic engine are concerned with the following problems: 1) at the micro-level, how do people form different types of social ties and how people influence each other? 2) at the meso-level, how do people group into communities? 3) at the macro-level, what are the hottest topics in a social network and how the topics evolve over time? We propose methods to address the above questions. The methods are general and can be applied to various social networking data. We have deployed and validated the proposed analytic engine over multiple different networks and validated the effectiveness and efficiency of the proposed methods.

#index 1992292
#* U-Air: when urban air quality inference meets big data
#@ Yu Zheng;Furui Liu;Hsun-Ping Hsieh
#t 2013
#c 0
#% 316509
#% 464434
#% 1445728
#% 1605948
#% 1613884
#% 1613904
#% 1872249
#% 2010305
#! Information about urban air quality, e.g., the concentration of PM2.5, is of great importance to protect human health and control air pollution. While there are limited air-quality-monitor-stations in a city, air quality varies in urban spaces non-linearly and depends on multiple factors, such as meteorology, traffic volume, and land uses. In this paper, we infer the real-time and fine-grained air quality information throughout a city, based on the (historical and real-time) air quality data reported by existing monitor stations and a variety of data sources we observed in the city, such as meteorology, traffic flow, human mobility, structure of road networks, and point of interests (POIs). We propose a semi-supervised learning approach based on a co-training framework that consists of two separated classifiers. One is a spatial classifier based on an artificial neural network (ANN), which takes spatially-related features (e.g., the density of POIs and length of highways) as input to model the spatial correlation between air qualities of different locations. The other is a temporal classifier based on a linear-chain conditional random field (CRF), involving temporally-related features (e.g., traffic and meteorology) to model the temporal dependency of air quality in a location. We evaluated our approach with extensive experiments based on five real data sources obtained in Beijing and Shanghai. The results show the advantages of our method over four categories of baselines, including linear/Gaussian interpolations, classical dispersion models, well-known classification models like decision tree and CRF, and ANN.

#index 1992293
#* Modeling and probabilistic reasoning of population evacuation during large-scale disaster
#@ Xuan Song;Quanshi Zhang;Yoshihide Sekimoto;Teerayut Horanont;Satoshi Ueyama;Ryosuke Shibasaki
#t 2013
#c 0
#% 363744
#% 835018
#% 1089823
#% 1270316
#% 1426523
#% 1621337
#% 1872249
#% 1872250
#! The Great East Japan Earthquake and the Fukushima nuclear accident cause large human population movements and evacuations. Understanding and predicting these movements is critical for planning effective humanitarian relief, disaster management, and long-term societal reconstruction. In this paper, we construct a large human mobility database that stores and manages GPS records from mobile devices used by approximately 1.6 million people throughout Japan from 1 August 2010 to 31 July 2011. By mining this enormous set of Auto-GPS mobile sensor data, the short-term and long-term evacuation behaviors for individuals throughout Japan during this disaster are able to be automatically discovered. To better understand and simulate human mobility during the disasters, we develop a probabilistic model that is able to be effectively trained by the discovered evacuations via machine learning technique. Based on our training model, population mobility in various cities impacted by the disasters throughout the country is able to be automatically simulated or predicted. On the basis of the whole database, developed model, and experimental results, it is easy for us to find some new features or population mobility patterns after the recent severe earthquake, tsunami and release of radioactivity in Japan, which are likely to play a vital role in future disaster relief and management worldwide.

#index 1992294
#* Financing lead triggers: empowering sales reps through knowledge discovery and fusion
#@ Kareem S. Aggour;Bethany Hoogs
#t 2013
#c 0
#% 864488
#! Sales representatives must have access to meaningful and actionable intelligence about potential customers to be effective in their roles. Historically, GE Capital Americas sales reps identified leads by manually searching through news reports and financial statements either in print or online. Here we describe a system built to automate the collection and aggregation of information on companies, which is then mined to identify actionable sales leads. The Financing Lead Triggers system is comprised of three core components that perform information fusion, knowledge discovery and information visualization. Together these components extract raw data from disparate sources, fuse that data into information, and then automatically mine that information for actionable sales leads driven by a combination of expert-defined and statistically derived triggers. A web-based interface provides sales reps access to the company information and sales leads in a single location. The use of the Lead Triggers system has significantly improved the performance of the sales reps, providing them with actionable intelligence that has improved their productivity by 30-50%. In 2010, Lead Triggers provided leads on opportunities that represented over $44B in new deal commitments for GE Capital.

#index 1992295
#* Assessing team strategy using spatiotemporal data
#@ Patrick Lucey;Dean Oliver;Peter Carr;Joe Roth;Iain Matthews
#t 2013
#c 0
#% 283197
#% 1149114
#% 1187505
#% 1750454
#% 1775814
#% 1923376
#! The "Moneyball" revolution coincided with a shift in the way professional sporting organizations handle and utilize data in terms of decision making processes. Due to the demand for better sports analytics and the improvement in sensor technology, there has been a plethora of ball and player tracking information generated within professional sports for analytical purposes. However, due to the continuous nature of the data and the lack of associated high-level labels to describe it - this rich set of information has had very limited use especially in the analysis of a team's tactics and strategy. In this paper, we give an overview of the types of analysis currently performed mostly with hand-labeled event data and highlight the problems associated with the influx of spatiotemporal data. By way of example, we present an approach which uses an entire season of ball tracking data from the English Premier League (2010-2011 season) to reinforce the common held belief that teams should aim to "win home games and draw away ones". We do this by: i) forming a representation of team behavior by chunking the incoming spatiotemporal signal into a series of quantized bins, and ii) generate an expectation model of team behavior based on a code-book of past performances. We show that home advantage in soccer is partly due to the conservative strategy of the away team. We also show that our approach can flag anomalous team behavior which has many potential applications.

#index 1992296
#* Improving quality control by early prediction of manufacturing outcomes
#@ Sholom M. Weiss;Amit Dhurandhar;Robert J. Baseman
#t 2013
#c 0
#% 73372
#% 310492
#% 444865
#% 738972
#% 1214744
#% 1287233
#% 1312141
#% 1483782
#! We describe methods for continual prediction of manufactured product quality prior to final testing. In our most expansive modeling approach, an estimated final characteristic of a product is updated after each manufacturing operation. Our initial application is for the manufacture of microprocessors, and we predict final microprocessor speed. Using these predictions, early corrective manufacturing actions may be taken to increase the speed of expected slow wafers (a collection of microprocessors) or reduce the speed of fast wafers. Such predictions may also be used to initiate corrective supply chain management actions. Developing statistical learning models for this task has many complicating factors: (a) a temporally unstable population (b) missing data that is a result of sparsely sampled measurements and (c) relatively few available measurements prior to corrective action opportunities. In a real manufacturing pilot application, our automated models selected 125 fast wafers in real-time. As predicted, those wafers were significantly faster than average. During manufacture, downstream corrective processing restored 25 nominally unacceptable wafers to normal operation.

#index 1992297
#* Analysis of advanced meter infrastructure data of water consumption in apartment buildings
#@ Einat Kermany;Hanna Mazzawi;Dorit Baras;Yehuda Naveh;Hagai Michaelis
#t 2013
#c 0
#% 729437
#% 781774
#% 894552
#% 1132735
#% 1270561
#% 1590705
#% 1605939
#% 1799638
#! We present our experience of using machine learning techniques over data originating from advanced meter infrastructure (AMI) systems for water consumption in a medium-size city. We focus on two new use cases that are of special importance to city authorities. One use case is the automatic identification of malfunctioning meters, with a focus on distinguishing them from legitimate non-consumption such as during periods when the household residents are on vacation. The other use case is the identification of leaks or theft in the unmetered common areas of apartment buildings. These two use cases are highly important to city authorities both because of the lost revenue they imply and because of the hassle to the residents in cases of delayed identification. Both cases are inherently complex to analyze and require advanced data mining techniques in order to achieve high levels of correct identification. Our results provide for faster and more accurate detection of malfunctioning meters as well as leaks in the common areas. This results in significant tangible value to the authorities in terms of increase in technician efficiency and a decrease in the amount of wasted, non-revenue, water.

#index 1992298
#* Mining for geographically disperse communities in social networks by leveraging distance modularity
#@ Paulo Shakarian;Patrick Roos;Devon Callahan;Cory Kirk
#t 2013
#c 0
#% 1034723
#% 1040833
#% 1041778
#% 1214714
#% 1606049
#% 2000021
#! Social networks where the actors occupy geospatial locations are prevalent in military, intelligence, and policing operations such as counter-terrorism, counter-insurgency, and combating organized crime. These networks are often derived from a variety of intelligence sources. The discovery of communities that are geographically disperse stems from the requirement to identify higher-level organizational structures, such as a logistics group that provides support to various geographically disperse terrorist cells. We apply a variant of Newman-Girvan modularity to this problem known as distance modularity. To address the problem of finding geographically disperse communities, we modify the well-known Louvain algorithm to find partitions of networks that provide near-optimal solutions to this quantity. We apply this algorithm to numerous samples from two real-world social networks and a terrorism network data set whose nodes have associated geospatial locations. Our experiments show this to be an effective approach and highlight various practical considerations when applying the algorithm to distance modularity maximization. Several military, intelligence, and law-enforcement organizations are working with us to further test and field software for this emerging application.

#index 1992299
#* Exploratory analysis of highly heterogeneous document collections
#@ Arun S. Maiya;John P. Thompson;Francisco Loaiza-Lemos;Robert M. Rolfe
#t 2013
#c 0
#% 88045
#% 279755
#% 482511
#% 722904
#% 740900
#% 936928
#% 956649
#% 1077150
#% 1159864
#% 1254852
#% 1277964
#% 1292563
#% 1380097
#% 1451154
#% 1467778
#% 1872393
#% 1872424
#! We present an effective multifaceted system for exploratory analysis of highly heterogeneous document collections. Our system is based on intelligently tagging individual documents in a purely automated fashion and exploiting these tags in a powerful faceted browsing framework. Tagging strategies employed include both unsupervised and supervised approaches based on machine learning and natural language processing. As one of our key tagging strategies, we introduce the KERA algorithm (Keyword Extraction for Reports and Articles). KERA extracts topic-representative terms from individual documents in a purely unsupervised fashion and is revealed to be significantly more effective than state-of-the-art methods. Finally, we evaluate our system in its ability to help users locate documents pertaining to military critical technologies buried deep in a large heterogeneous sea of information.

#index 1992300
#* An integrated framework for suicide risk prediction
#@ Truyen Tran;Dinh Phung;Wei Luo;Richard Harvey;Michael Berk;Svetha Venkatesh
#t 2013
#c 0
#% 108340
#% 209021
#% 1872284
#! Suicide is a major concern in society. Despite of great attention paid by the community with very substantive medico-legal implications, there has been no satisfying method that can reliably predict the future attempted or completed suicide. We present an integrated machine learning framework to tackle this challenge. Our proposed framework consists of a novel feature extraction scheme, an embedded feature selection process, a set of risk classifiers and finally, a risk calibration procedure. For temporal feature extraction, we cast the patient's clinical history into a temporal image to which a bank of one-side filters are applied. The responses are then partly transformed into mid-level features and then selected in l1-norm framework under the extreme value theory. A set of probabilistic ordinal risk classifiers are then applied to compute the risk probabilities and further re-rank the features. Finally, the predicted risks are calibrated. Together with our Australian partner, we perform comprehensive study on data collected for the mental health cohort, and the experiments validate that our proposed framework outperforms risk assessment instruments by medical practitioners.

#index 1992301
#* Query clustering based on bid landscape for sponsored search auction optimization
#@ Ye Chen;Weiguo Liu;Jeonghee Yi;Anton Schwaighofer;Tak W. Yan
#t 2013
#c 0
#% 115608
#% 916785
#% 1584778
#% 1747572
#% 1872324
#! In sponsored search auctions, the auctioneer operates the marketplace by setting a number of auction parameters such as reserve prices for the task of auction optimization. The auction parameters may be set for each individual keyword, but the optimization problem becomes intractable since the number of keywords is in the millions. To reduce the dimensionality and generalize well, one wishes to cluster keywords or queries into meaningful groups, and set parameters at the keyword-cluster level. For auction optimization, keywords shall be deemed as interchangeable commodities with respect to their valuations from advertisers, represented as bid distributions or landscapes. Clustering keywords for auction optimization shall thus be based on their bid distributions. In this paper we present a formalism of clustering probability distributions, and its application to query clustering where each query is represented as a probability density of click-through rate (CTR) weighted bid and distortion is measured by KL divergence. We first derive a k-means variant for clustering Gaussian densities, which have a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalize a single Gaussian and are typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. The clustering algorithm has been deployed successfully into production, yielding significant improvement in revenue and clicks over the existing production system. While motivated by the specific setting of query clustering, the proposed clustering method is generally applicable to many real-world applications where an example is better characterized by a distribution than a finite-dimensional feature vector in Euclidean space as in the classical k-means.

#index 1992302
#* A unified search federation system based on online user feedback
#@ Luo Jie;Sudarshan Lamkhede;Rochit Sapra;Evans Hsu;Helen Song;Yi Chang
#t 2013
#c 0
#% 194246
#% 577224
#% 643012
#% 722906
#% 853543
#% 871302
#% 879581
#% 1061652
#% 1073970
#% 1074093
#% 1166523
#% 1190055
#% 1211829
#% 1227616
#% 1227617
#% 1250379
#% 1399999
#% 1482356
#% 1536576
#% 1560361
#% 1693908
#! Today's popular web search engines expand the search process beyond crawled web pages to specialized corpora ("verticals") like images, videos, news, local, sports, finance, shopping etc., each with its own specialized search engine. Search federation deals with problems of the selection of search engines to query and merging of their results into a single result set. Despite a few recent advances, the problem is still very challenging. First, due to the heterogeneous nature of different verticals, how the system merges the vertical results with the web documents to serve the user's information need is still an open problem. Moreover, the scale of the search engine and the increasing number of vertical properties requires a solution which is efficient and scaleable. In this paper, we propose a unified framework for the search federation problem. We model the search federation as a contextual bandit problem. The system uses reward as a proxy for user satisfaction. Given a query, our system predicts the expected reward for each vertical, then organizes the search result page (SERP) in a way which maximizes the total reward. Instead of relying on human judges, our system leverages implicit user feedback to learn the model. The method is efficient to implement and can be applied to verticals of different nature. We have successfully deployed the system to three different markets, and it handles multiple verticals in each market. The system is now serving hundreds of millions of queries live each day, and has improved user metrics considerably.

#index 1992303
#* iHR: an online recruiting system for Xiamen Talent Service Center
#@ Wenxing Hong;Lei Li;Tao Li;Wenfu Pan
#t 2013
#c 0
#% 297675
#% 345045
#% 535911
#% 835906
#% 847349
#% 939393
#% 958100
#% 981553
#% 983098
#% 987339
#% 989613
#% 1077150
#% 1176909
#% 1450837
#% 1470696
#% 1476471
#% 1482274
#% 1489892
#% 1598346
#% 1598524
#% 1606043
#% 1607074
#% 1625363
#% 1625392
#% 1625396
#% 1826412
#% 1918338
#! Online recruiting systems have gained immense attention in the wake of more and more job seekers searching jobs and enterprises finding candidates on the Internet. A critical problem in a recruiting system is how to maximally satisfy the desires of both job seekers and enterprises with reasonable recommendations or search results. In this paper, we investigate and compare various online recruiting systems from a product perspective. We then point out several key functions that help achieve a win-win situation between job seekers and enterprises for a successful recruiting system. Based on the observations and key functions, we design, implement and deploy a web-based application of recruiting system, named iHR, for Xiamen Talent Service Center. The system utilizes the latest advances in data mining and recommendation technologies to create a user-oriented service for a myriad of audience in job marketing community. Empirical evaluation and online user studies demonstrate the efficacy and effectiveness of our proposed system. Currently, iHR has been deployed at http://i.xmrc.com.cn/XMRCIntel.

#index 1992304
#* Ad click prediction: a view from the trenches
#@ H. Brendan McMahan;Gary Holt;D. Sculley;Michael Young;Dietmar Ebner;Julian Grady;Lan Nie;Todd Phillips;Eugene Davydov;Daniel Golovin;Sharat Chikkerur;Dan Liu;Martin Wattenberg;Arnar Mar Hrafnkelsson;Tom Boulos;Jeremy Kubica
#t 2013
#c 0
#% 276526
#% 307424
#% 312727
#% 322884
#% 840913
#% 889273
#% 956546
#% 1110366
#% 1190057
#% 1211829
#% 1232034
#% 1355048
#% 1355052
#% 1451140
#% 1451141
#% 1540225
#% 1693957
#% 1972746
#! Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.

#index 1992305
#* Palette power: enabling visual search through colors
#@ Anurag Bhardwaj;Atish Das Sarma;Wei Di;Raffay Hamid;Robinson Piramuthu;Neel Sundaresan
#t 2013
#c 0
#% 212690
#% 443864
#% 443889
#% 626324
#% 674162
#% 760805
#% 771025
#% 812418
#% 883972
#% 961270
#% 1058303
#% 1148464
#% 1180468
#% 1854918
#! With the explosion of mobile devices with cameras, online search has moved beyond text to other modalities like images, voice, and writing. For many applications like Fashion, image-based search offers a compelling interface as compared to text forms by better capturing the visual attributes. In this paper we present a simple and fast search algorithm that uses color as the main feature for building visual search. We show that low level cues such as color can be used to quantify image similarity and also to discriminate among products with different visual appearances. We demonstrate the effectiveness of our approach through a mobile shopping application\footnote{eBay Fashion App available at https://itunes.apple.com/us/app/ebay-fashion/id378358380?mt=8 and eBay image swatch is the feature indexing millions of real world fashion images}. Our approach outperforms several other state-of-the-art image retrieval algorithms for large scale image data.

#index 1992306
#* Why people hate your app: making sense of user feedback in a mobile app store
#@ Bin Fu;Jialiu Lin;Lei Li;Christos Faloutsos;Jason Hong;Norman Sadeh
#t 2013
#c 0
#% 722904
#% 769892
#% 875959
#% 1035590
#% 1035591
#% 1470600
#% 1561559
#% 1605862
#% 1633079
#% 1688551
#% 1746817
#% 1826458
#% 1872330
#% 1913277
#% 1913693
#% 1942741
#% 1978816
#! User review is a crucial component of open mobile app markets such as the Google Play Store. How do we automatically summarize millions of user reviews and make sense out of them? Unfortunately, beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. In this paper, we propose Wiscom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail. Our system is able to (a) discover inconsistencies in reviews; (b) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users' reviews evolve over time; and (c) provide valuable insights into the entire app market, identifying users' major concerns and preferences of different types of apps. Results using our techniques are reported on a 32GB dataset consisting of over 13 million user reviews of 171,493 Android apps in the Google Play Store. We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users.

#index 1992307
#* Heat pump detection from coarse grained smart meter data with positive and unlabeled learning
#@ Hongliang Fei;Younghun Kim;Sambit Sahu;Milind Naphade;Sanjay K. Mamidipalli;John Hutchinson
#t 2013
#c 0
#% 316560
#% 400847
#% 464641
#% 464888
#% 575972
#% 577235
#% 659936
#% 727883
#% 941426
#% 1083647
#% 1144481
#% 1176959
#% 1230681
#% 1414183
#% 1688467
#% 1923935
#! Recent advances in smart metering technology enable utility companies to have access to tremendous amount of smart meter data, from which the utility companies are eager to gain more insight about their customers. In this paper, we aim to detect electric heat pumps from coarse grained smart meter data for a heat pump marketing campaign. However, appliance detection is a challenging task, especially given a very low granularity and partial labeled even unlabeled data. Traditional methods install either a high granularity smart meter or sensors at every appliance, which is either too expensive or requires technical expertise. We propose a novel approach to detect heat pumps that utilizes low granularity smart meter data, prior sales data and weather data. In particular, motivated by the characteristics of heat pump consumption pattern, we extract novel features that are highly relevant to heat pump usage from smart meter data and weather data. Under the constraint that only a subset of heat pump users are available, we formalize the problem into a positive and unlabeled data classification and apply biased Support Vector Machine (BSVM) to our extracted features. Our empirical study on a real-world data set demonstrates the effectiveness of our method. Furthermore, our method has been deployed in a real-life setting where the partner electric company runs a targeted campaign for 292,496 customers. Based on the initial feedback, our detection algorithm can successfully detect substantial number of non-heat pump users who were identified heat pump users with the prior algorithm the company had used.

#index 1992308
#* A data mining driven risk profiling method for road asset management
#@ Daniel Emerson;Justin Z. Weligamage;Richi Nayak
#t 2013
#c 0
#% 446783
#% 448204
#% 1166160
#% 1497774
#% 1549882
#% 1642776
#% 1799491
#! Road surface skid resistance has been shown to have a strong relationship to road crash risk, however, applying the current method of using investigatory levels to identify crash prone roads is problematic as they may fail in identifying risky roads outside of the norm. The proposed method analyses a complex and formerly impenetrable volume of data from roads and crashes using data mining. This method rapidly identifies roads with elevated crash-rate, potentially due to skid resistance deficit, for investigation. A hypothetical skid resistance/crash risk curve is developed for each road segment, driven by the model deployed in a novel regression tree extrapolation method. The method potentially solves the problem of missing skid resistance values which occurs during network-wide crash analysis, and allows risk assessment of the major proportion of roads without skid resistance values.

#index 1992309
#* Knowledge discovery from massive healthcare claims data
#@ Varun Chandola;Sreenivas R. Sukumar;Jack C. Schryver
#t 2013
#c 0
#% 280413
#% 722904
#% 1077150
#% 1202160
#% 1212796
#% 1328066
#% 1428692
#% 1605949
#! he role of big data in addressing the needs of the present healthcare system in US and rest of the world has been echoed by government, private, and academic sectors. There has been a growing emphasis to explore the promise of big data analytics in tapping the potential of the massive healthcare data emanating from private and government health insurance providers. While the domain implications of such collaboration are well known, this type of data has been explored to a limited extent in the data mining community. The objective of this paper is two fold: first, we introduce the emerging domain of "big" healthcare claims data to the KDD community, and second, we describe the success and challenges that we encountered in analyzing this data using state of art analytics for massive data. Specifically, we translate the problem of analyzing healthcare data into some of the most well-known analysis problems in the data mining community, social network analysis, text mining, and temporal analysis and higher order feature construction, and describe how advances within each of these areas can be leveraged to understand the domain of healthcare. Each case study illustrates a unique intersection of data mining and healthcare with a common objective of improving the cost-care ratio by mining for opportunities to improve healthcare operations and reducing what seems to fall under fraud, waste, and abuse.

#index 1992310
#* A privacy preserving framework for managing vehicle data in road pricing systems
#@ Huayu Wu;Wee Siong Ng;Kian-Lee Tan;Wei Wu;Shili Xiang;Mingqiang Xue
#t 2013
#c 0
#% 300179
#% 915810
#% 993943
#% 1206727
#% 1206853
#% 1217240
#% 1425704
#! The Electronic Road Pricing (ERP) system was implemented by the Land Transport Authority of Singapore to control traffic by road pricing since 1998. To better understand the traffic condition and improve the pricing scheme, the government initiated the next generation ERP (ERP 2) project, which aims to use the Global Navigation Satellite System (GNSS) collecting positional data from vehicles for analysis. However, most drivers fear of being monitored once the government installs the devices in their vehicles to collect GPS data. The existing data stream management systems (DSMS) centralize both data management and privacy control at server site. This framework assumes DSMS server is secure and trustable, and protects providers' data from illegal access by data users. In ERP 2, the DSMS server is maintained by the government, i.e., data user. Thus, the existing framework is not adoptable. We propose a novel framework in which privacy protection is pushed to data provider site. By doing this, the system could be safer and more efficient. Our framework can be used for the situations such as ERP 2, i.e., data providers would like to control their own privacy policies and/or the workload of DSMS server needs to be reduced.

#index 1992311
#* Using co-visitation networks for detecting large scale online display advertising exchange fraud
#@ Ori Stitelman;Claudia Perlich;Brian Dalessandro;Rod Hook;Troy Raeder;Foster Provost
#t 2013
#c 0
#% 1214692
#% 1506248
#% 1646504
#% 1872325
#% 1872392
#! Data generated by observing the actions of web browsers across the internet is being used at an ever increasing rate for both building models and making decisions. In fact, a quarter of the industry-track papers for KDD in 2012 were based on data generated by online actions. The models, analytics and decisions they inform all stem from the assumption that observed data captures the intent of users. However, a large portion of these observed actions are not intentional, and are effectively polluting the models. Much of this observed activity is either generated by robots traversing the internet or the result of unintended actions of real users. These non-intentional actions observed in the web logs severely bias both analytics and the models created from the data. In this paper, we will show examples of how non-intentional traffic that is produced by fraudulent activities adversely affects both general analytics and predictive models, and propose an approach using co-visitation networks to identify sites that have large amounts of this fraudulent traffic. We will then show how this approach, along with a second stage classifier that identifies non-intentional traffic at the browser level, is deployed in production at Media6Degrees (m6d), a targeting technology company for display advertising. This deployed product acts both to filter out the fraudulent traffic from the input data and to insure that we don't serve ads during unintended website visits.

#index 1992312
#* Scalable supervised dimensionality reduction using clustering
#@ Troy Raeder;Claudia Perlich;Brian Dalessandro;Ori Stitelman;Foster Provost
#t 2013
#c 0
#% 208181
#% 316478
#% 482502
#% 850432
#% 876034
#% 1128929
#% 1211829
#% 1214692
#% 1260273
#% 1451160
#% 1464068
#% 1654518
#% 1781626
#% 1860500
#% 1872325
#% 1872392
#! The automated targeting of online display ads at scale requires the simultaneous evaluation of a single prospect against many independent models. When deciding which ad to show to a user, one must calculate likelihood-to-convert scores for that user across all potential advertisers in the system. For modern machine-learning-based targeting, as conducted by Media6Degrees (M6D), this can mean scoring against thousands of models in a large, sparse feature space. Dimensionality reduction within this space is useful, as it decreases scoring time and model storage requirements. To meet this need, we develop a novel algorithm for scalable supervised dimensionality reduction across hundreds of simultaneous classification tasks. The algorithm performs hierarchical clustering in the space of model parameters from historical models in order to collapse related features into a single dimension. This allows us to implicitly incorporate feature and label data across all tasks without operating directly in a massive space. We present experimental results showing that for this task our algorithm outperforms other popular dimensionality-reduction algorithms across a wide variety of ad campaigns, as well as production results that showcase its performance in practice.

#index 1992313
#* An integrated framework for optimizing automatic monitoring systems in large IT infrastructures
#@ Liang Tang;Tao Li;Larisa Shwartz;Florian Pinel;Genady Ya Grabarnik
#t 2013
#c 0
#% 210160
#% 406493
#% 473439
#% 569762
#% 646218
#% 729999
#% 820375
#% 821929
#% 823418
#% 835018
#% 889086
#% 893211
#% 978386
#% 989678
#% 1083670
#% 1176920
#% 1206765
#% 1238937
#% 1268047
#% 1271973
#% 1306057
#% 1426517
#% 1530552
#% 1535456
#% 1642004
#% 1872306
#! The competitive business climate and the complexity of IT environments dictate efficient and cost-effective service delivery and support of IT services. These are largely achieved by automating routine maintenance procedures, including problem detection, determination and resolution. System monitoring provides an effective and reliable means for problem detection. Coupled with automated ticket creation, it ensures that a degradation of the vital signs, defined by acceptable thresholds or monitoring conditions, is flagged as a problem candidate and sent to supporting personnel as an incident ticket. This paper describes an integrated framework for minimizing false positive tickets and maximizing the monitoring coverage for system faults. In particular, the integrated framework defines monitoring conditions and the optimal corresponding delay times based on an off-line analysis of historical alerts and incident tickets. Potential monitoring conditions are built on a set of predictive rules which are automatically generated by a rule-based learning algorithm with coverage, confidence and rule complexity criteria. These conditions and delay times are propagated as configurations into run-time monitoring systems. Moreover, a part of misconfigured monitoring conditions can be corrected according to false negative tickets that are discovered by another text classification algorithm in this framework. This paper also provides implementation details of a program product that uses this framework and shows some illustrative examples of successful results.

#index 1992314
#* Gaussian multiple instance learning approach for mapping the slums of the world using very high resolution imagery
#@ Ranga Raju Vatsavai
#t 2013
#c 0
#% 224755
#% 272527
#% 565537
#% 722985
#% 803771
#% 1150811
#% 1589340
#% 1608962
#% 1689764
#% 1775152
#% 1979073
#! In this paper, we present a computationally efficient algorithm based on multiple instance learning for mapping informal settlements (slums) using very high-resolution remote sensing imagery. From remote sensing perspective, informal settlements share unique spatial characteristics that distinguish them from other urban structures like industrial, commercial, and formal residential settlements. However, regular pattern recognition and machine learning methods, which are predominantly single-instance or per-pixel classifiers, often fail to accurately map the informal settlements as they do not capture the complex spatial patterns. To overcome these limitations we employed a multiple instance based machine learning approach, where groups of contiguous pixels (image patches) are modeled as generated by a Gaussian distribution. We have conducted several experiments on very high-resolution satellite imagery, representing four unique geographic regions across the world. Our method showed consistent improvement in accurately identifying informal settlements.

#index 1992315
#* Efficiently rewriting large multimedia application execution traces with few event sequences
#@ Christiane Kamdem Kengne;Leon Constantin Fopa;Alexandre Termier;Noha Ibrahim;Marie-Christine Rousset;Takashi Washio;Miguel Santana
#t 2013
#c 0
#% 786653
#% 982068
#% 1202657
#% 1451164
#% 1535347
#% 1582567
#% 1636520
#% 1688495
#% 1872237
#% 1872285
#% 1872309
#% 1978700
#% 1979118
#! The analysis of multimedia application traces can reveal important information to enhance program execution comprehension. However typical size of traces can be in gigabytes, which hinders their effective exploitation by application developers. In this paper, we study the problem of finding a set of sequences of events that allows a reduced-size rewriting of the original trace. These sequences of events, that we call blocks, can simplify the exploration of large execution traces by allowing application developers to see an abstraction instead of low-level events. The problem of computing such set of blocks is NP-hard and naive approaches lead to prohibitive running times that prevent analysing real world traces. We propose a novel algorithm that directly mines the set of blocks. Our experiments show that our algorithm can analyse real traces of up to two hours of video. We also show experimentally the quality of the set of blocks proposed, and the interest of the rewriting to understand actual trace data.

#index 1992316
#* Experience from hosting a corporate prediction market: benefits beyond the forecasts
#@ Thomas A. Montgomery;Paul M. Stieg;Michael J. Cavaretta;Paul E. Moraal
#t 2013
#c 0
#% 976955
#! Prediction markets are virtual stock markets used to gain insight and forecast events by leveraging the wisdom of crowds. Popularly applied in the public to cultural questions (election results, box-office returns), they have recently been applied by corporations to leverage employee knowledge and forecast answers to business questions (sales volumes, products and features, release timing). Determining whether to run a prediction market requires practical experience that is rarely described. Over the last few years, Ford Motor Company obtained practical experience by deploying one of the largest corporate prediction markets known. Business partners in the US, Europe, and South America provided questions on new vehicle features, sales volumes, take rates, pricing, and macroeconomic trends. We describe our experience, including both the strong and weak correlations found between predictions and real world results. Evaluating this methodology goes beyond prediction accuracy, however, since there are many side benefits. In addition to the predictions, we discuss the value of comments, stock price changes over time, the ability to overcome bureaucratic limits, and flexibly filling holes in corporate knowledge, enabling better decision making. We conclude with advice on running prediction markets, including writing good questions, market duration, motivating traders and protecting confidential information.

#index 1992317
#* Detecting insider threats in a real corporate database of computer usage activity
#@ Ted E. Senator;Henry G. Goldberg;Alex Memory;William T. Young;Brad Rees;Robert Pierce;Daniel Huang;Matthew Reardon;David A. Bader;Edmond Chow;Irfan Essa;Joshua Jones;Vinay Bettadapura;Duen Horng Chau;Oded Green;Oguz Kaya;Anita Zakrzewska;Erica Briscoe;Rudolph IV L. Mappus;Robert McColl;Lora Weiss;Thomas G. Dietterich;Alan Fern;Weng--Keen Wong;Shubhomoy Das;Andrew Emmott;Jed Irvine;Jay-Yoon Lee;Danai Koutra;Christos Faloutsos;Daniel Corkill;Lisa Friedland;Amanda Gentzel;David Jensen
#t 2013
#c 0
#% 277483
#% 551723
#% 844330
#% 961168
#% 1108867
#% 1231845
#% 1573362
#% 1865043
#% 1886802
#% 1906161
#% 1930652
#% 2010160
#% 2010581
#% 2010587
#% 2014187
#! This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations' information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users' computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.

#index 1992318
#* Empirical bayes model to combine signals of adverse drug reactions
#@ Rave Harpaz;William DuMouchel;Paea LePendu;Nigam H. Shah
#t 2013
#c 0
#! Data mining is a crucial tool for identifying risk signals of potential adverse drug reactions (ADRs). However, mining of ADR signals is currently limited to leveraging a single data source at a time. It is widely believed that combining ADR evidence from multiple data sources will result in a more accurate risk identification system. We present a methodology based on empirical Bayes modeling to combine ADR signals mined from ~5 million adverse event reports collected by the FDA, and healthcare data corresponding to 46 million patients' the main two types of information sources currently employed for signal detection. Based on four sets of test cases (gold standard), we demonstrate that our method leads to a statistically significant and substantial improvement in signal detection accuracy, averaging 40% over the use of each source independently, and an area under the ROC curve of 0.87. We also compare the method with alternative supervised learning approaches, and argue that our approach is preferable as it does not require labeled (training) samples whose availability is currently limited. To our knowledge, this is the first effort to combine signals from these two complementary data sources, and to demonstrate the benefits of a computationally integrative strategy for drug safety surveillance.

#index 1992319
#* Predictive model performance: offline and online evaluations
#@ Jeonghee Yi;Ye Chen;Jie Li;Swaraj Sett;Tak W. Yan
#t 2013
#c 0
#% 309095
#% 310519
#% 466086
#% 466733
#% 769882
#% 875974
#% 956546
#% 1040857
#% 1055694
#% 1055713
#% 1095876
#% 1131448
#% 1490607
#% 1560361
#% 1560395
#% 1693890
#% 1693891
#% 1693911
#% 1747005
#! We study the accuracy of evaluation metrics used to estimate the efficacy of predictive models. Offline evaluation metrics are indicators of the expected model performance on real data. However, in practice we often experience substantial discrepancy between the offline and online performance of the models. We investigate the characteristics and behaviors of the evaluation metrics on offline and online testing both analytically and empirically by experimenting them on online advertising data from the Bing search engine. One of our findings is that some offline metrics like AUC (the Area Under the Receiver Operating Characteristic Curve) and RIG (Relative Information Gain) that summarize the model performance on the entire spectrum of operating points could be quite misleading sometimes and result in significant discrepancy in offline and online metrics. For example, for click prediction models for search advertising, errors in predictions in the very low range of predicted click scores impact the online performance much more negatively than errors in other regions. Most of the offline metrics we studied including AUC and RIG, however, are insensitive to such model behavior. We designed a new model evaluation paradigm that simulates the online behavior of predictive models. For a set of ads selected by a new prediction model, the online user behavior is estimated from the historic user behavior in the search logs. The experimental results on click prediction model for search advertising are highly promising.

#index 1992320
#* Amplifying the voice of youth in Africa via text analytics
#@ Prem Melville;Vijil Chenthamarakshan;Richard D. Lawrence;James Powell;Moses Mugisha;Sharad Sapra;Rajesh Anandan;Solomon Assefa
#t 2013
#c 0
#% 465754
#% 840882
#% 875974
#% 889273
#% 1176920
#% 1214749
#! U-report is an open-source SMS platform operated by UNICEF Uganda, designed to give community members a voice on issues that impact them. Data received by the system are either SMS responses to a poll conducted by UNICEF, or unsolicited reports of a problem occurring within the community. There are currently 200,000 U-report participants, and they send up to 10,000 unsolicited text messages a week. The objective of the program in Uganda is to understand the data in real-time, and have issues addressed by the appropriate department in UNICEF in a timely manner. Given the high volume and velocity of the data streams, manual inspection of all messages is no longer sustainable. This paper describes an automated message-understanding and routing system deployed by IBM at UNICEF. We employ recent advances in data mining to get the most out of labeled training data, while incorporating domain knowledge from experts. We discuss the trade-offs, design choices and challenges in applying such techniques in a real-world deployment.

#index 1992321
#* Online controlled experiments at large scale
#@ Ron Kohavi;Alex Deng;Brian Frasca;Toby Walker;Ya Xu;Nils Pohlmann
#t 2013
#c 0
#% 1087056
#% 1154062
#% 1214732
#% 1451140
#% 1512676
#% 1561592
#% 1872323
#% 1893805
#% 1910909
#% 1948135
#! Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.

#index 1992322
#* Uncertainty in online experiments with dependent data: an evaluation of bootstrap methods
#@ Eytan Bakshy;Dean Eckles
#t 2013
#c 0
#% 342636
#% 1154062
#% 1214732
#% 1543893
#% 1783934
#% 1872323
#! Many online experiments exhibit dependence between users and items. For example, in online advertising, observations that have a user or an ad in common are likely to be associated. Because of this, even in experiments involving millions of subjects, the difference in mean outcomes between control and treatment conditions can have substantial variance. Previous theoretical and simulation results demonstrate that not accounting for this kind of dependence structure can result in confidence intervals that are too narrow, leading to inaccurate hypothesis tests. We develop a framework for understanding how dependence affects uncertainty in user-item experiments and evaluate how bootstrap methods that account for differing levels of dependence perform in practice. We use three real datasets describing user behaviors on Facebook - user responses to ads, search results, and News Feed stories - to generate data for synthetic experiments in which there is no effect of the treatment on average by design. We then estimate empirical Type I error rates for each bootstrap method. Accounting for dependence within a single type of unit (i.e., within-user dependence) is often sufficient to get reasonable error rates. But when experiments have effects, as one might expect in the field, accounting for multiple units with a multiway bootstrap can be necessary to get close to the advertised Type I error rates. This work provides guidance to practitioners evaluating large-scale experiments, and highlights the importance of analysis of inferential methods for complex dependence structures common to online experiments.

#index 1992323
#* Discriminant malware distance learning on structural information for automated malware classification
#@ Deguang Kong;Guanhua Yan
#t 2013
#c 0
#% 520224
#% 664717
#% 961232
#% 1139257
#% 1298831
#% 1331284
#% 1403007
#% 1599341
#% 1627632
#% 1627673
#% 1647928
#% 1647930
#% 1712409
#% 1906835
#! The voluminous malware variants that appear in the Internet have posed severe threats to its security. In this work, we explore techniques that can automatically classify malware variants into their corresponding families. We present a generic framework that extracts structural information from malware programs as attributed function call graphs, in which rich malware features are encoded as attributes at the function level. Our framework further learns discriminant malware distance metrics that evaluate the similarity between the attributed function call graphs of two malware programs. To combine various types of malware attributes, our method adaptively learns the confidence level associated with the classification capability of each attribute type and then adopts an ensemble of classifiers for automated malware classification. We evaluate our approach with a number of Windows-based malware instances belonging to 11 families, and experimental results show that our automated malware classification method is able to achieve high classification accuracy.

#index 1992324
#* Towards long-lead forecasting of extreme flood events: a data mining framework for precipitation cluster precursors identification
#@ Dawei Wang;Wei Ding;Kui Yu;Xindong Wu;Ping Chen;David L. Small;Shafiqul Islam
#t 2013
#c 0
#% 232646
#% 243728
#% 722929
#% 765527
#% 793239
#% 1144401
#% 1984353
#! The development of disastrous flood forecasting techniques able to provide warnings at a long lead-time (5-15 days) is of great importance to society. Extreme Flood is usually a consequence of a sequence of precipitation events occurring over from several days to several weeks. Though precise short-term forecasting the magnitude and extent of individual precipitation event is still beyond our reach, long-term forecasting of precipitation clusters can be attempted by identifying persistent atmospheric regimes that are conducive for the precipitation clusters. However, such forecasting will suffer from overwhelming number of relevant features and high imbalance of sample sets. In this paper, we propose an integrated data mining framework for identifying the precursors to precipitation event clusters and use this information to predict extended periods of extreme precipitation and subsequent floods. We synthesize a representative feature set that describes the atmosphere motion, and apply a streaming feature selection algorithm to online identify the precipitation precursors from the enormous feature space. A hierarchical re-sampling approach is embedded in the framework to deal with the imbalance problem. An extensive empirical study is conducted on historical precipitation and associated flood data collected in the State of Iowa. Utilizing our framework a few physically meaningful precipitation cluster precursor sets are identified from millions of features. More than 90% of extreme precipitation events are captured by the proposed prediction model using precipitation cluster precursors with a lead time of more than 5 days.

#index 1992325
#* Dynamic memory allocation policies for postings in real-time Twitter search
#@ Nima Asadi;Jimmy Lin;Michael Busch
#t 2013
#c 0
#% 86532
#% 172922
#% 213786
#% 473237
#% 481439
#% 655485
#% 878624
#% 963669
#% 978404
#% 1055710
#% 1070892
#% 1190095
#% 1292511
#% 1486652
#% 1526990
#% 1846784
#! We explore a real-time Twitter search application where tweets are arriving at a rate of several thousands per second. Real-time search demands that they be indexed and searchable immediately, which leads to a number of implementation challenges. In this paper, we focus on one aspect: dynamic postings allocation policies for index structures that are completely held in main memory. The core issue can be characterized as a "Goldilocks Problem". Because memory remains today a scare resource, an allocation policy that is too aggressive leads to inefficient utilization, while a policy that is too conservative is slow and leads to fragmented postings lists. We present a dynamic postings allocation policy that allocates memory in increasingly-larger "slices" from a small number of large, fixed pools of memory. With an analytical model and experiments, we explore different settings that balance time (query evaluation speed) and space (memory utilization).

#index 1994433
#* The business impact of deep learning
#@ Jeremy Howard
#t 2013
#c 0
#! In the last year deep learning has gone from being a special purpose machine learning technique used mainly for image and speech recognition, to becoming a general purpose machine learning tool. This has broad implications for all organizations that rely on data analysis. It represents the latest development in a general trend towards more automated algorithms, and away from domain specific knowledge. For organizations that rely on domain expertise for their competitive advantage, this trend could be extremely disruptive. For start-ups interested in entering established markets, this trend could be a major opportunity. This talk will be a non-technical introduction to general-purpose deep learning, and its potential business impact.

#index 1994434
#* Hadoop: a view from the trenches
#@ Milind Bhandarkar
#t 2013
#c 0
#! From it's beginnings as a framework for building web crawlers for small-scale search engines to being one of the most promising technologies for building datacenter-scale distributed computing and storage platforms, Apache Hadoop has come far in the last seven years. In this talk I will reminisce about the early days of Hadoop, and will give an overview of the current state of the Hadoop ecosystem, and some real-world use cases of this open source platform. I will conclude with some crystal gazing in the future of Hadoop and associated technologies.

#index 1994435
#* To buy or not to buy: that is the question
#@ Oren Etzioni
#t 2013
#c 0
#! Shopping can be decomposed into three basic questions: what, where, and when to buy? In this talk, I'll describe how we utilize advanced data-mining and text-mining techniques at Decide.com (and earlier at Farecast) to solve these problems for on-line shoppers. Our algorithms have predicted prices utilizing billions of data points, and ranked products based on millions of reviews.

#index 1994436
#* Mining the digital universe of data to develop personalized cancer therapies
#@ Eric Schadt
#t 2013
#c 0
#! The development of a personalized approach to medical care is now well recognized as an urgent priority. This approach is particularly important in oncology, where it is well understood that each cancer diagnosis is unique at the molecular level, arising from a particular and specific collection of genetic alterations. Furthermore, taking a personalized approach to oncology may expedite the treatment process, pre-empting therapeutic decisions based on fewer data in favor of treatments targeted to an individual's tumor. This directed course may be key to survival for many patients who are terminal or have failed standard therapies.

#index 1994437
#* Targeting and influencing at scale: from presidential elections to social good
#@ Rayid Ghani
#t 2013
#c 0
#! If you're still recovering from the barrage of ads, news, emails, Facebook posts, and newspaper articles that were giving you the latest poll numbers, asking you to volunteer, donate money, and vote, this talk will give you a look behind the scenes on why you were seeing what you were seeing. I will talk about how machine learning and data mining along with randomized experiments were used to target and influence tens of millions of people. Beyond the presidential elections, these methodologies for targeting and influence have the power to solve big problems in education, healthcare, energy, transportation, and related areas. I will talk about some recent work we're doing at the University of Chicago Data Science for Social Good summer fellowship program working with non-profits and government organizations to tackle some of these challenges.

#index 1994438
#* Cyber security: how visual analytics unlock insight
#@ Raffael Marty
#t 2013
#c 0
#! In the Cyber Security domain, we have been collecting 'big data' for almost two decades. The volume and variety of our data is extremely large, but understanding and capturing the semantics of the data is even more of a challenge. Finding the needle in the proverbial haystack has been attempted from many different angles. In this talk we will have a look at what approaches have been explored, what has worked, and what has not. We will see that there is still a large amount of work to be done and data mining is going to play a central role. We'll try to motivate that in order to successfully find bad guys, we will have to embrace a solution that not only leverages clever data mining, but employs the right mix between human computer interfaces, data mining, and scalable data platforms. Traditionally, cyber security has been having its challenges with data mining. We are different. We will explore how to adopt data mining algorithms to the security domain. Some approaches like predictive analytics are extremely hard, if not impossible. How would you predict the next cyber attack? Others need to be tailored to the security domain to make them work. Visualization and visual analytics seem to be extremely promising to solve cyber security issues. Situational awareness, large-scale data exploration, knowledge capture, and forensic investigations are four top use-cases we will discuss. Visualization alone, however, does not solve security problems. We need algorithms that support the visualizations. For example to reduce the amount of data so an analyst can deal with it, in both volume and semantics.

#index 1994439
#* Adaptive adversaries: building systems to fight fraud and cyber intruders
#@ Ari Gesher
#t 2013
#c 0
#! Statistical machine learning / knowledge discovery techniques tend to fail when faced with an adaptive adversary attempting to evade detection in the data. Humans do an excellent job of correctly spotting adaptive adversaries given a good way to digest the data. On the other hand, humans are glacially slow and error-prone when it comes to moving through very large volumes of data, a task best left to the machines. Fighting complex fraud and cyber-security threats requires a symbiosis between the computers and teams of human analysts. The computers use algorithmic analysis, heuristics, and/or statistical characterization to find interesting 'simple' patterns in the data. These candidate events are then queued for in-depth human analysis in rich, expressive, interactive analysis environments. In this talk, we'll take a look at case studies of three different systems, using a partnership of automation and human analysis on large scale data to find the clandestine human behavior that these datasets hold, including a discussion of the backend systems architecture and a demo of the interactive analysis environment. The backend systems architecture is a mix of open source technologies, like Cassandra, Lucene, and Hadoop, and some new components that bind them all together. The interactive analysis environment allows seamless pivoting between semantic, geospatial, and temporal analysis with a powerful GUI interface that's usable by non-data scientists. The systems are real systems currently in use by commercial banks, pharmaceutical companies, and governments.

#index 1994440
#* Using "big data" to solve "small data" problems
#@ Chris Neumann
#t 2013
#c 0
#! The brief history of knowledge discovery is filled with products that promised to bring "BI to the masses". But how do you build a product that truly bridges the gap between the conceptual simplicity of "questions and answers" and the structure needed to query traditional data stores? In this talk, Chris Neumann will discuss how DataHero applied the principles of user-centric design and development over a year and a half to create a product with which more than 95% of new users can get answers on their first attempt. He'll demonstrate the process DataHero uses to determine the best combination of algorithms and user interface concepts needed to create intuitive solutions to potentially complex interactions, including: Determining the structure of files uploaded by users Accurately identifying data types within files Presenting users with an optimal visualization for any combination of data Helping users to ask questions of data when they don't know what to do Chris will also talk about what it's like to start a "Big Data" company and how he applied lessons from his time as the first engineer at Aster Data Systems to DataHero.

#index 1995186
#* The online revolution: education for everyone
#@ Andrew Ng;Daphne Koller
#t 2013
#c 0
#! In 2011, Stanford University offered three online courses, which anyone in the world could enroll in and take for free. Together, these three courses had enrollments of around 350,000 students, making this one of the largest experiments in online education ever performed. Since the beginning of 2012, we have transitioned this effort into a new venture, Coursera, a social entrepreneurship company whose mission is to make high-quality education accessible to everyone by allowing the best universities to offer courses to everyone around the world, for free. Coursera classes provide a real course experience to students, including video content, interactive exercises with meaningful feedback, using both auto-grading and peer-grading, and a rich peer-to-peer interaction around the course materials. Currently, Coursera has 62 university partners, and over 3 million students enrolled in its over 300 courses. These courses span a range of topics including computer science, business, medicine, science, humanities, social sciences, and more. In this talk, I'll report on this far-reaching experiment in education, and why we believe this model can provide both an improved classroom experience for our on-campus students, via a flipped classroom model, as well as a meaningful learning experience for the millions of students around the world who would otherwise never have access to education of this quality.

#index 1995187
#* Optimization in learning and data analysis
#@ Stephen J. Wright
#t 2013
#c 0
#! Optimization tools are vital to data analysis and learning. The optimization perspective has provided valuable insights, and optimization formulations have led to practical algorithms with good theoretical properties. In turn, the rich collection of problems in learning and data analysis is providing fresh perspectives on optimization algorithms and is driving new fundamental research in the area. We discuss research on several areas in this domain, including signal reconstruction, manifold learning, and regression/classification, describing in each case recent research in which optimization algorithms have been developed and applied successfully. A particular focus is asynchronous parallel algorithms for optimization and linear algebra, and their applications in data analysis and learning.

#index 1995188
#* Predicting the present with search engine data
#@ Hal Varian
#t 2013
#c 0
#! Many businesses now have almost real time data available about their operations. This data can be helpful in contemporaneous prediction ("nowcasting") of various economic indicators. We illustrate how one can use Google search data to nowcast economic metrics of interest, and discuss some of the ramifications for research and policy. Our approach combines three Bayesian techniques: Kalman filtering, spike-and-slab regression, and model averaging. We use Kalman filtering to whiten the time series in question by removing the trend and seasonal behavior. Spike-and-slab regression is a Bayesian method for variable selection that works even in cases where the number of predictors is far larger than the number of observations. Finally, we use Markov Chain Monte Carlo methods to sample from the posterior distribution for our model; the final forecast is an average over thousands of draws from the posterior. An advantage of the Bayesian approach is that it allows us to specify informative priors that affect the number and type of predictors in a flexible way.

#index 1995189
#* Scale-out beyond map-reduce
#@ Raghu Ramakrishnan;Team Members CISL
#t 2013
#c 0
#% 1468231
#! The amount and variety of data being collected in the enterprise is growing at a staggering pace. The default now is to capture and store any and all data, in anticipation of potential future strategic value, and vast amounts of data are being generated by instrumenting key customer and systems touch points. Until recently, data was gathered for well-defined objectives such as auditing, forensics, reporting and line-of-business operations; now, exploratory and predictive analysis is becoming ubiquitous. These differences in data heterogeneity, scale and usage are leading to a new generation of data management and analytic systems, where the emphasis is on supporting a wide range of large datasets to be stored uniformly and analyzed seamlessly using whatever techniques are most appropriate, including traditional tools like SQL and BI and newer tools, e.g., for machine learning. These new systems are necessarily based on scale-out architectures for both storage and computation. The terms Big Data and data science are often used to refer to this class of systems and applications. Hadoop has become a key building block in the new generation of scale-out systems. Early versions of analytic tools over Hadoop, such as Hive [1] and Pig [2] for SQL-like queries, were implemented by translation into Map-Reduce computations. This approach has inherent limitations, and the emergence of resource managers such as YARN [3] and Mesos [4] has opened the door for newer analytic tools to bypass the Map-Reduce layer. This trend is especially significant for iterative computations such as graph analytics and machine learning, for which Map-Reduce is widely recognized to be a poor fit. In fact, the website of the machine learning toolkit Apache Mahout [5] explicitly warns about the slow performance of some of the algorithms on Hadoop. In this talk, I will examine this architectural trend, and argue that resource managers are a first step in re-factoring the early implementations of Map-Reduce, and that more work is needed if we wish to support a variety of analytic tools on a common scale-out computational fabric. I will then present REEF, which runs on top of resource managers like YARN and provides support for task monitoring and restart, data movement and communications, and distributed state management. Finally, I will illustrate the value of using REEF to implement iterative algorithms for graph analytics and machine learning.

#index 1996904
#* Panel: a data scientist's guide to making money from start-ups
#@ Foster Provost;Geoffrey I. Webb
#t 2013
#c 0

#index 2002256
#* Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description
#@ Leman Akoglu;Emmanuel Müller;Jilles Vreeken
#t 2013
#c 0
#! Traditionally, outlier mining and anomaly discovery focused on the automatic detection of highly deviating objects. It has been studied for several decades in statistics, machine learning, and data mining, and has led to a lot of insight as well as automated systems for the detection of outliers. However, for today's applications to be successful, mere identification of anomalies alone is not enough. With more and more applications using outlier analysis for data exploration and knowledge discovery, the demand for manual verification and understanding of outliers is steadily increasing. Examples include applications such as health surveillance, fraud analysis, or sensor monitoring, where one is particularly interested in why an object seems outlying.

#index 2002265
#* Proceedings of the 12th International Workshop on Data Mining in Bioinformatics
#@ Jake Chen;Mohammed Zaki;Gaurav Pandey;Huzefa Rangwala;George Karypis
#t 2013
#c 0
#! Bioinformatics is the science of managing, mining, and interpreting information from biological data. Various genome projects have contributed to an exponential growth in DNA and protein sequence databases. Rapid advances in high-throughput technologies, such as microarrays, mass spectrometry and new/next-generation sequencing, can monitor quantitatively the presence or activity of thousands of genes, RNAs, proteins, metabolites, and compounds in a given biological state. The ongoing influx of these data, the pressing need to address complex biomedical challenge, and the gap between the two have collectively created exciting opportunities for data mining researchers.

#index 2002371
#* Proceedings of the 4th MultiClust Workshop on Multiple Clusterings, Multi-view Data, and Multi-source Knowledge-driven Clustering
#@ Ira Assent;Carlotta Domeniconi;Francesco Gullo;Andrea Tagarelli;Arthur Zimek
#t 2013
#c 0
#! Cluster detection is a very traditional data analysis task with several decades of research. However, it also includes a large variety of different subtopics investigated by different communities such as data mining, machine learning, statistics, and database systems. "Multiple Clusterings, Multi-view Data, and Multi-source Knowledge-driven Clustering" names several challenges around clustering: making sense or even making use of many, possibly redundant clustering results, of different representations and properties of data, of different sources of knowledge. Approaches such as ensemble clustering, semi-supervised clustering, subspace clustering meet around these problems. Yet they tackle these problems with different backgrounds, focus on different details, and include ideas from different research communities. This diversity is a major potential for this emerging field and should be highlighted by this workshop. A core motivation for this workshop series is our believe that these approaches are not just tackling different parts of the problem but that they should benefit from each other and, ultimately, combine the different perspectives and techniques to tackle the clustering problem more effectively. In paper presentations and discussions, we therefore would like to encourage the workshop participants to look at their own research problems from multiple perspectives.

#index 2002379
#* Proceedings of the 7th Workshop on Social Network Mining and Analysis
#@ 
#t 2013
#c 0
#! The seventh SNA-KDD workshop is proposed as the seventh in a successful series of workshops on social network mining and analysis co-held with KDD, soliciting experimental and theoretical work on social network mining and analysis in both online and offline social network systems. In recent years, social network research has advanced significantly, thanks to the prevalence of the online social websites and instant messaging systems as well as the availability of a variety of large-scale offline social network systems. These social network systems are usually characterized by the complex network structures and rich accompanying contextual information. Researchers are increasingly interested in addressing a wide range of challenges residing in these disparate social network systems, including identifying common static topological properties and dynamic properties during the formation and evolution of these social networks, and how contextual information can help in analyzing the pertaining social networks. These issues have important implications on community discovery, anomaly detection, trend prediction and can enhance applications in multiple domains such as information retrieval, recommendation systems, security and so on. The past SNA-KDD workshops have achieved significant attentions from the world-wide researchers working in different aspects of social network analysis, including knowledge discovery and data mining in social network, social network modeling, multi-agent based social network simulation, complex generic network analysis and other related studies that can bring inspirations or be directly applied to social network analysis. Each year we received more than 30 submissions. The average acceptance rate is around 1/3.

#index 2002394
#* Proceedings of the Seventh International Workshop on Data Mining for Online Advertising
#@ Esin Saka;Dou Shen;Bin Gao;Jun Yan;Ying Li
#t 2013
#c 0
#! Online advertising is a key component in the whole internet ecosystem and is growing rapidly with constantly evolving business models and practices. Examples of online advertising include sponsored search, display advertising, Rich Media Ads, interstitial ads, online classified advertising, e-mail marketing and so on. With the rapid growth of social network, social network advertising (as exemplified by Facebook and Groupon) is taking off and playing an important role in the whole landscape. Also, more and more offline ads for both big brands and local businesses are moving online. The online advertising industry is facing tons of challenges. For example, how to understand end users' need and advertisers' goals; what is the right strategy to connect user and ads; what ads should be delivered through which type of of advertising. These challenges bring great opportunities for researchers and data miners to come up with new technologies. Therefore, a forum for researchers and industry practitioners to exchange latest research results and construct collaborations will be of great service to the data mining community and generate value for the industry.

#index 2002491
#* Proceedings of the Thirteenth International Workshop on Multimedia Data Mining
#@ Aaron Baughman;Jia-Yu Pan;Jiang (John) Gao;Yu-Ru Lin;Vincent Oria
#t 2013
#c 0
#! The theme of the MDMKDD'13 workshop is\mining large scale rich content in a networked society." The workshop brought together experts in computational analysis on digital media content and multimedia databases, as well as knowledge engineers and domain experts from different applied disciplines with potential in multimedia data mining. The new theme of this year's workshop highlights the integration of multimedia in people's daily lives. With the rapid growth of mobile devices and personalized data, we wanted the workshop to focus on mining large scale rich content which is abundant in today's networked societies. Vast amount of multimedia data are produced, shared, and accessed everyday in various social platforms. These multimedia objects (images, videos, texts, tags, etc.) represent rich, multifaceted recordings of human behavior in the networked society, which lead to a range of social applications such as consumer behavior forecasting, social driven advertising / business, local knowledge discovery (e.g., for tourism or shopping), detection of emergent news events and trends, and so on. In addition to techniques for mining single media items, all of these applications require new methods for extracting robust features and discovering stable relationships among different media modalities and the users, in a dynamic, social context rich, and likely noisy environment. Multimedia is designed to stimulate the human senses beyond text. Using the rich content in multiple media types, we can capture the full experience of an event. Mining the data within media rich platforms and immersive environments (such as online communities, blogs, social networks, and virtual worlds) integrates the digital and physical experiences and information. The aim of the MDMKDD'13 workshop is to explore the role that multimedia data mining can play to enhance virtual society experiences.

#index 2002495
#* Proceedings of the 2nd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications
#@ Wei Fan;Albert Bifet;Qiang Yang;Philip Yu
#t 2013
#c 0
#! The aim of this workshop is to bring together people from both academia and industry to present their most recent work related to big-data issues, and exchange ideas and thoughts in order to advance this big-data challenge, which has been considered as one of the most exciting opportunities in the past 10 years. Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.

#index 2002750
#* Proceedings of the ACM SIGKDD Workshop on Interactive Data Exploration and Analytics
#@ Duen Horng Chau;Jilles Vreeken;Matthijs van Leeuwen;Christos Faloutsos
#t 2013
#c 0
#! We have entered the era of big data. Massive datasets, surpassing terabytes and petabytes in size are now commonplace. They arise in numerous settings in science, government, and enterprises, and technology exists by which we can collect and store such massive amounts of information. Yet, making sense of these data remains a fundamental challenge. We lack the means to exploratively analyze databases of this scale. Currently, few technologies allow us to freely "wander" around the data, and make discoveries by following our intuition, or serendipity. While standard data mining aims at finding highly interesting results, it is typically computationally demanding and time consuming, thus may not be well-suited for interactive exploration of large datasets. Interactive data mining techniques that aptly integrate human intuition, by means of visualization and intuitive human-computer interaction techniques, and machine computation support have been shown to help people gain significant insights into a wide range of problems. However, as datasets are being generated in larger volumes, higher velocity, and greater variety, creating effective interactive data mining techniques becomes a much harder task.

#index 2003177
#* Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining
#@ Erik Cambria;Bing Liu;Yongzheng Zhang;Yunqing Xia
#t 2013
#c 0
#! The exponential growth of the Social Web is virally infecting more and more critical business processes such as customer support and satisfaction, brand and reputation management, product design and marketing. Because of this global trend, web users already evolved from the era of social relationships, in which they began to get connected and started to share contents, to the era of social functionality, in which they started using social networks as the main platform for communication and dissemination of information. Today, web users are going through the era of social colonization, in which every experience on the Web can be social (e.g., Facebook Like button), and are getting ready for the era of social context, in which web contents will be highly targeted and personalized. The final stage of such Social Web evolution is the so called era of social commerce, in which communities will define future products and services. In such context, the research field of sentiment analysis, which has already been rapidly growing in the last decade, is destined to become more and more important for Web and business dynamics. To this end, the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM 2013: http://sentic.net/wisdom) aims to explore how the wisdom of the crowds is affecting (and will affect) the evolution of the Web and of businesses gravitating around it. In particular, the workshop explores two different stages of sentiment analysis: the former focusing on the identification of opinionated text over the Web, the latter focusing on the classification of such text either in terms of polarity detection or emotion recognition.

#index 2006313
#* Proceedings of the 2nd ACM SIGKDD International Workshop on Urban Computing
#@ Steven E. Koonin;Ouri E. Wolfson;Yu Zheng
#t 2013
#c 0
#! Urbanization's rapid progress has led to many big cities, which have modernized people's lives but also engendered big challenges, such as air pollution, increased energy consumption and traffic congestion. Tackling these challenges can seem nearly impossible years ago given the complex and dynamic settings of cities. Nowadays, sensing technologies and large-scale computing infrastructures have produced a variety of big data in urban spaces, e.g. human mobility, air quality, traffic patterns, and geographical data. The big data implies rich knowledge about a city and can help tackle these challenges when used correctly. Urban computing is a process of acquisition, integration, and analysis of big and heterogeneous data generated by a diversity of sources in urban spaces, such as sensors, devices, vehicles, buildings, and human, to tackle the major issues that cities face, e.g. air pollution, increased energy consumption and traffic congestion. Urban computing connects unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods, to create win-win-win solutions that improve urban environment, human life quality, and city operation systems. Urban computing also helps us understand the nature of urban phenomena and even predict the future of cities.

#index 2006596
#* Algorithmic techniques for modeling and mining large graphs (AMAzING)
#@ Alan Frieze;Aristides Gionis;Charalampos Tsourakakis
#t 2013
#c 0
#! Network science has emerged over the last years as an interdisciplinary area spanning traditional domains including mathematics, computer science, sociology, biology and economics. Since complexity in social, biological and economical systems, and more generally in complex systems, arises through pairwise interactions there exists a surging interest in understanding networks. In this tutorial, we will provide an in-depth presentation of the most popular random-graph models used for modeling real-world networks. We will then discuss efficient algorithmic techniques for mining large graphs, with emphasis on the problems of extracting graph sparsifiers, partitioning graphs into densely connected components, and finding dense subgraphs. We will motivate the problems we will discuss and the algorithms we will present with real-world applications. Our aim is to survey important results in the areas of modeling and mining large graphs, to uncover the intuition behind the key ideas, and to present future research directions.

#index 2006597
#* Mining data from mobile devices: a survey of smart sensing and analytics
#@ Spiros Papadimitriou;Tina Eliassi-Rad
#t 2013
#c 0
#! Mobile connected devices, and smartphones in particular, are rapidly emerging as a dominant computing and sensing platform. This poses several unique opportunities for data collection and analysis, as well as new challenges. In this tutorial, we survey the state-of-the-art in terms of mining data from mobile devices across different application areas such as ads, healthcare, geosocial, public policy, etc. Our tutorial has three parts. In part one, we summarize data collection in terms of various sensing modalities. In part two, we present cross-cutting challenges such as real-time analysis, security, and we outline cross cutting methods for mobile data mining such as network inference, streaming algorithms, etc. In the last part, we specifically overview emerging and fast-growing application areas, such as noted above. Concluding, we briefly highlight the opportunities for joint design of new data collection techniques and analysis methods, suggesting additional directions for future research.

#index 2006598
#* Big data analytics for healthcare
#@ Jimeng Sun;Chandan K. Reddy
#t 2013
#c 0
#! Large amounts of heterogeneous medical data have become available in various healthcare organizations (payers, providers, pharmaceuticals). Those data could be an enabling resource for deriving insights for improving care delivery and reducing waste. The enormity and complexity of these datasets present great challenges in analyses and subsequent applications to a practical clinical environment. In this tutorial, we introduce the characteristics and related mining challenges on dealing with big medical data. Many of those insights come from medical informatics community, which is highly related to data mining but focuses on biomedical specifics. We survey various related papers from data mining venues as well as medical informatics venues to share with the audiences key problems and trends in healthcare analytics research, with different applications ranging from clinical text mining, predictive modeling, survival analysis, patient similarity, genetic data analysis, and public health. The tutorial will include several case studies dealing with some of the important healthcare applications.

#index 2006599
#* Entity resolution for big data
#@ Lise Getoor;Ashwin Machanavajjhala
#t 2013
#c 0
#! Entity resolution (ER), the problem of extracting, matching and resolving entity mentions in structured and unstructured data, is a long-standing challenge in database management, information retrieval, machine learning, natural language processing and statistics. Accurate and fast entity resolution has huge practical implications in a wide variety of commercial, scientific and security domains. Despite the long history of work on entity resolution, there is still a surprising diversity of approaches, and lack of guiding theory. Meanwhile, in the age of big data, the need for high quality entity resolution is growing, as we are inundated with more and more data, all of which needs to be integrated, aligned and matched, before further utility can be extracted. In this tutorial, we bring together perspectives on entity resolution from a variety of fields, including databases, information retrieval, natural language processing and machine learning, to provide, in one setting, a survey of a large body of work. We discuss both the practical aspects and theoretical underpinnings of ER. We describe existing solutions, current challenges and open research problems. In addition to giving attendees a thorough understanding of existing ER models, algorithms and evaluation methods, the tutorial will cover important research topics such as scalable ER, active and lightly supervised ER, and query-driven ER.

#index 2006600
#* Network sampling
#@ Lise Getoor;Ashwin Machanavajjhala
#t 2013
#c 0
#! Network data appears in various domains, including social, communication, and information sciences. Analysis of such data is crucial for making inferences and predictions about these networks, and moreover, for understanding the different processes that drive their evolution. However, a major bottleneck to perform such an analysis is the massive size of real-life networks, which makes modeling and analyzing these networks simply infeasible. Further, many networks, specifically those that belong to social and communication domains, are not visible to the public due to privacy concerns, and other networks, such as the Web, are only accessible via crawling. Therefore, to overcome the above challenges, researchers use network sampling overwhelmingly as a key statistical approach to select a sub-population of interest that can be studied thoroughly. In this tutorial, we aim to cover a diverse collection of methodologies and applications of network sampling. We will begin with a discussion of the problem setting in terms of objectives (such as, sampling a representative subgraph, sampling graphlets, etc.), population of interest (vertices, edges, motifs), and sampling methodologies (such as Metropolis-Hastings, random walk, and snowball sampling). We will then present a number of applications of these methods, and will outline both the resulting opportunities and possible biases of different methods in each application.

#index 2006601
#* The dataminer's guide to scalable mixed-membership and nonparametric bayesian models
#@ Amr Ahmed;Alex Smola
#t 2013
#c 0
#! Large amounts of data arise in a multitude of situations, ranging from bioinformatics to astronomy, manufacturing, and medical applications. For concreteness our tutorial focuses on data obtained in the context of the internet, such as user generated content (microblogs, e-mails, messages), behavioral data (locations, interactions, clicks, queries), and graphs. Due to its magnitude, much of the challenges are to extract structure and interpretable models without the need for additional labels, i.e. to design effective unsupervised techniques. We present design patterns for hierarchical nonparametric Bayesian models, efficient inference algorithms, and modeling tools to describe salient aspects of the data.

#index 2015178
#* Proceedings of the 2013 KDD Cup 2013 Workshop
#@ 
#t 2013
#c 0

